[
    {
        "loss": 3.733,
        "grad_norm": 2.4812660217285156,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 0.00013785497656465398,
        "step": 1
    },
    {
        "loss": 4.0429,
        "grad_norm": 2.6583497524261475,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.00027570995312930797,
        "step": 2
    },
    {
        "loss": 3.7837,
        "grad_norm": 2.3952529430389404,
        "learning_rate": 2.4e-05,
        "epoch": 0.00041356492969396195,
        "step": 3
    },
    {
        "loss": 4.6086,
        "grad_norm": 4.531763553619385,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.0005514199062586159,
        "step": 4
    },
    {
        "loss": 4.251,
        "grad_norm": 3.7957420349121094,
        "learning_rate": 4e-05,
        "epoch": 0.0006892748828232699,
        "step": 5
    },
    {
        "loss": 3.9197,
        "grad_norm": 2.8660643100738525,
        "learning_rate": 4.8e-05,
        "epoch": 0.0008271298593879239,
        "step": 6
    },
    {
        "loss": 3.6061,
        "grad_norm": 2.17120361328125,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0009649848359525779,
        "step": 7
    },
    {
        "loss": 3.5339,
        "grad_norm": 2.3328139781951904,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0011028398125172319,
        "step": 8
    },
    {
        "loss": 3.8968,
        "grad_norm": 3.909489154815674,
        "learning_rate": 7.2e-05,
        "epoch": 0.0012406947890818859,
        "step": 9
    },
    {
        "loss": 3.4333,
        "grad_norm": 1.902338981628418,
        "learning_rate": 8e-05,
        "epoch": 0.0013785497656465398,
        "step": 10
    },
    {
        "loss": 3.4039,
        "grad_norm": 1.5733451843261719,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0015164047422111938,
        "step": 11
    },
    {
        "loss": 3.072,
        "grad_norm": 1.3452683687210083,
        "learning_rate": 9.6e-05,
        "epoch": 0.0016542597187758478,
        "step": 12
    },
    {
        "loss": 3.1304,
        "grad_norm": 1.468794584274292,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.0017921146953405018,
        "step": 13
    },
    {
        "loss": 3.5056,
        "grad_norm": 3.5360660552978516,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0019299696719051558,
        "step": 14
    },
    {
        "loss": 3.2565,
        "grad_norm": 2.4556221961975098,
        "learning_rate": 0.00012,
        "epoch": 0.0020678246484698098,
        "step": 15
    },
    {
        "loss": 3.3218,
        "grad_norm": 2.792968988418579,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0022056796250344637,
        "step": 16
    },
    {
        "loss": 2.6679,
        "grad_norm": 2.9995405673980713,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.0023435346015991177,
        "step": 17
    },
    {
        "loss": 3.1585,
        "grad_norm": 2.685710906982422,
        "learning_rate": 0.000144,
        "epoch": 0.0024813895781637717,
        "step": 18
    },
    {
        "loss": 2.8331,
        "grad_norm": 2.6747708320617676,
        "learning_rate": 0.000152,
        "epoch": 0.0026192445547284257,
        "step": 19
    },
    {
        "loss": 3.1201,
        "grad_norm": 1.8492579460144043,
        "learning_rate": 0.00016,
        "epoch": 0.0027570995312930797,
        "step": 20
    },
    {
        "loss": 1.8483,
        "grad_norm": 1.0352627038955688,
        "learning_rate": 0.000168,
        "epoch": 0.0028949545078577337,
        "step": 21
    },
    {
        "loss": 2.9793,
        "grad_norm": 1.9752631187438965,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.0030328094844223876,
        "step": 22
    },
    {
        "loss": 2.6077,
        "grad_norm": 2.531731605529785,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.0031706644609870416,
        "step": 23
    },
    {
        "loss": 2.697,
        "grad_norm": 2.0068628787994385,
        "learning_rate": 0.000192,
        "epoch": 0.0033085194375516956,
        "step": 24
    },
    {
        "loss": 2.7905,
        "grad_norm": 1.2493335008621216,
        "learning_rate": 0.0002,
        "epoch": 0.0034463744141163496,
        "step": 25
    },
    {
        "loss": 2.6454,
        "grad_norm": 1.8549350500106812,
        "learning_rate": 0.00019999991501236418,
        "epoch": 0.0035842293906810036,
        "step": 26
    },
    {
        "loss": 2.6851,
        "grad_norm": 1.6524975299835205,
        "learning_rate": 0.00019999966004960123,
        "epoch": 0.0037220843672456576,
        "step": 27
    },
    {
        "loss": 2.5899,
        "grad_norm": 1.6911808252334595,
        "learning_rate": 0.0001999992351121445,
        "epoch": 0.0038599393438103115,
        "step": 28
    },
    {
        "loss": 2.6415,
        "grad_norm": 1.4103202819824219,
        "learning_rate": 0.0001999986402007163,
        "epoch": 0.003997794320374966,
        "step": 29
    },
    {
        "loss": 2.5624,
        "grad_norm": 2.223031759262085,
        "learning_rate": 0.00019999787531632782,
        "epoch": 0.0041356492969396195,
        "step": 30
    },
    {
        "loss": 2.8321,
        "grad_norm": 1.322571873664856,
        "learning_rate": 0.00019999694046027913,
        "epoch": 0.004273504273504274,
        "step": 31
    },
    {
        "loss": 1.9628,
        "grad_norm": 1.1550815105438232,
        "learning_rate": 0.00019999583563415932,
        "epoch": 0.0044113592500689275,
        "step": 32
    },
    {
        "loss": 2.9346,
        "grad_norm": 1.5532951354980469,
        "learning_rate": 0.00019999456083984627,
        "epoch": 0.004549214226633582,
        "step": 33
    },
    {
        "loss": 2.6632,
        "grad_norm": 1.9106767177581787,
        "learning_rate": 0.00019999311607950685,
        "epoch": 0.0046870692031982355,
        "step": 34
    },
    {
        "loss": 2.8133,
        "grad_norm": 1.2613482475280762,
        "learning_rate": 0.00019999150135559677,
        "epoch": 0.00482492417976289,
        "step": 35
    },
    {
        "loss": 2.5824,
        "grad_norm": 1.639443039894104,
        "learning_rate": 0.0001999897166708607,
        "epoch": 0.004962779156327543,
        "step": 36
    },
    {
        "loss": 2.1111,
        "grad_norm": 1.996574878692627,
        "learning_rate": 0.00019998776202833214,
        "epoch": 0.005100634132892198,
        "step": 37
    },
    {
        "loss": 2.4241,
        "grad_norm": 1.8366892337799072,
        "learning_rate": 0.00019998563743133348,
        "epoch": 0.005238489109456851,
        "step": 38
    },
    {
        "loss": 2.1852,
        "grad_norm": 1.704593300819397,
        "learning_rate": 0.00019998334288347605,
        "epoch": 0.005376344086021506,
        "step": 39
    },
    {
        "loss": 2.073,
        "grad_norm": 2.196467876434326,
        "learning_rate": 0.00019998087838865995,
        "epoch": 0.005514199062586159,
        "step": 40
    },
    {
        "loss": 2.5197,
        "grad_norm": 1.4166030883789062,
        "learning_rate": 0.00019997824395107428,
        "epoch": 0.005652054039150814,
        "step": 41
    },
    {
        "loss": 2.2931,
        "grad_norm": 1.5646823644638062,
        "learning_rate": 0.0001999754395751969,
        "epoch": 0.005789909015715467,
        "step": 42
    },
    {
        "loss": 2.9153,
        "grad_norm": 1.2802444696426392,
        "learning_rate": 0.00019997246526579456,
        "epoch": 0.005927763992280122,
        "step": 43
    },
    {
        "loss": 2.1985,
        "grad_norm": 1.8888781070709229,
        "learning_rate": 0.00019996932102792287,
        "epoch": 0.006065618968844775,
        "step": 44
    },
    {
        "loss": 2.6886,
        "grad_norm": 1.5769163370132446,
        "learning_rate": 0.00019996600686692623,
        "epoch": 0.00620347394540943,
        "step": 45
    },
    {
        "loss": 2.0655,
        "grad_norm": 1.8471567630767822,
        "learning_rate": 0.0001999625227884379,
        "epoch": 0.006341328921974083,
        "step": 46
    },
    {
        "loss": 2.8395,
        "grad_norm": 1.2980602979660034,
        "learning_rate": 0.00019995886879837994,
        "epoch": 0.006479183898538738,
        "step": 47
    },
    {
        "loss": 2.4451,
        "grad_norm": 1.4437843561172485,
        "learning_rate": 0.00019995504490296324,
        "epoch": 0.006617038875103391,
        "step": 48
    },
    {
        "loss": 2.6606,
        "grad_norm": 1.5743868350982666,
        "learning_rate": 0.0001999510511086875,
        "epoch": 0.006754893851668046,
        "step": 49
    },
    {
        "loss": 2.442,
        "grad_norm": 1.326983094215393,
        "learning_rate": 0.00019994688742234115,
        "epoch": 0.006892748828232699,
        "step": 50
    },
    {
        "loss": 2.5952,
        "grad_norm": 1.397895336151123,
        "learning_rate": 0.00019994255385100146,
        "epoch": 0.007030603804797354,
        "step": 51
    },
    {
        "loss": 2.3273,
        "grad_norm": 1.804884433746338,
        "learning_rate": 0.0001999380504020344,
        "epoch": 0.007168458781362007,
        "step": 52
    },
    {
        "loss": 2.4983,
        "grad_norm": 1.5567487478256226,
        "learning_rate": 0.00019993337708309471,
        "epoch": 0.007306313757926662,
        "step": 53
    },
    {
        "loss": 2.457,
        "grad_norm": 1.4266104698181152,
        "learning_rate": 0.00019992853390212592,
        "epoch": 0.007444168734491315,
        "step": 54
    },
    {
        "loss": 2.4017,
        "grad_norm": 1.917037010192871,
        "learning_rate": 0.00019992352086736022,
        "epoch": 0.0075820237110559695,
        "step": 55
    },
    {
        "loss": 2.3731,
        "grad_norm": 1.9765232801437378,
        "learning_rate": 0.0001999183379873185,
        "epoch": 0.007719878687620623,
        "step": 56
    },
    {
        "loss": 2.7282,
        "grad_norm": 0.9562610983848572,
        "learning_rate": 0.00019991298527081042,
        "epoch": 0.007857733664185277,
        "step": 57
    },
    {
        "loss": 1.9754,
        "grad_norm": 1.9678025245666504,
        "learning_rate": 0.00019990746272693423,
        "epoch": 0.007995588640749932,
        "step": 58
    },
    {
        "loss": 2.5712,
        "grad_norm": 1.8895686864852905,
        "learning_rate": 0.00019990177036507693,
        "epoch": 0.008133443617314585,
        "step": 59
    },
    {
        "loss": 2.4502,
        "grad_norm": 1.440156102180481,
        "learning_rate": 0.0001998959081949141,
        "epoch": 0.008271298593879239,
        "step": 60
    },
    {
        "loss": 2.6237,
        "grad_norm": 1.6790943145751953,
        "learning_rate": 0.00019988987622641003,
        "epoch": 0.008409153570443893,
        "step": 61
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.9013272523880005,
        "learning_rate": 0.00019988367446981752,
        "epoch": 0.008547008547008548,
        "step": 62
    },
    {
        "loss": 2.3799,
        "grad_norm": 1.983487606048584,
        "learning_rate": 0.000199877302935678,
        "epoch": 0.008684863523573201,
        "step": 63
    },
    {
        "loss": 2.2031,
        "grad_norm": 2.1116716861724854,
        "learning_rate": 0.00019987076163482156,
        "epoch": 0.008822718500137855,
        "step": 64
    },
    {
        "loss": 1.9681,
        "grad_norm": 1.6722257137298584,
        "learning_rate": 0.00019986405057836678,
        "epoch": 0.008960573476702509,
        "step": 65
    },
    {
        "loss": 2.1057,
        "grad_norm": 1.697593092918396,
        "learning_rate": 0.00019985716977772078,
        "epoch": 0.009098428453267164,
        "step": 66
    },
    {
        "loss": 2.4473,
        "grad_norm": 1.1185401678085327,
        "learning_rate": 0.00019985011924457925,
        "epoch": 0.009236283429831817,
        "step": 67
    },
    {
        "loss": 1.8249,
        "grad_norm": 2.3212201595306396,
        "learning_rate": 0.0001998428989909263,
        "epoch": 0.009374138406396471,
        "step": 68
    },
    {
        "loss": 2.6023,
        "grad_norm": 1.1330971717834473,
        "learning_rate": 0.00019983550902903465,
        "epoch": 0.009511993382961124,
        "step": 69
    },
    {
        "loss": 2.647,
        "grad_norm": 0.9145181179046631,
        "learning_rate": 0.00019982794937146534,
        "epoch": 0.00964984835952578,
        "step": 70
    },
    {
        "loss": 2.3243,
        "grad_norm": 1.1580275297164917,
        "learning_rate": 0.00019982022003106799,
        "epoch": 0.009787703336090433,
        "step": 71
    },
    {
        "loss": 2.5607,
        "grad_norm": 1.8695615530014038,
        "learning_rate": 0.00019981232102098047,
        "epoch": 0.009925558312655087,
        "step": 72
    },
    {
        "loss": 2.4894,
        "grad_norm": 1.7224925756454468,
        "learning_rate": 0.0001998042523546292,
        "epoch": 0.01006341328921974,
        "step": 73
    },
    {
        "loss": 2.2303,
        "grad_norm": 1.7349908351898193,
        "learning_rate": 0.00019979601404572893,
        "epoch": 0.010201268265784396,
        "step": 74
    },
    {
        "loss": 2.4687,
        "grad_norm": 1.4124194383621216,
        "learning_rate": 0.00019978760610828272,
        "epoch": 0.01033912324234905,
        "step": 75
    },
    {
        "loss": 2.2647,
        "grad_norm": 1.3298313617706299,
        "learning_rate": 0.000199779028556582,
        "epoch": 0.010476978218913703,
        "step": 76
    },
    {
        "loss": 1.7229,
        "grad_norm": 2.263627529144287,
        "learning_rate": 0.00019977028140520648,
        "epoch": 0.010614833195478356,
        "step": 77
    },
    {
        "loss": 1.9766,
        "grad_norm": 1.1087218523025513,
        "learning_rate": 0.00019976136466902419,
        "epoch": 0.010752688172043012,
        "step": 78
    },
    {
        "loss": 1.7925,
        "grad_norm": 1.2963083982467651,
        "learning_rate": 0.0001997522783631913,
        "epoch": 0.010890543148607665,
        "step": 79
    },
    {
        "loss": 2.3046,
        "grad_norm": 2.5547432899475098,
        "learning_rate": 0.00019974302250315237,
        "epoch": 0.011028398125172319,
        "step": 80
    },
    {
        "loss": 2.6229,
        "grad_norm": 1.4332640171051025,
        "learning_rate": 0.00019973359710464,
        "epoch": 0.011166253101736972,
        "step": 81
    },
    {
        "loss": 2.284,
        "grad_norm": 1.8376456499099731,
        "learning_rate": 0.00019972400218367504,
        "epoch": 0.011304108078301628,
        "step": 82
    },
    {
        "loss": 2.7863,
        "grad_norm": 1.0437380075454712,
        "learning_rate": 0.00019971423775656656,
        "epoch": 0.011441963054866281,
        "step": 83
    },
    {
        "loss": 2.333,
        "grad_norm": 1.6069413423538208,
        "learning_rate": 0.0001997043038399116,
        "epoch": 0.011579818031430935,
        "step": 84
    },
    {
        "loss": 2.6908,
        "grad_norm": 2.334489107131958,
        "learning_rate": 0.0001996942004505954,
        "epoch": 0.011717673007995588,
        "step": 85
    },
    {
        "loss": 2.5052,
        "grad_norm": 1.8015397787094116,
        "learning_rate": 0.00019968392760579118,
        "epoch": 0.011855527984560243,
        "step": 86
    },
    {
        "loss": 1.9973,
        "grad_norm": 1.671631097793579,
        "learning_rate": 0.00019967348532296027,
        "epoch": 0.011993382961124897,
        "step": 87
    },
    {
        "loss": 2.2812,
        "grad_norm": 1.5484081506729126,
        "learning_rate": 0.000199662873619852,
        "epoch": 0.01213123793768955,
        "step": 88
    },
    {
        "loss": 2.5824,
        "grad_norm": 1.159646987915039,
        "learning_rate": 0.00019965209251450355,
        "epoch": 0.012269092914254204,
        "step": 89
    },
    {
        "loss": 2.5353,
        "grad_norm": 1.4393157958984375,
        "learning_rate": 0.0001996411420252402,
        "epoch": 0.01240694789081886,
        "step": 90
    },
    {
        "loss": 2.4761,
        "grad_norm": 1.963489055633545,
        "learning_rate": 0.00019963002217067506,
        "epoch": 0.012544802867383513,
        "step": 91
    },
    {
        "loss": 2.8275,
        "grad_norm": 1.0017483234405518,
        "learning_rate": 0.00019961873296970914,
        "epoch": 0.012682657843948167,
        "step": 92
    },
    {
        "loss": 2.155,
        "grad_norm": 1.9784103631973267,
        "learning_rate": 0.00019960727444153127,
        "epoch": 0.01282051282051282,
        "step": 93
    },
    {
        "loss": 2.6972,
        "grad_norm": 1.2334778308868408,
        "learning_rate": 0.00019959564660561815,
        "epoch": 0.012958367797077475,
        "step": 94
    },
    {
        "loss": 1.9819,
        "grad_norm": 2.449298858642578,
        "learning_rate": 0.0001995838494817342,
        "epoch": 0.013096222773642129,
        "step": 95
    },
    {
        "loss": 2.2067,
        "grad_norm": 1.5252512693405151,
        "learning_rate": 0.0001995718830899316,
        "epoch": 0.013234077750206782,
        "step": 96
    },
    {
        "loss": 1.3966,
        "grad_norm": 2.430755615234375,
        "learning_rate": 0.00019955974745055027,
        "epoch": 0.013371932726771436,
        "step": 97
    },
    {
        "loss": 2.286,
        "grad_norm": 1.443727731704712,
        "learning_rate": 0.0001995474425842178,
        "epoch": 0.013509787703336091,
        "step": 98
    },
    {
        "loss": 2.4711,
        "grad_norm": 1.8697566986083984,
        "learning_rate": 0.0001995349685118494,
        "epoch": 0.013647642679900745,
        "step": 99
    },
    {
        "loss": 2.3419,
        "grad_norm": 1.9055222272872925,
        "learning_rate": 0.00019952232525464797,
        "epoch": 0.013785497656465398,
        "step": 100
    },
    {
        "loss": 2.7531,
        "grad_norm": 1.3645894527435303,
        "learning_rate": 0.00019950951283410383,
        "epoch": 0.013923352633030052,
        "step": 101
    },
    {
        "loss": 2.3302,
        "grad_norm": 3.5097622871398926,
        "learning_rate": 0.000199496531271995,
        "epoch": 0.014061207609594707,
        "step": 102
    },
    {
        "loss": 1.7103,
        "grad_norm": 2.512552261352539,
        "learning_rate": 0.00019948338059038692,
        "epoch": 0.01419906258615936,
        "step": 103
    },
    {
        "loss": 2.2561,
        "grad_norm": 2.0871105194091797,
        "learning_rate": 0.00019947006081163244,
        "epoch": 0.014336917562724014,
        "step": 104
    },
    {
        "loss": 2.275,
        "grad_norm": 1.7006080150604248,
        "learning_rate": 0.00019945657195837195,
        "epoch": 0.014474772539288668,
        "step": 105
    },
    {
        "loss": 2.1236,
        "grad_norm": 1.9191681146621704,
        "learning_rate": 0.00019944291405353312,
        "epoch": 0.014612627515853323,
        "step": 106
    },
    {
        "loss": 2.4142,
        "grad_norm": 1.1665804386138916,
        "learning_rate": 0.0001994290871203311,
        "epoch": 0.014750482492417977,
        "step": 107
    },
    {
        "loss": 2.533,
        "grad_norm": 1.6149389743804932,
        "learning_rate": 0.0001994150911822681,
        "epoch": 0.01488833746898263,
        "step": 108
    },
    {
        "loss": 1.5003,
        "grad_norm": 1.8156670331954956,
        "learning_rate": 0.0001994009262631339,
        "epoch": 0.015026192445547284,
        "step": 109
    },
    {
        "loss": 2.5833,
        "grad_norm": 1.219030499458313,
        "learning_rate": 0.00019938659238700527,
        "epoch": 0.015164047422111939,
        "step": 110
    },
    {
        "loss": 1.9961,
        "grad_norm": 2.167914390563965,
        "learning_rate": 0.0001993720895782463,
        "epoch": 0.015301902398676593,
        "step": 111
    },
    {
        "loss": 2.3694,
        "grad_norm": 1.5130391120910645,
        "learning_rate": 0.00019935741786150817,
        "epoch": 0.015439757375241246,
        "step": 112
    },
    {
        "loss": 2.5262,
        "grad_norm": 1.6696982383728027,
        "learning_rate": 0.00019934257726172917,
        "epoch": 0.0155776123518059,
        "step": 113
    },
    {
        "loss": 2.1347,
        "grad_norm": 1.482757806777954,
        "learning_rate": 0.00019932756780413464,
        "epoch": 0.015715467328370553,
        "step": 114
    },
    {
        "loss": 2.2627,
        "grad_norm": 1.6233919858932495,
        "learning_rate": 0.00019931238951423696,
        "epoch": 0.01585332230493521,
        "step": 115
    },
    {
        "loss": 2.0965,
        "grad_norm": 1.872603416442871,
        "learning_rate": 0.00019929704241783547,
        "epoch": 0.015991177281499864,
        "step": 116
    },
    {
        "loss": 1.9889,
        "grad_norm": 1.7991899251937866,
        "learning_rate": 0.00019928152654101643,
        "epoch": 0.016129032258064516,
        "step": 117
    },
    {
        "loss": 2.2551,
        "grad_norm": 1.6347565650939941,
        "learning_rate": 0.00019926584191015296,
        "epoch": 0.01626688723462917,
        "step": 118
    },
    {
        "loss": 2.5922,
        "grad_norm": 1.1450022459030151,
        "learning_rate": 0.00019924998855190513,
        "epoch": 0.016404742211193823,
        "step": 119
    },
    {
        "loss": 2.4531,
        "grad_norm": 0.9369381070137024,
        "learning_rate": 0.00019923396649321968,
        "epoch": 0.016542597187758478,
        "step": 120
    },
    {
        "loss": 2.4011,
        "grad_norm": 1.1244628429412842,
        "learning_rate": 0.00019921777576133013,
        "epoch": 0.016680452164323133,
        "step": 121
    },
    {
        "loss": 2.4377,
        "grad_norm": 1.1524031162261963,
        "learning_rate": 0.00019920141638375677,
        "epoch": 0.016818307140887785,
        "step": 122
    },
    {
        "loss": 2.6739,
        "grad_norm": 0.9374446272850037,
        "learning_rate": 0.00019918488838830648,
        "epoch": 0.01695616211745244,
        "step": 123
    },
    {
        "loss": 2.5871,
        "grad_norm": 1.0724024772644043,
        "learning_rate": 0.00019916819180307275,
        "epoch": 0.017094017094017096,
        "step": 124
    },
    {
        "loss": 1.941,
        "grad_norm": 2.1268739700317383,
        "learning_rate": 0.00019915132665643563,
        "epoch": 0.017231872070581748,
        "step": 125
    },
    {
        "loss": 2.4626,
        "grad_norm": 1.40766179561615,
        "learning_rate": 0.00019913429297706175,
        "epoch": 0.017369727047146403,
        "step": 126
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.3543466329574585,
        "learning_rate": 0.00019911709079390412,
        "epoch": 0.017507582023711055,
        "step": 127
    },
    {
        "loss": 2.4886,
        "grad_norm": 1.7321996688842773,
        "learning_rate": 0.0001990997201362022,
        "epoch": 0.01764543700027571,
        "step": 128
    },
    {
        "loss": 2.1754,
        "grad_norm": 1.5787001848220825,
        "learning_rate": 0.00019908218103348183,
        "epoch": 0.017783291976840365,
        "step": 129
    },
    {
        "loss": 1.5154,
        "grad_norm": 2.0467166900634766,
        "learning_rate": 0.00019906447351555513,
        "epoch": 0.017921146953405017,
        "step": 130
    },
    {
        "loss": 2.5303,
        "grad_norm": 1.4853287935256958,
        "learning_rate": 0.00019904659761252046,
        "epoch": 0.018059001929969672,
        "step": 131
    },
    {
        "loss": 2.6515,
        "grad_norm": 1.2950727939605713,
        "learning_rate": 0.00019902855335476253,
        "epoch": 0.018196856906534328,
        "step": 132
    },
    {
        "loss": 2.7928,
        "grad_norm": 1.3146111965179443,
        "learning_rate": 0.00019901034077295207,
        "epoch": 0.01833471188309898,
        "step": 133
    },
    {
        "loss": 2.3387,
        "grad_norm": 1.6734124422073364,
        "learning_rate": 0.00019899195989804593,
        "epoch": 0.018472566859663635,
        "step": 134
    },
    {
        "loss": 2.5069,
        "grad_norm": 1.029517412185669,
        "learning_rate": 0.0001989734107612871,
        "epoch": 0.018610421836228287,
        "step": 135
    },
    {
        "loss": 2.3957,
        "grad_norm": 1.3912023305892944,
        "learning_rate": 0.0001989546933942045,
        "epoch": 0.018748276812792942,
        "step": 136
    },
    {
        "loss": 2.568,
        "grad_norm": 1.0570510625839233,
        "learning_rate": 0.00019893580782861302,
        "epoch": 0.018886131789357597,
        "step": 137
    },
    {
        "loss": 2.4471,
        "grad_norm": 1.103439211845398,
        "learning_rate": 0.0001989167540966135,
        "epoch": 0.01902398676592225,
        "step": 138
    },
    {
        "loss": 2.3678,
        "grad_norm": 1.266758680343628,
        "learning_rate": 0.00019889753223059251,
        "epoch": 0.019161841742486904,
        "step": 139
    },
    {
        "loss": 2.1481,
        "grad_norm": 1.994914174079895,
        "learning_rate": 0.0001988781422632225,
        "epoch": 0.01929969671905156,
        "step": 140
    },
    {
        "loss": 2.2801,
        "grad_norm": 1.243700623512268,
        "learning_rate": 0.00019885858422746166,
        "epoch": 0.01943755169561621,
        "step": 141
    },
    {
        "loss": 2.4583,
        "grad_norm": 1.0914943218231201,
        "learning_rate": 0.00019883885815655372,
        "epoch": 0.019575406672180867,
        "step": 142
    },
    {
        "loss": 2.6402,
        "grad_norm": 1.3632216453552246,
        "learning_rate": 0.0001988189640840282,
        "epoch": 0.01971326164874552,
        "step": 143
    },
    {
        "loss": 2.6597,
        "grad_norm": 0.9657469391822815,
        "learning_rate": 0.00019879890204370007,
        "epoch": 0.019851116625310174,
        "step": 144
    },
    {
        "loss": 2.0458,
        "grad_norm": 2.203744888305664,
        "learning_rate": 0.00019877867206966984,
        "epoch": 0.01998897160187483,
        "step": 145
    },
    {
        "loss": 1.9002,
        "grad_norm": 2.257368803024292,
        "learning_rate": 0.00019875827419632347,
        "epoch": 0.02012682657843948,
        "step": 146
    },
    {
        "loss": 2.4524,
        "grad_norm": 1.4596073627471924,
        "learning_rate": 0.0001987377084583323,
        "epoch": 0.020264681555004136,
        "step": 147
    },
    {
        "loss": 2.3857,
        "grad_norm": 1.8128849267959595,
        "learning_rate": 0.00019871697489065303,
        "epoch": 0.02040253653156879,
        "step": 148
    },
    {
        "loss": 2.7853,
        "grad_norm": 1.3527870178222656,
        "learning_rate": 0.00019869607352852753,
        "epoch": 0.020540391508133443,
        "step": 149
    },
    {
        "loss": 2.199,
        "grad_norm": 1.3014061450958252,
        "learning_rate": 0.000198675004407483,
        "epoch": 0.0206782464846981,
        "step": 150
    },
    {
        "loss": 2.2846,
        "grad_norm": 1.8358429670333862,
        "learning_rate": 0.00019865376756333168,
        "epoch": 0.02081610146126275,
        "step": 151
    },
    {
        "loss": 2.236,
        "grad_norm": 1.7601447105407715,
        "learning_rate": 0.00019863236303217105,
        "epoch": 0.020953956437827406,
        "step": 152
    },
    {
        "loss": 1.8032,
        "grad_norm": 1.8100416660308838,
        "learning_rate": 0.00019861079085038342,
        "epoch": 0.02109181141439206,
        "step": 153
    },
    {
        "loss": 1.7021,
        "grad_norm": 3.0495550632476807,
        "learning_rate": 0.00019858905105463623,
        "epoch": 0.021229666390956713,
        "step": 154
    },
    {
        "loss": 2.4816,
        "grad_norm": 1.7759277820587158,
        "learning_rate": 0.0001985671436818817,
        "epoch": 0.021367521367521368,
        "step": 155
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.6404742002487183,
        "learning_rate": 0.00019854506876935705,
        "epoch": 0.021505376344086023,
        "step": 156
    },
    {
        "loss": 2.4378,
        "grad_norm": 1.22380793094635,
        "learning_rate": 0.00019852282635458407,
        "epoch": 0.021643231320650675,
        "step": 157
    },
    {
        "loss": 2.5254,
        "grad_norm": 1.2916706800460815,
        "learning_rate": 0.00019850041647536936,
        "epoch": 0.02178108629721533,
        "step": 158
    },
    {
        "loss": 2.4851,
        "grad_norm": 1.506791114807129,
        "learning_rate": 0.00019847783916980425,
        "epoch": 0.021918941273779982,
        "step": 159
    },
    {
        "loss": 2.3742,
        "grad_norm": 1.3894981145858765,
        "learning_rate": 0.0001984550944762645,
        "epoch": 0.022056796250344637,
        "step": 160
    },
    {
        "loss": 1.5682,
        "grad_norm": 1.9581093788146973,
        "learning_rate": 0.00019843218243341054,
        "epoch": 0.022194651226909293,
        "step": 161
    },
    {
        "loss": 2.0897,
        "grad_norm": 1.6073024272918701,
        "learning_rate": 0.00019840910308018713,
        "epoch": 0.022332506203473945,
        "step": 162
    },
    {
        "loss": 2.5448,
        "grad_norm": 1.3325166702270508,
        "learning_rate": 0.00019838585645582345,
        "epoch": 0.0224703611800386,
        "step": 163
    },
    {
        "loss": 2.346,
        "grad_norm": 1.0179061889648438,
        "learning_rate": 0.00019836244259983308,
        "epoch": 0.022608216156603255,
        "step": 164
    },
    {
        "loss": 2.5631,
        "grad_norm": 1.177680492401123,
        "learning_rate": 0.00019833886155201373,
        "epoch": 0.022746071133167907,
        "step": 165
    },
    {
        "loss": 2.2855,
        "grad_norm": 1.710446834564209,
        "learning_rate": 0.00019831511335244732,
        "epoch": 0.022883926109732562,
        "step": 166
    },
    {
        "loss": 2.1845,
        "grad_norm": 1.397425889968872,
        "learning_rate": 0.0001982911980415,
        "epoch": 0.023021781086297214,
        "step": 167
    },
    {
        "loss": 2.2951,
        "grad_norm": 1.8682912588119507,
        "learning_rate": 0.00019826711565982183,
        "epoch": 0.02315963606286187,
        "step": 168
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.5266937017440796,
        "learning_rate": 0.0001982428662483469,
        "epoch": 0.023297491039426525,
        "step": 169
    },
    {
        "loss": 1.7003,
        "grad_norm": 1.775898814201355,
        "learning_rate": 0.00019821844984829325,
        "epoch": 0.023435346015991176,
        "step": 170
    },
    {
        "loss": 2.3387,
        "grad_norm": 1.8080425262451172,
        "learning_rate": 0.0001981938665011627,
        "epoch": 0.02357320099255583,
        "step": 171
    },
    {
        "loss": 2.0024,
        "grad_norm": 2.4956111907958984,
        "learning_rate": 0.00019816911624874085,
        "epoch": 0.023711055969120487,
        "step": 172
    },
    {
        "loss": 2.0879,
        "grad_norm": 1.6325620412826538,
        "learning_rate": 0.000198144199133097,
        "epoch": 0.02384891094568514,
        "step": 173
    },
    {
        "loss": 2.4969,
        "grad_norm": 1.2093043327331543,
        "learning_rate": 0.00019811911519658414,
        "epoch": 0.023986765922249794,
        "step": 174
    },
    {
        "loss": 1.8131,
        "grad_norm": 1.6698325872421265,
        "learning_rate": 0.00019809386448183872,
        "epoch": 0.024124620898814446,
        "step": 175
    },
    {
        "loss": 1.6275,
        "grad_norm": 1.7123953104019165,
        "learning_rate": 0.0001980684470317807,
        "epoch": 0.0242624758753791,
        "step": 176
    },
    {
        "loss": 1.7283,
        "grad_norm": 1.7910096645355225,
        "learning_rate": 0.00019804286288961352,
        "epoch": 0.024400330851943756,
        "step": 177
    },
    {
        "loss": 2.5287,
        "grad_norm": 1.2649906873703003,
        "learning_rate": 0.0001980171120988238,
        "epoch": 0.024538185828508408,
        "step": 178
    },
    {
        "loss": 2.5238,
        "grad_norm": 1.8632314205169678,
        "learning_rate": 0.0001979911947031816,
        "epoch": 0.024676040805073064,
        "step": 179
    },
    {
        "loss": 2.4134,
        "grad_norm": 2.454719066619873,
        "learning_rate": 0.00019796511074674005,
        "epoch": 0.02481389578163772,
        "step": 180
    },
    {
        "loss": 2.551,
        "grad_norm": 1.7046502828598022,
        "learning_rate": 0.00019793886027383542,
        "epoch": 0.02495175075820237,
        "step": 181
    },
    {
        "loss": 2.4692,
        "grad_norm": 1.8269226551055908,
        "learning_rate": 0.000197912443329087,
        "epoch": 0.025089605734767026,
        "step": 182
    },
    {
        "loss": 1.383,
        "grad_norm": 2.1829755306243896,
        "learning_rate": 0.00019788585995739712,
        "epoch": 0.025227460711331678,
        "step": 183
    },
    {
        "loss": 1.4449,
        "grad_norm": 0.8752633929252625,
        "learning_rate": 0.0001978591102039509,
        "epoch": 0.025365315687896333,
        "step": 184
    },
    {
        "loss": 2.2834,
        "grad_norm": 1.22714364528656,
        "learning_rate": 0.00019783219411421634,
        "epoch": 0.02550317066446099,
        "step": 185
    },
    {
        "loss": 2.2157,
        "grad_norm": 1.0709137916564941,
        "learning_rate": 0.00019780511173394413,
        "epoch": 0.02564102564102564,
        "step": 186
    },
    {
        "loss": 2.5296,
        "grad_norm": 1.0396562814712524,
        "learning_rate": 0.00019777786310916755,
        "epoch": 0.025778880617590295,
        "step": 187
    },
    {
        "loss": 2.4217,
        "grad_norm": 1.5955201387405396,
        "learning_rate": 0.00019775044828620263,
        "epoch": 0.02591673559415495,
        "step": 188
    },
    {
        "loss": 2.3634,
        "grad_norm": 1.7138986587524414,
        "learning_rate": 0.0001977228673116477,
        "epoch": 0.026054590570719603,
        "step": 189
    },
    {
        "loss": 2.686,
        "grad_norm": 1.008980631828308,
        "learning_rate": 0.0001976951202323836,
        "epoch": 0.026192445547284258,
        "step": 190
    },
    {
        "loss": 2.0014,
        "grad_norm": 2.306152820587158,
        "learning_rate": 0.00019766720709557363,
        "epoch": 0.02633030052384891,
        "step": 191
    },
    {
        "loss": 1.9042,
        "grad_norm": 1.172062635421753,
        "learning_rate": 0.00019763912794866305,
        "epoch": 0.026468155500413565,
        "step": 192
    },
    {
        "loss": 2.3722,
        "grad_norm": 1.8076443672180176,
        "learning_rate": 0.0001976108828393796,
        "epoch": 0.02660601047697822,
        "step": 193
    },
    {
        "loss": 2.6703,
        "grad_norm": 1.372022032737732,
        "learning_rate": 0.00019758247181573288,
        "epoch": 0.026743865453542872,
        "step": 194
    },
    {
        "loss": 2.2557,
        "grad_norm": 1.9401355981826782,
        "learning_rate": 0.0001975538949260147,
        "epoch": 0.026881720430107527,
        "step": 195
    },
    {
        "loss": 2.134,
        "grad_norm": 2.0222439765930176,
        "learning_rate": 0.0001975251522187986,
        "epoch": 0.027019575406672183,
        "step": 196
    },
    {
        "loss": 2.1956,
        "grad_norm": 2.3975203037261963,
        "learning_rate": 0.00019749624374294013,
        "epoch": 0.027157430383236834,
        "step": 197
    },
    {
        "loss": 2.0269,
        "grad_norm": 2.436561107635498,
        "learning_rate": 0.00019746716954757658,
        "epoch": 0.02729528535980149,
        "step": 198
    },
    {
        "loss": 2.3288,
        "grad_norm": 1.4501888751983643,
        "learning_rate": 0.00019743792968212684,
        "epoch": 0.02743314033636614,
        "step": 199
    },
    {
        "loss": 2.7051,
        "grad_norm": 1.2250468730926514,
        "learning_rate": 0.0001974085241962915,
        "epoch": 0.027570995312930797,
        "step": 200
    },
    {
        "loss": 2.5563,
        "grad_norm": 1.2324661016464233,
        "learning_rate": 0.0001973789531400526,
        "epoch": 0.027708850289495452,
        "step": 201
    },
    {
        "loss": 2.4014,
        "grad_norm": 1.5027416944503784,
        "learning_rate": 0.00019734921656367358,
        "epoch": 0.027846705266060104,
        "step": 202
    },
    {
        "loss": 2.5255,
        "grad_norm": 1.3058960437774658,
        "learning_rate": 0.0001973193145176993,
        "epoch": 0.02798456024262476,
        "step": 203
    },
    {
        "loss": 2.7523,
        "grad_norm": 1.2841354608535767,
        "learning_rate": 0.00019728924705295583,
        "epoch": 0.028122415219189414,
        "step": 204
    },
    {
        "loss": 2.4358,
        "grad_norm": 1.2262771129608154,
        "learning_rate": 0.00019725901422055047,
        "epoch": 0.028260270195754066,
        "step": 205
    },
    {
        "loss": 2.6357,
        "grad_norm": 1.2326467037200928,
        "learning_rate": 0.0001972286160718715,
        "epoch": 0.02839812517231872,
        "step": 206
    },
    {
        "loss": 2.376,
        "grad_norm": 1.5135109424591064,
        "learning_rate": 0.00019719805265858828,
        "epoch": 0.028535980148883373,
        "step": 207
    },
    {
        "loss": 2.3942,
        "grad_norm": 1.1022158861160278,
        "learning_rate": 0.00019716732403265107,
        "epoch": 0.02867383512544803,
        "step": 208
    },
    {
        "loss": 2.0466,
        "grad_norm": 2.858400344848633,
        "learning_rate": 0.00019713643024629095,
        "epoch": 0.028811690102012684,
        "step": 209
    },
    {
        "loss": 2.6986,
        "grad_norm": 1.9165422916412354,
        "learning_rate": 0.00019710537135201963,
        "epoch": 0.028949545078577336,
        "step": 210
    },
    {
        "loss": 2.1261,
        "grad_norm": 0.9872068762779236,
        "learning_rate": 0.00019707414740262964,
        "epoch": 0.02908740005514199,
        "step": 211
    },
    {
        "loss": 2.4596,
        "grad_norm": 1.271957516670227,
        "learning_rate": 0.00019704275845119394,
        "epoch": 0.029225255031706646,
        "step": 212
    },
    {
        "loss": 2.1665,
        "grad_norm": 1.2375866174697876,
        "learning_rate": 0.00019701120455106597,
        "epoch": 0.029363110008271298,
        "step": 213
    },
    {
        "loss": 2.3837,
        "grad_norm": 1.2081108093261719,
        "learning_rate": 0.0001969794857558796,
        "epoch": 0.029500964984835953,
        "step": 214
    },
    {
        "loss": 2.4954,
        "grad_norm": 1.4118211269378662,
        "learning_rate": 0.00019694760211954888,
        "epoch": 0.029638819961400605,
        "step": 215
    },
    {
        "loss": 2.4828,
        "grad_norm": 1.7011804580688477,
        "learning_rate": 0.00019691555369626815,
        "epoch": 0.02977667493796526,
        "step": 216
    },
    {
        "loss": 1.6028,
        "grad_norm": 2.7793147563934326,
        "learning_rate": 0.0001968833405405118,
        "epoch": 0.029914529914529916,
        "step": 217
    },
    {
        "loss": 2.1374,
        "grad_norm": 1.0986253023147583,
        "learning_rate": 0.00019685096270703425,
        "epoch": 0.030052384891094568,
        "step": 218
    },
    {
        "loss": 2.3247,
        "grad_norm": 1.9495670795440674,
        "learning_rate": 0.00019681842025086975,
        "epoch": 0.030190239867659223,
        "step": 219
    },
    {
        "loss": 2.2573,
        "grad_norm": 1.8670032024383545,
        "learning_rate": 0.00019678571322733247,
        "epoch": 0.030328094844223878,
        "step": 220
    },
    {
        "loss": 2.5295,
        "grad_norm": 1.3438920974731445,
        "learning_rate": 0.00019675284169201623,
        "epoch": 0.03046594982078853,
        "step": 221
    },
    {
        "loss": 2.6506,
        "grad_norm": 1.4047077894210815,
        "learning_rate": 0.00019671980570079459,
        "epoch": 0.030603804797353185,
        "step": 222
    },
    {
        "loss": 1.8595,
        "grad_norm": 1.9663162231445312,
        "learning_rate": 0.00019668660530982045,
        "epoch": 0.030741659773917837,
        "step": 223
    },
    {
        "loss": 2.1012,
        "grad_norm": 1.3836549520492554,
        "learning_rate": 0.00019665324057552635,
        "epoch": 0.030879514750482492,
        "step": 224
    },
    {
        "loss": 2.3965,
        "grad_norm": 1.6484618186950684,
        "learning_rate": 0.0001966197115546241,
        "epoch": 0.031017369727047148,
        "step": 225
    },
    {
        "loss": 2.5819,
        "grad_norm": 1.159587025642395,
        "learning_rate": 0.00019658601830410466,
        "epoch": 0.0311552247036118,
        "step": 226
    },
    {
        "loss": 2.2075,
        "grad_norm": 1.5368095636367798,
        "learning_rate": 0.0001965521608812383,
        "epoch": 0.03129307968017645,
        "step": 227
    },
    {
        "loss": 2.4126,
        "grad_norm": 1.283727765083313,
        "learning_rate": 0.00019651813934357424,
        "epoch": 0.03143093465674111,
        "step": 228
    },
    {
        "loss": 2.2614,
        "grad_norm": 1.2484855651855469,
        "learning_rate": 0.00019648395374894067,
        "epoch": 0.03156878963330576,
        "step": 229
    },
    {
        "loss": 1.8887,
        "grad_norm": 1.450015902519226,
        "learning_rate": 0.00019644960415544466,
        "epoch": 0.03170664460987042,
        "step": 230
    },
    {
        "loss": 2.1992,
        "grad_norm": 1.4644246101379395,
        "learning_rate": 0.00019641509062147202,
        "epoch": 0.03184449958643507,
        "step": 231
    },
    {
        "loss": 2.0947,
        "grad_norm": 1.630894660949707,
        "learning_rate": 0.0001963804132056872,
        "epoch": 0.03198235456299973,
        "step": 232
    },
    {
        "loss": 2.3365,
        "grad_norm": 1.5699082612991333,
        "learning_rate": 0.00019634557196703328,
        "epoch": 0.032120209539564376,
        "step": 233
    },
    {
        "loss": 1.7931,
        "grad_norm": 1.461956262588501,
        "learning_rate": 0.00019631056696473172,
        "epoch": 0.03225806451612903,
        "step": 234
    },
    {
        "loss": 2.6337,
        "grad_norm": 1.0749553442001343,
        "learning_rate": 0.0001962753982582824,
        "epoch": 0.03239591949269369,
        "step": 235
    },
    {
        "loss": 2.3428,
        "grad_norm": 1.3997056484222412,
        "learning_rate": 0.00019624006590746338,
        "epoch": 0.03253377446925834,
        "step": 236
    },
    {
        "loss": 2.4724,
        "grad_norm": 1.103049635887146,
        "learning_rate": 0.00019620456997233095,
        "epoch": 0.032671629445823,
        "step": 237
    },
    {
        "loss": 1.981,
        "grad_norm": 2.0890841484069824,
        "learning_rate": 0.00019616891051321938,
        "epoch": 0.032809484422387646,
        "step": 238
    },
    {
        "loss": 2.4197,
        "grad_norm": 1.2902681827545166,
        "learning_rate": 0.000196133087590741,
        "epoch": 0.0329473393989523,
        "step": 239
    },
    {
        "loss": 2.0286,
        "grad_norm": 1.9122294187545776,
        "learning_rate": 0.00019609710126578585,
        "epoch": 0.033085194375516956,
        "step": 240
    },
    {
        "loss": 1.992,
        "grad_norm": 2.4429984092712402,
        "learning_rate": 0.00019606095159952185,
        "epoch": 0.03322304935208161,
        "step": 241
    },
    {
        "loss": 2.3655,
        "grad_norm": 1.5581843852996826,
        "learning_rate": 0.00019602463865339443,
        "epoch": 0.03336090432864627,
        "step": 242
    },
    {
        "loss": 2.2074,
        "grad_norm": 1.8164865970611572,
        "learning_rate": 0.00019598816248912667,
        "epoch": 0.033498759305210915,
        "step": 243
    },
    {
        "loss": 1.6598,
        "grad_norm": 2.2719359397888184,
        "learning_rate": 0.00019595152316871897,
        "epoch": 0.03363661428177557,
        "step": 244
    },
    {
        "loss": 1.7668,
        "grad_norm": 1.6780946254730225,
        "learning_rate": 0.00019591472075444917,
        "epoch": 0.033774469258340226,
        "step": 245
    },
    {
        "loss": 2.1005,
        "grad_norm": 1.6588455438613892,
        "learning_rate": 0.00019587775530887226,
        "epoch": 0.03391232423490488,
        "step": 246
    },
    {
        "loss": 2.4304,
        "grad_norm": 0.8961185812950134,
        "learning_rate": 0.00019584062689482037,
        "epoch": 0.034050179211469536,
        "step": 247
    },
    {
        "loss": 2.395,
        "grad_norm": 1.1660805940628052,
        "learning_rate": 0.00019580333557540257,
        "epoch": 0.03418803418803419,
        "step": 248
    },
    {
        "loss": 2.3272,
        "grad_norm": 1.6541740894317627,
        "learning_rate": 0.00019576588141400497,
        "epoch": 0.03432588916459884,
        "step": 249
    },
    {
        "loss": 2.2323,
        "grad_norm": 2.297675371170044,
        "learning_rate": 0.00019572826447429028,
        "epoch": 0.034463744141163495,
        "step": 250
    },
    {
        "loss": 2.4658,
        "grad_norm": 1.1763628721237183,
        "learning_rate": 0.00019569048482019806,
        "epoch": 0.03460159911772815,
        "step": 251
    },
    {
        "loss": 2.7104,
        "grad_norm": 1.1794780492782593,
        "learning_rate": 0.00019565254251594435,
        "epoch": 0.034739454094292806,
        "step": 252
    },
    {
        "loss": 2.5502,
        "grad_norm": 0.9459394216537476,
        "learning_rate": 0.0001956144376260217,
        "epoch": 0.03487730907085746,
        "step": 253
    },
    {
        "loss": 1.927,
        "grad_norm": 1.7233638763427734,
        "learning_rate": 0.00019557617021519902,
        "epoch": 0.03501516404742211,
        "step": 254
    },
    {
        "loss": 2.2895,
        "grad_norm": 1.507610559463501,
        "learning_rate": 0.00019553774034852142,
        "epoch": 0.035153019023986765,
        "step": 255
    },
    {
        "loss": 2.47,
        "grad_norm": 1.3376381397247314,
        "learning_rate": 0.00019549914809131015,
        "epoch": 0.03529087400055142,
        "step": 256
    },
    {
        "loss": 1.7859,
        "grad_norm": 1.9325538873672485,
        "learning_rate": 0.00019546039350916257,
        "epoch": 0.035428728977116075,
        "step": 257
    },
    {
        "loss": 2.0128,
        "grad_norm": 1.518345832824707,
        "learning_rate": 0.0001954214766679518,
        "epoch": 0.03556658395368073,
        "step": 258
    },
    {
        "loss": 1.8039,
        "grad_norm": 2.8609297275543213,
        "learning_rate": 0.00019538239763382693,
        "epoch": 0.03570443893024538,
        "step": 259
    },
    {
        "loss": 2.8364,
        "grad_norm": 1.0904929637908936,
        "learning_rate": 0.00019534315647321256,
        "epoch": 0.035842293906810034,
        "step": 260
    },
    {
        "loss": 2.3848,
        "grad_norm": 1.1347346305847168,
        "learning_rate": 0.00019530375325280908,
        "epoch": 0.03598014888337469,
        "step": 261
    },
    {
        "loss": 2.3976,
        "grad_norm": 1.2819175720214844,
        "learning_rate": 0.00019526418803959212,
        "epoch": 0.036118003859939345,
        "step": 262
    },
    {
        "loss": 2.0686,
        "grad_norm": 1.5464105606079102,
        "learning_rate": 0.00019522446090081282,
        "epoch": 0.036255858836504,
        "step": 263
    },
    {
        "loss": 2.4221,
        "grad_norm": 1.4252885580062866,
        "learning_rate": 0.00019518457190399743,
        "epoch": 0.036393713813068655,
        "step": 264
    },
    {
        "loss": 1.78,
        "grad_norm": 1.7439295053482056,
        "learning_rate": 0.00019514452111694746,
        "epoch": 0.036531568789633304,
        "step": 265
    },
    {
        "loss": 2.505,
        "grad_norm": 1.1854020357131958,
        "learning_rate": 0.00019510430860773928,
        "epoch": 0.03666942376619796,
        "step": 266
    },
    {
        "loss": 2.6604,
        "grad_norm": 1.0736392736434937,
        "learning_rate": 0.00019506393444472422,
        "epoch": 0.036807278742762614,
        "step": 267
    },
    {
        "loss": 1.8808,
        "grad_norm": 1.5568753480911255,
        "learning_rate": 0.00019502339869652838,
        "epoch": 0.03694513371932727,
        "step": 268
    },
    {
        "loss": 1.707,
        "grad_norm": 1.8950746059417725,
        "learning_rate": 0.00019498270143205253,
        "epoch": 0.037082988695891925,
        "step": 269
    },
    {
        "loss": 2.1032,
        "grad_norm": 1.5339200496673584,
        "learning_rate": 0.00019494184272047193,
        "epoch": 0.03722084367245657,
        "step": 270
    },
    {
        "loss": 2.4419,
        "grad_norm": 0.9874740839004517,
        "learning_rate": 0.00019490082263123632,
        "epoch": 0.03735869864902123,
        "step": 271
    },
    {
        "loss": 2.606,
        "grad_norm": 1.620147466659546,
        "learning_rate": 0.00019485964123406967,
        "epoch": 0.037496553625585884,
        "step": 272
    },
    {
        "loss": 2.3212,
        "grad_norm": 1.3810889720916748,
        "learning_rate": 0.0001948182985989702,
        "epoch": 0.03763440860215054,
        "step": 273
    },
    {
        "loss": 2.2526,
        "grad_norm": 1.1687371730804443,
        "learning_rate": 0.00019477679479621015,
        "epoch": 0.037772263578715194,
        "step": 274
    },
    {
        "loss": 2.1627,
        "grad_norm": 1.4485231637954712,
        "learning_rate": 0.0001947351298963357,
        "epoch": 0.03791011855527984,
        "step": 275
    },
    {
        "loss": 2.7423,
        "grad_norm": 1.1143559217453003,
        "learning_rate": 0.00019469330397016692,
        "epoch": 0.0380479735318445,
        "step": 276
    },
    {
        "loss": 2.2513,
        "grad_norm": 1.7300527095794678,
        "learning_rate": 0.00019465131708879752,
        "epoch": 0.03818582850840915,
        "step": 277
    },
    {
        "loss": 2.5369,
        "grad_norm": 0.9242891073226929,
        "learning_rate": 0.0001946091693235948,
        "epoch": 0.03832368348497381,
        "step": 278
    },
    {
        "loss": 2.4145,
        "grad_norm": 1.1205792427062988,
        "learning_rate": 0.0001945668607461996,
        "epoch": 0.038461538461538464,
        "step": 279
    },
    {
        "loss": 1.9488,
        "grad_norm": 1.555871605873108,
        "learning_rate": 0.00019452439142852594,
        "epoch": 0.03859939343810312,
        "step": 280
    },
    {
        "loss": 2.6914,
        "grad_norm": 1.3157166242599487,
        "learning_rate": 0.00019448176144276126,
        "epoch": 0.03873724841466777,
        "step": 281
    },
    {
        "loss": 1.8296,
        "grad_norm": 1.3286726474761963,
        "learning_rate": 0.00019443897086136592,
        "epoch": 0.03887510339123242,
        "step": 282
    },
    {
        "loss": 2.6853,
        "grad_norm": 0.8512793779373169,
        "learning_rate": 0.00019439601975707334,
        "epoch": 0.03901295836779708,
        "step": 283
    },
    {
        "loss": 1.9744,
        "grad_norm": 1.8496862649917603,
        "learning_rate": 0.00019435290820288983,
        "epoch": 0.03915081334436173,
        "step": 284
    },
    {
        "loss": 2.4637,
        "grad_norm": 1.7336868047714233,
        "learning_rate": 0.0001943096362720943,
        "epoch": 0.03928866832092639,
        "step": 285
    },
    {
        "loss": 1.9585,
        "grad_norm": 2.020185947418213,
        "learning_rate": 0.00019426620403823836,
        "epoch": 0.03942652329749104,
        "step": 286
    },
    {
        "loss": 2.5439,
        "grad_norm": 1.130624532699585,
        "learning_rate": 0.00019422261157514613,
        "epoch": 0.03956437827405569,
        "step": 287
    },
    {
        "loss": 2.5694,
        "grad_norm": 1.0499496459960938,
        "learning_rate": 0.0001941788589569139,
        "epoch": 0.03970223325062035,
        "step": 288
    },
    {
        "loss": 2.5775,
        "grad_norm": 1.1088101863861084,
        "learning_rate": 0.00019413494625791037,
        "epoch": 0.039840088227185,
        "step": 289
    },
    {
        "loss": 2.0268,
        "grad_norm": 1.5354291200637817,
        "learning_rate": 0.00019409087355277626,
        "epoch": 0.03997794320374966,
        "step": 290
    },
    {
        "loss": 2.3541,
        "grad_norm": 1.249058485031128,
        "learning_rate": 0.00019404664091642428,
        "epoch": 0.040115798180314306,
        "step": 291
    },
    {
        "loss": 2.2207,
        "grad_norm": 1.4170295000076294,
        "learning_rate": 0.00019400224842403897,
        "epoch": 0.04025365315687896,
        "step": 292
    },
    {
        "loss": 1.9171,
        "grad_norm": 1.793627381324768,
        "learning_rate": 0.0001939576961510766,
        "epoch": 0.04039150813344362,
        "step": 293
    },
    {
        "loss": 2.5607,
        "grad_norm": 0.9887132048606873,
        "learning_rate": 0.00019391298417326496,
        "epoch": 0.04052936311000827,
        "step": 294
    },
    {
        "loss": 2.5247,
        "grad_norm": 1.0534218549728394,
        "learning_rate": 0.0001938681125666034,
        "epoch": 0.04066721808657293,
        "step": 295
    },
    {
        "loss": 2.2167,
        "grad_norm": 2.1049916744232178,
        "learning_rate": 0.00019382308140736257,
        "epoch": 0.04080507306313758,
        "step": 296
    },
    {
        "loss": 2.1065,
        "grad_norm": 1.6700040102005005,
        "learning_rate": 0.00019377789077208428,
        "epoch": 0.04094292803970223,
        "step": 297
    },
    {
        "loss": 2.1034,
        "grad_norm": 1.5370652675628662,
        "learning_rate": 0.00019373254073758146,
        "epoch": 0.041080783016266886,
        "step": 298
    },
    {
        "loss": 2.2107,
        "grad_norm": 1.5346096754074097,
        "learning_rate": 0.00019368703138093792,
        "epoch": 0.04121863799283154,
        "step": 299
    },
    {
        "loss": 1.3772,
        "grad_norm": 2.1808419227600098,
        "learning_rate": 0.0001936413627795083,
        "epoch": 0.0413564929693962,
        "step": 300
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.3834691047668457,
        "learning_rate": 0.000193595535010918,
        "epoch": 0.04149434794596085,
        "step": 301
    },
    {
        "loss": 2.6654,
        "grad_norm": 1.2136116027832031,
        "learning_rate": 0.00019354954815306284,
        "epoch": 0.0416322029225255,
        "step": 302
    },
    {
        "loss": 2.5735,
        "grad_norm": 0.996204674243927,
        "learning_rate": 0.00019350340228410914,
        "epoch": 0.041770057899090156,
        "step": 303
    },
    {
        "loss": 2.3781,
        "grad_norm": 0.7847776412963867,
        "learning_rate": 0.00019345709748249342,
        "epoch": 0.04190791287565481,
        "step": 304
    },
    {
        "loss": 1.4275,
        "grad_norm": 1.8580173254013062,
        "learning_rate": 0.0001934106338269224,
        "epoch": 0.042045767852219466,
        "step": 305
    },
    {
        "loss": 2.425,
        "grad_norm": 1.1270031929016113,
        "learning_rate": 0.00019336401139637285,
        "epoch": 0.04218362282878412,
        "step": 306
    },
    {
        "loss": 1.9173,
        "grad_norm": 1.6500517129898071,
        "learning_rate": 0.00019331723027009133,
        "epoch": 0.04232147780534877,
        "step": 307
    },
    {
        "loss": 2.3571,
        "grad_norm": 1.7097631692886353,
        "learning_rate": 0.00019327029052759422,
        "epoch": 0.042459332781913425,
        "step": 308
    },
    {
        "loss": 1.6555,
        "grad_norm": 1.8968923091888428,
        "learning_rate": 0.00019322319224866744,
        "epoch": 0.04259718775847808,
        "step": 309
    },
    {
        "loss": 2.1209,
        "grad_norm": 1.8511725664138794,
        "learning_rate": 0.00019317593551336645,
        "epoch": 0.042735042735042736,
        "step": 310
    },
    {
        "loss": 2.551,
        "grad_norm": 1.1068421602249146,
        "learning_rate": 0.00019312852040201598,
        "epoch": 0.04287289771160739,
        "step": 311
    },
    {
        "loss": 2.6055,
        "grad_norm": 1.582425832748413,
        "learning_rate": 0.00019308094699521,
        "epoch": 0.043010752688172046,
        "step": 312
    },
    {
        "loss": 2.1088,
        "grad_norm": 1.8245774507522583,
        "learning_rate": 0.00019303321537381157,
        "epoch": 0.043148607664736695,
        "step": 313
    },
    {
        "loss": 2.1537,
        "grad_norm": 1.458335518836975,
        "learning_rate": 0.00019298532561895263,
        "epoch": 0.04328646264130135,
        "step": 314
    },
    {
        "loss": 1.5539,
        "grad_norm": 2.0792219638824463,
        "learning_rate": 0.0001929372778120339,
        "epoch": 0.043424317617866005,
        "step": 315
    },
    {
        "loss": 1.989,
        "grad_norm": 1.524552345275879,
        "learning_rate": 0.00019288907203472477,
        "epoch": 0.04356217259443066,
        "step": 316
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.1843572854995728,
        "learning_rate": 0.00019284070836896318,
        "epoch": 0.043700027570995316,
        "step": 317
    },
    {
        "loss": 2.4602,
        "grad_norm": 1.2524617910385132,
        "learning_rate": 0.00019279218689695538,
        "epoch": 0.043837882547559964,
        "step": 318
    },
    {
        "loss": 2.2336,
        "grad_norm": 1.4053499698638916,
        "learning_rate": 0.00019274350770117585,
        "epoch": 0.04397573752412462,
        "step": 319
    },
    {
        "loss": 2.4984,
        "grad_norm": 1.1393340826034546,
        "learning_rate": 0.00019269467086436721,
        "epoch": 0.044113592500689275,
        "step": 320
    },
    {
        "loss": 2.0816,
        "grad_norm": 2.1695618629455566,
        "learning_rate": 0.00019264567646954,
        "epoch": 0.04425144747725393,
        "step": 321
    },
    {
        "loss": 2.8699,
        "grad_norm": 1.0488353967666626,
        "learning_rate": 0.0001925965245999726,
        "epoch": 0.044389302453818585,
        "step": 322
    },
    {
        "loss": 1.7681,
        "grad_norm": 1.9173437356948853,
        "learning_rate": 0.00019254721533921098,
        "epoch": 0.044527157430383234,
        "step": 323
    },
    {
        "loss": 2.4211,
        "grad_norm": 1.2383790016174316,
        "learning_rate": 0.00019249774877106875,
        "epoch": 0.04466501240694789,
        "step": 324
    },
    {
        "loss": 2.9558,
        "grad_norm": 1.3910975456237793,
        "learning_rate": 0.00019244812497962678,
        "epoch": 0.044802867383512544,
        "step": 325
    },
    {
        "loss": 2.3574,
        "grad_norm": 1.5548807382583618,
        "learning_rate": 0.0001923983440492333,
        "epoch": 0.0449407223600772,
        "step": 326
    },
    {
        "loss": 2.1519,
        "grad_norm": 1.911472201347351,
        "learning_rate": 0.0001923484060645036,
        "epoch": 0.045078577336641855,
        "step": 327
    },
    {
        "loss": 1.8625,
        "grad_norm": 1.3195244073867798,
        "learning_rate": 0.0001922983111103198,
        "epoch": 0.04521643231320651,
        "step": 328
    },
    {
        "loss": 2.2088,
        "grad_norm": 1.529284119606018,
        "learning_rate": 0.00019224805927183102,
        "epoch": 0.04535428728977116,
        "step": 329
    },
    {
        "loss": 2.2535,
        "grad_norm": 1.673205852508545,
        "learning_rate": 0.000192197650634453,
        "epoch": 0.045492142266335814,
        "step": 330
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.3096024990081787,
        "learning_rate": 0.00019214708528386782,
        "epoch": 0.04562999724290047,
        "step": 331
    },
    {
        "loss": 2.0799,
        "grad_norm": 1.5619288682937622,
        "learning_rate": 0.00019209636330602418,
        "epoch": 0.045767852219465124,
        "step": 332
    },
    {
        "loss": 2.2537,
        "grad_norm": 1.7633998394012451,
        "learning_rate": 0.00019204548478713688,
        "epoch": 0.04590570719602978,
        "step": 333
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.402000069618225,
        "learning_rate": 0.0001919944498136868,
        "epoch": 0.04604356217259443,
        "step": 334
    },
    {
        "loss": 2.4263,
        "grad_norm": 1.20048987865448,
        "learning_rate": 0.0001919432584724208,
        "epoch": 0.04618141714915908,
        "step": 335
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.5438237190246582,
        "learning_rate": 0.00019189191085035144,
        "epoch": 0.04631927212572374,
        "step": 336
    },
    {
        "loss": 2.6424,
        "grad_norm": 1.1466134786605835,
        "learning_rate": 0.00019184040703475706,
        "epoch": 0.046457127102288394,
        "step": 337
    },
    {
        "loss": 2.2837,
        "grad_norm": 1.2415192127227783,
        "learning_rate": 0.00019178874711318135,
        "epoch": 0.04659498207885305,
        "step": 338
    },
    {
        "loss": 2.3036,
        "grad_norm": 1.1691890954971313,
        "learning_rate": 0.00019173693117343347,
        "epoch": 0.0467328370554177,
        "step": 339
    },
    {
        "loss": 2.5786,
        "grad_norm": 1.3678364753723145,
        "learning_rate": 0.00019168495930358762,
        "epoch": 0.04687069203198235,
        "step": 340
    },
    {
        "loss": 1.9731,
        "grad_norm": 1.0625073909759521,
        "learning_rate": 0.00019163283159198322,
        "epoch": 0.04700854700854701,
        "step": 341
    },
    {
        "loss": 2.0302,
        "grad_norm": 0.8176015615463257,
        "learning_rate": 0.0001915805481272244,
        "epoch": 0.04714640198511166,
        "step": 342
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.5799825191497803,
        "learning_rate": 0.00019152810899818016,
        "epoch": 0.04728425696167632,
        "step": 343
    },
    {
        "loss": 1.6566,
        "grad_norm": 1.7885830402374268,
        "learning_rate": 0.00019147551429398406,
        "epoch": 0.047422111938240974,
        "step": 344
    },
    {
        "loss": 2.4777,
        "grad_norm": 1.39110267162323,
        "learning_rate": 0.0001914227641040341,
        "epoch": 0.04755996691480562,
        "step": 345
    },
    {
        "loss": 2.3877,
        "grad_norm": 1.065755009651184,
        "learning_rate": 0.00019136985851799253,
        "epoch": 0.04769782189137028,
        "step": 346
    },
    {
        "loss": 2.2522,
        "grad_norm": 1.6046055555343628,
        "learning_rate": 0.00019131679762578578,
        "epoch": 0.04783567686793493,
        "step": 347
    },
    {
        "loss": 1.7519,
        "grad_norm": 1.6373658180236816,
        "learning_rate": 0.00019126358151760423,
        "epoch": 0.04797353184449959,
        "step": 348
    },
    {
        "loss": 2.3793,
        "grad_norm": 1.3161967992782593,
        "learning_rate": 0.00019121021028390214,
        "epoch": 0.04811138682106424,
        "step": 349
    },
    {
        "loss": 2.1905,
        "grad_norm": 1.8909335136413574,
        "learning_rate": 0.00019115668401539738,
        "epoch": 0.04824924179762889,
        "step": 350
    },
    {
        "loss": 2.1801,
        "grad_norm": 1.2059742212295532,
        "learning_rate": 0.00019110300280307136,
        "epoch": 0.04838709677419355,
        "step": 351
    },
    {
        "loss": 2.2472,
        "grad_norm": 1.4746592044830322,
        "learning_rate": 0.0001910491667381689,
        "epoch": 0.0485249517507582,
        "step": 352
    },
    {
        "loss": 2.4758,
        "grad_norm": 1.2468745708465576,
        "learning_rate": 0.00019099517591219796,
        "epoch": 0.04866280672732286,
        "step": 353
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.4573076963424683,
        "learning_rate": 0.00019094103041692964,
        "epoch": 0.04880066170388751,
        "step": 354
    },
    {
        "loss": 1.809,
        "grad_norm": 1.7303857803344727,
        "learning_rate": 0.00019088673034439784,
        "epoch": 0.04893851668045216,
        "step": 355
    },
    {
        "loss": 2.277,
        "grad_norm": 1.1433993577957153,
        "learning_rate": 0.00019083227578689928,
        "epoch": 0.049076371657016817,
        "step": 356
    },
    {
        "loss": 1.8329,
        "grad_norm": 2.2934792041778564,
        "learning_rate": 0.00019077766683699325,
        "epoch": 0.04921422663358147,
        "step": 357
    },
    {
        "loss": 1.697,
        "grad_norm": 1.5970534086227417,
        "learning_rate": 0.00019072290358750147,
        "epoch": 0.04935208161014613,
        "step": 358
    },
    {
        "loss": 2.555,
        "grad_norm": 1.260094165802002,
        "learning_rate": 0.00019066798613150788,
        "epoch": 0.04948993658671078,
        "step": 359
    },
    {
        "loss": 2.358,
        "grad_norm": 1.8921416997909546,
        "learning_rate": 0.00019061291456235862,
        "epoch": 0.04962779156327544,
        "step": 360
    },
    {
        "loss": 2.033,
        "grad_norm": 1.9173377752304077,
        "learning_rate": 0.00019055768897366173,
        "epoch": 0.049765646539840086,
        "step": 361
    },
    {
        "loss": 2.4286,
        "grad_norm": 1.7687866687774658,
        "learning_rate": 0.00019050230945928703,
        "epoch": 0.04990350151640474,
        "step": 362
    },
    {
        "loss": 2.6565,
        "grad_norm": 0.8161817789077759,
        "learning_rate": 0.000190446776113366,
        "epoch": 0.0500413564929694,
        "step": 363
    },
    {
        "loss": 1.5715,
        "grad_norm": 1.7154511213302612,
        "learning_rate": 0.00019039108903029164,
        "epoch": 0.05017921146953405,
        "step": 364
    },
    {
        "loss": 1.8578,
        "grad_norm": 1.9889769554138184,
        "learning_rate": 0.00019033524830471818,
        "epoch": 0.05031706644609871,
        "step": 365
    },
    {
        "loss": 2.1755,
        "grad_norm": 1.6693586111068726,
        "learning_rate": 0.00019027925403156104,
        "epoch": 0.050454921422663356,
        "step": 366
    },
    {
        "loss": 2.5342,
        "grad_norm": 1.405509352684021,
        "learning_rate": 0.00019022310630599666,
        "epoch": 0.05059277639922801,
        "step": 367
    },
    {
        "loss": 2.6259,
        "grad_norm": 1.1163485050201416,
        "learning_rate": 0.0001901668052234623,
        "epoch": 0.050730631375792666,
        "step": 368
    },
    {
        "loss": 1.6738,
        "grad_norm": 1.8744533061981201,
        "learning_rate": 0.00019011035087965588,
        "epoch": 0.05086848635235732,
        "step": 369
    },
    {
        "loss": 2.4196,
        "grad_norm": 1.6167320013046265,
        "learning_rate": 0.00019005374337053573,
        "epoch": 0.05100634132892198,
        "step": 370
    },
    {
        "loss": 2.3269,
        "grad_norm": 0.9980979561805725,
        "learning_rate": 0.00018999698279232075,
        "epoch": 0.051144196305486625,
        "step": 371
    },
    {
        "loss": 2.1496,
        "grad_norm": 1.2621914148330688,
        "learning_rate": 0.00018994006924148983,
        "epoch": 0.05128205128205128,
        "step": 372
    },
    {
        "loss": 2.1882,
        "grad_norm": 1.5469167232513428,
        "learning_rate": 0.00018988300281478192,
        "epoch": 0.051419906258615936,
        "step": 373
    },
    {
        "loss": 1.7786,
        "grad_norm": 2.0402631759643555,
        "learning_rate": 0.00018982578360919586,
        "epoch": 0.05155776123518059,
        "step": 374
    },
    {
        "loss": 2.4842,
        "grad_norm": 1.546478033065796,
        "learning_rate": 0.00018976841172199012,
        "epoch": 0.051695616211745246,
        "step": 375
    },
    {
        "loss": 2.3747,
        "grad_norm": 1.7416828870773315,
        "learning_rate": 0.00018971088725068275,
        "epoch": 0.0518334711883099,
        "step": 376
    },
    {
        "loss": 2.192,
        "grad_norm": 1.419215202331543,
        "learning_rate": 0.0001896532102930511,
        "epoch": 0.05197132616487455,
        "step": 377
    },
    {
        "loss": 1.9584,
        "grad_norm": 1.9841880798339844,
        "learning_rate": 0.00018959538094713175,
        "epoch": 0.052109181141439205,
        "step": 378
    },
    {
        "loss": 1.6702,
        "grad_norm": 1.6900748014450073,
        "learning_rate": 0.0001895373993112203,
        "epoch": 0.05224703611800386,
        "step": 379
    },
    {
        "loss": 2.683,
        "grad_norm": 1.5050503015518188,
        "learning_rate": 0.00018947926548387115,
        "epoch": 0.052384891094568516,
        "step": 380
    },
    {
        "loss": 1.9478,
        "grad_norm": 1.5957887172698975,
        "learning_rate": 0.0001894209795638975,
        "epoch": 0.05252274607113317,
        "step": 381
    },
    {
        "loss": 2.3343,
        "grad_norm": 1.7429527044296265,
        "learning_rate": 0.00018936254165037095,
        "epoch": 0.05266060104769782,
        "step": 382
    },
    {
        "loss": 2.5119,
        "grad_norm": 1.6494449377059937,
        "learning_rate": 0.0001893039518426215,
        "epoch": 0.052798456024262475,
        "step": 383
    },
    {
        "loss": 1.8434,
        "grad_norm": 1.9998621940612793,
        "learning_rate": 0.00018924521024023738,
        "epoch": 0.05293631100082713,
        "step": 384
    },
    {
        "loss": 2.3853,
        "grad_norm": 0.914114773273468,
        "learning_rate": 0.00018918631694306472,
        "epoch": 0.053074165977391785,
        "step": 385
    },
    {
        "loss": 2.1841,
        "grad_norm": 1.4663996696472168,
        "learning_rate": 0.0001891272720512076,
        "epoch": 0.05321202095395644,
        "step": 386
    },
    {
        "loss": 2.1318,
        "grad_norm": 1.335862398147583,
        "learning_rate": 0.00018906807566502776,
        "epoch": 0.05334987593052109,
        "step": 387
    },
    {
        "loss": 1.8465,
        "grad_norm": 1.6509807109832764,
        "learning_rate": 0.00018900872788514436,
        "epoch": 0.053487730907085744,
        "step": 388
    },
    {
        "loss": 2.513,
        "grad_norm": 1.1341849565505981,
        "learning_rate": 0.00018894922881243403,
        "epoch": 0.0536255858836504,
        "step": 389
    },
    {
        "loss": 1.7004,
        "grad_norm": 1.536004662513733,
        "learning_rate": 0.00018888957854803037,
        "epoch": 0.053763440860215055,
        "step": 390
    },
    {
        "loss": 2.58,
        "grad_norm": 1.2750489711761475,
        "learning_rate": 0.0001888297771933242,
        "epoch": 0.05390129583677971,
        "step": 391
    },
    {
        "loss": 2.2573,
        "grad_norm": 1.377977967262268,
        "learning_rate": 0.00018876982484996294,
        "epoch": 0.054039150813344365,
        "step": 392
    },
    {
        "loss": 2.767,
        "grad_norm": 1.110376238822937,
        "learning_rate": 0.0001887097216198508,
        "epoch": 0.054177005789909014,
        "step": 393
    },
    {
        "loss": 2.2985,
        "grad_norm": 1.0466862916946411,
        "learning_rate": 0.0001886494676051484,
        "epoch": 0.05431486076647367,
        "step": 394
    },
    {
        "loss": 2.572,
        "grad_norm": 0.9835441708564758,
        "learning_rate": 0.00018858906290827266,
        "epoch": 0.054452715743038324,
        "step": 395
    },
    {
        "loss": 2.2719,
        "grad_norm": 1.8276774883270264,
        "learning_rate": 0.0001885285076318966,
        "epoch": 0.05459057071960298,
        "step": 396
    },
    {
        "loss": 2.2867,
        "grad_norm": 1.598577618598938,
        "learning_rate": 0.00018846780187894928,
        "epoch": 0.054728425696167635,
        "step": 397
    },
    {
        "loss": 2.0394,
        "grad_norm": 1.8595479726791382,
        "learning_rate": 0.00018840694575261542,
        "epoch": 0.05486628067273228,
        "step": 398
    },
    {
        "loss": 2.6825,
        "grad_norm": 1.0955525636672974,
        "learning_rate": 0.00018834593935633537,
        "epoch": 0.05500413564929694,
        "step": 399
    },
    {
        "loss": 1.8811,
        "grad_norm": 2.1722123622894287,
        "learning_rate": 0.00018828478279380497,
        "epoch": 0.055141990625861594,
        "step": 400
    },
    {
        "loss": 2.4686,
        "grad_norm": 1.1620900630950928,
        "learning_rate": 0.0001882234761689752,
        "epoch": 0.05527984560242625,
        "step": 401
    },
    {
        "loss": 2.1583,
        "grad_norm": 1.1566022634506226,
        "learning_rate": 0.00018816201958605223,
        "epoch": 0.055417700578990904,
        "step": 402
    },
    {
        "loss": 1.649,
        "grad_norm": 1.9194189310073853,
        "learning_rate": 0.00018810041314949697,
        "epoch": 0.05555555555555555,
        "step": 403
    },
    {
        "loss": 2.331,
        "grad_norm": 1.732253074645996,
        "learning_rate": 0.00018803865696402522,
        "epoch": 0.05569341053212021,
        "step": 404
    },
    {
        "loss": 2.1149,
        "grad_norm": 1.8351410627365112,
        "learning_rate": 0.00018797675113460714,
        "epoch": 0.05583126550868486,
        "step": 405
    },
    {
        "loss": 2.4688,
        "grad_norm": 1.6912049055099487,
        "learning_rate": 0.00018791469576646738,
        "epoch": 0.05596912048524952,
        "step": 406
    },
    {
        "loss": 2.2082,
        "grad_norm": 1.4893676042556763,
        "learning_rate": 0.00018785249096508468,
        "epoch": 0.056106975461814174,
        "step": 407
    },
    {
        "loss": 2.0939,
        "grad_norm": 1.5944433212280273,
        "learning_rate": 0.0001877901368361919,
        "epoch": 0.05624483043837883,
        "step": 408
    },
    {
        "loss": 2.6697,
        "grad_norm": 1.376508355140686,
        "learning_rate": 0.00018772763348577556,
        "epoch": 0.05638268541494348,
        "step": 409
    },
    {
        "loss": 2.3499,
        "grad_norm": 1.4000928401947021,
        "learning_rate": 0.0001876649810200759,
        "epoch": 0.05652054039150813,
        "step": 410
    },
    {
        "loss": 2.4868,
        "grad_norm": 1.2353429794311523,
        "learning_rate": 0.0001876021795455867,
        "epoch": 0.05665839536807279,
        "step": 411
    },
    {
        "loss": 2.0302,
        "grad_norm": 1.951518177986145,
        "learning_rate": 0.00018753922916905485,
        "epoch": 0.05679625034463744,
        "step": 412
    },
    {
        "loss": 2.5403,
        "grad_norm": 1.2457138299942017,
        "learning_rate": 0.00018747612999748045,
        "epoch": 0.0569341053212021,
        "step": 413
    },
    {
        "loss": 2.4658,
        "grad_norm": 1.0884898900985718,
        "learning_rate": 0.0001874128821381165,
        "epoch": 0.05707196029776675,
        "step": 414
    },
    {
        "loss": 2.2857,
        "grad_norm": 1.5650242567062378,
        "learning_rate": 0.00018734948569846873,
        "epoch": 0.0572098152743314,
        "step": 415
    },
    {
        "loss": 1.7591,
        "grad_norm": 2.1178884506225586,
        "learning_rate": 0.0001872859407862954,
        "epoch": 0.05734767025089606,
        "step": 416
    },
    {
        "loss": 2.2647,
        "grad_norm": 1.4045493602752686,
        "learning_rate": 0.00018722224750960712,
        "epoch": 0.05748552522746071,
        "step": 417
    },
    {
        "loss": 1.8963,
        "grad_norm": 1.126072883605957,
        "learning_rate": 0.0001871584059766667,
        "epoch": 0.05762338020402537,
        "step": 418
    },
    {
        "loss": 2.0766,
        "grad_norm": 1.322779655456543,
        "learning_rate": 0.00018709441629598906,
        "epoch": 0.057761235180590016,
        "step": 419
    },
    {
        "loss": 2.3855,
        "grad_norm": 1.4629948139190674,
        "learning_rate": 0.00018703027857634074,
        "epoch": 0.05789909015715467,
        "step": 420
    },
    {
        "loss": 1.5291,
        "grad_norm": 1.9129955768585205,
        "learning_rate": 0.00018696599292674005,
        "epoch": 0.05803694513371933,
        "step": 421
    },
    {
        "loss": 1.6856,
        "grad_norm": 0.9768770337104797,
        "learning_rate": 0.00018690155945645664,
        "epoch": 0.05817480011028398,
        "step": 422
    },
    {
        "loss": 1.9881,
        "grad_norm": 1.690186619758606,
        "learning_rate": 0.00018683697827501152,
        "epoch": 0.05831265508684864,
        "step": 423
    },
    {
        "loss": 2.1639,
        "grad_norm": 1.8383909463882446,
        "learning_rate": 0.00018677224949217674,
        "epoch": 0.05845051006341329,
        "step": 424
    },
    {
        "loss": 2.6507,
        "grad_norm": 1.4944208860397339,
        "learning_rate": 0.0001867073732179752,
        "epoch": 0.05858836503997794,
        "step": 425
    },
    {
        "loss": 2.3147,
        "grad_norm": 1.659413456916809,
        "learning_rate": 0.00018664234956268053,
        "epoch": 0.058726220016542596,
        "step": 426
    },
    {
        "loss": 2.332,
        "grad_norm": 0.923395037651062,
        "learning_rate": 0.00018657717863681682,
        "epoch": 0.05886407499310725,
        "step": 427
    },
    {
        "loss": 2.3973,
        "grad_norm": 1.179978847503662,
        "learning_rate": 0.00018651186055115862,
        "epoch": 0.05900192996967191,
        "step": 428
    },
    {
        "loss": 2.1375,
        "grad_norm": 1.5234777927398682,
        "learning_rate": 0.00018644639541673047,
        "epoch": 0.05913978494623656,
        "step": 429
    },
    {
        "loss": 2.5038,
        "grad_norm": 1.6223938465118408,
        "learning_rate": 0.0001863807833448069,
        "epoch": 0.05927763992280121,
        "step": 430
    },
    {
        "loss": 2.506,
        "grad_norm": 1.644031047821045,
        "learning_rate": 0.00018631502444691223,
        "epoch": 0.059415494899365866,
        "step": 431
    },
    {
        "loss": 2.0481,
        "grad_norm": 1.4969260692596436,
        "learning_rate": 0.00018624911883482033,
        "epoch": 0.05955334987593052,
        "step": 432
    },
    {
        "loss": 2.2362,
        "grad_norm": 1.775132656097412,
        "learning_rate": 0.0001861830666205544,
        "epoch": 0.059691204852495176,
        "step": 433
    },
    {
        "loss": 2.3658,
        "grad_norm": 1.1198806762695312,
        "learning_rate": 0.0001861168679163869,
        "epoch": 0.05982905982905983,
        "step": 434
    },
    {
        "loss": 1.822,
        "grad_norm": 2.2165074348449707,
        "learning_rate": 0.0001860505228348393,
        "epoch": 0.05996691480562448,
        "step": 435
    },
    {
        "loss": 1.9324,
        "grad_norm": 1.6085693836212158,
        "learning_rate": 0.00018598403148868177,
        "epoch": 0.060104769782189135,
        "step": 436
    },
    {
        "loss": 2.5877,
        "grad_norm": 1.2149547338485718,
        "learning_rate": 0.00018591739399093318,
        "epoch": 0.06024262475875379,
        "step": 437
    },
    {
        "loss": 1.9429,
        "grad_norm": 2.025029420852661,
        "learning_rate": 0.00018585061045486078,
        "epoch": 0.060380479735318446,
        "step": 438
    },
    {
        "loss": 2.1568,
        "grad_norm": 1.7011841535568237,
        "learning_rate": 0.00018578368099398009,
        "epoch": 0.0605183347118831,
        "step": 439
    },
    {
        "loss": 1.936,
        "grad_norm": 1.9106438159942627,
        "learning_rate": 0.00018571660572205465,
        "epoch": 0.060656189688447756,
        "step": 440
    },
    {
        "loss": 1.4909,
        "grad_norm": 1.4979768991470337,
        "learning_rate": 0.0001856493847530958,
        "epoch": 0.060794044665012405,
        "step": 441
    },
    {
        "loss": 2.3831,
        "grad_norm": 1.512268304824829,
        "learning_rate": 0.00018558201820136262,
        "epoch": 0.06093189964157706,
        "step": 442
    },
    {
        "loss": 2.5807,
        "grad_norm": 1.7364590167999268,
        "learning_rate": 0.00018551450618136151,
        "epoch": 0.061069754618141715,
        "step": 443
    },
    {
        "loss": 2.2552,
        "grad_norm": 1.2300407886505127,
        "learning_rate": 0.00018544684880784627,
        "epoch": 0.06120760959470637,
        "step": 444
    },
    {
        "loss": 2.2444,
        "grad_norm": 1.5470130443572998,
        "learning_rate": 0.0001853790461958177,
        "epoch": 0.061345464571271026,
        "step": 445
    },
    {
        "loss": 2.388,
        "grad_norm": 1.1731860637664795,
        "learning_rate": 0.00018531109846052347,
        "epoch": 0.061483319547835674,
        "step": 446
    },
    {
        "loss": 2.5854,
        "grad_norm": 1.023159384727478,
        "learning_rate": 0.0001852430057174579,
        "epoch": 0.06162117452440033,
        "step": 447
    },
    {
        "loss": 2.513,
        "grad_norm": 1.689929723739624,
        "learning_rate": 0.00018517476808236182,
        "epoch": 0.061759029500964985,
        "step": 448
    },
    {
        "loss": 2.315,
        "grad_norm": 1.8395253419876099,
        "learning_rate": 0.00018510638567122237,
        "epoch": 0.06189688447752964,
        "step": 449
    },
    {
        "loss": 2.3127,
        "grad_norm": 1.3086014986038208,
        "learning_rate": 0.0001850378586002727,
        "epoch": 0.062034739454094295,
        "step": 450
    },
    {
        "loss": 2.4858,
        "grad_norm": 1.8096392154693604,
        "learning_rate": 0.00018496918698599195,
        "epoch": 0.062172594430658944,
        "step": 451
    },
    {
        "loss": 2.1386,
        "grad_norm": 1.4405425786972046,
        "learning_rate": 0.0001849003709451048,
        "epoch": 0.0623104494072236,
        "step": 452
    },
    {
        "loss": 2.6019,
        "grad_norm": 1.7277204990386963,
        "learning_rate": 0.00018483141059458155,
        "epoch": 0.062448304383788254,
        "step": 453
    },
    {
        "loss": 2.0165,
        "grad_norm": 1.5100069046020508,
        "learning_rate": 0.00018476230605163775,
        "epoch": 0.0625861593603529,
        "step": 454
    },
    {
        "loss": 1.5442,
        "grad_norm": 1.932115077972412,
        "learning_rate": 0.000184693057433734,
        "epoch": 0.06272401433691756,
        "step": 455
    },
    {
        "loss": 2.6422,
        "grad_norm": 1.2863785028457642,
        "learning_rate": 0.00018462366485857582,
        "epoch": 0.06286186931348221,
        "step": 456
    },
    {
        "loss": 1.9771,
        "grad_norm": 1.3072614669799805,
        "learning_rate": 0.0001845541284441135,
        "epoch": 0.06299972429004687,
        "step": 457
    },
    {
        "loss": 2.1964,
        "grad_norm": 1.2175711393356323,
        "learning_rate": 0.0001844844483085417,
        "epoch": 0.06313757926661152,
        "step": 458
    },
    {
        "loss": 2.4972,
        "grad_norm": 1.0804493427276611,
        "learning_rate": 0.00018441462457029936,
        "epoch": 0.06327543424317618,
        "step": 459
    },
    {
        "loss": 2.4495,
        "grad_norm": 1.7200497388839722,
        "learning_rate": 0.0001843446573480697,
        "epoch": 0.06341328921974083,
        "step": 460
    },
    {
        "loss": 1.6018,
        "grad_norm": 1.9115287065505981,
        "learning_rate": 0.0001842745467607796,
        "epoch": 0.06355114419630549,
        "step": 461
    },
    {
        "loss": 1.8021,
        "grad_norm": 1.872841715812683,
        "learning_rate": 0.00018420429292759973,
        "epoch": 0.06368899917287014,
        "step": 462
    },
    {
        "loss": 2.5069,
        "grad_norm": 2.045797348022461,
        "learning_rate": 0.00018413389596794427,
        "epoch": 0.0638268541494348,
        "step": 463
    },
    {
        "loss": 1.8491,
        "grad_norm": 1.6436456441879272,
        "learning_rate": 0.0001840633560014706,
        "epoch": 0.06396470912599946,
        "step": 464
    },
    {
        "loss": 1.9385,
        "grad_norm": 1.6926748752593994,
        "learning_rate": 0.00018399267314807925,
        "epoch": 0.0641025641025641,
        "step": 465
    },
    {
        "loss": 1.3854,
        "grad_norm": 2.4163742065429688,
        "learning_rate": 0.0001839218475279136,
        "epoch": 0.06424041907912875,
        "step": 466
    },
    {
        "loss": 2.1232,
        "grad_norm": 1.967408537864685,
        "learning_rate": 0.00018385087926135965,
        "epoch": 0.06437827405569341,
        "step": 467
    },
    {
        "loss": 2.1024,
        "grad_norm": 2.020270347595215,
        "learning_rate": 0.00018377976846904596,
        "epoch": 0.06451612903225806,
        "step": 468
    },
    {
        "loss": 2.2158,
        "grad_norm": 1.5518505573272705,
        "learning_rate": 0.00018370851527184327,
        "epoch": 0.06465398400882272,
        "step": 469
    },
    {
        "loss": 2.5417,
        "grad_norm": 1.641078233718872,
        "learning_rate": 0.00018363711979086435,
        "epoch": 0.06479183898538737,
        "step": 470
    },
    {
        "loss": 2.6692,
        "grad_norm": 1.309520959854126,
        "learning_rate": 0.00018356558214746395,
        "epoch": 0.06492969396195203,
        "step": 471
    },
    {
        "loss": 1.595,
        "grad_norm": 2.2362260818481445,
        "learning_rate": 0.00018349390246323827,
        "epoch": 0.06506754893851668,
        "step": 472
    },
    {
        "loss": 2.2911,
        "grad_norm": 0.949476957321167,
        "learning_rate": 0.00018342208086002514,
        "epoch": 0.06520540391508134,
        "step": 473
    },
    {
        "loss": 1.9902,
        "grad_norm": 1.6715871095657349,
        "learning_rate": 0.00018335011745990347,
        "epoch": 0.065343258891646,
        "step": 474
    },
    {
        "loss": 2.6532,
        "grad_norm": 1.1392055749893188,
        "learning_rate": 0.00018327801238519328,
        "epoch": 0.06548111386821064,
        "step": 475
    },
    {
        "loss": 2.2674,
        "grad_norm": 1.0686261653900146,
        "learning_rate": 0.00018320576575845535,
        "epoch": 0.06561896884477529,
        "step": 476
    },
    {
        "loss": 2.3381,
        "grad_norm": 1.141676425933838,
        "learning_rate": 0.00018313337770249104,
        "epoch": 0.06575682382133995,
        "step": 477
    },
    {
        "loss": 1.8417,
        "grad_norm": 1.7289131879806519,
        "learning_rate": 0.0001830608483403422,
        "epoch": 0.0658946787979046,
        "step": 478
    },
    {
        "loss": 1.9049,
        "grad_norm": 1.719094157218933,
        "learning_rate": 0.00018298817779529084,
        "epoch": 0.06603253377446926,
        "step": 479
    },
    {
        "loss": 2.078,
        "grad_norm": 1.8300927877426147,
        "learning_rate": 0.00018291536619085877,
        "epoch": 0.06617038875103391,
        "step": 480
    },
    {
        "loss": 1.3005,
        "grad_norm": 1.992667317390442,
        "learning_rate": 0.0001828424136508079,
        "epoch": 0.06630824372759857,
        "step": 481
    },
    {
        "loss": 1.7965,
        "grad_norm": 1.9124895334243774,
        "learning_rate": 0.0001827693202991394,
        "epoch": 0.06644609870416322,
        "step": 482
    },
    {
        "loss": 2.6784,
        "grad_norm": 1.3610588312149048,
        "learning_rate": 0.0001826960862600939,
        "epoch": 0.06658395368072788,
        "step": 483
    },
    {
        "loss": 1.8733,
        "grad_norm": 1.845078706741333,
        "learning_rate": 0.00018262271165815118,
        "epoch": 0.06672180865729253,
        "step": 484
    },
    {
        "loss": 1.7081,
        "grad_norm": 1.9183181524276733,
        "learning_rate": 0.00018254919661802988,
        "epoch": 0.06685966363385719,
        "step": 485
    },
    {
        "loss": 2.4399,
        "grad_norm": 1.4056535959243774,
        "learning_rate": 0.00018247554126468748,
        "epoch": 0.06699751861042183,
        "step": 486
    },
    {
        "loss": 1.935,
        "grad_norm": 2.2567129135131836,
        "learning_rate": 0.00018240174572331977,
        "epoch": 0.06713537358698649,
        "step": 487
    },
    {
        "loss": 2.4263,
        "grad_norm": 1.4008958339691162,
        "learning_rate": 0.00018232781011936092,
        "epoch": 0.06727322856355114,
        "step": 488
    },
    {
        "loss": 2.0995,
        "grad_norm": 1.7789300680160522,
        "learning_rate": 0.00018225373457848325,
        "epoch": 0.0674110835401158,
        "step": 489
    },
    {
        "loss": 1.5795,
        "grad_norm": 2.229647397994995,
        "learning_rate": 0.00018217951922659684,
        "epoch": 0.06754893851668045,
        "step": 490
    },
    {
        "loss": 2.5972,
        "grad_norm": 2.026496410369873,
        "learning_rate": 0.00018210516418984938,
        "epoch": 0.0676867934932451,
        "step": 491
    },
    {
        "loss": 2.4353,
        "grad_norm": 1.27970290184021,
        "learning_rate": 0.0001820306695946261,
        "epoch": 0.06782464846980976,
        "step": 492
    },
    {
        "loss": 2.6436,
        "grad_norm": 1.6374201774597168,
        "learning_rate": 0.00018195603556754938,
        "epoch": 0.06796250344637442,
        "step": 493
    },
    {
        "loss": 1.6619,
        "grad_norm": 2.1031877994537354,
        "learning_rate": 0.00018188126223547861,
        "epoch": 0.06810035842293907,
        "step": 494
    },
    {
        "loss": 1.7165,
        "grad_norm": 2.317976713180542,
        "learning_rate": 0.00018180634972550993,
        "epoch": 0.06823821339950373,
        "step": 495
    },
    {
        "loss": 2.2092,
        "grad_norm": 2.1330080032348633,
        "learning_rate": 0.00018173129816497613,
        "epoch": 0.06837606837606838,
        "step": 496
    },
    {
        "loss": 2.0832,
        "grad_norm": 1.2878130674362183,
        "learning_rate": 0.0001816561076814463,
        "epoch": 0.06851392335263302,
        "step": 497
    },
    {
        "loss": 2.3505,
        "grad_norm": 1.2106791734695435,
        "learning_rate": 0.00018158077840272564,
        "epoch": 0.06865177832919768,
        "step": 498
    },
    {
        "loss": 2.2623,
        "grad_norm": 1.392785906791687,
        "learning_rate": 0.00018150531045685534,
        "epoch": 0.06878963330576233,
        "step": 499
    },
    {
        "loss": 2.1746,
        "grad_norm": 1.743062973022461,
        "learning_rate": 0.00018142970397211217,
        "epoch": 0.06892748828232699,
        "step": 500
    },
    {
        "loss": 2.5363,
        "grad_norm": 1.437826156616211,
        "learning_rate": 0.00018135395907700852,
        "epoch": 0.06906534325889165,
        "step": 501
    },
    {
        "loss": 2.3251,
        "grad_norm": 1.5576533079147339,
        "learning_rate": 0.000181278075900292,
        "epoch": 0.0692031982354563,
        "step": 502
    },
    {
        "loss": 2.3105,
        "grad_norm": 1.147500991821289,
        "learning_rate": 0.00018120205457094514,
        "epoch": 0.06934105321202096,
        "step": 503
    },
    {
        "loss": 2.7105,
        "grad_norm": 0.9539979696273804,
        "learning_rate": 0.0001811258952181855,
        "epoch": 0.06947890818858561,
        "step": 504
    },
    {
        "loss": 1.5478,
        "grad_norm": 1.7775578498840332,
        "learning_rate": 0.00018104959797146512,
        "epoch": 0.06961676316515027,
        "step": 505
    },
    {
        "loss": 2.4424,
        "grad_norm": 1.6586707830429077,
        "learning_rate": 0.00018097316296047047,
        "epoch": 0.06975461814171492,
        "step": 506
    },
    {
        "loss": 1.6478,
        "grad_norm": 2.109741449356079,
        "learning_rate": 0.0001808965903151221,
        "epoch": 0.06989247311827956,
        "step": 507
    },
    {
        "loss": 1.7861,
        "grad_norm": 1.7166153192520142,
        "learning_rate": 0.00018081988016557468,
        "epoch": 0.07003032809484422,
        "step": 508
    },
    {
        "loss": 1.6179,
        "grad_norm": 2.1644198894500732,
        "learning_rate": 0.0001807430326422164,
        "epoch": 0.07016818307140887,
        "step": 509
    },
    {
        "loss": 2.3999,
        "grad_norm": 1.7547188997268677,
        "learning_rate": 0.0001806660478756691,
        "epoch": 0.07030603804797353,
        "step": 510
    },
    {
        "loss": 2.2738,
        "grad_norm": 1.6512243747711182,
        "learning_rate": 0.00018058892599678782,
        "epoch": 0.07044389302453818,
        "step": 511
    },
    {
        "loss": 1.2117,
        "grad_norm": 1.981851577758789,
        "learning_rate": 0.0001805116671366607,
        "epoch": 0.07058174800110284,
        "step": 512
    },
    {
        "loss": 2.1925,
        "grad_norm": 1.8467154502868652,
        "learning_rate": 0.0001804342714266087,
        "epoch": 0.0707196029776675,
        "step": 513
    },
    {
        "loss": 2.0732,
        "grad_norm": 1.291541576385498,
        "learning_rate": 0.00018035673899818532,
        "epoch": 0.07085745795423215,
        "step": 514
    },
    {
        "loss": 2.0384,
        "grad_norm": 1.878565788269043,
        "learning_rate": 0.00018027906998317663,
        "epoch": 0.0709953129307968,
        "step": 515
    },
    {
        "loss": 2.374,
        "grad_norm": 1.2118372917175293,
        "learning_rate": 0.00018020126451360067,
        "epoch": 0.07113316790736146,
        "step": 516
    },
    {
        "loss": 2.5666,
        "grad_norm": 1.1250120401382446,
        "learning_rate": 0.0001801233227217075,
        "epoch": 0.07127102288392612,
        "step": 517
    },
    {
        "loss": 2.4132,
        "grad_norm": 1.467814326286316,
        "learning_rate": 0.00018004524473997895,
        "epoch": 0.07140887786049076,
        "step": 518
    },
    {
        "loss": 1.849,
        "grad_norm": 1.8796415328979492,
        "learning_rate": 0.00017996703070112822,
        "epoch": 0.07154673283705541,
        "step": 519
    },
    {
        "loss": 1.9817,
        "grad_norm": 1.6403478384017944,
        "learning_rate": 0.00017988868073809985,
        "epoch": 0.07168458781362007,
        "step": 520
    },
    {
        "loss": 2.4392,
        "grad_norm": 1.1601626873016357,
        "learning_rate": 0.00017981019498406945,
        "epoch": 0.07182244279018472,
        "step": 521
    },
    {
        "loss": 1.8452,
        "grad_norm": 2.398050308227539,
        "learning_rate": 0.00017973157357244332,
        "epoch": 0.07196029776674938,
        "step": 522
    },
    {
        "loss": 2.4501,
        "grad_norm": 1.0553789138793945,
        "learning_rate": 0.00017965281663685846,
        "epoch": 0.07209815274331403,
        "step": 523
    },
    {
        "loss": 2.6608,
        "grad_norm": 1.7714195251464844,
        "learning_rate": 0.00017957392431118218,
        "epoch": 0.07223600771987869,
        "step": 524
    },
    {
        "loss": 1.8867,
        "grad_norm": 1.9234278202056885,
        "learning_rate": 0.00017949489672951195,
        "epoch": 0.07237386269644334,
        "step": 525
    },
    {
        "loss": 2.2613,
        "grad_norm": 1.4428997039794922,
        "learning_rate": 0.00017941573402617504,
        "epoch": 0.072511717673008,
        "step": 526
    },
    {
        "loss": 2.6269,
        "grad_norm": 1.3732408285140991,
        "learning_rate": 0.00017933643633572855,
        "epoch": 0.07264957264957266,
        "step": 527
    },
    {
        "loss": 2.4341,
        "grad_norm": 1.3558684587478638,
        "learning_rate": 0.00017925700379295892,
        "epoch": 0.07278742762613731,
        "step": 528
    },
    {
        "loss": 2.311,
        "grad_norm": 1.7680110931396484,
        "learning_rate": 0.00017917743653288182,
        "epoch": 0.07292528260270195,
        "step": 529
    },
    {
        "loss": 1.922,
        "grad_norm": 1.3675392866134644,
        "learning_rate": 0.0001790977346907419,
        "epoch": 0.07306313757926661,
        "step": 530
    },
    {
        "loss": 1.4035,
        "grad_norm": 1.9807391166687012,
        "learning_rate": 0.0001790178984020126,
        "epoch": 0.07320099255583126,
        "step": 531
    },
    {
        "loss": 2.3748,
        "grad_norm": 1.5509017705917358,
        "learning_rate": 0.0001789379278023959,
        "epoch": 0.07333884753239592,
        "step": 532
    },
    {
        "loss": 1.8166,
        "grad_norm": 1.7493089437484741,
        "learning_rate": 0.00017885782302782201,
        "epoch": 0.07347670250896057,
        "step": 533
    },
    {
        "loss": 2.357,
        "grad_norm": 1.2779268026351929,
        "learning_rate": 0.00017877758421444922,
        "epoch": 0.07361455748552523,
        "step": 534
    },
    {
        "loss": 2.7315,
        "grad_norm": 1.2947351932525635,
        "learning_rate": 0.00017869721149866373,
        "epoch": 0.07375241246208988,
        "step": 535
    },
    {
        "loss": 1.7965,
        "grad_norm": 1.8552982807159424,
        "learning_rate": 0.00017861670501707923,
        "epoch": 0.07389026743865454,
        "step": 536
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.4950716495513916,
        "learning_rate": 0.00017853606490653685,
        "epoch": 0.0740281224152192,
        "step": 537
    },
    {
        "loss": 1.7375,
        "grad_norm": 1.5512598752975464,
        "learning_rate": 0.00017845529130410483,
        "epoch": 0.07416597739178385,
        "step": 538
    },
    {
        "loss": 1.635,
        "grad_norm": 1.1624276638031006,
        "learning_rate": 0.00017837438434707833,
        "epoch": 0.07430383236834849,
        "step": 539
    },
    {
        "loss": 2.6825,
        "grad_norm": 1.0978779792785645,
        "learning_rate": 0.00017829334417297917,
        "epoch": 0.07444168734491315,
        "step": 540
    },
    {
        "loss": 1.471,
        "grad_norm": 2.380277395248413,
        "learning_rate": 0.00017821217091955563,
        "epoch": 0.0745795423214778,
        "step": 541
    },
    {
        "loss": 1.4809,
        "grad_norm": 2.3230106830596924,
        "learning_rate": 0.00017813086472478207,
        "epoch": 0.07471739729804246,
        "step": 542
    },
    {
        "loss": 2.0648,
        "grad_norm": 1.5355497598648071,
        "learning_rate": 0.00017804942572685901,
        "epoch": 0.07485525227460711,
        "step": 543
    },
    {
        "loss": 2.1042,
        "grad_norm": 2.1084091663360596,
        "learning_rate": 0.0001779678540642126,
        "epoch": 0.07499310725117177,
        "step": 544
    },
    {
        "loss": 2.1991,
        "grad_norm": 1.600236177444458,
        "learning_rate": 0.00017788614987549446,
        "epoch": 0.07513096222773642,
        "step": 545
    },
    {
        "loss": 2.5462,
        "grad_norm": 0.9758430123329163,
        "learning_rate": 0.00017780431329958156,
        "epoch": 0.07526881720430108,
        "step": 546
    },
    {
        "loss": 2.3714,
        "grad_norm": 1.4347163438796997,
        "learning_rate": 0.00017772234447557578,
        "epoch": 0.07540667218086573,
        "step": 547
    },
    {
        "loss": 2.3255,
        "grad_norm": 1.2554513216018677,
        "learning_rate": 0.00017764024354280386,
        "epoch": 0.07554452715743039,
        "step": 548
    },
    {
        "loss": 2.6826,
        "grad_norm": 1.4856449365615845,
        "learning_rate": 0.0001775580106408171,
        "epoch": 0.07568238213399504,
        "step": 549
    },
    {
        "loss": 2.1935,
        "grad_norm": 1.8609455823898315,
        "learning_rate": 0.00017747564590939115,
        "epoch": 0.07582023711055969,
        "step": 550
    },
    {
        "loss": 2.177,
        "grad_norm": 1.359616756439209,
        "learning_rate": 0.0001773931494885256,
        "epoch": 0.07595809208712434,
        "step": 551
    },
    {
        "loss": 2.3934,
        "grad_norm": 1.3227546215057373,
        "learning_rate": 0.00017731052151844398,
        "epoch": 0.076095947063689,
        "step": 552
    },
    {
        "loss": 2.3638,
        "grad_norm": 1.4331456422805786,
        "learning_rate": 0.00017722776213959348,
        "epoch": 0.07623380204025365,
        "step": 553
    },
    {
        "loss": 2.225,
        "grad_norm": 1.1453479528427124,
        "learning_rate": 0.00017714487149264447,
        "epoch": 0.0763716570168183,
        "step": 554
    },
    {
        "loss": 2.3649,
        "grad_norm": 1.424330234527588,
        "learning_rate": 0.00017706184971849064,
        "epoch": 0.07650951199338296,
        "step": 555
    },
    {
        "loss": 2.0982,
        "grad_norm": 2.2056469917297363,
        "learning_rate": 0.00017697869695824844,
        "epoch": 0.07664736696994762,
        "step": 556
    },
    {
        "loss": 2.4132,
        "grad_norm": 1.1666319370269775,
        "learning_rate": 0.00017689541335325702,
        "epoch": 0.07678522194651227,
        "step": 557
    },
    {
        "loss": 1.5887,
        "grad_norm": 1.6850309371948242,
        "learning_rate": 0.0001768119990450779,
        "epoch": 0.07692307692307693,
        "step": 558
    },
    {
        "loss": 2.158,
        "grad_norm": 1.6913609504699707,
        "learning_rate": 0.00017672845417549473,
        "epoch": 0.07706093189964158,
        "step": 559
    },
    {
        "loss": 1.9665,
        "grad_norm": 1.719702959060669,
        "learning_rate": 0.00017664477888651317,
        "epoch": 0.07719878687620624,
        "step": 560
    },
    {
        "loss": 2.4048,
        "grad_norm": 1.6508262157440186,
        "learning_rate": 0.0001765609733203606,
        "epoch": 0.07733664185277088,
        "step": 561
    },
    {
        "loss": 1.9715,
        "grad_norm": 1.9306007623672485,
        "learning_rate": 0.00017647703761948565,
        "epoch": 0.07747449682933553,
        "step": 562
    },
    {
        "loss": 1.835,
        "grad_norm": 1.6639931201934814,
        "learning_rate": 0.00017639297192655827,
        "epoch": 0.07761235180590019,
        "step": 563
    },
    {
        "loss": 2.2795,
        "grad_norm": 1.4049298763275146,
        "learning_rate": 0.00017630877638446938,
        "epoch": 0.07775020678246485,
        "step": 564
    },
    {
        "loss": 2.3696,
        "grad_norm": 1.08574640750885,
        "learning_rate": 0.00017622445113633055,
        "epoch": 0.0778880617590295,
        "step": 565
    },
    {
        "loss": 2.189,
        "grad_norm": 1.337363839149475,
        "learning_rate": 0.0001761399963254739,
        "epoch": 0.07802591673559416,
        "step": 566
    },
    {
        "loss": 2.1662,
        "grad_norm": 2.074611186981201,
        "learning_rate": 0.00017605541209545167,
        "epoch": 0.07816377171215881,
        "step": 567
    },
    {
        "loss": 2.4805,
        "grad_norm": 1.4210152626037598,
        "learning_rate": 0.0001759706985900362,
        "epoch": 0.07830162668872347,
        "step": 568
    },
    {
        "loss": 1.7396,
        "grad_norm": 1.5432844161987305,
        "learning_rate": 0.0001758858559532194,
        "epoch": 0.07843948166528812,
        "step": 569
    },
    {
        "loss": 2.1746,
        "grad_norm": 1.8163286447525024,
        "learning_rate": 0.00017580088432921288,
        "epoch": 0.07857733664185278,
        "step": 570
    },
    {
        "loss": 2.3443,
        "grad_norm": 1.6082628965377808,
        "learning_rate": 0.0001757157838624473,
        "epoch": 0.07871519161841742,
        "step": 571
    },
    {
        "loss": 2.2915,
        "grad_norm": 1.7325140237808228,
        "learning_rate": 0.00017563055469757246,
        "epoch": 0.07885304659498207,
        "step": 572
    },
    {
        "loss": 2.2325,
        "grad_norm": 0.8263524770736694,
        "learning_rate": 0.00017554519697945688,
        "epoch": 0.07899090157154673,
        "step": 573
    },
    {
        "loss": 1.5082,
        "grad_norm": 2.2423508167266846,
        "learning_rate": 0.00017545971085318753,
        "epoch": 0.07912875654811138,
        "step": 574
    },
    {
        "loss": 2.4495,
        "grad_norm": 1.3138798475265503,
        "learning_rate": 0.0001753740964640697,
        "epoch": 0.07926661152467604,
        "step": 575
    },
    {
        "loss": 2.2207,
        "grad_norm": 1.0176868438720703,
        "learning_rate": 0.00017528835395762667,
        "epoch": 0.0794044665012407,
        "step": 576
    },
    {
        "loss": 2.1177,
        "grad_norm": 1.9602234363555908,
        "learning_rate": 0.00017520248347959954,
        "epoch": 0.07954232147780535,
        "step": 577
    },
    {
        "loss": 1.9734,
        "grad_norm": 2.07175350189209,
        "learning_rate": 0.00017511648517594684,
        "epoch": 0.07968017645437,
        "step": 578
    },
    {
        "loss": 2.1201,
        "grad_norm": 1.5630847215652466,
        "learning_rate": 0.00017503035919284445,
        "epoch": 0.07981803143093466,
        "step": 579
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.282841682434082,
        "learning_rate": 0.00017494410567668521,
        "epoch": 0.07995588640749932,
        "step": 580
    },
    {
        "loss": 2.3422,
        "grad_norm": 1.0658342838287354,
        "learning_rate": 0.00017485772477407886,
        "epoch": 0.08009374138406397,
        "step": 581
    },
    {
        "loss": 2.3655,
        "grad_norm": 1.1382094621658325,
        "learning_rate": 0.00017477121663185143,
        "epoch": 0.08023159636062861,
        "step": 582
    },
    {
        "loss": 2.0862,
        "grad_norm": 1.6380356550216675,
        "learning_rate": 0.00017468458139704545,
        "epoch": 0.08036945133719327,
        "step": 583
    },
    {
        "loss": 2.3557,
        "grad_norm": 1.5130294561386108,
        "learning_rate": 0.00017459781921691945,
        "epoch": 0.08050730631375792,
        "step": 584
    },
    {
        "loss": 2.3732,
        "grad_norm": 1.2703391313552856,
        "learning_rate": 0.00017451093023894757,
        "epoch": 0.08064516129032258,
        "step": 585
    },
    {
        "loss": 2.2906,
        "grad_norm": 1.4670884609222412,
        "learning_rate": 0.00017442391461081966,
        "epoch": 0.08078301626688723,
        "step": 586
    },
    {
        "loss": 2.6615,
        "grad_norm": 0.9794681072235107,
        "learning_rate": 0.00017433677248044073,
        "epoch": 0.08092087124345189,
        "step": 587
    },
    {
        "loss": 1.8853,
        "grad_norm": 1.0377981662750244,
        "learning_rate": 0.00017424950399593092,
        "epoch": 0.08105872622001654,
        "step": 588
    },
    {
        "loss": 2.4285,
        "grad_norm": 1.5424600839614868,
        "learning_rate": 0.00017416210930562497,
        "epoch": 0.0811965811965812,
        "step": 589
    },
    {
        "loss": 2.4339,
        "grad_norm": 0.8572791814804077,
        "learning_rate": 0.00017407458855807232,
        "epoch": 0.08133443617314585,
        "step": 590
    },
    {
        "loss": 2.282,
        "grad_norm": 1.2878990173339844,
        "learning_rate": 0.00017398694190203656,
        "epoch": 0.08147229114971051,
        "step": 591
    },
    {
        "loss": 2.539,
        "grad_norm": 2.1081833839416504,
        "learning_rate": 0.00017389916948649538,
        "epoch": 0.08161014612627517,
        "step": 592
    },
    {
        "loss": 1.5103,
        "grad_norm": 1.9121545553207397,
        "learning_rate": 0.00017381127146064013,
        "epoch": 0.0817480011028398,
        "step": 593
    },
    {
        "loss": 2.1016,
        "grad_norm": 3.1387546062469482,
        "learning_rate": 0.00017372324797387573,
        "epoch": 0.08188585607940446,
        "step": 594
    },
    {
        "loss": 1.8605,
        "grad_norm": 1.691416621208191,
        "learning_rate": 0.00017363509917582036,
        "epoch": 0.08202371105596912,
        "step": 595
    },
    {
        "loss": 2.4488,
        "grad_norm": 1.1851704120635986,
        "learning_rate": 0.00017354682521630516,
        "epoch": 0.08216156603253377,
        "step": 596
    },
    {
        "loss": 2.4703,
        "grad_norm": 1.1345700025558472,
        "learning_rate": 0.00017345842624537404,
        "epoch": 0.08229942100909843,
        "step": 597
    },
    {
        "loss": 1.8807,
        "grad_norm": 1.7733596563339233,
        "learning_rate": 0.00017336990241328338,
        "epoch": 0.08243727598566308,
        "step": 598
    },
    {
        "loss": 2.4012,
        "grad_norm": 1.246582269668579,
        "learning_rate": 0.00017328125387050186,
        "epoch": 0.08257513096222774,
        "step": 599
    },
    {
        "loss": 1.6161,
        "grad_norm": 1.554224967956543,
        "learning_rate": 0.00017319248076771,
        "epoch": 0.0827129859387924,
        "step": 600
    },
    {
        "loss": 2.426,
        "grad_norm": 1.1344612836837769,
        "learning_rate": 0.00017310358325580016,
        "epoch": 0.08285084091535705,
        "step": 601
    },
    {
        "loss": 2.0503,
        "grad_norm": 1.130231499671936,
        "learning_rate": 0.0001730145614858761,
        "epoch": 0.0829886958919217,
        "step": 602
    },
    {
        "loss": 2.87,
        "grad_norm": 1.1988742351531982,
        "learning_rate": 0.0001729254156092529,
        "epoch": 0.08312655086848635,
        "step": 603
    },
    {
        "loss": 1.9804,
        "grad_norm": 1.8670334815979004,
        "learning_rate": 0.00017283614577745643,
        "epoch": 0.083264405845051,
        "step": 604
    },
    {
        "loss": 1.8554,
        "grad_norm": 1.5688071250915527,
        "learning_rate": 0.0001727467521422233,
        "epoch": 0.08340226082161566,
        "step": 605
    },
    {
        "loss": 2.3385,
        "grad_norm": 1.2622236013412476,
        "learning_rate": 0.00017265723485550066,
        "epoch": 0.08354011579818031,
        "step": 606
    },
    {
        "loss": 2.1736,
        "grad_norm": 1.3225576877593994,
        "learning_rate": 0.00017256759406944573,
        "epoch": 0.08367797077474497,
        "step": 607
    },
    {
        "loss": 1.7234,
        "grad_norm": 1.8110867738723755,
        "learning_rate": 0.0001724778299364257,
        "epoch": 0.08381582575130962,
        "step": 608
    },
    {
        "loss": 2.0841,
        "grad_norm": 1.5169428586959839,
        "learning_rate": 0.00017238794260901737,
        "epoch": 0.08395368072787428,
        "step": 609
    },
    {
        "loss": 2.3629,
        "grad_norm": 1.690782904624939,
        "learning_rate": 0.00017229793224000697,
        "epoch": 0.08409153570443893,
        "step": 610
    },
    {
        "loss": 1.8874,
        "grad_norm": 1.8804023265838623,
        "learning_rate": 0.00017220779898238987,
        "epoch": 0.08422939068100359,
        "step": 611
    },
    {
        "loss": 2.1626,
        "grad_norm": 1.5419542789459229,
        "learning_rate": 0.00017211754298937035,
        "epoch": 0.08436724565756824,
        "step": 612
    },
    {
        "loss": 2.5602,
        "grad_norm": 0.9687097072601318,
        "learning_rate": 0.00017202716441436125,
        "epoch": 0.0845051006341329,
        "step": 613
    },
    {
        "loss": 2.1637,
        "grad_norm": 1.1286348104476929,
        "learning_rate": 0.00017193666341098378,
        "epoch": 0.08464295561069754,
        "step": 614
    },
    {
        "loss": 1.7295,
        "grad_norm": 1.9839979410171509,
        "learning_rate": 0.0001718460401330673,
        "epoch": 0.0847808105872622,
        "step": 615
    },
    {
        "loss": 2.2567,
        "grad_norm": 1.4514604806900024,
        "learning_rate": 0.00017175529473464898,
        "epoch": 0.08491866556382685,
        "step": 616
    },
    {
        "loss": 2.5723,
        "grad_norm": 2.3294596672058105,
        "learning_rate": 0.00017166442736997356,
        "epoch": 0.0850565205403915,
        "step": 617
    },
    {
        "loss": 1.8593,
        "grad_norm": 1.1952030658721924,
        "learning_rate": 0.00017157343819349303,
        "epoch": 0.08519437551695616,
        "step": 618
    },
    {
        "loss": 2.4408,
        "grad_norm": 1.0274425745010376,
        "learning_rate": 0.00017148232735986655,
        "epoch": 0.08533223049352082,
        "step": 619
    },
    {
        "loss": 2.4275,
        "grad_norm": 1.2508867979049683,
        "learning_rate": 0.00017139109502396,
        "epoch": 0.08547008547008547,
        "step": 620
    },
    {
        "loss": 1.6728,
        "grad_norm": 2.1185216903686523,
        "learning_rate": 0.00017129974134084575,
        "epoch": 0.08560794044665013,
        "step": 621
    },
    {
        "loss": 2.3782,
        "grad_norm": 1.3419263362884521,
        "learning_rate": 0.00017120826646580253,
        "epoch": 0.08574579542321478,
        "step": 622
    },
    {
        "loss": 2.2445,
        "grad_norm": 1.6222420930862427,
        "learning_rate": 0.00017111667055431498,
        "epoch": 0.08588365039977944,
        "step": 623
    },
    {
        "loss": 2.0638,
        "grad_norm": 1.7424852848052979,
        "learning_rate": 0.00017102495376207348,
        "epoch": 0.08602150537634409,
        "step": 624
    },
    {
        "loss": 1.3247,
        "grad_norm": 1.802312970161438,
        "learning_rate": 0.0001709331162449739,
        "epoch": 0.08615936035290873,
        "step": 625
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.7668639421463013,
        "learning_rate": 0.00017084115815911734,
        "epoch": 0.08629721532947339,
        "step": 626
    },
    {
        "loss": 1.1962,
        "grad_norm": 2.4309537410736084,
        "learning_rate": 0.00017074907966080978,
        "epoch": 0.08643507030603804,
        "step": 627
    },
    {
        "loss": 2.3649,
        "grad_norm": 1.004601001739502,
        "learning_rate": 0.00017065688090656193,
        "epoch": 0.0865729252826027,
        "step": 628
    },
    {
        "loss": 2.4927,
        "grad_norm": 1.1537367105484009,
        "learning_rate": 0.0001705645620530888,
        "epoch": 0.08671078025916736,
        "step": 629
    },
    {
        "loss": 1.8948,
        "grad_norm": 2.869654893875122,
        "learning_rate": 0.00017047212325730973,
        "epoch": 0.08684863523573201,
        "step": 630
    },
    {
        "loss": 2.2773,
        "grad_norm": 1.1093684434890747,
        "learning_rate": 0.0001703795646763477,
        "epoch": 0.08698649021229667,
        "step": 631
    },
    {
        "loss": 2.3897,
        "grad_norm": 1.425830602645874,
        "learning_rate": 0.00017028688646752947,
        "epoch": 0.08712434518886132,
        "step": 632
    },
    {
        "loss": 1.6929,
        "grad_norm": 1.5540940761566162,
        "learning_rate": 0.00017019408878838504,
        "epoch": 0.08726220016542598,
        "step": 633
    },
    {
        "loss": 2.1808,
        "grad_norm": 1.1354668140411377,
        "learning_rate": 0.00017010117179664755,
        "epoch": 0.08740005514199063,
        "step": 634
    },
    {
        "loss": 1.6184,
        "grad_norm": 1.8736311197280884,
        "learning_rate": 0.00017000813565025291,
        "epoch": 0.08753791011855527,
        "step": 635
    },
    {
        "loss": 2.0736,
        "grad_norm": 1.6098606586456299,
        "learning_rate": 0.00016991498050733953,
        "epoch": 0.08767576509511993,
        "step": 636
    },
    {
        "loss": 1.9903,
        "grad_norm": 1.6329078674316406,
        "learning_rate": 0.0001698217065262482,
        "epoch": 0.08781362007168458,
        "step": 637
    },
    {
        "loss": 2.5941,
        "grad_norm": 1.6323624849319458,
        "learning_rate": 0.00016972831386552148,
        "epoch": 0.08795147504824924,
        "step": 638
    },
    {
        "loss": 2.4706,
        "grad_norm": 1.574784755706787,
        "learning_rate": 0.0001696348026839039,
        "epoch": 0.0880893300248139,
        "step": 639
    },
    {
        "loss": 2.3682,
        "grad_norm": 1.229602575302124,
        "learning_rate": 0.00016954117314034136,
        "epoch": 0.08822718500137855,
        "step": 640
    },
    {
        "loss": 2.0577,
        "grad_norm": 1.2048146724700928,
        "learning_rate": 0.00016944742539398084,
        "epoch": 0.0883650399779432,
        "step": 641
    },
    {
        "loss": 2.4615,
        "grad_norm": 1.6434049606323242,
        "learning_rate": 0.00016935355960417042,
        "epoch": 0.08850289495450786,
        "step": 642
    },
    {
        "loss": 1.9713,
        "grad_norm": 1.5535709857940674,
        "learning_rate": 0.0001692595759304587,
        "epoch": 0.08864074993107252,
        "step": 643
    },
    {
        "loss": 2.1623,
        "grad_norm": 1.4402313232421875,
        "learning_rate": 0.00016916547453259465,
        "epoch": 0.08877860490763717,
        "step": 644
    },
    {
        "loss": 1.8839,
        "grad_norm": 1.711637258529663,
        "learning_rate": 0.00016907125557052744,
        "epoch": 0.08891645988420183,
        "step": 645
    },
    {
        "loss": 2.2547,
        "grad_norm": 0.9428300857543945,
        "learning_rate": 0.00016897691920440594,
        "epoch": 0.08905431486076647,
        "step": 646
    },
    {
        "loss": 1.8615,
        "grad_norm": 1.9362359046936035,
        "learning_rate": 0.0001688824655945787,
        "epoch": 0.08919216983733112,
        "step": 647
    },
    {
        "loss": 2.308,
        "grad_norm": 1.5211213827133179,
        "learning_rate": 0.00016878789490159344,
        "epoch": 0.08933002481389578,
        "step": 648
    },
    {
        "loss": 1.9766,
        "grad_norm": 1.6853299140930176,
        "learning_rate": 0.000168693207286197,
        "epoch": 0.08946787979046043,
        "step": 649
    },
    {
        "loss": 2.1313,
        "grad_norm": 2.7601277828216553,
        "learning_rate": 0.00016859840290933494,
        "epoch": 0.08960573476702509,
        "step": 650
    },
    {
        "loss": 2.1916,
        "grad_norm": 1.2631738185882568,
        "learning_rate": 0.00016850348193215116,
        "epoch": 0.08974358974358974,
        "step": 651
    },
    {
        "loss": 2.1373,
        "grad_norm": 1.5631095170974731,
        "learning_rate": 0.00016840844451598792,
        "epoch": 0.0898814447201544,
        "step": 652
    },
    {
        "loss": 1.701,
        "grad_norm": 1.8320105075836182,
        "learning_rate": 0.0001683132908223853,
        "epoch": 0.09001929969671905,
        "step": 653
    },
    {
        "loss": 2.1851,
        "grad_norm": 1.9459127187728882,
        "learning_rate": 0.0001682180210130811,
        "epoch": 0.09015715467328371,
        "step": 654
    },
    {
        "loss": 2.1335,
        "grad_norm": 1.7037112712860107,
        "learning_rate": 0.00016812263525001038,
        "epoch": 0.09029500964984837,
        "step": 655
    },
    {
        "loss": 2.2553,
        "grad_norm": 1.578708529472351,
        "learning_rate": 0.00016802713369530538,
        "epoch": 0.09043286462641302,
        "step": 656
    },
    {
        "loss": 1.4463,
        "grad_norm": 1.858466386795044,
        "learning_rate": 0.0001679315165112951,
        "epoch": 0.09057071960297766,
        "step": 657
    },
    {
        "loss": 1.9581,
        "grad_norm": 1.4996678829193115,
        "learning_rate": 0.0001678357838605051,
        "epoch": 0.09070857457954232,
        "step": 658
    },
    {
        "loss": 2.4378,
        "grad_norm": 1.5028040409088135,
        "learning_rate": 0.00016773993590565726,
        "epoch": 0.09084642955610697,
        "step": 659
    },
    {
        "loss": 2.4056,
        "grad_norm": 1.4604239463806152,
        "learning_rate": 0.00016764397280966933,
        "epoch": 0.09098428453267163,
        "step": 660
    },
    {
        "loss": 2.519,
        "grad_norm": 1.3562768697738647,
        "learning_rate": 0.00016754789473565496,
        "epoch": 0.09112213950923628,
        "step": 661
    },
    {
        "loss": 2.0334,
        "grad_norm": 2.3690664768218994,
        "learning_rate": 0.00016745170184692305,
        "epoch": 0.09125999448580094,
        "step": 662
    },
    {
        "loss": 1.5371,
        "grad_norm": 2.289301872253418,
        "learning_rate": 0.00016735539430697767,
        "epoch": 0.0913978494623656,
        "step": 663
    },
    {
        "loss": 1.5605,
        "grad_norm": 1.7447305917739868,
        "learning_rate": 0.0001672589722795179,
        "epoch": 0.09153570443893025,
        "step": 664
    },
    {
        "loss": 2.4407,
        "grad_norm": 0.9893500804901123,
        "learning_rate": 0.0001671624359284373,
        "epoch": 0.0916735594154949,
        "step": 665
    },
    {
        "loss": 1.738,
        "grad_norm": 1.635013222694397,
        "learning_rate": 0.00016706578541782384,
        "epoch": 0.09181141439205956,
        "step": 666
    },
    {
        "loss": 1.5332,
        "grad_norm": 1.706618070602417,
        "learning_rate": 0.00016696902091195944,
        "epoch": 0.0919492693686242,
        "step": 667
    },
    {
        "loss": 2.4249,
        "grad_norm": 1.5226725339889526,
        "learning_rate": 0.00016687214257531986,
        "epoch": 0.09208712434518886,
        "step": 668
    },
    {
        "loss": 2.1865,
        "grad_norm": 0.9962248206138611,
        "learning_rate": 0.0001667751505725743,
        "epoch": 0.09222497932175351,
        "step": 669
    },
    {
        "loss": 2.45,
        "grad_norm": 1.3395475149154663,
        "learning_rate": 0.0001666780450685852,
        "epoch": 0.09236283429831817,
        "step": 670
    },
    {
        "loss": 1.2695,
        "grad_norm": 2.2319345474243164,
        "learning_rate": 0.00016658082622840787,
        "epoch": 0.09250068927488282,
        "step": 671
    },
    {
        "loss": 2.2675,
        "grad_norm": 1.5772160291671753,
        "learning_rate": 0.00016648349421729031,
        "epoch": 0.09263854425144748,
        "step": 672
    },
    {
        "loss": 1.841,
        "grad_norm": 1.7623236179351807,
        "learning_rate": 0.00016638604920067294,
        "epoch": 0.09277639922801213,
        "step": 673
    },
    {
        "loss": 2.3252,
        "grad_norm": 1.3283623456954956,
        "learning_rate": 0.00016628849134418808,
        "epoch": 0.09291425420457679,
        "step": 674
    },
    {
        "loss": 1.948,
        "grad_norm": 1.2501245737075806,
        "learning_rate": 0.00016619082081366005,
        "epoch": 0.09305210918114144,
        "step": 675
    },
    {
        "loss": 1.9463,
        "grad_norm": 1.6067885160446167,
        "learning_rate": 0.00016609303777510452,
        "epoch": 0.0931899641577061,
        "step": 676
    },
    {
        "loss": 1.9396,
        "grad_norm": 1.6335783004760742,
        "learning_rate": 0.00016599514239472852,
        "epoch": 0.09332781913427075,
        "step": 677
    },
    {
        "loss": 2.2597,
        "grad_norm": 1.7135010957717896,
        "learning_rate": 0.00016589713483893,
        "epoch": 0.0934656741108354,
        "step": 678
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.4589201211929321,
        "learning_rate": 0.00016579901527429756,
        "epoch": 0.09360352908740005,
        "step": 679
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.602478504180908,
        "learning_rate": 0.00016570078386761021,
        "epoch": 0.0937413840639647,
        "step": 680
    },
    {
        "loss": 2.528,
        "grad_norm": 1.377367377281189,
        "learning_rate": 0.00016560244078583702,
        "epoch": 0.09387923904052936,
        "step": 681
    },
    {
        "loss": 2.3533,
        "grad_norm": 0.9329996109008789,
        "learning_rate": 0.00016550398619613688,
        "epoch": 0.09401709401709402,
        "step": 682
    },
    {
        "loss": 2.1769,
        "grad_norm": 1.2648770809173584,
        "learning_rate": 0.00016540542026585836,
        "epoch": 0.09415494899365867,
        "step": 683
    },
    {
        "loss": 2.1706,
        "grad_norm": 1.5074172019958496,
        "learning_rate": 0.00016530674316253907,
        "epoch": 0.09429280397022333,
        "step": 684
    },
    {
        "loss": 1.8719,
        "grad_norm": 1.9644266366958618,
        "learning_rate": 0.00016520795505390568,
        "epoch": 0.09443065894678798,
        "step": 685
    },
    {
        "loss": 2.6961,
        "grad_norm": 1.0854637622833252,
        "learning_rate": 0.0001651090561078736,
        "epoch": 0.09456851392335264,
        "step": 686
    },
    {
        "loss": 1.5737,
        "grad_norm": 1.7308835983276367,
        "learning_rate": 0.00016501004649254657,
        "epoch": 0.09470636889991729,
        "step": 687
    },
    {
        "loss": 2.0524,
        "grad_norm": 1.443651795387268,
        "learning_rate": 0.00016491092637621642,
        "epoch": 0.09484422387648195,
        "step": 688
    },
    {
        "loss": 2.2117,
        "grad_norm": 1.2477785348892212,
        "learning_rate": 0.00016481169592736286,
        "epoch": 0.09498207885304659,
        "step": 689
    },
    {
        "loss": 2.5862,
        "grad_norm": 0.8843035101890564,
        "learning_rate": 0.00016471235531465312,
        "epoch": 0.09511993382961124,
        "step": 690
    },
    {
        "loss": 2.1243,
        "grad_norm": 1.6485437154769897,
        "learning_rate": 0.00016461290470694168,
        "epoch": 0.0952577888061759,
        "step": 691
    },
    {
        "loss": 1.9706,
        "grad_norm": 1.522268295288086,
        "learning_rate": 0.00016451334427326997,
        "epoch": 0.09539564378274056,
        "step": 692
    },
    {
        "loss": 2.0881,
        "grad_norm": 2.1105360984802246,
        "learning_rate": 0.00016441367418286608,
        "epoch": 0.09553349875930521,
        "step": 693
    },
    {
        "loss": 2.3637,
        "grad_norm": 1.277424693107605,
        "learning_rate": 0.0001643138946051446,
        "epoch": 0.09567135373586987,
        "step": 694
    },
    {
        "loss": 1.4174,
        "grad_norm": 2.1703648567199707,
        "learning_rate": 0.00016421400570970605,
        "epoch": 0.09580920871243452,
        "step": 695
    },
    {
        "loss": 2.3095,
        "grad_norm": 2.1114511489868164,
        "learning_rate": 0.00016411400766633687,
        "epoch": 0.09594706368899918,
        "step": 696
    },
    {
        "loss": 2.1856,
        "grad_norm": 1.3459807634353638,
        "learning_rate": 0.00016401390064500903,
        "epoch": 0.09608491866556383,
        "step": 697
    },
    {
        "loss": 2.0049,
        "grad_norm": 1.2853316068649292,
        "learning_rate": 0.00016391368481587972,
        "epoch": 0.09622277364212849,
        "step": 698
    },
    {
        "loss": 1.9001,
        "grad_norm": 2.099651575088501,
        "learning_rate": 0.00016381336034929103,
        "epoch": 0.09636062861869313,
        "step": 699
    },
    {
        "loss": 2.4363,
        "grad_norm": 1.1435779333114624,
        "learning_rate": 0.00016371292741576977,
        "epoch": 0.09649848359525778,
        "step": 700
    },
    {
        "loss": 1.9273,
        "grad_norm": 1.4714833498001099,
        "learning_rate": 0.00016361238618602708,
        "epoch": 0.09663633857182244,
        "step": 701
    },
    {
        "loss": 2.1692,
        "grad_norm": 1.685625433921814,
        "learning_rate": 0.00016351173683095823,
        "epoch": 0.0967741935483871,
        "step": 702
    },
    {
        "loss": 1.47,
        "grad_norm": 1.365646243095398,
        "learning_rate": 0.00016341097952164214,
        "epoch": 0.09691204852495175,
        "step": 703
    },
    {
        "loss": 1.4398,
        "grad_norm": 2.2128612995147705,
        "learning_rate": 0.00016331011442934142,
        "epoch": 0.0970499035015164,
        "step": 704
    },
    {
        "loss": 2.2943,
        "grad_norm": 1.6424813270568848,
        "learning_rate": 0.0001632091417255017,
        "epoch": 0.09718775847808106,
        "step": 705
    },
    {
        "loss": 2.3092,
        "grad_norm": 1.330912709236145,
        "learning_rate": 0.00016310806158175169,
        "epoch": 0.09732561345464572,
        "step": 706
    },
    {
        "loss": 1.3625,
        "grad_norm": 1.617444396018982,
        "learning_rate": 0.00016300687416990258,
        "epoch": 0.09746346843121037,
        "step": 707
    },
    {
        "loss": 2.0331,
        "grad_norm": 1.2320457696914673,
        "learning_rate": 0.000162905579661948,
        "epoch": 0.09760132340777503,
        "step": 708
    },
    {
        "loss": 1.575,
        "grad_norm": 1.8948053121566772,
        "learning_rate": 0.0001628041782300635,
        "epoch": 0.09773917838433968,
        "step": 709
    },
    {
        "loss": 2.2704,
        "grad_norm": 1.1390407085418701,
        "learning_rate": 0.0001627026700466065,
        "epoch": 0.09787703336090432,
        "step": 710
    },
    {
        "loss": 2.281,
        "grad_norm": 2.2014784812927246,
        "learning_rate": 0.00016260105528411576,
        "epoch": 0.09801488833746898,
        "step": 711
    },
    {
        "loss": 2.5039,
        "grad_norm": 1.956503987312317,
        "learning_rate": 0.00016249933411531127,
        "epoch": 0.09815274331403363,
        "step": 712
    },
    {
        "loss": 2.1507,
        "grad_norm": 1.7975778579711914,
        "learning_rate": 0.00016239750671309387,
        "epoch": 0.09829059829059829,
        "step": 713
    },
    {
        "loss": 2.283,
        "grad_norm": 1.4874638319015503,
        "learning_rate": 0.00016229557325054498,
        "epoch": 0.09842845326716294,
        "step": 714
    },
    {
        "loss": 2.0349,
        "grad_norm": 2.660472869873047,
        "learning_rate": 0.00016219353390092622,
        "epoch": 0.0985663082437276,
        "step": 715
    },
    {
        "loss": 2.4673,
        "grad_norm": 1.2451884746551514,
        "learning_rate": 0.0001620913888376793,
        "epoch": 0.09870416322029225,
        "step": 716
    },
    {
        "loss": 2.6732,
        "grad_norm": 1.2255295515060425,
        "learning_rate": 0.00016198913823442559,
        "epoch": 0.09884201819685691,
        "step": 717
    },
    {
        "loss": 2.2764,
        "grad_norm": 1.1296151876449585,
        "learning_rate": 0.00016188678226496575,
        "epoch": 0.09897987317342156,
        "step": 718
    },
    {
        "loss": 1.4069,
        "grad_norm": 1.4881260395050049,
        "learning_rate": 0.00016178432110327973,
        "epoch": 0.09911772814998622,
        "step": 719
    },
    {
        "loss": 1.6834,
        "grad_norm": 1.381980299949646,
        "learning_rate": 0.000161681754923526,
        "epoch": 0.09925558312655088,
        "step": 720
    },
    {
        "loss": 1.9948,
        "grad_norm": 1.323948860168457,
        "learning_rate": 0.0001615790839000419,
        "epoch": 0.09939343810311552,
        "step": 721
    },
    {
        "loss": 2.3291,
        "grad_norm": 1.4832922220230103,
        "learning_rate": 0.00016147630820734267,
        "epoch": 0.09953129307968017,
        "step": 722
    },
    {
        "loss": 2.399,
        "grad_norm": 1.1432032585144043,
        "learning_rate": 0.00016137342802012157,
        "epoch": 0.09966914805624483,
        "step": 723
    },
    {
        "loss": 1.9608,
        "grad_norm": 1.6875237226486206,
        "learning_rate": 0.0001612704435132495,
        "epoch": 0.09980700303280948,
        "step": 724
    },
    {
        "loss": 2.4276,
        "grad_norm": 1.6326595544815063,
        "learning_rate": 0.00016116735486177466,
        "epoch": 0.09994485800937414,
        "step": 725
    },
    {
        "loss": 2.6,
        "grad_norm": 1.7914514541625977,
        "learning_rate": 0.00016106416224092224,
        "epoch": 0.1000827129859388,
        "step": 726
    },
    {
        "loss": 2.112,
        "grad_norm": 1.6689990758895874,
        "learning_rate": 0.0001609608658260942,
        "epoch": 0.10022056796250345,
        "step": 727
    },
    {
        "loss": 2.0811,
        "grad_norm": 1.3518197536468506,
        "learning_rate": 0.0001608574657928689,
        "epoch": 0.1003584229390681,
        "step": 728
    },
    {
        "loss": 1.8598,
        "grad_norm": 1.8460602760314941,
        "learning_rate": 0.00016075396231700085,
        "epoch": 0.10049627791563276,
        "step": 729
    },
    {
        "loss": 2.5337,
        "grad_norm": 1.3216350078582764,
        "learning_rate": 0.00016065035557442033,
        "epoch": 0.10063413289219741,
        "step": 730
    },
    {
        "loss": 1.3568,
        "grad_norm": 2.299190044403076,
        "learning_rate": 0.00016054664574123318,
        "epoch": 0.10077198786876206,
        "step": 731
    },
    {
        "loss": 2.3235,
        "grad_norm": 1.1914725303649902,
        "learning_rate": 0.0001604428329937205,
        "epoch": 0.10090984284532671,
        "step": 732
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.4723374843597412,
        "learning_rate": 0.00016033891750833826,
        "epoch": 0.10104769782189137,
        "step": 733
    },
    {
        "loss": 2.4521,
        "grad_norm": 1.9171929359436035,
        "learning_rate": 0.0001602348994617171,
        "epoch": 0.10118555279845602,
        "step": 734
    },
    {
        "loss": 1.9961,
        "grad_norm": 1.637590765953064,
        "learning_rate": 0.00016013077903066195,
        "epoch": 0.10132340777502068,
        "step": 735
    },
    {
        "loss": 2.3924,
        "grad_norm": 1.7248387336730957,
        "learning_rate": 0.00016002655639215182,
        "epoch": 0.10146126275158533,
        "step": 736
    },
    {
        "loss": 1.8951,
        "grad_norm": 1.5835306644439697,
        "learning_rate": 0.0001599222317233395,
        "epoch": 0.10159911772814999,
        "step": 737
    },
    {
        "loss": 2.2246,
        "grad_norm": 1.6634684801101685,
        "learning_rate": 0.00015981780520155097,
        "epoch": 0.10173697270471464,
        "step": 738
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.5911856889724731,
        "learning_rate": 0.00015971327700428563,
        "epoch": 0.1018748276812793,
        "step": 739
    },
    {
        "loss": 2.0373,
        "grad_norm": 1.428601622581482,
        "learning_rate": 0.0001596086473092155,
        "epoch": 0.10201268265784395,
        "step": 740
    },
    {
        "loss": 1.7911,
        "grad_norm": 2.1850621700286865,
        "learning_rate": 0.0001595039162941852,
        "epoch": 0.10215053763440861,
        "step": 741
    },
    {
        "loss": 1.5104,
        "grad_norm": 1.7641054391860962,
        "learning_rate": 0.00015939908413721156,
        "epoch": 0.10228839261097325,
        "step": 742
    },
    {
        "loss": 1.9433,
        "grad_norm": 1.602116584777832,
        "learning_rate": 0.00015929415101648332,
        "epoch": 0.1024262475875379,
        "step": 743
    },
    {
        "loss": 1.8525,
        "grad_norm": 1.7590856552124023,
        "learning_rate": 0.00015918911711036086,
        "epoch": 0.10256410256410256,
        "step": 744
    },
    {
        "loss": 2.245,
        "grad_norm": 1.0990631580352783,
        "learning_rate": 0.00015908398259737583,
        "epoch": 0.10270195754066722,
        "step": 745
    },
    {
        "loss": 1.6543,
        "grad_norm": 1.8786158561706543,
        "learning_rate": 0.0001589787476562309,
        "epoch": 0.10283981251723187,
        "step": 746
    },
    {
        "loss": 1.8629,
        "grad_norm": 1.9413470029830933,
        "learning_rate": 0.00015887341246579944,
        "epoch": 0.10297766749379653,
        "step": 747
    },
    {
        "loss": 1.9693,
        "grad_norm": 1.9435193538665771,
        "learning_rate": 0.00015876797720512523,
        "epoch": 0.10311552247036118,
        "step": 748
    },
    {
        "loss": 2.3599,
        "grad_norm": 1.6163398027420044,
        "learning_rate": 0.00015866244205342217,
        "epoch": 0.10325337744692584,
        "step": 749
    },
    {
        "loss": 2.4815,
        "grad_norm": 1.0408341884613037,
        "learning_rate": 0.0001585568071900739,
        "epoch": 0.10339123242349049,
        "step": 750
    },
    {
        "loss": 1.9326,
        "grad_norm": 1.6936959028244019,
        "learning_rate": 0.00015845107279463357,
        "epoch": 0.10352908740005515,
        "step": 751
    },
    {
        "loss": 2.4651,
        "grad_norm": 1.3279882669448853,
        "learning_rate": 0.00015834523904682346,
        "epoch": 0.1036669423766198,
        "step": 752
    },
    {
        "loss": 2.3991,
        "grad_norm": 1.3995742797851562,
        "learning_rate": 0.0001582393061265348,
        "epoch": 0.10380479735318444,
        "step": 753
    },
    {
        "loss": 2.2103,
        "grad_norm": 1.8572248220443726,
        "learning_rate": 0.00015813327421382742,
        "epoch": 0.1039426523297491,
        "step": 754
    },
    {
        "loss": 2.0447,
        "grad_norm": 1.1805288791656494,
        "learning_rate": 0.0001580271434889293,
        "epoch": 0.10408050730631375,
        "step": 755
    },
    {
        "loss": 2.0273,
        "grad_norm": 1.2718567848205566,
        "learning_rate": 0.00015792091413223638,
        "epoch": 0.10421836228287841,
        "step": 756
    },
    {
        "loss": 2.1856,
        "grad_norm": 1.2026561498641968,
        "learning_rate": 0.0001578145863243124,
        "epoch": 0.10435621725944307,
        "step": 757
    },
    {
        "loss": 1.9266,
        "grad_norm": 2.162895917892456,
        "learning_rate": 0.00015770816024588823,
        "epoch": 0.10449407223600772,
        "step": 758
    },
    {
        "loss": 2.3072,
        "grad_norm": 0.9944829344749451,
        "learning_rate": 0.00015760163607786196,
        "epoch": 0.10463192721257238,
        "step": 759
    },
    {
        "loss": 1.9977,
        "grad_norm": 1.4694644212722778,
        "learning_rate": 0.00015749501400129834,
        "epoch": 0.10476978218913703,
        "step": 760
    },
    {
        "loss": 2.5684,
        "grad_norm": 1.0208488702774048,
        "learning_rate": 0.00015738829419742852,
        "epoch": 0.10490763716570169,
        "step": 761
    },
    {
        "loss": 2.5529,
        "grad_norm": 1.0686448812484741,
        "learning_rate": 0.00015728147684764975,
        "epoch": 0.10504549214226634,
        "step": 762
    },
    {
        "loss": 2.3927,
        "grad_norm": 1.8845001459121704,
        "learning_rate": 0.00015717456213352508,
        "epoch": 0.10518334711883098,
        "step": 763
    },
    {
        "loss": 2.2906,
        "grad_norm": 1.7628778219223022,
        "learning_rate": 0.00015706755023678319,
        "epoch": 0.10532120209539564,
        "step": 764
    },
    {
        "loss": 2.2586,
        "grad_norm": 1.3229124546051025,
        "learning_rate": 0.00015696044133931775,
        "epoch": 0.1054590570719603,
        "step": 765
    },
    {
        "loss": 2.2653,
        "grad_norm": 1.7257493734359741,
        "learning_rate": 0.00015685323562318745,
        "epoch": 0.10559691204852495,
        "step": 766
    },
    {
        "loss": 2.4765,
        "grad_norm": 1.2439446449279785,
        "learning_rate": 0.0001567459332706155,
        "epoch": 0.1057347670250896,
        "step": 767
    },
    {
        "loss": 1.9803,
        "grad_norm": 1.6748405694961548,
        "learning_rate": 0.00015663853446398936,
        "epoch": 0.10587262200165426,
        "step": 768
    },
    {
        "loss": 1.7755,
        "grad_norm": 1.689056158065796,
        "learning_rate": 0.0001565310393858604,
        "epoch": 0.10601047697821891,
        "step": 769
    },
    {
        "loss": 2.2318,
        "grad_norm": 1.2318768501281738,
        "learning_rate": 0.00015642344821894367,
        "epoch": 0.10614833195478357,
        "step": 770
    },
    {
        "loss": 1.9645,
        "grad_norm": 1.716741919517517,
        "learning_rate": 0.00015631576114611765,
        "epoch": 0.10628618693134823,
        "step": 771
    },
    {
        "loss": 2.7677,
        "grad_norm": 1.2646219730377197,
        "learning_rate": 0.0001562079783504236,
        "epoch": 0.10642404190791288,
        "step": 772
    },
    {
        "loss": 2.2435,
        "grad_norm": 1.1247773170471191,
        "learning_rate": 0.0001561001000150657,
        "epoch": 0.10656189688447754,
        "step": 773
    },
    {
        "loss": 2.1782,
        "grad_norm": 1.5996116399765015,
        "learning_rate": 0.00015599212632341045,
        "epoch": 0.10669975186104218,
        "step": 774
    },
    {
        "loss": 1.5528,
        "grad_norm": 1.873757243156433,
        "learning_rate": 0.0001558840574589864,
        "epoch": 0.10683760683760683,
        "step": 775
    },
    {
        "loss": 1.7865,
        "grad_norm": 1.894213318824768,
        "learning_rate": 0.00015577589360548389,
        "epoch": 0.10697546181417149,
        "step": 776
    },
    {
        "loss": 2.1995,
        "grad_norm": 1.489780306816101,
        "learning_rate": 0.0001556676349467547,
        "epoch": 0.10711331679073614,
        "step": 777
    },
    {
        "loss": 2.0857,
        "grad_norm": 1.7185075283050537,
        "learning_rate": 0.00015555928166681183,
        "epoch": 0.1072511717673008,
        "step": 778
    },
    {
        "loss": 2.0582,
        "grad_norm": 1.7498939037322998,
        "learning_rate": 0.00015545083394982907,
        "epoch": 0.10738902674386545,
        "step": 779
    },
    {
        "loss": 1.7613,
        "grad_norm": 1.4852646589279175,
        "learning_rate": 0.00015534229198014069,
        "epoch": 0.10752688172043011,
        "step": 780
    },
    {
        "loss": 2.5014,
        "grad_norm": 1.2127255201339722,
        "learning_rate": 0.0001552336559422412,
        "epoch": 0.10766473669699476,
        "step": 781
    },
    {
        "loss": 2.0882,
        "grad_norm": 1.5377354621887207,
        "learning_rate": 0.00015512492602078498,
        "epoch": 0.10780259167355942,
        "step": 782
    },
    {
        "loss": 2.6556,
        "grad_norm": 1.3302260637283325,
        "learning_rate": 0.00015501610240058607,
        "epoch": 0.10794044665012408,
        "step": 783
    },
    {
        "loss": 1.9124,
        "grad_norm": 1.607540488243103,
        "learning_rate": 0.00015490718526661764,
        "epoch": 0.10807830162668873,
        "step": 784
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.3092137575149536,
        "learning_rate": 0.00015479817480401194,
        "epoch": 0.10821615660325337,
        "step": 785
    },
    {
        "loss": 2.3747,
        "grad_norm": 1.6222120523452759,
        "learning_rate": 0.00015468907119805975,
        "epoch": 0.10835401157981803,
        "step": 786
    },
    {
        "loss": 2.3439,
        "grad_norm": 1.5647118091583252,
        "learning_rate": 0.00015457987463421028,
        "epoch": 0.10849186655638268,
        "step": 787
    },
    {
        "loss": 2.0647,
        "grad_norm": 1.7198450565338135,
        "learning_rate": 0.00015447058529807065,
        "epoch": 0.10862972153294734,
        "step": 788
    },
    {
        "loss": 2.1888,
        "grad_norm": 1.623779535293579,
        "learning_rate": 0.00015436120337540573,
        "epoch": 0.10876757650951199,
        "step": 789
    },
    {
        "loss": 1.2811,
        "grad_norm": 2.141458034515381,
        "learning_rate": 0.00015425172905213772,
        "epoch": 0.10890543148607665,
        "step": 790
    },
    {
        "loss": 2.1987,
        "grad_norm": 1.3704307079315186,
        "learning_rate": 0.00015414216251434588,
        "epoch": 0.1090432864626413,
        "step": 791
    },
    {
        "loss": 1.9472,
        "grad_norm": 2.2386834621429443,
        "learning_rate": 0.00015403250394826627,
        "epoch": 0.10918114143920596,
        "step": 792
    },
    {
        "loss": 2.0629,
        "grad_norm": 1.7387076616287231,
        "learning_rate": 0.0001539227535402913,
        "epoch": 0.10931899641577061,
        "step": 793
    },
    {
        "loss": 2.1984,
        "grad_norm": 1.2243666648864746,
        "learning_rate": 0.00015381291147696958,
        "epoch": 0.10945685139233527,
        "step": 794
    },
    {
        "loss": 2.1057,
        "grad_norm": 2.2975752353668213,
        "learning_rate": 0.00015370297794500536,
        "epoch": 0.10959470636889991,
        "step": 795
    },
    {
        "loss": 2.2793,
        "grad_norm": 2.008213758468628,
        "learning_rate": 0.00015359295313125855,
        "epoch": 0.10973256134546457,
        "step": 796
    },
    {
        "loss": 0.8362,
        "grad_norm": 1.9760799407958984,
        "learning_rate": 0.0001534828372227441,
        "epoch": 0.10987041632202922,
        "step": 797
    },
    {
        "loss": 1.6545,
        "grad_norm": 1.2884411811828613,
        "learning_rate": 0.0001533726304066318,
        "epoch": 0.11000827129859388,
        "step": 798
    },
    {
        "loss": 2.3987,
        "grad_norm": 1.8575050830841064,
        "learning_rate": 0.00015326233287024597,
        "epoch": 0.11014612627515853,
        "step": 799
    },
    {
        "loss": 2.0288,
        "grad_norm": 1.8094675540924072,
        "learning_rate": 0.00015315194480106526,
        "epoch": 0.11028398125172319,
        "step": 800
    },
    {
        "loss": 1.6596,
        "grad_norm": 2.7194252014160156,
        "learning_rate": 0.00015304146638672194,
        "epoch": 0.11042183622828784,
        "step": 801
    },
    {
        "loss": 2.3428,
        "grad_norm": 1.1772730350494385,
        "learning_rate": 0.0001529308978150021,
        "epoch": 0.1105596912048525,
        "step": 802
    },
    {
        "loss": 2.3194,
        "grad_norm": 0.9829029440879822,
        "learning_rate": 0.00015282023927384492,
        "epoch": 0.11069754618141715,
        "step": 803
    },
    {
        "loss": 1.2463,
        "grad_norm": 2.122861623764038,
        "learning_rate": 0.00015270949095134255,
        "epoch": 0.11083540115798181,
        "step": 804
    },
    {
        "loss": 1.9906,
        "grad_norm": 2.0135982036590576,
        "learning_rate": 0.00015259865303573981,
        "epoch": 0.11097325613454646,
        "step": 805
    },
    {
        "loss": 2.23,
        "grad_norm": 1.3962516784667969,
        "learning_rate": 0.00015248772571543372,
        "epoch": 0.1111111111111111,
        "step": 806
    },
    {
        "loss": 1.4701,
        "grad_norm": 2.4166176319122314,
        "learning_rate": 0.00015237670917897324,
        "epoch": 0.11124896608767576,
        "step": 807
    },
    {
        "loss": 2.3605,
        "grad_norm": 1.520262360572815,
        "learning_rate": 0.0001522656036150591,
        "epoch": 0.11138682106424042,
        "step": 808
    },
    {
        "loss": 2.3383,
        "grad_norm": 1.4303476810455322,
        "learning_rate": 0.00015215440921254324,
        "epoch": 0.11152467604080507,
        "step": 809
    },
    {
        "loss": 2.5172,
        "grad_norm": 1.0618952512741089,
        "learning_rate": 0.0001520431261604287,
        "epoch": 0.11166253101736973,
        "step": 810
    },
    {
        "loss": 2.2587,
        "grad_norm": 1.0571839809417725,
        "learning_rate": 0.00015193175464786908,
        "epoch": 0.11180038599393438,
        "step": 811
    },
    {
        "loss": 2.0426,
        "grad_norm": 1.4090338945388794,
        "learning_rate": 0.00015182029486416845,
        "epoch": 0.11193824097049904,
        "step": 812
    },
    {
        "loss": 1.9232,
        "grad_norm": 1.7367348670959473,
        "learning_rate": 0.0001517087469987809,
        "epoch": 0.11207609594706369,
        "step": 813
    },
    {
        "loss": 2.1916,
        "grad_norm": 2.0023601055145264,
        "learning_rate": 0.00015159711124131017,
        "epoch": 0.11221395092362835,
        "step": 814
    },
    {
        "loss": 2.0597,
        "grad_norm": 1.836805820465088,
        "learning_rate": 0.00015148538778150946,
        "epoch": 0.112351805900193,
        "step": 815
    },
    {
        "loss": 1.8471,
        "grad_norm": 1.9446992874145508,
        "learning_rate": 0.00015137357680928106,
        "epoch": 0.11248966087675766,
        "step": 816
    },
    {
        "loss": 1.4845,
        "grad_norm": 1.6547367572784424,
        "learning_rate": 0.00015126167851467592,
        "epoch": 0.1126275158533223,
        "step": 817
    },
    {
        "loss": 1.7781,
        "grad_norm": 1.5513806343078613,
        "learning_rate": 0.0001511496930878935,
        "epoch": 0.11276537082988695,
        "step": 818
    },
    {
        "loss": 1.9762,
        "grad_norm": 2.601557970046997,
        "learning_rate": 0.00015103762071928136,
        "epoch": 0.11290322580645161,
        "step": 819
    },
    {
        "loss": 2.0407,
        "grad_norm": 1.308939814567566,
        "learning_rate": 0.00015092546159933473,
        "epoch": 0.11304108078301627,
        "step": 820
    },
    {
        "loss": 2.4403,
        "grad_norm": 1.6223911046981812,
        "learning_rate": 0.00015081321591869644,
        "epoch": 0.11317893575958092,
        "step": 821
    },
    {
        "loss": 2.3902,
        "grad_norm": 1.145351767539978,
        "learning_rate": 0.00015070088386815643,
        "epoch": 0.11331679073614558,
        "step": 822
    },
    {
        "loss": 2.0734,
        "grad_norm": 2.0002284049987793,
        "learning_rate": 0.00015058846563865128,
        "epoch": 0.11345464571271023,
        "step": 823
    },
    {
        "loss": 2.364,
        "grad_norm": 1.7710468769073486,
        "learning_rate": 0.0001504759614212643,
        "epoch": 0.11359250068927489,
        "step": 824
    },
    {
        "loss": 2.2572,
        "grad_norm": 1.7072341442108154,
        "learning_rate": 0.00015036337140722478,
        "epoch": 0.11373035566583954,
        "step": 825
    },
    {
        "loss": 2.1624,
        "grad_norm": 1.7981266975402832,
        "learning_rate": 0.00015025069578790794,
        "epoch": 0.1138682106424042,
        "step": 826
    },
    {
        "loss": 1.807,
        "grad_norm": 1.5883996486663818,
        "learning_rate": 0.00015013793475483447,
        "epoch": 0.11400606561896884,
        "step": 827
    },
    {
        "loss": 2.3851,
        "grad_norm": 1.208885669708252,
        "learning_rate": 0.00015002508849967016,
        "epoch": 0.1141439205955335,
        "step": 828
    },
    {
        "loss": 2.6082,
        "grad_norm": 0.9542651176452637,
        "learning_rate": 0.00014991215721422583,
        "epoch": 0.11428177557209815,
        "step": 829
    },
    {
        "loss": 2.3558,
        "grad_norm": 1.6191812753677368,
        "learning_rate": 0.00014979914109045672,
        "epoch": 0.1144196305486628,
        "step": 830
    },
    {
        "loss": 1.988,
        "grad_norm": 1.143902063369751,
        "learning_rate": 0.00014968604032046228,
        "epoch": 0.11455748552522746,
        "step": 831
    },
    {
        "loss": 2.2651,
        "grad_norm": 1.0950068235397339,
        "learning_rate": 0.00014957285509648583,
        "epoch": 0.11469534050179211,
        "step": 832
    },
    {
        "loss": 2.4075,
        "grad_norm": 1.5468934774398804,
        "learning_rate": 0.00014945958561091431,
        "epoch": 0.11483319547835677,
        "step": 833
    },
    {
        "loss": 2.1915,
        "grad_norm": 1.503649115562439,
        "learning_rate": 0.00014934623205627783,
        "epoch": 0.11497105045492143,
        "step": 834
    },
    {
        "loss": 2.3083,
        "grad_norm": 1.7129404544830322,
        "learning_rate": 0.00014923279462524935,
        "epoch": 0.11510890543148608,
        "step": 835
    },
    {
        "loss": 1.6472,
        "grad_norm": 1.2788039445877075,
        "learning_rate": 0.0001491192735106445,
        "epoch": 0.11524676040805074,
        "step": 836
    },
    {
        "loss": 2.4327,
        "grad_norm": 1.3878103494644165,
        "learning_rate": 0.00014900566890542106,
        "epoch": 0.11538461538461539,
        "step": 837
    },
    {
        "loss": 2.4467,
        "grad_norm": 2.0342891216278076,
        "learning_rate": 0.00014889198100267884,
        "epoch": 0.11552247036118003,
        "step": 838
    },
    {
        "loss": 2.2592,
        "grad_norm": 1.5232094526290894,
        "learning_rate": 0.0001487782099956591,
        "epoch": 0.11566032533774469,
        "step": 839
    },
    {
        "loss": 2.2789,
        "grad_norm": 1.8846546411514282,
        "learning_rate": 0.00014866435607774442,
        "epoch": 0.11579818031430934,
        "step": 840
    },
    {
        "loss": 2.3356,
        "grad_norm": 1.3604209423065186,
        "learning_rate": 0.00014855041944245834,
        "epoch": 0.115936035290874,
        "step": 841
    },
    {
        "loss": 1.7548,
        "grad_norm": 1.4024475812911987,
        "learning_rate": 0.00014843640028346493,
        "epoch": 0.11607389026743865,
        "step": 842
    },
    {
        "loss": 2.2425,
        "grad_norm": 1.1862099170684814,
        "learning_rate": 0.0001483222987945686,
        "epoch": 0.11621174524400331,
        "step": 843
    },
    {
        "loss": 0.9413,
        "grad_norm": 1.6501580476760864,
        "learning_rate": 0.00014820811516971362,
        "epoch": 0.11634960022056796,
        "step": 844
    },
    {
        "loss": 2.1304,
        "grad_norm": 1.117775321006775,
        "learning_rate": 0.00014809384960298396,
        "epoch": 0.11648745519713262,
        "step": 845
    },
    {
        "loss": 2.2733,
        "grad_norm": 1.5663233995437622,
        "learning_rate": 0.00014797950228860281,
        "epoch": 0.11662531017369727,
        "step": 846
    },
    {
        "loss": 1.4642,
        "grad_norm": 1.7273961305618286,
        "learning_rate": 0.00014786507342093232,
        "epoch": 0.11676316515026193,
        "step": 847
    },
    {
        "loss": 1.549,
        "grad_norm": 1.955764651298523,
        "learning_rate": 0.0001477505631944733,
        "epoch": 0.11690102012682659,
        "step": 848
    },
    {
        "loss": 2.009,
        "grad_norm": 1.9841783046722412,
        "learning_rate": 0.00014763597180386478,
        "epoch": 0.11703887510339123,
        "step": 849
    },
    {
        "loss": 2.2756,
        "grad_norm": 1.7575007677078247,
        "learning_rate": 0.00014752129944388376,
        "epoch": 0.11717673007995588,
        "step": 850
    },
    {
        "loss": 2.1799,
        "grad_norm": 2.0539205074310303,
        "learning_rate": 0.000147406546309445,
        "epoch": 0.11731458505652054,
        "step": 851
    },
    {
        "loss": 2.5397,
        "grad_norm": 1.0311328172683716,
        "learning_rate": 0.00014729171259560036,
        "epoch": 0.11745244003308519,
        "step": 852
    },
    {
        "loss": 2.232,
        "grad_norm": 1.5533573627471924,
        "learning_rate": 0.00014717679849753882,
        "epoch": 0.11759029500964985,
        "step": 853
    },
    {
        "loss": 2.1536,
        "grad_norm": 1.68128502368927,
        "learning_rate": 0.00014706180421058588,
        "epoch": 0.1177281499862145,
        "step": 854
    },
    {
        "loss": 2.1274,
        "grad_norm": 1.6497409343719482,
        "learning_rate": 0.0001469467299302034,
        "epoch": 0.11786600496277916,
        "step": 855
    },
    {
        "loss": 2.2825,
        "grad_norm": 1.4378671646118164,
        "learning_rate": 0.00014683157585198924,
        "epoch": 0.11800385993934381,
        "step": 856
    },
    {
        "loss": 2.2374,
        "grad_norm": 1.2669687271118164,
        "learning_rate": 0.0001467163421716768,
        "epoch": 0.11814171491590847,
        "step": 857
    },
    {
        "loss": 2.3745,
        "grad_norm": 1.0759106874465942,
        "learning_rate": 0.00014660102908513486,
        "epoch": 0.11827956989247312,
        "step": 858
    },
    {
        "loss": 1.9871,
        "grad_norm": 1.5708998441696167,
        "learning_rate": 0.00014648563678836717,
        "epoch": 0.11841742486903777,
        "step": 859
    },
    {
        "loss": 2.5561,
        "grad_norm": 1.9148789644241333,
        "learning_rate": 0.0001463701654775121,
        "epoch": 0.11855527984560242,
        "step": 860
    },
    {
        "loss": 2.2646,
        "grad_norm": 1.1717585325241089,
        "learning_rate": 0.00014625461534884231,
        "epoch": 0.11869313482216708,
        "step": 861
    },
    {
        "loss": 2.1015,
        "grad_norm": 1.7387856245040894,
        "learning_rate": 0.00014613898659876444,
        "epoch": 0.11883098979873173,
        "step": 862
    },
    {
        "loss": 2.5418,
        "grad_norm": 1.1782069206237793,
        "learning_rate": 0.00014602327942381878,
        "epoch": 0.11896884477529639,
        "step": 863
    },
    {
        "loss": 2.0076,
        "grad_norm": 1.77373468875885,
        "learning_rate": 0.0001459074940206789,
        "epoch": 0.11910669975186104,
        "step": 864
    },
    {
        "loss": 2.3088,
        "grad_norm": 1.5081108808517456,
        "learning_rate": 0.00014579163058615142,
        "epoch": 0.1192445547284257,
        "step": 865
    },
    {
        "loss": 2.4485,
        "grad_norm": 1.1767282485961914,
        "learning_rate": 0.00014567568931717547,
        "epoch": 0.11938240970499035,
        "step": 866
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.5732895135879517,
        "learning_rate": 0.00014555967041082253,
        "epoch": 0.11952026468155501,
        "step": 867
    },
    {
        "loss": 1.8788,
        "grad_norm": 1.586212158203125,
        "learning_rate": 0.00014544357406429606,
        "epoch": 0.11965811965811966,
        "step": 868
    },
    {
        "loss": 2.329,
        "grad_norm": 1.6585334539413452,
        "learning_rate": 0.00014532740047493113,
        "epoch": 0.11979597463468432,
        "step": 869
    },
    {
        "loss": 1.1397,
        "grad_norm": 1.726084589958191,
        "learning_rate": 0.00014521114984019414,
        "epoch": 0.11993382961124896,
        "step": 870
    },
    {
        "loss": 2.1331,
        "grad_norm": 1.3395589590072632,
        "learning_rate": 0.0001450948223576824,
        "epoch": 0.12007168458781362,
        "step": 871
    },
    {
        "loss": 2.2823,
        "grad_norm": 1.3657543659210205,
        "learning_rate": 0.0001449784182251239,
        "epoch": 0.12020953956437827,
        "step": 872
    },
    {
        "loss": 1.639,
        "grad_norm": 2.1347434520721436,
        "learning_rate": 0.0001448619376403769,
        "epoch": 0.12034739454094293,
        "step": 873
    },
    {
        "loss": 2.342,
        "grad_norm": 0.9947338700294495,
        "learning_rate": 0.00014474538080142948,
        "epoch": 0.12048524951750758,
        "step": 874
    },
    {
        "loss": 2.1387,
        "grad_norm": 1.0930685997009277,
        "learning_rate": 0.00014462874790639952,
        "epoch": 0.12062310449407224,
        "step": 875
    },
    {
        "loss": 2.2324,
        "grad_norm": 1.5906716585159302,
        "learning_rate": 0.0001445120391535341,
        "epoch": 0.12076095947063689,
        "step": 876
    },
    {
        "loss": 1.9037,
        "grad_norm": 2.062089204788208,
        "learning_rate": 0.00014439525474120924,
        "epoch": 0.12089881444720155,
        "step": 877
    },
    {
        "loss": 1.9529,
        "grad_norm": 1.3527051210403442,
        "learning_rate": 0.00014427839486792955,
        "epoch": 0.1210366694237662,
        "step": 878
    },
    {
        "loss": 2.5468,
        "grad_norm": 1.2410314083099365,
        "learning_rate": 0.0001441614597323279,
        "epoch": 0.12117452440033086,
        "step": 879
    },
    {
        "loss": 1.479,
        "grad_norm": 2.1855404376983643,
        "learning_rate": 0.00014404444953316513,
        "epoch": 0.12131237937689551,
        "step": 880
    },
    {
        "loss": 1.2519,
        "grad_norm": 2.4092020988464355,
        "learning_rate": 0.00014392736446932966,
        "epoch": 0.12145023435346015,
        "step": 881
    },
    {
        "loss": 2.4523,
        "grad_norm": 1.8032188415527344,
        "learning_rate": 0.00014381020473983714,
        "epoch": 0.12158808933002481,
        "step": 882
    },
    {
        "loss": 2.2898,
        "grad_norm": 1.5216232538223267,
        "learning_rate": 0.00014369297054383007,
        "epoch": 0.12172594430658946,
        "step": 883
    },
    {
        "loss": 1.7813,
        "grad_norm": 1.69392728805542,
        "learning_rate": 0.00014357566208057768,
        "epoch": 0.12186379928315412,
        "step": 884
    },
    {
        "loss": 2.3253,
        "grad_norm": 1.798345685005188,
        "learning_rate": 0.0001434582795494753,
        "epoch": 0.12200165425971878,
        "step": 885
    },
    {
        "loss": 2.3791,
        "grad_norm": 1.202338695526123,
        "learning_rate": 0.00014334082315004421,
        "epoch": 0.12213950923628343,
        "step": 886
    },
    {
        "loss": 2.0074,
        "grad_norm": 2.3369579315185547,
        "learning_rate": 0.00014322329308193125,
        "epoch": 0.12227736421284809,
        "step": 887
    },
    {
        "loss": 2.0967,
        "grad_norm": 1.2482001781463623,
        "learning_rate": 0.00014310568954490848,
        "epoch": 0.12241521918941274,
        "step": 888
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.5472593307495117,
        "learning_rate": 0.00014298801273887284,
        "epoch": 0.1225530741659774,
        "step": 889
    },
    {
        "loss": 2.059,
        "grad_norm": 1.967108964920044,
        "learning_rate": 0.00014287026286384578,
        "epoch": 0.12269092914254205,
        "step": 890
    },
    {
        "loss": 1.5832,
        "grad_norm": 1.43135404586792,
        "learning_rate": 0.00014275244011997297,
        "epoch": 0.1228287841191067,
        "step": 891
    },
    {
        "loss": 2.1097,
        "grad_norm": 2.022263526916504,
        "learning_rate": 0.00014263454470752398,
        "epoch": 0.12296663909567135,
        "step": 892
    },
    {
        "loss": 2.1809,
        "grad_norm": 1.3847815990447998,
        "learning_rate": 0.00014251657682689178,
        "epoch": 0.123104494072236,
        "step": 893
    },
    {
        "loss": 1.9758,
        "grad_norm": 1.2200547456741333,
        "learning_rate": 0.00014239853667859267,
        "epoch": 0.12324234904880066,
        "step": 894
    },
    {
        "loss": 1.3995,
        "grad_norm": 1.718182921409607,
        "learning_rate": 0.0001422804244632657,
        "epoch": 0.12338020402536531,
        "step": 895
    },
    {
        "loss": 1.9488,
        "grad_norm": 1.5000022649765015,
        "learning_rate": 0.0001421622403816724,
        "epoch": 0.12351805900192997,
        "step": 896
    },
    {
        "loss": 1.45,
        "grad_norm": 1.8045402765274048,
        "learning_rate": 0.00014204398463469643,
        "epoch": 0.12365591397849462,
        "step": 897
    },
    {
        "loss": 2.4994,
        "grad_norm": 1.4875531196594238,
        "learning_rate": 0.00014192565742334346,
        "epoch": 0.12379376895505928,
        "step": 898
    },
    {
        "loss": 1.8776,
        "grad_norm": 2.2212700843811035,
        "learning_rate": 0.00014180725894874039,
        "epoch": 0.12393162393162394,
        "step": 899
    },
    {
        "loss": 2.1119,
        "grad_norm": 1.5472559928894043,
        "learning_rate": 0.00014168878941213538,
        "epoch": 0.12406947890818859,
        "step": 900
    },
    {
        "loss": 1.9758,
        "grad_norm": 2.02543568611145,
        "learning_rate": 0.0001415702490148973,
        "epoch": 0.12420733388475325,
        "step": 901
    },
    {
        "loss": 2.5115,
        "grad_norm": 1.5574915409088135,
        "learning_rate": 0.00014145163795851556,
        "epoch": 0.12434518886131789,
        "step": 902
    },
    {
        "loss": 2.1403,
        "grad_norm": 1.416927456855774,
        "learning_rate": 0.00014133295644459964,
        "epoch": 0.12448304383788254,
        "step": 903
    },
    {
        "loss": 2.3423,
        "grad_norm": 1.7343426942825317,
        "learning_rate": 0.0001412142046748787,
        "epoch": 0.1246208988144472,
        "step": 904
    },
    {
        "loss": 1.8518,
        "grad_norm": 1.1188404560089111,
        "learning_rate": 0.00014109538285120144,
        "epoch": 0.12475875379101185,
        "step": 905
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.5067626237869263,
        "learning_rate": 0.0001409764911755356,
        "epoch": 0.12489660876757651,
        "step": 906
    },
    {
        "loss": 1.9287,
        "grad_norm": 1.6139243841171265,
        "learning_rate": 0.00014085752984996757,
        "epoch": 0.12503446374414115,
        "step": 907
    },
    {
        "loss": 1.5363,
        "grad_norm": 1.4273791313171387,
        "learning_rate": 0.00014073849907670217,
        "epoch": 0.1251723187207058,
        "step": 908
    },
    {
        "loss": 2.6931,
        "grad_norm": 1.0431593656539917,
        "learning_rate": 0.00014061939905806234,
        "epoch": 0.12531017369727046,
        "step": 909
    },
    {
        "loss": 2.34,
        "grad_norm": 0.9458901286125183,
        "learning_rate": 0.0001405002299964886,
        "epoch": 0.12544802867383512,
        "step": 910
    },
    {
        "loss": 1.8764,
        "grad_norm": 1.9079880714416504,
        "learning_rate": 0.00014038099209453896,
        "epoch": 0.12558588365039977,
        "step": 911
    },
    {
        "loss": 2.0277,
        "grad_norm": 1.757543683052063,
        "learning_rate": 0.00014026168555488833,
        "epoch": 0.12572373862696443,
        "step": 912
    },
    {
        "loss": 1.6524,
        "grad_norm": 1.65325129032135,
        "learning_rate": 0.00014014231058032833,
        "epoch": 0.12586159360352908,
        "step": 913
    },
    {
        "loss": 2.3164,
        "grad_norm": 1.5285882949829102,
        "learning_rate": 0.0001400228673737669,
        "epoch": 0.12599944858009374,
        "step": 914
    },
    {
        "loss": 2.4926,
        "grad_norm": 1.3978420495986938,
        "learning_rate": 0.0001399033561382279,
        "epoch": 0.1261373035566584,
        "step": 915
    },
    {
        "loss": 2.2422,
        "grad_norm": 1.3920905590057373,
        "learning_rate": 0.00013978377707685096,
        "epoch": 0.12627515853322305,
        "step": 916
    },
    {
        "loss": 1.8984,
        "grad_norm": 1.5258837938308716,
        "learning_rate": 0.00013966413039289086,
        "epoch": 0.1264130135097877,
        "step": 917
    },
    {
        "loss": 2.3549,
        "grad_norm": 1.4619110822677612,
        "learning_rate": 0.0001395444162897174,
        "epoch": 0.12655086848635236,
        "step": 918
    },
    {
        "loss": 2.0893,
        "grad_norm": 1.4881813526153564,
        "learning_rate": 0.00013942463497081494,
        "epoch": 0.126688723462917,
        "step": 919
    },
    {
        "loss": 2.0743,
        "grad_norm": 1.3317205905914307,
        "learning_rate": 0.00013930478663978213,
        "epoch": 0.12682657843948167,
        "step": 920
    },
    {
        "loss": 1.568,
        "grad_norm": 1.528301477432251,
        "learning_rate": 0.00013918487150033147,
        "epoch": 0.12696443341604632,
        "step": 921
    },
    {
        "loss": 2.2516,
        "grad_norm": 1.9629491567611694,
        "learning_rate": 0.000139064889756289,
        "epoch": 0.12710228839261098,
        "step": 922
    },
    {
        "loss": 2.0585,
        "grad_norm": 1.5022307634353638,
        "learning_rate": 0.0001389448416115941,
        "epoch": 0.12724014336917563,
        "step": 923
    },
    {
        "loss": 2.214,
        "grad_norm": 1.32772696018219,
        "learning_rate": 0.0001388247272702989,
        "epoch": 0.1273779983457403,
        "step": 924
    },
    {
        "loss": 2.1894,
        "grad_norm": 1.4707293510437012,
        "learning_rate": 0.00013870454693656808,
        "epoch": 0.12751585332230495,
        "step": 925
    },
    {
        "loss": 2.4027,
        "grad_norm": 1.1121101379394531,
        "learning_rate": 0.00013858430081467845,
        "epoch": 0.1276537082988696,
        "step": 926
    },
    {
        "loss": 2.2762,
        "grad_norm": 1.08271324634552,
        "learning_rate": 0.00013846398910901872,
        "epoch": 0.12779156327543426,
        "step": 927
    },
    {
        "loss": 2.6461,
        "grad_norm": 1.2409305572509766,
        "learning_rate": 0.00013834361202408904,
        "epoch": 0.1279294182519989,
        "step": 928
    },
    {
        "loss": 1.9104,
        "grad_norm": 1.3891830444335938,
        "learning_rate": 0.00013822316976450067,
        "epoch": 0.12806727322856354,
        "step": 929
    },
    {
        "loss": 2.5651,
        "grad_norm": 1.805629849433899,
        "learning_rate": 0.00013810266253497572,
        "epoch": 0.1282051282051282,
        "step": 930
    },
    {
        "loss": 2.6928,
        "grad_norm": 1.6093189716339111,
        "learning_rate": 0.00013798209054034658,
        "epoch": 0.12834298318169285,
        "step": 931
    },
    {
        "loss": 2.0965,
        "grad_norm": 1.51455819606781,
        "learning_rate": 0.00013786145398555588,
        "epoch": 0.1284808381582575,
        "step": 932
    },
    {
        "loss": 2.0397,
        "grad_norm": 1.380837321281433,
        "learning_rate": 0.00013774075307565597,
        "epoch": 0.12861869313482216,
        "step": 933
    },
    {
        "loss": 2.1544,
        "grad_norm": 1.2012871503829956,
        "learning_rate": 0.00013761998801580847,
        "epoch": 0.12875654811138681,
        "step": 934
    },
    {
        "loss": 2.3824,
        "grad_norm": 1.2626396417617798,
        "learning_rate": 0.0001374991590112842,
        "epoch": 0.12889440308795147,
        "step": 935
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.335259199142456,
        "learning_rate": 0.00013737826626746252,
        "epoch": 0.12903225806451613,
        "step": 936
    },
    {
        "loss": 1.0407,
        "grad_norm": 2.801471471786499,
        "learning_rate": 0.00013725730998983125,
        "epoch": 0.12917011304108078,
        "step": 937
    },
    {
        "loss": 2.0915,
        "grad_norm": 1.9979734420776367,
        "learning_rate": 0.0001371362903839861,
        "epoch": 0.12930796801764544,
        "step": 938
    },
    {
        "loss": 2.3856,
        "grad_norm": 1.1784404516220093,
        "learning_rate": 0.00013701520765563055,
        "epoch": 0.1294458229942101,
        "step": 939
    },
    {
        "loss": 0.8961,
        "grad_norm": 1.4381054639816284,
        "learning_rate": 0.0001368940620105752,
        "epoch": 0.12958367797077475,
        "step": 940
    },
    {
        "loss": 1.6652,
        "grad_norm": 1.8396023511886597,
        "learning_rate": 0.0001367728536547378,
        "epoch": 0.1297215329473394,
        "step": 941
    },
    {
        "loss": 2.096,
        "grad_norm": 1.326738715171814,
        "learning_rate": 0.00013665158279414248,
        "epoch": 0.12985938792390406,
        "step": 942
    },
    {
        "loss": 1.9898,
        "grad_norm": 1.9545270204544067,
        "learning_rate": 0.0001365302496349198,
        "epoch": 0.1299972429004687,
        "step": 943
    },
    {
        "loss": 2.1653,
        "grad_norm": 1.891243815422058,
        "learning_rate": 0.00013640885438330605,
        "epoch": 0.13013509787703337,
        "step": 944
    },
    {
        "loss": 2.1467,
        "grad_norm": 1.0620887279510498,
        "learning_rate": 0.00013628739724564314,
        "epoch": 0.13027295285359802,
        "step": 945
    },
    {
        "loss": 2.071,
        "grad_norm": 1.5461645126342773,
        "learning_rate": 0.00013616587842837826,
        "epoch": 0.13041080783016268,
        "step": 946
    },
    {
        "loss": 2.444,
        "grad_norm": 1.1899582147598267,
        "learning_rate": 0.00013604429813806327,
        "epoch": 0.13054866280672733,
        "step": 947
    },
    {
        "loss": 2.204,
        "grad_norm": 1.7328321933746338,
        "learning_rate": 0.00013592265658135462,
        "epoch": 0.130686517783292,
        "step": 948
    },
    {
        "loss": 2.0512,
        "grad_norm": 1.2669036388397217,
        "learning_rate": 0.0001358009539650129,
        "epoch": 0.13082437275985664,
        "step": 949
    },
    {
        "loss": 2.0482,
        "grad_norm": 2.0454654693603516,
        "learning_rate": 0.0001356791904959024,
        "epoch": 0.13096222773642127,
        "step": 950
    },
    {
        "loss": 2.4102,
        "grad_norm": 1.2380006313323975,
        "learning_rate": 0.00013555736638099094,
        "epoch": 0.13110008271298593,
        "step": 951
    },
    {
        "loss": 2.4261,
        "grad_norm": 1.320400595664978,
        "learning_rate": 0.00013543548182734947,
        "epoch": 0.13123793768955058,
        "step": 952
    },
    {
        "loss": 2.5756,
        "grad_norm": 1.2137361764907837,
        "learning_rate": 0.00013531353704215144,
        "epoch": 0.13137579266611524,
        "step": 953
    },
    {
        "loss": 1.2947,
        "grad_norm": 1.8447680473327637,
        "learning_rate": 0.00013519153223267295,
        "epoch": 0.1315136476426799,
        "step": 954
    },
    {
        "loss": 1.9787,
        "grad_norm": 1.6559486389160156,
        "learning_rate": 0.00013506946760629196,
        "epoch": 0.13165150261924455,
        "step": 955
    },
    {
        "loss": 2.2355,
        "grad_norm": 1.0868737697601318,
        "learning_rate": 0.00013494734337048814,
        "epoch": 0.1317893575958092,
        "step": 956
    },
    {
        "loss": 1.9056,
        "grad_norm": 2.2085912227630615,
        "learning_rate": 0.0001348251597328425,
        "epoch": 0.13192721257237386,
        "step": 957
    },
    {
        "loss": 1.8868,
        "grad_norm": 1.9569982290267944,
        "learning_rate": 0.00013470291690103706,
        "epoch": 0.13206506754893851,
        "step": 958
    },
    {
        "loss": 2.0414,
        "grad_norm": 1.6914799213409424,
        "learning_rate": 0.0001345806150828543,
        "epoch": 0.13220292252550317,
        "step": 959
    },
    {
        "loss": 1.6771,
        "grad_norm": 1.703554391860962,
        "learning_rate": 0.0001344582544861772,
        "epoch": 0.13234077750206782,
        "step": 960
    },
    {
        "loss": 2.3552,
        "grad_norm": 1.335350513458252,
        "learning_rate": 0.00013433583531898837,
        "epoch": 0.13247863247863248,
        "step": 961
    },
    {
        "loss": 1.6218,
        "grad_norm": 2.5153582096099854,
        "learning_rate": 0.00013421335778937026,
        "epoch": 0.13261648745519714,
        "step": 962
    },
    {
        "loss": 1.9907,
        "grad_norm": 1.2905919551849365,
        "learning_rate": 0.0001340908221055043,
        "epoch": 0.1327543424317618,
        "step": 963
    },
    {
        "loss": 2.1726,
        "grad_norm": 1.0884184837341309,
        "learning_rate": 0.00013396822847567085,
        "epoch": 0.13289219740832645,
        "step": 964
    },
    {
        "loss": 2.4178,
        "grad_norm": 2.1160197257995605,
        "learning_rate": 0.0001338455771082488,
        "epoch": 0.1330300523848911,
        "step": 965
    },
    {
        "loss": 1.3266,
        "grad_norm": 2.3088018894195557,
        "learning_rate": 0.00013372286821171516,
        "epoch": 0.13316790736145576,
        "step": 966
    },
    {
        "loss": 2.401,
        "grad_norm": 1.5762603282928467,
        "learning_rate": 0.00013360010199464466,
        "epoch": 0.1333057623380204,
        "step": 967
    },
    {
        "loss": 2.1971,
        "grad_norm": 0.9927732348442078,
        "learning_rate": 0.00013347727866570952,
        "epoch": 0.13344361731458507,
        "step": 968
    },
    {
        "loss": 2.2296,
        "grad_norm": 1.7563161849975586,
        "learning_rate": 0.00013335439843367909,
        "epoch": 0.13358147229114972,
        "step": 969
    },
    {
        "loss": 2.0739,
        "grad_norm": 1.314699649810791,
        "learning_rate": 0.0001332314615074193,
        "epoch": 0.13371932726771438,
        "step": 970
    },
    {
        "loss": 2.5449,
        "grad_norm": 1.4999569654464722,
        "learning_rate": 0.00013310846809589253,
        "epoch": 0.133857182244279,
        "step": 971
    },
    {
        "loss": 1.5259,
        "grad_norm": 2.5195863246917725,
        "learning_rate": 0.0001329854184081572,
        "epoch": 0.13399503722084366,
        "step": 972
    },
    {
        "loss": 1.8164,
        "grad_norm": 1.6306846141815186,
        "learning_rate": 0.00013286231265336734,
        "epoch": 0.13413289219740832,
        "step": 973
    },
    {
        "loss": 2.1072,
        "grad_norm": 1.3711458444595337,
        "learning_rate": 0.0001327391510407723,
        "epoch": 0.13427074717397297,
        "step": 974
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.5321996212005615,
        "learning_rate": 0.00013261593377971638,
        "epoch": 0.13440860215053763,
        "step": 975
    },
    {
        "loss": 2.1427,
        "grad_norm": 2.4537625312805176,
        "learning_rate": 0.00013249266107963838,
        "epoch": 0.13454645712710228,
        "step": 976
    },
    {
        "loss": 2.1683,
        "grad_norm": 1.5504406690597534,
        "learning_rate": 0.0001323693331500715,
        "epoch": 0.13468431210366694,
        "step": 977
    },
    {
        "loss": 2.2485,
        "grad_norm": 0.9008718132972717,
        "learning_rate": 0.00013224595020064263,
        "epoch": 0.1348221670802316,
        "step": 978
    },
    {
        "loss": 1.6045,
        "grad_norm": 2.0029048919677734,
        "learning_rate": 0.0001321225124410724,
        "epoch": 0.13496002205679625,
        "step": 979
    },
    {
        "loss": 1.7474,
        "grad_norm": 1.217677354812622,
        "learning_rate": 0.00013199902008117435,
        "epoch": 0.1350978770333609,
        "step": 980
    },
    {
        "loss": 2.193,
        "grad_norm": 2.0188369750976562,
        "learning_rate": 0.00013187547333085503,
        "epoch": 0.13523573200992556,
        "step": 981
    },
    {
        "loss": 1.4378,
        "grad_norm": 2.40110445022583,
        "learning_rate": 0.0001317518724001133,
        "epoch": 0.1353735869864902,
        "step": 982
    },
    {
        "loss": 2.6622,
        "grad_norm": 1.068299412727356,
        "learning_rate": 0.00013162821749904024,
        "epoch": 0.13551144196305487,
        "step": 983
    },
    {
        "loss": 2.282,
        "grad_norm": 1.2598254680633545,
        "learning_rate": 0.0001315045088378186,
        "epoch": 0.13564929693961952,
        "step": 984
    },
    {
        "loss": 1.9249,
        "grad_norm": 1.1871840953826904,
        "learning_rate": 0.00013138074662672248,
        "epoch": 0.13578715191618418,
        "step": 985
    },
    {
        "loss": 1.8973,
        "grad_norm": 1.8234683275222778,
        "learning_rate": 0.000131256931076117,
        "epoch": 0.13592500689274883,
        "step": 986
    },
    {
        "loss": 2.1659,
        "grad_norm": 1.584346055984497,
        "learning_rate": 0.00013113306239645808,
        "epoch": 0.1360628618693135,
        "step": 987
    },
    {
        "loss": 2.2933,
        "grad_norm": 1.4197381734848022,
        "learning_rate": 0.0001310091407982918,
        "epoch": 0.13620071684587814,
        "step": 988
    },
    {
        "loss": 2.0716,
        "grad_norm": 1.529697299003601,
        "learning_rate": 0.00013088516649225416,
        "epoch": 0.1363385718224428,
        "step": 989
    },
    {
        "loss": 2.0847,
        "grad_norm": 1.8605355024337769,
        "learning_rate": 0.000130761139689071,
        "epoch": 0.13647642679900746,
        "step": 990
    },
    {
        "loss": 1.7449,
        "grad_norm": 1.808803677558899,
        "learning_rate": 0.000130637060599557,
        "epoch": 0.1366142817755721,
        "step": 991
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.8145936727523804,
        "learning_rate": 0.00013051292943461608,
        "epoch": 0.13675213675213677,
        "step": 992
    },
    {
        "loss": 2.0424,
        "grad_norm": 1.2692099809646606,
        "learning_rate": 0.0001303887464052405,
        "epoch": 0.1368899917287014,
        "step": 993
    },
    {
        "loss": 1.7911,
        "grad_norm": 1.9947586059570312,
        "learning_rate": 0.00013026451172251064,
        "epoch": 0.13702784670526605,
        "step": 994
    },
    {
        "loss": 2.4069,
        "grad_norm": 1.676082968711853,
        "learning_rate": 0.00013014022559759477,
        "epoch": 0.1371657016818307,
        "step": 995
    },
    {
        "loss": 2.2962,
        "grad_norm": 1.947097659111023,
        "learning_rate": 0.0001300158882417486,
        "epoch": 0.13730355665839536,
        "step": 996
    },
    {
        "loss": 2.1227,
        "grad_norm": 1.4818533658981323,
        "learning_rate": 0.00012989149986631485,
        "epoch": 0.13744141163496001,
        "step": 997
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.0593019723892212,
        "learning_rate": 0.0001297670606827231,
        "epoch": 0.13757926661152467,
        "step": 998
    },
    {
        "loss": 2.3715,
        "grad_norm": 1.0374467372894287,
        "learning_rate": 0.00012964257090248896,
        "epoch": 0.13771712158808933,
        "step": 999
    },
    {
        "loss": 2.0565,
        "grad_norm": 1.1360658407211304,
        "learning_rate": 0.00012951803073721452,
        "epoch": 0.13785497656465398,
        "step": 1000
    },
    {
        "loss": 2.3307,
        "grad_norm": 1.505651593208313,
        "learning_rate": 0.00012939344039858715,
        "epoch": 0.13799283154121864,
        "step": 1001
    },
    {
        "loss": 2.4406,
        "grad_norm": 1.6445749998092651,
        "learning_rate": 0.0001292688000983796,
        "epoch": 0.1381306865177833,
        "step": 1002
    },
    {
        "loss": 1.4762,
        "grad_norm": 1.698536992073059,
        "learning_rate": 0.00012914411004844958,
        "epoch": 0.13826854149434795,
        "step": 1003
    },
    {
        "loss": 2.3897,
        "grad_norm": 0.911652147769928,
        "learning_rate": 0.00012901937046073933,
        "epoch": 0.1384063964709126,
        "step": 1004
    },
    {
        "loss": 2.2,
        "grad_norm": 1.4407163858413696,
        "learning_rate": 0.00012889458154727537,
        "epoch": 0.13854425144747726,
        "step": 1005
    },
    {
        "loss": 2.2027,
        "grad_norm": 1.0544049739837646,
        "learning_rate": 0.00012876974352016792,
        "epoch": 0.1386821064240419,
        "step": 1006
    },
    {
        "loss": 2.0728,
        "grad_norm": 2.0708327293395996,
        "learning_rate": 0.0001286448565916108,
        "epoch": 0.13881996140060657,
        "step": 1007
    },
    {
        "loss": 2.248,
        "grad_norm": 1.452657699584961,
        "learning_rate": 0.00012851992097388085,
        "epoch": 0.13895781637717122,
        "step": 1008
    },
    {
        "loss": 2.0395,
        "grad_norm": 1.553221344947815,
        "learning_rate": 0.0001283949368793378,
        "epoch": 0.13909567135373588,
        "step": 1009
    },
    {
        "loss": 1.9015,
        "grad_norm": 2.3260416984558105,
        "learning_rate": 0.00012826990452042365,
        "epoch": 0.13923352633030053,
        "step": 1010
    },
    {
        "loss": 1.8796,
        "grad_norm": 2.097346782684326,
        "learning_rate": 0.00012814482410966252,
        "epoch": 0.1393713813068652,
        "step": 1011
    },
    {
        "loss": 2.498,
        "grad_norm": 1.4060930013656616,
        "learning_rate": 0.00012801969585966014,
        "epoch": 0.13950923628342984,
        "step": 1012
    },
    {
        "loss": 2.5805,
        "grad_norm": 1.1918950080871582,
        "learning_rate": 0.00012789451998310365,
        "epoch": 0.1396470912599945,
        "step": 1013
    },
    {
        "loss": 2.4306,
        "grad_norm": 1.6421083211898804,
        "learning_rate": 0.00012776929669276103,
        "epoch": 0.13978494623655913,
        "step": 1014
    },
    {
        "loss": 1.9447,
        "grad_norm": 2.129119396209717,
        "learning_rate": 0.00012764402620148095,
        "epoch": 0.13992280121312378,
        "step": 1015
    },
    {
        "loss": 2.0301,
        "grad_norm": 1.7811579704284668,
        "learning_rate": 0.00012751870872219223,
        "epoch": 0.14006065618968844,
        "step": 1016
    },
    {
        "loss": 2.2481,
        "grad_norm": 1.3222724199295044,
        "learning_rate": 0.00012739334446790364,
        "epoch": 0.1401985111662531,
        "step": 1017
    },
    {
        "loss": 2.0635,
        "grad_norm": 1.7964261770248413,
        "learning_rate": 0.00012726793365170337,
        "epoch": 0.14033636614281775,
        "step": 1018
    },
    {
        "loss": 2.2434,
        "grad_norm": 1.143430233001709,
        "learning_rate": 0.00012714247648675886,
        "epoch": 0.1404742211193824,
        "step": 1019
    },
    {
        "loss": 2.1427,
        "grad_norm": 1.7965221405029297,
        "learning_rate": 0.00012701697318631615,
        "epoch": 0.14061207609594706,
        "step": 1020
    },
    {
        "loss": 2.3828,
        "grad_norm": 1.3752468824386597,
        "learning_rate": 0.0001268914239636999,
        "epoch": 0.1407499310725117,
        "step": 1021
    },
    {
        "loss": 2.3385,
        "grad_norm": 1.0339435338974,
        "learning_rate": 0.00012676582903231278,
        "epoch": 0.14088778604907637,
        "step": 1022
    },
    {
        "loss": 2.577,
        "grad_norm": 1.1132944822311401,
        "learning_rate": 0.00012664018860563504,
        "epoch": 0.14102564102564102,
        "step": 1023
    },
    {
        "loss": 2.0084,
        "grad_norm": 2.04199481010437,
        "learning_rate": 0.0001265145028972243,
        "epoch": 0.14116349600220568,
        "step": 1024
    },
    {
        "loss": 2.0054,
        "grad_norm": 1.5152167081832886,
        "learning_rate": 0.0001263887721207153,
        "epoch": 0.14130135097877033,
        "step": 1025
    },
    {
        "loss": 2.3653,
        "grad_norm": 1.4175004959106445,
        "learning_rate": 0.00012626299648981924,
        "epoch": 0.141439205955335,
        "step": 1026
    },
    {
        "loss": 2.5661,
        "grad_norm": 1.8303519487380981,
        "learning_rate": 0.0001261371762183235,
        "epoch": 0.14157706093189965,
        "step": 1027
    },
    {
        "loss": 2.3982,
        "grad_norm": 1.0486127138137817,
        "learning_rate": 0.00012601131152009155,
        "epoch": 0.1417149159084643,
        "step": 1028
    },
    {
        "loss": 2.2322,
        "grad_norm": 1.4617170095443726,
        "learning_rate": 0.00012588540260906217,
        "epoch": 0.14185277088502896,
        "step": 1029
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.4382754564285278,
        "learning_rate": 0.0001257594496992494,
        "epoch": 0.1419906258615936,
        "step": 1030
    },
    {
        "loss": 2.0684,
        "grad_norm": 1.6815261840820312,
        "learning_rate": 0.00012563345300474206,
        "epoch": 0.14212848083815827,
        "step": 1031
    },
    {
        "loss": 2.3063,
        "grad_norm": 1.0398154258728027,
        "learning_rate": 0.0001255074127397033,
        "epoch": 0.14226633581472292,
        "step": 1032
    },
    {
        "loss": 2.2026,
        "grad_norm": 1.0607048273086548,
        "learning_rate": 0.00012538132911837048,
        "epoch": 0.14240419079128758,
        "step": 1033
    },
    {
        "loss": 2.539,
        "grad_norm": 1.2837368249893188,
        "learning_rate": 0.00012525520235505457,
        "epoch": 0.14254204576785223,
        "step": 1034
    },
    {
        "loss": 2.2391,
        "grad_norm": 1.588644027709961,
        "learning_rate": 0.00012512903266413984,
        "epoch": 0.14267990074441686,
        "step": 1035
    },
    {
        "loss": 1.7917,
        "grad_norm": 1.5622451305389404,
        "learning_rate": 0.00012500282026008366,
        "epoch": 0.14281775572098152,
        "step": 1036
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.8848800659179688,
        "learning_rate": 0.00012487656535741572,
        "epoch": 0.14295561069754617,
        "step": 1037
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.5457662343978882,
        "learning_rate": 0.00012475026817073832,
        "epoch": 0.14309346567411083,
        "step": 1038
    },
    {
        "loss": 1.8363,
        "grad_norm": 1.8650550842285156,
        "learning_rate": 0.00012462392891472533,
        "epoch": 0.14323132065067548,
        "step": 1039
    },
    {
        "loss": 1.9343,
        "grad_norm": 1.9395365715026855,
        "learning_rate": 0.0001244975478041223,
        "epoch": 0.14336917562724014,
        "step": 1040
    },
    {
        "loss": 2.1834,
        "grad_norm": 1.913476824760437,
        "learning_rate": 0.0001243711250537459,
        "epoch": 0.1435070306038048,
        "step": 1041
    },
    {
        "loss": 2.0221,
        "grad_norm": 1.229542851448059,
        "learning_rate": 0.00012424466087848342,
        "epoch": 0.14364488558036945,
        "step": 1042
    },
    {
        "loss": 1.7586,
        "grad_norm": 2.174851655960083,
        "learning_rate": 0.00012411815549329282,
        "epoch": 0.1437827405569341,
        "step": 1043
    },
    {
        "loss": 2.276,
        "grad_norm": 1.3841235637664795,
        "learning_rate": 0.00012399160911320188,
        "epoch": 0.14392059553349876,
        "step": 1044
    },
    {
        "loss": 2.2657,
        "grad_norm": 1.232291340827942,
        "learning_rate": 0.0001238650219533082,
        "epoch": 0.1440584505100634,
        "step": 1045
    },
    {
        "loss": 2.5051,
        "grad_norm": 1.0760551691055298,
        "learning_rate": 0.0001237383942287786,
        "epoch": 0.14419630548662807,
        "step": 1046
    },
    {
        "loss": 2.5224,
        "grad_norm": 0.9783053994178772,
        "learning_rate": 0.000123611726154849,
        "epoch": 0.14433416046319272,
        "step": 1047
    },
    {
        "loss": 2.1243,
        "grad_norm": 1.5962399244308472,
        "learning_rate": 0.0001234850179468237,
        "epoch": 0.14447201543975738,
        "step": 1048
    },
    {
        "loss": 2.2026,
        "grad_norm": 1.2473820447921753,
        "learning_rate": 0.0001233582698200754,
        "epoch": 0.14460987041632203,
        "step": 1049
    },
    {
        "loss": 2.331,
        "grad_norm": 1.282618522644043,
        "learning_rate": 0.00012323148199004446,
        "epoch": 0.1447477253928867,
        "step": 1050
    },
    {
        "loss": 1.9426,
        "grad_norm": 1.5924293994903564,
        "learning_rate": 0.00012310465467223894,
        "epoch": 0.14488558036945134,
        "step": 1051
    },
    {
        "loss": 1.872,
        "grad_norm": 2.6276698112487793,
        "learning_rate": 0.0001229777880822339,
        "epoch": 0.145023435346016,
        "step": 1052
    },
    {
        "loss": 1.666,
        "grad_norm": 2.4646196365356445,
        "learning_rate": 0.00012285088243567114,
        "epoch": 0.14516129032258066,
        "step": 1053
    },
    {
        "loss": 1.8271,
        "grad_norm": 1.677871823310852,
        "learning_rate": 0.0001227239379482589,
        "epoch": 0.1452991452991453,
        "step": 1054
    },
    {
        "loss": 2.353,
        "grad_norm": 1.252887487411499,
        "learning_rate": 0.00012259695483577145,
        "epoch": 0.14543700027570997,
        "step": 1055
    },
    {
        "loss": 1.8747,
        "grad_norm": 1.6292363405227661,
        "learning_rate": 0.0001224699333140486,
        "epoch": 0.14557485525227462,
        "step": 1056
    },
    {
        "loss": 2.2556,
        "grad_norm": 1.7625412940979004,
        "learning_rate": 0.00012234287359899555,
        "epoch": 0.14571271022883925,
        "step": 1057
    },
    {
        "loss": 1.8622,
        "grad_norm": 1.8742878437042236,
        "learning_rate": 0.00012221577590658245,
        "epoch": 0.1458505652054039,
        "step": 1058
    },
    {
        "loss": 1.5608,
        "grad_norm": 1.5504274368286133,
        "learning_rate": 0.00012208864045284388,
        "epoch": 0.14598842018196856,
        "step": 1059
    },
    {
        "loss": 2.0178,
        "grad_norm": 1.8567849397659302,
        "learning_rate": 0.00012196146745387875,
        "epoch": 0.14612627515853321,
        "step": 1060
    },
    {
        "loss": 2.4843,
        "grad_norm": 1.839740514755249,
        "learning_rate": 0.00012183425712584964,
        "epoch": 0.14626413013509787,
        "step": 1061
    },
    {
        "loss": 1.9528,
        "grad_norm": 1.4057939052581787,
        "learning_rate": 0.00012170700968498268,
        "epoch": 0.14640198511166252,
        "step": 1062
    },
    {
        "loss": 2.71,
        "grad_norm": 0.857779860496521,
        "learning_rate": 0.00012157972534756705,
        "epoch": 0.14653984008822718,
        "step": 1063
    },
    {
        "loss": 2.4243,
        "grad_norm": 1.2642229795455933,
        "learning_rate": 0.00012145240432995466,
        "epoch": 0.14667769506479184,
        "step": 1064
    },
    {
        "loss": 2.2415,
        "grad_norm": 1.7972235679626465,
        "learning_rate": 0.00012132504684855972,
        "epoch": 0.1468155500413565,
        "step": 1065
    },
    {
        "loss": 1.3504,
        "grad_norm": 1.4853452444076538,
        "learning_rate": 0.00012119765311985853,
        "epoch": 0.14695340501792115,
        "step": 1066
    },
    {
        "loss": 2.3141,
        "grad_norm": 0.9477360844612122,
        "learning_rate": 0.0001210702233603888,
        "epoch": 0.1470912599944858,
        "step": 1067
    },
    {
        "loss": 2.4234,
        "grad_norm": 1.2656036615371704,
        "learning_rate": 0.00012094275778674975,
        "epoch": 0.14722911497105046,
        "step": 1068
    },
    {
        "loss": 1.7358,
        "grad_norm": 2.102269172668457,
        "learning_rate": 0.00012081525661560125,
        "epoch": 0.1473669699476151,
        "step": 1069
    },
    {
        "loss": 1.5643,
        "grad_norm": 2.511411666870117,
        "learning_rate": 0.00012068772006366376,
        "epoch": 0.14750482492417977,
        "step": 1070
    },
    {
        "loss": 1.9805,
        "grad_norm": 1.6950173377990723,
        "learning_rate": 0.00012056014834771791,
        "epoch": 0.14764267990074442,
        "step": 1071
    },
    {
        "loss": 2.3087,
        "grad_norm": 1.140952706336975,
        "learning_rate": 0.00012043254168460407,
        "epoch": 0.14778053487730908,
        "step": 1072
    },
    {
        "loss": 2.5577,
        "grad_norm": 1.130627155303955,
        "learning_rate": 0.00012030490029122202,
        "epoch": 0.14791838985387373,
        "step": 1073
    },
    {
        "loss": 2.0557,
        "grad_norm": 1.2182002067565918,
        "learning_rate": 0.00012017722438453057,
        "epoch": 0.1480562448304384,
        "step": 1074
    },
    {
        "loss": 2.461,
        "grad_norm": 1.6327224969863892,
        "learning_rate": 0.00012004951418154709,
        "epoch": 0.14819409980700304,
        "step": 1075
    },
    {
        "loss": 2.3754,
        "grad_norm": 1.048654556274414,
        "learning_rate": 0.00011992176989934747,
        "epoch": 0.1483319547835677,
        "step": 1076
    },
    {
        "loss": 1.9055,
        "grad_norm": 2.0352678298950195,
        "learning_rate": 0.00011979399175506536,
        "epoch": 0.14846980976013235,
        "step": 1077
    },
    {
        "loss": 1.8753,
        "grad_norm": 1.6701996326446533,
        "learning_rate": 0.00011966617996589198,
        "epoch": 0.14860766473669698,
        "step": 1078
    },
    {
        "loss": 1.8691,
        "grad_norm": 1.2378042936325073,
        "learning_rate": 0.00011953833474907581,
        "epoch": 0.14874551971326164,
        "step": 1079
    },
    {
        "loss": 2.306,
        "grad_norm": 1.7510464191436768,
        "learning_rate": 0.00011941045632192207,
        "epoch": 0.1488833746898263,
        "step": 1080
    },
    {
        "loss": 2.4032,
        "grad_norm": 1.2633988857269287,
        "learning_rate": 0.0001192825449017925,
        "epoch": 0.14902122966639095,
        "step": 1081
    },
    {
        "loss": 2.3446,
        "grad_norm": 1.0869228839874268,
        "learning_rate": 0.00011915460070610482,
        "epoch": 0.1491590846429556,
        "step": 1082
    },
    {
        "loss": 2.2873,
        "grad_norm": 1.3144214153289795,
        "learning_rate": 0.0001190266239523326,
        "epoch": 0.14929693961952026,
        "step": 1083
    },
    {
        "loss": 1.1802,
        "grad_norm": 1.9776536226272583,
        "learning_rate": 0.0001188986148580046,
        "epoch": 0.1494347945960849,
        "step": 1084
    },
    {
        "loss": 1.7107,
        "grad_norm": 1.3874900341033936,
        "learning_rate": 0.00011877057364070472,
        "epoch": 0.14957264957264957,
        "step": 1085
    },
    {
        "loss": 2.2638,
        "grad_norm": 1.6933165788650513,
        "learning_rate": 0.00011864250051807126,
        "epoch": 0.14971050454921422,
        "step": 1086
    },
    {
        "loss": 2.1151,
        "grad_norm": 1.3493472337722778,
        "learning_rate": 0.00011851439570779697,
        "epoch": 0.14984835952577888,
        "step": 1087
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.1970207691192627,
        "learning_rate": 0.00011838625942762824,
        "epoch": 0.14998621450234353,
        "step": 1088
    },
    {
        "loss": 1.5631,
        "grad_norm": 2.1127686500549316,
        "learning_rate": 0.0001182580918953651,
        "epoch": 0.1501240694789082,
        "step": 1089
    },
    {
        "loss": 2.2005,
        "grad_norm": 1.187753677368164,
        "learning_rate": 0.00011812989332886071,
        "epoch": 0.15026192445547285,
        "step": 1090
    },
    {
        "loss": 2.2983,
        "grad_norm": 1.5122785568237305,
        "learning_rate": 0.00011800166394602088,
        "epoch": 0.1503997794320375,
        "step": 1091
    },
    {
        "loss": 1.6473,
        "grad_norm": 2.0054140090942383,
        "learning_rate": 0.00011787340396480382,
        "epoch": 0.15053763440860216,
        "step": 1092
    },
    {
        "loss": 2.1952,
        "grad_norm": 1.1770741939544678,
        "learning_rate": 0.00011774511360321991,
        "epoch": 0.1506754893851668,
        "step": 1093
    },
    {
        "loss": 2.7545,
        "grad_norm": 0.9529610276222229,
        "learning_rate": 0.00011761679307933089,
        "epoch": 0.15081334436173147,
        "step": 1094
    },
    {
        "loss": 2.0134,
        "grad_norm": 1.838278889656067,
        "learning_rate": 0.00011748844261124998,
        "epoch": 0.15095119933829612,
        "step": 1095
    },
    {
        "loss": 2.3671,
        "grad_norm": 1.3853020668029785,
        "learning_rate": 0.00011736006241714128,
        "epoch": 0.15108905431486078,
        "step": 1096
    },
    {
        "loss": 2.2845,
        "grad_norm": 1.2797218561172485,
        "learning_rate": 0.00011723165271521929,
        "epoch": 0.15122690929142543,
        "step": 1097
    },
    {
        "loss": 1.7909,
        "grad_norm": 2.2198646068573,
        "learning_rate": 0.00011710321372374884,
        "epoch": 0.1513647642679901,
        "step": 1098
    },
    {
        "loss": 2.35,
        "grad_norm": 1.3419336080551147,
        "learning_rate": 0.00011697474566104441,
        "epoch": 0.15150261924455471,
        "step": 1099
    },
    {
        "loss": 1.7957,
        "grad_norm": 2.282989025115967,
        "learning_rate": 0.00011684624874546993,
        "epoch": 0.15164047422111937,
        "step": 1100
    },
    {
        "loss": 2.0759,
        "grad_norm": 1.232771396636963,
        "learning_rate": 0.00011671772319543835,
        "epoch": 0.15177832919768403,
        "step": 1101
    },
    {
        "loss": 2.1406,
        "grad_norm": 1.4311293363571167,
        "learning_rate": 0.00011658916922941141,
        "epoch": 0.15191618417424868,
        "step": 1102
    },
    {
        "loss": 2.388,
        "grad_norm": 1.2632150650024414,
        "learning_rate": 0.00011646058706589899,
        "epoch": 0.15205403915081334,
        "step": 1103
    },
    {
        "loss": 2.3947,
        "grad_norm": 1.5069705247879028,
        "learning_rate": 0.00011633197692345903,
        "epoch": 0.152191894127378,
        "step": 1104
    },
    {
        "loss": 2.2787,
        "grad_norm": 1.0797537565231323,
        "learning_rate": 0.00011620333902069689,
        "epoch": 0.15232974910394265,
        "step": 1105
    },
    {
        "loss": 1.3219,
        "grad_norm": 1.9452829360961914,
        "learning_rate": 0.00011607467357626527,
        "epoch": 0.1524676040805073,
        "step": 1106
    },
    {
        "loss": 1.7623,
        "grad_norm": 1.800352931022644,
        "learning_rate": 0.00011594598080886356,
        "epoch": 0.15260545905707196,
        "step": 1107
    },
    {
        "loss": 1.4981,
        "grad_norm": 1.7914958000183105,
        "learning_rate": 0.00011581726093723766,
        "epoch": 0.1527433140336366,
        "step": 1108
    },
    {
        "loss": 1.7572,
        "grad_norm": 2.190086841583252,
        "learning_rate": 0.00011568851418017952,
        "epoch": 0.15288116901020127,
        "step": 1109
    },
    {
        "loss": 2.4131,
        "grad_norm": 1.1231719255447388,
        "learning_rate": 0.00011555974075652682,
        "epoch": 0.15301902398676592,
        "step": 1110
    },
    {
        "loss": 2.248,
        "grad_norm": 1.5959750413894653,
        "learning_rate": 0.00011543094088516249,
        "epoch": 0.15315687896333058,
        "step": 1111
    },
    {
        "loss": 2.0986,
        "grad_norm": 1.0870460271835327,
        "learning_rate": 0.00011530211478501449,
        "epoch": 0.15329473393989523,
        "step": 1112
    },
    {
        "loss": 2.1876,
        "grad_norm": 1.4751616716384888,
        "learning_rate": 0.00011517326267505528,
        "epoch": 0.1534325889164599,
        "step": 1113
    },
    {
        "loss": 1.8805,
        "grad_norm": 2.0418851375579834,
        "learning_rate": 0.00011504438477430165,
        "epoch": 0.15357044389302454,
        "step": 1114
    },
    {
        "loss": 2.0347,
        "grad_norm": 1.7978180646896362,
        "learning_rate": 0.00011491548130181415,
        "epoch": 0.1537082988695892,
        "step": 1115
    },
    {
        "loss": 2.1874,
        "grad_norm": 1.6377391815185547,
        "learning_rate": 0.00011478655247669678,
        "epoch": 0.15384615384615385,
        "step": 1116
    },
    {
        "loss": 1.9481,
        "grad_norm": 1.471463680267334,
        "learning_rate": 0.0001146575985180967,
        "epoch": 0.1539840088227185,
        "step": 1117
    },
    {
        "loss": 2.0767,
        "grad_norm": 1.5858423709869385,
        "learning_rate": 0.00011452861964520369,
        "epoch": 0.15412186379928317,
        "step": 1118
    },
    {
        "loss": 1.7649,
        "grad_norm": 1.7851148843765259,
        "learning_rate": 0.00011439961607725,
        "epoch": 0.15425971877584782,
        "step": 1119
    },
    {
        "loss": 1.1951,
        "grad_norm": 1.9111053943634033,
        "learning_rate": 0.00011427058803350979,
        "epoch": 0.15439757375241248,
        "step": 1120
    },
    {
        "loss": 1.8768,
        "grad_norm": 1.4342373609542847,
        "learning_rate": 0.0001141415357332988,
        "epoch": 0.1545354287289771,
        "step": 1121
    },
    {
        "loss": 2.1361,
        "grad_norm": 1.1403197050094604,
        "learning_rate": 0.000114012459395974,
        "epoch": 0.15467328370554176,
        "step": 1122
    },
    {
        "loss": 1.7102,
        "grad_norm": 2.0719058513641357,
        "learning_rate": 0.00011388335924093334,
        "epoch": 0.15481113868210641,
        "step": 1123
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.8343231678009033,
        "learning_rate": 0.00011375423548761505,
        "epoch": 0.15494899365867107,
        "step": 1124
    },
    {
        "loss": 1.6953,
        "grad_norm": 1.9462988376617432,
        "learning_rate": 0.00011362508835549768,
        "epoch": 0.15508684863523572,
        "step": 1125
    },
    {
        "loss": 2.3202,
        "grad_norm": 2.012038469314575,
        "learning_rate": 0.00011349591806409933,
        "epoch": 0.15522470361180038,
        "step": 1126
    },
    {
        "loss": 2.1117,
        "grad_norm": 2.5181968212127686,
        "learning_rate": 0.00011336672483297761,
        "epoch": 0.15536255858836504,
        "step": 1127
    },
    {
        "loss": 1.5412,
        "grad_norm": 2.198636531829834,
        "learning_rate": 0.00011323750888172906,
        "epoch": 0.1555004135649297,
        "step": 1128
    },
    {
        "loss": 2.3264,
        "grad_norm": 1.8757234811782837,
        "learning_rate": 0.00011310827042998884,
        "epoch": 0.15563826854149435,
        "step": 1129
    },
    {
        "loss": 1.8272,
        "grad_norm": 2.364398956298828,
        "learning_rate": 0.00011297900969743032,
        "epoch": 0.155776123518059,
        "step": 1130
    },
    {
        "loss": 1.816,
        "grad_norm": 1.9038997888565063,
        "learning_rate": 0.0001128497269037648,
        "epoch": 0.15591397849462366,
        "step": 1131
    },
    {
        "loss": 2.6549,
        "grad_norm": 1.4416534900665283,
        "learning_rate": 0.00011272042226874114,
        "epoch": 0.1560518334711883,
        "step": 1132
    },
    {
        "loss": 1.9645,
        "grad_norm": 1.5700047016143799,
        "learning_rate": 0.00011259109601214513,
        "epoch": 0.15618968844775297,
        "step": 1133
    },
    {
        "loss": 2.3664,
        "grad_norm": 1.198801875114441,
        "learning_rate": 0.00011246174835379952,
        "epoch": 0.15632754342431762,
        "step": 1134
    },
    {
        "loss": 1.914,
        "grad_norm": 1.380577802658081,
        "learning_rate": 0.00011233237951356326,
        "epoch": 0.15646539840088228,
        "step": 1135
    },
    {
        "loss": 2.1577,
        "grad_norm": 1.4115604162216187,
        "learning_rate": 0.00011220298971133142,
        "epoch": 0.15660325337744693,
        "step": 1136
    },
    {
        "loss": 2.0094,
        "grad_norm": 2.363168716430664,
        "learning_rate": 0.00011207357916703472,
        "epoch": 0.1567411083540116,
        "step": 1137
    },
    {
        "loss": 1.9719,
        "grad_norm": 1.288587212562561,
        "learning_rate": 0.000111944148100639,
        "epoch": 0.15687896333057624,
        "step": 1138
    },
    {
        "loss": 2.2039,
        "grad_norm": 1.7968010902404785,
        "learning_rate": 0.0001118146967321451,
        "epoch": 0.1570168183071409,
        "step": 1139
    },
    {
        "loss": 2.5623,
        "grad_norm": 1.9062389135360718,
        "learning_rate": 0.00011168522528158836,
        "epoch": 0.15715467328370555,
        "step": 1140
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.3779988288879395,
        "learning_rate": 0.0001115557339690382,
        "epoch": 0.1572925282602702,
        "step": 1141
    },
    {
        "loss": 2.1305,
        "grad_norm": 1.1105637550354004,
        "learning_rate": 0.00011142622301459789,
        "epoch": 0.15743038323683484,
        "step": 1142
    },
    {
        "loss": 2.2903,
        "grad_norm": 1.3779642581939697,
        "learning_rate": 0.00011129669263840388,
        "epoch": 0.1575682382133995,
        "step": 1143
    },
    {
        "loss": 2.3413,
        "grad_norm": 1.5086115598678589,
        "learning_rate": 0.00011116714306062594,
        "epoch": 0.15770609318996415,
        "step": 1144
    },
    {
        "loss": 2.1848,
        "grad_norm": 1.012416124343872,
        "learning_rate": 0.00011103757450146623,
        "epoch": 0.1578439481665288,
        "step": 1145
    },
    {
        "loss": 2.2084,
        "grad_norm": 1.6614667177200317,
        "learning_rate": 0.00011090798718115929,
        "epoch": 0.15798180314309346,
        "step": 1146
    },
    {
        "loss": 2.5111,
        "grad_norm": 1.1912472248077393,
        "learning_rate": 0.00011077838131997152,
        "epoch": 0.1581196581196581,
        "step": 1147
    },
    {
        "loss": 1.834,
        "grad_norm": 1.6211203336715698,
        "learning_rate": 0.00011064875713820082,
        "epoch": 0.15825751309622277,
        "step": 1148
    },
    {
        "loss": 1.9315,
        "grad_norm": 1.9904747009277344,
        "learning_rate": 0.00011051911485617624,
        "epoch": 0.15839536807278742,
        "step": 1149
    },
    {
        "loss": 1.8101,
        "grad_norm": 2.2099976539611816,
        "learning_rate": 0.0001103894546942576,
        "epoch": 0.15853322304935208,
        "step": 1150
    },
    {
        "loss": 2.0442,
        "grad_norm": 1.6165629625320435,
        "learning_rate": 0.00011025977687283515,
        "epoch": 0.15867107802591673,
        "step": 1151
    },
    {
        "loss": 1.8188,
        "grad_norm": 1.2121390104293823,
        "learning_rate": 0.00011013008161232907,
        "epoch": 0.1588089330024814,
        "step": 1152
    },
    {
        "loss": 2.1752,
        "grad_norm": 1.59244704246521,
        "learning_rate": 0.00011000036913318932,
        "epoch": 0.15894678797904604,
        "step": 1153
    },
    {
        "loss": 2.4343,
        "grad_norm": 1.1263298988342285,
        "learning_rate": 0.0001098706396558949,
        "epoch": 0.1590846429556107,
        "step": 1154
    },
    {
        "loss": 2.0918,
        "grad_norm": 1.2176350355148315,
        "learning_rate": 0.00010974089340095397,
        "epoch": 0.15922249793217536,
        "step": 1155
    },
    {
        "loss": 1.724,
        "grad_norm": 2.6051886081695557,
        "learning_rate": 0.00010961113058890298,
        "epoch": 0.15936035290874,
        "step": 1156
    },
    {
        "loss": 1.7713,
        "grad_norm": 1.3807988166809082,
        "learning_rate": 0.00010948135144030667,
        "epoch": 0.15949820788530467,
        "step": 1157
    },
    {
        "loss": 2.4286,
        "grad_norm": 1.590019941329956,
        "learning_rate": 0.00010935155617575754,
        "epoch": 0.15963606286186932,
        "step": 1158
    },
    {
        "loss": 1.2513,
        "grad_norm": 1.6392446756362915,
        "learning_rate": 0.00010922174501587538,
        "epoch": 0.15977391783843398,
        "step": 1159
    },
    {
        "loss": 1.7364,
        "grad_norm": 2.026812791824341,
        "learning_rate": 0.00010909191818130704,
        "epoch": 0.15991177281499863,
        "step": 1160
    },
    {
        "loss": 2.3445,
        "grad_norm": 1.350765585899353,
        "learning_rate": 0.00010896207589272616,
        "epoch": 0.1600496277915633,
        "step": 1161
    },
    {
        "loss": 2.381,
        "grad_norm": 1.2621926069259644,
        "learning_rate": 0.00010883221837083237,
        "epoch": 0.16018748276812794,
        "step": 1162
    },
    {
        "loss": 2.4692,
        "grad_norm": 1.7986149787902832,
        "learning_rate": 0.00010870234583635146,
        "epoch": 0.16032533774469257,
        "step": 1163
    },
    {
        "loss": 1.3324,
        "grad_norm": 2.0305352210998535,
        "learning_rate": 0.00010857245851003455,
        "epoch": 0.16046319272125723,
        "step": 1164
    },
    {
        "loss": 2.1815,
        "grad_norm": 1.46308171749115,
        "learning_rate": 0.00010844255661265799,
        "epoch": 0.16060104769782188,
        "step": 1165
    },
    {
        "loss": 2.2213,
        "grad_norm": 1.6697276830673218,
        "learning_rate": 0.0001083126403650229,
        "epoch": 0.16073890267438654,
        "step": 1166
    },
    {
        "loss": 2.3228,
        "grad_norm": 1.303580641746521,
        "learning_rate": 0.00010818270998795481,
        "epoch": 0.1608767576509512,
        "step": 1167
    },
    {
        "loss": 2.2759,
        "grad_norm": 1.567745566368103,
        "learning_rate": 0.00010805276570230316,
        "epoch": 0.16101461262751585,
        "step": 1168
    },
    {
        "loss": 1.8632,
        "grad_norm": 1.2568484544754028,
        "learning_rate": 0.00010792280772894115,
        "epoch": 0.1611524676040805,
        "step": 1169
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.5395290851593018,
        "learning_rate": 0.00010779283628876518,
        "epoch": 0.16129032258064516,
        "step": 1170
    },
    {
        "loss": 1.8875,
        "grad_norm": 2.1270854473114014,
        "learning_rate": 0.00010766285160269456,
        "epoch": 0.1614281775572098,
        "step": 1171
    },
    {
        "loss": 2.0203,
        "grad_norm": 2.310285806655884,
        "learning_rate": 0.00010753285389167114,
        "epoch": 0.16156603253377447,
        "step": 1172
    },
    {
        "loss": 2.349,
        "grad_norm": 1.9598432779312134,
        "learning_rate": 0.0001074028433766588,
        "epoch": 0.16170388751033912,
        "step": 1173
    },
    {
        "loss": 2.1887,
        "grad_norm": 1.0571551322937012,
        "learning_rate": 0.00010727282027864338,
        "epoch": 0.16184174248690378,
        "step": 1174
    },
    {
        "loss": 2.4066,
        "grad_norm": 1.019073724746704,
        "learning_rate": 0.00010714278481863194,
        "epoch": 0.16197959746346843,
        "step": 1175
    },
    {
        "loss": 1.629,
        "grad_norm": 2.0830304622650146,
        "learning_rate": 0.00010701273721765256,
        "epoch": 0.1621174524400331,
        "step": 1176
    },
    {
        "loss": 1.8486,
        "grad_norm": 1.4846694469451904,
        "learning_rate": 0.00010688267769675406,
        "epoch": 0.16225530741659774,
        "step": 1177
    },
    {
        "loss": 2.5665,
        "grad_norm": 1.144571304321289,
        "learning_rate": 0.00010675260647700547,
        "epoch": 0.1623931623931624,
        "step": 1178
    },
    {
        "loss": 1.7257,
        "grad_norm": 1.309220314025879,
        "learning_rate": 0.00010662252377949567,
        "epoch": 0.16253101736972705,
        "step": 1179
    },
    {
        "loss": 1.9151,
        "grad_norm": 1.7975760698318481,
        "learning_rate": 0.00010649242982533312,
        "epoch": 0.1626688723462917,
        "step": 1180
    },
    {
        "loss": 1.4166,
        "grad_norm": 2.6888387203216553,
        "learning_rate": 0.00010636232483564528,
        "epoch": 0.16280672732285636,
        "step": 1181
    },
    {
        "loss": 2.2618,
        "grad_norm": 1.072996735572815,
        "learning_rate": 0.00010623220903157855,
        "epoch": 0.16294458229942102,
        "step": 1182
    },
    {
        "loss": 2.1839,
        "grad_norm": 1.245030403137207,
        "learning_rate": 0.00010610208263429759,
        "epoch": 0.16308243727598568,
        "step": 1183
    },
    {
        "loss": 2.0114,
        "grad_norm": 1.1186740398406982,
        "learning_rate": 0.00010597194586498511,
        "epoch": 0.16322029225255033,
        "step": 1184
    },
    {
        "loss": 1.7703,
        "grad_norm": 1.8769007921218872,
        "learning_rate": 0.0001058417989448414,
        "epoch": 0.16335814722911496,
        "step": 1185
    },
    {
        "loss": 1.34,
        "grad_norm": 1.845624327659607,
        "learning_rate": 0.00010571164209508408,
        "epoch": 0.1634960022056796,
        "step": 1186
    },
    {
        "loss": 2.6351,
        "grad_norm": 0.9541211128234863,
        "learning_rate": 0.00010558147553694762,
        "epoch": 0.16363385718224427,
        "step": 1187
    },
    {
        "loss": 2.5983,
        "grad_norm": 1.542258620262146,
        "learning_rate": 0.00010545129949168291,
        "epoch": 0.16377171215880892,
        "step": 1188
    },
    {
        "loss": 2.0558,
        "grad_norm": 1.5393569469451904,
        "learning_rate": 0.00010532111418055713,
        "epoch": 0.16390956713537358,
        "step": 1189
    },
    {
        "loss": 2.3757,
        "grad_norm": 2.433736801147461,
        "learning_rate": 0.000105190919824853,
        "epoch": 0.16404742211193823,
        "step": 1190
    },
    {
        "loss": 2.3316,
        "grad_norm": 1.3343223333358765,
        "learning_rate": 0.0001050607166458689,
        "epoch": 0.1641852770885029,
        "step": 1191
    },
    {
        "loss": 2.1388,
        "grad_norm": 1.421760082244873,
        "learning_rate": 0.00010493050486491787,
        "epoch": 0.16432313206506755,
        "step": 1192
    },
    {
        "loss": 2.737,
        "grad_norm": 1.1593085527420044,
        "learning_rate": 0.00010480028470332781,
        "epoch": 0.1644609870416322,
        "step": 1193
    },
    {
        "loss": 1.8426,
        "grad_norm": 1.4138737916946411,
        "learning_rate": 0.00010467005638244079,
        "epoch": 0.16459884201819686,
        "step": 1194
    },
    {
        "loss": 1.9778,
        "grad_norm": 1.8191219568252563,
        "learning_rate": 0.00010453982012361274,
        "epoch": 0.1647366969947615,
        "step": 1195
    },
    {
        "loss": 1.8461,
        "grad_norm": 2.243161916732788,
        "learning_rate": 0.00010440957614821312,
        "epoch": 0.16487455197132617,
        "step": 1196
    },
    {
        "loss": 2.6392,
        "grad_norm": 0.8925246596336365,
        "learning_rate": 0.00010427932467762449,
        "epoch": 0.16501240694789082,
        "step": 1197
    },
    {
        "loss": 2.1486,
        "grad_norm": 0.9486908912658691,
        "learning_rate": 0.00010414906593324206,
        "epoch": 0.16515026192445548,
        "step": 1198
    },
    {
        "loss": 2.0972,
        "grad_norm": 1.5859545469284058,
        "learning_rate": 0.00010401880013647358,
        "epoch": 0.16528811690102013,
        "step": 1199
    },
    {
        "loss": 1.8638,
        "grad_norm": 1.3671913146972656,
        "learning_rate": 0.00010388852750873863,
        "epoch": 0.1654259718775848,
        "step": 1200
    },
    {
        "loss": 1.6047,
        "grad_norm": 1.9054912328720093,
        "learning_rate": 0.00010375824827146852,
        "epoch": 0.16556382685414944,
        "step": 1201
    },
    {
        "loss": 2.1951,
        "grad_norm": 1.6133817434310913,
        "learning_rate": 0.00010362796264610565,
        "epoch": 0.1657016818307141,
        "step": 1202
    },
    {
        "loss": 1.7171,
        "grad_norm": 1.426760196685791,
        "learning_rate": 0.00010349767085410342,
        "epoch": 0.16583953680727875,
        "step": 1203
    },
    {
        "loss": 2.0317,
        "grad_norm": 1.5361971855163574,
        "learning_rate": 0.0001033673731169257,
        "epoch": 0.1659773917838434,
        "step": 1204
    },
    {
        "loss": 2.6478,
        "grad_norm": 1.4212983846664429,
        "learning_rate": 0.00010323706965604638,
        "epoch": 0.16611524676040806,
        "step": 1205
    },
    {
        "loss": 1.9238,
        "grad_norm": 1.6748253107070923,
        "learning_rate": 0.00010310676069294909,
        "epoch": 0.1662531017369727,
        "step": 1206
    },
    {
        "loss": 2.207,
        "grad_norm": 2.216041326522827,
        "learning_rate": 0.00010297644644912688,
        "epoch": 0.16639095671353735,
        "step": 1207
    },
    {
        "loss": 2.3524,
        "grad_norm": 1.090659737586975,
        "learning_rate": 0.00010284612714608175,
        "epoch": 0.166528811690102,
        "step": 1208
    },
    {
        "loss": 2.3362,
        "grad_norm": 0.9806957244873047,
        "learning_rate": 0.00010271580300532426,
        "epoch": 0.16666666666666666,
        "step": 1209
    },
    {
        "loss": 2.3757,
        "grad_norm": 1.605179786682129,
        "learning_rate": 0.00010258547424837329,
        "epoch": 0.1668045216432313,
        "step": 1210
    },
    {
        "loss": 1.9787,
        "grad_norm": 1.9190940856933594,
        "learning_rate": 0.00010245514109675534,
        "epoch": 0.16694237661979597,
        "step": 1211
    },
    {
        "loss": 2.3671,
        "grad_norm": 1.7998292446136475,
        "learning_rate": 0.0001023248037720047,
        "epoch": 0.16708023159636062,
        "step": 1212
    },
    {
        "loss": 2.0887,
        "grad_norm": 2.0733070373535156,
        "learning_rate": 0.00010219446249566255,
        "epoch": 0.16721808657292528,
        "step": 1213
    },
    {
        "loss": 2.2625,
        "grad_norm": 1.4135463237762451,
        "learning_rate": 0.00010206411748927678,
        "epoch": 0.16735594154948993,
        "step": 1214
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.4975537061691284,
        "learning_rate": 0.0001019337689744017,
        "epoch": 0.1674937965260546,
        "step": 1215
    },
    {
        "loss": 2.0626,
        "grad_norm": 1.8376520872116089,
        "learning_rate": 0.00010180341717259759,
        "epoch": 0.16763165150261924,
        "step": 1216
    },
    {
        "loss": 2.3691,
        "grad_norm": 2.042733669281006,
        "learning_rate": 0.00010167306230543021,
        "epoch": 0.1677695064791839,
        "step": 1217
    },
    {
        "loss": 2.0289,
        "grad_norm": 1.6706809997558594,
        "learning_rate": 0.00010154270459447064,
        "epoch": 0.16790736145574855,
        "step": 1218
    },
    {
        "loss": 1.9279,
        "grad_norm": 1.5080482959747314,
        "learning_rate": 0.00010141234426129473,
        "epoch": 0.1680452164323132,
        "step": 1219
    },
    {
        "loss": 2.1154,
        "grad_norm": 1.5195828676223755,
        "learning_rate": 0.00010128198152748287,
        "epoch": 0.16818307140887787,
        "step": 1220
    },
    {
        "loss": 2.3987,
        "grad_norm": 1.2998462915420532,
        "learning_rate": 0.00010115161661461938,
        "epoch": 0.16832092638544252,
        "step": 1221
    },
    {
        "loss": 2.5175,
        "grad_norm": 2.1985788345336914,
        "learning_rate": 0.00010102124974429243,
        "epoch": 0.16845878136200718,
        "step": 1222
    },
    {
        "loss": 2.335,
        "grad_norm": 0.9280740022659302,
        "learning_rate": 0.00010089088113809348,
        "epoch": 0.16859663633857183,
        "step": 1223
    },
    {
        "loss": 2.0686,
        "grad_norm": 1.2590378522872925,
        "learning_rate": 0.00010076051101761687,
        "epoch": 0.1687344913151365,
        "step": 1224
    },
    {
        "loss": 2.3867,
        "grad_norm": 1.5501675605773926,
        "learning_rate": 0.00010063013960445959,
        "epoch": 0.16887234629170114,
        "step": 1225
    },
    {
        "loss": 1.8487,
        "grad_norm": 1.9848850965499878,
        "learning_rate": 0.00010049976712022085,
        "epoch": 0.1690102012682658,
        "step": 1226
    },
    {
        "loss": 2.5846,
        "grad_norm": 0.9626376032829285,
        "learning_rate": 0.00010036939378650156,
        "epoch": 0.16914805624483042,
        "step": 1227
    },
    {
        "loss": 2.1502,
        "grad_norm": 1.3914167881011963,
        "learning_rate": 0.00010023901982490413,
        "epoch": 0.16928591122139508,
        "step": 1228
    },
    {
        "loss": 1.9537,
        "grad_norm": 1.6281520128250122,
        "learning_rate": 0.00010010864545703221,
        "epoch": 0.16942376619795974,
        "step": 1229
    },
    {
        "loss": 1.8133,
        "grad_norm": 1.1032414436340332,
        "learning_rate": 9.99782709044898e-05,
        "epoch": 0.1695616211745244,
        "step": 1230
    },
    {
        "loss": 2.6287,
        "grad_norm": 1.3356761932373047,
        "learning_rate": 9.98478963888815e-05,
        "epoch": 0.16969947615108905,
        "step": 1231
    },
    {
        "loss": 2.0026,
        "grad_norm": 1.7656991481781006,
        "learning_rate": 9.971752213181171e-05,
        "epoch": 0.1698373311276537,
        "step": 1232
    },
    {
        "loss": 2.065,
        "grad_norm": 1.3737859725952148,
        "learning_rate": 9.958714835488446e-05,
        "epoch": 0.16997518610421836,
        "step": 1233
    },
    {
        "loss": 1.2536,
        "grad_norm": 2.345726251602173,
        "learning_rate": 9.945677527970292e-05,
        "epoch": 0.170113041080783,
        "step": 1234
    },
    {
        "loss": 1.9872,
        "grad_norm": 1.5100064277648926,
        "learning_rate": 9.932640312786905e-05,
        "epoch": 0.17025089605734767,
        "step": 1235
    },
    {
        "loss": 1.8952,
        "grad_norm": 1.8620967864990234,
        "learning_rate": 9.919603212098331e-05,
        "epoch": 0.17038875103391232,
        "step": 1236
    },
    {
        "loss": 2.1977,
        "grad_norm": 1.3858121633529663,
        "learning_rate": 9.906566248064418e-05,
        "epoch": 0.17052660601047698,
        "step": 1237
    },
    {
        "loss": 1.6454,
        "grad_norm": 1.8153679370880127,
        "learning_rate": 9.893529442844773e-05,
        "epoch": 0.17066446098704163,
        "step": 1238
    },
    {
        "loss": 2.3006,
        "grad_norm": 1.0914573669433594,
        "learning_rate": 9.880492818598749e-05,
        "epoch": 0.1708023159636063,
        "step": 1239
    },
    {
        "loss": 1.8224,
        "grad_norm": 1.614938497543335,
        "learning_rate": 9.867456397485386e-05,
        "epoch": 0.17094017094017094,
        "step": 1240
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.5821938514709473,
        "learning_rate": 9.854420201663362e-05,
        "epoch": 0.1710780259167356,
        "step": 1241
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.433309555053711,
        "learning_rate": 9.841384253291004e-05,
        "epoch": 0.17121588089330025,
        "step": 1242
    },
    {
        "loss": 1.9478,
        "grad_norm": 1.3912403583526611,
        "learning_rate": 9.82834857452619e-05,
        "epoch": 0.1713537358698649,
        "step": 1243
    },
    {
        "loss": 2.6985,
        "grad_norm": 1.4533298015594482,
        "learning_rate": 9.815313187526351e-05,
        "epoch": 0.17149159084642956,
        "step": 1244
    },
    {
        "loss": 2.0682,
        "grad_norm": 1.1263521909713745,
        "learning_rate": 9.802278114448424e-05,
        "epoch": 0.17162944582299422,
        "step": 1245
    },
    {
        "loss": 2.2624,
        "grad_norm": 1.0592900514602661,
        "learning_rate": 9.789243377448812e-05,
        "epoch": 0.17176730079955888,
        "step": 1246
    },
    {
        "loss": 1.9599,
        "grad_norm": 1.6367878913879395,
        "learning_rate": 9.776208998683336e-05,
        "epoch": 0.17190515577612353,
        "step": 1247
    },
    {
        "loss": 1.9303,
        "grad_norm": 1.604231834411621,
        "learning_rate": 9.763175000307229e-05,
        "epoch": 0.17204301075268819,
        "step": 1248
    },
    {
        "loss": 2.4162,
        "grad_norm": 1.605127215385437,
        "learning_rate": 9.750141404475053e-05,
        "epoch": 0.1721808657292528,
        "step": 1249
    },
    {
        "loss": 1.9353,
        "grad_norm": 1.3334416151046753,
        "learning_rate": 9.737108233340703e-05,
        "epoch": 0.17231872070581747,
        "step": 1250
    },
    {
        "loss": 2.4678,
        "grad_norm": 1.0486475229263306,
        "learning_rate": 9.72407550905735e-05,
        "epoch": 0.17245657568238212,
        "step": 1251
    },
    {
        "loss": 2.182,
        "grad_norm": 1.5980112552642822,
        "learning_rate": 9.7110432537774e-05,
        "epoch": 0.17259443065894678,
        "step": 1252
    },
    {
        "loss": 2.17,
        "grad_norm": 1.5570727586746216,
        "learning_rate": 9.698011489652462e-05,
        "epoch": 0.17273228563551143,
        "step": 1253
    },
    {
        "loss": 1.389,
        "grad_norm": 1.936051607131958,
        "learning_rate": 9.684980238833315e-05,
        "epoch": 0.1728701406120761,
        "step": 1254
    },
    {
        "loss": 1.9299,
        "grad_norm": 1.4211782217025757,
        "learning_rate": 9.671949523469865e-05,
        "epoch": 0.17300799558864075,
        "step": 1255
    },
    {
        "loss": 2.0165,
        "grad_norm": 1.2074683904647827,
        "learning_rate": 9.658919365711104e-05,
        "epoch": 0.1731458505652054,
        "step": 1256
    },
    {
        "loss": 1.9333,
        "grad_norm": 2.309276819229126,
        "learning_rate": 9.645889787705072e-05,
        "epoch": 0.17328370554177006,
        "step": 1257
    },
    {
        "loss": 2.4281,
        "grad_norm": 1.3898576498031616,
        "learning_rate": 9.632860811598838e-05,
        "epoch": 0.1734215605183347,
        "step": 1258
    },
    {
        "loss": 2.3361,
        "grad_norm": 2.2907652854919434,
        "learning_rate": 9.619832459538441e-05,
        "epoch": 0.17355941549489937,
        "step": 1259
    },
    {
        "loss": 2.2344,
        "grad_norm": 1.7606488466262817,
        "learning_rate": 9.606804753668848e-05,
        "epoch": 0.17369727047146402,
        "step": 1260
    },
    {
        "loss": 1.9046,
        "grad_norm": 2.1307592391967773,
        "learning_rate": 9.593777716133949e-05,
        "epoch": 0.17383512544802868,
        "step": 1261
    },
    {
        "loss": 1.6801,
        "grad_norm": 1.8054357767105103,
        "learning_rate": 9.580751369076474e-05,
        "epoch": 0.17397298042459333,
        "step": 1262
    },
    {
        "loss": 1.9583,
        "grad_norm": 2.036243200302124,
        "learning_rate": 9.567725734638e-05,
        "epoch": 0.174110835401158,
        "step": 1263
    },
    {
        "loss": 2.4907,
        "grad_norm": 1.1708213090896606,
        "learning_rate": 9.554700834958884e-05,
        "epoch": 0.17424869037772264,
        "step": 1264
    },
    {
        "loss": 2.2795,
        "grad_norm": 1.0924426317214966,
        "learning_rate": 9.541676692178233e-05,
        "epoch": 0.1743865453542873,
        "step": 1265
    },
    {
        "loss": 2.1133,
        "grad_norm": 1.7756783962249756,
        "learning_rate": 9.528653328433867e-05,
        "epoch": 0.17452440033085195,
        "step": 1266
    },
    {
        "loss": 2.0963,
        "grad_norm": 1.4063730239868164,
        "learning_rate": 9.51563076586229e-05,
        "epoch": 0.1746622553074166,
        "step": 1267
    },
    {
        "loss": 2.116,
        "grad_norm": 1.7644473314285278,
        "learning_rate": 9.502609026598632e-05,
        "epoch": 0.17480011028398126,
        "step": 1268
    },
    {
        "loss": 2.6358,
        "grad_norm": 1.1267036199569702,
        "learning_rate": 9.489588132776633e-05,
        "epoch": 0.17493796526054592,
        "step": 1269
    },
    {
        "loss": 1.92,
        "grad_norm": 1.508150577545166,
        "learning_rate": 9.476568106528589e-05,
        "epoch": 0.17507582023711055,
        "step": 1270
    },
    {
        "loss": 2.1052,
        "grad_norm": 1.406200885772705,
        "learning_rate": 9.463548969985329e-05,
        "epoch": 0.1752136752136752,
        "step": 1271
    },
    {
        "loss": 2.0997,
        "grad_norm": 1.2686911821365356,
        "learning_rate": 9.450530745276163e-05,
        "epoch": 0.17535153019023986,
        "step": 1272
    },
    {
        "loss": 2.3671,
        "grad_norm": 1.6615241765975952,
        "learning_rate": 9.43751345452886e-05,
        "epoch": 0.1754893851668045,
        "step": 1273
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.5254888534545898,
        "learning_rate": 9.424497119869585e-05,
        "epoch": 0.17562724014336917,
        "step": 1274
    },
    {
        "loss": 1.9863,
        "grad_norm": 1.8631070852279663,
        "learning_rate": 9.411481763422899e-05,
        "epoch": 0.17576509511993382,
        "step": 1275
    },
    {
        "loss": 2.5727,
        "grad_norm": 1.2531898021697998,
        "learning_rate": 9.398467407311678e-05,
        "epoch": 0.17590295009649848,
        "step": 1276
    },
    {
        "loss": 2.0516,
        "grad_norm": 1.5987300872802734,
        "learning_rate": 9.385454073657117e-05,
        "epoch": 0.17604080507306313,
        "step": 1277
    },
    {
        "loss": 1.9975,
        "grad_norm": 1.687699317932129,
        "learning_rate": 9.372441784578668e-05,
        "epoch": 0.1761786600496278,
        "step": 1278
    },
    {
        "loss": 2.1168,
        "grad_norm": 1.8600225448608398,
        "learning_rate": 9.359430562193992e-05,
        "epoch": 0.17631651502619244,
        "step": 1279
    },
    {
        "loss": 2.7768,
        "grad_norm": 1.3683377504348755,
        "learning_rate": 9.346420428618966e-05,
        "epoch": 0.1764543700027571,
        "step": 1280
    },
    {
        "loss": 1.9059,
        "grad_norm": 2.044118881225586,
        "learning_rate": 9.333411405967589e-05,
        "epoch": 0.17659222497932175,
        "step": 1281
    },
    {
        "loss": 2.3828,
        "grad_norm": 1.4204251766204834,
        "learning_rate": 9.320403516351984e-05,
        "epoch": 0.1767300799558864,
        "step": 1282
    },
    {
        "loss": 1.7947,
        "grad_norm": 1.7742092609405518,
        "learning_rate": 9.307396781882348e-05,
        "epoch": 0.17686793493245107,
        "step": 1283
    },
    {
        "loss": 1.8546,
        "grad_norm": 1.1020365953445435,
        "learning_rate": 9.294391224666916e-05,
        "epoch": 0.17700578990901572,
        "step": 1284
    },
    {
        "loss": 1.7182,
        "grad_norm": 2.0795633792877197,
        "learning_rate": 9.28138686681191e-05,
        "epoch": 0.17714364488558038,
        "step": 1285
    },
    {
        "loss": 2.2135,
        "grad_norm": 1.7094172239303589,
        "learning_rate": 9.268383730421538e-05,
        "epoch": 0.17728149986214503,
        "step": 1286
    },
    {
        "loss": 2.0559,
        "grad_norm": 1.6906930208206177,
        "learning_rate": 9.255381837597901e-05,
        "epoch": 0.1774193548387097,
        "step": 1287
    },
    {
        "loss": 2.2257,
        "grad_norm": 1.3767365217208862,
        "learning_rate": 9.242381210441007e-05,
        "epoch": 0.17755720981527434,
        "step": 1288
    },
    {
        "loss": 2.1043,
        "grad_norm": 1.4908677339553833,
        "learning_rate": 9.22938187104871e-05,
        "epoch": 0.177695064791839,
        "step": 1289
    },
    {
        "loss": 2.2615,
        "grad_norm": 1.5802947282791138,
        "learning_rate": 9.216383841516673e-05,
        "epoch": 0.17783291976840365,
        "step": 1290
    },
    {
        "loss": 2.0943,
        "grad_norm": 1.8537213802337646,
        "learning_rate": 9.203387143938326e-05,
        "epoch": 0.17797077474496828,
        "step": 1291
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.2714797258377075,
        "learning_rate": 9.190391800404846e-05,
        "epoch": 0.17810862972153294,
        "step": 1292
    },
    {
        "loss": 2.238,
        "grad_norm": 1.3041366338729858,
        "learning_rate": 9.177397833005101e-05,
        "epoch": 0.1782464846980976,
        "step": 1293
    },
    {
        "loss": 1.5238,
        "grad_norm": 2.2880208492279053,
        "learning_rate": 9.164405263825627e-05,
        "epoch": 0.17838433967466225,
        "step": 1294
    },
    {
        "loss": 1.8082,
        "grad_norm": 1.1848082542419434,
        "learning_rate": 9.151414114950574e-05,
        "epoch": 0.1785221946512269,
        "step": 1295
    },
    {
        "loss": 2.395,
        "grad_norm": 1.7872906923294067,
        "learning_rate": 9.138424408461682e-05,
        "epoch": 0.17866004962779156,
        "step": 1296
    },
    {
        "loss": 1.1678,
        "grad_norm": 1.971694827079773,
        "learning_rate": 9.125436166438247e-05,
        "epoch": 0.1787979046043562,
        "step": 1297
    },
    {
        "loss": 2.4126,
        "grad_norm": 1.0568253993988037,
        "learning_rate": 9.112449410957056e-05,
        "epoch": 0.17893575958092087,
        "step": 1298
    },
    {
        "loss": 2.1707,
        "grad_norm": 1.3987630605697632,
        "learning_rate": 9.099464164092399e-05,
        "epoch": 0.17907361455748552,
        "step": 1299
    },
    {
        "loss": 1.887,
        "grad_norm": 1.2282878160476685,
        "learning_rate": 9.086480447915968e-05,
        "epoch": 0.17921146953405018,
        "step": 1300
    },
    {
        "loss": 2.0757,
        "grad_norm": 1.8058792352676392,
        "learning_rate": 9.073498284496877e-05,
        "epoch": 0.17934932451061483,
        "step": 1301
    },
    {
        "loss": 2.1782,
        "grad_norm": 1.358892798423767,
        "learning_rate": 9.060517695901595e-05,
        "epoch": 0.1794871794871795,
        "step": 1302
    },
    {
        "loss": 2.3159,
        "grad_norm": 1.6854194402694702,
        "learning_rate": 9.047538704193912e-05,
        "epoch": 0.17962503446374414,
        "step": 1303
    },
    {
        "loss": 1.8289,
        "grad_norm": 1.4512234926223755,
        "learning_rate": 9.0345613314349e-05,
        "epoch": 0.1797628894403088,
        "step": 1304
    },
    {
        "loss": 1.7981,
        "grad_norm": 1.237791657447815,
        "learning_rate": 9.021585599682894e-05,
        "epoch": 0.17990074441687345,
        "step": 1305
    },
    {
        "loss": 2.2975,
        "grad_norm": 1.696038842201233,
        "learning_rate": 9.008611530993416e-05,
        "epoch": 0.1800385993934381,
        "step": 1306
    },
    {
        "loss": 1.9715,
        "grad_norm": 1.2347277402877808,
        "learning_rate": 8.995639147419184e-05,
        "epoch": 0.18017645437000276,
        "step": 1307
    },
    {
        "loss": 2.5033,
        "grad_norm": 1.2574201822280884,
        "learning_rate": 8.982668471010038e-05,
        "epoch": 0.18031430934656742,
        "step": 1308
    },
    {
        "loss": 1.7189,
        "grad_norm": 2.2773327827453613,
        "learning_rate": 8.969699523812919e-05,
        "epoch": 0.18045216432313207,
        "step": 1309
    },
    {
        "loss": 2.4824,
        "grad_norm": 1.2530052661895752,
        "learning_rate": 8.956732327871835e-05,
        "epoch": 0.18059001929969673,
        "step": 1310
    },
    {
        "loss": 1.7636,
        "grad_norm": 1.9614310264587402,
        "learning_rate": 8.943766905227812e-05,
        "epoch": 0.18072787427626139,
        "step": 1311
    },
    {
        "loss": 2.6183,
        "grad_norm": 1.2924268245697021,
        "learning_rate": 8.930803277918858e-05,
        "epoch": 0.18086572925282604,
        "step": 1312
    },
    {
        "loss": 2.1956,
        "grad_norm": 1.6880087852478027,
        "learning_rate": 8.917841467979936e-05,
        "epoch": 0.18100358422939067,
        "step": 1313
    },
    {
        "loss": 2.3086,
        "grad_norm": 1.6002241373062134,
        "learning_rate": 8.904881497442919e-05,
        "epoch": 0.18114143920595532,
        "step": 1314
    },
    {
        "loss": 2.4381,
        "grad_norm": 1.6545203924179077,
        "learning_rate": 8.891923388336552e-05,
        "epoch": 0.18127929418251998,
        "step": 1315
    },
    {
        "loss": 1.3787,
        "grad_norm": 2.108550548553467,
        "learning_rate": 8.87896716268642e-05,
        "epoch": 0.18141714915908463,
        "step": 1316
    },
    {
        "loss": 1.6407,
        "grad_norm": 1.0462769269943237,
        "learning_rate": 8.866012842514886e-05,
        "epoch": 0.1815550041356493,
        "step": 1317
    },
    {
        "loss": 2.3951,
        "grad_norm": 1.6318868398666382,
        "learning_rate": 8.853060449841115e-05,
        "epoch": 0.18169285911221394,
        "step": 1318
    },
    {
        "loss": 2.2799,
        "grad_norm": 1.1074376106262207,
        "learning_rate": 8.840110006680953e-05,
        "epoch": 0.1818307140887786,
        "step": 1319
    },
    {
        "loss": 2.077,
        "grad_norm": 2.315145254135132,
        "learning_rate": 8.827161535046959e-05,
        "epoch": 0.18196856906534326,
        "step": 1320
    },
    {
        "loss": 2.2495,
        "grad_norm": 1.8419240713119507,
        "learning_rate": 8.814215056948331e-05,
        "epoch": 0.1821064240419079,
        "step": 1321
    },
    {
        "loss": 2.181,
        "grad_norm": 2.3779478073120117,
        "learning_rate": 8.801270594390885e-05,
        "epoch": 0.18224427901847257,
        "step": 1322
    },
    {
        "loss": 1.8953,
        "grad_norm": 1.6307330131530762,
        "learning_rate": 8.788328169376994e-05,
        "epoch": 0.18238213399503722,
        "step": 1323
    },
    {
        "loss": 2.2238,
        "grad_norm": 1.2706303596496582,
        "learning_rate": 8.775387803905593e-05,
        "epoch": 0.18251998897160188,
        "step": 1324
    },
    {
        "loss": 2.2676,
        "grad_norm": 1.9227595329284668,
        "learning_rate": 8.762449519972099e-05,
        "epoch": 0.18265784394816653,
        "step": 1325
    },
    {
        "loss": 1.6331,
        "grad_norm": 2.1726858615875244,
        "learning_rate": 8.749513339568393e-05,
        "epoch": 0.1827956989247312,
        "step": 1326
    },
    {
        "loss": 1.8709,
        "grad_norm": 2.3111863136291504,
        "learning_rate": 8.736579284682784e-05,
        "epoch": 0.18293355390129584,
        "step": 1327
    },
    {
        "loss": 2.7404,
        "grad_norm": 1.877618670463562,
        "learning_rate": 8.723647377299969e-05,
        "epoch": 0.1830714088778605,
        "step": 1328
    },
    {
        "loss": 2.6153,
        "grad_norm": 1.341403841972351,
        "learning_rate": 8.710717639400989e-05,
        "epoch": 0.18320926385442515,
        "step": 1329
    },
    {
        "loss": 2.2972,
        "grad_norm": 1.551026463508606,
        "learning_rate": 8.697790092963204e-05,
        "epoch": 0.1833471188309898,
        "step": 1330
    },
    {
        "loss": 1.8193,
        "grad_norm": 1.7779572010040283,
        "learning_rate": 8.684864759960245e-05,
        "epoch": 0.18348497380755446,
        "step": 1331
    },
    {
        "loss": 2.5787,
        "grad_norm": 1.0209895372390747,
        "learning_rate": 8.671941662361985e-05,
        "epoch": 0.18362282878411912,
        "step": 1332
    },
    {
        "loss": 1.7828,
        "grad_norm": 2.3661303520202637,
        "learning_rate": 8.659020822134487e-05,
        "epoch": 0.18376068376068377,
        "step": 1333
    },
    {
        "loss": 1.9238,
        "grad_norm": 1.8110476732254028,
        "learning_rate": 8.64610226123999e-05,
        "epoch": 0.1838985387372484,
        "step": 1334
    },
    {
        "loss": 1.9426,
        "grad_norm": 1.7117891311645508,
        "learning_rate": 8.633186001636858e-05,
        "epoch": 0.18403639371381306,
        "step": 1335
    },
    {
        "loss": 1.8865,
        "grad_norm": 1.408470630645752,
        "learning_rate": 8.620272065279525e-05,
        "epoch": 0.1841742486903777,
        "step": 1336
    },
    {
        "loss": 1.9396,
        "grad_norm": 1.1929612159729004,
        "learning_rate": 8.607360474118501e-05,
        "epoch": 0.18431210366694237,
        "step": 1337
    },
    {
        "loss": 2.0641,
        "grad_norm": 1.1829627752304077,
        "learning_rate": 8.59445125010029e-05,
        "epoch": 0.18444995864350702,
        "step": 1338
    },
    {
        "loss": 2.5443,
        "grad_norm": 1.1402655839920044,
        "learning_rate": 8.581544415167384e-05,
        "epoch": 0.18458781362007168,
        "step": 1339
    },
    {
        "loss": 1.5917,
        "grad_norm": 1.900407075881958,
        "learning_rate": 8.568639991258214e-05,
        "epoch": 0.18472566859663633,
        "step": 1340
    },
    {
        "loss": 2.3068,
        "grad_norm": 2.1015307903289795,
        "learning_rate": 8.555738000307108e-05,
        "epoch": 0.184863523573201,
        "step": 1341
    },
    {
        "loss": 2.3399,
        "grad_norm": 1.4436315298080444,
        "learning_rate": 8.542838464244253e-05,
        "epoch": 0.18500137854976564,
        "step": 1342
    },
    {
        "loss": 1.9757,
        "grad_norm": 2.072446823120117,
        "learning_rate": 8.529941404995682e-05,
        "epoch": 0.1851392335263303,
        "step": 1343
    },
    {
        "loss": 2.0399,
        "grad_norm": 1.484113335609436,
        "learning_rate": 8.517046844483197e-05,
        "epoch": 0.18527708850289495,
        "step": 1344
    },
    {
        "loss": 1.2926,
        "grad_norm": 1.7346470355987549,
        "learning_rate": 8.504154804624366e-05,
        "epoch": 0.1854149434794596,
        "step": 1345
    },
    {
        "loss": 2.1422,
        "grad_norm": 1.7760010957717896,
        "learning_rate": 8.49126530733247e-05,
        "epoch": 0.18555279845602426,
        "step": 1346
    },
    {
        "loss": 2.2843,
        "grad_norm": 1.3074018955230713,
        "learning_rate": 8.478378374516465e-05,
        "epoch": 0.18569065343258892,
        "step": 1347
    },
    {
        "loss": 2.2713,
        "grad_norm": 1.368379831314087,
        "learning_rate": 8.465494028080949e-05,
        "epoch": 0.18582850840915358,
        "step": 1348
    },
    {
        "loss": 2.4168,
        "grad_norm": 1.0521594285964966,
        "learning_rate": 8.45261228992613e-05,
        "epoch": 0.18596636338571823,
        "step": 1349
    },
    {
        "loss": 2.0468,
        "grad_norm": 1.6540851593017578,
        "learning_rate": 8.439733181947771e-05,
        "epoch": 0.18610421836228289,
        "step": 1350
    },
    {
        "loss": 1.7374,
        "grad_norm": 1.871080994606018,
        "learning_rate": 8.426856726037175e-05,
        "epoch": 0.18624207333884754,
        "step": 1351
    },
    {
        "loss": 1.776,
        "grad_norm": 1.5816527605056763,
        "learning_rate": 8.41398294408113e-05,
        "epoch": 0.1863799283154122,
        "step": 1352
    },
    {
        "loss": 1.8559,
        "grad_norm": 1.819442629814148,
        "learning_rate": 8.401111857961887e-05,
        "epoch": 0.18651778329197685,
        "step": 1353
    },
    {
        "loss": 1.6778,
        "grad_norm": 1.9341578483581543,
        "learning_rate": 8.388243489557109e-05,
        "epoch": 0.1866556382685415,
        "step": 1354
    },
    {
        "loss": 2.1309,
        "grad_norm": 1.6718007326126099,
        "learning_rate": 8.375377860739826e-05,
        "epoch": 0.18679349324510613,
        "step": 1355
    },
    {
        "loss": 2.3555,
        "grad_norm": 1.1797497272491455,
        "learning_rate": 8.362514993378448e-05,
        "epoch": 0.1869313482216708,
        "step": 1356
    },
    {
        "loss": 2.052,
        "grad_norm": 1.0831162929534912,
        "learning_rate": 8.349654909336652e-05,
        "epoch": 0.18706920319823545,
        "step": 1357
    },
    {
        "loss": 2.7626,
        "grad_norm": 1.06354820728302,
        "learning_rate": 8.336797630473408e-05,
        "epoch": 0.1872070581748001,
        "step": 1358
    },
    {
        "loss": 1.7341,
        "grad_norm": 1.4041844606399536,
        "learning_rate": 8.323943178642907e-05,
        "epoch": 0.18734491315136476,
        "step": 1359
    },
    {
        "loss": 1.8196,
        "grad_norm": 1.8335561752319336,
        "learning_rate": 8.311091575694546e-05,
        "epoch": 0.1874827681279294,
        "step": 1360
    },
    {
        "loss": 1.6857,
        "grad_norm": 1.7713578939437866,
        "learning_rate": 8.29824284347286e-05,
        "epoch": 0.18762062310449407,
        "step": 1361
    },
    {
        "loss": 2.2029,
        "grad_norm": 1.6260851621627808,
        "learning_rate": 8.285397003817526e-05,
        "epoch": 0.18775847808105872,
        "step": 1362
    },
    {
        "loss": 2.3251,
        "grad_norm": 1.44706130027771,
        "learning_rate": 8.272554078563293e-05,
        "epoch": 0.18789633305762338,
        "step": 1363
    },
    {
        "loss": 1.4509,
        "grad_norm": 1.7807369232177734,
        "learning_rate": 8.259714089539954e-05,
        "epoch": 0.18803418803418803,
        "step": 1364
    },
    {
        "loss": 2.2379,
        "grad_norm": 1.0159564018249512,
        "learning_rate": 8.246877058572317e-05,
        "epoch": 0.1881720430107527,
        "step": 1365
    },
    {
        "loss": 2.0706,
        "grad_norm": 1.1799211502075195,
        "learning_rate": 8.234043007480163e-05,
        "epoch": 0.18830989798731734,
        "step": 1366
    },
    {
        "loss": 1.7034,
        "grad_norm": 1.701697826385498,
        "learning_rate": 8.221211958078205e-05,
        "epoch": 0.188447752963882,
        "step": 1367
    },
    {
        "loss": 1.8011,
        "grad_norm": 2.4052317142486572,
        "learning_rate": 8.20838393217605e-05,
        "epoch": 0.18858560794044665,
        "step": 1368
    },
    {
        "loss": 2.4978,
        "grad_norm": 1.2913413047790527,
        "learning_rate": 8.195558951578173e-05,
        "epoch": 0.1887234629170113,
        "step": 1369
    },
    {
        "loss": 1.8378,
        "grad_norm": 1.5844610929489136,
        "learning_rate": 8.182737038083873e-05,
        "epoch": 0.18886131789357596,
        "step": 1370
    },
    {
        "loss": 1.9153,
        "grad_norm": 1.4968039989471436,
        "learning_rate": 8.169918213487223e-05,
        "epoch": 0.18899917287014062,
        "step": 1371
    },
    {
        "loss": 2.149,
        "grad_norm": 1.8749691247940063,
        "learning_rate": 8.157102499577061e-05,
        "epoch": 0.18913702784670527,
        "step": 1372
    },
    {
        "loss": 2.2695,
        "grad_norm": 1.8214794397354126,
        "learning_rate": 8.144289918136936e-05,
        "epoch": 0.18927488282326993,
        "step": 1373
    },
    {
        "loss": 2.0615,
        "grad_norm": 1.6385825872421265,
        "learning_rate": 8.131480490945056e-05,
        "epoch": 0.18941273779983459,
        "step": 1374
    },
    {
        "loss": 2.1899,
        "grad_norm": 1.5778319835662842,
        "learning_rate": 8.118674239774294e-05,
        "epoch": 0.18955059277639924,
        "step": 1375
    },
    {
        "loss": 2.0676,
        "grad_norm": 1.436696171760559,
        "learning_rate": 8.105871186392097e-05,
        "epoch": 0.1896884477529639,
        "step": 1376
    },
    {
        "loss": 2.1194,
        "grad_norm": 1.1695069074630737,
        "learning_rate": 8.093071352560496e-05,
        "epoch": 0.18982630272952852,
        "step": 1377
    },
    {
        "loss": 1.3744,
        "grad_norm": 2.2726800441741943,
        "learning_rate": 8.080274760036043e-05,
        "epoch": 0.18996415770609318,
        "step": 1378
    },
    {
        "loss": 2.4464,
        "grad_norm": 1.7774147987365723,
        "learning_rate": 8.067481430569786e-05,
        "epoch": 0.19010201268265783,
        "step": 1379
    },
    {
        "loss": 1.9018,
        "grad_norm": 1.7790995836257935,
        "learning_rate": 8.054691385907209e-05,
        "epoch": 0.1902398676592225,
        "step": 1380
    },
    {
        "loss": 2.2149,
        "grad_norm": 1.8011385202407837,
        "learning_rate": 8.041904647788239e-05,
        "epoch": 0.19037772263578714,
        "step": 1381
    },
    {
        "loss": 2.3479,
        "grad_norm": 2.041377544403076,
        "learning_rate": 8.029121237947157e-05,
        "epoch": 0.1905155776123518,
        "step": 1382
    },
    {
        "loss": 2.2594,
        "grad_norm": 1.4567196369171143,
        "learning_rate": 8.016341178112604e-05,
        "epoch": 0.19065343258891645,
        "step": 1383
    },
    {
        "loss": 2.0293,
        "grad_norm": 1.5730136632919312,
        "learning_rate": 8.003564490007525e-05,
        "epoch": 0.1907912875654811,
        "step": 1384
    },
    {
        "loss": 1.965,
        "grad_norm": 1.8672101497650146,
        "learning_rate": 7.990791195349123e-05,
        "epoch": 0.19092914254204577,
        "step": 1385
    },
    {
        "loss": 2.3413,
        "grad_norm": 1.3982309103012085,
        "learning_rate": 7.978021315848844e-05,
        "epoch": 0.19106699751861042,
        "step": 1386
    },
    {
        "loss": 1.9899,
        "grad_norm": 1.429363489151001,
        "learning_rate": 7.965254873212327e-05,
        "epoch": 0.19120485249517508,
        "step": 1387
    },
    {
        "loss": 1.9157,
        "grad_norm": 1.5403293371200562,
        "learning_rate": 7.952491889139365e-05,
        "epoch": 0.19134270747173973,
        "step": 1388
    },
    {
        "loss": 2.2918,
        "grad_norm": 2.212636947631836,
        "learning_rate": 7.939732385323875e-05,
        "epoch": 0.1914805624483044,
        "step": 1389
    },
    {
        "loss": 2.3237,
        "grad_norm": 1.4758961200714111,
        "learning_rate": 7.926976383453859e-05,
        "epoch": 0.19161841742486904,
        "step": 1390
    },
    {
        "loss": 1.895,
        "grad_norm": 2.429395914077759,
        "learning_rate": 7.914223905211368e-05,
        "epoch": 0.1917562724014337,
        "step": 1391
    },
    {
        "loss": 1.9611,
        "grad_norm": 2.1416714191436768,
        "learning_rate": 7.901474972272459e-05,
        "epoch": 0.19189412737799835,
        "step": 1392
    },
    {
        "loss": 2.054,
        "grad_norm": 1.6609143018722534,
        "learning_rate": 7.888729606307162e-05,
        "epoch": 0.192031982354563,
        "step": 1393
    },
    {
        "loss": 2.1105,
        "grad_norm": 1.634136438369751,
        "learning_rate": 7.875987828979457e-05,
        "epoch": 0.19216983733112766,
        "step": 1394
    },
    {
        "loss": 1.9086,
        "grad_norm": 1.4614055156707764,
        "learning_rate": 7.863249661947204e-05,
        "epoch": 0.19230769230769232,
        "step": 1395
    },
    {
        "loss": 1.857,
        "grad_norm": 1.4898083209991455,
        "learning_rate": 7.850515126862143e-05,
        "epoch": 0.19244554728425697,
        "step": 1396
    },
    {
        "loss": 2.2619,
        "grad_norm": 1.6381230354309082,
        "learning_rate": 7.837784245369834e-05,
        "epoch": 0.19258340226082163,
        "step": 1397
    },
    {
        "loss": 2.654,
        "grad_norm": 1.1882480382919312,
        "learning_rate": 7.825057039109628e-05,
        "epoch": 0.19272125723738626,
        "step": 1398
    },
    {
        "loss": 2.9032,
        "grad_norm": 1.1954134702682495,
        "learning_rate": 7.81233352971462e-05,
        "epoch": 0.1928591122139509,
        "step": 1399
    },
    {
        "loss": 2.0062,
        "grad_norm": 1.6429316997528076,
        "learning_rate": 7.799613738811646e-05,
        "epoch": 0.19299696719051557,
        "step": 1400
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.0754963159561157,
        "learning_rate": 7.786897688021192e-05,
        "epoch": 0.19313482216708022,
        "step": 1401
    },
    {
        "loss": 1.486,
        "grad_norm": 2.157907247543335,
        "learning_rate": 7.774185398957399e-05,
        "epoch": 0.19327267714364488,
        "step": 1402
    },
    {
        "loss": 2.0823,
        "grad_norm": 1.480858325958252,
        "learning_rate": 7.761476893228027e-05,
        "epoch": 0.19341053212020953,
        "step": 1403
    },
    {
        "loss": 2.2477,
        "grad_norm": 1.18490469455719,
        "learning_rate": 7.748772192434382e-05,
        "epoch": 0.1935483870967742,
        "step": 1404
    },
    {
        "loss": 2.298,
        "grad_norm": 1.2424685955047607,
        "learning_rate": 7.736071318171318e-05,
        "epoch": 0.19368624207333884,
        "step": 1405
    },
    {
        "loss": 2.0971,
        "grad_norm": 1.0509189367294312,
        "learning_rate": 7.723374292027181e-05,
        "epoch": 0.1938240970499035,
        "step": 1406
    },
    {
        "loss": 1.7052,
        "grad_norm": 1.7276310920715332,
        "learning_rate": 7.710681135583772e-05,
        "epoch": 0.19396195202646815,
        "step": 1407
    },
    {
        "loss": 1.2348,
        "grad_norm": 2.0003557205200195,
        "learning_rate": 7.697991870416324e-05,
        "epoch": 0.1940998070030328,
        "step": 1408
    },
    {
        "loss": 2.1641,
        "grad_norm": 1.9574573040008545,
        "learning_rate": 7.685306518093444e-05,
        "epoch": 0.19423766197959746,
        "step": 1409
    },
    {
        "loss": 1.6204,
        "grad_norm": 2.171889543533325,
        "learning_rate": 7.672625100177096e-05,
        "epoch": 0.19437551695616212,
        "step": 1410
    },
    {
        "loss": 2.1735,
        "grad_norm": 1.765924334526062,
        "learning_rate": 7.659947638222561e-05,
        "epoch": 0.19451337193272678,
        "step": 1411
    },
    {
        "loss": 1.8427,
        "grad_norm": 1.6450084447860718,
        "learning_rate": 7.647274153778377e-05,
        "epoch": 0.19465122690929143,
        "step": 1412
    },
    {
        "loss": 1.9052,
        "grad_norm": 0.9746035933494568,
        "learning_rate": 7.634604668386347e-05,
        "epoch": 0.19478908188585609,
        "step": 1413
    },
    {
        "loss": 1.9603,
        "grad_norm": 1.1195114850997925,
        "learning_rate": 7.621939203581452e-05,
        "epoch": 0.19492693686242074,
        "step": 1414
    },
    {
        "loss": 2.1571,
        "grad_norm": 1.1185662746429443,
        "learning_rate": 7.609277780891855e-05,
        "epoch": 0.1950647918389854,
        "step": 1415
    },
    {
        "loss": 2.0444,
        "grad_norm": 2.2409214973449707,
        "learning_rate": 7.596620421838843e-05,
        "epoch": 0.19520264681555005,
        "step": 1416
    },
    {
        "loss": 2.2355,
        "grad_norm": 1.0937690734863281,
        "learning_rate": 7.5839671479368e-05,
        "epoch": 0.1953405017921147,
        "step": 1417
    },
    {
        "loss": 2.5467,
        "grad_norm": 1.236547589302063,
        "learning_rate": 7.571317980693154e-05,
        "epoch": 0.19547835676867936,
        "step": 1418
    },
    {
        "loss": 2.0652,
        "grad_norm": 1.2487224340438843,
        "learning_rate": 7.558672941608373e-05,
        "epoch": 0.195616211745244,
        "step": 1419
    },
    {
        "loss": 2.4152,
        "grad_norm": 1.141726016998291,
        "learning_rate": 7.546032052175886e-05,
        "epoch": 0.19575406672180864,
        "step": 1420
    },
    {
        "loss": 2.396,
        "grad_norm": 1.1501051187515259,
        "learning_rate": 7.533395333882081e-05,
        "epoch": 0.1958919216983733,
        "step": 1421
    },
    {
        "loss": 1.7981,
        "grad_norm": 2.4424476623535156,
        "learning_rate": 7.520762808206265e-05,
        "epoch": 0.19602977667493796,
        "step": 1422
    },
    {
        "loss": 2.4796,
        "grad_norm": 1.4405778646469116,
        "learning_rate": 7.508134496620593e-05,
        "epoch": 0.1961676316515026,
        "step": 1423
    },
    {
        "loss": 1.7647,
        "grad_norm": 2.0341556072235107,
        "learning_rate": 7.495510420590079e-05,
        "epoch": 0.19630548662806727,
        "step": 1424
    },
    {
        "loss": 2.0666,
        "grad_norm": 0.9545177817344666,
        "learning_rate": 7.482890601572532e-05,
        "epoch": 0.19644334160463192,
        "step": 1425
    },
    {
        "loss": 2.6159,
        "grad_norm": 1.4387975931167603,
        "learning_rate": 7.470275061018522e-05,
        "epoch": 0.19658119658119658,
        "step": 1426
    },
    {
        "loss": 1.6762,
        "grad_norm": 1.8015499114990234,
        "learning_rate": 7.457663820371345e-05,
        "epoch": 0.19671905155776123,
        "step": 1427
    },
    {
        "loss": 2.1036,
        "grad_norm": 2.0529959201812744,
        "learning_rate": 7.445056901066997e-05,
        "epoch": 0.1968569065343259,
        "step": 1428
    },
    {
        "loss": 1.3684,
        "grad_norm": 2.239391326904297,
        "learning_rate": 7.432454324534121e-05,
        "epoch": 0.19699476151089054,
        "step": 1429
    },
    {
        "loss": 2.1232,
        "grad_norm": 2.6283016204833984,
        "learning_rate": 7.419856112193982e-05,
        "epoch": 0.1971326164874552,
        "step": 1430
    },
    {
        "loss": 2.1167,
        "grad_norm": 1.6219767332077026,
        "learning_rate": 7.407262285460419e-05,
        "epoch": 0.19727047146401985,
        "step": 1431
    },
    {
        "loss": 1.9986,
        "grad_norm": 1.5779986381530762,
        "learning_rate": 7.394672865739835e-05,
        "epoch": 0.1974083264405845,
        "step": 1432
    },
    {
        "loss": 1.2326,
        "grad_norm": 1.1800187826156616,
        "learning_rate": 7.382087874431118e-05,
        "epoch": 0.19754618141714916,
        "step": 1433
    },
    {
        "loss": 1.9131,
        "grad_norm": 1.3594416379928589,
        "learning_rate": 7.369507332925652e-05,
        "epoch": 0.19768403639371382,
        "step": 1434
    },
    {
        "loss": 2.2448,
        "grad_norm": 0.9845568537712097,
        "learning_rate": 7.356931262607237e-05,
        "epoch": 0.19782189137027847,
        "step": 1435
    },
    {
        "loss": 2.6533,
        "grad_norm": 1.1337143182754517,
        "learning_rate": 7.344359684852092e-05,
        "epoch": 0.19795974634684313,
        "step": 1436
    },
    {
        "loss": 2.0227,
        "grad_norm": 1.062011480331421,
        "learning_rate": 7.331792621028776e-05,
        "epoch": 0.19809760132340778,
        "step": 1437
    },
    {
        "loss": 1.9585,
        "grad_norm": 1.6574362516403198,
        "learning_rate": 7.31923009249821e-05,
        "epoch": 0.19823545629997244,
        "step": 1438
    },
    {
        "loss": 1.9967,
        "grad_norm": 1.1915639638900757,
        "learning_rate": 7.306672120613571e-05,
        "epoch": 0.1983733112765371,
        "step": 1439
    },
    {
        "loss": 2.4667,
        "grad_norm": 1.4427357912063599,
        "learning_rate": 7.294118726720307e-05,
        "epoch": 0.19851116625310175,
        "step": 1440
    },
    {
        "loss": 2.3093,
        "grad_norm": 1.5052825212478638,
        "learning_rate": 7.281569932156093e-05,
        "epoch": 0.19864902122966638,
        "step": 1441
    },
    {
        "loss": 2.4763,
        "grad_norm": 1.7032550573349,
        "learning_rate": 7.269025758250767e-05,
        "epoch": 0.19878687620623103,
        "step": 1442
    },
    {
        "loss": 2.0525,
        "grad_norm": 0.9929137229919434,
        "learning_rate": 7.256486226326328e-05,
        "epoch": 0.1989247311827957,
        "step": 1443
    },
    {
        "loss": 2.2466,
        "grad_norm": 1.527769684791565,
        "learning_rate": 7.243951357696874e-05,
        "epoch": 0.19906258615936034,
        "step": 1444
    },
    {
        "loss": 1.3779,
        "grad_norm": 2.1221375465393066,
        "learning_rate": 7.231421173668587e-05,
        "epoch": 0.199200441135925,
        "step": 1445
    },
    {
        "loss": 2.1062,
        "grad_norm": 1.5502084493637085,
        "learning_rate": 7.218895695539681e-05,
        "epoch": 0.19933829611248965,
        "step": 1446
    },
    {
        "loss": 2.0935,
        "grad_norm": 1.0189381837844849,
        "learning_rate": 7.206374944600369e-05,
        "epoch": 0.1994761510890543,
        "step": 1447
    },
    {
        "loss": 1.3314,
        "grad_norm": 1.653111219406128,
        "learning_rate": 7.193858942132832e-05,
        "epoch": 0.19961400606561897,
        "step": 1448
    },
    {
        "loss": 2.1335,
        "grad_norm": 1.187874436378479,
        "learning_rate": 7.181347709411184e-05,
        "epoch": 0.19975186104218362,
        "step": 1449
    },
    {
        "loss": 1.6317,
        "grad_norm": 1.8648045063018799,
        "learning_rate": 7.168841267701417e-05,
        "epoch": 0.19988971601874828,
        "step": 1450
    },
    {
        "loss": 1.7114,
        "grad_norm": 1.5753766298294067,
        "learning_rate": 7.156339638261398e-05,
        "epoch": 0.20002757099531293,
        "step": 1451
    },
    {
        "loss": 2.5272,
        "grad_norm": 1.449336290359497,
        "learning_rate": 7.143842842340803e-05,
        "epoch": 0.2001654259718776,
        "step": 1452
    },
    {
        "loss": 2.387,
        "grad_norm": 1.1700469255447388,
        "learning_rate": 7.13135090118109e-05,
        "epoch": 0.20030328094844224,
        "step": 1453
    },
    {
        "loss": 1.9513,
        "grad_norm": 1.1175339221954346,
        "learning_rate": 7.118863836015479e-05,
        "epoch": 0.2004411359250069,
        "step": 1454
    },
    {
        "loss": 2.1742,
        "grad_norm": 1.4410959482192993,
        "learning_rate": 7.106381668068889e-05,
        "epoch": 0.20057899090157155,
        "step": 1455
    },
    {
        "loss": 1.7071,
        "grad_norm": 1.8518736362457275,
        "learning_rate": 7.093904418557911e-05,
        "epoch": 0.2007168458781362,
        "step": 1456
    },
    {
        "loss": 2.117,
        "grad_norm": 1.5569201707839966,
        "learning_rate": 7.081432108690795e-05,
        "epoch": 0.20085470085470086,
        "step": 1457
    },
    {
        "loss": 2.3173,
        "grad_norm": 1.5058954954147339,
        "learning_rate": 7.068964759667382e-05,
        "epoch": 0.20099255583126552,
        "step": 1458
    },
    {
        "loss": 2.1356,
        "grad_norm": 1.1998240947723389,
        "learning_rate": 7.056502392679074e-05,
        "epoch": 0.20113041080783017,
        "step": 1459
    },
    {
        "loss": 2.2146,
        "grad_norm": 1.7729454040527344,
        "learning_rate": 7.044045028908825e-05,
        "epoch": 0.20126826578439483,
        "step": 1460
    },
    {
        "loss": 2.6097,
        "grad_norm": 1.272351861000061,
        "learning_rate": 7.031592689531063e-05,
        "epoch": 0.20140612076095948,
        "step": 1461
    },
    {
        "loss": 1.8107,
        "grad_norm": 1.7317826747894287,
        "learning_rate": 7.01914539571169e-05,
        "epoch": 0.2015439757375241,
        "step": 1462
    },
    {
        "loss": 1.6029,
        "grad_norm": 2.1446309089660645,
        "learning_rate": 7.006703168608025e-05,
        "epoch": 0.20168183071408877,
        "step": 1463
    },
    {
        "loss": 1.733,
        "grad_norm": 2.1994686126708984,
        "learning_rate": 6.994266029368784e-05,
        "epoch": 0.20181968569065342,
        "step": 1464
    },
    {
        "loss": 1.1816,
        "grad_norm": 1.911832332611084,
        "learning_rate": 6.981833999134018e-05,
        "epoch": 0.20195754066721808,
        "step": 1465
    },
    {
        "loss": 2.3243,
        "grad_norm": 1.8151443004608154,
        "learning_rate": 6.969407099035112e-05,
        "epoch": 0.20209539564378273,
        "step": 1466
    },
    {
        "loss": 2.4317,
        "grad_norm": 0.9954354763031006,
        "learning_rate": 6.956985350194719e-05,
        "epoch": 0.2022332506203474,
        "step": 1467
    },
    {
        "loss": 2.344,
        "grad_norm": 1.4261637926101685,
        "learning_rate": 6.944568773726747e-05,
        "epoch": 0.20237110559691204,
        "step": 1468
    },
    {
        "loss": 1.8808,
        "grad_norm": 1.5319387912750244,
        "learning_rate": 6.932157390736294e-05,
        "epoch": 0.2025089605734767,
        "step": 1469
    },
    {
        "loss": 2.0283,
        "grad_norm": 1.5096155405044556,
        "learning_rate": 6.919751222319657e-05,
        "epoch": 0.20264681555004135,
        "step": 1470
    },
    {
        "loss": 1.8086,
        "grad_norm": 2.2886745929718018,
        "learning_rate": 6.90735028956424e-05,
        "epoch": 0.202784670526606,
        "step": 1471
    },
    {
        "loss": 2.6889,
        "grad_norm": 0.8608628511428833,
        "learning_rate": 6.894954613548568e-05,
        "epoch": 0.20292252550317066,
        "step": 1472
    },
    {
        "loss": 2.3824,
        "grad_norm": 1.4063224792480469,
        "learning_rate": 6.882564215342228e-05,
        "epoch": 0.20306038047973532,
        "step": 1473
    },
    {
        "loss": 2.3746,
        "grad_norm": 1.8105385303497314,
        "learning_rate": 6.870179116005832e-05,
        "epoch": 0.20319823545629997,
        "step": 1474
    },
    {
        "loss": 1.6651,
        "grad_norm": 1.7620173692703247,
        "learning_rate": 6.857799336590976e-05,
        "epoch": 0.20333609043286463,
        "step": 1475
    },
    {
        "loss": 1.0587,
        "grad_norm": 2.1747031211853027,
        "learning_rate": 6.845424898140238e-05,
        "epoch": 0.20347394540942929,
        "step": 1476
    },
    {
        "loss": 1.8431,
        "grad_norm": 1.8345993757247925,
        "learning_rate": 6.833055821687098e-05,
        "epoch": 0.20361180038599394,
        "step": 1477
    },
    {
        "loss": 2.375,
        "grad_norm": 1.1795564889907837,
        "learning_rate": 6.820692128255922e-05,
        "epoch": 0.2037496553625586,
        "step": 1478
    },
    {
        "loss": 1.8492,
        "grad_norm": 1.4432448148727417,
        "learning_rate": 6.808333838861943e-05,
        "epoch": 0.20388751033912325,
        "step": 1479
    },
    {
        "loss": 2.6919,
        "grad_norm": 1.1944016218185425,
        "learning_rate": 6.795980974511187e-05,
        "epoch": 0.2040253653156879,
        "step": 1480
    },
    {
        "loss": 2.5011,
        "grad_norm": 1.8090523481369019,
        "learning_rate": 6.783633556200473e-05,
        "epoch": 0.20416322029225256,
        "step": 1481
    },
    {
        "loss": 2.0943,
        "grad_norm": 1.2471107244491577,
        "learning_rate": 6.771291604917356e-05,
        "epoch": 0.20430107526881722,
        "step": 1482
    },
    {
        "loss": 2.1248,
        "grad_norm": 1.253777265548706,
        "learning_rate": 6.758955141640102e-05,
        "epoch": 0.20443893024538184,
        "step": 1483
    },
    {
        "loss": 1.9697,
        "grad_norm": 1.6977707147598267,
        "learning_rate": 6.746624187337649e-05,
        "epoch": 0.2045767852219465,
        "step": 1484
    },
    {
        "loss": 1.8015,
        "grad_norm": 1.8217347860336304,
        "learning_rate": 6.734298762969572e-05,
        "epoch": 0.20471464019851116,
        "step": 1485
    },
    {
        "loss": 1.9683,
        "grad_norm": 1.3793256282806396,
        "learning_rate": 6.721978889486043e-05,
        "epoch": 0.2048524951750758,
        "step": 1486
    },
    {
        "loss": 2.509,
        "grad_norm": 1.3432400226593018,
        "learning_rate": 6.709664587827802e-05,
        "epoch": 0.20499035015164047,
        "step": 1487
    },
    {
        "loss": 2.0449,
        "grad_norm": 1.5163644552230835,
        "learning_rate": 6.697355878926108e-05,
        "epoch": 0.20512820512820512,
        "step": 1488
    },
    {
        "loss": 2.2759,
        "grad_norm": 1.2790755033493042,
        "learning_rate": 6.685052783702734e-05,
        "epoch": 0.20526606010476978,
        "step": 1489
    },
    {
        "loss": 2.0127,
        "grad_norm": 1.555550456047058,
        "learning_rate": 6.672755323069895e-05,
        "epoch": 0.20540391508133443,
        "step": 1490
    },
    {
        "loss": 2.2886,
        "grad_norm": 1.7489349842071533,
        "learning_rate": 6.66046351793023e-05,
        "epoch": 0.2055417700578991,
        "step": 1491
    },
    {
        "loss": 1.7815,
        "grad_norm": 2.016537666320801,
        "learning_rate": 6.64817738917677e-05,
        "epoch": 0.20567962503446374,
        "step": 1492
    },
    {
        "loss": 1.2228,
        "grad_norm": 2.1071431636810303,
        "learning_rate": 6.635896957692899e-05,
        "epoch": 0.2058174800110284,
        "step": 1493
    },
    {
        "loss": 2.0086,
        "grad_norm": 1.6271576881408691,
        "learning_rate": 6.623622244352312e-05,
        "epoch": 0.20595533498759305,
        "step": 1494
    },
    {
        "loss": 2.3448,
        "grad_norm": 1.1354429721832275,
        "learning_rate": 6.611353270018982e-05,
        "epoch": 0.2060931899641577,
        "step": 1495
    },
    {
        "loss": 2.4224,
        "grad_norm": 1.120901107788086,
        "learning_rate": 6.599090055547142e-05,
        "epoch": 0.20623104494072236,
        "step": 1496
    },
    {
        "loss": 1.4339,
        "grad_norm": 1.5520625114440918,
        "learning_rate": 6.58683262178121e-05,
        "epoch": 0.20636889991728702,
        "step": 1497
    },
    {
        "loss": 2.2862,
        "grad_norm": 1.1113495826721191,
        "learning_rate": 6.574580989555804e-05,
        "epoch": 0.20650675489385167,
        "step": 1498
    },
    {
        "loss": 2.0091,
        "grad_norm": 0.9673463106155396,
        "learning_rate": 6.562335179695663e-05,
        "epoch": 0.20664460987041633,
        "step": 1499
    },
    {
        "loss": 1.7152,
        "grad_norm": 1.988515853881836,
        "learning_rate": 6.550095213015637e-05,
        "epoch": 0.20678246484698098,
        "step": 1500
    },
    {
        "loss": 2.0939,
        "grad_norm": 1.2634409666061401,
        "learning_rate": 6.537861110320642e-05,
        "epoch": 0.20692031982354564,
        "step": 1501
    },
    {
        "loss": 1.4932,
        "grad_norm": 2.854362964630127,
        "learning_rate": 6.525632892405631e-05,
        "epoch": 0.2070581748001103,
        "step": 1502
    },
    {
        "loss": 1.778,
        "grad_norm": 1.9996155500411987,
        "learning_rate": 6.513410580055545e-05,
        "epoch": 0.20719602977667495,
        "step": 1503
    },
    {
        "loss": 2.2389,
        "grad_norm": 1.035082221031189,
        "learning_rate": 6.501194194045296e-05,
        "epoch": 0.2073338847532396,
        "step": 1504
    },
    {
        "loss": 2.2763,
        "grad_norm": 1.1259589195251465,
        "learning_rate": 6.488983755139718e-05,
        "epoch": 0.20747173972980423,
        "step": 1505
    },
    {
        "loss": 2.0569,
        "grad_norm": 2.0978240966796875,
        "learning_rate": 6.476779284093544e-05,
        "epoch": 0.2076095947063689,
        "step": 1506
    },
    {
        "loss": 2.2596,
        "grad_norm": 1.2040832042694092,
        "learning_rate": 6.464580801651343e-05,
        "epoch": 0.20774744968293354,
        "step": 1507
    },
    {
        "loss": 1.6115,
        "grad_norm": 2.2308623790740967,
        "learning_rate": 6.452388328547534e-05,
        "epoch": 0.2078853046594982,
        "step": 1508
    },
    {
        "loss": 2.3919,
        "grad_norm": 1.1388965845108032,
        "learning_rate": 6.440201885506296e-05,
        "epoch": 0.20802315963606285,
        "step": 1509
    },
    {
        "loss": 1.8021,
        "grad_norm": 1.2086584568023682,
        "learning_rate": 6.428021493241571e-05,
        "epoch": 0.2081610146126275,
        "step": 1510
    },
    {
        "loss": 2.3604,
        "grad_norm": 0.859358012676239,
        "learning_rate": 6.415847172457016e-05,
        "epoch": 0.20829886958919216,
        "step": 1511
    },
    {
        "loss": 1.0844,
        "grad_norm": 1.3131425380706787,
        "learning_rate": 6.403678943845964e-05,
        "epoch": 0.20843672456575682,
        "step": 1512
    },
    {
        "loss": 1.7386,
        "grad_norm": 1.5528526306152344,
        "learning_rate": 6.391516828091393e-05,
        "epoch": 0.20857457954232148,
        "step": 1513
    },
    {
        "loss": 2.1163,
        "grad_norm": 2.0541250705718994,
        "learning_rate": 6.379360845865896e-05,
        "epoch": 0.20871243451888613,
        "step": 1514
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.6236183643341064,
        "learning_rate": 6.367211017831638e-05,
        "epoch": 0.20885028949545079,
        "step": 1515
    },
    {
        "loss": 2.0831,
        "grad_norm": 1.5741864442825317,
        "learning_rate": 6.355067364640315e-05,
        "epoch": 0.20898814447201544,
        "step": 1516
    },
    {
        "loss": 1.4838,
        "grad_norm": 1.8935818672180176,
        "learning_rate": 6.342929906933144e-05,
        "epoch": 0.2091259994485801,
        "step": 1517
    },
    {
        "loss": 2.0427,
        "grad_norm": 1.5363487005233765,
        "learning_rate": 6.330798665340791e-05,
        "epoch": 0.20926385442514475,
        "step": 1518
    },
    {
        "loss": 2.3307,
        "grad_norm": 2.153144598007202,
        "learning_rate": 6.318673660483376e-05,
        "epoch": 0.2094017094017094,
        "step": 1519
    },
    {
        "loss": 1.8433,
        "grad_norm": 1.3557153940200806,
        "learning_rate": 6.306554912970404e-05,
        "epoch": 0.20953956437827406,
        "step": 1520
    },
    {
        "loss": 2.517,
        "grad_norm": 2.1304121017456055,
        "learning_rate": 6.29444244340075e-05,
        "epoch": 0.20967741935483872,
        "step": 1521
    },
    {
        "loss": 2.1767,
        "grad_norm": 1.732112169265747,
        "learning_rate": 6.282336272362616e-05,
        "epoch": 0.20981527433140337,
        "step": 1522
    },
    {
        "loss": 2.3761,
        "grad_norm": 1.1044007539749146,
        "learning_rate": 6.270236420433505e-05,
        "epoch": 0.20995312930796803,
        "step": 1523
    },
    {
        "loss": 2.4699,
        "grad_norm": 1.5322386026382446,
        "learning_rate": 6.258142908180165e-05,
        "epoch": 0.21009098428453268,
        "step": 1524
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.3858084678649902,
        "learning_rate": 6.246055756158584e-05,
        "epoch": 0.21022883926109734,
        "step": 1525
    },
    {
        "loss": 2.0364,
        "grad_norm": 2.17095947265625,
        "learning_rate": 6.23397498491392e-05,
        "epoch": 0.21036669423766197,
        "step": 1526
    },
    {
        "loss": 1.6361,
        "grad_norm": 1.4394381046295166,
        "learning_rate": 6.221900614980513e-05,
        "epoch": 0.21050454921422662,
        "step": 1527
    },
    {
        "loss": 1.5943,
        "grad_norm": 1.8904733657836914,
        "learning_rate": 6.209832666881795e-05,
        "epoch": 0.21064240419079128,
        "step": 1528
    },
    {
        "loss": 1.8165,
        "grad_norm": 2.1310830116271973,
        "learning_rate": 6.197771161130288e-05,
        "epoch": 0.21078025916735593,
        "step": 1529
    },
    {
        "loss": 1.3712,
        "grad_norm": 1.2455705404281616,
        "learning_rate": 6.185716118227587e-05,
        "epoch": 0.2109181141439206,
        "step": 1530
    },
    {
        "loss": 2.0152,
        "grad_norm": 1.5767486095428467,
        "learning_rate": 6.173667558664269e-05,
        "epoch": 0.21105596912048524,
        "step": 1531
    },
    {
        "loss": 2.2325,
        "grad_norm": 1.4810456037521362,
        "learning_rate": 6.161625502919914e-05,
        "epoch": 0.2111938240970499,
        "step": 1532
    },
    {
        "loss": 1.9524,
        "grad_norm": 1.3167657852172852,
        "learning_rate": 6.149589971463032e-05,
        "epoch": 0.21133167907361455,
        "step": 1533
    },
    {
        "loss": 1.959,
        "grad_norm": 1.0711431503295898,
        "learning_rate": 6.137560984751059e-05,
        "epoch": 0.2114695340501792,
        "step": 1534
    },
    {
        "loss": 1.5101,
        "grad_norm": 1.8114237785339355,
        "learning_rate": 6.125538563230286e-05,
        "epoch": 0.21160738902674386,
        "step": 1535
    },
    {
        "loss": 2.1774,
        "grad_norm": 1.2685052156448364,
        "learning_rate": 6.113522727335869e-05,
        "epoch": 0.21174524400330852,
        "step": 1536
    },
    {
        "loss": 2.1686,
        "grad_norm": 1.3294041156768799,
        "learning_rate": 6.101513497491751e-05,
        "epoch": 0.21188309897987317,
        "step": 1537
    },
    {
        "loss": 2.3968,
        "grad_norm": 1.867587924003601,
        "learning_rate": 6.089510894110653e-05,
        "epoch": 0.21202095395643783,
        "step": 1538
    },
    {
        "loss": 1.3709,
        "grad_norm": 2.3006441593170166,
        "learning_rate": 6.0775149375940335e-05,
        "epoch": 0.21215880893300249,
        "step": 1539
    },
    {
        "loss": 2.1934,
        "grad_norm": 1.9725642204284668,
        "learning_rate": 6.065525648332053e-05,
        "epoch": 0.21229666390956714,
        "step": 1540
    },
    {
        "loss": 1.9898,
        "grad_norm": 1.4922198057174683,
        "learning_rate": 6.053543046703539e-05,
        "epoch": 0.2124345188861318,
        "step": 1541
    },
    {
        "loss": 2.1913,
        "grad_norm": 1.3432601690292358,
        "learning_rate": 6.041567153075949e-05,
        "epoch": 0.21257237386269645,
        "step": 1542
    },
    {
        "loss": 2.3507,
        "grad_norm": 1.0572271347045898,
        "learning_rate": 6.0295979878053424e-05,
        "epoch": 0.2127102288392611,
        "step": 1543
    },
    {
        "loss": 2.411,
        "grad_norm": 1.4757905006408691,
        "learning_rate": 6.017635571236344e-05,
        "epoch": 0.21284808381582576,
        "step": 1544
    },
    {
        "loss": 2.5104,
        "grad_norm": 1.2336885929107666,
        "learning_rate": 6.005679923702092e-05,
        "epoch": 0.21298593879239042,
        "step": 1545
    },
    {
        "loss": 2.0856,
        "grad_norm": 1.2116507291793823,
        "learning_rate": 5.993731065524246e-05,
        "epoch": 0.21312379376895507,
        "step": 1546
    },
    {
        "loss": 2.308,
        "grad_norm": 1.330613613128662,
        "learning_rate": 5.9817890170129e-05,
        "epoch": 0.2132616487455197,
        "step": 1547
    },
    {
        "loss": 1.8444,
        "grad_norm": 1.6719454526901245,
        "learning_rate": 5.9698537984665825e-05,
        "epoch": 0.21339950372208435,
        "step": 1548
    },
    {
        "loss": 2.4001,
        "grad_norm": 1.2347412109375,
        "learning_rate": 5.9579254301722245e-05,
        "epoch": 0.213537358698649,
        "step": 1549
    },
    {
        "loss": 1.875,
        "grad_norm": 1.3956249952316284,
        "learning_rate": 5.946003932405089e-05,
        "epoch": 0.21367521367521367,
        "step": 1550
    },
    {
        "loss": 1.8602,
        "grad_norm": 2.329691171646118,
        "learning_rate": 5.934089325428782e-05,
        "epoch": 0.21381306865177832,
        "step": 1551
    },
    {
        "loss": 1.5815,
        "grad_norm": 2.2100772857666016,
        "learning_rate": 5.922181629495187e-05,
        "epoch": 0.21395092362834298,
        "step": 1552
    },
    {
        "loss": 2.3571,
        "grad_norm": 1.6193324327468872,
        "learning_rate": 5.910280864844446e-05,
        "epoch": 0.21408877860490763,
        "step": 1553
    },
    {
        "loss": 1.8936,
        "grad_norm": 1.9081960916519165,
        "learning_rate": 5.898387051704909e-05,
        "epoch": 0.2142266335814723,
        "step": 1554
    },
    {
        "loss": 2.0084,
        "grad_norm": 2.0793206691741943,
        "learning_rate": 5.8865002102931266e-05,
        "epoch": 0.21436448855803694,
        "step": 1555
    },
    {
        "loss": 2.1149,
        "grad_norm": 1.5962464809417725,
        "learning_rate": 5.874620360813779e-05,
        "epoch": 0.2145023435346016,
        "step": 1556
    },
    {
        "loss": 2.082,
        "grad_norm": 0.9035237431526184,
        "learning_rate": 5.86274752345968e-05,
        "epoch": 0.21464019851116625,
        "step": 1557
    },
    {
        "loss": 1.9197,
        "grad_norm": 1.7199944257736206,
        "learning_rate": 5.850881718411716e-05,
        "epoch": 0.2147780534877309,
        "step": 1558
    },
    {
        "loss": 2.3184,
        "grad_norm": 1.4475654363632202,
        "learning_rate": 5.839022965838819e-05,
        "epoch": 0.21491590846429556,
        "step": 1559
    },
    {
        "loss": 2.0417,
        "grad_norm": 1.8434176445007324,
        "learning_rate": 5.827171285897936e-05,
        "epoch": 0.21505376344086022,
        "step": 1560
    },
    {
        "loss": 1.5663,
        "grad_norm": 1.9124853610992432,
        "learning_rate": 5.815326698733996e-05,
        "epoch": 0.21519161841742487,
        "step": 1561
    },
    {
        "loss": 1.9527,
        "grad_norm": 1.5755146741867065,
        "learning_rate": 5.803489224479862e-05,
        "epoch": 0.21532947339398953,
        "step": 1562
    },
    {
        "loss": 1.7019,
        "grad_norm": 1.530734896659851,
        "learning_rate": 5.7916588832563254e-05,
        "epoch": 0.21546732837055418,
        "step": 1563
    },
    {
        "loss": 1.7259,
        "grad_norm": 1.6605817079544067,
        "learning_rate": 5.7798356951720234e-05,
        "epoch": 0.21560518334711884,
        "step": 1564
    },
    {
        "loss": 1.7758,
        "grad_norm": 1.6637837886810303,
        "learning_rate": 5.768019680323464e-05,
        "epoch": 0.2157430383236835,
        "step": 1565
    },
    {
        "loss": 2.2559,
        "grad_norm": 0.9386260509490967,
        "learning_rate": 5.756210858794949e-05,
        "epoch": 0.21588089330024815,
        "step": 1566
    },
    {
        "loss": 2.1771,
        "grad_norm": 1.5351933240890503,
        "learning_rate": 5.744409250658549e-05,
        "epoch": 0.2160187482768128,
        "step": 1567
    },
    {
        "loss": 2.3243,
        "grad_norm": 1.0051817893981934,
        "learning_rate": 5.7326148759740874e-05,
        "epoch": 0.21615660325337746,
        "step": 1568
    },
    {
        "loss": 1.6016,
        "grad_norm": 1.6958751678466797,
        "learning_rate": 5.720827754789081e-05,
        "epoch": 0.2162944582299421,
        "step": 1569
    },
    {
        "loss": 1.8051,
        "grad_norm": 2.0096616744995117,
        "learning_rate": 5.709047907138717e-05,
        "epoch": 0.21643231320650674,
        "step": 1570
    },
    {
        "loss": 2.5502,
        "grad_norm": 1.3637851476669312,
        "learning_rate": 5.697275353045831e-05,
        "epoch": 0.2165701681830714,
        "step": 1571
    },
    {
        "loss": 2.1974,
        "grad_norm": 1.3975952863693237,
        "learning_rate": 5.685510112520851e-05,
        "epoch": 0.21670802315963605,
        "step": 1572
    },
    {
        "loss": 2.57,
        "grad_norm": 1.2225450277328491,
        "learning_rate": 5.673752205561771e-05,
        "epoch": 0.2168458781362007,
        "step": 1573
    },
    {
        "loss": 2.1998,
        "grad_norm": 1.112660527229309,
        "learning_rate": 5.662001652154139e-05,
        "epoch": 0.21698373311276536,
        "step": 1574
    },
    {
        "loss": 2.4275,
        "grad_norm": 1.8755134344100952,
        "learning_rate": 5.650258472270972e-05,
        "epoch": 0.21712158808933002,
        "step": 1575
    },
    {
        "loss": 2.187,
        "grad_norm": 2.236140727996826,
        "learning_rate": 5.6385226858727844e-05,
        "epoch": 0.21725944306589468,
        "step": 1576
    },
    {
        "loss": 1.2863,
        "grad_norm": 1.9312160015106201,
        "learning_rate": 5.62679431290751e-05,
        "epoch": 0.21739729804245933,
        "step": 1577
    },
    {
        "loss": 1.5742,
        "grad_norm": 1.8946776390075684,
        "learning_rate": 5.615073373310474e-05,
        "epoch": 0.21753515301902399,
        "step": 1578
    },
    {
        "loss": 2.1002,
        "grad_norm": 1.3507264852523804,
        "learning_rate": 5.603359887004387e-05,
        "epoch": 0.21767300799558864,
        "step": 1579
    },
    {
        "loss": 1.1804,
        "grad_norm": 1.9421881437301636,
        "learning_rate": 5.5916538738992685e-05,
        "epoch": 0.2178108629721533,
        "step": 1580
    },
    {
        "loss": 1.5975,
        "grad_norm": 1.7957112789154053,
        "learning_rate": 5.579955353892456e-05,
        "epoch": 0.21794871794871795,
        "step": 1581
    },
    {
        "loss": 1.4306,
        "grad_norm": 2.23313570022583,
        "learning_rate": 5.568264346868539e-05,
        "epoch": 0.2180865729252826,
        "step": 1582
    },
    {
        "loss": 1.6148,
        "grad_norm": 1.8942787647247314,
        "learning_rate": 5.556580872699325e-05,
        "epoch": 0.21822442790184726,
        "step": 1583
    },
    {
        "loss": 2.3147,
        "grad_norm": 1.2972012758255005,
        "learning_rate": 5.544904951243848e-05,
        "epoch": 0.21836228287841192,
        "step": 1584
    },
    {
        "loss": 1.9557,
        "grad_norm": 1.5432032346725464,
        "learning_rate": 5.5332366023482815e-05,
        "epoch": 0.21850013785497657,
        "step": 1585
    },
    {
        "loss": 1.4175,
        "grad_norm": 2.0290699005126953,
        "learning_rate": 5.521575845845928e-05,
        "epoch": 0.21863799283154123,
        "step": 1586
    },
    {
        "loss": 1.8079,
        "grad_norm": 1.4237146377563477,
        "learning_rate": 5.509922701557202e-05,
        "epoch": 0.21877584780810588,
        "step": 1587
    },
    {
        "loss": 2.333,
        "grad_norm": 1.448702096939087,
        "learning_rate": 5.498277189289551e-05,
        "epoch": 0.21891370278467054,
        "step": 1588
    },
    {
        "loss": 2.0232,
        "grad_norm": 1.6436738967895508,
        "learning_rate": 5.4866393288374794e-05,
        "epoch": 0.2190515577612352,
        "step": 1589
    },
    {
        "loss": 1.3771,
        "grad_norm": 1.7591840028762817,
        "learning_rate": 5.4750091399824655e-05,
        "epoch": 0.21918941273779982,
        "step": 1590
    },
    {
        "loss": 2.0333,
        "grad_norm": 2.074840545654297,
        "learning_rate": 5.463386642492958e-05,
        "epoch": 0.21932726771436448,
        "step": 1591
    },
    {
        "loss": 1.8481,
        "grad_norm": 1.7684109210968018,
        "learning_rate": 5.451771856124322e-05,
        "epoch": 0.21946512269092913,
        "step": 1592
    },
    {
        "loss": 1.5055,
        "grad_norm": 1.6946135759353638,
        "learning_rate": 5.4401648006188323e-05,
        "epoch": 0.2196029776674938,
        "step": 1593
    },
    {
        "loss": 1.8452,
        "grad_norm": 1.7005380392074585,
        "learning_rate": 5.428565495705605e-05,
        "epoch": 0.21974083264405844,
        "step": 1594
    },
    {
        "loss": 1.4459,
        "grad_norm": 2.0109567642211914,
        "learning_rate": 5.4169739611005954e-05,
        "epoch": 0.2198786876206231,
        "step": 1595
    },
    {
        "loss": 1.2034,
        "grad_norm": 1.992351770401001,
        "learning_rate": 5.4053902165065376e-05,
        "epoch": 0.22001654259718775,
        "step": 1596
    },
    {
        "loss": 2.2688,
        "grad_norm": 1.6367934942245483,
        "learning_rate": 5.393814281612943e-05,
        "epoch": 0.2201543975737524,
        "step": 1597
    },
    {
        "loss": 2.4531,
        "grad_norm": 1.7993512153625488,
        "learning_rate": 5.382246176096034e-05,
        "epoch": 0.22029225255031706,
        "step": 1598
    },
    {
        "loss": 1.9987,
        "grad_norm": 2.1525583267211914,
        "learning_rate": 5.370685919618725e-05,
        "epoch": 0.22043010752688172,
        "step": 1599
    },
    {
        "loss": 1.8493,
        "grad_norm": 1.594671607017517,
        "learning_rate": 5.359133531830602e-05,
        "epoch": 0.22056796250344637,
        "step": 1600
    },
    {
        "loss": 2.2632,
        "grad_norm": 1.234407901763916,
        "learning_rate": 5.347589032367863e-05,
        "epoch": 0.22070581748001103,
        "step": 1601
    },
    {
        "loss": 1.6852,
        "grad_norm": 1.615613341331482,
        "learning_rate": 5.336052440853301e-05,
        "epoch": 0.22084367245657568,
        "step": 1602
    },
    {
        "loss": 1.9269,
        "grad_norm": 1.4429371356964111,
        "learning_rate": 5.324523776896271e-05,
        "epoch": 0.22098152743314034,
        "step": 1603
    },
    {
        "loss": 2.4762,
        "grad_norm": 1.4512817859649658,
        "learning_rate": 5.313003060092652e-05,
        "epoch": 0.221119382409705,
        "step": 1604
    },
    {
        "loss": 1.4057,
        "grad_norm": 2.377925395965576,
        "learning_rate": 5.301490310024808e-05,
        "epoch": 0.22125723738626965,
        "step": 1605
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.4159547090530396,
        "learning_rate": 5.289985546261573e-05,
        "epoch": 0.2213950923628343,
        "step": 1606
    },
    {
        "loss": 1.903,
        "grad_norm": 1.2827290296554565,
        "learning_rate": 5.2784887883582e-05,
        "epoch": 0.22153294733939896,
        "step": 1607
    },
    {
        "loss": 2.1704,
        "grad_norm": 2.1289432048797607,
        "learning_rate": 5.2670000558563336e-05,
        "epoch": 0.22167080231596362,
        "step": 1608
    },
    {
        "loss": 2.305,
        "grad_norm": 1.4332844018936157,
        "learning_rate": 5.255519368283973e-05,
        "epoch": 0.22180865729252827,
        "step": 1609
    },
    {
        "loss": 0.9564,
        "grad_norm": 1.289290189743042,
        "learning_rate": 5.2440467451554596e-05,
        "epoch": 0.22194651226909293,
        "step": 1610
    },
    {
        "loss": 1.8362,
        "grad_norm": 2.1730191707611084,
        "learning_rate": 5.232582205971403e-05,
        "epoch": 0.22208436724565755,
        "step": 1611
    },
    {
        "loss": 2.3738,
        "grad_norm": 1.8788511753082275,
        "learning_rate": 5.221125770218692e-05,
        "epoch": 0.2222222222222222,
        "step": 1612
    },
    {
        "loss": 1.4582,
        "grad_norm": 1.8092509508132935,
        "learning_rate": 5.209677457370434e-05,
        "epoch": 0.22236007719878687,
        "step": 1613
    },
    {
        "loss": 1.8749,
        "grad_norm": 2.074826240539551,
        "learning_rate": 5.1982372868859254e-05,
        "epoch": 0.22249793217535152,
        "step": 1614
    },
    {
        "loss": 2.0512,
        "grad_norm": 1.9811497926712036,
        "learning_rate": 5.186805278210632e-05,
        "epoch": 0.22263578715191618,
        "step": 1615
    },
    {
        "loss": 2.3745,
        "grad_norm": 0.9793375134468079,
        "learning_rate": 5.175381450776142e-05,
        "epoch": 0.22277364212848083,
        "step": 1616
    },
    {
        "loss": 2.5369,
        "grad_norm": 1.6888058185577393,
        "learning_rate": 5.163965824000131e-05,
        "epoch": 0.2229114971050455,
        "step": 1617
    },
    {
        "loss": 0.9512,
        "grad_norm": 1.7184371948242188,
        "learning_rate": 5.152558417286348e-05,
        "epoch": 0.22304935208161014,
        "step": 1618
    },
    {
        "loss": 2.16,
        "grad_norm": 1.5738492012023926,
        "learning_rate": 5.141159250024563e-05,
        "epoch": 0.2231872070581748,
        "step": 1619
    },
    {
        "loss": 1.5055,
        "grad_norm": 1.913466453552246,
        "learning_rate": 5.129768341590536e-05,
        "epoch": 0.22332506203473945,
        "step": 1620
    },
    {
        "loss": 1.554,
        "grad_norm": 1.9521589279174805,
        "learning_rate": 5.118385711346002e-05,
        "epoch": 0.2234629170113041,
        "step": 1621
    },
    {
        "loss": 2.1458,
        "grad_norm": 1.5761045217514038,
        "learning_rate": 5.107011378638613e-05,
        "epoch": 0.22360077198786876,
        "step": 1622
    },
    {
        "loss": 1.3301,
        "grad_norm": 1.5078996419906616,
        "learning_rate": 5.095645362801924e-05,
        "epoch": 0.22373862696443342,
        "step": 1623
    },
    {
        "loss": 1.4426,
        "grad_norm": 2.246853828430176,
        "learning_rate": 5.084287683155346e-05,
        "epoch": 0.22387648194099807,
        "step": 1624
    },
    {
        "loss": 1.995,
        "grad_norm": 1.7841076850891113,
        "learning_rate": 5.072938359004134e-05,
        "epoch": 0.22401433691756273,
        "step": 1625
    },
    {
        "loss": 2.3009,
        "grad_norm": 1.1000930070877075,
        "learning_rate": 5.0615974096393285e-05,
        "epoch": 0.22415219189412738,
        "step": 1626
    },
    {
        "loss": 1.8714,
        "grad_norm": 1.8106962442398071,
        "learning_rate": 5.050264854337734e-05,
        "epoch": 0.22429004687069204,
        "step": 1627
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.1556711196899414,
        "learning_rate": 5.038940712361904e-05,
        "epoch": 0.2244279018472567,
        "step": 1628
    },
    {
        "loss": 1.7901,
        "grad_norm": 2.4637210369110107,
        "learning_rate": 5.0276250029600734e-05,
        "epoch": 0.22456575682382135,
        "step": 1629
    },
    {
        "loss": 1.9438,
        "grad_norm": 2.0374953746795654,
        "learning_rate": 5.016317745366139e-05,
        "epoch": 0.224703611800386,
        "step": 1630
    },
    {
        "loss": 2.2055,
        "grad_norm": 1.5045161247253418,
        "learning_rate": 5.005018958799665e-05,
        "epoch": 0.22484146677695066,
        "step": 1631
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.8985768556594849,
        "learning_rate": 4.993728662465775e-05,
        "epoch": 0.22497932175351532,
        "step": 1632
    },
    {
        "loss": 1.8723,
        "grad_norm": 2.2634360790252686,
        "learning_rate": 4.982446875555192e-05,
        "epoch": 0.22511717673007994,
        "step": 1633
    },
    {
        "loss": 2.8492,
        "grad_norm": 1.1304649114608765,
        "learning_rate": 4.97117361724416e-05,
        "epoch": 0.2252550317066446,
        "step": 1634
    },
    {
        "loss": 2.1046,
        "grad_norm": 1.3954437971115112,
        "learning_rate": 4.9599089066944274e-05,
        "epoch": 0.22539288668320925,
        "step": 1635
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.233292818069458,
        "learning_rate": 4.948652763053223e-05,
        "epoch": 0.2255307416597739,
        "step": 1636
    },
    {
        "loss": 2.4356,
        "grad_norm": 1.173807978630066,
        "learning_rate": 4.937405205453206e-05,
        "epoch": 0.22566859663633856,
        "step": 1637
    },
    {
        "loss": 2.4276,
        "grad_norm": 1.1181066036224365,
        "learning_rate": 4.9261662530124365e-05,
        "epoch": 0.22580645161290322,
        "step": 1638
    },
    {
        "loss": 2.0053,
        "grad_norm": 1.7334660291671753,
        "learning_rate": 4.914935924834363e-05,
        "epoch": 0.22594430658946787,
        "step": 1639
    },
    {
        "loss": 2.5178,
        "grad_norm": 0.8601929545402527,
        "learning_rate": 4.903714240007763e-05,
        "epoch": 0.22608216156603253,
        "step": 1640
    },
    {
        "loss": 1.3797,
        "grad_norm": 2.62634539604187,
        "learning_rate": 4.892501217606722e-05,
        "epoch": 0.22622001654259719,
        "step": 1641
    },
    {
        "loss": 2.1834,
        "grad_norm": 1.0355513095855713,
        "learning_rate": 4.8812968766906166e-05,
        "epoch": 0.22635787151916184,
        "step": 1642
    },
    {
        "loss": 1.9447,
        "grad_norm": 1.8024051189422607,
        "learning_rate": 4.870101236304039e-05,
        "epoch": 0.2264957264957265,
        "step": 1643
    },
    {
        "loss": 1.7678,
        "grad_norm": 1.7971374988555908,
        "learning_rate": 4.858914315476822e-05,
        "epoch": 0.22663358147229115,
        "step": 1644
    },
    {
        "loss": 2.132,
        "grad_norm": 1.649608850479126,
        "learning_rate": 4.8477361332239625e-05,
        "epoch": 0.2267714364488558,
        "step": 1645
    },
    {
        "loss": 2.3987,
        "grad_norm": 1.2416325807571411,
        "learning_rate": 4.8365667085456e-05,
        "epoch": 0.22690929142542046,
        "step": 1646
    },
    {
        "loss": 1.8578,
        "grad_norm": 1.3666497468948364,
        "learning_rate": 4.8254060604270025e-05,
        "epoch": 0.22704714640198512,
        "step": 1647
    },
    {
        "loss": 1.9364,
        "grad_norm": 1.7469167709350586,
        "learning_rate": 4.814254207838509e-05,
        "epoch": 0.22718500137854977,
        "step": 1648
    },
    {
        "loss": 2.0569,
        "grad_norm": 1.625309944152832,
        "learning_rate": 4.8031111697355055e-05,
        "epoch": 0.22732285635511443,
        "step": 1649
    },
    {
        "loss": 0.9724,
        "grad_norm": 2.358611822128296,
        "learning_rate": 4.791976965058416e-05,
        "epoch": 0.22746071133167908,
        "step": 1650
    },
    {
        "loss": 2.0826,
        "grad_norm": 1.8031998872756958,
        "learning_rate": 4.780851612732616e-05,
        "epoch": 0.22759856630824374,
        "step": 1651
    },
    {
        "loss": 2.1152,
        "grad_norm": 1.7808594703674316,
        "learning_rate": 4.769735131668467e-05,
        "epoch": 0.2277364212848084,
        "step": 1652
    },
    {
        "loss": 2.4707,
        "grad_norm": 0.9657763838768005,
        "learning_rate": 4.758627540761231e-05,
        "epoch": 0.22787427626137305,
        "step": 1653
    },
    {
        "loss": 2.6609,
        "grad_norm": 1.4688739776611328,
        "learning_rate": 4.747528858891072e-05,
        "epoch": 0.22801213123793768,
        "step": 1654
    },
    {
        "loss": 2.5755,
        "grad_norm": 1.200636863708496,
        "learning_rate": 4.736439104923001e-05,
        "epoch": 0.22814998621450233,
        "step": 1655
    },
    {
        "loss": 2.0928,
        "grad_norm": 1.287654161453247,
        "learning_rate": 4.725358297706853e-05,
        "epoch": 0.228287841191067,
        "step": 1656
    },
    {
        "loss": 1.1554,
        "grad_norm": 1.984292984008789,
        "learning_rate": 4.71428645607727e-05,
        "epoch": 0.22842569616763164,
        "step": 1657
    },
    {
        "loss": 2.3594,
        "grad_norm": 1.613779902458191,
        "learning_rate": 4.70322359885364e-05,
        "epoch": 0.2285635511441963,
        "step": 1658
    },
    {
        "loss": 2.0283,
        "grad_norm": 1.474865436553955,
        "learning_rate": 4.692169744840079e-05,
        "epoch": 0.22870140612076095,
        "step": 1659
    },
    {
        "loss": 2.5453,
        "grad_norm": 1.4506677389144897,
        "learning_rate": 4.6811249128254155e-05,
        "epoch": 0.2288392610973256,
        "step": 1660
    },
    {
        "loss": 1.9507,
        "grad_norm": 1.0644607543945312,
        "learning_rate": 4.670089121583128e-05,
        "epoch": 0.22897711607389026,
        "step": 1661
    },
    {
        "loss": 2.4579,
        "grad_norm": 1.7662280797958374,
        "learning_rate": 4.6590623898713284e-05,
        "epoch": 0.22911497105045492,
        "step": 1662
    },
    {
        "loss": 1.7809,
        "grad_norm": 1.7976582050323486,
        "learning_rate": 4.648044736432745e-05,
        "epoch": 0.22925282602701957,
        "step": 1663
    },
    {
        "loss": 1.8945,
        "grad_norm": 1.5620090961456299,
        "learning_rate": 4.637036179994651e-05,
        "epoch": 0.22939068100358423,
        "step": 1664
    },
    {
        "loss": 2.1603,
        "grad_norm": 1.2036694288253784,
        "learning_rate": 4.626036739268879e-05,
        "epoch": 0.22952853598014888,
        "step": 1665
    },
    {
        "loss": 2.021,
        "grad_norm": 1.94547700881958,
        "learning_rate": 4.615046432951756e-05,
        "epoch": 0.22966639095671354,
        "step": 1666
    },
    {
        "loss": 2.1299,
        "grad_norm": 1.151776909828186,
        "learning_rate": 4.6040652797240844e-05,
        "epoch": 0.2298042459332782,
        "step": 1667
    },
    {
        "loss": 1.7855,
        "grad_norm": 1.1725165843963623,
        "learning_rate": 4.5930932982511055e-05,
        "epoch": 0.22994210090984285,
        "step": 1668
    },
    {
        "loss": 1.8298,
        "grad_norm": 1.8522922992706299,
        "learning_rate": 4.5821305071824824e-05,
        "epoch": 0.2300799558864075,
        "step": 1669
    },
    {
        "loss": 2.3897,
        "grad_norm": 1.5880765914916992,
        "learning_rate": 4.571176925152245e-05,
        "epoch": 0.23021781086297216,
        "step": 1670
    },
    {
        "loss": 2.4525,
        "grad_norm": 1.0940264463424683,
        "learning_rate": 4.5602325707787755e-05,
        "epoch": 0.23035566583953682,
        "step": 1671
    },
    {
        "loss": 2.4515,
        "grad_norm": 1.873382329940796,
        "learning_rate": 4.5492974626647634e-05,
        "epoch": 0.23049352081610147,
        "step": 1672
    },
    {
        "loss": 2.0553,
        "grad_norm": 1.6438268423080444,
        "learning_rate": 4.538371619397197e-05,
        "epoch": 0.23063137579266613,
        "step": 1673
    },
    {
        "loss": 1.7955,
        "grad_norm": 2.4068005084991455,
        "learning_rate": 4.527455059547305e-05,
        "epoch": 0.23076923076923078,
        "step": 1674
    },
    {
        "loss": 2.1293,
        "grad_norm": 1.6793512105941772,
        "learning_rate": 4.516547801670537e-05,
        "epoch": 0.2309070857457954,
        "step": 1675
    },
    {
        "loss": 1.6204,
        "grad_norm": 1.790635347366333,
        "learning_rate": 4.5056498643065393e-05,
        "epoch": 0.23104494072236006,
        "step": 1676
    },
    {
        "loss": 2.3805,
        "grad_norm": 1.2266908884048462,
        "learning_rate": 4.494761265979105e-05,
        "epoch": 0.23118279569892472,
        "step": 1677
    },
    {
        "loss": 1.6762,
        "grad_norm": 1.9465079307556152,
        "learning_rate": 4.483882025196164e-05,
        "epoch": 0.23132065067548938,
        "step": 1678
    },
    {
        "loss": 1.9096,
        "grad_norm": 1.0632623434066772,
        "learning_rate": 4.473012160449733e-05,
        "epoch": 0.23145850565205403,
        "step": 1679
    },
    {
        "loss": 2.292,
        "grad_norm": 1.3090996742248535,
        "learning_rate": 4.462151690215895e-05,
        "epoch": 0.23159636062861869,
        "step": 1680
    },
    {
        "loss": 1.5647,
        "grad_norm": 2.3991751670837402,
        "learning_rate": 4.451300632954759e-05,
        "epoch": 0.23173421560518334,
        "step": 1681
    },
    {
        "loss": 1.956,
        "grad_norm": 1.788469910621643,
        "learning_rate": 4.4404590071104456e-05,
        "epoch": 0.231872070581748,
        "step": 1682
    },
    {
        "loss": 2.2704,
        "grad_norm": 1.745227336883545,
        "learning_rate": 4.4296268311110354e-05,
        "epoch": 0.23200992555831265,
        "step": 1683
    },
    {
        "loss": 2.169,
        "grad_norm": 1.6113799810409546,
        "learning_rate": 4.41880412336855e-05,
        "epoch": 0.2321477805348773,
        "step": 1684
    },
    {
        "loss": 2.0956,
        "grad_norm": 1.4483823776245117,
        "learning_rate": 4.407990902278909e-05,
        "epoch": 0.23228563551144196,
        "step": 1685
    },
    {
        "loss": 2.0648,
        "grad_norm": 1.1332238912582397,
        "learning_rate": 4.397187186221929e-05,
        "epoch": 0.23242349048800662,
        "step": 1686
    },
    {
        "loss": 2.1536,
        "grad_norm": 1.9308093786239624,
        "learning_rate": 4.386392993561239e-05,
        "epoch": 0.23256134546457127,
        "step": 1687
    },
    {
        "loss": 1.7567,
        "grad_norm": 1.8012694120407104,
        "learning_rate": 4.375608342644309e-05,
        "epoch": 0.23269920044113593,
        "step": 1688
    },
    {
        "loss": 2.2583,
        "grad_norm": 1.1751909255981445,
        "learning_rate": 4.364833251802375e-05,
        "epoch": 0.23283705541770058,
        "step": 1689
    },
    {
        "loss": 1.695,
        "grad_norm": 1.687153697013855,
        "learning_rate": 4.3540677393504224e-05,
        "epoch": 0.23297491039426524,
        "step": 1690
    },
    {
        "loss": 1.144,
        "grad_norm": 2.900839328765869,
        "learning_rate": 4.34331182358717e-05,
        "epoch": 0.2331127653708299,
        "step": 1691
    },
    {
        "loss": 2.5817,
        "grad_norm": 1.9942481517791748,
        "learning_rate": 4.332565522795008e-05,
        "epoch": 0.23325062034739455,
        "step": 1692
    },
    {
        "loss": 2.0531,
        "grad_norm": 1.271316409111023,
        "learning_rate": 4.3218288552399875e-05,
        "epoch": 0.2333884753239592,
        "step": 1693
    },
    {
        "loss": 2.2296,
        "grad_norm": 1.6596364974975586,
        "learning_rate": 4.3111018391717986e-05,
        "epoch": 0.23352633030052386,
        "step": 1694
    },
    {
        "loss": 1.819,
        "grad_norm": 1.9426069259643555,
        "learning_rate": 4.300384492823708e-05,
        "epoch": 0.23366418527708852,
        "step": 1695
    },
    {
        "loss": 1.9625,
        "grad_norm": 1.9660911560058594,
        "learning_rate": 4.289676834412554e-05,
        "epoch": 0.23380204025365317,
        "step": 1696
    },
    {
        "loss": 2.0847,
        "grad_norm": 1.4451382160186768,
        "learning_rate": 4.2789788821387136e-05,
        "epoch": 0.2339398952302178,
        "step": 1697
    },
    {
        "loss": 2.1472,
        "grad_norm": 1.0258985757827759,
        "learning_rate": 4.2682906541860536e-05,
        "epoch": 0.23407775020678245,
        "step": 1698
    },
    {
        "loss": 2.3285,
        "grad_norm": 1.2019630670547485,
        "learning_rate": 4.257612168721928e-05,
        "epoch": 0.2342156051833471,
        "step": 1699
    },
    {
        "loss": 1.7493,
        "grad_norm": 1.668205976486206,
        "learning_rate": 4.24694344389711e-05,
        "epoch": 0.23435346015991176,
        "step": 1700
    },
    {
        "loss": 2.0696,
        "grad_norm": 2.009633779525757,
        "learning_rate": 4.236284497845802e-05,
        "epoch": 0.23449131513647642,
        "step": 1701
    },
    {
        "loss": 2.0864,
        "grad_norm": 1.5335662364959717,
        "learning_rate": 4.225635348685573e-05,
        "epoch": 0.23462917011304107,
        "step": 1702
    },
    {
        "loss": 2.0803,
        "grad_norm": 2.1952903270721436,
        "learning_rate": 4.214996014517341e-05,
        "epoch": 0.23476702508960573,
        "step": 1703
    },
    {
        "loss": 2.0554,
        "grad_norm": 1.6326658725738525,
        "learning_rate": 4.2043665134253485e-05,
        "epoch": 0.23490488006617039,
        "step": 1704
    },
    {
        "loss": 1.934,
        "grad_norm": 1.9025516510009766,
        "learning_rate": 4.193746863477116e-05,
        "epoch": 0.23504273504273504,
        "step": 1705
    },
    {
        "loss": 2.4839,
        "grad_norm": 1.6298913955688477,
        "learning_rate": 4.1831370827234194e-05,
        "epoch": 0.2351805900192997,
        "step": 1706
    },
    {
        "loss": 2.2386,
        "grad_norm": 1.4015724658966064,
        "learning_rate": 4.1725371891982735e-05,
        "epoch": 0.23531844499586435,
        "step": 1707
    },
    {
        "loss": 1.7933,
        "grad_norm": 1.6807893514633179,
        "learning_rate": 4.16194720091886e-05,
        "epoch": 0.235456299972429,
        "step": 1708
    },
    {
        "loss": 2.2074,
        "grad_norm": 1.4612998962402344,
        "learning_rate": 4.151367135885552e-05,
        "epoch": 0.23559415494899366,
        "step": 1709
    },
    {
        "loss": 1.7762,
        "grad_norm": 1.7173515558242798,
        "learning_rate": 4.1407970120818406e-05,
        "epoch": 0.23573200992555832,
        "step": 1710
    },
    {
        "loss": 1.8757,
        "grad_norm": 2.416719675064087,
        "learning_rate": 4.130236847474318e-05,
        "epoch": 0.23586986490212297,
        "step": 1711
    },
    {
        "loss": 2.4148,
        "grad_norm": 0.999359667301178,
        "learning_rate": 4.11968666001266e-05,
        "epoch": 0.23600771987868763,
        "step": 1712
    },
    {
        "loss": 2.0515,
        "grad_norm": 1.1571648120880127,
        "learning_rate": 4.109146467629574e-05,
        "epoch": 0.23614557485525228,
        "step": 1713
    },
    {
        "loss": 1.6873,
        "grad_norm": 1.8467295169830322,
        "learning_rate": 4.0986162882407754e-05,
        "epoch": 0.23628342983181694,
        "step": 1714
    },
    {
        "loss": 2.0072,
        "grad_norm": 1.7871288061141968,
        "learning_rate": 4.0880961397449736e-05,
        "epoch": 0.2364212848083816,
        "step": 1715
    },
    {
        "loss": 2.6666,
        "grad_norm": 1.05000638961792,
        "learning_rate": 4.077586040023815e-05,
        "epoch": 0.23655913978494625,
        "step": 1716
    },
    {
        "loss": 2.1069,
        "grad_norm": 1.51707923412323,
        "learning_rate": 4.067086006941867e-05,
        "epoch": 0.2366969947615109,
        "step": 1717
    },
    {
        "loss": 1.5284,
        "grad_norm": 2.0587351322174072,
        "learning_rate": 4.0565960583466e-05,
        "epoch": 0.23683484973807553,
        "step": 1718
    },
    {
        "loss": 2.3501,
        "grad_norm": 1.1287089586257935,
        "learning_rate": 4.046116212068315e-05,
        "epoch": 0.2369727047146402,
        "step": 1719
    },
    {
        "loss": 2.1024,
        "grad_norm": 1.4874743223190308,
        "learning_rate": 4.0356464859201816e-05,
        "epoch": 0.23711055969120484,
        "step": 1720
    },
    {
        "loss": 1.9481,
        "grad_norm": 1.912656545639038,
        "learning_rate": 4.025186897698128e-05,
        "epoch": 0.2372484146677695,
        "step": 1721
    },
    {
        "loss": 2.5019,
        "grad_norm": 1.137939691543579,
        "learning_rate": 4.014737465180871e-05,
        "epoch": 0.23738626964433415,
        "step": 1722
    },
    {
        "loss": 1.8429,
        "grad_norm": 1.950588345527649,
        "learning_rate": 4.004298206129866e-05,
        "epoch": 0.2375241246208988,
        "step": 1723
    },
    {
        "loss": 2.3635,
        "grad_norm": 0.845773458480835,
        "learning_rate": 3.993869138289271e-05,
        "epoch": 0.23766197959746346,
        "step": 1724
    },
    {
        "loss": 2.0765,
        "grad_norm": 2.108952522277832,
        "learning_rate": 3.9834502793859185e-05,
        "epoch": 0.23779983457402812,
        "step": 1725
    },
    {
        "loss": 1.279,
        "grad_norm": 1.9719951152801514,
        "learning_rate": 3.973041647129297e-05,
        "epoch": 0.23793768955059277,
        "step": 1726
    },
    {
        "loss": 1.7184,
        "grad_norm": 1.9187439680099487,
        "learning_rate": 3.962643259211507e-05,
        "epoch": 0.23807554452715743,
        "step": 1727
    },
    {
        "loss": 2.3084,
        "grad_norm": 1.4570194482803345,
        "learning_rate": 3.952255133307234e-05,
        "epoch": 0.23821339950372208,
        "step": 1728
    },
    {
        "loss": 2.3615,
        "grad_norm": 1.297855257987976,
        "learning_rate": 3.941877287073722e-05,
        "epoch": 0.23835125448028674,
        "step": 1729
    },
    {
        "loss": 1.8184,
        "grad_norm": 1.8432399034500122,
        "learning_rate": 3.931509738150748e-05,
        "epoch": 0.2384891094568514,
        "step": 1730
    },
    {
        "loss": 2.3422,
        "grad_norm": 1.325514793395996,
        "learning_rate": 3.921152504160579e-05,
        "epoch": 0.23862696443341605,
        "step": 1731
    },
    {
        "loss": 1.5302,
        "grad_norm": 2.2314841747283936,
        "learning_rate": 3.9108056027079486e-05,
        "epoch": 0.2387648194099807,
        "step": 1732
    },
    {
        "loss": 1.8436,
        "grad_norm": 1.7069358825683594,
        "learning_rate": 3.900469051380038e-05,
        "epoch": 0.23890267438654536,
        "step": 1733
    },
    {
        "loss": 2.3667,
        "grad_norm": 1.1329394578933716,
        "learning_rate": 3.8901428677464225e-05,
        "epoch": 0.23904052936311002,
        "step": 1734
    },
    {
        "loss": 1.6066,
        "grad_norm": 1.5570814609527588,
        "learning_rate": 3.87982706935906e-05,
        "epoch": 0.23917838433967467,
        "step": 1735
    },
    {
        "loss": 1.6723,
        "grad_norm": 1.6346757411956787,
        "learning_rate": 3.869521673752263e-05,
        "epoch": 0.23931623931623933,
        "step": 1736
    },
    {
        "loss": 2.257,
        "grad_norm": 1.218078374862671,
        "learning_rate": 3.8592266984426516e-05,
        "epoch": 0.23945409429280398,
        "step": 1737
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.9482572078704834,
        "learning_rate": 3.848942160929134e-05,
        "epoch": 0.23959194926936864,
        "step": 1738
    },
    {
        "loss": 1.5348,
        "grad_norm": 1.9931437969207764,
        "learning_rate": 3.838668078692893e-05,
        "epoch": 0.23972980424593326,
        "step": 1739
    },
    {
        "loss": 2.1286,
        "grad_norm": 1.7197037935256958,
        "learning_rate": 3.828404469197313e-05,
        "epoch": 0.23986765922249792,
        "step": 1740
    },
    {
        "loss": 1.429,
        "grad_norm": 2.0076427459716797,
        "learning_rate": 3.8181513498880026e-05,
        "epoch": 0.24000551419906258,
        "step": 1741
    },
    {
        "loss": 1.9711,
        "grad_norm": 2.011869430541992,
        "learning_rate": 3.807908738192725e-05,
        "epoch": 0.24014336917562723,
        "step": 1742
    },
    {
        "loss": 2.3994,
        "grad_norm": 1.4163405895233154,
        "learning_rate": 3.797676651521388e-05,
        "epoch": 0.24028122415219189,
        "step": 1743
    },
    {
        "loss": 2.021,
        "grad_norm": 1.3749371767044067,
        "learning_rate": 3.7874551072660056e-05,
        "epoch": 0.24041907912875654,
        "step": 1744
    },
    {
        "loss": 1.5151,
        "grad_norm": 1.4287759065628052,
        "learning_rate": 3.777244122800682e-05,
        "epoch": 0.2405569341053212,
        "step": 1745
    },
    {
        "loss": 2.3162,
        "grad_norm": 1.878249168395996,
        "learning_rate": 3.7670437154815606e-05,
        "epoch": 0.24069478908188585,
        "step": 1746
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.8864282369613647,
        "learning_rate": 3.756853902646811e-05,
        "epoch": 0.2408326440584505,
        "step": 1747
    },
    {
        "loss": 2.1497,
        "grad_norm": 1.7520605325698853,
        "learning_rate": 3.7466747016166003e-05,
        "epoch": 0.24097049903501516,
        "step": 1748
    },
    {
        "loss": 2.1654,
        "grad_norm": 1.5603327751159668,
        "learning_rate": 3.73650612969305e-05,
        "epoch": 0.24110835401157982,
        "step": 1749
    },
    {
        "loss": 2.2893,
        "grad_norm": 2.1400344371795654,
        "learning_rate": 3.726348204160215e-05,
        "epoch": 0.24124620898814447,
        "step": 1750
    },
    {
        "loss": 2.3492,
        "grad_norm": 1.6028672456741333,
        "learning_rate": 3.716200942284063e-05,
        "epoch": 0.24138406396470913,
        "step": 1751
    },
    {
        "loss": 2.1763,
        "grad_norm": 1.0641978979110718,
        "learning_rate": 3.706064361312427e-05,
        "epoch": 0.24152191894127378,
        "step": 1752
    },
    {
        "loss": 2.2396,
        "grad_norm": 1.3166708946228027,
        "learning_rate": 3.695938478474984e-05,
        "epoch": 0.24165977391783844,
        "step": 1753
    },
    {
        "loss": 2.2319,
        "grad_norm": 1.993654727935791,
        "learning_rate": 3.685823310983237e-05,
        "epoch": 0.2417976288944031,
        "step": 1754
    },
    {
        "loss": 1.1229,
        "grad_norm": 1.6122804880142212,
        "learning_rate": 3.675718876030468e-05,
        "epoch": 0.24193548387096775,
        "step": 1755
    },
    {
        "loss": 2.3846,
        "grad_norm": 1.3897387981414795,
        "learning_rate": 3.665625190791716e-05,
        "epoch": 0.2420733388475324,
        "step": 1756
    },
    {
        "loss": 1.8045,
        "grad_norm": 1.6435548067092896,
        "learning_rate": 3.6555422724237476e-05,
        "epoch": 0.24221119382409706,
        "step": 1757
    },
    {
        "loss": 1.9958,
        "grad_norm": 1.478090763092041,
        "learning_rate": 3.645470138065038e-05,
        "epoch": 0.24234904880066171,
        "step": 1758
    },
    {
        "loss": 1.4382,
        "grad_norm": 1.9701257944107056,
        "learning_rate": 3.635408804835719e-05,
        "epoch": 0.24248690377722637,
        "step": 1759
    },
    {
        "loss": 2.0934,
        "grad_norm": 1.1324595212936401,
        "learning_rate": 3.625358289837571e-05,
        "epoch": 0.24262475875379103,
        "step": 1760
    },
    {
        "loss": 1.452,
        "grad_norm": 1.5750133991241455,
        "learning_rate": 3.6153186101539816e-05,
        "epoch": 0.24276261373035565,
        "step": 1761
    },
    {
        "loss": 2.2558,
        "grad_norm": 1.3873697519302368,
        "learning_rate": 3.6052897828499324e-05,
        "epoch": 0.2429004687069203,
        "step": 1762
    },
    {
        "loss": 1.7081,
        "grad_norm": 1.9617568254470825,
        "learning_rate": 3.595271824971936e-05,
        "epoch": 0.24303832368348496,
        "step": 1763
    },
    {
        "loss": 2.28,
        "grad_norm": 1.6666040420532227,
        "learning_rate": 3.5852647535480546e-05,
        "epoch": 0.24317617866004962,
        "step": 1764
    },
    {
        "loss": 1.1195,
        "grad_norm": 2.315448760986328,
        "learning_rate": 3.575268585587831e-05,
        "epoch": 0.24331403363661427,
        "step": 1765
    },
    {
        "loss": 2.2308,
        "grad_norm": 1.454195261001587,
        "learning_rate": 3.565283338082273e-05,
        "epoch": 0.24345188861317893,
        "step": 1766
    },
    {
        "loss": 1.7827,
        "grad_norm": 1.7323977947235107,
        "learning_rate": 3.555309028003848e-05,
        "epoch": 0.24358974358974358,
        "step": 1767
    },
    {
        "loss": 2.6533,
        "grad_norm": 1.6844655275344849,
        "learning_rate": 3.545345672306404e-05,
        "epoch": 0.24372759856630824,
        "step": 1768
    },
    {
        "loss": 2.2445,
        "grad_norm": 1.0558791160583496,
        "learning_rate": 3.5353932879251805e-05,
        "epoch": 0.2438654535428729,
        "step": 1769
    },
    {
        "loss": 2.1868,
        "grad_norm": 1.41150963306427,
        "learning_rate": 3.5254518917767776e-05,
        "epoch": 0.24400330851943755,
        "step": 1770
    },
    {
        "loss": 1.9517,
        "grad_norm": 1.4097678661346436,
        "learning_rate": 3.515521500759105e-05,
        "epoch": 0.2441411634960022,
        "step": 1771
    },
    {
        "loss": 1.8585,
        "grad_norm": 1.94771146774292,
        "learning_rate": 3.5056021317513765e-05,
        "epoch": 0.24427901847256686,
        "step": 1772
    },
    {
        "loss": 1.7403,
        "grad_norm": 2.5604910850524902,
        "learning_rate": 3.495693801614064e-05,
        "epoch": 0.24441687344913152,
        "step": 1773
    },
    {
        "loss": 2.358,
        "grad_norm": 1.4656927585601807,
        "learning_rate": 3.485796527188875e-05,
        "epoch": 0.24455472842569617,
        "step": 1774
    },
    {
        "loss": 2.3481,
        "grad_norm": 1.530068278312683,
        "learning_rate": 3.475910325298738e-05,
        "epoch": 0.24469258340226083,
        "step": 1775
    },
    {
        "loss": 2.3076,
        "grad_norm": 1.287383794784546,
        "learning_rate": 3.4660352127477394e-05,
        "epoch": 0.24483043837882548,
        "step": 1776
    },
    {
        "loss": 2.0682,
        "grad_norm": 1.9110339879989624,
        "learning_rate": 3.456171206321138e-05,
        "epoch": 0.24496829335539014,
        "step": 1777
    },
    {
        "loss": 2.3633,
        "grad_norm": 1.1382251977920532,
        "learning_rate": 3.446318322785303e-05,
        "epoch": 0.2451061483319548,
        "step": 1778
    },
    {
        "loss": 2.2559,
        "grad_norm": 1.6461939811706543,
        "learning_rate": 3.436476578887695e-05,
        "epoch": 0.24524400330851945,
        "step": 1779
    },
    {
        "loss": 2.3985,
        "grad_norm": 1.3617792129516602,
        "learning_rate": 3.4266459913568526e-05,
        "epoch": 0.2453818582850841,
        "step": 1780
    },
    {
        "loss": 1.3092,
        "grad_norm": 2.497401237487793,
        "learning_rate": 3.41682657690234e-05,
        "epoch": 0.24551971326164876,
        "step": 1781
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.418681025505066,
        "learning_rate": 3.4070183522147305e-05,
        "epoch": 0.2456575682382134,
        "step": 1782
    },
    {
        "loss": 1.4722,
        "grad_norm": 1.1409884691238403,
        "learning_rate": 3.397221333965588e-05,
        "epoch": 0.24579542321477804,
        "step": 1783
    },
    {
        "loss": 2.1179,
        "grad_norm": 2.3942763805389404,
        "learning_rate": 3.3874355388074154e-05,
        "epoch": 0.2459332781913427,
        "step": 1784
    },
    {
        "loss": 2.167,
        "grad_norm": 1.463691234588623,
        "learning_rate": 3.377660983373643e-05,
        "epoch": 0.24607113316790735,
        "step": 1785
    },
    {
        "loss": 2.1031,
        "grad_norm": 1.519006371498108,
        "learning_rate": 3.367897684278608e-05,
        "epoch": 0.246208988144472,
        "step": 1786
    },
    {
        "loss": 2.6173,
        "grad_norm": 1.6724773645401,
        "learning_rate": 3.358145658117491e-05,
        "epoch": 0.24634684312103666,
        "step": 1787
    },
    {
        "loss": 2.1222,
        "grad_norm": 2.127868175506592,
        "learning_rate": 3.348404921466334e-05,
        "epoch": 0.24648469809760132,
        "step": 1788
    },
    {
        "loss": 1.7031,
        "grad_norm": 1.8327332735061646,
        "learning_rate": 3.3386754908819784e-05,
        "epoch": 0.24662255307416597,
        "step": 1789
    },
    {
        "loss": 2.1897,
        "grad_norm": 2.209045648574829,
        "learning_rate": 3.328957382902047e-05,
        "epoch": 0.24676040805073063,
        "step": 1790
    },
    {
        "loss": 2.2777,
        "grad_norm": 1.28954017162323,
        "learning_rate": 3.3192506140449265e-05,
        "epoch": 0.24689826302729528,
        "step": 1791
    },
    {
        "loss": 1.9009,
        "grad_norm": 1.7124145030975342,
        "learning_rate": 3.3095552008097165e-05,
        "epoch": 0.24703611800385994,
        "step": 1792
    },
    {
        "loss": 1.5601,
        "grad_norm": 2.207327127456665,
        "learning_rate": 3.299871159676229e-05,
        "epoch": 0.2471739729804246,
        "step": 1793
    },
    {
        "loss": 2.3328,
        "grad_norm": 1.1569849252700806,
        "learning_rate": 3.290198507104939e-05,
        "epoch": 0.24731182795698925,
        "step": 1794
    },
    {
        "loss": 2.2248,
        "grad_norm": 1.2141404151916504,
        "learning_rate": 3.280537259536951e-05,
        "epoch": 0.2474496829335539,
        "step": 1795
    },
    {
        "loss": 2.0625,
        "grad_norm": 2.348698854446411,
        "learning_rate": 3.270887433394017e-05,
        "epoch": 0.24758753791011856,
        "step": 1796
    },
    {
        "loss": 2.3182,
        "grad_norm": 1.5738375186920166,
        "learning_rate": 3.2612490450784374e-05,
        "epoch": 0.24772539288668322,
        "step": 1797
    },
    {
        "loss": 1.9521,
        "grad_norm": 1.9188296794891357,
        "learning_rate": 3.251622110973101e-05,
        "epoch": 0.24786324786324787,
        "step": 1798
    },
    {
        "loss": 2.0516,
        "grad_norm": 1.9292603731155396,
        "learning_rate": 3.24200664744141e-05,
        "epoch": 0.24800110283981253,
        "step": 1799
    },
    {
        "loss": 2.4206,
        "grad_norm": 1.3903337717056274,
        "learning_rate": 3.232402670827275e-05,
        "epoch": 0.24813895781637718,
        "step": 1800
    },
    {
        "loss": 2.2197,
        "grad_norm": 1.061236023902893,
        "learning_rate": 3.222810197455078e-05,
        "epoch": 0.24827681279294184,
        "step": 1801
    },
    {
        "loss": 2.6121,
        "grad_norm": 1.2862080335617065,
        "learning_rate": 3.21322924362966e-05,
        "epoch": 0.2484146677695065,
        "step": 1802
    },
    {
        "loss": 1.6362,
        "grad_norm": 1.8335825204849243,
        "learning_rate": 3.203659825636267e-05,
        "epoch": 0.24855252274607112,
        "step": 1803
    },
    {
        "loss": 2.3575,
        "grad_norm": 2.331085205078125,
        "learning_rate": 3.194101959740543e-05,
        "epoch": 0.24869037772263577,
        "step": 1804
    },
    {
        "loss": 1.7977,
        "grad_norm": 2.221550226211548,
        "learning_rate": 3.184555662188501e-05,
        "epoch": 0.24882823269920043,
        "step": 1805
    },
    {
        "loss": 2.3107,
        "grad_norm": 1.5683802366256714,
        "learning_rate": 3.175020949206483e-05,
        "epoch": 0.24896608767576509,
        "step": 1806
    },
    {
        "loss": 2.2075,
        "grad_norm": 1.6642497777938843,
        "learning_rate": 3.1654978370011436e-05,
        "epoch": 0.24910394265232974,
        "step": 1807
    },
    {
        "loss": 1.8484,
        "grad_norm": 2.1289806365966797,
        "learning_rate": 3.155986341759416e-05,
        "epoch": 0.2492417976288944,
        "step": 1808
    },
    {
        "loss": 1.8249,
        "grad_norm": 1.650418996810913,
        "learning_rate": 3.146486479648496e-05,
        "epoch": 0.24937965260545905,
        "step": 1809
    },
    {
        "loss": 1.8464,
        "grad_norm": 1.3572760820388794,
        "learning_rate": 3.136998266815797e-05,
        "epoch": 0.2495175075820237,
        "step": 1810
    },
    {
        "loss": 1.5818,
        "grad_norm": 2.264043092727661,
        "learning_rate": 3.127521719388932e-05,
        "epoch": 0.24965536255858836,
        "step": 1811
    },
    {
        "loss": 1.7791,
        "grad_norm": 1.5210046768188477,
        "learning_rate": 3.118056853475693e-05,
        "epoch": 0.24979321753515302,
        "step": 1812
    },
    {
        "loss": 2.2241,
        "grad_norm": 1.0841491222381592,
        "learning_rate": 3.108603685164009e-05,
        "epoch": 0.24993107251171767,
        "step": 1813
    },
    {
        "loss": 1.374,
        "grad_norm": 2.5732502937316895,
        "learning_rate": 3.099162230521926e-05,
        "epoch": 0.2500689274882823,
        "step": 1814
    },
    {
        "loss": 2.4437,
        "grad_norm": 1.3658069372177124,
        "learning_rate": 3.089732505597591e-05,
        "epoch": 0.25020678246484696,
        "step": 1815
    },
    {
        "loss": 2.1741,
        "grad_norm": 1.1206426620483398,
        "learning_rate": 3.080314526419192e-05,
        "epoch": 0.2503446374414116,
        "step": 1816
    },
    {
        "loss": 2.6269,
        "grad_norm": 1.4381605386734009,
        "learning_rate": 3.070908308994975e-05,
        "epoch": 0.25048249241797627,
        "step": 1817
    },
    {
        "loss": 2.0992,
        "grad_norm": 2.0161020755767822,
        "learning_rate": 3.0615138693131765e-05,
        "epoch": 0.2506203473945409,
        "step": 1818
    },
    {
        "loss": 2.0125,
        "grad_norm": 1.2972099781036377,
        "learning_rate": 3.052131223342027e-05,
        "epoch": 0.2507582023711056,
        "step": 1819
    },
    {
        "loss": 2.4533,
        "grad_norm": 1.6279875040054321,
        "learning_rate": 3.0427603870297017e-05,
        "epoch": 0.25089605734767023,
        "step": 1820
    },
    {
        "loss": 1.7115,
        "grad_norm": 1.5180541276931763,
        "learning_rate": 3.0334013763043014e-05,
        "epoch": 0.2510339123242349,
        "step": 1821
    },
    {
        "loss": 1.6912,
        "grad_norm": 2.003000497817993,
        "learning_rate": 3.024054207073841e-05,
        "epoch": 0.25117176730079954,
        "step": 1822
    },
    {
        "loss": 1.3219,
        "grad_norm": 2.483943462371826,
        "learning_rate": 3.0147188952261818e-05,
        "epoch": 0.2513096222773642,
        "step": 1823
    },
    {
        "loss": 2.1059,
        "grad_norm": 1.753253698348999,
        "learning_rate": 3.0053954566290566e-05,
        "epoch": 0.25144747725392885,
        "step": 1824
    },
    {
        "loss": 1.8592,
        "grad_norm": 2.093737840652466,
        "learning_rate": 2.9960839071300018e-05,
        "epoch": 0.2515853322304935,
        "step": 1825
    },
    {
        "loss": 1.915,
        "grad_norm": 1.9314002990722656,
        "learning_rate": 2.9867842625563458e-05,
        "epoch": 0.25172318720705816,
        "step": 1826
    },
    {
        "loss": 2.7858,
        "grad_norm": 2.0010478496551514,
        "learning_rate": 2.977496538715191e-05,
        "epoch": 0.2518610421836228,
        "step": 1827
    },
    {
        "loss": 1.7317,
        "grad_norm": 2.358999490737915,
        "learning_rate": 2.968220751393369e-05,
        "epoch": 0.2519988971601875,
        "step": 1828
    },
    {
        "loss": 1.9006,
        "grad_norm": 2.4567458629608154,
        "learning_rate": 2.9589569163574206e-05,
        "epoch": 0.25213675213675213,
        "step": 1829
    },
    {
        "loss": 1.8799,
        "grad_norm": 1.3832770586013794,
        "learning_rate": 2.9497050493535794e-05,
        "epoch": 0.2522746071133168,
        "step": 1830
    },
    {
        "loss": 1.7834,
        "grad_norm": 1.795557975769043,
        "learning_rate": 2.940465166107731e-05,
        "epoch": 0.25241246208988144,
        "step": 1831
    },
    {
        "loss": 2.3568,
        "grad_norm": 1.580289602279663,
        "learning_rate": 2.9312372823253908e-05,
        "epoch": 0.2525503170664461,
        "step": 1832
    },
    {
        "loss": 2.5486,
        "grad_norm": 1.3127723932266235,
        "learning_rate": 2.9220214136916757e-05,
        "epoch": 0.25268817204301075,
        "step": 1833
    },
    {
        "loss": 2.2063,
        "grad_norm": 1.3627610206604004,
        "learning_rate": 2.9128175758712905e-05,
        "epoch": 0.2528260270195754,
        "step": 1834
    },
    {
        "loss": 2.4566,
        "grad_norm": 1.6043145656585693,
        "learning_rate": 2.9036257845084823e-05,
        "epoch": 0.25296388199614006,
        "step": 1835
    },
    {
        "loss": 1.9894,
        "grad_norm": 2.1549689769744873,
        "learning_rate": 2.8944460552270124e-05,
        "epoch": 0.2531017369727047,
        "step": 1836
    },
    {
        "loss": 1.3538,
        "grad_norm": 1.325132131576538,
        "learning_rate": 2.8852784036301663e-05,
        "epoch": 0.25323959194926937,
        "step": 1837
    },
    {
        "loss": 1.3635,
        "grad_norm": 2.3691787719726562,
        "learning_rate": 2.8761228453006795e-05,
        "epoch": 0.253377446925834,
        "step": 1838
    },
    {
        "loss": 1.9623,
        "grad_norm": 1.8290528059005737,
        "learning_rate": 2.8669793958007272e-05,
        "epoch": 0.2535153019023987,
        "step": 1839
    },
    {
        "loss": 2.3235,
        "grad_norm": 1.2398934364318848,
        "learning_rate": 2.857848070671927e-05,
        "epoch": 0.25365315687896334,
        "step": 1840
    },
    {
        "loss": 2.0141,
        "grad_norm": 1.887817621231079,
        "learning_rate": 2.84872888543527e-05,
        "epoch": 0.253791011855528,
        "step": 1841
    },
    {
        "loss": 2.227,
        "grad_norm": 0.9907327890396118,
        "learning_rate": 2.839621855591107e-05,
        "epoch": 0.25392886683209265,
        "step": 1842
    },
    {
        "loss": 1.8952,
        "grad_norm": 1.4468905925750732,
        "learning_rate": 2.830526996619144e-05,
        "epoch": 0.2540667218086573,
        "step": 1843
    },
    {
        "loss": 1.2729,
        "grad_norm": 2.422699213027954,
        "learning_rate": 2.8214443239783938e-05,
        "epoch": 0.25420457678522196,
        "step": 1844
    },
    {
        "loss": 1.5802,
        "grad_norm": 2.726344108581543,
        "learning_rate": 2.8123738531071508e-05,
        "epoch": 0.2543424317617866,
        "step": 1845
    },
    {
        "loss": 2.3451,
        "grad_norm": 1.1800787448883057,
        "learning_rate": 2.803315599422974e-05,
        "epoch": 0.25448028673835127,
        "step": 1846
    },
    {
        "loss": 1.4617,
        "grad_norm": 1.4483224153518677,
        "learning_rate": 2.7942695783226513e-05,
        "epoch": 0.2546181417149159,
        "step": 1847
    },
    {
        "loss": 2.5617,
        "grad_norm": 1.1867690086364746,
        "learning_rate": 2.7852358051821858e-05,
        "epoch": 0.2547559966914806,
        "step": 1848
    },
    {
        "loss": 2.4076,
        "grad_norm": 1.673662543296814,
        "learning_rate": 2.7762142953567582e-05,
        "epoch": 0.25489385166804523,
        "step": 1849
    },
    {
        "loss": 2.3159,
        "grad_norm": 1.1676360368728638,
        "learning_rate": 2.7672050641807014e-05,
        "epoch": 0.2550317066446099,
        "step": 1850
    },
    {
        "loss": 2.344,
        "grad_norm": 1.7353962659835815,
        "learning_rate": 2.7582081269674788e-05,
        "epoch": 0.25516956162117455,
        "step": 1851
    },
    {
        "loss": 1.9256,
        "grad_norm": 1.7965192794799805,
        "learning_rate": 2.7492234990096643e-05,
        "epoch": 0.2553074165977392,
        "step": 1852
    },
    {
        "loss": 1.9143,
        "grad_norm": 1.3242988586425781,
        "learning_rate": 2.740251195578901e-05,
        "epoch": 0.25544527157430386,
        "step": 1853
    },
    {
        "loss": 2.5213,
        "grad_norm": 1.9026339054107666,
        "learning_rate": 2.7312912319258864e-05,
        "epoch": 0.2555831265508685,
        "step": 1854
    },
    {
        "loss": 1.9656,
        "grad_norm": 2.0928866863250732,
        "learning_rate": 2.7223436232803366e-05,
        "epoch": 0.25572098152743317,
        "step": 1855
    },
    {
        "loss": 1.6372,
        "grad_norm": 1.7541656494140625,
        "learning_rate": 2.7134083848509883e-05,
        "epoch": 0.2558588365039978,
        "step": 1856
    },
    {
        "loss": 1.6793,
        "grad_norm": 1.6724592447280884,
        "learning_rate": 2.704485531825527e-05,
        "epoch": 0.2559966914805624,
        "step": 1857
    },
    {
        "loss": 2.3976,
        "grad_norm": 1.1241334676742554,
        "learning_rate": 2.6955750793705915e-05,
        "epoch": 0.2561345464571271,
        "step": 1858
    },
    {
        "loss": 1.9066,
        "grad_norm": 1.9064992666244507,
        "learning_rate": 2.6866770426317635e-05,
        "epoch": 0.25627240143369173,
        "step": 1859
    },
    {
        "loss": 1.9138,
        "grad_norm": 1.657263159751892,
        "learning_rate": 2.677791436733492e-05,
        "epoch": 0.2564102564102564,
        "step": 1860
    },
    {
        "loss": 1.9721,
        "grad_norm": 1.580395221710205,
        "learning_rate": 2.6689182767791144e-05,
        "epoch": 0.25654811138682104,
        "step": 1861
    },
    {
        "loss": 2.4401,
        "grad_norm": 1.1371705532073975,
        "learning_rate": 2.6600575778508052e-05,
        "epoch": 0.2566859663633857,
        "step": 1862
    },
    {
        "loss": 1.3201,
        "grad_norm": 2.04594087600708,
        "learning_rate": 2.6512093550095662e-05,
        "epoch": 0.25682382133995035,
        "step": 1863
    },
    {
        "loss": 1.651,
        "grad_norm": 1.7069984674453735,
        "learning_rate": 2.6423736232951856e-05,
        "epoch": 0.256961676316515,
        "step": 1864
    },
    {
        "loss": 2.0852,
        "grad_norm": 1.7939231395721436,
        "learning_rate": 2.63355039772622e-05,
        "epoch": 0.25709953129307966,
        "step": 1865
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.1885663270950317,
        "learning_rate": 2.624739693299977e-05,
        "epoch": 0.2572373862696443,
        "step": 1866
    },
    {
        "loss": 2.0054,
        "grad_norm": 2.5831096172332764,
        "learning_rate": 2.6159415249924725e-05,
        "epoch": 0.257375241246209,
        "step": 1867
    },
    {
        "loss": 1.6522,
        "grad_norm": 2.0595591068267822,
        "learning_rate": 2.607155907758413e-05,
        "epoch": 0.25751309622277363,
        "step": 1868
    },
    {
        "loss": 2.0224,
        "grad_norm": 2.2196242809295654,
        "learning_rate": 2.5983828565311842e-05,
        "epoch": 0.2576509511993383,
        "step": 1869
    },
    {
        "loss": 2.0927,
        "grad_norm": 1.0939990282058716,
        "learning_rate": 2.589622386222793e-05,
        "epoch": 0.25778880617590294,
        "step": 1870
    },
    {
        "loss": 2.5895,
        "grad_norm": 1.2590746879577637,
        "learning_rate": 2.5808745117238785e-05,
        "epoch": 0.2579266611524676,
        "step": 1871
    },
    {
        "loss": 1.9651,
        "grad_norm": 1.5181353092193604,
        "learning_rate": 2.5721392479036687e-05,
        "epoch": 0.25806451612903225,
        "step": 1872
    },
    {
        "loss": 2.1587,
        "grad_norm": 1.3672415018081665,
        "learning_rate": 2.5634166096099455e-05,
        "epoch": 0.2582023711055969,
        "step": 1873
    },
    {
        "loss": 1.9301,
        "grad_norm": 2.3144192695617676,
        "learning_rate": 2.554706611669031e-05,
        "epoch": 0.25834022608216156,
        "step": 1874
    },
    {
        "loss": 1.3138,
        "grad_norm": 2.121652126312256,
        "learning_rate": 2.546009268885785e-05,
        "epoch": 0.2584780810587262,
        "step": 1875
    },
    {
        "loss": 1.8212,
        "grad_norm": 1.8712583780288696,
        "learning_rate": 2.5373245960435266e-05,
        "epoch": 0.25861593603529087,
        "step": 1876
    },
    {
        "loss": 2.2121,
        "grad_norm": 2.3607194423675537,
        "learning_rate": 2.5286526079040497e-05,
        "epoch": 0.2587537910118555,
        "step": 1877
    },
    {
        "loss": 1.9181,
        "grad_norm": 1.977921962738037,
        "learning_rate": 2.519993319207603e-05,
        "epoch": 0.2588916459884202,
        "step": 1878
    },
    {
        "loss": 2.1761,
        "grad_norm": 1.794798493385315,
        "learning_rate": 2.5113467446728265e-05,
        "epoch": 0.25902950096498484,
        "step": 1879
    },
    {
        "loss": 2.6107,
        "grad_norm": 1.943719506263733,
        "learning_rate": 2.502712898996761e-05,
        "epoch": 0.2591673559415495,
        "step": 1880
    },
    {
        "loss": 2.1864,
        "grad_norm": 1.4839496612548828,
        "learning_rate": 2.4940917968548016e-05,
        "epoch": 0.25930521091811415,
        "step": 1881
    },
    {
        "loss": 2.0279,
        "grad_norm": 1.7727614641189575,
        "learning_rate": 2.4854834529007066e-05,
        "epoch": 0.2594430658946788,
        "step": 1882
    },
    {
        "loss": 1.7209,
        "grad_norm": 1.8246874809265137,
        "learning_rate": 2.4768878817665186e-05,
        "epoch": 0.25958092087124346,
        "step": 1883
    },
    {
        "loss": 2.4639,
        "grad_norm": 1.2658647298812866,
        "learning_rate": 2.4683050980625856e-05,
        "epoch": 0.2597187758478081,
        "step": 1884
    },
    {
        "loss": 1.2773,
        "grad_norm": 2.039454460144043,
        "learning_rate": 2.4597351163775205e-05,
        "epoch": 0.25985663082437277,
        "step": 1885
    },
    {
        "loss": 1.9796,
        "grad_norm": 2.7459418773651123,
        "learning_rate": 2.4511779512781695e-05,
        "epoch": 0.2599944858009374,
        "step": 1886
    },
    {
        "loss": 2.4176,
        "grad_norm": 1.2284923791885376,
        "learning_rate": 2.442633617309601e-05,
        "epoch": 0.2601323407775021,
        "step": 1887
    },
    {
        "loss": 2.2187,
        "grad_norm": 2.0604515075683594,
        "learning_rate": 2.4341021289950715e-05,
        "epoch": 0.26027019575406674,
        "step": 1888
    },
    {
        "loss": 1.5586,
        "grad_norm": 1.990761637687683,
        "learning_rate": 2.4255835008359885e-05,
        "epoch": 0.2604080507306314,
        "step": 1889
    },
    {
        "loss": 1.7766,
        "grad_norm": 2.2771658897399902,
        "learning_rate": 2.4170777473119288e-05,
        "epoch": 0.26054590570719605,
        "step": 1890
    },
    {
        "loss": 2.2052,
        "grad_norm": 1.276002287864685,
        "learning_rate": 2.4085848828805635e-05,
        "epoch": 0.2606837606837607,
        "step": 1891
    },
    {
        "loss": 1.6874,
        "grad_norm": 2.141791343688965,
        "learning_rate": 2.4001049219776607e-05,
        "epoch": 0.26082161566032536,
        "step": 1892
    },
    {
        "loss": 2.014,
        "grad_norm": 2.7531237602233887,
        "learning_rate": 2.3916378790170602e-05,
        "epoch": 0.26095947063689,
        "step": 1893
    },
    {
        "loss": 1.0069,
        "grad_norm": 2.2025856971740723,
        "learning_rate": 2.383183768390641e-05,
        "epoch": 0.26109732561345467,
        "step": 1894
    },
    {
        "loss": 1.8597,
        "grad_norm": 1.5551592111587524,
        "learning_rate": 2.3747426044683007e-05,
        "epoch": 0.2612351805900193,
        "step": 1895
    },
    {
        "loss": 2.0986,
        "grad_norm": 1.0306462049484253,
        "learning_rate": 2.3663144015979223e-05,
        "epoch": 0.261373035566584,
        "step": 1896
    },
    {
        "loss": 2.2855,
        "grad_norm": 1.8134206533432007,
        "learning_rate": 2.357899174105378e-05,
        "epoch": 0.26151089054314863,
        "step": 1897
    },
    {
        "loss": 2.3301,
        "grad_norm": 1.252282977104187,
        "learning_rate": 2.349496936294473e-05,
        "epoch": 0.2616487455197133,
        "step": 1898
    },
    {
        "loss": 1.8435,
        "grad_norm": 1.1016488075256348,
        "learning_rate": 2.341107702446923e-05,
        "epoch": 0.2617866004962779,
        "step": 1899
    },
    {
        "loss": 2.2199,
        "grad_norm": 1.442918062210083,
        "learning_rate": 2.332731486822359e-05,
        "epoch": 0.26192445547284254,
        "step": 1900
    },
    {
        "loss": 2.2884,
        "grad_norm": 1.2687373161315918,
        "learning_rate": 2.3243683036582797e-05,
        "epoch": 0.2620623104494072,
        "step": 1901
    },
    {
        "loss": 1.5506,
        "grad_norm": 2.227381706237793,
        "learning_rate": 2.3160181671700197e-05,
        "epoch": 0.26220016542597185,
        "step": 1902
    },
    {
        "loss": 2.3241,
        "grad_norm": 1.445730209350586,
        "learning_rate": 2.3076810915507517e-05,
        "epoch": 0.2623380204025365,
        "step": 1903
    },
    {
        "loss": 1.9575,
        "grad_norm": 1.3878687620162964,
        "learning_rate": 2.2993570909714423e-05,
        "epoch": 0.26247587537910116,
        "step": 1904
    },
    {
        "loss": 1.6483,
        "grad_norm": 4.244173526763916,
        "learning_rate": 2.29104617958083e-05,
        "epoch": 0.2626137303556658,
        "step": 1905
    },
    {
        "loss": 2.2443,
        "grad_norm": 1.6254377365112305,
        "learning_rate": 2.2827483715054153e-05,
        "epoch": 0.2627515853322305,
        "step": 1906
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.522078514099121,
        "learning_rate": 2.274463680849417e-05,
        "epoch": 0.26288944030879513,
        "step": 1907
    },
    {
        "loss": 2.2937,
        "grad_norm": 1.536834478378296,
        "learning_rate": 2.266192121694757e-05,
        "epoch": 0.2630272952853598,
        "step": 1908
    },
    {
        "loss": 2.1192,
        "grad_norm": 1.780869722366333,
        "learning_rate": 2.2579337081010455e-05,
        "epoch": 0.26316515026192444,
        "step": 1909
    },
    {
        "loss": 1.8297,
        "grad_norm": 1.0112541913986206,
        "learning_rate": 2.249688454105543e-05,
        "epoch": 0.2633030052384891,
        "step": 1910
    },
    {
        "loss": 2.244,
        "grad_norm": 1.7274272441864014,
        "learning_rate": 2.2414563737231388e-05,
        "epoch": 0.26344086021505375,
        "step": 1911
    },
    {
        "loss": 1.8614,
        "grad_norm": 2.875812292098999,
        "learning_rate": 2.2332374809463376e-05,
        "epoch": 0.2635787151916184,
        "step": 1912
    },
    {
        "loss": 2.1668,
        "grad_norm": 1.7025330066680908,
        "learning_rate": 2.2250317897452244e-05,
        "epoch": 0.26371657016818306,
        "step": 1913
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.7112760543823242,
        "learning_rate": 2.216839314067446e-05,
        "epoch": 0.2638544251447477,
        "step": 1914
    },
    {
        "loss": 2.1534,
        "grad_norm": 1.1950128078460693,
        "learning_rate": 2.208660067838175e-05,
        "epoch": 0.2639922801213124,
        "step": 1915
    },
    {
        "loss": 2.6068,
        "grad_norm": 1.2770484685897827,
        "learning_rate": 2.2004940649601225e-05,
        "epoch": 0.26413013509787703,
        "step": 1916
    },
    {
        "loss": 1.5517,
        "grad_norm": 2.437169075012207,
        "learning_rate": 2.1923413193134655e-05,
        "epoch": 0.2642679900744417,
        "step": 1917
    },
    {
        "loss": 2.2467,
        "grad_norm": 1.8684650659561157,
        "learning_rate": 2.1842018447558577e-05,
        "epoch": 0.26440584505100634,
        "step": 1918
    },
    {
        "loss": 1.7112,
        "grad_norm": 1.7580136060714722,
        "learning_rate": 2.1760756551223905e-05,
        "epoch": 0.264543700027571,
        "step": 1919
    },
    {
        "loss": 2.1375,
        "grad_norm": 1.4407134056091309,
        "learning_rate": 2.1679627642255806e-05,
        "epoch": 0.26468155500413565,
        "step": 1920
    },
    {
        "loss": 1.9109,
        "grad_norm": 1.9504510164260864,
        "learning_rate": 2.1598631858553353e-05,
        "epoch": 0.2648194099807003,
        "step": 1921
    },
    {
        "loss": 1.9263,
        "grad_norm": 1.1275910139083862,
        "learning_rate": 2.151776933778934e-05,
        "epoch": 0.26495726495726496,
        "step": 1922
    },
    {
        "loss": 2.3712,
        "grad_norm": 1.701493740081787,
        "learning_rate": 2.143704021741004e-05,
        "epoch": 0.2650951199338296,
        "step": 1923
    },
    {
        "loss": 2.1664,
        "grad_norm": 1.1440227031707764,
        "learning_rate": 2.1356444634635033e-05,
        "epoch": 0.26523297491039427,
        "step": 1924
    },
    {
        "loss": 1.9444,
        "grad_norm": 1.0790976285934448,
        "learning_rate": 2.1275982726456867e-05,
        "epoch": 0.2653708298869589,
        "step": 1925
    },
    {
        "loss": 1.9803,
        "grad_norm": 1.4870434999465942,
        "learning_rate": 2.1195654629640873e-05,
        "epoch": 0.2655086848635236,
        "step": 1926
    },
    {
        "loss": 1.9907,
        "grad_norm": 1.811989426612854,
        "learning_rate": 2.1115460480724935e-05,
        "epoch": 0.26564653984008824,
        "step": 1927
    },
    {
        "loss": 1.7464,
        "grad_norm": 1.3046637773513794,
        "learning_rate": 2.1035400416019312e-05,
        "epoch": 0.2657843948166529,
        "step": 1928
    },
    {
        "loss": 2.3,
        "grad_norm": 1.621764898300171,
        "learning_rate": 2.095547457160635e-05,
        "epoch": 0.26592224979321755,
        "step": 1929
    },
    {
        "loss": 2.0742,
        "grad_norm": 1.686774730682373,
        "learning_rate": 2.0875683083340125e-05,
        "epoch": 0.2660601047697822,
        "step": 1930
    },
    {
        "loss": 1.9242,
        "grad_norm": 1.957991600036621,
        "learning_rate": 2.0796026086846488e-05,
        "epoch": 0.26619795974634686,
        "step": 1931
    },
    {
        "loss": 1.6592,
        "grad_norm": 2.3802483081817627,
        "learning_rate": 2.0716503717522696e-05,
        "epoch": 0.2663358147229115,
        "step": 1932
    },
    {
        "loss": 1.7545,
        "grad_norm": 1.9839760065078735,
        "learning_rate": 2.0637116110537013e-05,
        "epoch": 0.26647366969947617,
        "step": 1933
    },
    {
        "loss": 2.0399,
        "grad_norm": 2.054131031036377,
        "learning_rate": 2.0557863400828724e-05,
        "epoch": 0.2666115246760408,
        "step": 1934
    },
    {
        "loss": 1.1297,
        "grad_norm": 2.6599488258361816,
        "learning_rate": 2.0478745723107972e-05,
        "epoch": 0.2667493796526055,
        "step": 1935
    },
    {
        "loss": 2.3719,
        "grad_norm": 1.7195521593093872,
        "learning_rate": 2.0399763211855137e-05,
        "epoch": 0.26688723462917013,
        "step": 1936
    },
    {
        "loss": 2.0045,
        "grad_norm": 1.5794541835784912,
        "learning_rate": 2.032091600132099e-05,
        "epoch": 0.2670250896057348,
        "step": 1937
    },
    {
        "loss": 1.5885,
        "grad_norm": 2.644002914428711,
        "learning_rate": 2.024220422552625e-05,
        "epoch": 0.26716294458229944,
        "step": 1938
    },
    {
        "loss": 1.3802,
        "grad_norm": 2.4686710834503174,
        "learning_rate": 2.0163628018261537e-05,
        "epoch": 0.2673007995588641,
        "step": 1939
    },
    {
        "loss": 1.8236,
        "grad_norm": 2.2364444732666016,
        "learning_rate": 2.0085187513086935e-05,
        "epoch": 0.26743865453542875,
        "step": 1940
    },
    {
        "loss": 1.6763,
        "grad_norm": 2.491713047027588,
        "learning_rate": 2.00068828433319e-05,
        "epoch": 0.2675765095119934,
        "step": 1941
    },
    {
        "loss": 1.967,
        "grad_norm": 1.8014578819274902,
        "learning_rate": 1.9928714142095028e-05,
        "epoch": 0.267714364488558,
        "step": 1942
    },
    {
        "loss": 2.3735,
        "grad_norm": 1.225978970527649,
        "learning_rate": 1.985068154224379e-05,
        "epoch": 0.26785221946512267,
        "step": 1943
    },
    {
        "loss": 1.9344,
        "grad_norm": 1.3261888027191162,
        "learning_rate": 1.9772785176414256e-05,
        "epoch": 0.2679900744416873,
        "step": 1944
    },
    {
        "loss": 2.4085,
        "grad_norm": 0.8590861558914185,
        "learning_rate": 1.9695025177011074e-05,
        "epoch": 0.268127929418252,
        "step": 1945
    },
    {
        "loss": 1.8414,
        "grad_norm": 1.5841476917266846,
        "learning_rate": 1.9617401676206916e-05,
        "epoch": 0.26826578439481663,
        "step": 1946
    },
    {
        "loss": 2.3904,
        "grad_norm": 1.1200250387191772,
        "learning_rate": 1.9539914805942583e-05,
        "epoch": 0.2684036393713813,
        "step": 1947
    },
    {
        "loss": 2.0496,
        "grad_norm": 1.4393062591552734,
        "learning_rate": 1.9462564697926645e-05,
        "epoch": 0.26854149434794594,
        "step": 1948
    },
    {
        "loss": 1.9173,
        "grad_norm": 2.2742397785186768,
        "learning_rate": 1.938535148363505e-05,
        "epoch": 0.2686793493245106,
        "step": 1949
    },
    {
        "loss": 1.1939,
        "grad_norm": 2.7810726165771484,
        "learning_rate": 1.930827529431124e-05,
        "epoch": 0.26881720430107525,
        "step": 1950
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.260028839111328,
        "learning_rate": 1.9231336260965703e-05,
        "epoch": 0.2689550592776399,
        "step": 1951
    },
    {
        "loss": 1.981,
        "grad_norm": 2.242461919784546,
        "learning_rate": 1.915453451437571e-05,
        "epoch": 0.26909291425420456,
        "step": 1952
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.6488627195358276,
        "learning_rate": 1.907787018508521e-05,
        "epoch": 0.2692307692307692,
        "step": 1953
    },
    {
        "loss": 1.5727,
        "grad_norm": 2.2641868591308594,
        "learning_rate": 1.9001343403404704e-05,
        "epoch": 0.2693686242073339,
        "step": 1954
    },
    {
        "loss": 2.3127,
        "grad_norm": 1.3172624111175537,
        "learning_rate": 1.8924954299410768e-05,
        "epoch": 0.26950647918389853,
        "step": 1955
    },
    {
        "loss": 2.2939,
        "grad_norm": 1.2550890445709229,
        "learning_rate": 1.8848703002945943e-05,
        "epoch": 0.2696443341604632,
        "step": 1956
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.2852518558502197,
        "learning_rate": 1.8772589643618555e-05,
        "epoch": 0.26978218913702784,
        "step": 1957
    },
    {
        "loss": 2.0562,
        "grad_norm": 2.1232314109802246,
        "learning_rate": 1.869661435080262e-05,
        "epoch": 0.2699200441135925,
        "step": 1958
    },
    {
        "loss": 1.9999,
        "grad_norm": 1.8979822397232056,
        "learning_rate": 1.8620777253637213e-05,
        "epoch": 0.27005789909015715,
        "step": 1959
    },
    {
        "loss": 2.183,
        "grad_norm": 1.5900822877883911,
        "learning_rate": 1.8545078481026724e-05,
        "epoch": 0.2701957540667218,
        "step": 1960
    },
    {
        "loss": 2.0151,
        "grad_norm": 1.4970203638076782,
        "learning_rate": 1.846951816164033e-05,
        "epoch": 0.27033360904328646,
        "step": 1961
    },
    {
        "loss": 1.7336,
        "grad_norm": 2.002899169921875,
        "learning_rate": 1.8394096423911855e-05,
        "epoch": 0.2704714640198511,
        "step": 1962
    },
    {
        "loss": 1.3231,
        "grad_norm": 1.9975494146347046,
        "learning_rate": 1.831881339603967e-05,
        "epoch": 0.27060931899641577,
        "step": 1963
    },
    {
        "loss": 2.3304,
        "grad_norm": 1.516495704650879,
        "learning_rate": 1.8243669205986247e-05,
        "epoch": 0.2707471739729804,
        "step": 1964
    },
    {
        "loss": 1.6734,
        "grad_norm": 1.9712740182876587,
        "learning_rate": 1.816866398147814e-05,
        "epoch": 0.2708850289495451,
        "step": 1965
    },
    {
        "loss": 2.3959,
        "grad_norm": 1.6148356199264526,
        "learning_rate": 1.8093797850005712e-05,
        "epoch": 0.27102288392610974,
        "step": 1966
    },
    {
        "loss": 2.1388,
        "grad_norm": 1.9356915950775146,
        "learning_rate": 1.8019070938822848e-05,
        "epoch": 0.2711607389026744,
        "step": 1967
    },
    {
        "loss": 2.2217,
        "grad_norm": 1.657918095588684,
        "learning_rate": 1.7944483374946796e-05,
        "epoch": 0.27129859387923905,
        "step": 1968
    },
    {
        "loss": 1.5577,
        "grad_norm": 2.1224730014801025,
        "learning_rate": 1.7870035285158025e-05,
        "epoch": 0.2714364488558037,
        "step": 1969
    },
    {
        "loss": 2.1985,
        "grad_norm": 1.899564266204834,
        "learning_rate": 1.7795726795999847e-05,
        "epoch": 0.27157430383236836,
        "step": 1970
    },
    {
        "loss": 2.0884,
        "grad_norm": 1.7409389019012451,
        "learning_rate": 1.772155803377835e-05,
        "epoch": 0.271712158808933,
        "step": 1971
    },
    {
        "loss": 1.1825,
        "grad_norm": 1.6247996091842651,
        "learning_rate": 1.7647529124561978e-05,
        "epoch": 0.27185001378549767,
        "step": 1972
    },
    {
        "loss": 1.2727,
        "grad_norm": 2.600707769393921,
        "learning_rate": 1.7573640194181717e-05,
        "epoch": 0.2719878687620623,
        "step": 1973
    },
    {
        "loss": 2.0444,
        "grad_norm": 1.7551192045211792,
        "learning_rate": 1.7499891368230427e-05,
        "epoch": 0.272125723738627,
        "step": 1974
    },
    {
        "loss": 2.3379,
        "grad_norm": 1.7556968927383423,
        "learning_rate": 1.7426282772062807e-05,
        "epoch": 0.27226357871519163,
        "step": 1975
    },
    {
        "loss": 1.6335,
        "grad_norm": 2.0589799880981445,
        "learning_rate": 1.7352814530795323e-05,
        "epoch": 0.2724014336917563,
        "step": 1976
    },
    {
        "loss": 0.521,
        "grad_norm": 2.643721103668213,
        "learning_rate": 1.7279486769305875e-05,
        "epoch": 0.27253928866832094,
        "step": 1977
    },
    {
        "loss": 2.2858,
        "grad_norm": 1.2241427898406982,
        "learning_rate": 1.720629961223341e-05,
        "epoch": 0.2726771436448856,
        "step": 1978
    },
    {
        "loss": 1.6642,
        "grad_norm": 3.3581972122192383,
        "learning_rate": 1.7133253183978083e-05,
        "epoch": 0.27281499862145026,
        "step": 1979
    },
    {
        "loss": 1.9035,
        "grad_norm": 2.3398592472076416,
        "learning_rate": 1.706034760870071e-05,
        "epoch": 0.2729528535980149,
        "step": 1980
    },
    {
        "loss": 1.8334,
        "grad_norm": 2.45538067817688,
        "learning_rate": 1.698758301032277e-05,
        "epoch": 0.27309070857457957,
        "step": 1981
    },
    {
        "loss": 1.3628,
        "grad_norm": 2.335697889328003,
        "learning_rate": 1.6914959512526085e-05,
        "epoch": 0.2732285635511442,
        "step": 1982
    },
    {
        "loss": 2.4562,
        "grad_norm": 1.4348056316375732,
        "learning_rate": 1.6842477238752608e-05,
        "epoch": 0.2733664185277089,
        "step": 1983
    },
    {
        "loss": 1.5809,
        "grad_norm": 1.7597097158432007,
        "learning_rate": 1.6770136312204332e-05,
        "epoch": 0.27350427350427353,
        "step": 1984
    },
    {
        "loss": 1.9217,
        "grad_norm": 1.5099761486053467,
        "learning_rate": 1.6697936855842923e-05,
        "epoch": 0.27364212848083813,
        "step": 1985
    },
    {
        "loss": 1.1874,
        "grad_norm": 2.3085989952087402,
        "learning_rate": 1.6625878992389587e-05,
        "epoch": 0.2737799834574028,
        "step": 1986
    },
    {
        "loss": 2.0812,
        "grad_norm": 1.8807380199432373,
        "learning_rate": 1.6553962844324865e-05,
        "epoch": 0.27391783843396744,
        "step": 1987
    },
    {
        "loss": 2.6848,
        "grad_norm": 1.0449950695037842,
        "learning_rate": 1.6482188533888475e-05,
        "epoch": 0.2740556934105321,
        "step": 1988
    },
    {
        "loss": 2.4421,
        "grad_norm": 1.7104227542877197,
        "learning_rate": 1.6410556183078963e-05,
        "epoch": 0.27419354838709675,
        "step": 1989
    },
    {
        "loss": 2.1782,
        "grad_norm": 1.730936050415039,
        "learning_rate": 1.6339065913653605e-05,
        "epoch": 0.2743314033636614,
        "step": 1990
    },
    {
        "loss": 1.9504,
        "grad_norm": 1.6262755393981934,
        "learning_rate": 1.6267717847128184e-05,
        "epoch": 0.27446925834022606,
        "step": 1991
    },
    {
        "loss": 2.297,
        "grad_norm": 1.398676872253418,
        "learning_rate": 1.6196512104776786e-05,
        "epoch": 0.2746071133167907,
        "step": 1992
    },
    {
        "loss": 2.1764,
        "grad_norm": 1.3833656311035156,
        "learning_rate": 1.612544880763156e-05,
        "epoch": 0.2747449682933554,
        "step": 1993
    },
    {
        "loss": 2.1962,
        "grad_norm": 1.5239784717559814,
        "learning_rate": 1.6054528076482532e-05,
        "epoch": 0.27488282326992003,
        "step": 1994
    },
    {
        "loss": 2.1117,
        "grad_norm": 1.5956307649612427,
        "learning_rate": 1.5983750031877388e-05,
        "epoch": 0.2750206782464847,
        "step": 1995
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.3863686323165894,
        "learning_rate": 1.591311479412133e-05,
        "epoch": 0.27515853322304934,
        "step": 1996
    },
    {
        "loss": 2.0526,
        "grad_norm": 1.4004002809524536,
        "learning_rate": 1.5842622483276803e-05,
        "epoch": 0.275296388199614,
        "step": 1997
    },
    {
        "loss": 2.0159,
        "grad_norm": 1.747017502784729,
        "learning_rate": 1.577227321916329e-05,
        "epoch": 0.27543424317617865,
        "step": 1998
    },
    {
        "loss": 1.1431,
        "grad_norm": 2.7737209796905518,
        "learning_rate": 1.570206712135711e-05,
        "epoch": 0.2755720981527433,
        "step": 1999
    },
    {
        "loss": 2.401,
        "grad_norm": 1.0495108366012573,
        "learning_rate": 1.563200430919133e-05,
        "epoch": 0.27570995312930796,
        "step": 2000
    },
    {
        "loss": 1.6798,
        "grad_norm": 2.1177284717559814,
        "learning_rate": 1.556208490175537e-05,
        "epoch": 0.2758478081058726,
        "step": 2001
    },
    {
        "loss": 2.3897,
        "grad_norm": 1.1905165910720825,
        "learning_rate": 1.5492309017894925e-05,
        "epoch": 0.27598566308243727,
        "step": 2002
    },
    {
        "loss": 1.4066,
        "grad_norm": 2.155690908432007,
        "learning_rate": 1.5422676776211775e-05,
        "epoch": 0.2761235180590019,
        "step": 2003
    },
    {
        "loss": 0.5516,
        "grad_norm": 1.3159691095352173,
        "learning_rate": 1.5353188295063457e-05,
        "epoch": 0.2762613730355666,
        "step": 2004
    },
    {
        "loss": 1.7933,
        "grad_norm": 1.9133793115615845,
        "learning_rate": 1.5283843692563305e-05,
        "epoch": 0.27639922801213124,
        "step": 2005
    },
    {
        "loss": 1.9749,
        "grad_norm": 1.7655236721038818,
        "learning_rate": 1.5214643086579884e-05,
        "epoch": 0.2765370829886959,
        "step": 2006
    },
    {
        "loss": 2.28,
        "grad_norm": 1.7667776346206665,
        "learning_rate": 1.514558659473716e-05,
        "epoch": 0.27667493796526055,
        "step": 2007
    },
    {
        "loss": 2.4283,
        "grad_norm": 1.422847867012024,
        "learning_rate": 1.5076674334414132e-05,
        "epoch": 0.2768127929418252,
        "step": 2008
    },
    {
        "loss": 1.2504,
        "grad_norm": 2.0840344429016113,
        "learning_rate": 1.5007906422744544e-05,
        "epoch": 0.27695064791838986,
        "step": 2009
    },
    {
        "loss": 1.2079,
        "grad_norm": 1.7924249172210693,
        "learning_rate": 1.493928297661682e-05,
        "epoch": 0.2770885028949545,
        "step": 2010
    },
    {
        "loss": 2.2535,
        "grad_norm": 1.1675468683242798,
        "learning_rate": 1.4870804112673941e-05,
        "epoch": 0.27722635787151917,
        "step": 2011
    },
    {
        "loss": 1.9811,
        "grad_norm": 1.758337140083313,
        "learning_rate": 1.4802469947312981e-05,
        "epoch": 0.2773642128480838,
        "step": 2012
    },
    {
        "loss": 2.5926,
        "grad_norm": 1.1486401557922363,
        "learning_rate": 1.4734280596685123e-05,
        "epoch": 0.2775020678246485,
        "step": 2013
    },
    {
        "loss": 2.2247,
        "grad_norm": 1.511077880859375,
        "learning_rate": 1.4666236176695391e-05,
        "epoch": 0.27763992280121313,
        "step": 2014
    },
    {
        "loss": 1.312,
        "grad_norm": 2.6308863162994385,
        "learning_rate": 1.4598336803002522e-05,
        "epoch": 0.2777777777777778,
        "step": 2015
    },
    {
        "loss": 1.2414,
        "grad_norm": 2.470869541168213,
        "learning_rate": 1.4530582591018615e-05,
        "epoch": 0.27791563275434245,
        "step": 2016
    },
    {
        "loss": 2.3955,
        "grad_norm": 1.3643267154693604,
        "learning_rate": 1.4462973655909073e-05,
        "epoch": 0.2780534877309071,
        "step": 2017
    },
    {
        "loss": 2.231,
        "grad_norm": 1.2918778657913208,
        "learning_rate": 1.4395510112592414e-05,
        "epoch": 0.27819134270747176,
        "step": 2018
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.503601312637329,
        "learning_rate": 1.4328192075739944e-05,
        "epoch": 0.2783291976840364,
        "step": 2019
    },
    {
        "loss": 2.1264,
        "grad_norm": 1.5945777893066406,
        "learning_rate": 1.4261019659775653e-05,
        "epoch": 0.27846705266060107,
        "step": 2020
    },
    {
        "loss": 2.298,
        "grad_norm": 1.4683901071548462,
        "learning_rate": 1.4193992978876102e-05,
        "epoch": 0.2786049076371657,
        "step": 2021
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.9773060083389282,
        "learning_rate": 1.4127112146970045e-05,
        "epoch": 0.2787427626137304,
        "step": 2022
    },
    {
        "loss": 2.3952,
        "grad_norm": 1.2511317729949951,
        "learning_rate": 1.4060377277738323e-05,
        "epoch": 0.27888061759029503,
        "step": 2023
    },
    {
        "loss": 1.6825,
        "grad_norm": 2.5845601558685303,
        "learning_rate": 1.399378848461379e-05,
        "epoch": 0.2790184725668597,
        "step": 2024
    },
    {
        "loss": 1.9609,
        "grad_norm": 1.4723217487335205,
        "learning_rate": 1.3927345880780818e-05,
        "epoch": 0.27915632754342434,
        "step": 2025
    },
    {
        "loss": 2.0768,
        "grad_norm": 1.6778948307037354,
        "learning_rate": 1.386104957917551e-05,
        "epoch": 0.279294182519989,
        "step": 2026
    },
    {
        "loss": 1.8126,
        "grad_norm": 2.2153398990631104,
        "learning_rate": 1.379489969248513e-05,
        "epoch": 0.2794320374965536,
        "step": 2027
    },
    {
        "loss": 2.1537,
        "grad_norm": 1.937827467918396,
        "learning_rate": 1.3728896333148167e-05,
        "epoch": 0.27956989247311825,
        "step": 2028
    },
    {
        "loss": 2.0825,
        "grad_norm": 1.944326400756836,
        "learning_rate": 1.3663039613353912e-05,
        "epoch": 0.2797077474496829,
        "step": 2029
    },
    {
        "loss": 1.5779,
        "grad_norm": 1.7559841871261597,
        "learning_rate": 1.3597329645042622e-05,
        "epoch": 0.27984560242624756,
        "step": 2030
    },
    {
        "loss": 2.3116,
        "grad_norm": 0.9644691348075867,
        "learning_rate": 1.3531766539904967e-05,
        "epoch": 0.2799834574028122,
        "step": 2031
    },
    {
        "loss": 1.6756,
        "grad_norm": 1.6590033769607544,
        "learning_rate": 1.3466350409381933e-05,
        "epoch": 0.2801213123793769,
        "step": 2032
    },
    {
        "loss": 1.8824,
        "grad_norm": 1.2345473766326904,
        "learning_rate": 1.3401081364664835e-05,
        "epoch": 0.28025916735594153,
        "step": 2033
    },
    {
        "loss": 2.3459,
        "grad_norm": 2.051567554473877,
        "learning_rate": 1.333595951669494e-05,
        "epoch": 0.2803970223325062,
        "step": 2034
    },
    {
        "loss": 1.2834,
        "grad_norm": 2.273419141769409,
        "learning_rate": 1.3270984976163193e-05,
        "epoch": 0.28053487730907084,
        "step": 2035
    },
    {
        "loss": 2.2672,
        "grad_norm": 1.0680571794509888,
        "learning_rate": 1.3206157853510327e-05,
        "epoch": 0.2806727322856355,
        "step": 2036
    },
    {
        "loss": 2.0557,
        "grad_norm": 1.407089352607727,
        "learning_rate": 1.3141478258926377e-05,
        "epoch": 0.28081058726220015,
        "step": 2037
    },
    {
        "loss": 1.9797,
        "grad_norm": 1.4212406873703003,
        "learning_rate": 1.3076946302350656e-05,
        "epoch": 0.2809484422387648,
        "step": 2038
    },
    {
        "loss": 1.0506,
        "grad_norm": 2.464724540710449,
        "learning_rate": 1.301256209347157e-05,
        "epoch": 0.28108629721532946,
        "step": 2039
    },
    {
        "loss": 1.6615,
        "grad_norm": 1.7667254209518433,
        "learning_rate": 1.2948325741726331e-05,
        "epoch": 0.2812241521918941,
        "step": 2040
    },
    {
        "loss": 2.3867,
        "grad_norm": 1.8154906034469604,
        "learning_rate": 1.2884237356300832e-05,
        "epoch": 0.28136200716845877,
        "step": 2041
    },
    {
        "loss": 1.8355,
        "grad_norm": 2.2806332111358643,
        "learning_rate": 1.2820297046129514e-05,
        "epoch": 0.2814998621450234,
        "step": 2042
    },
    {
        "loss": 1.8618,
        "grad_norm": 2.2399344444274902,
        "learning_rate": 1.2756504919895085e-05,
        "epoch": 0.2816377171215881,
        "step": 2043
    },
    {
        "loss": 2.3518,
        "grad_norm": 1.9434423446655273,
        "learning_rate": 1.269286108602835e-05,
        "epoch": 0.28177557209815274,
        "step": 2044
    },
    {
        "loss": 2.3602,
        "grad_norm": 1.6988493204116821,
        "learning_rate": 1.2629365652708147e-05,
        "epoch": 0.2819134270747174,
        "step": 2045
    },
    {
        "loss": 2.2179,
        "grad_norm": 1.1529083251953125,
        "learning_rate": 1.2566018727860984e-05,
        "epoch": 0.28205128205128205,
        "step": 2046
    },
    {
        "loss": 1.8393,
        "grad_norm": 2.7211813926696777,
        "learning_rate": 1.2502820419160977e-05,
        "epoch": 0.2821891370278467,
        "step": 2047
    },
    {
        "loss": 1.0776,
        "grad_norm": 1.9578293561935425,
        "learning_rate": 1.2439770834029552e-05,
        "epoch": 0.28232699200441136,
        "step": 2048
    },
    {
        "loss": 1.9144,
        "grad_norm": 1.7777199745178223,
        "learning_rate": 1.2376870079635517e-05,
        "epoch": 0.282464846980976,
        "step": 2049
    },
    {
        "loss": 1.7093,
        "grad_norm": 2.086026430130005,
        "learning_rate": 1.231411826289458e-05,
        "epoch": 0.28260270195754067,
        "step": 2050
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.6977859735488892,
        "learning_rate": 1.2251515490469235e-05,
        "epoch": 0.2827405569341053,
        "step": 2051
    },
    {
        "loss": 2.3115,
        "grad_norm": 1.3031941652297974,
        "learning_rate": 1.2189061868768782e-05,
        "epoch": 0.28287841191067,
        "step": 2052
    },
    {
        "loss": 1.5952,
        "grad_norm": 2.2434980869293213,
        "learning_rate": 1.2126757503948926e-05,
        "epoch": 0.28301626688723464,
        "step": 2053
    },
    {
        "loss": 1.6182,
        "grad_norm": 2.0727148056030273,
        "learning_rate": 1.2064602501911682e-05,
        "epoch": 0.2831541218637993,
        "step": 2054
    },
    {
        "loss": 1.2497,
        "grad_norm": 1.8861091136932373,
        "learning_rate": 1.2002596968305146e-05,
        "epoch": 0.28329197684036395,
        "step": 2055
    },
    {
        "loss": 1.5722,
        "grad_norm": 1.8814151287078857,
        "learning_rate": 1.1940741008523448e-05,
        "epoch": 0.2834298318169286,
        "step": 2056
    },
    {
        "loss": 1.9987,
        "grad_norm": 3.85191011428833,
        "learning_rate": 1.1879034727706395e-05,
        "epoch": 0.28356768679349326,
        "step": 2057
    },
    {
        "loss": 2.1203,
        "grad_norm": 1.561808705329895,
        "learning_rate": 1.181747823073941e-05,
        "epoch": 0.2837055417700579,
        "step": 2058
    },
    {
        "loss": 2.6239,
        "grad_norm": 1.3213785886764526,
        "learning_rate": 1.1756071622253295e-05,
        "epoch": 0.28384339674662257,
        "step": 2059
    },
    {
        "loss": 1.5646,
        "grad_norm": 2.428685188293457,
        "learning_rate": 1.1694815006624128e-05,
        "epoch": 0.2839812517231872,
        "step": 2060
    },
    {
        "loss": 2.0601,
        "grad_norm": 1.807098388671875,
        "learning_rate": 1.1633708487973005e-05,
        "epoch": 0.2841191066997519,
        "step": 2061
    },
    {
        "loss": 1.87,
        "grad_norm": 1.9801944494247437,
        "learning_rate": 1.1572752170165891e-05,
        "epoch": 0.28425696167631653,
        "step": 2062
    },
    {
        "loss": 1.7502,
        "grad_norm": 1.1346124410629272,
        "learning_rate": 1.1511946156813425e-05,
        "epoch": 0.2843948166528812,
        "step": 2063
    },
    {
        "loss": 2.1917,
        "grad_norm": 1.442273497581482,
        "learning_rate": 1.1451290551270844e-05,
        "epoch": 0.28453267162944584,
        "step": 2064
    },
    {
        "loss": 1.5554,
        "grad_norm": 2.1533854007720947,
        "learning_rate": 1.1390785456637654e-05,
        "epoch": 0.2846705266060105,
        "step": 2065
    },
    {
        "loss": 2.0114,
        "grad_norm": 1.840576171875,
        "learning_rate": 1.1330430975757555e-05,
        "epoch": 0.28480838158257515,
        "step": 2066
    },
    {
        "loss": 2.2069,
        "grad_norm": 2.501206398010254,
        "learning_rate": 1.1270227211218187e-05,
        "epoch": 0.2849462365591398,
        "step": 2067
    },
    {
        "loss": 2.2951,
        "grad_norm": 1.893693447113037,
        "learning_rate": 1.1210174265351193e-05,
        "epoch": 0.28508409153570446,
        "step": 2068
    },
    {
        "loss": 1.9611,
        "grad_norm": 1.7943482398986816,
        "learning_rate": 1.1150272240231618e-05,
        "epoch": 0.2852219465122691,
        "step": 2069
    },
    {
        "loss": 1.9227,
        "grad_norm": 1.8863108158111572,
        "learning_rate": 1.109052123767813e-05,
        "epoch": 0.2853598014888337,
        "step": 2070
    },
    {
        "loss": 1.5375,
        "grad_norm": 1.96980619430542,
        "learning_rate": 1.1030921359252633e-05,
        "epoch": 0.2854976564653984,
        "step": 2071
    },
    {
        "loss": 1.9077,
        "grad_norm": 1.9095118045806885,
        "learning_rate": 1.0971472706260222e-05,
        "epoch": 0.28563551144196303,
        "step": 2072
    },
    {
        "loss": 2.3538,
        "grad_norm": 1.4930205345153809,
        "learning_rate": 1.091217537974889e-05,
        "epoch": 0.2857733664185277,
        "step": 2073
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.3325269222259521,
        "learning_rate": 1.0853029480509403e-05,
        "epoch": 0.28591122139509234,
        "step": 2074
    },
    {
        "loss": 2.2052,
        "grad_norm": 1.4459835290908813,
        "learning_rate": 1.0794035109075207e-05,
        "epoch": 0.286049076371657,
        "step": 2075
    },
    {
        "loss": 2.221,
        "grad_norm": 1.2268658876419067,
        "learning_rate": 1.0735192365722125e-05,
        "epoch": 0.28618693134822165,
        "step": 2076
    },
    {
        "loss": 2.2047,
        "grad_norm": 1.5783555507659912,
        "learning_rate": 1.0676501350468248e-05,
        "epoch": 0.2863247863247863,
        "step": 2077
    },
    {
        "loss": 2.345,
        "grad_norm": 1.2504701614379883,
        "learning_rate": 1.0617962163073836e-05,
        "epoch": 0.28646264130135096,
        "step": 2078
    },
    {
        "loss": 2.6857,
        "grad_norm": 1.6242594718933105,
        "learning_rate": 1.0559574903041002e-05,
        "epoch": 0.2866004962779156,
        "step": 2079
    },
    {
        "loss": 2.1261,
        "grad_norm": 1.8871328830718994,
        "learning_rate": 1.0501339669613642e-05,
        "epoch": 0.2867383512544803,
        "step": 2080
    },
    {
        "loss": 2.0764,
        "grad_norm": 1.3691006898880005,
        "learning_rate": 1.0443256561777292e-05,
        "epoch": 0.28687620623104493,
        "step": 2081
    },
    {
        "loss": 2.185,
        "grad_norm": 1.2678049802780151,
        "learning_rate": 1.0385325678258806e-05,
        "epoch": 0.2870140612076096,
        "step": 2082
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.4874980449676514,
        "learning_rate": 1.0327547117526403e-05,
        "epoch": 0.28715191618417424,
        "step": 2083
    },
    {
        "loss": 2.0826,
        "grad_norm": 1.9123764038085938,
        "learning_rate": 1.0269920977789382e-05,
        "epoch": 0.2872897711607389,
        "step": 2084
    },
    {
        "loss": 1.5028,
        "grad_norm": 1.5961493253707886,
        "learning_rate": 1.0212447356997878e-05,
        "epoch": 0.28742762613730355,
        "step": 2085
    },
    {
        "loss": 1.8829,
        "grad_norm": 1.391530156135559,
        "learning_rate": 1.0155126352842814e-05,
        "epoch": 0.2875654811138682,
        "step": 2086
    },
    {
        "loss": 2.1112,
        "grad_norm": 2.1871895790100098,
        "learning_rate": 1.0097958062755808e-05,
        "epoch": 0.28770333609043286,
        "step": 2087
    },
    {
        "loss": 2.126,
        "grad_norm": 1.6220948696136475,
        "learning_rate": 1.0040942583908741e-05,
        "epoch": 0.2878411910669975,
        "step": 2088
    },
    {
        "loss": 2.031,
        "grad_norm": 1.510788917541504,
        "learning_rate": 9.98408001321387e-06,
        "epoch": 0.28797904604356217,
        "step": 2089
    },
    {
        "loss": 1.7982,
        "grad_norm": 1.9596565961837769,
        "learning_rate": 9.927370447323458e-06,
        "epoch": 0.2881169010201268,
        "step": 2090
    },
    {
        "loss": 2.0794,
        "grad_norm": 0.996300995349884,
        "learning_rate": 9.870813982629789e-06,
        "epoch": 0.2882547559966915,
        "step": 2091
    },
    {
        "loss": 1.7584,
        "grad_norm": 1.2803924083709717,
        "learning_rate": 9.814410715264854e-06,
        "epoch": 0.28839261097325614,
        "step": 2092
    },
    {
        "loss": 2.0849,
        "grad_norm": 2.0570802688598633,
        "learning_rate": 9.75816074110022e-06,
        "epoch": 0.2885304659498208,
        "step": 2093
    },
    {
        "loss": 2.1555,
        "grad_norm": 1.5168101787567139,
        "learning_rate": 9.702064155747026e-06,
        "epoch": 0.28866832092638545,
        "step": 2094
    },
    {
        "loss": 2.3324,
        "grad_norm": 1.8164490461349487,
        "learning_rate": 9.646121054555523e-06,
        "epoch": 0.2888061759029501,
        "step": 2095
    },
    {
        "loss": 1.9515,
        "grad_norm": 1.759986400604248,
        "learning_rate": 9.590331532615137e-06,
        "epoch": 0.28894403087951476,
        "step": 2096
    },
    {
        "loss": 2.3033,
        "grad_norm": 0.9524194002151489,
        "learning_rate": 9.534695684754325e-06,
        "epoch": 0.2890818858560794,
        "step": 2097
    },
    {
        "loss": 1.5816,
        "grad_norm": 1.8854116201400757,
        "learning_rate": 9.479213605540205e-06,
        "epoch": 0.28921974083264407,
        "step": 2098
    },
    {
        "loss": 1.8578,
        "grad_norm": 1.2133874893188477,
        "learning_rate": 9.42388538927863e-06,
        "epoch": 0.2893575958092087,
        "step": 2099
    },
    {
        "loss": 2.3226,
        "grad_norm": 0.9101772904396057,
        "learning_rate": 9.368711130013907e-06,
        "epoch": 0.2894954507857734,
        "step": 2100
    },
    {
        "loss": 1.5794,
        "grad_norm": 1.0668468475341797,
        "learning_rate": 9.313690921528551e-06,
        "epoch": 0.28963330576233803,
        "step": 2101
    },
    {
        "loss": 2.0005,
        "grad_norm": 1.491655707359314,
        "learning_rate": 9.258824857343396e-06,
        "epoch": 0.2897711607389027,
        "step": 2102
    },
    {
        "loss": 1.1826,
        "grad_norm": 2.791942834854126,
        "learning_rate": 9.204113030717143e-06,
        "epoch": 0.28990901571546734,
        "step": 2103
    },
    {
        "loss": 2.4035,
        "grad_norm": 1.3854836225509644,
        "learning_rate": 9.149555534646403e-06,
        "epoch": 0.290046870692032,
        "step": 2104
    },
    {
        "loss": 2.2059,
        "grad_norm": 1.350433111190796,
        "learning_rate": 9.09515246186533e-06,
        "epoch": 0.29018472566859665,
        "step": 2105
    },
    {
        "loss": 2.0149,
        "grad_norm": 1.9681943655014038,
        "learning_rate": 9.04090390484582e-06,
        "epoch": 0.2903225806451613,
        "step": 2106
    },
    {
        "loss": 1.1834,
        "grad_norm": 2.1955833435058594,
        "learning_rate": 8.98680995579696e-06,
        "epoch": 0.29046043562172597,
        "step": 2107
    },
    {
        "loss": 2.1916,
        "grad_norm": 1.298601508140564,
        "learning_rate": 8.932870706665043e-06,
        "epoch": 0.2905982905982906,
        "step": 2108
    },
    {
        "loss": 1.7041,
        "grad_norm": 1.6493357419967651,
        "learning_rate": 8.879086249133484e-06,
        "epoch": 0.2907361455748553,
        "step": 2109
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.5489099025726318,
        "learning_rate": 8.825456674622601e-06,
        "epoch": 0.29087400055141993,
        "step": 2110
    },
    {
        "loss": 1.9979,
        "grad_norm": 2.1746466159820557,
        "learning_rate": 8.77198207428933e-06,
        "epoch": 0.2910118555279846,
        "step": 2111
    },
    {
        "loss": 0.8518,
        "grad_norm": 1.8113843202590942,
        "learning_rate": 8.718662539027322e-06,
        "epoch": 0.29114971050454924,
        "step": 2112
    },
    {
        "loss": 2.1319,
        "grad_norm": 1.4875373840332031,
        "learning_rate": 8.665498159466624e-06,
        "epoch": 0.29128756548111384,
        "step": 2113
    },
    {
        "loss": 1.1329,
        "grad_norm": 1.9443626403808594,
        "learning_rate": 8.61248902597347e-06,
        "epoch": 0.2914254204576785,
        "step": 2114
    },
    {
        "loss": 2.1637,
        "grad_norm": 1.203925371170044,
        "learning_rate": 8.55963522865032e-06,
        "epoch": 0.29156327543424315,
        "step": 2115
    },
    {
        "loss": 2.1947,
        "grad_norm": 1.9204156398773193,
        "learning_rate": 8.506936857335556e-06,
        "epoch": 0.2917011304108078,
        "step": 2116
    },
    {
        "loss": 2.0309,
        "grad_norm": 2.5048775672912598,
        "learning_rate": 8.454394001603349e-06,
        "epoch": 0.29183898538737246,
        "step": 2117
    },
    {
        "loss": 1.9817,
        "grad_norm": 1.3070485591888428,
        "learning_rate": 8.402006750763602e-06,
        "epoch": 0.2919768403639371,
        "step": 2118
    },
    {
        "loss": 2.0041,
        "grad_norm": 3.1058945655822754,
        "learning_rate": 8.349775193861665e-06,
        "epoch": 0.2921146953405018,
        "step": 2119
    },
    {
        "loss": 2.55,
        "grad_norm": 0.916259229183197,
        "learning_rate": 8.297699419678251e-06,
        "epoch": 0.29225255031706643,
        "step": 2120
    },
    {
        "loss": 2.1888,
        "grad_norm": 1.107954978942871,
        "learning_rate": 8.245779516729347e-06,
        "epoch": 0.2923904052936311,
        "step": 2121
    },
    {
        "loss": 2.6665,
        "grad_norm": 1.6270095109939575,
        "learning_rate": 8.194015573265911e-06,
        "epoch": 0.29252826027019574,
        "step": 2122
    },
    {
        "loss": 1.7157,
        "grad_norm": 2.2587504386901855,
        "learning_rate": 8.142407677273888e-06,
        "epoch": 0.2926661152467604,
        "step": 2123
    },
    {
        "loss": 1.9899,
        "grad_norm": 1.8532706499099731,
        "learning_rate": 8.090955916473841e-06,
        "epoch": 0.29280397022332505,
        "step": 2124
    },
    {
        "loss": 2.2529,
        "grad_norm": 1.45742928981781,
        "learning_rate": 8.039660378321156e-06,
        "epoch": 0.2929418251998897,
        "step": 2125
    },
    {
        "loss": 2.0614,
        "grad_norm": 1.3149735927581787,
        "learning_rate": 7.988521150005524e-06,
        "epoch": 0.29307968017645436,
        "step": 2126
    },
    {
        "loss": 1.8016,
        "grad_norm": 1.7484883069992065,
        "learning_rate": 7.93753831845092e-06,
        "epoch": 0.293217535153019,
        "step": 2127
    },
    {
        "loss": 1.679,
        "grad_norm": 1.7732166051864624,
        "learning_rate": 7.886711970315652e-06,
        "epoch": 0.29335539012958367,
        "step": 2128
    },
    {
        "loss": 1.7487,
        "grad_norm": 2.1333982944488525,
        "learning_rate": 7.836042191991899e-06,
        "epoch": 0.2934932451061483,
        "step": 2129
    },
    {
        "loss": 1.7056,
        "grad_norm": 1.850574254989624,
        "learning_rate": 7.785529069605757e-06,
        "epoch": 0.293631100082713,
        "step": 2130
    },
    {
        "loss": 2.0906,
        "grad_norm": 1.334516167640686,
        "learning_rate": 7.735172689017011e-06,
        "epoch": 0.29376895505927764,
        "step": 2131
    },
    {
        "loss": 2.2482,
        "grad_norm": 1.762542963027954,
        "learning_rate": 7.684973135819119e-06,
        "epoch": 0.2939068100358423,
        "step": 2132
    },
    {
        "loss": 1.4799,
        "grad_norm": 1.8136944770812988,
        "learning_rate": 7.634930495338866e-06,
        "epoch": 0.29404466501240695,
        "step": 2133
    },
    {
        "loss": 2.2448,
        "grad_norm": 1.7544336318969727,
        "learning_rate": 7.585044852636369e-06,
        "epoch": 0.2941825199889716,
        "step": 2134
    },
    {
        "loss": 2.0308,
        "grad_norm": 2.567589521408081,
        "learning_rate": 7.535316292504879e-06,
        "epoch": 0.29432037496553626,
        "step": 2135
    },
    {
        "loss": 1.2393,
        "grad_norm": 1.8347491025924683,
        "learning_rate": 7.4857448994706815e-06,
        "epoch": 0.2944582299421009,
        "step": 2136
    },
    {
        "loss": 1.8287,
        "grad_norm": 1.4061453342437744,
        "learning_rate": 7.436330757792864e-06,
        "epoch": 0.29459608491866557,
        "step": 2137
    },
    {
        "loss": 1.9719,
        "grad_norm": 2.033277988433838,
        "learning_rate": 7.3870739514632525e-06,
        "epoch": 0.2947339398952302,
        "step": 2138
    },
    {
        "loss": 2.3457,
        "grad_norm": 1.3697043657302856,
        "learning_rate": 7.337974564206229e-06,
        "epoch": 0.2948717948717949,
        "step": 2139
    },
    {
        "loss": 1.8351,
        "grad_norm": 1.7670083045959473,
        "learning_rate": 7.289032679478602e-06,
        "epoch": 0.29500964984835953,
        "step": 2140
    },
    {
        "loss": 1.7459,
        "grad_norm": 1.9823853969573975,
        "learning_rate": 7.240248380469528e-06,
        "epoch": 0.2951475048249242,
        "step": 2141
    },
    {
        "loss": 2.0262,
        "grad_norm": 1.8388423919677734,
        "learning_rate": 7.191621750100197e-06,
        "epoch": 0.29528535980148884,
        "step": 2142
    },
    {
        "loss": 1.7559,
        "grad_norm": 1.3326268196105957,
        "learning_rate": 7.143152871023839e-06,
        "epoch": 0.2954232147780535,
        "step": 2143
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.303784728050232,
        "learning_rate": 7.094841825625642e-06,
        "epoch": 0.29556106975461816,
        "step": 2144
    },
    {
        "loss": 2.2773,
        "grad_norm": 1.488470196723938,
        "learning_rate": 7.046688696022375e-06,
        "epoch": 0.2956989247311828,
        "step": 2145
    },
    {
        "loss": 1.9481,
        "grad_norm": 1.6586884260177612,
        "learning_rate": 6.998693564062431e-06,
        "epoch": 0.29583677970774747,
        "step": 2146
    },
    {
        "loss": 2.4402,
        "grad_norm": 1.2731443643569946,
        "learning_rate": 6.950856511325754e-06,
        "epoch": 0.2959746346843121,
        "step": 2147
    },
    {
        "loss": 1.2898,
        "grad_norm": 1.4346115589141846,
        "learning_rate": 6.903177619123424e-06,
        "epoch": 0.2961124896608768,
        "step": 2148
    },
    {
        "loss": 2.53,
        "grad_norm": 1.1456198692321777,
        "learning_rate": 6.855656968497803e-06,
        "epoch": 0.29625034463744143,
        "step": 2149
    },
    {
        "loss": 1.32,
        "grad_norm": 2.2997701168060303,
        "learning_rate": 6.808294640222224e-06,
        "epoch": 0.2963881996140061,
        "step": 2150
    },
    {
        "loss": 2.156,
        "grad_norm": 0.8350834846496582,
        "learning_rate": 6.761090714800955e-06,
        "epoch": 0.29652605459057074,
        "step": 2151
    },
    {
        "loss": 2.2507,
        "grad_norm": 1.1335890293121338,
        "learning_rate": 6.714045272468994e-06,
        "epoch": 0.2966639095671354,
        "step": 2152
    },
    {
        "loss": 2.2073,
        "grad_norm": 1.4157649278640747,
        "learning_rate": 6.6671583931919305e-06,
        "epoch": 0.29680176454370005,
        "step": 2153
    },
    {
        "loss": 1.3724,
        "grad_norm": 2.353747844696045,
        "learning_rate": 6.6204301566659135e-06,
        "epoch": 0.2969396195202647,
        "step": 2154
    },
    {
        "loss": 2.2288,
        "grad_norm": 1.5764409303665161,
        "learning_rate": 6.573860642317354e-06,
        "epoch": 0.2970774744968293,
        "step": 2155
    },
    {
        "loss": 2.0011,
        "grad_norm": 1.3683199882507324,
        "learning_rate": 6.527449929302909e-06,
        "epoch": 0.29721532947339396,
        "step": 2156
    },
    {
        "loss": 2.0297,
        "grad_norm": 1.7018641233444214,
        "learning_rate": 6.481198096509355e-06,
        "epoch": 0.2973531844499586,
        "step": 2157
    },
    {
        "loss": 0.9769,
        "grad_norm": 1.9485042095184326,
        "learning_rate": 6.435105222553317e-06,
        "epoch": 0.2974910394265233,
        "step": 2158
    },
    {
        "loss": 1.8674,
        "grad_norm": 1.6918390989303589,
        "learning_rate": 6.389171385781301e-06,
        "epoch": 0.29762889440308793,
        "step": 2159
    },
    {
        "loss": 2.2978,
        "grad_norm": 1.158137559890747,
        "learning_rate": 6.343396664269507e-06,
        "epoch": 0.2977667493796526,
        "step": 2160
    },
    {
        "loss": 0.9852,
        "grad_norm": 2.4155001640319824,
        "learning_rate": 6.2977811358236e-06,
        "epoch": 0.29790460435621724,
        "step": 2161
    },
    {
        "loss": 2.5312,
        "grad_norm": 1.2432111501693726,
        "learning_rate": 6.252324877978677e-06,
        "epoch": 0.2980424593327819,
        "step": 2162
    },
    {
        "loss": 2.4386,
        "grad_norm": 1.1597788333892822,
        "learning_rate": 6.207027967999213e-06,
        "epoch": 0.29818031430934655,
        "step": 2163
    },
    {
        "loss": 2.1247,
        "grad_norm": 1.5111415386199951,
        "learning_rate": 6.161890482878696e-06,
        "epoch": 0.2983181692859112,
        "step": 2164
    },
    {
        "loss": 2.3594,
        "grad_norm": 1.1251184940338135,
        "learning_rate": 6.1169124993396775e-06,
        "epoch": 0.29845602426247586,
        "step": 2165
    },
    {
        "loss": 2.1801,
        "grad_norm": 1.4226956367492676,
        "learning_rate": 6.072094093833658e-06,
        "epoch": 0.2985938792390405,
        "step": 2166
    },
    {
        "loss": 2.4647,
        "grad_norm": 0.9586705565452576,
        "learning_rate": 6.027435342540833e-06,
        "epoch": 0.29873173421560517,
        "step": 2167
    },
    {
        "loss": 1.4936,
        "grad_norm": 1.8880633115768433,
        "learning_rate": 5.982936321370003e-06,
        "epoch": 0.2988695891921698,
        "step": 2168
    },
    {
        "loss": 1.7671,
        "grad_norm": 1.2694354057312012,
        "learning_rate": 5.938597105958487e-06,
        "epoch": 0.2990074441687345,
        "step": 2169
    },
    {
        "loss": 1.537,
        "grad_norm": 2.1444091796875,
        "learning_rate": 5.894417771672067e-06,
        "epoch": 0.29914529914529914,
        "step": 2170
    },
    {
        "loss": 1.9575,
        "grad_norm": 1.7790749073028564,
        "learning_rate": 5.850398393604617e-06,
        "epoch": 0.2992831541218638,
        "step": 2171
    },
    {
        "loss": 1.1447,
        "grad_norm": 1.8051893711090088,
        "learning_rate": 5.806539046578219e-06,
        "epoch": 0.29942100909842845,
        "step": 2172
    },
    {
        "loss": 1.6484,
        "grad_norm": 1.441726803779602,
        "learning_rate": 5.762839805142917e-06,
        "epoch": 0.2995588640749931,
        "step": 2173
    },
    {
        "loss": 1.9677,
        "grad_norm": 0.9672680497169495,
        "learning_rate": 5.719300743576594e-06,
        "epoch": 0.29969671905155776,
        "step": 2174
    },
    {
        "loss": 0.7397,
        "grad_norm": 2.4531586170196533,
        "learning_rate": 5.6759219358849315e-06,
        "epoch": 0.2998345740281224,
        "step": 2175
    },
    {
        "loss": 1.8366,
        "grad_norm": 2.375842809677124,
        "learning_rate": 5.632703455801147e-06,
        "epoch": 0.29997242900468707,
        "step": 2176
    },
    {
        "loss": 2.2553,
        "grad_norm": 1.865112066268921,
        "learning_rate": 5.589645376785946e-06,
        "epoch": 0.3001102839812517,
        "step": 2177
    },
    {
        "loss": 1.4698,
        "grad_norm": 1.913219690322876,
        "learning_rate": 5.546747772027472e-06,
        "epoch": 0.3002481389578164,
        "step": 2178
    },
    {
        "loss": 2.4676,
        "grad_norm": 1.8492937088012695,
        "learning_rate": 5.504010714440999e-06,
        "epoch": 0.30038599393438103,
        "step": 2179
    },
    {
        "loss": 2.1029,
        "grad_norm": 2.196986198425293,
        "learning_rate": 5.461434276668998e-06,
        "epoch": 0.3005238489109457,
        "step": 2180
    },
    {
        "loss": 2.1756,
        "grad_norm": 1.3122031688690186,
        "learning_rate": 5.419018531080822e-06,
        "epoch": 0.30066170388751035,
        "step": 2181
    },
    {
        "loss": 1.6343,
        "grad_norm": 3.4112606048583984,
        "learning_rate": 5.376763549772801e-06,
        "epoch": 0.300799558864075,
        "step": 2182
    },
    {
        "loss": 2.1936,
        "grad_norm": 1.4427112340927124,
        "learning_rate": 5.3346694045679715e-06,
        "epoch": 0.30093741384063966,
        "step": 2183
    },
    {
        "loss": 2.364,
        "grad_norm": 1.5445778369903564,
        "learning_rate": 5.292736167015888e-06,
        "epoch": 0.3010752688172043,
        "step": 2184
    },
    {
        "loss": 2.1721,
        "grad_norm": 1.0243037939071655,
        "learning_rate": 5.250963908392758e-06,
        "epoch": 0.30121312379376897,
        "step": 2185
    },
    {
        "loss": 2.3493,
        "grad_norm": 1.3822208642959595,
        "learning_rate": 5.209352699701098e-06,
        "epoch": 0.3013509787703336,
        "step": 2186
    },
    {
        "loss": 1.8788,
        "grad_norm": 2.678309440612793,
        "learning_rate": 5.167902611669606e-06,
        "epoch": 0.3014888337468983,
        "step": 2187
    },
    {
        "loss": 2.2987,
        "grad_norm": 1.666946291923523,
        "learning_rate": 5.1266137147532365e-06,
        "epoch": 0.30162668872346293,
        "step": 2188
    },
    {
        "loss": 1.8653,
        "grad_norm": 1.585652470588684,
        "learning_rate": 5.085486079132906e-06,
        "epoch": 0.3017645437000276,
        "step": 2189
    },
    {
        "loss": 2.4409,
        "grad_norm": 1.4072601795196533,
        "learning_rate": 5.044519774715384e-06,
        "epoch": 0.30190239867659224,
        "step": 2190
    },
    {
        "loss": 1.8696,
        "grad_norm": 1.275787353515625,
        "learning_rate": 5.003714871133291e-06,
        "epoch": 0.3020402536531569,
        "step": 2191
    },
    {
        "loss": 1.8367,
        "grad_norm": 1.847647786140442,
        "learning_rate": 4.963071437744848e-06,
        "epoch": 0.30217810862972155,
        "step": 2192
    },
    {
        "loss": 1.8434,
        "grad_norm": 1.8262497186660767,
        "learning_rate": 4.922589543633882e-06,
        "epoch": 0.3023159636062862,
        "step": 2193
    },
    {
        "loss": 1.9495,
        "grad_norm": 1.1205461025238037,
        "learning_rate": 4.882269257609573e-06,
        "epoch": 0.30245381858285086,
        "step": 2194
    },
    {
        "loss": 2.4712,
        "grad_norm": 2.0399675369262695,
        "learning_rate": 4.842110648206432e-06,
        "epoch": 0.3025916735594155,
        "step": 2195
    },
    {
        "loss": 2.3044,
        "grad_norm": 1.8159890174865723,
        "learning_rate": 4.80211378368417e-06,
        "epoch": 0.3027295285359802,
        "step": 2196
    },
    {
        "loss": 1.3933,
        "grad_norm": 2.313363552093506,
        "learning_rate": 4.762278732027581e-06,
        "epoch": 0.30286738351254483,
        "step": 2197
    },
    {
        "loss": 1.9264,
        "grad_norm": 1.860360026359558,
        "learning_rate": 4.722605560946381e-06,
        "epoch": 0.30300523848910943,
        "step": 2198
    },
    {
        "loss": 2.0909,
        "grad_norm": 1.1218650341033936,
        "learning_rate": 4.683094337875183e-06,
        "epoch": 0.3031430934656741,
        "step": 2199
    },
    {
        "loss": 1.8596,
        "grad_norm": 1.5020090341567993,
        "learning_rate": 4.643745129973231e-06,
        "epoch": 0.30328094844223874,
        "step": 2200
    },
    {
        "loss": 2.058,
        "grad_norm": 0.9659823179244995,
        "learning_rate": 4.604558004124515e-06,
        "epoch": 0.3034188034188034,
        "step": 2201
    },
    {
        "loss": 2.1117,
        "grad_norm": 1.6580337285995483,
        "learning_rate": 4.56553302693743e-06,
        "epoch": 0.30355665839536805,
        "step": 2202
    },
    {
        "loss": 1.3718,
        "grad_norm": 2.271286725997925,
        "learning_rate": 4.52667026474477e-06,
        "epoch": 0.3036945133719327,
        "step": 2203
    },
    {
        "loss": 1.907,
        "grad_norm": 1.4815318584442139,
        "learning_rate": 4.487969783603674e-06,
        "epoch": 0.30383236834849736,
        "step": 2204
    },
    {
        "loss": 2.1325,
        "grad_norm": 1.3330583572387695,
        "learning_rate": 4.4494316492953545e-06,
        "epoch": 0.303970223325062,
        "step": 2205
    },
    {
        "loss": 2.4136,
        "grad_norm": 1.7591561079025269,
        "learning_rate": 4.411055927325103e-06,
        "epoch": 0.30410807830162667,
        "step": 2206
    },
    {
        "loss": 1.9739,
        "grad_norm": 1.3912672996520996,
        "learning_rate": 4.3728426829221515e-06,
        "epoch": 0.3042459332781913,
        "step": 2207
    },
    {
        "loss": 2.1254,
        "grad_norm": 1.2569831609725952,
        "learning_rate": 4.334791981039599e-06,
        "epoch": 0.304383788254756,
        "step": 2208
    },
    {
        "loss": 1.9568,
        "grad_norm": 1.3846406936645508,
        "learning_rate": 4.296903886354209e-06,
        "epoch": 0.30452164323132064,
        "step": 2209
    },
    {
        "loss": 1.7637,
        "grad_norm": 1.7479956150054932,
        "learning_rate": 4.259178463266378e-06,
        "epoch": 0.3046594982078853,
        "step": 2210
    },
    {
        "loss": 2.1174,
        "grad_norm": 1.3806061744689941,
        "learning_rate": 4.221615775899979e-06,
        "epoch": 0.30479735318444995,
        "step": 2211
    },
    {
        "loss": 1.9609,
        "grad_norm": 1.2792402505874634,
        "learning_rate": 4.184215888102316e-06,
        "epoch": 0.3049352081610146,
        "step": 2212
    },
    {
        "loss": 1.793,
        "grad_norm": 2.0201058387756348,
        "learning_rate": 4.146978863443929e-06,
        "epoch": 0.30507306313757926,
        "step": 2213
    },
    {
        "loss": 1.9731,
        "grad_norm": 1.1806116104125977,
        "learning_rate": 4.109904765218598e-06,
        "epoch": 0.3052109181141439,
        "step": 2214
    },
    {
        "loss": 1.4756,
        "grad_norm": 1.9720921516418457,
        "learning_rate": 4.072993656443047e-06,
        "epoch": 0.30534877309070857,
        "step": 2215
    },
    {
        "loss": 1.1474,
        "grad_norm": 2.1361372470855713,
        "learning_rate": 4.03624559985708e-06,
        "epoch": 0.3054866280672732,
        "step": 2216
    },
    {
        "loss": 2.3376,
        "grad_norm": 1.0598918199539185,
        "learning_rate": 3.999660657923332e-06,
        "epoch": 0.3056244830438379,
        "step": 2217
    },
    {
        "loss": 2.0691,
        "grad_norm": 2.844271421432495,
        "learning_rate": 3.963238892827093e-06,
        "epoch": 0.30576233802040254,
        "step": 2218
    },
    {
        "loss": 2.0595,
        "grad_norm": 1.0893540382385254,
        "learning_rate": 3.926980366476396e-06,
        "epoch": 0.3059001929969672,
        "step": 2219
    },
    {
        "loss": 1.7451,
        "grad_norm": 1.046500563621521,
        "learning_rate": 3.890885140501788e-06,
        "epoch": 0.30603804797353185,
        "step": 2220
    },
    {
        "loss": 1.7435,
        "grad_norm": 1.8064303398132324,
        "learning_rate": 3.854953276256179e-06,
        "epoch": 0.3061759029500965,
        "step": 2221
    },
    {
        "loss": 2.4362,
        "grad_norm": 1.8788809776306152,
        "learning_rate": 3.81918483481486e-06,
        "epoch": 0.30631375792666116,
        "step": 2222
    },
    {
        "loss": 2.3681,
        "grad_norm": 2.06144118309021,
        "learning_rate": 3.783579876975385e-06,
        "epoch": 0.3064516129032258,
        "step": 2223
    },
    {
        "loss": 2.1634,
        "grad_norm": 1.8533523082733154,
        "learning_rate": 3.7481384632573447e-06,
        "epoch": 0.30658946787979047,
        "step": 2224
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.34755539894104,
        "learning_rate": 3.7128606539023725e-06,
        "epoch": 0.3067273228563551,
        "step": 2225
    },
    {
        "loss": 2.3317,
        "grad_norm": 1.0762916803359985,
        "learning_rate": 3.6777465088740136e-06,
        "epoch": 0.3068651778329198,
        "step": 2226
    },
    {
        "loss": 2.2264,
        "grad_norm": 1.7107962369918823,
        "learning_rate": 3.6427960878576585e-06,
        "epoch": 0.30700303280948443,
        "step": 2227
    },
    {
        "loss": 2.4505,
        "grad_norm": 1.4811280965805054,
        "learning_rate": 3.6080094502603633e-06,
        "epoch": 0.3071408877860491,
        "step": 2228
    },
    {
        "loss": 1.1358,
        "grad_norm": 2.470028877258301,
        "learning_rate": 3.5733866552108086e-06,
        "epoch": 0.30727874276261374,
        "step": 2229
    },
    {
        "loss": 2.1412,
        "grad_norm": 1.107917308807373,
        "learning_rate": 3.5389277615591967e-06,
        "epoch": 0.3074165977391784,
        "step": 2230
    },
    {
        "loss": 2.1993,
        "grad_norm": 1.0956205129623413,
        "learning_rate": 3.504632827877119e-06,
        "epoch": 0.30755445271574305,
        "step": 2231
    },
    {
        "loss": 1.6588,
        "grad_norm": 1.9650418758392334,
        "learning_rate": 3.4705019124574687e-06,
        "epoch": 0.3076923076923077,
        "step": 2232
    },
    {
        "loss": 2.2355,
        "grad_norm": 1.3071216344833374,
        "learning_rate": 3.4365350733143955e-06,
        "epoch": 0.30783016266887236,
        "step": 2233
    },
    {
        "loss": 1.25,
        "grad_norm": 2.610327959060669,
        "learning_rate": 3.4027323681830702e-06,
        "epoch": 0.307968017645437,
        "step": 2234
    },
    {
        "loss": 2.0951,
        "grad_norm": 1.0591825246810913,
        "learning_rate": 3.3690938545198004e-06,
        "epoch": 0.3081058726220017,
        "step": 2235
    },
    {
        "loss": 2.0837,
        "grad_norm": 1.1296310424804688,
        "learning_rate": 3.335619589501715e-06,
        "epoch": 0.30824372759856633,
        "step": 2236
    },
    {
        "loss": 2.1671,
        "grad_norm": 1.5801103115081787,
        "learning_rate": 3.302309630026745e-06,
        "epoch": 0.308381582575131,
        "step": 2237
    },
    {
        "loss": 2.2811,
        "grad_norm": 1.0140962600708008,
        "learning_rate": 3.269164032713645e-06,
        "epoch": 0.30851943755169564,
        "step": 2238
    },
    {
        "loss": 1.593,
        "grad_norm": 1.7328206300735474,
        "learning_rate": 3.236182853901704e-06,
        "epoch": 0.3086572925282603,
        "step": 2239
    },
    {
        "loss": 2.0339,
        "grad_norm": 1.9100006818771362,
        "learning_rate": 3.203366149650788e-06,
        "epoch": 0.30879514750482495,
        "step": 2240
    },
    {
        "loss": 2.3369,
        "grad_norm": 1.793520212173462,
        "learning_rate": 3.170713975741135e-06,
        "epoch": 0.30893300248138955,
        "step": 2241
    },
    {
        "loss": 2.0706,
        "grad_norm": 1.0334914922714233,
        "learning_rate": 3.1382263876734263e-06,
        "epoch": 0.3090708574579542,
        "step": 2242
    },
    {
        "loss": 2.5007,
        "grad_norm": 1.4231923818588257,
        "learning_rate": 3.1059034406685004e-06,
        "epoch": 0.30920871243451886,
        "step": 2243
    },
    {
        "loss": 2.3465,
        "grad_norm": 1.130616307258606,
        "learning_rate": 3.073745189667354e-06,
        "epoch": 0.3093465674110835,
        "step": 2244
    },
    {
        "loss": 2.2409,
        "grad_norm": 1.003056526184082,
        "learning_rate": 3.041751689331085e-06,
        "epoch": 0.3094844223876482,
        "step": 2245
    },
    {
        "loss": 2.2987,
        "grad_norm": 0.9844005107879639,
        "learning_rate": 3.009922994040748e-06,
        "epoch": 0.30962227736421283,
        "step": 2246
    },
    {
        "loss": 1.494,
        "grad_norm": 3.2490358352661133,
        "learning_rate": 2.978259157897212e-06,
        "epoch": 0.3097601323407775,
        "step": 2247
    },
    {
        "loss": 2.4188,
        "grad_norm": 1.9347327947616577,
        "learning_rate": 2.946760234721202e-06,
        "epoch": 0.30989798731734214,
        "step": 2248
    },
    {
        "loss": 2.2694,
        "grad_norm": 1.9015529155731201,
        "learning_rate": 2.9154262780530906e-06,
        "epoch": 0.3100358422939068,
        "step": 2249
    },
    {
        "loss": 1.93,
        "grad_norm": 1.3708590269088745,
        "learning_rate": 2.8842573411528405e-06,
        "epoch": 0.31017369727047145,
        "step": 2250
    },
    {
        "loss": 2.1868,
        "grad_norm": 1.691959023475647,
        "learning_rate": 2.853253476999962e-06,
        "epoch": 0.3103115522470361,
        "step": 2251
    },
    {
        "loss": 2.0498,
        "grad_norm": 1.5585654973983765,
        "learning_rate": 2.822414738293344e-06,
        "epoch": 0.31044940722360076,
        "step": 2252
    },
    {
        "loss": 2.2808,
        "grad_norm": 1.076932668685913,
        "learning_rate": 2.7917411774512014e-06,
        "epoch": 0.3105872622001654,
        "step": 2253
    },
    {
        "loss": 2.3198,
        "grad_norm": 1.7182328701019287,
        "learning_rate": 2.7612328466110282e-06,
        "epoch": 0.31072511717673007,
        "step": 2254
    },
    {
        "loss": 1.9363,
        "grad_norm": 1.4800007343292236,
        "learning_rate": 2.7308897976294324e-06,
        "epoch": 0.3108629721532947,
        "step": 2255
    },
    {
        "loss": 1.7904,
        "grad_norm": 1.0967761278152466,
        "learning_rate": 2.700712082082091e-06,
        "epoch": 0.3110008271298594,
        "step": 2256
    },
    {
        "loss": 1.3067,
        "grad_norm": 2.3130264282226562,
        "learning_rate": 2.670699751263661e-06,
        "epoch": 0.31113868210642404,
        "step": 2257
    },
    {
        "loss": 1.9784,
        "grad_norm": 2.1485073566436768,
        "learning_rate": 2.640852856187692e-06,
        "epoch": 0.3112765370829887,
        "step": 2258
    },
    {
        "loss": 2.4755,
        "grad_norm": 1.2485896348953247,
        "learning_rate": 2.6111714475865245e-06,
        "epoch": 0.31141439205955335,
        "step": 2259
    },
    {
        "loss": 1.8284,
        "grad_norm": 1.8134143352508545,
        "learning_rate": 2.5816555759111683e-06,
        "epoch": 0.311552247036118,
        "step": 2260
    },
    {
        "loss": 1.5454,
        "grad_norm": 2.3339943885803223,
        "learning_rate": 2.5523052913313694e-06,
        "epoch": 0.31169010201268266,
        "step": 2261
    },
    {
        "loss": 2.2541,
        "grad_norm": 1.315882682800293,
        "learning_rate": 2.5231206437353327e-06,
        "epoch": 0.3118279569892473,
        "step": 2262
    },
    {
        "loss": 2.409,
        "grad_norm": 0.9259834289550781,
        "learning_rate": 2.4941016827297214e-06,
        "epoch": 0.31196581196581197,
        "step": 2263
    },
    {
        "loss": 1.4582,
        "grad_norm": 2.1663501262664795,
        "learning_rate": 2.465248457639602e-06,
        "epoch": 0.3121036669423766,
        "step": 2264
    },
    {
        "loss": 1.5275,
        "grad_norm": 1.4560885429382324,
        "learning_rate": 2.436561017508343e-06,
        "epoch": 0.3122415219189413,
        "step": 2265
    },
    {
        "loss": 2.2675,
        "grad_norm": 1.2153273820877075,
        "learning_rate": 2.4080394110974846e-06,
        "epoch": 0.31237937689550593,
        "step": 2266
    },
    {
        "loss": 2.2847,
        "grad_norm": 2.2465007305145264,
        "learning_rate": 2.3796836868867132e-06,
        "epoch": 0.3125172318720706,
        "step": 2267
    },
    {
        "loss": 1.4564,
        "grad_norm": 1.7504353523254395,
        "learning_rate": 2.3514938930737194e-06,
        "epoch": 0.31265508684863524,
        "step": 2268
    },
    {
        "loss": 2.1103,
        "grad_norm": 1.745151162147522,
        "learning_rate": 2.323470077574208e-06,
        "epoch": 0.3127929418251999,
        "step": 2269
    },
    {
        "loss": 0.5641,
        "grad_norm": 1.5741267204284668,
        "learning_rate": 2.2956122880217423e-06,
        "epoch": 0.31293079680176455,
        "step": 2270
    },
    {
        "loss": 2.0678,
        "grad_norm": 2.048750638961792,
        "learning_rate": 2.2679205717676568e-06,
        "epoch": 0.3130686517783292,
        "step": 2271
    },
    {
        "loss": 2.3601,
        "grad_norm": 1.5242503881454468,
        "learning_rate": 2.2403949758810107e-06,
        "epoch": 0.31320650675489387,
        "step": 2272
    },
    {
        "loss": 2.0582,
        "grad_norm": 1.593367338180542,
        "learning_rate": 2.213035547148545e-06,
        "epoch": 0.3133443617314585,
        "step": 2273
    },
    {
        "loss": 1.7684,
        "grad_norm": 1.9294124841690063,
        "learning_rate": 2.185842332074506e-06,
        "epoch": 0.3134822167080232,
        "step": 2274
    },
    {
        "loss": 2.383,
        "grad_norm": 1.7690736055374146,
        "learning_rate": 2.1588153768806295e-06,
        "epoch": 0.31362007168458783,
        "step": 2275
    },
    {
        "loss": 1.8118,
        "grad_norm": 1.8296303749084473,
        "learning_rate": 2.131954727506058e-06,
        "epoch": 0.3137579266611525,
        "step": 2276
    },
    {
        "loss": 1.8023,
        "grad_norm": 1.4692546129226685,
        "learning_rate": 2.1052604296072586e-06,
        "epoch": 0.31389578163771714,
        "step": 2277
    },
    {
        "loss": 1.9526,
        "grad_norm": 2.102771282196045,
        "learning_rate": 2.0787325285579363e-06,
        "epoch": 0.3140336366142818,
        "step": 2278
    },
    {
        "loss": 2.1951,
        "grad_norm": 1.6395471096038818,
        "learning_rate": 2.052371069448944e-06,
        "epoch": 0.31417149159084645,
        "step": 2279
    },
    {
        "loss": 2.0913,
        "grad_norm": 1.3665915727615356,
        "learning_rate": 2.026176097088295e-06,
        "epoch": 0.3143093465674111,
        "step": 2280
    },
    {
        "loss": 1.5749,
        "grad_norm": 2.2786219120025635,
        "learning_rate": 2.0001476560009057e-06,
        "epoch": 0.31444720154397576,
        "step": 2281
    },
    {
        "loss": 2.0241,
        "grad_norm": 1.1261717081069946,
        "learning_rate": 1.9742857904287204e-06,
        "epoch": 0.3145850565205404,
        "step": 2282
    },
    {
        "loss": 2.0736,
        "grad_norm": 1.2278053760528564,
        "learning_rate": 1.9485905443304973e-06,
        "epoch": 0.314722911497105,
        "step": 2283
    },
    {
        "loss": 1.8316,
        "grad_norm": 2.158010721206665,
        "learning_rate": 1.9230619613818224e-06,
        "epoch": 0.3148607664736697,
        "step": 2284
    },
    {
        "loss": 2.389,
        "grad_norm": 1.2560498714447021,
        "learning_rate": 1.8977000849749626e-06,
        "epoch": 0.31499862145023433,
        "step": 2285
    },
    {
        "loss": 2.2632,
        "grad_norm": 1.1199653148651123,
        "learning_rate": 1.8725049582188348e-06,
        "epoch": 0.315136476426799,
        "step": 2286
    },
    {
        "loss": 1.9226,
        "grad_norm": 1.0257563591003418,
        "learning_rate": 1.8474766239389264e-06,
        "epoch": 0.31527433140336364,
        "step": 2287
    },
    {
        "loss": 1.973,
        "grad_norm": 1.7010303735733032,
        "learning_rate": 1.8226151246772182e-06,
        "epoch": 0.3154121863799283,
        "step": 2288
    },
    {
        "loss": 1.4533,
        "grad_norm": 2.0577216148376465,
        "learning_rate": 1.7979205026921075e-06,
        "epoch": 0.31555004135649295,
        "step": 2289
    },
    {
        "loss": 2.4415,
        "grad_norm": 1.8876760005950928,
        "learning_rate": 1.7733927999583511e-06,
        "epoch": 0.3156878963330576,
        "step": 2290
    },
    {
        "loss": 1.9467,
        "grad_norm": 2.329026937484741,
        "learning_rate": 1.7490320581669772e-06,
        "epoch": 0.31582575130962226,
        "step": 2291
    },
    {
        "loss": 1.5557,
        "grad_norm": 2.582077741622925,
        "learning_rate": 1.724838318725208e-06,
        "epoch": 0.3159636062861869,
        "step": 2292
    },
    {
        "loss": 2.2212,
        "grad_norm": 1.898520588874817,
        "learning_rate": 1.7008116227564486e-06,
        "epoch": 0.31610146126275157,
        "step": 2293
    },
    {
        "loss": 1.7796,
        "grad_norm": 1.8762041330337524,
        "learning_rate": 1.676952011100097e-06,
        "epoch": 0.3162393162393162,
        "step": 2294
    },
    {
        "loss": 2.096,
        "grad_norm": 1.2298476696014404,
        "learning_rate": 1.6532595243116345e-06,
        "epoch": 0.3163771712158809,
        "step": 2295
    },
    {
        "loss": 2.3735,
        "grad_norm": 1.1093425750732422,
        "learning_rate": 1.629734202662414e-06,
        "epoch": 0.31651502619244554,
        "step": 2296
    },
    {
        "loss": 2.0419,
        "grad_norm": 1.4306856393814087,
        "learning_rate": 1.6063760861396603e-06,
        "epoch": 0.3166528811690102,
        "step": 2297
    },
    {
        "loss": 2.162,
        "grad_norm": 2.1416707038879395,
        "learning_rate": 1.5831852144463922e-06,
        "epoch": 0.31679073614557485,
        "step": 2298
    },
    {
        "loss": 1.4278,
        "grad_norm": 2.162449598312378,
        "learning_rate": 1.5601616270013774e-06,
        "epoch": 0.3169285911221395,
        "step": 2299
    },
    {
        "loss": 2.0416,
        "grad_norm": 1.734943151473999,
        "learning_rate": 1.537305362939001e-06,
        "epoch": 0.31706644609870416,
        "step": 2300
    },
    {
        "loss": 1.7446,
        "grad_norm": 1.5417695045471191,
        "learning_rate": 1.5146164611092638e-06,
        "epoch": 0.3172043010752688,
        "step": 2301
    },
    {
        "loss": 2.1631,
        "grad_norm": 1.1786274909973145,
        "learning_rate": 1.4920949600776835e-06,
        "epoch": 0.31734215605183347,
        "step": 2302
    },
    {
        "loss": 1.9747,
        "grad_norm": 1.5371688604354858,
        "learning_rate": 1.4697408981252713e-06,
        "epoch": 0.3174800110283981,
        "step": 2303
    },
    {
        "loss": 1.8496,
        "grad_norm": 1.5080021619796753,
        "learning_rate": 1.4475543132483781e-06,
        "epoch": 0.3176178660049628,
        "step": 2304
    },
    {
        "loss": 2.2895,
        "grad_norm": 1.4019781351089478,
        "learning_rate": 1.4255352431587043e-06,
        "epoch": 0.31775572098152743,
        "step": 2305
    },
    {
        "loss": 2.1774,
        "grad_norm": 1.3450655937194824,
        "learning_rate": 1.4036837252832446e-06,
        "epoch": 0.3178935759580921,
        "step": 2306
    },
    {
        "loss": 2.0025,
        "grad_norm": 1.1512620449066162,
        "learning_rate": 1.3819997967641662e-06,
        "epoch": 0.31803143093465674,
        "step": 2307
    },
    {
        "loss": 2.5192,
        "grad_norm": 1.7327262163162231,
        "learning_rate": 1.3604834944587863e-06,
        "epoch": 0.3181692859112214,
        "step": 2308
    },
    {
        "loss": 2.3614,
        "grad_norm": 1.837113618850708,
        "learning_rate": 1.3391348549395055e-06,
        "epoch": 0.31830714088778606,
        "step": 2309
    },
    {
        "loss": 1.2183,
        "grad_norm": 1.6671696901321411,
        "learning_rate": 1.3179539144937192e-06,
        "epoch": 0.3184449958643507,
        "step": 2310
    },
    {
        "loss": 2.42,
        "grad_norm": 1.4195082187652588,
        "learning_rate": 1.2969407091237951e-06,
        "epoch": 0.31858285084091537,
        "step": 2311
    },
    {
        "loss": 1.9474,
        "grad_norm": 1.6191195249557495,
        "learning_rate": 1.2760952745469956e-06,
        "epoch": 0.31872070581748,
        "step": 2312
    },
    {
        "loss": 2.1326,
        "grad_norm": 1.5031440258026123,
        "learning_rate": 1.2554176461953893e-06,
        "epoch": 0.3188585607940447,
        "step": 2313
    },
    {
        "loss": 2.4206,
        "grad_norm": 2.1001360416412354,
        "learning_rate": 1.234907859215839e-06,
        "epoch": 0.31899641577060933,
        "step": 2314
    },
    {
        "loss": 1.6321,
        "grad_norm": 1.5192471742630005,
        "learning_rate": 1.2145659484699035e-06,
        "epoch": 0.319134270747174,
        "step": 2315
    },
    {
        "loss": 1.5719,
        "grad_norm": 1.9336001873016357,
        "learning_rate": 1.194391948533824e-06,
        "epoch": 0.31927212572373864,
        "step": 2316
    },
    {
        "loss": 1.6698,
        "grad_norm": 1.88713800907135,
        "learning_rate": 1.174385893698371e-06,
        "epoch": 0.3194099807003033,
        "step": 2317
    },
    {
        "loss": 1.2474,
        "grad_norm": 1.9233609437942505,
        "learning_rate": 1.1545478179689206e-06,
        "epoch": 0.31954783567686795,
        "step": 2318
    },
    {
        "loss": 2.4205,
        "grad_norm": 2.268155813217163,
        "learning_rate": 1.1348777550653001e-06,
        "epoch": 0.3196856906534326,
        "step": 2319
    },
    {
        "loss": 1.5002,
        "grad_norm": 1.8052467107772827,
        "learning_rate": 1.1153757384217312e-06,
        "epoch": 0.31982354562999726,
        "step": 2320
    },
    {
        "loss": 1.8017,
        "grad_norm": 2.2548158168792725,
        "learning_rate": 1.0960418011868202e-06,
        "epoch": 0.3199614006065619,
        "step": 2321
    },
    {
        "loss": 1.8327,
        "grad_norm": 0.9912903904914856,
        "learning_rate": 1.0768759762235126e-06,
        "epoch": 0.3200992555831266,
        "step": 2322
    },
    {
        "loss": 1.8551,
        "grad_norm": 1.8133118152618408,
        "learning_rate": 1.0578782961089273e-06,
        "epoch": 0.32023711055969123,
        "step": 2323
    },
    {
        "loss": 2.2723,
        "grad_norm": 1.2888416051864624,
        "learning_rate": 1.039048793134456e-06,
        "epoch": 0.3203749655362559,
        "step": 2324
    },
    {
        "loss": 1.9461,
        "grad_norm": 2.10918927192688,
        "learning_rate": 1.020387499305575e-06,
        "epoch": 0.32051282051282054,
        "step": 2325
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.313859701156616,
        "learning_rate": 1.0018944463418778e-06,
        "epoch": 0.32065067548938514,
        "step": 2326
    },
    {
        "loss": 2.1036,
        "grad_norm": 2.377127170562744,
        "learning_rate": 9.835696656769866e-07,
        "epoch": 0.3207885304659498,
        "step": 2327
    },
    {
        "loss": 1.9212,
        "grad_norm": 2.0135610103607178,
        "learning_rate": 9.654131884584861e-07,
        "epoch": 0.32092638544251445,
        "step": 2328
    },
    {
        "loss": 1.832,
        "grad_norm": 1.5896186828613281,
        "learning_rate": 9.474250455479117e-07,
        "epoch": 0.3210642404190791,
        "step": 2329
    },
    {
        "loss": 1.9081,
        "grad_norm": 1.9696803092956543,
        "learning_rate": 9.296052675206501e-07,
        "epoch": 0.32120209539564376,
        "step": 2330
    },
    {
        "loss": 1.9954,
        "grad_norm": 1.869309663772583,
        "learning_rate": 9.119538846659281e-07,
        "epoch": 0.3213399503722084,
        "step": 2331
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.609445571899414,
        "learning_rate": 8.944709269867013e-07,
        "epoch": 0.32147780534877307,
        "step": 2332
    },
    {
        "loss": 1.6814,
        "grad_norm": 1.8622465133666992,
        "learning_rate": 8.771564241997099e-07,
        "epoch": 0.3216156603253377,
        "step": 2333
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.5153197050094604,
        "learning_rate": 8.60010405735312e-07,
        "epoch": 0.3217535153019024,
        "step": 2334
    },
    {
        "loss": 1.6977,
        "grad_norm": 1.9914313554763794,
        "learning_rate": 8.430329007374949e-07,
        "epoch": 0.32189137027846704,
        "step": 2335
    },
    {
        "loss": 1.0643,
        "grad_norm": 1.8929054737091064,
        "learning_rate": 8.262239380638192e-07,
        "epoch": 0.3220292252550317,
        "step": 2336
    },
    {
        "loss": 2.3097,
        "grad_norm": 1.5224981307983398,
        "learning_rate": 8.095835462853751e-07,
        "epoch": 0.32216708023159635,
        "step": 2337
    },
    {
        "loss": 1.797,
        "grad_norm": 1.789137363433838,
        "learning_rate": 7.931117536867038e-07,
        "epoch": 0.322304935208161,
        "step": 2338
    },
    {
        "loss": 2.4642,
        "grad_norm": 0.9782262444496155,
        "learning_rate": 7.768085882657871e-07,
        "epoch": 0.32244279018472566,
        "step": 2339
    },
    {
        "loss": 2.5925,
        "grad_norm": 1.8299022912979126,
        "learning_rate": 7.606740777339582e-07,
        "epoch": 0.3225806451612903,
        "step": 2340
    },
    {
        "loss": 2.0536,
        "grad_norm": 1.8052558898925781,
        "learning_rate": 7.447082495159241e-07,
        "epoch": 0.32271850013785497,
        "step": 2341
    },
    {
        "loss": 1.8751,
        "grad_norm": 2.2215168476104736,
        "learning_rate": 7.28911130749621e-07,
        "epoch": 0.3228563551144196,
        "step": 2342
    },
    {
        "loss": 1.5326,
        "grad_norm": 2.331048011779785,
        "learning_rate": 7.132827482862591e-07,
        "epoch": 0.3229942100909843,
        "step": 2343
    },
    {
        "loss": 1.9537,
        "grad_norm": 1.6878799200057983,
        "learning_rate": 6.978231286902004e-07,
        "epoch": 0.32313206506754893,
        "step": 2344
    },
    {
        "loss": 2.5458,
        "grad_norm": 1.9947807788848877,
        "learning_rate": 6.825322982390025e-07,
        "epoch": 0.3232699200441136,
        "step": 2345
    },
    {
        "loss": 2.0456,
        "grad_norm": 2.4068548679351807,
        "learning_rate": 6.674102829232864e-07,
        "epoch": 0.32340777502067825,
        "step": 2346
    },
    {
        "loss": 1.3308,
        "grad_norm": 1.6261345148086548,
        "learning_rate": 6.524571084467246e-07,
        "epoch": 0.3235456299972429,
        "step": 2347
    },
    {
        "loss": 1.8033,
        "grad_norm": 1.7252141237258911,
        "learning_rate": 6.376728002260413e-07,
        "epoch": 0.32368348497380756,
        "step": 2348
    },
    {
        "loss": 2.6281,
        "grad_norm": 1.1306092739105225,
        "learning_rate": 6.230573833908904e-07,
        "epoch": 0.3238213399503722,
        "step": 2349
    },
    {
        "loss": 2.4511,
        "grad_norm": 0.8797435164451599,
        "learning_rate": 6.086108827838666e-07,
        "epoch": 0.32395919492693687,
        "step": 2350
    },
    {
        "loss": 1.7348,
        "grad_norm": 1.7926385402679443,
        "learning_rate": 5.943333229604497e-07,
        "epoch": 0.3240970499035015,
        "step": 2351
    },
    {
        "loss": 2.278,
        "grad_norm": 0.9913903474807739,
        "learning_rate": 5.802247281889494e-07,
        "epoch": 0.3242349048800662,
        "step": 2352
    },
    {
        "loss": 1.784,
        "grad_norm": 2.49794864654541,
        "learning_rate": 5.662851224505161e-07,
        "epoch": 0.32437275985663083,
        "step": 2353
    },
    {
        "loss": 2.534,
        "grad_norm": 2.0155322551727295,
        "learning_rate": 5.525145294390189e-07,
        "epoch": 0.3245106148331955,
        "step": 2354
    },
    {
        "loss": 1.8528,
        "grad_norm": 2.2751944065093994,
        "learning_rate": 5.389129725610343e-07,
        "epoch": 0.32464846980976014,
        "step": 2355
    },
    {
        "loss": 2.3639,
        "grad_norm": 1.280584692955017,
        "learning_rate": 5.25480474935891e-07,
        "epoch": 0.3247863247863248,
        "step": 2356
    },
    {
        "loss": 2.0201,
        "grad_norm": 1.6640015840530396,
        "learning_rate": 5.122170593954923e-07,
        "epoch": 0.32492417976288945,
        "step": 2357
    },
    {
        "loss": 2.3653,
        "grad_norm": 1.5296940803527832,
        "learning_rate": 4.991227484843486e-07,
        "epoch": 0.3250620347394541,
        "step": 2358
    },
    {
        "loss": 2.1043,
        "grad_norm": 1.2230020761489868,
        "learning_rate": 4.861975644595673e-07,
        "epoch": 0.32519988971601876,
        "step": 2359
    },
    {
        "loss": 2.4622,
        "grad_norm": 1.211875081062317,
        "learning_rate": 4.734415292907746e-07,
        "epoch": 0.3253377446925834,
        "step": 2360
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.1024366617202759,
        "learning_rate": 4.6085466466005977e-07,
        "epoch": 0.3254755996691481,
        "step": 2361
    },
    {
        "loss": 2.2906,
        "grad_norm": 1.039880633354187,
        "learning_rate": 4.4843699196197574e-07,
        "epoch": 0.32561345464571273,
        "step": 2362
    },
    {
        "loss": 1.6994,
        "grad_norm": 2.5102343559265137,
        "learning_rate": 4.3618853230351643e-07,
        "epoch": 0.3257513096222774,
        "step": 2363
    },
    {
        "loss": 1.5815,
        "grad_norm": 2.4101526737213135,
        "learning_rate": 4.241093065040169e-07,
        "epoch": 0.32588916459884204,
        "step": 2364
    },
    {
        "loss": 1.7443,
        "grad_norm": 1.551642894744873,
        "learning_rate": 4.1219933509518694e-07,
        "epoch": 0.3260270195754067,
        "step": 2365
    },
    {
        "loss": 2.1828,
        "grad_norm": 1.4767441749572754,
        "learning_rate": 4.004586383210218e-07,
        "epoch": 0.32616487455197135,
        "step": 2366
    },
    {
        "loss": 1.9824,
        "grad_norm": 2.3102805614471436,
        "learning_rate": 3.8888723613780264e-07,
        "epoch": 0.326302729528536,
        "step": 2367
    },
    {
        "loss": 1.5719,
        "grad_norm": 1.867828130722046,
        "learning_rate": 3.774851482140518e-07,
        "epoch": 0.32644058450510066,
        "step": 2368
    },
    {
        "loss": 2.2861,
        "grad_norm": 2.0946404933929443,
        "learning_rate": 3.662523939305218e-07,
        "epoch": 0.32657843948166526,
        "step": 2369
    },
    {
        "loss": 1.8552,
        "grad_norm": 1.8179707527160645,
        "learning_rate": 3.551889923800733e-07,
        "epoch": 0.3267162944582299,
        "step": 2370
    },
    {
        "loss": 1.8787,
        "grad_norm": 1.4327070713043213,
        "learning_rate": 3.4429496236779713e-07,
        "epoch": 0.32685414943479457,
        "step": 2371
    },
    {
        "loss": 1.287,
        "grad_norm": 1.9105252027511597,
        "learning_rate": 3.3357032241082553e-07,
        "epoch": 0.3269920044113592,
        "step": 2372
    },
    {
        "loss": 2.2485,
        "grad_norm": 1.6258665323257446,
        "learning_rate": 3.230150907383989e-07,
        "epoch": 0.3271298593879239,
        "step": 2373
    },
    {
        "loss": 1.8981,
        "grad_norm": 1.5727664232254028,
        "learning_rate": 3.126292852917989e-07,
        "epoch": 0.32726771436448854,
        "step": 2374
    },
    {
        "loss": 1.9615,
        "grad_norm": 0.9991735219955444,
        "learning_rate": 3.024129237243378e-07,
        "epoch": 0.3274055693410532,
        "step": 2375
    },
    {
        "loss": 1.7694,
        "grad_norm": 1.7975119352340698,
        "learning_rate": 2.923660234013026e-07,
        "epoch": 0.32754342431761785,
        "step": 2376
    },
    {
        "loss": 1.6466,
        "grad_norm": 2.298781394958496,
        "learning_rate": 2.8248860139992173e-07,
        "epoch": 0.3276812792941825,
        "step": 2377
    },
    {
        "loss": 2.3292,
        "grad_norm": 2.1919362545013428,
        "learning_rate": 2.727806745093764e-07,
        "epoch": 0.32781913427074716,
        "step": 2378
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.5733951330184937,
        "learning_rate": 2.63242259230756e-07,
        "epoch": 0.3279569892473118,
        "step": 2379
    },
    {
        "loss": 1.7781,
        "grad_norm": 1.206959843635559,
        "learning_rate": 2.538733717770025e-07,
        "epoch": 0.32809484422387647,
        "step": 2380
    },
    {
        "loss": 2.2798,
        "grad_norm": 1.9864531755447388,
        "learning_rate": 2.4467402807288875e-07,
        "epoch": 0.3282326992004411,
        "step": 2381
    },
    {
        "loss": 1.2729,
        "grad_norm": 2.470304250717163,
        "learning_rate": 2.3564424375505102e-07,
        "epoch": 0.3283705541770058,
        "step": 2382
    },
    {
        "loss": 2.1464,
        "grad_norm": 1.7381242513656616,
        "learning_rate": 2.2678403417187854e-07,
        "epoch": 0.32850840915357044,
        "step": 2383
    },
    {
        "loss": 1.8092,
        "grad_norm": 1.9438449144363403,
        "learning_rate": 2.1809341438353558e-07,
        "epoch": 0.3286462641301351,
        "step": 2384
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.994136929512024,
        "learning_rate": 2.0957239916192807e-07,
        "epoch": 0.32878411910669975,
        "step": 2385
    },
    {
        "loss": 2.4939,
        "grad_norm": 1.1106505393981934,
        "learning_rate": 2.0122100299067026e-07,
        "epoch": 0.3289219740832644,
        "step": 2386
    },
    {
        "loss": 2.0296,
        "grad_norm": 1.4519307613372803,
        "learning_rate": 1.930392400650738e-07,
        "epoch": 0.32905982905982906,
        "step": 2387
    },
    {
        "loss": 1.7485,
        "grad_norm": 1.999016284942627,
        "learning_rate": 1.8502712429211422e-07,
        "epoch": 0.3291976840363937,
        "step": 2388
    },
    {
        "loss": 2.5588,
        "grad_norm": 1.571679711341858,
        "learning_rate": 1.7718466929039778e-07,
        "epoch": 0.32933553901295837,
        "step": 2389
    },
    {
        "loss": 1.7481,
        "grad_norm": 2.1196417808532715,
        "learning_rate": 1.6951188839017253e-07,
        "epoch": 0.329473393989523,
        "step": 2390
    },
    {
        "loss": 2.2139,
        "grad_norm": 1.9518879652023315,
        "learning_rate": 1.6200879463327267e-07,
        "epoch": 0.3296112489660877,
        "step": 2391
    },
    {
        "loss": 2.3334,
        "grad_norm": 1.7841744422912598,
        "learning_rate": 1.546754007730855e-07,
        "epoch": 0.32974910394265233,
        "step": 2392
    },
    {
        "loss": 2.4038,
        "grad_norm": 1.5780560970306396,
        "learning_rate": 1.475117192745623e-07,
        "epoch": 0.329886958919217,
        "step": 2393
    },
    {
        "loss": 1.6124,
        "grad_norm": 2.1352202892303467,
        "learning_rate": 1.4051776231421844e-07,
        "epoch": 0.33002481389578164,
        "step": 2394
    },
    {
        "loss": 2.2199,
        "grad_norm": 1.3971728086471558,
        "learning_rate": 1.3369354178002225e-07,
        "epoch": 0.3301626688723463,
        "step": 2395
    },
    {
        "loss": 1.9224,
        "grad_norm": 2.6099071502685547,
        "learning_rate": 1.27039069271484e-07,
        "epoch": 0.33030052384891095,
        "step": 2396
    },
    {
        "loss": 1.4614,
        "grad_norm": 2.5930376052856445,
        "learning_rate": 1.205543560995448e-07,
        "epoch": 0.3304383788254756,
        "step": 2397
    },
    {
        "loss": 0.8525,
        "grad_norm": 2.5262861251831055,
        "learning_rate": 1.1423941328662091e-07,
        "epoch": 0.33057623380204026,
        "step": 2398
    },
    {
        "loss": 1.7659,
        "grad_norm": 2.1942999362945557,
        "learning_rate": 1.080942515665484e-07,
        "epoch": 0.3307140887786049,
        "step": 2399
    },
    {
        "loss": 2.1855,
        "grad_norm": 1.3357441425323486,
        "learning_rate": 1.0211888138458304e-07,
        "epoch": 0.3308519437551696,
        "step": 2400
    },
    {
        "loss": 1.7564,
        "grad_norm": 2.512274980545044,
        "learning_rate": 9.63133128973892e-08,
        "epoch": 0.33098979873173423,
        "step": 2401
    },
    {
        "loss": 2.2032,
        "grad_norm": 1.4566141366958618,
        "learning_rate": 9.067755597298444e-08,
        "epoch": 0.3311276537082989,
        "step": 2402
    },
    {
        "loss": 1.5982,
        "grad_norm": 1.9042714834213257,
        "learning_rate": 8.521162019076157e-08,
        "epoch": 0.33126550868486354,
        "step": 2403
    },
    {
        "loss": 2.4344,
        "grad_norm": 1.0251483917236328,
        "learning_rate": 7.991551484146653e-08,
        "epoch": 0.3314033636614282,
        "step": 2404
    },
    {
        "loss": 2.1781,
        "grad_norm": 1.7558367252349854,
        "learning_rate": 7.478924892717621e-08,
        "epoch": 0.33154121863799285,
        "step": 2405
    },
    {
        "loss": 2.199,
        "grad_norm": 1.5780029296875,
        "learning_rate": 6.983283116125395e-08,
        "epoch": 0.3316790736145575,
        "step": 2406
    },
    {
        "loss": 1.7977,
        "grad_norm": 1.4925556182861328,
        "learning_rate": 6.504626996839402e-08,
        "epoch": 0.33181692859112216,
        "step": 2407
    },
    {
        "loss": 1.412,
        "grad_norm": 2.1907927989959717,
        "learning_rate": 6.042957348457723e-08,
        "epoch": 0.3319547835676868,
        "step": 2408
    },
    {
        "loss": 1.2511,
        "grad_norm": 2.019906520843506,
        "learning_rate": 5.5982749557026424e-08,
        "epoch": 0.3320926385442515,
        "step": 2409
    },
    {
        "loss": 2.105,
        "grad_norm": 2.057023286819458,
        "learning_rate": 5.170580574426209e-08,
        "epoch": 0.33223049352081613,
        "step": 2410
    },
    {
        "loss": 2.4777,
        "grad_norm": 1.0424245595932007,
        "learning_rate": 4.759874931601349e-08,
        "epoch": 0.33236834849738073,
        "step": 2411
    },
    {
        "loss": 1.5859,
        "grad_norm": 1.7339473962783813,
        "learning_rate": 4.366158725327418e-08,
        "epoch": 0.3325062034739454,
        "step": 2412
    },
    {
        "loss": 1.5846,
        "grad_norm": 2.2821950912475586,
        "learning_rate": 3.9894326248246515e-08,
        "epoch": 0.33264405845051004,
        "step": 2413
    },
    {
        "loss": 2.1901,
        "grad_norm": 2.0299594402313232,
        "learning_rate": 3.629697270433052e-08,
        "epoch": 0.3327819134270747,
        "step": 2414
    },
    {
        "loss": 2.356,
        "grad_norm": 1.0316903591156006,
        "learning_rate": 3.286953273614612e-08,
        "epoch": 0.33291976840363935,
        "step": 2415
    },
    {
        "loss": 2.2725,
        "grad_norm": 0.9757974147796631,
        "learning_rate": 2.9612012169488723e-08,
        "epoch": 0.333057623380204,
        "step": 2416
    },
    {
        "loss": 2.0423,
        "grad_norm": 1.923272967338562,
        "learning_rate": 2.6524416541351406e-08,
        "epoch": 0.33319547835676866,
        "step": 2417
    },
    {
        "loss": 2.1537,
        "grad_norm": 1.0627042055130005,
        "learning_rate": 2.3606751099858326e-08,
        "epoch": 0.3333333333333333,
        "step": 2418
    },
    {
        "loss": 2.2161,
        "grad_norm": 1.1383053064346313,
        "learning_rate": 2.0859020804342432e-08,
        "epoch": 0.33347118830989797,
        "step": 2419
    },
    {
        "loss": 2.1285,
        "grad_norm": 1.6142889261245728,
        "learning_rate": 1.828123032526774e-08,
        "epoch": 0.3336090432864626,
        "step": 2420
    },
    {
        "loss": 1.5636,
        "grad_norm": 2.1576175689697266,
        "learning_rate": 1.5873384044218233e-08,
        "epoch": 0.3337468982630273,
        "step": 2421
    },
    {
        "loss": 2.2452,
        "grad_norm": 1.3318049907684326,
        "learning_rate": 1.363548605394227e-08,
        "epoch": 0.33388475323959194,
        "step": 2422
    },
    {
        "loss": 1.8641,
        "grad_norm": 1.2577084302902222,
        "learning_rate": 1.1567540158330392e-08,
        "epoch": 0.3340226082161566,
        "step": 2423
    },
    {
        "loss": 1.3299,
        "grad_norm": 2.7607367038726807,
        "learning_rate": 9.669549872370897e-09,
        "epoch": 0.33416046319272125,
        "step": 2424
    },
    {
        "loss": 2.3092,
        "grad_norm": 1.4555435180664062,
        "learning_rate": 7.941518422172056e-09,
        "epoch": 0.3342983181692859,
        "step": 2425
    },
    {
        "loss": 1.7172,
        "grad_norm": 1.621970534324646,
        "learning_rate": 6.383448744962106e-09,
        "epoch": 0.33443617314585056,
        "step": 2426
    },
    {
        "loss": 1.3863,
        "grad_norm": 2.0923871994018555,
        "learning_rate": 4.9953434890781526e-09,
        "epoch": 0.3345740281224152,
        "step": 2427
    },
    {
        "loss": 1.8267,
        "grad_norm": 1.8921093940734863,
        "learning_rate": 3.777205013955065e-09,
        "epoch": 0.33471188309897987,
        "step": 2428
    },
    {
        "loss": 1.9545,
        "grad_norm": 1.7678569555282593,
        "learning_rate": 2.729035390114376e-09,
        "epoch": 0.3348497380755445,
        "step": 2429
    },
    {
        "loss": 1.9002,
        "grad_norm": 1.2788082361221313,
        "learning_rate": 1.8508363992086885e-09,
        "epoch": 0.3349875930521092,
        "step": 2430
    },
    {
        "loss": 2.1822,
        "grad_norm": 0.9496129751205444,
        "learning_rate": 1.1426095339439614e-09,
        "epoch": 0.33512544802867383,
        "step": 2431
    },
    {
        "loss": 2.1869,
        "grad_norm": 1.1405869722366333,
        "learning_rate": 6.0435599813502e-10,
        "epoch": 0.3352633030052385,
        "step": 2432
    },
    {
        "loss": 1.7897,
        "grad_norm": 1.512205958366394,
        "learning_rate": 2.360767066722502e-10,
        "epoch": 0.33540115798180314,
        "step": 2433
    },
    {
        "loss": 2.0294,
        "grad_norm": 1.0270994901657104,
        "learning_rate": 3.777228555490453e-11,
        "epoch": 0.3355390129583678,
        "step": 2434
    },
    {
        "loss": 1.2236,
        "grad_norm": 2.6007747650146484,
        "learning_rate": 0.00019999999055692817,
        "epoch": 0.33567686793493245,
        "step": 2435
    },
    {
        "loss": 2.0851,
        "grad_norm": 0.9471049308776855,
        "learning_rate": 0.00019999984891088635,
        "epoch": 0.3358147229114971,
        "step": 2436
    },
    {
        "loss": 2.1352,
        "grad_norm": 1.4666380882263184,
        "learning_rate": 0.0001999995372898297,
        "epoch": 0.33595257788806177,
        "step": 2437
    },
    {
        "loss": 2.4759,
        "grad_norm": 1.4724349975585938,
        "learning_rate": 0.00019999905569428793,
        "epoch": 0.3360904328646264,
        "step": 2438
    },
    {
        "loss": 2.1894,
        "grad_norm": 1.1932339668273926,
        "learning_rate": 0.00019999840412507968,
        "epoch": 0.3362282878411911,
        "step": 2439
    },
    {
        "loss": 1.8328,
        "grad_norm": 2.235435724258423,
        "learning_rate": 0.0001999975825833124,
        "epoch": 0.33636614281775573,
        "step": 2440
    },
    {
        "loss": 2.4698,
        "grad_norm": 1.5004119873046875,
        "learning_rate": 0.00019999659107038253,
        "epoch": 0.3365039977943204,
        "step": 2441
    },
    {
        "loss": 0.7941,
        "grad_norm": 2.454866886138916,
        "learning_rate": 0.00019999542958797534,
        "epoch": 0.33664185277088504,
        "step": 2442
    },
    {
        "loss": 2.2304,
        "grad_norm": 1.6037988662719727,
        "learning_rate": 0.00019999409813806516,
        "epoch": 0.3367797077474497,
        "step": 2443
    },
    {
        "loss": 1.9934,
        "grad_norm": 2.3107972145080566,
        "learning_rate": 0.00019999259672291508,
        "epoch": 0.33691756272401435,
        "step": 2444
    },
    {
        "loss": 2.5074,
        "grad_norm": 1.8558785915374756,
        "learning_rate": 0.00019999092534507713,
        "epoch": 0.337055417700579,
        "step": 2445
    },
    {
        "loss": 2.0309,
        "grad_norm": 1.699374794960022,
        "learning_rate": 0.00019998908400739221,
        "epoch": 0.33719327267714366,
        "step": 2446
    },
    {
        "loss": 1.1564,
        "grad_norm": 2.120502471923828,
        "learning_rate": 0.0001999870727129902,
        "epoch": 0.3373311276537083,
        "step": 2447
    },
    {
        "loss": 2.1251,
        "grad_norm": 1.810509443283081,
        "learning_rate": 0.00019998489146528975,
        "epoch": 0.337468982630273,
        "step": 2448
    },
    {
        "loss": 1.8486,
        "grad_norm": 1.5403715372085571,
        "learning_rate": 0.0001999825402679985,
        "epoch": 0.33760683760683763,
        "step": 2449
    },
    {
        "loss": 1.6147,
        "grad_norm": 1.1988874673843384,
        "learning_rate": 0.00019998001912511284,
        "epoch": 0.3377446925834023,
        "step": 2450
    },
    {
        "loss": 2.1459,
        "grad_norm": 1.5595163106918335,
        "learning_rate": 0.0001999773280409181,
        "epoch": 0.33788254755996694,
        "step": 2451
    },
    {
        "loss": 1.5886,
        "grad_norm": 1.9835710525512695,
        "learning_rate": 0.00019997446701998852,
        "epoch": 0.3380204025365316,
        "step": 2452
    },
    {
        "loss": 1.6559,
        "grad_norm": 2.3665170669555664,
        "learning_rate": 0.00019997143606718707,
        "epoch": 0.33815825751309625,
        "step": 2453
    },
    {
        "loss": 1.8922,
        "grad_norm": 1.6827746629714966,
        "learning_rate": 0.00019996823518766564,
        "epoch": 0.33829611248966085,
        "step": 2454
    },
    {
        "loss": 2.4971,
        "grad_norm": 1.631801962852478,
        "learning_rate": 0.0001999648643868649,
        "epoch": 0.3384339674662255,
        "step": 2455
    },
    {
        "loss": 1.6065,
        "grad_norm": 1.4312028884887695,
        "learning_rate": 0.00019996132367051443,
        "epoch": 0.33857182244279016,
        "step": 2456
    },
    {
        "loss": 2.2375,
        "grad_norm": 1.5746747255325317,
        "learning_rate": 0.00019995761304463254,
        "epoch": 0.3387096774193548,
        "step": 2457
    },
    {
        "loss": 1.9947,
        "grad_norm": 3.178015947341919,
        "learning_rate": 0.00019995373251552637,
        "epoch": 0.33884753239591947,
        "step": 2458
    },
    {
        "loss": 2.3495,
        "grad_norm": 1.6952664852142334,
        "learning_rate": 0.0001999496820897919,
        "epoch": 0.3389853873724841,
        "step": 2459
    },
    {
        "loss": 2.2458,
        "grad_norm": 1.6858755350112915,
        "learning_rate": 0.0001999454617743138,
        "epoch": 0.3391232423490488,
        "step": 2460
    },
    {
        "loss": 2.1809,
        "grad_norm": 1.75954270362854,
        "learning_rate": 0.00019994107157626561,
        "epoch": 0.33926109732561344,
        "step": 2461
    },
    {
        "loss": 2.0077,
        "grad_norm": 1.791816234588623,
        "learning_rate": 0.00019993651150310957,
        "epoch": 0.3393989523021781,
        "step": 2462
    },
    {
        "loss": 2.5023,
        "grad_norm": 1.7593525648117065,
        "learning_rate": 0.00019993178156259663,
        "epoch": 0.33953680727874275,
        "step": 2463
    },
    {
        "loss": 0.6098,
        "grad_norm": 2.2111949920654297,
        "learning_rate": 0.0001999268817627666,
        "epoch": 0.3396746622553074,
        "step": 2464
    },
    {
        "loss": 1.8966,
        "grad_norm": 2.2454214096069336,
        "learning_rate": 0.00019992181211194788,
        "epoch": 0.33981251723187206,
        "step": 2465
    },
    {
        "loss": 2.2787,
        "grad_norm": 1.0766632556915283,
        "learning_rate": 0.0001999165726187576,
        "epoch": 0.3399503722084367,
        "step": 2466
    },
    {
        "loss": 1.8571,
        "grad_norm": 1.5407236814498901,
        "learning_rate": 0.00019991116329210165,
        "epoch": 0.34008822718500137,
        "step": 2467
    },
    {
        "loss": 2.1034,
        "grad_norm": 1.6080812215805054,
        "learning_rate": 0.0001999055841411745,
        "epoch": 0.340226082161566,
        "step": 2468
    },
    {
        "loss": 1.7206,
        "grad_norm": 2.519319534301758,
        "learning_rate": 0.00019989983517545938,
        "epoch": 0.3403639371381307,
        "step": 2469
    },
    {
        "loss": 0.663,
        "grad_norm": 2.616027593612671,
        "learning_rate": 0.00019989391640472807,
        "epoch": 0.34050179211469533,
        "step": 2470
    },
    {
        "loss": 1.8573,
        "grad_norm": 2.2176673412323,
        "learning_rate": 0.00019988782783904104,
        "epoch": 0.34063964709126,
        "step": 2471
    },
    {
        "loss": 2.4352,
        "grad_norm": 1.3334592580795288,
        "learning_rate": 0.00019988156948874734,
        "epoch": 0.34077750206782464,
        "step": 2472
    },
    {
        "loss": 2.3271,
        "grad_norm": 2.0032880306243896,
        "learning_rate": 0.0001998751413644846,
        "epoch": 0.3409153570443893,
        "step": 2473
    },
    {
        "loss": 2.227,
        "grad_norm": 1.5834083557128906,
        "learning_rate": 0.00019986854347717908,
        "epoch": 0.34105321202095396,
        "step": 2474
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.2330455780029297,
        "learning_rate": 0.00019986177583804552,
        "epoch": 0.3411910669975186,
        "step": 2475
    },
    {
        "loss": 2.2408,
        "grad_norm": 1.4189878702163696,
        "learning_rate": 0.00019985483845858727,
        "epoch": 0.34132892197408327,
        "step": 2476
    },
    {
        "loss": 1.1988,
        "grad_norm": 2.7672536373138428,
        "learning_rate": 0.0001998477313505961,
        "epoch": 0.3414667769506479,
        "step": 2477
    },
    {
        "loss": 2.3455,
        "grad_norm": 1.3987808227539062,
        "learning_rate": 0.00019984045452615238,
        "epoch": 0.3416046319272126,
        "step": 2478
    },
    {
        "loss": 1.9928,
        "grad_norm": 2.3471567630767822,
        "learning_rate": 0.0001998330079976249,
        "epoch": 0.34174248690377723,
        "step": 2479
    },
    {
        "loss": 2.1606,
        "grad_norm": 1.6337453126907349,
        "learning_rate": 0.00019982539177767095,
        "epoch": 0.3418803418803419,
        "step": 2480
    },
    {
        "loss": 1.6301,
        "grad_norm": 2.395263671875,
        "learning_rate": 0.00019981760587923616,
        "epoch": 0.34201819685690654,
        "step": 2481
    },
    {
        "loss": 1.9549,
        "grad_norm": 1.3437886238098145,
        "learning_rate": 0.00019980965031555467,
        "epoch": 0.3421560518334712,
        "step": 2482
    },
    {
        "loss": 1.7043,
        "grad_norm": 1.6214160919189453,
        "learning_rate": 0.00019980152510014898,
        "epoch": 0.34229390681003585,
        "step": 2483
    },
    {
        "loss": 1.696,
        "grad_norm": 2.0117316246032715,
        "learning_rate": 0.00019979323024682993,
        "epoch": 0.3424317617866005,
        "step": 2484
    },
    {
        "loss": 2.1327,
        "grad_norm": 2.4111926555633545,
        "learning_rate": 0.0001997847657696967,
        "epoch": 0.34256961676316516,
        "step": 2485
    },
    {
        "loss": 2.4847,
        "grad_norm": 1.6510167121887207,
        "learning_rate": 0.00019977613168313684,
        "epoch": 0.3427074717397298,
        "step": 2486
    },
    {
        "loss": 2.0071,
        "grad_norm": 1.908502459526062,
        "learning_rate": 0.0001997673280018262,
        "epoch": 0.3428453267162945,
        "step": 2487
    },
    {
        "loss": 2.3917,
        "grad_norm": 1.3224022388458252,
        "learning_rate": 0.00019975835474072876,
        "epoch": 0.34298318169285913,
        "step": 2488
    },
    {
        "loss": 1.8516,
        "grad_norm": 1.799892544746399,
        "learning_rate": 0.0001997492119150969,
        "epoch": 0.3431210366694238,
        "step": 2489
    },
    {
        "loss": 2.2541,
        "grad_norm": 1.3984711170196533,
        "learning_rate": 0.00019973989954047116,
        "epoch": 0.34325889164598844,
        "step": 2490
    },
    {
        "loss": 2.0475,
        "grad_norm": 1.6702197790145874,
        "learning_rate": 0.00019973041763268027,
        "epoch": 0.3433967466225531,
        "step": 2491
    },
    {
        "loss": 1.9614,
        "grad_norm": 2.0328664779663086,
        "learning_rate": 0.00019972076620784117,
        "epoch": 0.34353460159911775,
        "step": 2492
    },
    {
        "loss": 2.0165,
        "grad_norm": 2.001715898513794,
        "learning_rate": 0.00019971094528235884,
        "epoch": 0.3436724565756824,
        "step": 2493
    },
    {
        "loss": 1.5696,
        "grad_norm": 2.2099928855895996,
        "learning_rate": 0.00019970095487292642,
        "epoch": 0.34381031155224706,
        "step": 2494
    },
    {
        "loss": 1.6953,
        "grad_norm": 1.9425346851348877,
        "learning_rate": 0.00019969079499652516,
        "epoch": 0.3439481665288117,
        "step": 2495
    },
    {
        "loss": 1.4406,
        "grad_norm": 2.4729650020599365,
        "learning_rate": 0.00019968046567042438,
        "epoch": 0.34408602150537637,
        "step": 2496
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.1330679655075073,
        "learning_rate": 0.00019966996691218132,
        "epoch": 0.34422387648194097,
        "step": 2497
    },
    {
        "loss": 2.1394,
        "grad_norm": 1.684651494026184,
        "learning_rate": 0.00019965929873964127,
        "epoch": 0.3443617314585056,
        "step": 2498
    },
    {
        "loss": 2.2893,
        "grad_norm": 1.5485620498657227,
        "learning_rate": 0.0001996484611709375,
        "epoch": 0.3444995864350703,
        "step": 2499
    },
    {
        "loss": 2.0646,
        "grad_norm": 1.5306224822998047,
        "learning_rate": 0.0001996374542244912,
        "epoch": 0.34463744141163494,
        "step": 2500
    },
    {
        "loss": 2.1482,
        "grad_norm": 1.98751699924469,
        "learning_rate": 0.00019962627791901148,
        "epoch": 0.3447752963881996,
        "step": 2501
    },
    {
        "loss": 1.9147,
        "grad_norm": 1.4021961688995361,
        "learning_rate": 0.00019961493227349528,
        "epoch": 0.34491315136476425,
        "step": 2502
    },
    {
        "loss": 2.1059,
        "grad_norm": 2.137951135635376,
        "learning_rate": 0.00019960341730722737,
        "epoch": 0.3450510063413289,
        "step": 2503
    },
    {
        "loss": 1.4352,
        "grad_norm": 2.6901893615722656,
        "learning_rate": 0.00019959173303978037,
        "epoch": 0.34518886131789356,
        "step": 2504
    },
    {
        "loss": 1.9589,
        "grad_norm": 1.8201879262924194,
        "learning_rate": 0.00019957987949101465,
        "epoch": 0.3453267162944582,
        "step": 2505
    },
    {
        "loss": 1.6566,
        "grad_norm": 1.669140338897705,
        "learning_rate": 0.00019956785668107826,
        "epoch": 0.34546457127102287,
        "step": 2506
    },
    {
        "loss": 2.4169,
        "grad_norm": 1.390851378440857,
        "learning_rate": 0.00019955566463040705,
        "epoch": 0.3456024262475875,
        "step": 2507
    },
    {
        "loss": 2.1407,
        "grad_norm": 1.7023056745529175,
        "learning_rate": 0.00019954330335972452,
        "epoch": 0.3457402812241522,
        "step": 2508
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.7975194454193115,
        "learning_rate": 0.0001995307728900417,
        "epoch": 0.34587813620071683,
        "step": 2509
    },
    {
        "loss": 2.045,
        "grad_norm": 2.070816993713379,
        "learning_rate": 0.0001995180732426573,
        "epoch": 0.3460159911772815,
        "step": 2510
    },
    {
        "loss": 2.2437,
        "grad_norm": 0.983411967754364,
        "learning_rate": 0.0001995052044391577,
        "epoch": 0.34615384615384615,
        "step": 2511
    },
    {
        "loss": 2.7294,
        "grad_norm": 1.2755422592163086,
        "learning_rate": 0.00019949216650141653,
        "epoch": 0.3462917011304108,
        "step": 2512
    },
    {
        "loss": 1.8971,
        "grad_norm": 1.204664707183838,
        "learning_rate": 0.0001994789594515951,
        "epoch": 0.34642955610697546,
        "step": 2513
    },
    {
        "loss": 1.4369,
        "grad_norm": 1.87581205368042,
        "learning_rate": 0.0001994655833121422,
        "epoch": 0.3465674110835401,
        "step": 2514
    },
    {
        "loss": 2.1421,
        "grad_norm": 1.243715524673462,
        "learning_rate": 0.00019945203810579386,
        "epoch": 0.34670526606010477,
        "step": 2515
    },
    {
        "loss": 2.0755,
        "grad_norm": 1.6673932075500488,
        "learning_rate": 0.00019943832385557365,
        "epoch": 0.3468431210366694,
        "step": 2516
    },
    {
        "loss": 1.9651,
        "grad_norm": 1.9252521991729736,
        "learning_rate": 0.00019942444058479238,
        "epoch": 0.3469809760132341,
        "step": 2517
    },
    {
        "loss": 1.6425,
        "grad_norm": 1.9125616550445557,
        "learning_rate": 0.0001994103883170482,
        "epoch": 0.34711883098979873,
        "step": 2518
    },
    {
        "loss": 2.0836,
        "grad_norm": 1.4963136911392212,
        "learning_rate": 0.00019939616707622646,
        "epoch": 0.3472566859663634,
        "step": 2519
    },
    {
        "loss": 2.0228,
        "grad_norm": 1.1703896522521973,
        "learning_rate": 0.0001993817768864998,
        "epoch": 0.34739454094292804,
        "step": 2520
    },
    {
        "loss": 2.4194,
        "grad_norm": 1.2821377515792847,
        "learning_rate": 0.00019936721777232788,
        "epoch": 0.3475323959194927,
        "step": 2521
    },
    {
        "loss": 2.0998,
        "grad_norm": 1.414768099784851,
        "learning_rate": 0.00019935248975845772,
        "epoch": 0.34767025089605735,
        "step": 2522
    },
    {
        "loss": 1.9582,
        "grad_norm": 2.0193896293640137,
        "learning_rate": 0.00019933759286992322,
        "epoch": 0.347808105872622,
        "step": 2523
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.588110089302063,
        "learning_rate": 0.0001993225271320454,
        "epoch": 0.34794596084918666,
        "step": 2524
    },
    {
        "loss": 1.6951,
        "grad_norm": 2.010427474975586,
        "learning_rate": 0.00019930729257043237,
        "epoch": 0.3480838158257513,
        "step": 2525
    },
    {
        "loss": 2.5177,
        "grad_norm": 2.440415859222412,
        "learning_rate": 0.00019929188921097903,
        "epoch": 0.348221670802316,
        "step": 2526
    },
    {
        "loss": 1.5507,
        "grad_norm": 2.8104665279388428,
        "learning_rate": 0.00019927631707986735,
        "epoch": 0.34835952577888063,
        "step": 2527
    },
    {
        "loss": 1.7377,
        "grad_norm": 2.4809296131134033,
        "learning_rate": 0.00019926057620356608,
        "epoch": 0.3484973807554453,
        "step": 2528
    },
    {
        "loss": 2.2152,
        "grad_norm": 1.5789798498153687,
        "learning_rate": 0.00019924466660883075,
        "epoch": 0.34863523573200994,
        "step": 2529
    },
    {
        "loss": 2.893,
        "grad_norm": 1.6422245502471924,
        "learning_rate": 0.0001992285883227038,
        "epoch": 0.3487730907085746,
        "step": 2530
    },
    {
        "loss": 2.2171,
        "grad_norm": 1.5479705333709717,
        "learning_rate": 0.0001992123413725144,
        "epoch": 0.34891094568513925,
        "step": 2531
    },
    {
        "loss": 2.2743,
        "grad_norm": 1.2819510698318481,
        "learning_rate": 0.00019919592578587824,
        "epoch": 0.3490488006617039,
        "step": 2532
    },
    {
        "loss": 1.93,
        "grad_norm": 2.0221822261810303,
        "learning_rate": 0.0001991793415906978,
        "epoch": 0.34918665563826856,
        "step": 2533
    },
    {
        "loss": 2.3485,
        "grad_norm": 1.5096267461776733,
        "learning_rate": 0.00019916258881516212,
        "epoch": 0.3493245106148332,
        "step": 2534
    },
    {
        "loss": 1.2667,
        "grad_norm": 2.338771104812622,
        "learning_rate": 0.00019914566748774676,
        "epoch": 0.34946236559139787,
        "step": 2535
    },
    {
        "loss": 1.5655,
        "grad_norm": 1.8158847093582153,
        "learning_rate": 0.00019912857763721383,
        "epoch": 0.3496002205679625,
        "step": 2536
    },
    {
        "loss": 1.7755,
        "grad_norm": 1.4773551225662231,
        "learning_rate": 0.0001991113192926118,
        "epoch": 0.3497380755445272,
        "step": 2537
    },
    {
        "loss": 2.4559,
        "grad_norm": 1.6280254125595093,
        "learning_rate": 0.0001990938924832756,
        "epoch": 0.34987593052109184,
        "step": 2538
    },
    {
        "loss": 2.0019,
        "grad_norm": 2.090998411178589,
        "learning_rate": 0.00019907629723882653,
        "epoch": 0.35001378549765644,
        "step": 2539
    },
    {
        "loss": 2.2951,
        "grad_norm": 1.3770657777786255,
        "learning_rate": 0.0001990585335891721,
        "epoch": 0.3501516404742211,
        "step": 2540
    },
    {
        "loss": 0.7681,
        "grad_norm": 2.2994258403778076,
        "learning_rate": 0.00019904060156450615,
        "epoch": 0.35028949545078575,
        "step": 2541
    },
    {
        "loss": 1.9309,
        "grad_norm": 1.9443107843399048,
        "learning_rate": 0.0001990225011953087,
        "epoch": 0.3504273504273504,
        "step": 2542
    },
    {
        "loss": 2.2832,
        "grad_norm": 1.1455329656600952,
        "learning_rate": 0.0001990042325123459,
        "epoch": 0.35056520540391506,
        "step": 2543
    },
    {
        "loss": 1.0436,
        "grad_norm": 2.4172651767730713,
        "learning_rate": 0.00019898579554666996,
        "epoch": 0.3507030603804797,
        "step": 2544
    },
    {
        "loss": 2.0859,
        "grad_norm": 1.682737946510315,
        "learning_rate": 0.0001989671903296192,
        "epoch": 0.35084091535704437,
        "step": 2545
    },
    {
        "loss": 1.8661,
        "grad_norm": 2.1413869857788086,
        "learning_rate": 0.00019894841689281788,
        "epoch": 0.350978770333609,
        "step": 2546
    },
    {
        "loss": 2.5663,
        "grad_norm": 1.7159967422485352,
        "learning_rate": 0.0001989294752681762,
        "epoch": 0.3511166253101737,
        "step": 2547
    },
    {
        "loss": 2.2253,
        "grad_norm": 1.4258651733398438,
        "learning_rate": 0.0001989103654878902,
        "epoch": 0.35125448028673834,
        "step": 2548
    },
    {
        "loss": 2.6481,
        "grad_norm": 1.0572466850280762,
        "learning_rate": 0.00019889108758444187,
        "epoch": 0.351392335263303,
        "step": 2549
    },
    {
        "loss": 2.1661,
        "grad_norm": 1.5364233255386353,
        "learning_rate": 0.00019887164159059878,
        "epoch": 0.35153019023986765,
        "step": 2550
    },
    {
        "loss": 0.9995,
        "grad_norm": 3.8236827850341797,
        "learning_rate": 0.00019885202753941434,
        "epoch": 0.3516680452164323,
        "step": 2551
    },
    {
        "loss": 2.4667,
        "grad_norm": 2.0747427940368652,
        "learning_rate": 0.0001988322454642276,
        "epoch": 0.35180590019299696,
        "step": 2552
    },
    {
        "loss": 2.2613,
        "grad_norm": 1.4225724935531616,
        "learning_rate": 0.00019881229539866325,
        "epoch": 0.3519437551695616,
        "step": 2553
    },
    {
        "loss": 2.5767,
        "grad_norm": 1.1414862871170044,
        "learning_rate": 0.00019879217737663136,
        "epoch": 0.35208161014612627,
        "step": 2554
    },
    {
        "loss": 1.3527,
        "grad_norm": 2.1384270191192627,
        "learning_rate": 0.00019877189143232768,
        "epoch": 0.3522194651226909,
        "step": 2555
    },
    {
        "loss": 2.2618,
        "grad_norm": 1.3048324584960938,
        "learning_rate": 0.00019875143760023325,
        "epoch": 0.3523573200992556,
        "step": 2556
    },
    {
        "loss": 2.4566,
        "grad_norm": 1.7063343524932861,
        "learning_rate": 0.00019873081591511454,
        "epoch": 0.35249517507582023,
        "step": 2557
    },
    {
        "loss": 1.9429,
        "grad_norm": 1.1423611640930176,
        "learning_rate": 0.0001987100264120233,
        "epoch": 0.3526330300523849,
        "step": 2558
    },
    {
        "loss": 2.1345,
        "grad_norm": 1.413073182106018,
        "learning_rate": 0.0001986890691262966,
        "epoch": 0.35277088502894954,
        "step": 2559
    },
    {
        "loss": 1.7428,
        "grad_norm": 1.6510835886001587,
        "learning_rate": 0.0001986679440935566,
        "epoch": 0.3529087400055142,
        "step": 2560
    },
    {
        "loss": 2.0792,
        "grad_norm": 1.8366405963897705,
        "learning_rate": 0.0001986466513497106,
        "epoch": 0.35304659498207885,
        "step": 2561
    },
    {
        "loss": 1.9265,
        "grad_norm": 1.9417579174041748,
        "learning_rate": 0.00019862519093095108,
        "epoch": 0.3531844499586435,
        "step": 2562
    },
    {
        "loss": 1.6967,
        "grad_norm": 2.115597724914551,
        "learning_rate": 0.00019860356287375536,
        "epoch": 0.35332230493520816,
        "step": 2563
    },
    {
        "loss": 2.0581,
        "grad_norm": 1.2365310192108154,
        "learning_rate": 0.00019858176721488586,
        "epoch": 0.3534601599117728,
        "step": 2564
    },
    {
        "loss": 2.315,
        "grad_norm": 1.4740360975265503,
        "learning_rate": 0.00019855980399138975,
        "epoch": 0.3535980148883375,
        "step": 2565
    },
    {
        "loss": 2.3008,
        "grad_norm": 1.2575663328170776,
        "learning_rate": 0.00019853767324059916,
        "epoch": 0.35373586986490213,
        "step": 2566
    },
    {
        "loss": 1.6659,
        "grad_norm": 2.863309621810913,
        "learning_rate": 0.0001985153750001308,
        "epoch": 0.3538737248414668,
        "step": 2567
    },
    {
        "loss": 1.6175,
        "grad_norm": 1.079392671585083,
        "learning_rate": 0.00019849290930788624,
        "epoch": 0.35401157981803144,
        "step": 2568
    },
    {
        "loss": 1.5022,
        "grad_norm": 2.034902572631836,
        "learning_rate": 0.00019847027620205156,
        "epoch": 0.3541494347945961,
        "step": 2569
    },
    {
        "loss": 1.9783,
        "grad_norm": 1.644958734512329,
        "learning_rate": 0.00019844747572109742,
        "epoch": 0.35428728977116075,
        "step": 2570
    },
    {
        "loss": 1.5145,
        "grad_norm": 1.894660234451294,
        "learning_rate": 0.00019842450790377906,
        "epoch": 0.3544251447477254,
        "step": 2571
    },
    {
        "loss": 2.2681,
        "grad_norm": 1.564249873161316,
        "learning_rate": 0.00019840137278913605,
        "epoch": 0.35456299972429006,
        "step": 2572
    },
    {
        "loss": 1.975,
        "grad_norm": 1.5003764629364014,
        "learning_rate": 0.0001983780704164924,
        "epoch": 0.3547008547008547,
        "step": 2573
    },
    {
        "loss": 1.4213,
        "grad_norm": 1.4577827453613281,
        "learning_rate": 0.00019835460082545634,
        "epoch": 0.3548387096774194,
        "step": 2574
    },
    {
        "loss": 2.0945,
        "grad_norm": 1.1963255405426025,
        "learning_rate": 0.0001983309640559204,
        "epoch": 0.35497656465398403,
        "step": 2575
    },
    {
        "loss": 2.0964,
        "grad_norm": 1.7515450716018677,
        "learning_rate": 0.00019830716014806118,
        "epoch": 0.3551144196305487,
        "step": 2576
    },
    {
        "loss": 1.6701,
        "grad_norm": 3.0068488121032715,
        "learning_rate": 0.00019828318914233957,
        "epoch": 0.35525227460711334,
        "step": 2577
    },
    {
        "loss": 2.0663,
        "grad_norm": 1.2206116914749146,
        "learning_rate": 0.0001982590510795002,
        "epoch": 0.355390129583678,
        "step": 2578
    },
    {
        "loss": 2.1063,
        "grad_norm": 2.2350521087646484,
        "learning_rate": 0.00019823474600057193,
        "epoch": 0.35552798456024265,
        "step": 2579
    },
    {
        "loss": 1.7036,
        "grad_norm": 2.4383528232574463,
        "learning_rate": 0.0001982102739468673,
        "epoch": 0.3556658395368073,
        "step": 2580
    },
    {
        "loss": 2.5198,
        "grad_norm": 1.521851897239685,
        "learning_rate": 0.0001981856349599828,
        "epoch": 0.35580369451337196,
        "step": 2581
    },
    {
        "loss": 2.0685,
        "grad_norm": 2.4317219257354736,
        "learning_rate": 0.00019816082908179856,
        "epoch": 0.35594154948993656,
        "step": 2582
    },
    {
        "loss": 1.4415,
        "grad_norm": 2.5604567527770996,
        "learning_rate": 0.00019813585635447849,
        "epoch": 0.3560794044665012,
        "step": 2583
    },
    {
        "loss": 2.3839,
        "grad_norm": 1.417035460472107,
        "learning_rate": 0.00019811071682047007,
        "epoch": 0.35621725944306587,
        "step": 2584
    },
    {
        "loss": 2.4328,
        "grad_norm": 2.4937777519226074,
        "learning_rate": 0.00019808541052250425,
        "epoch": 0.3563551144196305,
        "step": 2585
    },
    {
        "loss": 2.198,
        "grad_norm": 1.4059956073760986,
        "learning_rate": 0.00019805993750359545,
        "epoch": 0.3564929693961952,
        "step": 2586
    },
    {
        "loss": 1.9152,
        "grad_norm": 1.8503657579421997,
        "learning_rate": 0.00019803429780704157,
        "epoch": 0.35663082437275984,
        "step": 2587
    },
    {
        "loss": 1.9633,
        "grad_norm": 2.062683582305908,
        "learning_rate": 0.00019800849147642372,
        "epoch": 0.3567686793493245,
        "step": 2588
    },
    {
        "loss": 1.8089,
        "grad_norm": 1.9926091432571411,
        "learning_rate": 0.00019798251855560628,
        "epoch": 0.35690653432588915,
        "step": 2589
    },
    {
        "loss": 2.4145,
        "grad_norm": 1.7125513553619385,
        "learning_rate": 0.00019795637908873677,
        "epoch": 0.3570443893024538,
        "step": 2590
    },
    {
        "loss": 2.4712,
        "grad_norm": 1.2533215284347534,
        "learning_rate": 0.00019793007312024589,
        "epoch": 0.35718224427901846,
        "step": 2591
    },
    {
        "loss": 1.859,
        "grad_norm": 2.1329081058502197,
        "learning_rate": 0.00019790360069484722,
        "epoch": 0.3573200992555831,
        "step": 2592
    },
    {
        "loss": 1.9371,
        "grad_norm": 1.7317184209823608,
        "learning_rate": 0.00019787696185753734,
        "epoch": 0.35745795423214777,
        "step": 2593
    },
    {
        "loss": 1.7906,
        "grad_norm": 2.2577524185180664,
        "learning_rate": 0.00019785015665359573,
        "epoch": 0.3575958092087124,
        "step": 2594
    },
    {
        "loss": 1.9691,
        "grad_norm": 2.439225673675537,
        "learning_rate": 0.00019782318512858455,
        "epoch": 0.3577336641852771,
        "step": 2595
    },
    {
        "loss": 2.1516,
        "grad_norm": 1.6626206636428833,
        "learning_rate": 0.0001977960473283488,
        "epoch": 0.35787151916184173,
        "step": 2596
    },
    {
        "loss": 1.4734,
        "grad_norm": 2.723034143447876,
        "learning_rate": 0.00019776874329901595,
        "epoch": 0.3580093741384064,
        "step": 2597
    },
    {
        "loss": 2.0597,
        "grad_norm": 2.3511900901794434,
        "learning_rate": 0.0001977412730869961,
        "epoch": 0.35814722911497104,
        "step": 2598
    },
    {
        "loss": 2.212,
        "grad_norm": 1.1416540145874023,
        "learning_rate": 0.00019771363673898188,
        "epoch": 0.3582850840915357,
        "step": 2599
    },
    {
        "loss": 0.7526,
        "grad_norm": 2.050922155380249,
        "learning_rate": 0.00019768583430194824,
        "epoch": 0.35842293906810035,
        "step": 2600
    },
    {
        "loss": 1.8068,
        "grad_norm": 2.327775716781616,
        "learning_rate": 0.00019765786582315236,
        "epoch": 0.358560794044665,
        "step": 2601
    },
    {
        "loss": 1.9386,
        "grad_norm": 1.9900031089782715,
        "learning_rate": 0.00019762973135013386,
        "epoch": 0.35869864902122967,
        "step": 2602
    },
    {
        "loss": 2.1227,
        "grad_norm": 1.8226354122161865,
        "learning_rate": 0.00019760143093071428,
        "epoch": 0.3588365039977943,
        "step": 2603
    },
    {
        "loss": 1.9197,
        "grad_norm": 1.6253249645233154,
        "learning_rate": 0.00019757296461299741,
        "epoch": 0.358974358974359,
        "step": 2604
    },
    {
        "loss": 1.9013,
        "grad_norm": 2.624630928039551,
        "learning_rate": 0.0001975443324453689,
        "epoch": 0.35911221395092363,
        "step": 2605
    },
    {
        "loss": 1.0795,
        "grad_norm": 2.266462564468384,
        "learning_rate": 0.00019751553447649642,
        "epoch": 0.3592500689274883,
        "step": 2606
    },
    {
        "loss": 1.6464,
        "grad_norm": 2.1772494316101074,
        "learning_rate": 0.00019748657075532934,
        "epoch": 0.35938792390405294,
        "step": 2607
    },
    {
        "loss": 1.8741,
        "grad_norm": 1.682227373123169,
        "learning_rate": 0.0001974574413310988,
        "epoch": 0.3595257788806176,
        "step": 2608
    },
    {
        "loss": 2.42,
        "grad_norm": 2.910130262374878,
        "learning_rate": 0.00019742814625331768,
        "epoch": 0.35966363385718225,
        "step": 2609
    },
    {
        "loss": 1.7163,
        "grad_norm": 2.2972981929779053,
        "learning_rate": 0.00019739868557178033,
        "epoch": 0.3598014888337469,
        "step": 2610
    },
    {
        "loss": 2.0428,
        "grad_norm": 1.7291051149368286,
        "learning_rate": 0.00019736905933656265,
        "epoch": 0.35993934381031156,
        "step": 2611
    },
    {
        "loss": 1.9543,
        "grad_norm": 1.5468838214874268,
        "learning_rate": 0.0001973392675980219,
        "epoch": 0.3600771987868762,
        "step": 2612
    },
    {
        "loss": 2.0434,
        "grad_norm": 1.7872214317321777,
        "learning_rate": 0.00019730931040679668,
        "epoch": 0.3602150537634409,
        "step": 2613
    },
    {
        "loss": 2.335,
        "grad_norm": 1.7402483224868774,
        "learning_rate": 0.00019727918781380682,
        "epoch": 0.36035290874000553,
        "step": 2614
    },
    {
        "loss": 1.6871,
        "grad_norm": 1.3222852945327759,
        "learning_rate": 0.0001972488998702532,
        "epoch": 0.3604907637165702,
        "step": 2615
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.4535236358642578,
        "learning_rate": 0.0001972184466276179,
        "epoch": 0.36062861869313484,
        "step": 2616
    },
    {
        "loss": 2.058,
        "grad_norm": 1.683435082435608,
        "learning_rate": 0.0001971878281376639,
        "epoch": 0.3607664736696995,
        "step": 2617
    },
    {
        "loss": 1.9628,
        "grad_norm": 1.9273338317871094,
        "learning_rate": 0.00019715704445243506,
        "epoch": 0.36090432864626415,
        "step": 2618
    },
    {
        "loss": 2.1261,
        "grad_norm": 1.5767667293548584,
        "learning_rate": 0.00019712609562425604,
        "epoch": 0.3610421836228288,
        "step": 2619
    },
    {
        "loss": 1.8856,
        "grad_norm": 1.6385753154754639,
        "learning_rate": 0.00019709498170573215,
        "epoch": 0.36118003859939346,
        "step": 2620
    },
    {
        "loss": 1.5286,
        "grad_norm": 1.8348329067230225,
        "learning_rate": 0.00019706370274974938,
        "epoch": 0.3613178935759581,
        "step": 2621
    },
    {
        "loss": 2.1834,
        "grad_norm": 1.5525065660476685,
        "learning_rate": 0.00019703225880947427,
        "epoch": 0.36145574855252277,
        "step": 2622
    },
    {
        "loss": 2.1915,
        "grad_norm": 2.01296067237854,
        "learning_rate": 0.00019700064993835367,
        "epoch": 0.3615936035290874,
        "step": 2623
    },
    {
        "loss": 2.1726,
        "grad_norm": 1.5829123258590698,
        "learning_rate": 0.00019696887619011485,
        "epoch": 0.3617314585056521,
        "step": 2624
    },
    {
        "loss": 1.9085,
        "grad_norm": 2.180055856704712,
        "learning_rate": 0.00019693693761876544,
        "epoch": 0.3618693134822167,
        "step": 2625
    },
    {
        "loss": 2.4371,
        "grad_norm": 1.7028013467788696,
        "learning_rate": 0.00019690483427859293,
        "epoch": 0.36200716845878134,
        "step": 2626
    },
    {
        "loss": 1.5818,
        "grad_norm": 2.6014175415039062,
        "learning_rate": 0.00019687256622416522,
        "epoch": 0.362145023435346,
        "step": 2627
    },
    {
        "loss": 0.9983,
        "grad_norm": 1.8908183574676514,
        "learning_rate": 0.0001968401335103299,
        "epoch": 0.36228287841191065,
        "step": 2628
    },
    {
        "loss": 2.5164,
        "grad_norm": 2.225701093673706,
        "learning_rate": 0.0001968075361922147,
        "epoch": 0.3624207333884753,
        "step": 2629
    },
    {
        "loss": 1.6255,
        "grad_norm": 1.8321139812469482,
        "learning_rate": 0.00019677477432522683,
        "epoch": 0.36255858836503996,
        "step": 2630
    },
    {
        "loss": 2.4362,
        "grad_norm": 1.5807996988296509,
        "learning_rate": 0.0001967418479650535,
        "epoch": 0.3626964433416046,
        "step": 2631
    },
    {
        "loss": 2.3692,
        "grad_norm": 1.7085250616073608,
        "learning_rate": 0.00019670875716766133,
        "epoch": 0.36283429831816927,
        "step": 2632
    },
    {
        "loss": 2.372,
        "grad_norm": 1.353128433227539,
        "learning_rate": 0.00019667550198929648,
        "epoch": 0.3629721532947339,
        "step": 2633
    },
    {
        "loss": 0.7991,
        "grad_norm": 1.941758394241333,
        "learning_rate": 0.00019664208248648457,
        "epoch": 0.3631100082712986,
        "step": 2634
    },
    {
        "loss": 1.7949,
        "grad_norm": 2.0042824745178223,
        "learning_rate": 0.00019660849871603044,
        "epoch": 0.36324786324786323,
        "step": 2635
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.2371810674667358,
        "learning_rate": 0.00019657475073501824,
        "epoch": 0.3633857182244279,
        "step": 2636
    },
    {
        "loss": 1.9419,
        "grad_norm": 1.7975081205368042,
        "learning_rate": 0.00019654083860081117,
        "epoch": 0.36352357320099254,
        "step": 2637
    },
    {
        "loss": 1.6423,
        "grad_norm": 1.8842742443084717,
        "learning_rate": 0.00019650676237105148,
        "epoch": 0.3636614281775572,
        "step": 2638
    },
    {
        "loss": 1.9786,
        "grad_norm": 1.4949439764022827,
        "learning_rate": 0.0001964725221036603,
        "epoch": 0.36379928315412186,
        "step": 2639
    },
    {
        "loss": 1.8283,
        "grad_norm": 1.8969210386276245,
        "learning_rate": 0.00019643811785683766,
        "epoch": 0.3639371381306865,
        "step": 2640
    },
    {
        "loss": 2.0709,
        "grad_norm": 2.202162504196167,
        "learning_rate": 0.00019640354968906228,
        "epoch": 0.36407499310725117,
        "step": 2641
    },
    {
        "loss": 0.8792,
        "grad_norm": 2.222048759460449,
        "learning_rate": 0.00019636881765909146,
        "epoch": 0.3642128480838158,
        "step": 2642
    },
    {
        "loss": 2.2868,
        "grad_norm": 1.7766426801681519,
        "learning_rate": 0.00019633392182596108,
        "epoch": 0.3643507030603805,
        "step": 2643
    },
    {
        "loss": 2.2686,
        "grad_norm": 1.1622250080108643,
        "learning_rate": 0.00019629886224898545,
        "epoch": 0.36448855803694513,
        "step": 2644
    },
    {
        "loss": 1.1271,
        "grad_norm": 1.7430071830749512,
        "learning_rate": 0.00019626363898775712,
        "epoch": 0.3646264130135098,
        "step": 2645
    },
    {
        "loss": 1.702,
        "grad_norm": 2.369234323501587,
        "learning_rate": 0.00019622825210214697,
        "epoch": 0.36476426799007444,
        "step": 2646
    },
    {
        "loss": 1.8609,
        "grad_norm": 2.0524818897247314,
        "learning_rate": 0.00019619270165230395,
        "epoch": 0.3649021229666391,
        "step": 2647
    },
    {
        "loss": 2.5262,
        "grad_norm": 1.630919337272644,
        "learning_rate": 0.00019615698769865505,
        "epoch": 0.36503997794320375,
        "step": 2648
    },
    {
        "loss": 1.5487,
        "grad_norm": 2.309265613555908,
        "learning_rate": 0.00019612111030190512,
        "epoch": 0.3651778329197684,
        "step": 2649
    },
    {
        "loss": 2.0777,
        "grad_norm": 1.9981038570404053,
        "learning_rate": 0.00019608506952303688,
        "epoch": 0.36531568789633306,
        "step": 2650
    },
    {
        "loss": 2.4327,
        "grad_norm": 1.3516929149627686,
        "learning_rate": 0.00019604886542331072,
        "epoch": 0.3654535428728977,
        "step": 2651
    },
    {
        "loss": 2.3794,
        "grad_norm": 1.6702502965927124,
        "learning_rate": 0.0001960124980642647,
        "epoch": 0.3655913978494624,
        "step": 2652
    },
    {
        "loss": 2.2395,
        "grad_norm": 1.4849320650100708,
        "learning_rate": 0.00019597596750771433,
        "epoch": 0.36572925282602703,
        "step": 2653
    },
    {
        "loss": 2.8152,
        "grad_norm": 1.4413172006607056,
        "learning_rate": 0.00019593927381575252,
        "epoch": 0.3658671078025917,
        "step": 2654
    },
    {
        "loss": 1.5771,
        "grad_norm": 2.327218532562256,
        "learning_rate": 0.00019590241705074943,
        "epoch": 0.36600496277915634,
        "step": 2655
    },
    {
        "loss": 2.0376,
        "grad_norm": 1.6045209169387817,
        "learning_rate": 0.0001958653972753525,
        "epoch": 0.366142817755721,
        "step": 2656
    },
    {
        "loss": 1.7938,
        "grad_norm": 1.7771944999694824,
        "learning_rate": 0.00019582821455248618,
        "epoch": 0.36628067273228565,
        "step": 2657
    },
    {
        "loss": 1.8666,
        "grad_norm": 2.4138941764831543,
        "learning_rate": 0.00019579086894535186,
        "epoch": 0.3664185277088503,
        "step": 2658
    },
    {
        "loss": 1.8272,
        "grad_norm": 2.1435937881469727,
        "learning_rate": 0.0001957533605174279,
        "epoch": 0.36655638268541496,
        "step": 2659
    },
    {
        "loss": 2.2113,
        "grad_norm": 1.513465166091919,
        "learning_rate": 0.00019571568933246934,
        "epoch": 0.3666942376619796,
        "step": 2660
    },
    {
        "loss": 2.1193,
        "grad_norm": 1.8439559936523438,
        "learning_rate": 0.00019567785545450788,
        "epoch": 0.36683209263854427,
        "step": 2661
    },
    {
        "loss": 2.1175,
        "grad_norm": 1.2695260047912598,
        "learning_rate": 0.00019563985894785168,
        "epoch": 0.3669699476151089,
        "step": 2662
    },
    {
        "loss": 2.164,
        "grad_norm": 2.20013427734375,
        "learning_rate": 0.0001956016998770855,
        "epoch": 0.3671078025916736,
        "step": 2663
    },
    {
        "loss": 2.2327,
        "grad_norm": 1.6089372634887695,
        "learning_rate": 0.00019556337830707032,
        "epoch": 0.36724565756823824,
        "step": 2664
    },
    {
        "loss": 1.1795,
        "grad_norm": 2.2846572399139404,
        "learning_rate": 0.00019552489430294328,
        "epoch": 0.3673835125448029,
        "step": 2665
    },
    {
        "loss": 2.4291,
        "grad_norm": 1.4565744400024414,
        "learning_rate": 0.00019548624793011766,
        "epoch": 0.36752136752136755,
        "step": 2666
    },
    {
        "loss": 2.127,
        "grad_norm": 1.7671897411346436,
        "learning_rate": 0.00019544743925428284,
        "epoch": 0.36765922249793215,
        "step": 2667
    },
    {
        "loss": 1.9614,
        "grad_norm": 2.162980079650879,
        "learning_rate": 0.00019540846834140388,
        "epoch": 0.3677970774744968,
        "step": 2668
    },
    {
        "loss": 2.2885,
        "grad_norm": 2.2151384353637695,
        "learning_rate": 0.00019536933525772168,
        "epoch": 0.36793493245106146,
        "step": 2669
    },
    {
        "loss": 2.179,
        "grad_norm": 1.6782004833221436,
        "learning_rate": 0.0001953300400697529,
        "epoch": 0.3680727874276261,
        "step": 2670
    },
    {
        "loss": 2.2932,
        "grad_norm": 1.477877140045166,
        "learning_rate": 0.00019529058284428955,
        "epoch": 0.36821064240419077,
        "step": 2671
    },
    {
        "loss": 2.5487,
        "grad_norm": 1.0303730964660645,
        "learning_rate": 0.0001952509636483992,
        "epoch": 0.3683484973807554,
        "step": 2672
    },
    {
        "loss": 2.3903,
        "grad_norm": 0.9366082549095154,
        "learning_rate": 0.00019521118254942467,
        "epoch": 0.3684863523573201,
        "step": 2673
    },
    {
        "loss": 1.9794,
        "grad_norm": 1.7157045602798462,
        "learning_rate": 0.00019517123961498403,
        "epoch": 0.36862420733388473,
        "step": 2674
    },
    {
        "loss": 1.8543,
        "grad_norm": 1.5469939708709717,
        "learning_rate": 0.00019513113491297036,
        "epoch": 0.3687620623104494,
        "step": 2675
    },
    {
        "loss": 1.8491,
        "grad_norm": 1.3921520709991455,
        "learning_rate": 0.00019509086851155174,
        "epoch": 0.36889991728701405,
        "step": 2676
    },
    {
        "loss": 0.566,
        "grad_norm": 2.3261852264404297,
        "learning_rate": 0.00019505044047917104,
        "epoch": 0.3690377722635787,
        "step": 2677
    },
    {
        "loss": 1.9683,
        "grad_norm": 1.3395335674285889,
        "learning_rate": 0.000195009850884546,
        "epoch": 0.36917562724014336,
        "step": 2678
    },
    {
        "loss": 2.367,
        "grad_norm": 1.363012671470642,
        "learning_rate": 0.0001949690997966689,
        "epoch": 0.369313482216708,
        "step": 2679
    },
    {
        "loss": 1.9911,
        "grad_norm": 1.3098849058151245,
        "learning_rate": 0.00019492818728480645,
        "epoch": 0.36945133719327267,
        "step": 2680
    },
    {
        "loss": 1.763,
        "grad_norm": 2.0646591186523438,
        "learning_rate": 0.00019488711341849983,
        "epoch": 0.3695891921698373,
        "step": 2681
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.103556752204895,
        "learning_rate": 0.00019484587826756446,
        "epoch": 0.369727047146402,
        "step": 2682
    },
    {
        "loss": 1.1238,
        "grad_norm": 2.080585241317749,
        "learning_rate": 0.0001948044819020899,
        "epoch": 0.36986490212296663,
        "step": 2683
    },
    {
        "loss": 2.1542,
        "grad_norm": 2.1541688442230225,
        "learning_rate": 0.00019476292439243973,
        "epoch": 0.3700027570995313,
        "step": 2684
    },
    {
        "loss": 1.4841,
        "grad_norm": 1.7285943031311035,
        "learning_rate": 0.0001947212058092514,
        "epoch": 0.37014061207609594,
        "step": 2685
    },
    {
        "loss": 2.1399,
        "grad_norm": 2.2934811115264893,
        "learning_rate": 0.00019467932622343627,
        "epoch": 0.3702784670526606,
        "step": 2686
    },
    {
        "loss": 2.0079,
        "grad_norm": 1.1401705741882324,
        "learning_rate": 0.00019463728570617923,
        "epoch": 0.37041632202922525,
        "step": 2687
    },
    {
        "loss": 2.1378,
        "grad_norm": 1.6450395584106445,
        "learning_rate": 0.00019459508432893876,
        "epoch": 0.3705541770057899,
        "step": 2688
    },
    {
        "loss": 2.3058,
        "grad_norm": 1.242638349533081,
        "learning_rate": 0.0001945527221634468,
        "epoch": 0.37069203198235456,
        "step": 2689
    },
    {
        "loss": 1.7562,
        "grad_norm": 2.3859663009643555,
        "learning_rate": 0.0001945101992817085,
        "epoch": 0.3708298869589192,
        "step": 2690
    },
    {
        "loss": 1.8769,
        "grad_norm": 1.669965147972107,
        "learning_rate": 0.00019446751575600226,
        "epoch": 0.3709677419354839,
        "step": 2691
    },
    {
        "loss": 2.0992,
        "grad_norm": 2.0556745529174805,
        "learning_rate": 0.00019442467165887956,
        "epoch": 0.37110559691204853,
        "step": 2692
    },
    {
        "loss": 1.6021,
        "grad_norm": 1.5760564804077148,
        "learning_rate": 0.00019438166706316472,
        "epoch": 0.3712434518886132,
        "step": 2693
    },
    {
        "loss": 1.8548,
        "grad_norm": 1.4270875453948975,
        "learning_rate": 0.00019433850204195497,
        "epoch": 0.37138130686517784,
        "step": 2694
    },
    {
        "loss": 1.516,
        "grad_norm": 2.158317804336548,
        "learning_rate": 0.00019429517666862014,
        "epoch": 0.3715191618417425,
        "step": 2695
    },
    {
        "loss": 1.4036,
        "grad_norm": 2.0962703227996826,
        "learning_rate": 0.00019425169101680264,
        "epoch": 0.37165701681830715,
        "step": 2696
    },
    {
        "loss": 2.2234,
        "grad_norm": 1.3534975051879883,
        "learning_rate": 0.0001942080451604173,
        "epoch": 0.3717948717948718,
        "step": 2697
    },
    {
        "loss": 1.8326,
        "grad_norm": 1.4488294124603271,
        "learning_rate": 0.00019416423917365138,
        "epoch": 0.37193272677143646,
        "step": 2698
    },
    {
        "loss": 2.2523,
        "grad_norm": 2.634744167327881,
        "learning_rate": 0.00019412027313096418,
        "epoch": 0.3720705817480011,
        "step": 2699
    },
    {
        "loss": 2.2278,
        "grad_norm": 1.4685723781585693,
        "learning_rate": 0.000194076147107087,
        "epoch": 0.37220843672456577,
        "step": 2700
    },
    {
        "loss": 1.6974,
        "grad_norm": 2.238050699234009,
        "learning_rate": 0.00019403186117702336,
        "epoch": 0.3723462917011304,
        "step": 2701
    },
    {
        "loss": 1.4479,
        "grad_norm": 1.215491533279419,
        "learning_rate": 0.00019398741541604822,
        "epoch": 0.3724841466776951,
        "step": 2702
    },
    {
        "loss": 2.5257,
        "grad_norm": 1.2644935846328735,
        "learning_rate": 0.0001939428098997085,
        "epoch": 0.37262200165425974,
        "step": 2703
    },
    {
        "loss": 1.963,
        "grad_norm": 1.8518869876861572,
        "learning_rate": 0.00019389804470382242,
        "epoch": 0.3727598566308244,
        "step": 2704
    },
    {
        "loss": 1.7812,
        "grad_norm": 2.328378915786743,
        "learning_rate": 0.00019385311990447992,
        "epoch": 0.37289771160738905,
        "step": 2705
    },
    {
        "loss": 1.035,
        "grad_norm": 1.31808340549469,
        "learning_rate": 0.00019380803557804188,
        "epoch": 0.3730355665839537,
        "step": 2706
    },
    {
        "loss": 1.9421,
        "grad_norm": 1.9884669780731201,
        "learning_rate": 0.0001937627918011406,
        "epoch": 0.37317342156051836,
        "step": 2707
    },
    {
        "loss": 1.193,
        "grad_norm": 2.4700779914855957,
        "learning_rate": 0.00019371738865067928,
        "epoch": 0.373311276537083,
        "step": 2708
    },
    {
        "loss": 2.2269,
        "grad_norm": 1.9455920457839966,
        "learning_rate": 0.00019367182620383204,
        "epoch": 0.37344913151364767,
        "step": 2709
    },
    {
        "loss": 2.2012,
        "grad_norm": 1.4220048189163208,
        "learning_rate": 0.00019362610453804382,
        "epoch": 0.37358698649021227,
        "step": 2710
    },
    {
        "loss": 1.6073,
        "grad_norm": 1.6279546022415161,
        "learning_rate": 0.00019358022373103012,
        "epoch": 0.3737248414667769,
        "step": 2711
    },
    {
        "loss": 1.8341,
        "grad_norm": 1.5823392868041992,
        "learning_rate": 0.00019353418386077693,
        "epoch": 0.3738626964433416,
        "step": 2712
    },
    {
        "loss": 1.9624,
        "grad_norm": 1.437638759613037,
        "learning_rate": 0.00019348798500554067,
        "epoch": 0.37400055141990624,
        "step": 2713
    },
    {
        "loss": 1.8539,
        "grad_norm": 1.3602395057678223,
        "learning_rate": 0.00019344162724384804,
        "epoch": 0.3741384063964709,
        "step": 2714
    },
    {
        "loss": 1.7094,
        "grad_norm": 1.8988778591156006,
        "learning_rate": 0.00019339511065449564,
        "epoch": 0.37427626137303555,
        "step": 2715
    },
    {
        "loss": 1.9959,
        "grad_norm": 1.6600192785263062,
        "learning_rate": 0.00019334843531655028,
        "epoch": 0.3744141163496002,
        "step": 2716
    },
    {
        "loss": 2.1849,
        "grad_norm": 1.766549825668335,
        "learning_rate": 0.0001933016013093484,
        "epoch": 0.37455197132616486,
        "step": 2717
    },
    {
        "loss": 2.1001,
        "grad_norm": 3.121654510498047,
        "learning_rate": 0.00019325460871249633,
        "epoch": 0.3746898263027295,
        "step": 2718
    },
    {
        "loss": 2.1375,
        "grad_norm": 1.8057153224945068,
        "learning_rate": 0.0001932074576058698,
        "epoch": 0.37482768127929417,
        "step": 2719
    },
    {
        "loss": 1.7796,
        "grad_norm": 2.2954397201538086,
        "learning_rate": 0.00019316014806961405,
        "epoch": 0.3749655362558588,
        "step": 2720
    },
    {
        "loss": 2.6783,
        "grad_norm": 1.1518794298171997,
        "learning_rate": 0.00019311268018414358,
        "epoch": 0.3751033912324235,
        "step": 2721
    },
    {
        "loss": 2.1051,
        "grad_norm": 1.9593961238861084,
        "learning_rate": 0.00019306505403014206,
        "epoch": 0.37524124620898813,
        "step": 2722
    },
    {
        "loss": 1.7914,
        "grad_norm": 1.8951036930084229,
        "learning_rate": 0.00019301726968856214,
        "epoch": 0.3753791011855528,
        "step": 2723
    },
    {
        "loss": 2.3194,
        "grad_norm": 1.1626741886138916,
        "learning_rate": 0.0001929693272406255,
        "epoch": 0.37551695616211744,
        "step": 2724
    },
    {
        "loss": 2.5665,
        "grad_norm": 1.3313348293304443,
        "learning_rate": 0.00019292122676782233,
        "epoch": 0.3756548111386821,
        "step": 2725
    },
    {
        "loss": 1.6877,
        "grad_norm": 2.3311333656311035,
        "learning_rate": 0.0001928729683519116,
        "epoch": 0.37579266611524675,
        "step": 2726
    },
    {
        "loss": 2.6629,
        "grad_norm": 1.2678292989730835,
        "learning_rate": 0.00019282455207492066,
        "epoch": 0.3759305210918114,
        "step": 2727
    },
    {
        "loss": 1.6563,
        "grad_norm": 2.1230669021606445,
        "learning_rate": 0.00019277597801914517,
        "epoch": 0.37606837606837606,
        "step": 2728
    },
    {
        "loss": 2.6508,
        "grad_norm": 1.6583176851272583,
        "learning_rate": 0.00019272724626714907,
        "epoch": 0.3762062310449407,
        "step": 2729
    },
    {
        "loss": 2.1391,
        "grad_norm": 1.6558681726455688,
        "learning_rate": 0.00019267835690176433,
        "epoch": 0.3763440860215054,
        "step": 2730
    },
    {
        "loss": 1.2401,
        "grad_norm": 2.4011168479919434,
        "learning_rate": 0.00019262931000609066,
        "epoch": 0.37648194099807003,
        "step": 2731
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.7265154123306274,
        "learning_rate": 0.00019258010566349575,
        "epoch": 0.3766197959746347,
        "step": 2732
    },
    {
        "loss": 1.8498,
        "grad_norm": 1.9388686418533325,
        "learning_rate": 0.0001925307439576148,
        "epoch": 0.37675765095119934,
        "step": 2733
    },
    {
        "loss": 2.0929,
        "grad_norm": 1.6717406511306763,
        "learning_rate": 0.00019248122497235044,
        "epoch": 0.376895505927764,
        "step": 2734
    },
    {
        "loss": 1.9012,
        "grad_norm": 1.987059235572815,
        "learning_rate": 0.00019243154879187277,
        "epoch": 0.37703336090432865,
        "step": 2735
    },
    {
        "loss": 2.4614,
        "grad_norm": 1.3870830535888672,
        "learning_rate": 0.000192381715500619,
        "epoch": 0.3771712158808933,
        "step": 2736
    },
    {
        "loss": 1.9407,
        "grad_norm": 1.4312423467636108,
        "learning_rate": 0.0001923317251832934,
        "epoch": 0.37730907085745796,
        "step": 2737
    },
    {
        "loss": 1.7915,
        "grad_norm": 1.9205836057662964,
        "learning_rate": 0.0001922815779248671,
        "epoch": 0.3774469258340226,
        "step": 2738
    },
    {
        "loss": 1.9382,
        "grad_norm": 1.597787618637085,
        "learning_rate": 0.00019223127381057812,
        "epoch": 0.3775847808105873,
        "step": 2739
    },
    {
        "loss": 2.0922,
        "grad_norm": 1.8654106855392456,
        "learning_rate": 0.00019218081292593096,
        "epoch": 0.37772263578715193,
        "step": 2740
    },
    {
        "loss": 2.4555,
        "grad_norm": 2.091176986694336,
        "learning_rate": 0.00019213019535669663,
        "epoch": 0.3778604907637166,
        "step": 2741
    },
    {
        "loss": 2.1669,
        "grad_norm": 1.9469808340072632,
        "learning_rate": 0.0001920794211889125,
        "epoch": 0.37799834574028124,
        "step": 2742
    },
    {
        "loss": 2.3311,
        "grad_norm": 1.4727733135223389,
        "learning_rate": 0.00019202849050888217,
        "epoch": 0.3781362007168459,
        "step": 2743
    },
    {
        "loss": 2.1372,
        "grad_norm": 1.7552578449249268,
        "learning_rate": 0.00019197740340317512,
        "epoch": 0.37827405569341055,
        "step": 2744
    },
    {
        "loss": 1.9479,
        "grad_norm": 1.5273181200027466,
        "learning_rate": 0.0001919261599586268,
        "epoch": 0.3784119106699752,
        "step": 2745
    },
    {
        "loss": 2.406,
        "grad_norm": 1.1401190757751465,
        "learning_rate": 0.00019187476026233838,
        "epoch": 0.37854976564653986,
        "step": 2746
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.288343667984009,
        "learning_rate": 0.00019182320440167675,
        "epoch": 0.3786876206231045,
        "step": 2747
    },
    {
        "loss": 2.1897,
        "grad_norm": 2.414647102355957,
        "learning_rate": 0.00019177149246427398,
        "epoch": 0.37882547559966917,
        "step": 2748
    },
    {
        "loss": 1.8073,
        "grad_norm": 1.374700665473938,
        "learning_rate": 0.00019171962453802763,
        "epoch": 0.3789633305762338,
        "step": 2749
    },
    {
        "loss": 1.1883,
        "grad_norm": 2.3486580848693848,
        "learning_rate": 0.00019166760071110038,
        "epoch": 0.3791011855527985,
        "step": 2750
    },
    {
        "loss": 1.9817,
        "grad_norm": 1.6184622049331665,
        "learning_rate": 0.00019161542107191983,
        "epoch": 0.37923904052936314,
        "step": 2751
    },
    {
        "loss": 1.9979,
        "grad_norm": 1.5380733013153076,
        "learning_rate": 0.0001915630857091785,
        "epoch": 0.3793768955059278,
        "step": 2752
    },
    {
        "loss": 2.3425,
        "grad_norm": 0.9521152973175049,
        "learning_rate": 0.0001915105947118335,
        "epoch": 0.3795147504824924,
        "step": 2753
    },
    {
        "loss": 2.2089,
        "grad_norm": 1.1746474504470825,
        "learning_rate": 0.00019145794816910662,
        "epoch": 0.37965260545905705,
        "step": 2754
    },
    {
        "loss": 2.0379,
        "grad_norm": 2.284416913986206,
        "learning_rate": 0.00019140514617048393,
        "epoch": 0.3797904604356217,
        "step": 2755
    },
    {
        "loss": 1.6569,
        "grad_norm": 2.5720930099487305,
        "learning_rate": 0.0001913521888057158,
        "epoch": 0.37992831541218636,
        "step": 2756
    },
    {
        "loss": 1.7383,
        "grad_norm": 2.556631326675415,
        "learning_rate": 0.00019129907616481656,
        "epoch": 0.380066170388751,
        "step": 2757
    },
    {
        "loss": 1.7615,
        "grad_norm": 2.601675033569336,
        "learning_rate": 0.00019124580833806469,
        "epoch": 0.38020402536531567,
        "step": 2758
    },
    {
        "loss": 2.1093,
        "grad_norm": 2.0107216835021973,
        "learning_rate": 0.00019119238541600225,
        "epoch": 0.3803418803418803,
        "step": 2759
    },
    {
        "loss": 1.8372,
        "grad_norm": 1.8859978914260864,
        "learning_rate": 0.00019113880748943503,
        "epoch": 0.380479735318445,
        "step": 2760
    },
    {
        "loss": 2.072,
        "grad_norm": 2.0808701515197754,
        "learning_rate": 0.0001910850746494322,
        "epoch": 0.38061759029500963,
        "step": 2761
    },
    {
        "loss": 2.4869,
        "grad_norm": 1.214476227760315,
        "learning_rate": 0.00019103118698732637,
        "epoch": 0.3807554452715743,
        "step": 2762
    },
    {
        "loss": 1.8432,
        "grad_norm": 1.525056004524231,
        "learning_rate": 0.00019097714459471318,
        "epoch": 0.38089330024813894,
        "step": 2763
    },
    {
        "loss": 2.1156,
        "grad_norm": 1.515763759613037,
        "learning_rate": 0.0001909229475634514,
        "epoch": 0.3810311552247036,
        "step": 2764
    },
    {
        "loss": 1.9407,
        "grad_norm": 1.6782481670379639,
        "learning_rate": 0.00019086859598566253,
        "epoch": 0.38116901020126825,
        "step": 2765
    },
    {
        "loss": 2.1008,
        "grad_norm": 1.4239815473556519,
        "learning_rate": 0.00019081408995373083,
        "epoch": 0.3813068651778329,
        "step": 2766
    },
    {
        "loss": 1.7344,
        "grad_norm": 1.494059443473816,
        "learning_rate": 0.00019075942956030305,
        "epoch": 0.38144472015439757,
        "step": 2767
    },
    {
        "loss": 1.4541,
        "grad_norm": 2.358330488204956,
        "learning_rate": 0.00019070461489828838,
        "epoch": 0.3815825751309622,
        "step": 2768
    },
    {
        "loss": 1.9257,
        "grad_norm": 2.907381296157837,
        "learning_rate": 0.0001906496460608582,
        "epoch": 0.3817204301075269,
        "step": 2769
    },
    {
        "loss": 2.0432,
        "grad_norm": 1.6799814701080322,
        "learning_rate": 0.0001905945231414459,
        "epoch": 0.38185828508409153,
        "step": 2770
    },
    {
        "loss": 2.4448,
        "grad_norm": 1.3773037195205688,
        "learning_rate": 0.00019053924623374684,
        "epoch": 0.3819961400606562,
        "step": 2771
    },
    {
        "loss": 2.1007,
        "grad_norm": 2.053689956665039,
        "learning_rate": 0.00019048381543171802,
        "epoch": 0.38213399503722084,
        "step": 2772
    },
    {
        "loss": 2.7006,
        "grad_norm": 1.7066237926483154,
        "learning_rate": 0.00019042823082957822,
        "epoch": 0.3822718500137855,
        "step": 2773
    },
    {
        "loss": 1.2758,
        "grad_norm": 2.4464597702026367,
        "learning_rate": 0.00019037249252180745,
        "epoch": 0.38240970499035015,
        "step": 2774
    },
    {
        "loss": 1.9401,
        "grad_norm": 1.6076009273529053,
        "learning_rate": 0.00019031660060314708,
        "epoch": 0.3825475599669148,
        "step": 2775
    },
    {
        "loss": 1.8698,
        "grad_norm": 1.6464929580688477,
        "learning_rate": 0.00019026055516859947,
        "epoch": 0.38268541494347946,
        "step": 2776
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.2391958236694336,
        "learning_rate": 0.0001902043563134281,
        "epoch": 0.3828232699200441,
        "step": 2777
    },
    {
        "loss": 2.1096,
        "grad_norm": 1.5071730613708496,
        "learning_rate": 0.00019014800413315707,
        "epoch": 0.3829611248966088,
        "step": 2778
    },
    {
        "loss": 2.1709,
        "grad_norm": 1.284224033355713,
        "learning_rate": 0.00019009149872357113,
        "epoch": 0.38309897987317343,
        "step": 2779
    },
    {
        "loss": 2.3484,
        "grad_norm": 1.4910802841186523,
        "learning_rate": 0.00019003484018071559,
        "epoch": 0.3832368348497381,
        "step": 2780
    },
    {
        "loss": 2.1229,
        "grad_norm": 2.1731374263763428,
        "learning_rate": 0.00018997802860089592,
        "epoch": 0.38337468982630274,
        "step": 2781
    },
    {
        "loss": 2.4158,
        "grad_norm": 1.8011044263839722,
        "learning_rate": 0.0001899210640806777,
        "epoch": 0.3835125448028674,
        "step": 2782
    },
    {
        "loss": 2.229,
        "grad_norm": 1.8369200229644775,
        "learning_rate": 0.0001898639467168866,
        "epoch": 0.38365039977943205,
        "step": 2783
    },
    {
        "loss": 2.1641,
        "grad_norm": 1.6810901165008545,
        "learning_rate": 0.000189806676606608,
        "epoch": 0.3837882547559967,
        "step": 2784
    },
    {
        "loss": 1.5053,
        "grad_norm": 2.0177719593048096,
        "learning_rate": 0.00018974925384718688,
        "epoch": 0.38392610973256136,
        "step": 2785
    },
    {
        "loss": 1.4471,
        "grad_norm": 1.6889545917510986,
        "learning_rate": 0.0001896916785362278,
        "epoch": 0.384063964709126,
        "step": 2786
    },
    {
        "loss": 1.9057,
        "grad_norm": 1.721261978149414,
        "learning_rate": 0.00018963395077159452,
        "epoch": 0.38420181968569067,
        "step": 2787
    },
    {
        "loss": 1.6502,
        "grad_norm": 2.359731435775757,
        "learning_rate": 0.00018957607065140994,
        "epoch": 0.3843396746622553,
        "step": 2788
    },
    {
        "loss": 2.1909,
        "grad_norm": 1.7300102710723877,
        "learning_rate": 0.000189518038274056,
        "epoch": 0.38447752963882,
        "step": 2789
    },
    {
        "loss": 1.9594,
        "grad_norm": 2.007979154586792,
        "learning_rate": 0.00018945985373817337,
        "epoch": 0.38461538461538464,
        "step": 2790
    },
    {
        "loss": 1.907,
        "grad_norm": 2.212226152420044,
        "learning_rate": 0.0001894015171426613,
        "epoch": 0.3847532395919493,
        "step": 2791
    },
    {
        "loss": 1.8672,
        "grad_norm": 1.8429479598999023,
        "learning_rate": 0.0001893430285866777,
        "epoch": 0.38489109456851395,
        "step": 2792
    },
    {
        "loss": 1.7332,
        "grad_norm": 1.6893277168273926,
        "learning_rate": 0.00018928438816963862,
        "epoch": 0.3850289495450786,
        "step": 2793
    },
    {
        "loss": 2.5303,
        "grad_norm": 1.226418137550354,
        "learning_rate": 0.00018922559599121824,
        "epoch": 0.38516680452164326,
        "step": 2794
    },
    {
        "loss": 2.6195,
        "grad_norm": 3.476388454437256,
        "learning_rate": 0.00018916665215134865,
        "epoch": 0.38530465949820786,
        "step": 2795
    },
    {
        "loss": 2.3689,
        "grad_norm": 1.3232849836349487,
        "learning_rate": 0.00018910755675021997,
        "epoch": 0.3854425144747725,
        "step": 2796
    },
    {
        "loss": 2.0419,
        "grad_norm": 1.5636703968048096,
        "learning_rate": 0.00018904830988827965,
        "epoch": 0.38558036945133717,
        "step": 2797
    },
    {
        "loss": 1.5751,
        "grad_norm": 2.345064163208008,
        "learning_rate": 0.00018898891166623273,
        "epoch": 0.3857182244279018,
        "step": 2798
    },
    {
        "loss": 1.9914,
        "grad_norm": 2.1355223655700684,
        "learning_rate": 0.00018892936218504149,
        "epoch": 0.3858560794044665,
        "step": 2799
    },
    {
        "loss": 2.209,
        "grad_norm": 2.4081645011901855,
        "learning_rate": 0.0001888696615459254,
        "epoch": 0.38599393438103113,
        "step": 2800
    },
    {
        "loss": 1.9814,
        "grad_norm": 1.9151040315628052,
        "learning_rate": 0.00018880980985036064,
        "epoch": 0.3861317893575958,
        "step": 2801
    },
    {
        "loss": 1.7197,
        "grad_norm": 2.398293972015381,
        "learning_rate": 0.00018874980720008045,
        "epoch": 0.38626964433416044,
        "step": 2802
    },
    {
        "loss": 1.9365,
        "grad_norm": 1.8499683141708374,
        "learning_rate": 0.00018868965369707436,
        "epoch": 0.3864074993107251,
        "step": 2803
    },
    {
        "loss": 2.1402,
        "grad_norm": 0.9736279249191284,
        "learning_rate": 0.0001886293494435885,
        "epoch": 0.38654535428728976,
        "step": 2804
    },
    {
        "loss": 2.557,
        "grad_norm": 1.135705828666687,
        "learning_rate": 0.00018856889454212526,
        "epoch": 0.3866832092638544,
        "step": 2805
    },
    {
        "loss": 1.7897,
        "grad_norm": 1.202318787574768,
        "learning_rate": 0.00018850828909544292,
        "epoch": 0.38682106424041907,
        "step": 2806
    },
    {
        "loss": 2.3392,
        "grad_norm": 1.4346959590911865,
        "learning_rate": 0.0001884475332065558,
        "epoch": 0.3869589192169837,
        "step": 2807
    },
    {
        "loss": 1.8426,
        "grad_norm": 1.425382375717163,
        "learning_rate": 0.00018838662697873387,
        "epoch": 0.3870967741935484,
        "step": 2808
    },
    {
        "loss": 2.3822,
        "grad_norm": 1.7455934286117554,
        "learning_rate": 0.00018832557051550272,
        "epoch": 0.38723462917011303,
        "step": 2809
    },
    {
        "loss": 1.6036,
        "grad_norm": 2.1906590461730957,
        "learning_rate": 0.00018826436392064313,
        "epoch": 0.3873724841466777,
        "step": 2810
    },
    {
        "loss": 1.6541,
        "grad_norm": 1.7105836868286133,
        "learning_rate": 0.00018820300729819125,
        "epoch": 0.38751033912324234,
        "step": 2811
    },
    {
        "loss": 1.9609,
        "grad_norm": 2.1188416481018066,
        "learning_rate": 0.00018814150075243812,
        "epoch": 0.387648194099807,
        "step": 2812
    },
    {
        "loss": 1.9587,
        "grad_norm": 2.0496582984924316,
        "learning_rate": 0.0001880798443879298,
        "epoch": 0.38778604907637165,
        "step": 2813
    },
    {
        "loss": 2.4393,
        "grad_norm": 1.3378045558929443,
        "learning_rate": 0.0001880180383094666,
        "epoch": 0.3879239040529363,
        "step": 2814
    },
    {
        "loss": 1.6577,
        "grad_norm": 1.6210598945617676,
        "learning_rate": 0.0001879560826221038,
        "epoch": 0.38806175902950096,
        "step": 2815
    },
    {
        "loss": 2.1346,
        "grad_norm": 1.5432072877883911,
        "learning_rate": 0.00018789397743115068,
        "epoch": 0.3881996140060656,
        "step": 2816
    },
    {
        "loss": 2.1094,
        "grad_norm": 1.7402616739273071,
        "learning_rate": 0.0001878317228421706,
        "epoch": 0.3883374689826303,
        "step": 2817
    },
    {
        "loss": 1.7674,
        "grad_norm": 2.2722432613372803,
        "learning_rate": 0.00018776931896098115,
        "epoch": 0.38847532395919493,
        "step": 2818
    },
    {
        "loss": 1.9479,
        "grad_norm": 2.226097345352173,
        "learning_rate": 0.00018770676589365342,
        "epoch": 0.3886131789357596,
        "step": 2819
    },
    {
        "loss": 1.6499,
        "grad_norm": 1.929048776626587,
        "learning_rate": 0.0001876440637465121,
        "epoch": 0.38875103391232424,
        "step": 2820
    },
    {
        "loss": 2.3474,
        "grad_norm": 2.0832462310791016,
        "learning_rate": 0.0001875812126261354,
        "epoch": 0.3888888888888889,
        "step": 2821
    },
    {
        "loss": 2.2483,
        "grad_norm": 1.097272515296936,
        "learning_rate": 0.00018751821263935465,
        "epoch": 0.38902674386545355,
        "step": 2822
    },
    {
        "loss": 1.9605,
        "grad_norm": 2.4336366653442383,
        "learning_rate": 0.00018745506389325424,
        "epoch": 0.3891645988420182,
        "step": 2823
    },
    {
        "loss": 1.8731,
        "grad_norm": 2.0430946350097656,
        "learning_rate": 0.00018739176649517147,
        "epoch": 0.38930245381858286,
        "step": 2824
    },
    {
        "loss": 1.8202,
        "grad_norm": 1.9649666547775269,
        "learning_rate": 0.00018732832055269624,
        "epoch": 0.3894403087951475,
        "step": 2825
    },
    {
        "loss": 2.2891,
        "grad_norm": 2.4049105644226074,
        "learning_rate": 0.00018726472617367095,
        "epoch": 0.38957816377171217,
        "step": 2826
    },
    {
        "loss": 1.8209,
        "grad_norm": 2.431036949157715,
        "learning_rate": 0.00018720098346619032,
        "epoch": 0.3897160187482768,
        "step": 2827
    },
    {
        "loss": 1.6742,
        "grad_norm": 1.8951992988586426,
        "learning_rate": 0.00018713709253860122,
        "epoch": 0.3898538737248415,
        "step": 2828
    },
    {
        "loss": 1.9802,
        "grad_norm": 2.66567063331604,
        "learning_rate": 0.00018707305349950236,
        "epoch": 0.38999172870140614,
        "step": 2829
    },
    {
        "loss": 2.3315,
        "grad_norm": 1.3394814729690552,
        "learning_rate": 0.00018700886645774435,
        "epoch": 0.3901295836779708,
        "step": 2830
    },
    {
        "loss": 1.6703,
        "grad_norm": 2.397814989089966,
        "learning_rate": 0.00018694453152242924,
        "epoch": 0.39026743865453545,
        "step": 2831
    },
    {
        "loss": 2.5151,
        "grad_norm": 1.3592690229415894,
        "learning_rate": 0.00018688004880291058,
        "epoch": 0.3904052936311001,
        "step": 2832
    },
    {
        "loss": 1.9814,
        "grad_norm": 1.6155891418457031,
        "learning_rate": 0.00018681541840879292,
        "epoch": 0.39054314860766476,
        "step": 2833
    },
    {
        "loss": 2.0204,
        "grad_norm": 1.919314980506897,
        "learning_rate": 0.0001867506404499321,
        "epoch": 0.3906810035842294,
        "step": 2834
    },
    {
        "loss": 1.8455,
        "grad_norm": 1.808364987373352,
        "learning_rate": 0.00018668571503643451,
        "epoch": 0.39081885856079407,
        "step": 2835
    },
    {
        "loss": 2.2487,
        "grad_norm": 1.3377724885940552,
        "learning_rate": 0.0001866206422786573,
        "epoch": 0.3909567135373587,
        "step": 2836
    },
    {
        "loss": 2.2601,
        "grad_norm": 1.2292975187301636,
        "learning_rate": 0.00018655542228720814,
        "epoch": 0.3910945685139234,
        "step": 2837
    },
    {
        "loss": 1.0401,
        "grad_norm": 2.3475277423858643,
        "learning_rate": 0.0001864900551729449,
        "epoch": 0.391232423490488,
        "step": 2838
    },
    {
        "loss": 1.9909,
        "grad_norm": 1.8478988409042358,
        "learning_rate": 0.00018642454104697543,
        "epoch": 0.39137027846705263,
        "step": 2839
    },
    {
        "loss": 2.0171,
        "grad_norm": 2.091869831085205,
        "learning_rate": 0.00018635888002065758,
        "epoch": 0.3915081334436173,
        "step": 2840
    },
    {
        "loss": 1.8656,
        "grad_norm": 2.3056485652923584,
        "learning_rate": 0.00018629307220559884,
        "epoch": 0.39164598842018195,
        "step": 2841
    },
    {
        "loss": 2.4234,
        "grad_norm": 1.727134108543396,
        "learning_rate": 0.00018622711771365625,
        "epoch": 0.3917838433967466,
        "step": 2842
    },
    {
        "loss": 1.8649,
        "grad_norm": 1.0493450164794922,
        "learning_rate": 0.00018616101665693615,
        "epoch": 0.39192169837331126,
        "step": 2843
    },
    {
        "loss": 2.001,
        "grad_norm": 1.5787701606750488,
        "learning_rate": 0.00018609476914779394,
        "epoch": 0.3920595533498759,
        "step": 2844
    },
    {
        "loss": 1.8459,
        "grad_norm": 1.6224058866500854,
        "learning_rate": 0.00018602837529883406,
        "epoch": 0.39219740832644057,
        "step": 2845
    },
    {
        "loss": 2.1632,
        "grad_norm": 1.24868643283844,
        "learning_rate": 0.00018596183522290956,
        "epoch": 0.3923352633030052,
        "step": 2846
    },
    {
        "loss": 2.5267,
        "grad_norm": 1.5521759986877441,
        "learning_rate": 0.0001858951490331222,
        "epoch": 0.3924731182795699,
        "step": 2847
    },
    {
        "loss": 2.1073,
        "grad_norm": 2.1280391216278076,
        "learning_rate": 0.00018582831684282197,
        "epoch": 0.39261097325613453,
        "step": 2848
    },
    {
        "loss": 1.6249,
        "grad_norm": 1.7016706466674805,
        "learning_rate": 0.00018576133876560704,
        "epoch": 0.3927488282326992,
        "step": 2849
    },
    {
        "loss": 2.1671,
        "grad_norm": 1.7663758993148804,
        "learning_rate": 0.00018569421491532363,
        "epoch": 0.39288668320926384,
        "step": 2850
    },
    {
        "loss": 2.4967,
        "grad_norm": 1.1085776090621948,
        "learning_rate": 0.00018562694540606568,
        "epoch": 0.3930245381858285,
        "step": 2851
    },
    {
        "loss": 2.2729,
        "grad_norm": 1.839129090309143,
        "learning_rate": 0.00018555953035217466,
        "epoch": 0.39316239316239315,
        "step": 2852
    },
    {
        "loss": 2.2177,
        "grad_norm": 1.5871293544769287,
        "learning_rate": 0.00018549196986823962,
        "epoch": 0.3933002481389578,
        "step": 2853
    },
    {
        "loss": 2.3961,
        "grad_norm": 1.0361474752426147,
        "learning_rate": 0.00018542426406909654,
        "epoch": 0.39343810311552246,
        "step": 2854
    },
    {
        "loss": 1.2267,
        "grad_norm": 2.2742629051208496,
        "learning_rate": 0.00018535641306982855,
        "epoch": 0.3935759580920871,
        "step": 2855
    },
    {
        "loss": 2.8325,
        "grad_norm": 1.2859220504760742,
        "learning_rate": 0.00018528841698576564,
        "epoch": 0.3937138130686518,
        "step": 2856
    },
    {
        "loss": 1.1445,
        "grad_norm": 2.653597831726074,
        "learning_rate": 0.00018522027593248435,
        "epoch": 0.39385166804521643,
        "step": 2857
    },
    {
        "loss": 2.1951,
        "grad_norm": 1.635560154914856,
        "learning_rate": 0.00018515199002580755,
        "epoch": 0.3939895230217811,
        "step": 2858
    },
    {
        "loss": 2.0554,
        "grad_norm": 1.8443726301193237,
        "learning_rate": 0.00018508355938180442,
        "epoch": 0.39412737799834574,
        "step": 2859
    },
    {
        "loss": 2.2837,
        "grad_norm": 1.4965002536773682,
        "learning_rate": 0.00018501498411679012,
        "epoch": 0.3942652329749104,
        "step": 2860
    },
    {
        "loss": 2.034,
        "grad_norm": 1.0499910116195679,
        "learning_rate": 0.0001849462643473257,
        "epoch": 0.39440308795147505,
        "step": 2861
    },
    {
        "loss": 2.2677,
        "grad_norm": 1.3948004245758057,
        "learning_rate": 0.00018487740019021767,
        "epoch": 0.3945409429280397,
        "step": 2862
    },
    {
        "loss": 1.6628,
        "grad_norm": 2.140484571456909,
        "learning_rate": 0.00018480839176251822,
        "epoch": 0.39467879790460436,
        "step": 2863
    },
    {
        "loss": 1.8168,
        "grad_norm": 1.8135126829147339,
        "learning_rate": 0.00018473923918152445,
        "epoch": 0.394816652881169,
        "step": 2864
    },
    {
        "loss": 2.4477,
        "grad_norm": 1.710292935371399,
        "learning_rate": 0.00018466994256477877,
        "epoch": 0.39495450785773367,
        "step": 2865
    },
    {
        "loss": 2.3388,
        "grad_norm": 2.644049882888794,
        "learning_rate": 0.00018460050203006825,
        "epoch": 0.3950923628342983,
        "step": 2866
    },
    {
        "loss": 1.6127,
        "grad_norm": 1.8045737743377686,
        "learning_rate": 0.00018453091769542456,
        "epoch": 0.395230217810863,
        "step": 2867
    },
    {
        "loss": 2.2913,
        "grad_norm": 1.3338874578475952,
        "learning_rate": 0.00018446118967912396,
        "epoch": 0.39536807278742764,
        "step": 2868
    },
    {
        "loss": 2.6105,
        "grad_norm": 1.4173681735992432,
        "learning_rate": 0.00018439131809968679,
        "epoch": 0.3955059277639923,
        "step": 2869
    },
    {
        "loss": 2.4716,
        "grad_norm": 2.0928232669830322,
        "learning_rate": 0.0001843213030758775,
        "epoch": 0.39564378274055695,
        "step": 2870
    },
    {
        "loss": 1.9217,
        "grad_norm": 1.3878042697906494,
        "learning_rate": 0.00018425114472670425,
        "epoch": 0.3957816377171216,
        "step": 2871
    },
    {
        "loss": 1.8365,
        "grad_norm": 1.6243319511413574,
        "learning_rate": 0.00018418084317141896,
        "epoch": 0.39591949269368626,
        "step": 2872
    },
    {
        "loss": 2.1657,
        "grad_norm": 2.0998029708862305,
        "learning_rate": 0.00018411039852951684,
        "epoch": 0.3960573476702509,
        "step": 2873
    },
    {
        "loss": 1.6935,
        "grad_norm": 2.4166178703308105,
        "learning_rate": 0.0001840398109207363,
        "epoch": 0.39619520264681557,
        "step": 2874
    },
    {
        "loss": 2.1882,
        "grad_norm": 2.0754234790802,
        "learning_rate": 0.00018396908046505893,
        "epoch": 0.3963330576233802,
        "step": 2875
    },
    {
        "loss": 2.0936,
        "grad_norm": 2.597231149673462,
        "learning_rate": 0.00018389820728270903,
        "epoch": 0.3964709125999449,
        "step": 2876
    },
    {
        "loss": 1.6722,
        "grad_norm": 1.9011365175247192,
        "learning_rate": 0.00018382719149415337,
        "epoch": 0.39660876757650954,
        "step": 2877
    },
    {
        "loss": 1.8816,
        "grad_norm": 2.1146297454833984,
        "learning_rate": 0.0001837560332201013,
        "epoch": 0.3967466225530742,
        "step": 2878
    },
    {
        "loss": 2.2117,
        "grad_norm": 1.1749995946884155,
        "learning_rate": 0.00018368473258150426,
        "epoch": 0.39688447752963885,
        "step": 2879
    },
    {
        "loss": 1.6559,
        "grad_norm": 1.8095849752426147,
        "learning_rate": 0.00018361328969955572,
        "epoch": 0.3970223325062035,
        "step": 2880
    },
    {
        "loss": 1.9058,
        "grad_norm": 2.3962395191192627,
        "learning_rate": 0.00018354170469569092,
        "epoch": 0.3971601874827681,
        "step": 2881
    },
    {
        "loss": 1.8272,
        "grad_norm": 1.8683161735534668,
        "learning_rate": 0.00018346997769158667,
        "epoch": 0.39729804245933276,
        "step": 2882
    },
    {
        "loss": 2.3157,
        "grad_norm": 1.2441521883010864,
        "learning_rate": 0.0001833981088091611,
        "epoch": 0.3974358974358974,
        "step": 2883
    },
    {
        "loss": 1.8798,
        "grad_norm": 1.5722942352294922,
        "learning_rate": 0.0001833260981705736,
        "epoch": 0.39757375241246207,
        "step": 2884
    },
    {
        "loss": 2.319,
        "grad_norm": 1.5486867427825928,
        "learning_rate": 0.00018325394589822441,
        "epoch": 0.3977116073890267,
        "step": 2885
    },
    {
        "loss": 2.1627,
        "grad_norm": 1.4393600225448608,
        "learning_rate": 0.0001831816521147545,
        "epoch": 0.3978494623655914,
        "step": 2886
    },
    {
        "loss": 2.1049,
        "grad_norm": 1.6928104162216187,
        "learning_rate": 0.00018310921694304554,
        "epoch": 0.39798731734215603,
        "step": 2887
    },
    {
        "loss": 1.9072,
        "grad_norm": 2.3988218307495117,
        "learning_rate": 0.00018303664050621933,
        "epoch": 0.3981251723187207,
        "step": 2888
    },
    {
        "loss": 2.1331,
        "grad_norm": 1.3177355527877808,
        "learning_rate": 0.00018296392292763787,
        "epoch": 0.39826302729528534,
        "step": 2889
    },
    {
        "loss": 1.4434,
        "grad_norm": 2.0955018997192383,
        "learning_rate": 0.0001828910643309031,
        "epoch": 0.39840088227185,
        "step": 2890
    },
    {
        "loss": 1.6998,
        "grad_norm": 2.523362398147583,
        "learning_rate": 0.0001828180648398566,
        "epoch": 0.39853873724841465,
        "step": 2891
    },
    {
        "loss": 2.106,
        "grad_norm": 1.9408541917800903,
        "learning_rate": 0.00018274492457857945,
        "epoch": 0.3986765922249793,
        "step": 2892
    },
    {
        "loss": 2.4488,
        "grad_norm": 1.6053067445755005,
        "learning_rate": 0.00018267164367139196,
        "epoch": 0.39881444720154396,
        "step": 2893
    },
    {
        "loss": 1.3627,
        "grad_norm": 2.1896748542785645,
        "learning_rate": 0.00018259822224285364,
        "epoch": 0.3989523021781086,
        "step": 2894
    },
    {
        "loss": 2.0148,
        "grad_norm": 2.327390670776367,
        "learning_rate": 0.00018252466041776276,
        "epoch": 0.3990901571546733,
        "step": 2895
    },
    {
        "loss": 2.2708,
        "grad_norm": 2.086484909057617,
        "learning_rate": 0.00018245095832115612,
        "epoch": 0.39922801213123793,
        "step": 2896
    },
    {
        "loss": 2.0672,
        "grad_norm": 2.2922964096069336,
        "learning_rate": 0.00018237711607830913,
        "epoch": 0.3993658671078026,
        "step": 2897
    },
    {
        "loss": 2.3249,
        "grad_norm": 1.4626444578170776,
        "learning_rate": 0.00018230313381473537,
        "epoch": 0.39950372208436724,
        "step": 2898
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.5212846994400024,
        "learning_rate": 0.00018222901165618636,
        "epoch": 0.3996415770609319,
        "step": 2899
    },
    {
        "loss": 1.9209,
        "grad_norm": 1.8370238542556763,
        "learning_rate": 0.00018215474972865144,
        "epoch": 0.39977943203749655,
        "step": 2900
    },
    {
        "loss": 1.8688,
        "grad_norm": 1.9847335815429688,
        "learning_rate": 0.00018208034815835752,
        "epoch": 0.3999172870140612,
        "step": 2901
    },
    {
        "loss": 1.9754,
        "grad_norm": 2.0223796367645264,
        "learning_rate": 0.0001820058070717689,
        "epoch": 0.40005514199062586,
        "step": 2902
    },
    {
        "loss": 2.4188,
        "grad_norm": 2.2377662658691406,
        "learning_rate": 0.00018193112659558696,
        "epoch": 0.4001929969671905,
        "step": 2903
    },
    {
        "loss": 2.1265,
        "grad_norm": 1.9999538660049438,
        "learning_rate": 0.00018185630685675012,
        "epoch": 0.4003308519437552,
        "step": 2904
    },
    {
        "loss": 1.865,
        "grad_norm": 1.629475474357605,
        "learning_rate": 0.0001817813479824333,
        "epoch": 0.40046870692031983,
        "step": 2905
    },
    {
        "loss": 2.1064,
        "grad_norm": 1.594346284866333,
        "learning_rate": 0.0001817062501000481,
        "epoch": 0.4006065618968845,
        "step": 2906
    },
    {
        "loss": 1.437,
        "grad_norm": 2.0029890537261963,
        "learning_rate": 0.00018163101333724244,
        "epoch": 0.40074441687344914,
        "step": 2907
    },
    {
        "loss": 2.523,
        "grad_norm": 1.309303879737854,
        "learning_rate": 0.0001815556378219001,
        "epoch": 0.4008822718500138,
        "step": 2908
    },
    {
        "loss": 1.9447,
        "grad_norm": 1.9603222608566284,
        "learning_rate": 0.0001814801236821409,
        "epoch": 0.40102012682657845,
        "step": 2909
    },
    {
        "loss": 1.8043,
        "grad_norm": 1.5915188789367676,
        "learning_rate": 0.00018140447104632017,
        "epoch": 0.4011579818031431,
        "step": 2910
    },
    {
        "loss": 1.0189,
        "grad_norm": 2.2127087116241455,
        "learning_rate": 0.00018132868004302867,
        "epoch": 0.40129583677970776,
        "step": 2911
    },
    {
        "loss": 1.79,
        "grad_norm": 1.7657647132873535,
        "learning_rate": 0.0001812527508010923,
        "epoch": 0.4014336917562724,
        "step": 2912
    },
    {
        "loss": 2.0089,
        "grad_norm": 2.107858657836914,
        "learning_rate": 0.0001811766834495721,
        "epoch": 0.40157154673283707,
        "step": 2913
    },
    {
        "loss": 1.869,
        "grad_norm": 2.1359851360321045,
        "learning_rate": 0.00018110047811776382,
        "epoch": 0.4017094017094017,
        "step": 2914
    },
    {
        "loss": 1.3497,
        "grad_norm": 2.0499939918518066,
        "learning_rate": 0.0001810241349351975,
        "epoch": 0.4018472566859664,
        "step": 2915
    },
    {
        "loss": 1.7606,
        "grad_norm": 1.1612889766693115,
        "learning_rate": 0.00018094765403163775,
        "epoch": 0.40198511166253104,
        "step": 2916
    },
    {
        "loss": 1.8912,
        "grad_norm": 1.5948370695114136,
        "learning_rate": 0.0001808710355370832,
        "epoch": 0.4021229666390957,
        "step": 2917
    },
    {
        "loss": 1.5578,
        "grad_norm": 2.6821765899658203,
        "learning_rate": 0.00018079427958176637,
        "epoch": 0.40226082161566035,
        "step": 2918
    },
    {
        "loss": 1.617,
        "grad_norm": 1.9581975936889648,
        "learning_rate": 0.00018071738629615335,
        "epoch": 0.402398676592225,
        "step": 2919
    },
    {
        "loss": 2.2622,
        "grad_norm": 1.0647424459457397,
        "learning_rate": 0.00018064035581094376,
        "epoch": 0.40253653156878966,
        "step": 2920
    },
    {
        "loss": 2.3128,
        "grad_norm": 1.6047868728637695,
        "learning_rate": 0.00018056318825707035,
        "epoch": 0.4026743865453543,
        "step": 2921
    },
    {
        "loss": 2.0503,
        "grad_norm": 1.618770956993103,
        "learning_rate": 0.00018048588376569886,
        "epoch": 0.40281224152191897,
        "step": 2922
    },
    {
        "loss": 2.403,
        "grad_norm": 1.6395801305770874,
        "learning_rate": 0.00018040844246822792,
        "epoch": 0.40295009649848357,
        "step": 2923
    },
    {
        "loss": 2.3879,
        "grad_norm": 1.6920762062072754,
        "learning_rate": 0.0001803308644962884,
        "epoch": 0.4030879514750482,
        "step": 2924
    },
    {
        "loss": 1.7439,
        "grad_norm": 2.330415964126587,
        "learning_rate": 0.00018025314998174378,
        "epoch": 0.4032258064516129,
        "step": 2925
    },
    {
        "loss": 2.3723,
        "grad_norm": 1.8014659881591797,
        "learning_rate": 0.00018017529905668952,
        "epoch": 0.40336366142817753,
        "step": 2926
    },
    {
        "loss": 2.6559,
        "grad_norm": 1.8190619945526123,
        "learning_rate": 0.00018009731185345294,
        "epoch": 0.4035015164047422,
        "step": 2927
    },
    {
        "loss": 2.4512,
        "grad_norm": 1.2960962057113647,
        "learning_rate": 0.00018001918850459297,
        "epoch": 0.40363937138130684,
        "step": 2928
    },
    {
        "loss": 1.9008,
        "grad_norm": 1.7319221496582031,
        "learning_rate": 0.00017994092914290003,
        "epoch": 0.4037772263578715,
        "step": 2929
    },
    {
        "loss": 2.5001,
        "grad_norm": 1.2031266689300537,
        "learning_rate": 0.00017986253390139565,
        "epoch": 0.40391508133443615,
        "step": 2930
    },
    {
        "loss": 2.3188,
        "grad_norm": 1.201282024383545,
        "learning_rate": 0.00017978400291333228,
        "epoch": 0.4040529363110008,
        "step": 2931
    },
    {
        "loss": 1.3016,
        "grad_norm": 2.2226450443267822,
        "learning_rate": 0.00017970533631219332,
        "epoch": 0.40419079128756547,
        "step": 2932
    },
    {
        "loss": 1.9858,
        "grad_norm": 1.69422447681427,
        "learning_rate": 0.0001796265342316925,
        "epoch": 0.4043286462641301,
        "step": 2933
    },
    {
        "loss": 2.4105,
        "grad_norm": 2.390155553817749,
        "learning_rate": 0.00017954759680577383,
        "epoch": 0.4044665012406948,
        "step": 2934
    },
    {
        "loss": 1.9837,
        "grad_norm": 1.59178626537323,
        "learning_rate": 0.0001794685241686114,
        "epoch": 0.40460435621725943,
        "step": 2935
    },
    {
        "loss": 1.5064,
        "grad_norm": 2.305299997329712,
        "learning_rate": 0.00017938931645460922,
        "epoch": 0.4047422111938241,
        "step": 2936
    },
    {
        "loss": 1.8677,
        "grad_norm": 2.8944482803344727,
        "learning_rate": 0.00017930997379840075,
        "epoch": 0.40488006617038874,
        "step": 2937
    },
    {
        "loss": 2.6148,
        "grad_norm": 1.327407956123352,
        "learning_rate": 0.0001792304963348489,
        "epoch": 0.4050179211469534,
        "step": 2938
    },
    {
        "loss": 1.0231,
        "grad_norm": 2.027750253677368,
        "learning_rate": 0.00017915088419904568,
        "epoch": 0.40515577612351805,
        "step": 2939
    },
    {
        "loss": 2.5201,
        "grad_norm": 1.5782390832901,
        "learning_rate": 0.0001790711375263121,
        "epoch": 0.4052936311000827,
        "step": 2940
    },
    {
        "loss": 1.7941,
        "grad_norm": 2.108367443084717,
        "learning_rate": 0.00017899125645219773,
        "epoch": 0.40543148607664736,
        "step": 2941
    },
    {
        "loss": 1.9314,
        "grad_norm": 2.1573221683502197,
        "learning_rate": 0.00017891124111248064,
        "epoch": 0.405569341053212,
        "step": 2942
    },
    {
        "loss": 1.9006,
        "grad_norm": 1.8191938400268555,
        "learning_rate": 0.0001788310916431672,
        "epoch": 0.4057071960297767,
        "step": 2943
    },
    {
        "loss": 1.8454,
        "grad_norm": 1.7929511070251465,
        "learning_rate": 0.00017875080818049156,
        "epoch": 0.40584505100634133,
        "step": 2944
    },
    {
        "loss": 2.0102,
        "grad_norm": 2.447977304458618,
        "learning_rate": 0.0001786703908609159,
        "epoch": 0.405982905982906,
        "step": 2945
    },
    {
        "loss": 2.3504,
        "grad_norm": 2.0627365112304688,
        "learning_rate": 0.0001785898398211296,
        "epoch": 0.40612076095947064,
        "step": 2946
    },
    {
        "loss": 1.6409,
        "grad_norm": 2.0130345821380615,
        "learning_rate": 0.00017850915519804968,
        "epoch": 0.4062586159360353,
        "step": 2947
    },
    {
        "loss": 2.0731,
        "grad_norm": 1.283265471458435,
        "learning_rate": 0.00017842833712881998,
        "epoch": 0.40639647091259995,
        "step": 2948
    },
    {
        "loss": 2.1406,
        "grad_norm": 0.8892272114753723,
        "learning_rate": 0.0001783473857508112,
        "epoch": 0.4065343258891646,
        "step": 2949
    },
    {
        "loss": 2.3926,
        "grad_norm": 1.5465658903121948,
        "learning_rate": 0.00017826630120162067,
        "epoch": 0.40667218086572926,
        "step": 2950
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.6022108793258667,
        "learning_rate": 0.00017818508361907212,
        "epoch": 0.4068100358422939,
        "step": 2951
    },
    {
        "loss": 1.6653,
        "grad_norm": 2.8194806575775146,
        "learning_rate": 0.00017810373314121537,
        "epoch": 0.40694789081885857,
        "step": 2952
    },
    {
        "loss": 1.8391,
        "grad_norm": 2.3324997425079346,
        "learning_rate": 0.00017802224990632603,
        "epoch": 0.4070857457954232,
        "step": 2953
    },
    {
        "loss": 1.5883,
        "grad_norm": 1.4839061498641968,
        "learning_rate": 0.0001779406340529055,
        "epoch": 0.4072236007719879,
        "step": 2954
    },
    {
        "loss": 1.2002,
        "grad_norm": 2.0056591033935547,
        "learning_rate": 0.00017785888571968055,
        "epoch": 0.40736145574855254,
        "step": 2955
    },
    {
        "loss": 2.0952,
        "grad_norm": 1.0085049867630005,
        "learning_rate": 0.00017777700504560312,
        "epoch": 0.4074993107251172,
        "step": 2956
    },
    {
        "loss": 2.2116,
        "grad_norm": 1.3892966508865356,
        "learning_rate": 0.00017769499216985013,
        "epoch": 0.40763716570168185,
        "step": 2957
    },
    {
        "loss": 1.6562,
        "grad_norm": 1.4518221616744995,
        "learning_rate": 0.00017761284723182317,
        "epoch": 0.4077750206782465,
        "step": 2958
    },
    {
        "loss": 1.4651,
        "grad_norm": 2.2673630714416504,
        "learning_rate": 0.0001775305703711483,
        "epoch": 0.40791287565481116,
        "step": 2959
    },
    {
        "loss": 2.3044,
        "grad_norm": 1.5976842641830444,
        "learning_rate": 0.0001774481617276759,
        "epoch": 0.4080507306313758,
        "step": 2960
    },
    {
        "loss": 1.8147,
        "grad_norm": 2.271296262741089,
        "learning_rate": 0.00017736562144148025,
        "epoch": 0.40818858560794047,
        "step": 2961
    },
    {
        "loss": 1.805,
        "grad_norm": 2.5822527408599854,
        "learning_rate": 0.00017728294965285938,
        "epoch": 0.4083264405845051,
        "step": 2962
    },
    {
        "loss": 2.2461,
        "grad_norm": 1.3352606296539307,
        "learning_rate": 0.00017720014650233495,
        "epoch": 0.4084642955610698,
        "step": 2963
    },
    {
        "loss": 1.5333,
        "grad_norm": 2.073974132537842,
        "learning_rate": 0.00017711721213065182,
        "epoch": 0.40860215053763443,
        "step": 2964
    },
    {
        "loss": 1.6382,
        "grad_norm": 1.989222526550293,
        "learning_rate": 0.00017703414667877791,
        "epoch": 0.4087400055141991,
        "step": 2965
    },
    {
        "loss": 1.0412,
        "grad_norm": 1.85688316822052,
        "learning_rate": 0.00017695095028790397,
        "epoch": 0.4088778604907637,
        "step": 2966
    },
    {
        "loss": 2.2007,
        "grad_norm": 1.9516445398330688,
        "learning_rate": 0.0001768676230994433,
        "epoch": 0.40901571546732834,
        "step": 2967
    },
    {
        "loss": 2.3175,
        "grad_norm": 1.2280640602111816,
        "learning_rate": 0.00017678416525503145,
        "epoch": 0.409153570443893,
        "step": 2968
    },
    {
        "loss": 2.1523,
        "grad_norm": 1.522666573524475,
        "learning_rate": 0.00017670057689652611,
        "epoch": 0.40929142542045766,
        "step": 2969
    },
    {
        "loss": 2.1164,
        "grad_norm": 1.6169832944869995,
        "learning_rate": 0.00017661685816600692,
        "epoch": 0.4094292803970223,
        "step": 2970
    },
    {
        "loss": 1.7245,
        "grad_norm": 1.768020510673523,
        "learning_rate": 0.000176533009205775,
        "epoch": 0.40956713537358697,
        "step": 2971
    },
    {
        "loss": 1.0112,
        "grad_norm": 2.1569883823394775,
        "learning_rate": 0.0001764490301583528,
        "epoch": 0.4097049903501516,
        "step": 2972
    },
    {
        "loss": 1.8959,
        "grad_norm": 1.7413361072540283,
        "learning_rate": 0.00017636492116648392,
        "epoch": 0.4098428453267163,
        "step": 2973
    },
    {
        "loss": 1.3639,
        "grad_norm": 2.858283281326294,
        "learning_rate": 0.00017628068237313284,
        "epoch": 0.40998070030328093,
        "step": 2974
    },
    {
        "loss": 2.3899,
        "grad_norm": 1.341441035270691,
        "learning_rate": 0.00017619631392148475,
        "epoch": 0.4101185552798456,
        "step": 2975
    },
    {
        "loss": 1.9598,
        "grad_norm": 2.198742628097534,
        "learning_rate": 0.0001761118159549451,
        "epoch": 0.41025641025641024,
        "step": 2976
    },
    {
        "loss": 2.1163,
        "grad_norm": 1.7682725191116333,
        "learning_rate": 0.00017602718861713958,
        "epoch": 0.4103942652329749,
        "step": 2977
    },
    {
        "loss": 2.1244,
        "grad_norm": 2.2565646171569824,
        "learning_rate": 0.00017594243205191367,
        "epoch": 0.41053212020953955,
        "step": 2978
    },
    {
        "loss": 1.3804,
        "grad_norm": 3.3381752967834473,
        "learning_rate": 0.00017585754640333265,
        "epoch": 0.4106699751861042,
        "step": 2979
    },
    {
        "loss": 1.5749,
        "grad_norm": 1.8312100172042847,
        "learning_rate": 0.0001757725318156811,
        "epoch": 0.41080783016266886,
        "step": 2980
    },
    {
        "loss": 2.0173,
        "grad_norm": 1.6997992992401123,
        "learning_rate": 0.00017568738843346275,
        "epoch": 0.4109456851392335,
        "step": 2981
    },
    {
        "loss": 2.1826,
        "grad_norm": 2.059877395629883,
        "learning_rate": 0.0001756021164014004,
        "epoch": 0.4110835401157982,
        "step": 2982
    },
    {
        "loss": 2.2556,
        "grad_norm": 1.9657586812973022,
        "learning_rate": 0.0001755167158644353,
        "epoch": 0.41122139509236283,
        "step": 2983
    },
    {
        "loss": 2.512,
        "grad_norm": 1.6693402528762817,
        "learning_rate": 0.00017543118696772733,
        "epoch": 0.4113592500689275,
        "step": 2984
    },
    {
        "loss": 1.4193,
        "grad_norm": 2.096684217453003,
        "learning_rate": 0.0001753455298566544,
        "epoch": 0.41149710504549214,
        "step": 2985
    },
    {
        "loss": 1.6947,
        "grad_norm": 2.132024049758911,
        "learning_rate": 0.00017525974467681255,
        "epoch": 0.4116349600220568,
        "step": 2986
    },
    {
        "loss": 1.982,
        "grad_norm": 1.3247270584106445,
        "learning_rate": 0.0001751738315740152,
        "epoch": 0.41177281499862145,
        "step": 2987
    },
    {
        "loss": 2.0764,
        "grad_norm": 2.424192428588867,
        "learning_rate": 0.0001750877906942934,
        "epoch": 0.4119106699751861,
        "step": 2988
    },
    {
        "loss": 1.8222,
        "grad_norm": 1.7955782413482666,
        "learning_rate": 0.00017500162218389548,
        "epoch": 0.41204852495175076,
        "step": 2989
    },
    {
        "loss": 1.3931,
        "grad_norm": 2.684392213821411,
        "learning_rate": 0.0001749153261892866,
        "epoch": 0.4121863799283154,
        "step": 2990
    },
    {
        "loss": 1.5646,
        "grad_norm": 2.8911356925964355,
        "learning_rate": 0.00017482890285714847,
        "epoch": 0.41232423490488007,
        "step": 2991
    },
    {
        "loss": 1.2606,
        "grad_norm": 2.6731836795806885,
        "learning_rate": 0.0001747423523343795,
        "epoch": 0.4124620898814447,
        "step": 2992
    },
    {
        "loss": 2.4538,
        "grad_norm": 1.564988374710083,
        "learning_rate": 0.00017465567476809412,
        "epoch": 0.4125999448580094,
        "step": 2993
    },
    {
        "loss": 1.2379,
        "grad_norm": 2.306877374649048,
        "learning_rate": 0.00017456887030562282,
        "epoch": 0.41273779983457404,
        "step": 2994
    },
    {
        "loss": 2.146,
        "grad_norm": 1.702621340751648,
        "learning_rate": 0.00017448193909451164,
        "epoch": 0.4128756548111387,
        "step": 2995
    },
    {
        "loss": 1.0963,
        "grad_norm": 2.449477195739746,
        "learning_rate": 0.0001743948812825222,
        "epoch": 0.41301350978770335,
        "step": 2996
    },
    {
        "loss": 2.192,
        "grad_norm": 1.178101658821106,
        "learning_rate": 0.00017430769701763126,
        "epoch": 0.413151364764268,
        "step": 2997
    },
    {
        "loss": 2.4503,
        "grad_norm": 1.0554914474487305,
        "learning_rate": 0.00017422038644803047,
        "epoch": 0.41328921974083266,
        "step": 2998
    },
    {
        "loss": 1.5924,
        "grad_norm": 2.687560796737671,
        "learning_rate": 0.0001741329497221262,
        "epoch": 0.4134270747173973,
        "step": 2999
    },
    {
        "loss": 2.5276,
        "grad_norm": 1.500512719154358,
        "learning_rate": 0.0001740453869885393,
        "epoch": 0.41356492969396197,
        "step": 3000
    },
    {
        "loss": 2.3452,
        "grad_norm": 1.4029960632324219,
        "learning_rate": 0.0001739576983961048,
        "epoch": 0.4137027846705266,
        "step": 3001
    },
    {
        "loss": 2.2433,
        "grad_norm": 1.4214483499526978,
        "learning_rate": 0.00017386988409387154,
        "epoch": 0.4138406396470913,
        "step": 3002
    },
    {
        "loss": 2.2218,
        "grad_norm": 1.9220576286315918,
        "learning_rate": 0.00017378194423110216,
        "epoch": 0.41397849462365593,
        "step": 3003
    },
    {
        "loss": 2.369,
        "grad_norm": 2.0127146244049072,
        "learning_rate": 0.00017369387895727268,
        "epoch": 0.4141163496002206,
        "step": 3004
    },
    {
        "loss": 1.6026,
        "grad_norm": 2.2424514293670654,
        "learning_rate": 0.00017360568842207232,
        "epoch": 0.41425420457678525,
        "step": 3005
    },
    {
        "loss": 2.1013,
        "grad_norm": 1.8384945392608643,
        "learning_rate": 0.0001735173727754031,
        "epoch": 0.4143920595533499,
        "step": 3006
    },
    {
        "loss": 2.3497,
        "grad_norm": 1.731911063194275,
        "learning_rate": 0.0001734289321673798,
        "epoch": 0.41452991452991456,
        "step": 3007
    },
    {
        "loss": 2.4948,
        "grad_norm": 1.1034131050109863,
        "learning_rate": 0.00017334036674832964,
        "epoch": 0.4146677695064792,
        "step": 3008
    },
    {
        "loss": 1.5577,
        "grad_norm": 1.966077446937561,
        "learning_rate": 0.00017325167666879193,
        "epoch": 0.4148056244830438,
        "step": 3009
    },
    {
        "loss": 1.8638,
        "grad_norm": 1.7922312021255493,
        "learning_rate": 0.00017316286207951783,
        "epoch": 0.41494347945960847,
        "step": 3010
    },
    {
        "loss": 2.3654,
        "grad_norm": 1.0698460340499878,
        "learning_rate": 0.00017307392313147018,
        "epoch": 0.4150813344361731,
        "step": 3011
    },
    {
        "loss": 1.7629,
        "grad_norm": 1.606138825416565,
        "learning_rate": 0.00017298485997582321,
        "epoch": 0.4152191894127378,
        "step": 3012
    },
    {
        "loss": 2.4681,
        "grad_norm": 1.2665297985076904,
        "learning_rate": 0.00017289567276396225,
        "epoch": 0.41535704438930243,
        "step": 3013
    },
    {
        "loss": 2.0576,
        "grad_norm": 1.3985803127288818,
        "learning_rate": 0.00017280636164748354,
        "epoch": 0.4154948993658671,
        "step": 3014
    },
    {
        "loss": 2.1932,
        "grad_norm": 1.433969259262085,
        "learning_rate": 0.00017271692677819388,
        "epoch": 0.41563275434243174,
        "step": 3015
    },
    {
        "loss": 1.5162,
        "grad_norm": 1.8727623224258423,
        "learning_rate": 0.00017262736830811042,
        "epoch": 0.4157706093189964,
        "step": 3016
    },
    {
        "loss": 1.958,
        "grad_norm": 1.453482985496521,
        "learning_rate": 0.00017253768638946038,
        "epoch": 0.41590846429556105,
        "step": 3017
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.8628122806549072,
        "learning_rate": 0.00017244788117468094,
        "epoch": 0.4160463192721257,
        "step": 3018
    },
    {
        "loss": 2.1998,
        "grad_norm": 1.238551378250122,
        "learning_rate": 0.00017235795281641864,
        "epoch": 0.41618417424869036,
        "step": 3019
    },
    {
        "loss": 1.3088,
        "grad_norm": 2.278977394104004,
        "learning_rate": 0.00017226790146752957,
        "epoch": 0.416322029225255,
        "step": 3020
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.9960927963256836,
        "learning_rate": 0.00017217772728107865,
        "epoch": 0.4164598842018197,
        "step": 3021
    },
    {
        "loss": 2.5372,
        "grad_norm": 1.2448527812957764,
        "learning_rate": 0.00017208743041033974,
        "epoch": 0.41659773917838433,
        "step": 3022
    },
    {
        "loss": 1.3993,
        "grad_norm": 2.4338011741638184,
        "learning_rate": 0.0001719970110087952,
        "epoch": 0.416735594154949,
        "step": 3023
    },
    {
        "loss": 1.4865,
        "grad_norm": 1.4805375337600708,
        "learning_rate": 0.00017190646923013572,
        "epoch": 0.41687344913151364,
        "step": 3024
    },
    {
        "loss": 2.3606,
        "grad_norm": 1.3976294994354248,
        "learning_rate": 0.0001718158052282598,
        "epoch": 0.4170113041080783,
        "step": 3025
    },
    {
        "loss": 2.2988,
        "grad_norm": 2.4945027828216553,
        "learning_rate": 0.00017172501915727384,
        "epoch": 0.41714915908464295,
        "step": 3026
    },
    {
        "loss": 1.6606,
        "grad_norm": 1.886569857597351,
        "learning_rate": 0.00017163411117149182,
        "epoch": 0.4172870140612076,
        "step": 3027
    },
    {
        "loss": 1.6636,
        "grad_norm": 2.530277729034424,
        "learning_rate": 0.0001715430814254348,
        "epoch": 0.41742486903777226,
        "step": 3028
    },
    {
        "loss": 2.1788,
        "grad_norm": 2.199310779571533,
        "learning_rate": 0.00017145193007383073,
        "epoch": 0.4175627240143369,
        "step": 3029
    },
    {
        "loss": 1.4409,
        "grad_norm": 2.49528169631958,
        "learning_rate": 0.00017136065727161453,
        "epoch": 0.41770057899090157,
        "step": 3030
    },
    {
        "loss": 1.4007,
        "grad_norm": 3.3623223304748535,
        "learning_rate": 0.00017126926317392725,
        "epoch": 0.4178384339674662,
        "step": 3031
    },
    {
        "loss": 1.0052,
        "grad_norm": 2.382838487625122,
        "learning_rate": 0.0001711777479361163,
        "epoch": 0.4179762889440309,
        "step": 3032
    },
    {
        "loss": 1.9192,
        "grad_norm": 1.740936279296875,
        "learning_rate": 0.00017108611171373498,
        "epoch": 0.41811414392059554,
        "step": 3033
    },
    {
        "loss": 2.1832,
        "grad_norm": 1.6343988180160522,
        "learning_rate": 0.00017099435466254228,
        "epoch": 0.4182519988971602,
        "step": 3034
    },
    {
        "loss": 2.2661,
        "grad_norm": 1.8414866924285889,
        "learning_rate": 0.00017090247693850235,
        "epoch": 0.41838985387372485,
        "step": 3035
    },
    {
        "loss": 2.3008,
        "grad_norm": 1.7001872062683105,
        "learning_rate": 0.00017081047869778467,
        "epoch": 0.4185277088502895,
        "step": 3036
    },
    {
        "loss": 2.2327,
        "grad_norm": 1.35244619846344,
        "learning_rate": 0.00017071836009676348,
        "epoch": 0.41866556382685416,
        "step": 3037
    },
    {
        "loss": 2.436,
        "grad_norm": 1.2062262296676636,
        "learning_rate": 0.00017062612129201766,
        "epoch": 0.4188034188034188,
        "step": 3038
    },
    {
        "loss": 2.2758,
        "grad_norm": 1.836323618888855,
        "learning_rate": 0.00017053376244033038,
        "epoch": 0.41894127377998347,
        "step": 3039
    },
    {
        "loss": 2.5319,
        "grad_norm": 1.6927226781845093,
        "learning_rate": 0.00017044128369868877,
        "epoch": 0.4190791287565481,
        "step": 3040
    },
    {
        "loss": 1.8186,
        "grad_norm": 2.2658851146698,
        "learning_rate": 0.00017034868522428388,
        "epoch": 0.4192169837331128,
        "step": 3041
    },
    {
        "loss": 1.8102,
        "grad_norm": 1.7531249523162842,
        "learning_rate": 0.00017025596717451023,
        "epoch": 0.41935483870967744,
        "step": 3042
    },
    {
        "loss": 2.1166,
        "grad_norm": 1.587883710861206,
        "learning_rate": 0.00017016312970696555,
        "epoch": 0.4194926936862421,
        "step": 3043
    },
    {
        "loss": 1.7108,
        "grad_norm": 1.8670248985290527,
        "learning_rate": 0.0001700701729794506,
        "epoch": 0.41963054866280675,
        "step": 3044
    },
    {
        "loss": 2.3849,
        "grad_norm": 1.2426276206970215,
        "learning_rate": 0.0001699770971499687,
        "epoch": 0.4197684036393714,
        "step": 3045
    },
    {
        "loss": 1.9348,
        "grad_norm": 2.2306532859802246,
        "learning_rate": 0.00016988390237672595,
        "epoch": 0.41990625861593606,
        "step": 3046
    },
    {
        "loss": 1.2354,
        "grad_norm": 2.8448283672332764,
        "learning_rate": 0.00016979058881813036,
        "epoch": 0.4200441135925007,
        "step": 3047
    },
    {
        "loss": 2.24,
        "grad_norm": 2.418617010116577,
        "learning_rate": 0.00016969715663279182,
        "epoch": 0.42018196856906537,
        "step": 3048
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.307659387588501,
        "learning_rate": 0.00016960360597952198,
        "epoch": 0.42031982354563,
        "step": 3049
    },
    {
        "loss": 2.4291,
        "grad_norm": 1.6194047927856445,
        "learning_rate": 0.00016950993701733385,
        "epoch": 0.4204576785221947,
        "step": 3050
    },
    {
        "loss": 1.507,
        "grad_norm": 1.8579154014587402,
        "learning_rate": 0.00016941614990544145,
        "epoch": 0.4205955334987593,
        "step": 3051
    },
    {
        "loss": 1.6134,
        "grad_norm": 2.0363383293151855,
        "learning_rate": 0.00016932224480325968,
        "epoch": 0.42073338847532393,
        "step": 3052
    },
    {
        "loss": 1.6181,
        "grad_norm": 1.9320907592773438,
        "learning_rate": 0.00016922822187040413,
        "epoch": 0.4208712434518886,
        "step": 3053
    },
    {
        "loss": 1.2518,
        "grad_norm": 1.955197811126709,
        "learning_rate": 0.00016913408126669036,
        "epoch": 0.42100909842845324,
        "step": 3054
    },
    {
        "loss": 2.0714,
        "grad_norm": 2.1079330444335938,
        "learning_rate": 0.0001690398231521342,
        "epoch": 0.4211469534050179,
        "step": 3055
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.799818515777588,
        "learning_rate": 0.00016894544768695108,
        "epoch": 0.42128480838158255,
        "step": 3056
    },
    {
        "loss": 1.4555,
        "grad_norm": 2.317495107650757,
        "learning_rate": 0.00016885095503155602,
        "epoch": 0.4214226633581472,
        "step": 3057
    },
    {
        "loss": 2.101,
        "grad_norm": 1.3540880680084229,
        "learning_rate": 0.00016875634534656314,
        "epoch": 0.42156051833471186,
        "step": 3058
    },
    {
        "loss": 2.2794,
        "grad_norm": 1.8228065967559814,
        "learning_rate": 0.0001686616187927855,
        "epoch": 0.4216983733112765,
        "step": 3059
    },
    {
        "loss": 2.2939,
        "grad_norm": 1.8273359537124634,
        "learning_rate": 0.00016856677553123484,
        "epoch": 0.4218362282878412,
        "step": 3060
    },
    {
        "loss": 1.865,
        "grad_norm": 2.542687177658081,
        "learning_rate": 0.00016847181572312122,
        "epoch": 0.42197408326440583,
        "step": 3061
    },
    {
        "loss": 1.9462,
        "grad_norm": 1.7540960311889648,
        "learning_rate": 0.0001683767395298529,
        "epoch": 0.4221119382409705,
        "step": 3062
    },
    {
        "loss": 1.5484,
        "grad_norm": 2.184112548828125,
        "learning_rate": 0.00016828154711303584,
        "epoch": 0.42224979321753514,
        "step": 3063
    },
    {
        "loss": 1.9672,
        "grad_norm": 1.9497727155685425,
        "learning_rate": 0.0001681862386344735,
        "epoch": 0.4223876481940998,
        "step": 3064
    },
    {
        "loss": 2.4448,
        "grad_norm": 1.4427788257598877,
        "learning_rate": 0.00016809081425616697,
        "epoch": 0.42252550317066445,
        "step": 3065
    },
    {
        "loss": 2.2108,
        "grad_norm": 1.213217854499817,
        "learning_rate": 0.00016799527414031396,
        "epoch": 0.4226633581472291,
        "step": 3066
    },
    {
        "loss": 1.5603,
        "grad_norm": 2.3329060077667236,
        "learning_rate": 0.00016789961844930902,
        "epoch": 0.42280121312379376,
        "step": 3067
    },
    {
        "loss": 2.2828,
        "grad_norm": 1.451318383216858,
        "learning_rate": 0.0001678038473457432,
        "epoch": 0.4229390681003584,
        "step": 3068
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.3365306854248047,
        "learning_rate": 0.00016770796099240367,
        "epoch": 0.4230769230769231,
        "step": 3069
    },
    {
        "loss": 1.7567,
        "grad_norm": 1.8856960535049438,
        "learning_rate": 0.00016761195955227357,
        "epoch": 0.42321477805348773,
        "step": 3070
    },
    {
        "loss": 1.4568,
        "grad_norm": 3.3103551864624023,
        "learning_rate": 0.00016751584318853153,
        "epoch": 0.4233526330300524,
        "step": 3071
    },
    {
        "loss": 2.5747,
        "grad_norm": 1.6805297136306763,
        "learning_rate": 0.00016741961206455174,
        "epoch": 0.42349048800661704,
        "step": 3072
    },
    {
        "loss": 1.6,
        "grad_norm": 2.657912492752075,
        "learning_rate": 0.0001673232663439032,
        "epoch": 0.4236283429831817,
        "step": 3073
    },
    {
        "loss": 1.1644,
        "grad_norm": 2.175112724304199,
        "learning_rate": 0.00016722680619034978,
        "epoch": 0.42376619795974635,
        "step": 3074
    },
    {
        "loss": 1.9746,
        "grad_norm": 1.411116361618042,
        "learning_rate": 0.00016713023176784998,
        "epoch": 0.423904052936311,
        "step": 3075
    },
    {
        "loss": 2.2224,
        "grad_norm": 1.4728270769119263,
        "learning_rate": 0.0001670335432405564,
        "epoch": 0.42404190791287566,
        "step": 3076
    },
    {
        "loss": 2.2747,
        "grad_norm": 1.7900159358978271,
        "learning_rate": 0.00016693674077281564,
        "epoch": 0.4241797628894403,
        "step": 3077
    },
    {
        "loss": 1.864,
        "grad_norm": 1.8312166929244995,
        "learning_rate": 0.00016683982452916793,
        "epoch": 0.42431761786600497,
        "step": 3078
    },
    {
        "loss": 2.4018,
        "grad_norm": 1.4765998125076294,
        "learning_rate": 0.00016674279467434694,
        "epoch": 0.4244554728425696,
        "step": 3079
    },
    {
        "loss": 2.0424,
        "grad_norm": 1.7138581275939941,
        "learning_rate": 0.00016664565137327944,
        "epoch": 0.4245933278191343,
        "step": 3080
    },
    {
        "loss": 1.7234,
        "grad_norm": 1.7540024518966675,
        "learning_rate": 0.00016654839479108503,
        "epoch": 0.42473118279569894,
        "step": 3081
    },
    {
        "loss": 2.2358,
        "grad_norm": 1.3475029468536377,
        "learning_rate": 0.0001664510250930758,
        "epoch": 0.4248690377722636,
        "step": 3082
    },
    {
        "loss": 1.3533,
        "grad_norm": 2.582102060317993,
        "learning_rate": 0.0001663535424447561,
        "epoch": 0.42500689274882825,
        "step": 3083
    },
    {
        "loss": 0.8101,
        "grad_norm": 2.098677158355713,
        "learning_rate": 0.00016625594701182248,
        "epoch": 0.4251447477253929,
        "step": 3084
    },
    {
        "loss": 2.2037,
        "grad_norm": 2.597944498062134,
        "learning_rate": 0.00016615823896016298,
        "epoch": 0.42528260270195756,
        "step": 3085
    },
    {
        "loss": 2.0546,
        "grad_norm": 1.8138279914855957,
        "learning_rate": 0.00016606041845585712,
        "epoch": 0.4254204576785222,
        "step": 3086
    },
    {
        "loss": 2.2336,
        "grad_norm": 1.5287110805511475,
        "learning_rate": 0.00016596248566517555,
        "epoch": 0.42555831265508687,
        "step": 3087
    },
    {
        "loss": 1.7937,
        "grad_norm": 1.3036504983901978,
        "learning_rate": 0.00016586444075457978,
        "epoch": 0.4256961676316515,
        "step": 3088
    },
    {
        "loss": 1.9212,
        "grad_norm": 1.5225130319595337,
        "learning_rate": 0.00016576628389072196,
        "epoch": 0.4258340226082162,
        "step": 3089
    },
    {
        "loss": 1.9628,
        "grad_norm": 2.184454917907715,
        "learning_rate": 0.00016566801524044443,
        "epoch": 0.42597187758478083,
        "step": 3090
    },
    {
        "loss": 1.5833,
        "grad_norm": 2.4031825065612793,
        "learning_rate": 0.00016556963497077972,
        "epoch": 0.4261097325613455,
        "step": 3091
    },
    {
        "loss": 1.5812,
        "grad_norm": 2.2823851108551025,
        "learning_rate": 0.00016547114324894985,
        "epoch": 0.42624758753791014,
        "step": 3092
    },
    {
        "loss": 2.1344,
        "grad_norm": 1.1468784809112549,
        "learning_rate": 0.00016537254024236641,
        "epoch": 0.4263854425144748,
        "step": 3093
    },
    {
        "loss": 1.741,
        "grad_norm": 1.3767024278640747,
        "learning_rate": 0.00016527382611863014,
        "epoch": 0.4265232974910394,
        "step": 3094
    },
    {
        "loss": 1.7637,
        "grad_norm": 2.222946882247925,
        "learning_rate": 0.00016517500104553062,
        "epoch": 0.42666115246760405,
        "step": 3095
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.3708149194717407,
        "learning_rate": 0.00016507606519104606,
        "epoch": 0.4267990074441687,
        "step": 3096
    },
    {
        "loss": 2.0322,
        "grad_norm": 1.9628973007202148,
        "learning_rate": 0.00016497701872334293,
        "epoch": 0.42693686242073337,
        "step": 3097
    },
    {
        "loss": 2.3946,
        "grad_norm": 1.8656495809555054,
        "learning_rate": 0.00016487786181077575,
        "epoch": 0.427074717397298,
        "step": 3098
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.5832622051239014,
        "learning_rate": 0.00016477859462188675,
        "epoch": 0.4272125723738627,
        "step": 3099
    },
    {
        "loss": 1.6489,
        "grad_norm": 2.0513479709625244,
        "learning_rate": 0.00016467921732540565,
        "epoch": 0.42735042735042733,
        "step": 3100
    },
    {
        "loss": 1.297,
        "grad_norm": 2.099536418914795,
        "learning_rate": 0.00016457973009024917,
        "epoch": 0.427488282326992,
        "step": 3101
    },
    {
        "loss": 1.9367,
        "grad_norm": 1.6737635135650635,
        "learning_rate": 0.000164480133085521,
        "epoch": 0.42762613730355664,
        "step": 3102
    },
    {
        "loss": 2.2867,
        "grad_norm": 1.4419430494308472,
        "learning_rate": 0.00016438042648051157,
        "epoch": 0.4277639922801213,
        "step": 3103
    },
    {
        "loss": 2.4545,
        "grad_norm": 1.3492403030395508,
        "learning_rate": 0.00016428061044469741,
        "epoch": 0.42790184725668595,
        "step": 3104
    },
    {
        "loss": 1.3444,
        "grad_norm": 2.4374046325683594,
        "learning_rate": 0.000164180685147741,
        "epoch": 0.4280397022332506,
        "step": 3105
    },
    {
        "loss": 1.3662,
        "grad_norm": 2.242323637008667,
        "learning_rate": 0.00016408065075949066,
        "epoch": 0.42817755720981526,
        "step": 3106
    },
    {
        "loss": 2.4486,
        "grad_norm": 1.660388708114624,
        "learning_rate": 0.00016398050744998016,
        "epoch": 0.4283154121863799,
        "step": 3107
    },
    {
        "loss": 2.4531,
        "grad_norm": 1.5156493186950684,
        "learning_rate": 0.00016388025538942834,
        "epoch": 0.4284532671629446,
        "step": 3108
    },
    {
        "loss": 2.0495,
        "grad_norm": 2.255741834640503,
        "learning_rate": 0.00016377989474823892,
        "epoch": 0.42859112213950923,
        "step": 3109
    },
    {
        "loss": 1.6448,
        "grad_norm": 2.1465368270874023,
        "learning_rate": 0.00016367942569700024,
        "epoch": 0.4287289771160739,
        "step": 3110
    },
    {
        "loss": 2.1595,
        "grad_norm": 1.9313552379608154,
        "learning_rate": 0.00016357884840648469,
        "epoch": 0.42886683209263854,
        "step": 3111
    },
    {
        "loss": 2.1761,
        "grad_norm": 1.342551350593567,
        "learning_rate": 0.00016347816304764892,
        "epoch": 0.4290046870692032,
        "step": 3112
    },
    {
        "loss": 2.0828,
        "grad_norm": 1.2293137311935425,
        "learning_rate": 0.00016337736979163306,
        "epoch": 0.42914254204576785,
        "step": 3113
    },
    {
        "loss": 1.4182,
        "grad_norm": 2.562164306640625,
        "learning_rate": 0.00016327646880976077,
        "epoch": 0.4292803970223325,
        "step": 3114
    },
    {
        "loss": 2.3404,
        "grad_norm": 1.3619980812072754,
        "learning_rate": 0.00016317546027353876,
        "epoch": 0.42941825199889716,
        "step": 3115
    },
    {
        "loss": 2.1428,
        "grad_norm": 1.2981764078140259,
        "learning_rate": 0.00016307434435465656,
        "epoch": 0.4295561069754618,
        "step": 3116
    },
    {
        "loss": 1.7414,
        "grad_norm": 2.1382551193237305,
        "learning_rate": 0.00016297312122498623,
        "epoch": 0.42969396195202647,
        "step": 3117
    },
    {
        "loss": 2.3511,
        "grad_norm": 1.4588631391525269,
        "learning_rate": 0.00016287179105658206,
        "epoch": 0.4298318169285911,
        "step": 3118
    },
    {
        "loss": 2.3079,
        "grad_norm": 1.5754715204238892,
        "learning_rate": 0.00016277035402168034,
        "epoch": 0.4299696719051558,
        "step": 3119
    },
    {
        "loss": 0.9025,
        "grad_norm": 2.031317710876465,
        "learning_rate": 0.0001626688102926988,
        "epoch": 0.43010752688172044,
        "step": 3120
    },
    {
        "loss": 1.294,
        "grad_norm": 2.2156214714050293,
        "learning_rate": 0.00016256716004223672,
        "epoch": 0.4302453818582851,
        "step": 3121
    },
    {
        "loss": 2.4517,
        "grad_norm": 2.0845353603363037,
        "learning_rate": 0.00016246540344307445,
        "epoch": 0.43038323683484975,
        "step": 3122
    },
    {
        "loss": 2.4779,
        "grad_norm": 1.900146484375,
        "learning_rate": 0.00016236354066817313,
        "epoch": 0.4305210918114144,
        "step": 3123
    },
    {
        "loss": 1.6532,
        "grad_norm": 2.8309080600738525,
        "learning_rate": 0.000162261571890674,
        "epoch": 0.43065894678797906,
        "step": 3124
    },
    {
        "loss": 2.7713,
        "grad_norm": 1.741837501525879,
        "learning_rate": 0.00016215949728389908,
        "epoch": 0.4307968017645437,
        "step": 3125
    },
    {
        "loss": 1.7804,
        "grad_norm": 2.105231761932373,
        "learning_rate": 0.00016205731702134977,
        "epoch": 0.43093465674110837,
        "step": 3126
    },
    {
        "loss": 2.1576,
        "grad_norm": 1.4755868911743164,
        "learning_rate": 0.0001619550312767073,
        "epoch": 0.431072511717673,
        "step": 3127
    },
    {
        "loss": 2.3351,
        "grad_norm": 2.1547651290893555,
        "learning_rate": 0.00016185264022383207,
        "epoch": 0.4312103666942377,
        "step": 3128
    },
    {
        "loss": 1.6128,
        "grad_norm": 1.9953128099441528,
        "learning_rate": 0.00016175014403676377,
        "epoch": 0.43134822167080233,
        "step": 3129
    },
    {
        "loss": 2.1648,
        "grad_norm": 1.8784631490707397,
        "learning_rate": 0.00016164754288972033,
        "epoch": 0.431486076647367,
        "step": 3130
    },
    {
        "loss": 2.1521,
        "grad_norm": 1.2915611267089844,
        "learning_rate": 0.00016154483695709845,
        "epoch": 0.43162393162393164,
        "step": 3131
    },
    {
        "loss": 1.4317,
        "grad_norm": 2.2997522354125977,
        "learning_rate": 0.00016144202641347276,
        "epoch": 0.4317617866004963,
        "step": 3132
    },
    {
        "loss": 1.6112,
        "grad_norm": 2.1023058891296387,
        "learning_rate": 0.0001613391114335958,
        "epoch": 0.43189964157706096,
        "step": 3133
    },
    {
        "loss": 1.9355,
        "grad_norm": 1.47821843624115,
        "learning_rate": 0.0001612360921923976,
        "epoch": 0.4320374965536256,
        "step": 3134
    },
    {
        "loss": 2.3012,
        "grad_norm": 1.9457987546920776,
        "learning_rate": 0.00016113296886498537,
        "epoch": 0.43217535153019027,
        "step": 3135
    },
    {
        "loss": 1.9362,
        "grad_norm": 1.8305296897888184,
        "learning_rate": 0.00016102974162664327,
        "epoch": 0.4323132065067549,
        "step": 3136
    },
    {
        "loss": 2.39,
        "grad_norm": 1.6919113397598267,
        "learning_rate": 0.00016092641065283206,
        "epoch": 0.4324510614833195,
        "step": 3137
    },
    {
        "loss": 1.6487,
        "grad_norm": 2.0015058517456055,
        "learning_rate": 0.00016082297611918895,
        "epoch": 0.4325889164598842,
        "step": 3138
    },
    {
        "loss": 2.2097,
        "grad_norm": 2.21258544921875,
        "learning_rate": 0.0001607194382015269,
        "epoch": 0.43272677143644883,
        "step": 3139
    },
    {
        "loss": 1.7273,
        "grad_norm": 1.3256826400756836,
        "learning_rate": 0.0001606157970758348,
        "epoch": 0.4328646264130135,
        "step": 3140
    },
    {
        "loss": 1.8267,
        "grad_norm": 1.2690588235855103,
        "learning_rate": 0.00016051205291827704,
        "epoch": 0.43300248138957814,
        "step": 3141
    },
    {
        "loss": 2.2425,
        "grad_norm": 1.2132598161697388,
        "learning_rate": 0.000160408205905193,
        "epoch": 0.4331403363661428,
        "step": 3142
    },
    {
        "loss": 2.1096,
        "grad_norm": 1.406449317932129,
        "learning_rate": 0.00016030425621309683,
        "epoch": 0.43327819134270745,
        "step": 3143
    },
    {
        "loss": 1.9597,
        "grad_norm": 1.7853907346725464,
        "learning_rate": 0.00016020020401867749,
        "epoch": 0.4334160463192721,
        "step": 3144
    },
    {
        "loss": 2.3159,
        "grad_norm": 1.2165170907974243,
        "learning_rate": 0.00016009604949879778,
        "epoch": 0.43355390129583676,
        "step": 3145
    },
    {
        "loss": 1.5762,
        "grad_norm": 2.109701156616211,
        "learning_rate": 0.0001599917928304947,
        "epoch": 0.4336917562724014,
        "step": 3146
    },
    {
        "loss": 1.8171,
        "grad_norm": 1.8510494232177734,
        "learning_rate": 0.00015988743419097877,
        "epoch": 0.4338296112489661,
        "step": 3147
    },
    {
        "loss": 1.4716,
        "grad_norm": 1.4296462535858154,
        "learning_rate": 0.00015978297375763404,
        "epoch": 0.43396746622553073,
        "step": 3148
    },
    {
        "loss": 2.5483,
        "grad_norm": 1.5508015155792236,
        "learning_rate": 0.00015967841170801722,
        "epoch": 0.4341053212020954,
        "step": 3149
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.826747179031372,
        "learning_rate": 0.000159573748219858,
        "epoch": 0.43424317617866004,
        "step": 3150
    },
    {
        "loss": 2.216,
        "grad_norm": 1.4003797769546509,
        "learning_rate": 0.00015946898347105846,
        "epoch": 0.4343810311552247,
        "step": 3151
    },
    {
        "loss": 1.5071,
        "grad_norm": 1.9658538103103638,
        "learning_rate": 0.00015936411763969272,
        "epoch": 0.43451888613178935,
        "step": 3152
    },
    {
        "loss": 2.1898,
        "grad_norm": 1.598268747329712,
        "learning_rate": 0.0001592591509040068,
        "epoch": 0.434656741108354,
        "step": 3153
    },
    {
        "loss": 1.2456,
        "grad_norm": 2.143094539642334,
        "learning_rate": 0.00015915408344241815,
        "epoch": 0.43479459608491866,
        "step": 3154
    },
    {
        "loss": 1.5763,
        "grad_norm": 2.5408992767333984,
        "learning_rate": 0.00015904891543351552,
        "epoch": 0.4349324510614833,
        "step": 3155
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.266631841659546,
        "learning_rate": 0.0001589436470560585,
        "epoch": 0.43507030603804797,
        "step": 3156
    },
    {
        "loss": 1.6906,
        "grad_norm": 1.7111742496490479,
        "learning_rate": 0.0001588382784889774,
        "epoch": 0.4352081610146126,
        "step": 3157
    },
    {
        "loss": 2.0183,
        "grad_norm": 2.429448127746582,
        "learning_rate": 0.0001587328099113725,
        "epoch": 0.4353460159911773,
        "step": 3158
    },
    {
        "loss": 2.3747,
        "grad_norm": 1.655746579170227,
        "learning_rate": 0.00015862724150251444,
        "epoch": 0.43548387096774194,
        "step": 3159
    },
    {
        "loss": 2.6204,
        "grad_norm": 1.605476975440979,
        "learning_rate": 0.0001585215734418434,
        "epoch": 0.4356217259443066,
        "step": 3160
    },
    {
        "loss": 2.0712,
        "grad_norm": 1.7651994228363037,
        "learning_rate": 0.000158415805908969,
        "epoch": 0.43575958092087125,
        "step": 3161
    },
    {
        "loss": 2.7017,
        "grad_norm": 1.608953595161438,
        "learning_rate": 0.00015830993908366977,
        "epoch": 0.4358974358974359,
        "step": 3162
    },
    {
        "loss": 2.1961,
        "grad_norm": 1.9241280555725098,
        "learning_rate": 0.0001582039731458933,
        "epoch": 0.43603529087400056,
        "step": 3163
    },
    {
        "loss": 2.3257,
        "grad_norm": 1.3737547397613525,
        "learning_rate": 0.00015809790827575536,
        "epoch": 0.4361731458505652,
        "step": 3164
    },
    {
        "loss": 1.5347,
        "grad_norm": 1.9608834981918335,
        "learning_rate": 0.00015799174465354004,
        "epoch": 0.43631100082712987,
        "step": 3165
    },
    {
        "loss": 1.8242,
        "grad_norm": 2.3656368255615234,
        "learning_rate": 0.00015788548245969914,
        "epoch": 0.4364488558036945,
        "step": 3166
    },
    {
        "loss": 2.0025,
        "grad_norm": 1.583756685256958,
        "learning_rate": 0.00015777912187485234,
        "epoch": 0.4365867107802592,
        "step": 3167
    },
    {
        "loss": 2.1524,
        "grad_norm": 1.8929569721221924,
        "learning_rate": 0.00015767266307978615,
        "epoch": 0.43672456575682383,
        "step": 3168
    },
    {
        "loss": 1.7979,
        "grad_norm": 2.16072154045105,
        "learning_rate": 0.00015756610625545424,
        "epoch": 0.4368624207333885,
        "step": 3169
    },
    {
        "loss": 1.8288,
        "grad_norm": 1.8951727151870728,
        "learning_rate": 0.00015745945158297683,
        "epoch": 0.43700027570995315,
        "step": 3170
    },
    {
        "loss": 1.9232,
        "grad_norm": 1.4224963188171387,
        "learning_rate": 0.0001573526992436406,
        "epoch": 0.4371381306865178,
        "step": 3171
    },
    {
        "loss": 1.9848,
        "grad_norm": 1.7524583339691162,
        "learning_rate": 0.00015724584941889796,
        "epoch": 0.43727598566308246,
        "step": 3172
    },
    {
        "loss": 2.5364,
        "grad_norm": 1.071287751197815,
        "learning_rate": 0.00015713890229036737,
        "epoch": 0.4374138406396471,
        "step": 3173
    },
    {
        "loss": 2.3082,
        "grad_norm": 2.0821897983551025,
        "learning_rate": 0.00015703185803983238,
        "epoch": 0.43755169561621177,
        "step": 3174
    },
    {
        "loss": 2.6225,
        "grad_norm": 0.9941545128822327,
        "learning_rate": 0.0001569247168492418,
        "epoch": 0.4376895505927764,
        "step": 3175
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.2815744876861572,
        "learning_rate": 0.0001568174789007092,
        "epoch": 0.4378274055693411,
        "step": 3176
    },
    {
        "loss": 1.6721,
        "grad_norm": 2.04433274269104,
        "learning_rate": 0.00015671014437651244,
        "epoch": 0.43796526054590573,
        "step": 3177
    },
    {
        "loss": 1.8362,
        "grad_norm": 1.208389401435852,
        "learning_rate": 0.00015660271345909371,
        "epoch": 0.4381031155224704,
        "step": 3178
    },
    {
        "loss": 2.1763,
        "grad_norm": 1.1451252698898315,
        "learning_rate": 0.00015649518633105907,
        "epoch": 0.438240970499035,
        "step": 3179
    },
    {
        "loss": 1.858,
        "grad_norm": 1.5065903663635254,
        "learning_rate": 0.00015638756317517811,
        "epoch": 0.43837882547559964,
        "step": 3180
    },
    {
        "loss": 2.3121,
        "grad_norm": 1.2542204856872559,
        "learning_rate": 0.00015627984417438336,
        "epoch": 0.4385166804521643,
        "step": 3181
    },
    {
        "loss": 2.6801,
        "grad_norm": 1.6427333354949951,
        "learning_rate": 0.00015617202951177073,
        "epoch": 0.43865453542872895,
        "step": 3182
    },
    {
        "loss": 2.3754,
        "grad_norm": 1.327536702156067,
        "learning_rate": 0.00015606411937059831,
        "epoch": 0.4387923904052936,
        "step": 3183
    },
    {
        "loss": 2.2542,
        "grad_norm": 1.9582266807556152,
        "learning_rate": 0.00015595611393428674,
        "epoch": 0.43893024538185826,
        "step": 3184
    },
    {
        "loss": 1.6466,
        "grad_norm": 2.0797176361083984,
        "learning_rate": 0.00015584801338641843,
        "epoch": 0.4390681003584229,
        "step": 3185
    },
    {
        "loss": 2.083,
        "grad_norm": 1.9076822996139526,
        "learning_rate": 0.00015573981791073782,
        "epoch": 0.4392059553349876,
        "step": 3186
    },
    {
        "loss": 1.2017,
        "grad_norm": 1.7176216840744019,
        "learning_rate": 0.00015563152769115026,
        "epoch": 0.43934381031155223,
        "step": 3187
    },
    {
        "loss": 2.4125,
        "grad_norm": 1.236620306968689,
        "learning_rate": 0.0001555231429117224,
        "epoch": 0.4394816652881169,
        "step": 3188
    },
    {
        "loss": 2.4807,
        "grad_norm": 1.4971086978912354,
        "learning_rate": 0.00015541466375668158,
        "epoch": 0.43961952026468154,
        "step": 3189
    },
    {
        "loss": 1.5703,
        "grad_norm": 2.1090190410614014,
        "learning_rate": 0.00015530609041041553,
        "epoch": 0.4397573752412462,
        "step": 3190
    },
    {
        "loss": 2.3122,
        "grad_norm": 2.11566162109375,
        "learning_rate": 0.00015519742305747207,
        "epoch": 0.43989523021781085,
        "step": 3191
    },
    {
        "loss": 2.0018,
        "grad_norm": 1.9188199043273926,
        "learning_rate": 0.00015508866188255888,
        "epoch": 0.4400330851943755,
        "step": 3192
    },
    {
        "loss": 2.0754,
        "grad_norm": 1.7457276582717896,
        "learning_rate": 0.000154979807070543,
        "epoch": 0.44017094017094016,
        "step": 3193
    },
    {
        "loss": 2.1873,
        "grad_norm": 2.0266408920288086,
        "learning_rate": 0.00015487085880645074,
        "epoch": 0.4403087951475048,
        "step": 3194
    },
    {
        "loss": 1.122,
        "grad_norm": 1.7328721284866333,
        "learning_rate": 0.00015476181727546722,
        "epoch": 0.44044665012406947,
        "step": 3195
    },
    {
        "loss": 1.7497,
        "grad_norm": 1.83481764793396,
        "learning_rate": 0.00015465268266293597,
        "epoch": 0.4405845051006341,
        "step": 3196
    },
    {
        "loss": 1.1257,
        "grad_norm": 2.1034467220306396,
        "learning_rate": 0.000154543455154359,
        "epoch": 0.4407223600771988,
        "step": 3197
    },
    {
        "loss": 1.0576,
        "grad_norm": 2.232529640197754,
        "learning_rate": 0.00015443413493539592,
        "epoch": 0.44086021505376344,
        "step": 3198
    },
    {
        "loss": 2.0475,
        "grad_norm": 1.728676199913025,
        "learning_rate": 0.00015432472219186423,
        "epoch": 0.4409980700303281,
        "step": 3199
    },
    {
        "loss": 1.6514,
        "grad_norm": 1.999693512916565,
        "learning_rate": 0.0001542152171097383,
        "epoch": 0.44113592500689275,
        "step": 3200
    },
    {
        "loss": 1.6395,
        "grad_norm": 2.2689361572265625,
        "learning_rate": 0.00015410561987515,
        "epoch": 0.4412737799834574,
        "step": 3201
    },
    {
        "loss": 1.4995,
        "grad_norm": 2.122035264968872,
        "learning_rate": 0.00015399593067438728,
        "epoch": 0.44141163496002206,
        "step": 3202
    },
    {
        "loss": 2.056,
        "grad_norm": 2.143366575241089,
        "learning_rate": 0.00015388614969389473,
        "epoch": 0.4415494899365867,
        "step": 3203
    },
    {
        "loss": 1.601,
        "grad_norm": 2.3088488578796387,
        "learning_rate": 0.00015377627712027283,
        "epoch": 0.44168734491315137,
        "step": 3204
    },
    {
        "loss": 2.0142,
        "grad_norm": 1.2071645259857178,
        "learning_rate": 0.00015366631314027792,
        "epoch": 0.441825199889716,
        "step": 3205
    },
    {
        "loss": 2.2053,
        "grad_norm": 1.237612247467041,
        "learning_rate": 0.00015355625794082148,
        "epoch": 0.4419630548662807,
        "step": 3206
    },
    {
        "loss": 1.9758,
        "grad_norm": 1.3165063858032227,
        "learning_rate": 0.00015344611170897015,
        "epoch": 0.44210090984284534,
        "step": 3207
    },
    {
        "loss": 1.1242,
        "grad_norm": 2.7443251609802246,
        "learning_rate": 0.00015333587463194523,
        "epoch": 0.44223876481941,
        "step": 3208
    },
    {
        "loss": 2.2006,
        "grad_norm": 1.647917628288269,
        "learning_rate": 0.00015322554689712253,
        "epoch": 0.44237661979597465,
        "step": 3209
    },
    {
        "loss": 2.4734,
        "grad_norm": 1.7360997200012207,
        "learning_rate": 0.000153115128692032,
        "epoch": 0.4425144747725393,
        "step": 3210
    },
    {
        "loss": 1.9625,
        "grad_norm": 1.487183928489685,
        "learning_rate": 0.00015300462020435712,
        "epoch": 0.44265232974910396,
        "step": 3211
    },
    {
        "loss": 2.1011,
        "grad_norm": 1.793243408203125,
        "learning_rate": 0.00015289402162193514,
        "epoch": 0.4427901847256686,
        "step": 3212
    },
    {
        "loss": 1.7608,
        "grad_norm": 1.852364420890808,
        "learning_rate": 0.00015278333313275625,
        "epoch": 0.44292803970223327,
        "step": 3213
    },
    {
        "loss": 1.9187,
        "grad_norm": 1.8938279151916504,
        "learning_rate": 0.00015267255492496356,
        "epoch": 0.4430658946787979,
        "step": 3214
    },
    {
        "loss": 2.2791,
        "grad_norm": 1.8013741970062256,
        "learning_rate": 0.00015256168718685243,
        "epoch": 0.4432037496553626,
        "step": 3215
    },
    {
        "loss": 2.0566,
        "grad_norm": 2.115168571472168,
        "learning_rate": 0.00015245073010687081,
        "epoch": 0.44334160463192723,
        "step": 3216
    },
    {
        "loss": 2.1013,
        "grad_norm": 2.812882900238037,
        "learning_rate": 0.00015233968387361822,
        "epoch": 0.4434794596084919,
        "step": 3217
    },
    {
        "loss": 1.7156,
        "grad_norm": 2.4963059425354004,
        "learning_rate": 0.00015222854867584588,
        "epoch": 0.44361731458505654,
        "step": 3218
    },
    {
        "loss": 1.9679,
        "grad_norm": 1.3015295267105103,
        "learning_rate": 0.00015211732470245596,
        "epoch": 0.4437551695616212,
        "step": 3219
    },
    {
        "loss": 1.7215,
        "grad_norm": 1.5380654335021973,
        "learning_rate": 0.00015200601214250192,
        "epoch": 0.44389302453818585,
        "step": 3220
    },
    {
        "loss": 1.3451,
        "grad_norm": 2.0260143280029297,
        "learning_rate": 0.00015189461118518747,
        "epoch": 0.4440308795147505,
        "step": 3221
    },
    {
        "loss": 2.6066,
        "grad_norm": 1.302628993988037,
        "learning_rate": 0.00015178312201986666,
        "epoch": 0.4441687344913151,
        "step": 3222
    },
    {
        "loss": 2.3932,
        "grad_norm": 1.469002366065979,
        "learning_rate": 0.0001516715448360435,
        "epoch": 0.44430658946787976,
        "step": 3223
    },
    {
        "loss": 2.4376,
        "grad_norm": 2.29591703414917,
        "learning_rate": 0.00015155987982337182,
        "epoch": 0.4444444444444444,
        "step": 3224
    },
    {
        "loss": 1.353,
        "grad_norm": 1.795682430267334,
        "learning_rate": 0.00015144812717165427,
        "epoch": 0.4445822994210091,
        "step": 3225
    },
    {
        "loss": 1.948,
        "grad_norm": 1.7597618103027344,
        "learning_rate": 0.0001513362870708428,
        "epoch": 0.44472015439757373,
        "step": 3226
    },
    {
        "loss": 2.2196,
        "grad_norm": 1.9853227138519287,
        "learning_rate": 0.000151224359711038,
        "epoch": 0.4448580093741384,
        "step": 3227
    },
    {
        "loss": 2.2151,
        "grad_norm": 2.187504291534424,
        "learning_rate": 0.00015111234528248862,
        "epoch": 0.44499586435070304,
        "step": 3228
    },
    {
        "loss": 2.3383,
        "grad_norm": 1.334225058555603,
        "learning_rate": 0.00015100024397559153,
        "epoch": 0.4451337193272677,
        "step": 3229
    },
    {
        "loss": 1.823,
        "grad_norm": 2.0076217651367188,
        "learning_rate": 0.0001508880559808912,
        "epoch": 0.44527157430383235,
        "step": 3230
    },
    {
        "loss": 1.7332,
        "grad_norm": 1.9104158878326416,
        "learning_rate": 0.00015077578148907955,
        "epoch": 0.445409429280397,
        "step": 3231
    },
    {
        "loss": 2.129,
        "grad_norm": 2.496540069580078,
        "learning_rate": 0.00015066342069099538,
        "epoch": 0.44554728425696166,
        "step": 3232
    },
    {
        "loss": 2.2744,
        "grad_norm": 1.5802412033081055,
        "learning_rate": 0.00015055097377762432,
        "epoch": 0.4456851392335263,
        "step": 3233
    },
    {
        "loss": 1.716,
        "grad_norm": 1.6525990962982178,
        "learning_rate": 0.0001504384409400982,
        "epoch": 0.445822994210091,
        "step": 3234
    },
    {
        "loss": 1.8856,
        "grad_norm": 2.1610002517700195,
        "learning_rate": 0.00015032582236969514,
        "epoch": 0.44596084918665563,
        "step": 3235
    },
    {
        "loss": 2.1986,
        "grad_norm": 2.0975341796875,
        "learning_rate": 0.00015021311825783883,
        "epoch": 0.4460987041632203,
        "step": 3236
    },
    {
        "loss": 1.8322,
        "grad_norm": 1.9409915208816528,
        "learning_rate": 0.0001501003287960984,
        "epoch": 0.44623655913978494,
        "step": 3237
    },
    {
        "loss": 2.2201,
        "grad_norm": 1.7848156690597534,
        "learning_rate": 0.00014998745417618795,
        "epoch": 0.4463744141163496,
        "step": 3238
    },
    {
        "loss": 1.7416,
        "grad_norm": 1.4493072032928467,
        "learning_rate": 0.0001498744945899666,
        "epoch": 0.44651226909291425,
        "step": 3239
    },
    {
        "loss": 1.1306,
        "grad_norm": 2.0462567806243896,
        "learning_rate": 0.00014976145022943752,
        "epoch": 0.4466501240694789,
        "step": 3240
    },
    {
        "loss": 1.8335,
        "grad_norm": 2.637507915496826,
        "learning_rate": 0.0001496483212867483,
        "epoch": 0.44678797904604356,
        "step": 3241
    },
    {
        "loss": 1.9894,
        "grad_norm": 2.1963977813720703,
        "learning_rate": 0.00014953510795419003,
        "epoch": 0.4469258340226082,
        "step": 3242
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.745017170906067,
        "learning_rate": 0.00014942181042419762,
        "epoch": 0.44706368899917287,
        "step": 3243
    },
    {
        "loss": 1.6545,
        "grad_norm": 2.3167059421539307,
        "learning_rate": 0.0001493084288893486,
        "epoch": 0.4472015439757375,
        "step": 3244
    },
    {
        "loss": 2.1253,
        "grad_norm": 1.5638952255249023,
        "learning_rate": 0.00014919496354236372,
        "epoch": 0.4473393989523022,
        "step": 3245
    },
    {
        "loss": 1.9676,
        "grad_norm": 2.262678861618042,
        "learning_rate": 0.00014908141457610586,
        "epoch": 0.44747725392886684,
        "step": 3246
    },
    {
        "loss": 1.8828,
        "grad_norm": 1.3322969675064087,
        "learning_rate": 0.00014896778218358031,
        "epoch": 0.4476151089054315,
        "step": 3247
    },
    {
        "loss": 1.3999,
        "grad_norm": 2.246523141860962,
        "learning_rate": 0.00014885406655793402,
        "epoch": 0.44775296388199615,
        "step": 3248
    },
    {
        "loss": 2.0032,
        "grad_norm": 1.459045171737671,
        "learning_rate": 0.00014874026789245536,
        "epoch": 0.4478908188585608,
        "step": 3249
    },
    {
        "loss": 1.8754,
        "grad_norm": 1.6030346155166626,
        "learning_rate": 0.000148626386380574,
        "epoch": 0.44802867383512546,
        "step": 3250
    },
    {
        "loss": 2.1146,
        "grad_norm": 1.2374030351638794,
        "learning_rate": 0.0001485124222158603,
        "epoch": 0.4481665288116901,
        "step": 3251
    },
    {
        "loss": 2.2861,
        "grad_norm": 2.27584171295166,
        "learning_rate": 0.00014839837559202523,
        "epoch": 0.44830438378825477,
        "step": 3252
    },
    {
        "loss": 2.0359,
        "grad_norm": 1.4829634428024292,
        "learning_rate": 0.0001482842467029197,
        "epoch": 0.4484422387648194,
        "step": 3253
    },
    {
        "loss": 2.3888,
        "grad_norm": 2.07157301902771,
        "learning_rate": 0.00014817003574253473,
        "epoch": 0.4485800937413841,
        "step": 3254
    },
    {
        "loss": 2.6501,
        "grad_norm": 1.5054905414581299,
        "learning_rate": 0.00014805574290500064,
        "epoch": 0.44871794871794873,
        "step": 3255
    },
    {
        "loss": 2.1073,
        "grad_norm": 2.8251147270202637,
        "learning_rate": 0.00014794136838458714,
        "epoch": 0.4488558036945134,
        "step": 3256
    },
    {
        "loss": 2.3676,
        "grad_norm": 2.309002161026001,
        "learning_rate": 0.00014782691237570238,
        "epoch": 0.44899365867107804,
        "step": 3257
    },
    {
        "loss": 1.8163,
        "grad_norm": 1.6919981241226196,
        "learning_rate": 0.00014771237507289352,
        "epoch": 0.4491315136476427,
        "step": 3258
    },
    {
        "loss": 2.4067,
        "grad_norm": 1.145639181137085,
        "learning_rate": 0.00014759775667084546,
        "epoch": 0.44926936862420735,
        "step": 3259
    },
    {
        "loss": 2.4313,
        "grad_norm": 1.5618170499801636,
        "learning_rate": 0.00014748305736438127,
        "epoch": 0.449407223600772,
        "step": 3260
    },
    {
        "loss": 2.3406,
        "grad_norm": 1.4777333736419678,
        "learning_rate": 0.00014736827734846127,
        "epoch": 0.44954507857733667,
        "step": 3261
    },
    {
        "loss": 1.9959,
        "grad_norm": 1.4577165842056274,
        "learning_rate": 0.0001472534168181833,
        "epoch": 0.4496829335539013,
        "step": 3262
    },
    {
        "loss": 1.7317,
        "grad_norm": 1.4569222927093506,
        "learning_rate": 0.0001471384759687817,
        "epoch": 0.449820788530466,
        "step": 3263
    },
    {
        "loss": 2.1458,
        "grad_norm": 2.0642075538635254,
        "learning_rate": 0.00014702345499562752,
        "epoch": 0.44995864350703063,
        "step": 3264
    },
    {
        "loss": 1.9753,
        "grad_norm": 1.9605857133865356,
        "learning_rate": 0.00014690835409422802,
        "epoch": 0.45009649848359523,
        "step": 3265
    },
    {
        "loss": 2.28,
        "grad_norm": 1.4621144533157349,
        "learning_rate": 0.00014679317346022626,
        "epoch": 0.4502343534601599,
        "step": 3266
    },
    {
        "loss": 2.3647,
        "grad_norm": 1.2874191999435425,
        "learning_rate": 0.00014667791328940076,
        "epoch": 0.45037220843672454,
        "step": 3267
    },
    {
        "loss": 2.145,
        "grad_norm": 1.6445146799087524,
        "learning_rate": 0.0001465625737776654,
        "epoch": 0.4505100634132892,
        "step": 3268
    },
    {
        "loss": 2.3381,
        "grad_norm": 1.6173018217086792,
        "learning_rate": 0.00014644715512106878,
        "epoch": 0.45064791838985385,
        "step": 3269
    },
    {
        "loss": 1.9973,
        "grad_norm": 1.8436057567596436,
        "learning_rate": 0.00014633165751579412,
        "epoch": 0.4507857733664185,
        "step": 3270
    },
    {
        "loss": 1.7715,
        "grad_norm": 2.2956089973449707,
        "learning_rate": 0.00014621608115815876,
        "epoch": 0.45092362834298316,
        "step": 3271
    },
    {
        "loss": 2.2394,
        "grad_norm": 1.1992920637130737,
        "learning_rate": 0.00014610042624461385,
        "epoch": 0.4510614833195478,
        "step": 3272
    },
    {
        "loss": 2.3642,
        "grad_norm": 1.8776323795318604,
        "learning_rate": 0.0001459846929717443,
        "epoch": 0.4511993382961125,
        "step": 3273
    },
    {
        "loss": 2.201,
        "grad_norm": 1.9607971906661987,
        "learning_rate": 0.00014586888153626793,
        "epoch": 0.45133719327267713,
        "step": 3274
    },
    {
        "loss": 1.7942,
        "grad_norm": 2.4265120029449463,
        "learning_rate": 0.00014575299213503564,
        "epoch": 0.4514750482492418,
        "step": 3275
    },
    {
        "loss": 2.1473,
        "grad_norm": 1.54909348487854,
        "learning_rate": 0.0001456370249650306,
        "epoch": 0.45161290322580644,
        "step": 3276
    },
    {
        "loss": 1.8585,
        "grad_norm": 1.6814640760421753,
        "learning_rate": 0.00014552098022336854,
        "epoch": 0.4517507582023711,
        "step": 3277
    },
    {
        "loss": 2.2534,
        "grad_norm": 1.3904428482055664,
        "learning_rate": 0.00014540485810729664,
        "epoch": 0.45188861317893575,
        "step": 3278
    },
    {
        "loss": 1.7487,
        "grad_norm": 1.4570410251617432,
        "learning_rate": 0.00014528865881419385,
        "epoch": 0.4520264681555004,
        "step": 3279
    },
    {
        "loss": 2.5907,
        "grad_norm": 1.135810136795044,
        "learning_rate": 0.0001451723825415702,
        "epoch": 0.45216432313206506,
        "step": 3280
    },
    {
        "loss": 1.6464,
        "grad_norm": 2.2319347858428955,
        "learning_rate": 0.0001450560294870667,
        "epoch": 0.4523021781086297,
        "step": 3281
    },
    {
        "loss": 2.0474,
        "grad_norm": 2.1380701065063477,
        "learning_rate": 0.00014493959984845467,
        "epoch": 0.45244003308519437,
        "step": 3282
    },
    {
        "loss": 1.2646,
        "grad_norm": 1.865609884262085,
        "learning_rate": 0.00014482309382363574,
        "epoch": 0.452577888061759,
        "step": 3283
    },
    {
        "loss": 2.4096,
        "grad_norm": 1.4419286251068115,
        "learning_rate": 0.00014470651161064134,
        "epoch": 0.4527157430383237,
        "step": 3284
    },
    {
        "loss": 2.0567,
        "grad_norm": 1.316818118095398,
        "learning_rate": 0.0001445898534076324,
        "epoch": 0.45285359801488834,
        "step": 3285
    },
    {
        "loss": 1.1688,
        "grad_norm": 2.034358263015747,
        "learning_rate": 0.00014447311941289903,
        "epoch": 0.452991452991453,
        "step": 3286
    },
    {
        "loss": 2.0397,
        "grad_norm": 1.3929811716079712,
        "learning_rate": 0.00014435630982486016,
        "epoch": 0.45312930796801765,
        "step": 3287
    },
    {
        "loss": 0.8239,
        "grad_norm": 1.9819220304489136,
        "learning_rate": 0.00014423942484206314,
        "epoch": 0.4532671629445823,
        "step": 3288
    },
    {
        "loss": 1.9145,
        "grad_norm": 1.4459176063537598,
        "learning_rate": 0.0001441224646631836,
        "epoch": 0.45340501792114696,
        "step": 3289
    },
    {
        "loss": 2.1916,
        "grad_norm": 1.6613613367080688,
        "learning_rate": 0.00014400542948702502,
        "epoch": 0.4535428728977116,
        "step": 3290
    },
    {
        "loss": 2.3925,
        "grad_norm": 1.8575211763381958,
        "learning_rate": 0.00014388831951251798,
        "epoch": 0.45368072787427627,
        "step": 3291
    },
    {
        "loss": 1.7396,
        "grad_norm": 1.7707551717758179,
        "learning_rate": 0.0001437711349387207,
        "epoch": 0.4538185828508409,
        "step": 3292
    },
    {
        "loss": 1.7986,
        "grad_norm": 2.3689823150634766,
        "learning_rate": 0.00014365387596481798,
        "epoch": 0.4539564378274056,
        "step": 3293
    },
    {
        "loss": 2.2083,
        "grad_norm": 1.777787208557129,
        "learning_rate": 0.00014353654279012103,
        "epoch": 0.45409429280397023,
        "step": 3294
    },
    {
        "loss": 1.7867,
        "grad_norm": 1.660422682762146,
        "learning_rate": 0.0001434191356140671,
        "epoch": 0.4542321477805349,
        "step": 3295
    },
    {
        "loss": 1.9098,
        "grad_norm": 2.1946444511413574,
        "learning_rate": 0.0001433016546362196,
        "epoch": 0.45437000275709954,
        "step": 3296
    },
    {
        "loss": 2.2178,
        "grad_norm": 1.196535348892212,
        "learning_rate": 0.00014318410005626699,
        "epoch": 0.4545078577336642,
        "step": 3297
    },
    {
        "loss": 2.1593,
        "grad_norm": 1.163240909576416,
        "learning_rate": 0.00014306647207402298,
        "epoch": 0.45464571271022886,
        "step": 3298
    },
    {
        "loss": 1.6203,
        "grad_norm": 2.120011568069458,
        "learning_rate": 0.00014294877088942605,
        "epoch": 0.4547835676867935,
        "step": 3299
    },
    {
        "loss": 1.6036,
        "grad_norm": 2.2633700370788574,
        "learning_rate": 0.00014283099670253928,
        "epoch": 0.45492142266335817,
        "step": 3300
    },
    {
        "loss": 2.1076,
        "grad_norm": 1.3462979793548584,
        "learning_rate": 0.0001427131497135495,
        "epoch": 0.4550592776399228,
        "step": 3301
    },
    {
        "loss": 1.8415,
        "grad_norm": 1.8823022842407227,
        "learning_rate": 0.00014259523012276744,
        "epoch": 0.4551971326164875,
        "step": 3302
    },
    {
        "loss": 1.9821,
        "grad_norm": 2.08587384223938,
        "learning_rate": 0.0001424772381306273,
        "epoch": 0.45533498759305213,
        "step": 3303
    },
    {
        "loss": 1.615,
        "grad_norm": 2.2052977085113525,
        "learning_rate": 0.00014235917393768625,
        "epoch": 0.4554728425696168,
        "step": 3304
    },
    {
        "loss": 1.987,
        "grad_norm": 1.3774399757385254,
        "learning_rate": 0.00014224103774462428,
        "epoch": 0.45561069754618144,
        "step": 3305
    },
    {
        "loss": 1.3295,
        "grad_norm": 2.310730457305908,
        "learning_rate": 0.00014212282975224364,
        "epoch": 0.4557485525227461,
        "step": 3306
    },
    {
        "loss": 1.9233,
        "grad_norm": 2.2853713035583496,
        "learning_rate": 0.00014200455016146875,
        "epoch": 0.4558864074993107,
        "step": 3307
    },
    {
        "loss": 1.1518,
        "grad_norm": 2.8368446826934814,
        "learning_rate": 0.00014188619917334563,
        "epoch": 0.45602426247587535,
        "step": 3308
    },
    {
        "loss": 2.0998,
        "grad_norm": 1.7139040231704712,
        "learning_rate": 0.00014176777698904176,
        "epoch": 0.45616211745244,
        "step": 3309
    },
    {
        "loss": 1.7607,
        "grad_norm": 2.0080478191375732,
        "learning_rate": 0.00014164928380984537,
        "epoch": 0.45629997242900466,
        "step": 3310
    },
    {
        "loss": 2.5927,
        "grad_norm": 1.9693868160247803,
        "learning_rate": 0.00014153071983716577,
        "epoch": 0.4564378274055693,
        "step": 3311
    },
    {
        "loss": 2.5819,
        "grad_norm": 1.8873271942138672,
        "learning_rate": 0.00014141208527253233,
        "epoch": 0.456575682382134,
        "step": 3312
    },
    {
        "loss": 2.1554,
        "grad_norm": 1.8269453048706055,
        "learning_rate": 0.00014129338031759452,
        "epoch": 0.45671353735869863,
        "step": 3313
    },
    {
        "loss": 2.2909,
        "grad_norm": 1.5286188125610352,
        "learning_rate": 0.00014117460517412125,
        "epoch": 0.4568513923352633,
        "step": 3314
    },
    {
        "loss": 2.0663,
        "grad_norm": 1.8261444568634033,
        "learning_rate": 0.00014105576004400114,
        "epoch": 0.45698924731182794,
        "step": 3315
    },
    {
        "loss": 2.0196,
        "grad_norm": 1.5665677785873413,
        "learning_rate": 0.0001409368451292413,
        "epoch": 0.4571271022883926,
        "step": 3316
    },
    {
        "loss": 1.7392,
        "grad_norm": 1.6960293054580688,
        "learning_rate": 0.00014081786063196777,
        "epoch": 0.45726495726495725,
        "step": 3317
    },
    {
        "loss": 1.6259,
        "grad_norm": 2.226555585861206,
        "learning_rate": 0.00014069880675442469,
        "epoch": 0.4574028122415219,
        "step": 3318
    },
    {
        "loss": 2.0649,
        "grad_norm": 2.2566285133361816,
        "learning_rate": 0.00014057968369897444,
        "epoch": 0.45754066721808656,
        "step": 3319
    },
    {
        "loss": 1.7599,
        "grad_norm": 2.0785579681396484,
        "learning_rate": 0.00014046049166809653,
        "epoch": 0.4576785221946512,
        "step": 3320
    },
    {
        "loss": 1.5542,
        "grad_norm": 2.0649003982543945,
        "learning_rate": 0.00014034123086438798,
        "epoch": 0.45781637717121587,
        "step": 3321
    },
    {
        "loss": 2.1735,
        "grad_norm": 1.220574140548706,
        "learning_rate": 0.00014022190149056275,
        "epoch": 0.4579542321477805,
        "step": 3322
    },
    {
        "loss": 2.5752,
        "grad_norm": 2.2864978313446045,
        "learning_rate": 0.00014010250374945116,
        "epoch": 0.4580920871243452,
        "step": 3323
    },
    {
        "loss": 1.4155,
        "grad_norm": 2.272890329360962,
        "learning_rate": 0.00013998303784399995,
        "epoch": 0.45822994210090984,
        "step": 3324
    },
    {
        "loss": 2.4187,
        "grad_norm": 1.6818302869796753,
        "learning_rate": 0.00013986350397727156,
        "epoch": 0.4583677970774745,
        "step": 3325
    },
    {
        "loss": 2.1246,
        "grad_norm": 1.477418065071106,
        "learning_rate": 0.00013974390235244402,
        "epoch": 0.45850565205403915,
        "step": 3326
    },
    {
        "loss": 1.5912,
        "grad_norm": 2.135080575942993,
        "learning_rate": 0.0001396242331728105,
        "epoch": 0.4586435070306038,
        "step": 3327
    },
    {
        "loss": 2.5206,
        "grad_norm": 1.219582438468933,
        "learning_rate": 0.00013950449664177906,
        "epoch": 0.45878136200716846,
        "step": 3328
    },
    {
        "loss": 2.1632,
        "grad_norm": 1.5562950372695923,
        "learning_rate": 0.00013938469296287205,
        "epoch": 0.4589192169837331,
        "step": 3329
    },
    {
        "loss": 1.6814,
        "grad_norm": 1.5189496278762817,
        "learning_rate": 0.00013926482233972626,
        "epoch": 0.45905707196029777,
        "step": 3330
    },
    {
        "loss": 2.0157,
        "grad_norm": 1.9259867668151855,
        "learning_rate": 0.00013914488497609204,
        "epoch": 0.4591949269368624,
        "step": 3331
    },
    {
        "loss": 2.1493,
        "grad_norm": 1.4785856008529663,
        "learning_rate": 0.00013902488107583335,
        "epoch": 0.4593327819134271,
        "step": 3332
    },
    {
        "loss": 2.368,
        "grad_norm": 1.3563361167907715,
        "learning_rate": 0.00013890481084292692,
        "epoch": 0.45947063688999173,
        "step": 3333
    },
    {
        "loss": 2.0497,
        "grad_norm": 1.9840797185897827,
        "learning_rate": 0.00013878467448146274,
        "epoch": 0.4596084918665564,
        "step": 3334
    },
    {
        "loss": 2.184,
        "grad_norm": 1.3822418451309204,
        "learning_rate": 0.00013866447219564268,
        "epoch": 0.45974634684312105,
        "step": 3335
    },
    {
        "loss": 2.0264,
        "grad_norm": 1.3750317096710205,
        "learning_rate": 0.000138544204189781,
        "epoch": 0.4598842018196857,
        "step": 3336
    },
    {
        "loss": 1.7294,
        "grad_norm": 2.3448657989501953,
        "learning_rate": 0.00013842387066830347,
        "epoch": 0.46002205679625036,
        "step": 3337
    },
    {
        "loss": 2.2365,
        "grad_norm": 1.3084992170333862,
        "learning_rate": 0.00013830347183574755,
        "epoch": 0.460159911772815,
        "step": 3338
    },
    {
        "loss": 1.6579,
        "grad_norm": 1.9744943380355835,
        "learning_rate": 0.0001381830078967613,
        "epoch": 0.46029776674937967,
        "step": 3339
    },
    {
        "loss": 1.2952,
        "grad_norm": 3.204026460647583,
        "learning_rate": 0.00013806247905610364,
        "epoch": 0.4604356217259443,
        "step": 3340
    },
    {
        "loss": 1.7862,
        "grad_norm": 1.624570369720459,
        "learning_rate": 0.00013794188551864383,
        "epoch": 0.460573476702509,
        "step": 3341
    },
    {
        "loss": 1.3166,
        "grad_norm": 1.6862128973007202,
        "learning_rate": 0.00013782122748936106,
        "epoch": 0.46071133167907363,
        "step": 3342
    },
    {
        "loss": 2.1539,
        "grad_norm": 1.315291166305542,
        "learning_rate": 0.00013770050517334417,
        "epoch": 0.4608491866556383,
        "step": 3343
    },
    {
        "loss": 2.5669,
        "grad_norm": 1.9715662002563477,
        "learning_rate": 0.00013757971877579117,
        "epoch": 0.46098704163220294,
        "step": 3344
    },
    {
        "loss": 1.9154,
        "grad_norm": 1.659140944480896,
        "learning_rate": 0.0001374588685020091,
        "epoch": 0.4611248966087676,
        "step": 3345
    },
    {
        "loss": 2.2591,
        "grad_norm": 2.3314108848571777,
        "learning_rate": 0.0001373379545574136,
        "epoch": 0.46126275158533225,
        "step": 3346
    },
    {
        "loss": 2.0779,
        "grad_norm": 1.0611683130264282,
        "learning_rate": 0.00013721697714752848,
        "epoch": 0.4614006065618969,
        "step": 3347
    },
    {
        "loss": 1.9292,
        "grad_norm": 2.7402114868164062,
        "learning_rate": 0.00013709593647798522,
        "epoch": 0.46153846153846156,
        "step": 3348
    },
    {
        "loss": 2.0821,
        "grad_norm": 1.708271861076355,
        "learning_rate": 0.00013697483275452326,
        "epoch": 0.4616763165150262,
        "step": 3349
    },
    {
        "loss": 2.7989,
        "grad_norm": 1.2458090782165527,
        "learning_rate": 0.00013685366618298889,
        "epoch": 0.4618141714915908,
        "step": 3350
    },
    {
        "loss": 2.11,
        "grad_norm": 2.7351346015930176,
        "learning_rate": 0.0001367324369693354,
        "epoch": 0.4619520264681555,
        "step": 3351
    },
    {
        "loss": 2.262,
        "grad_norm": 1.4574686288833618,
        "learning_rate": 0.00013661114531962225,
        "epoch": 0.46208988144472013,
        "step": 3352
    },
    {
        "loss": 2.1193,
        "grad_norm": 2.256226062774658,
        "learning_rate": 0.00013648979144001558,
        "epoch": 0.4622277364212848,
        "step": 3353
    },
    {
        "loss": 1.6547,
        "grad_norm": 1.6869646310806274,
        "learning_rate": 0.0001363683755367867,
        "epoch": 0.46236559139784944,
        "step": 3354
    },
    {
        "loss": 2.4556,
        "grad_norm": 1.2928297519683838,
        "learning_rate": 0.00013624689781631272,
        "epoch": 0.4625034463744141,
        "step": 3355
    },
    {
        "loss": 2.9077,
        "grad_norm": 1.3885257244110107,
        "learning_rate": 0.00013612535848507565,
        "epoch": 0.46264130135097875,
        "step": 3356
    },
    {
        "loss": 2.2203,
        "grad_norm": 2.1940369606018066,
        "learning_rate": 0.00013600375774966257,
        "epoch": 0.4627791563275434,
        "step": 3357
    },
    {
        "loss": 1.0128,
        "grad_norm": 1.808059573173523,
        "learning_rate": 0.00013588209581676434,
        "epoch": 0.46291701130410806,
        "step": 3358
    },
    {
        "loss": 1.3592,
        "grad_norm": 2.3541505336761475,
        "learning_rate": 0.00013576037289317629,
        "epoch": 0.4630548662806727,
        "step": 3359
    },
    {
        "loss": 2.4387,
        "grad_norm": 1.5989346504211426,
        "learning_rate": 0.00013563858918579734,
        "epoch": 0.46319272125723737,
        "step": 3360
    },
    {
        "loss": 1.9792,
        "grad_norm": 1.6084843873977661,
        "learning_rate": 0.00013551674490162955,
        "epoch": 0.463330576233802,
        "step": 3361
    },
    {
        "loss": 2.4188,
        "grad_norm": 1.8278088569641113,
        "learning_rate": 0.0001353948402477782,
        "epoch": 0.4634684312103667,
        "step": 3362
    },
    {
        "loss": 1.473,
        "grad_norm": 2.522477388381958,
        "learning_rate": 0.000135272875431451,
        "epoch": 0.46360628618693134,
        "step": 3363
    },
    {
        "loss": 1.9978,
        "grad_norm": 2.594355583190918,
        "learning_rate": 0.00013515085065995795,
        "epoch": 0.463744141163496,
        "step": 3364
    },
    {
        "loss": 1.9284,
        "grad_norm": 2.5562548637390137,
        "learning_rate": 0.00013502876614071102,
        "epoch": 0.46388199614006065,
        "step": 3365
    },
    {
        "loss": 2.3883,
        "grad_norm": 1.094279408454895,
        "learning_rate": 0.0001349066220812238,
        "epoch": 0.4640198511166253,
        "step": 3366
    },
    {
        "loss": 2.2412,
        "grad_norm": 1.113632321357727,
        "learning_rate": 0.00013478441868911074,
        "epoch": 0.46415770609318996,
        "step": 3367
    },
    {
        "loss": 1.7401,
        "grad_norm": 2.428553342819214,
        "learning_rate": 0.0001346621561720876,
        "epoch": 0.4642955610697546,
        "step": 3368
    },
    {
        "loss": 1.8587,
        "grad_norm": 2.099493980407715,
        "learning_rate": 0.00013453983473797035,
        "epoch": 0.46443341604631927,
        "step": 3369
    },
    {
        "loss": 1.4276,
        "grad_norm": 2.6673130989074707,
        "learning_rate": 0.0001344174545946753,
        "epoch": 0.4645712710228839,
        "step": 3370
    },
    {
        "loss": 2.1634,
        "grad_norm": 1.4998201131820679,
        "learning_rate": 0.00013429501595021813,
        "epoch": 0.4647091259994486,
        "step": 3371
    },
    {
        "loss": 2.1308,
        "grad_norm": 2.2644460201263428,
        "learning_rate": 0.00013417251901271464,
        "epoch": 0.46484698097601324,
        "step": 3372
    },
    {
        "loss": 1.6087,
        "grad_norm": 1.434261441230774,
        "learning_rate": 0.00013404996399037905,
        "epoch": 0.4649848359525779,
        "step": 3373
    },
    {
        "loss": 1.6868,
        "grad_norm": 2.285064697265625,
        "learning_rate": 0.0001339273510915247,
        "epoch": 0.46512269092914255,
        "step": 3374
    },
    {
        "loss": 2.2798,
        "grad_norm": 1.3640084266662598,
        "learning_rate": 0.0001338046805245631,
        "epoch": 0.4652605459057072,
        "step": 3375
    },
    {
        "loss": 1.7272,
        "grad_norm": 2.3451879024505615,
        "learning_rate": 0.00013368195249800405,
        "epoch": 0.46539840088227186,
        "step": 3376
    },
    {
        "loss": 1.6195,
        "grad_norm": 2.4660513401031494,
        "learning_rate": 0.00013355916722045477,
        "epoch": 0.4655362558588365,
        "step": 3377
    },
    {
        "loss": 2.3433,
        "grad_norm": 0.8897910714149475,
        "learning_rate": 0.00013343632490061982,
        "epoch": 0.46567411083540117,
        "step": 3378
    },
    {
        "loss": 2.0227,
        "grad_norm": 2.1136982440948486,
        "learning_rate": 0.0001333134257473008,
        "epoch": 0.4658119658119658,
        "step": 3379
    },
    {
        "loss": 2.532,
        "grad_norm": 1.3430827856063843,
        "learning_rate": 0.0001331904699693959,
        "epoch": 0.4659498207885305,
        "step": 3380
    },
    {
        "loss": 2.1284,
        "grad_norm": 1.6428310871124268,
        "learning_rate": 0.00013306745777589952,
        "epoch": 0.46608767576509513,
        "step": 3381
    },
    {
        "loss": 1.84,
        "grad_norm": 2.2301015853881836,
        "learning_rate": 0.00013294438937590196,
        "epoch": 0.4662255307416598,
        "step": 3382
    },
    {
        "loss": 1.637,
        "grad_norm": 2.0520262718200684,
        "learning_rate": 0.0001328212649785891,
        "epoch": 0.46636338571822444,
        "step": 3383
    },
    {
        "loss": 1.8713,
        "grad_norm": 1.4891644716262817,
        "learning_rate": 0.00013269808479324192,
        "epoch": 0.4665012406947891,
        "step": 3384
    },
    {
        "loss": 2.1446,
        "grad_norm": 2.2353622913360596,
        "learning_rate": 0.00013257484902923637,
        "epoch": 0.46663909567135375,
        "step": 3385
    },
    {
        "loss": 2.1366,
        "grad_norm": 1.8724827766418457,
        "learning_rate": 0.0001324515578960426,
        "epoch": 0.4667769506479184,
        "step": 3386
    },
    {
        "loss": 1.7226,
        "grad_norm": 1.9513649940490723,
        "learning_rate": 0.0001323282116032252,
        "epoch": 0.46691480562448306,
        "step": 3387
    },
    {
        "loss": 2.2979,
        "grad_norm": 2.158751964569092,
        "learning_rate": 0.00013220481036044235,
        "epoch": 0.4670526606010477,
        "step": 3388
    },
    {
        "loss": 2.1554,
        "grad_norm": 1.5118753910064697,
        "learning_rate": 0.00013208135437744564,
        "epoch": 0.4671905155776124,
        "step": 3389
    },
    {
        "loss": 1.5443,
        "grad_norm": 1.3418723344802856,
        "learning_rate": 0.0001319578438640796,
        "epoch": 0.46732837055417703,
        "step": 3390
    },
    {
        "loss": 1.7869,
        "grad_norm": 1.6823103427886963,
        "learning_rate": 0.00013183427903028176,
        "epoch": 0.4674662255307417,
        "step": 3391
    },
    {
        "loss": 2.2008,
        "grad_norm": 1.7429730892181396,
        "learning_rate": 0.0001317106600860816,
        "epoch": 0.46760408050730634,
        "step": 3392
    },
    {
        "loss": 1.9283,
        "grad_norm": 1.2024344205856323,
        "learning_rate": 0.00013158698724160086,
        "epoch": 0.46774193548387094,
        "step": 3393
    },
    {
        "loss": 2.19,
        "grad_norm": 1.7519923448562622,
        "learning_rate": 0.0001314632607070527,
        "epoch": 0.4678797904604356,
        "step": 3394
    },
    {
        "loss": 2.0894,
        "grad_norm": 1.8694007396697998,
        "learning_rate": 0.0001313394806927418,
        "epoch": 0.46801764543700025,
        "step": 3395
    },
    {
        "loss": 2.5844,
        "grad_norm": 1.014657735824585,
        "learning_rate": 0.0001312156474090634,
        "epoch": 0.4681555004135649,
        "step": 3396
    },
    {
        "loss": 1.9152,
        "grad_norm": 1.9287233352661133,
        "learning_rate": 0.00013109176106650353,
        "epoch": 0.46829335539012956,
        "step": 3397
    },
    {
        "loss": 1.4079,
        "grad_norm": 2.626476287841797,
        "learning_rate": 0.0001309678218756384,
        "epoch": 0.4684312103666942,
        "step": 3398
    },
    {
        "loss": 1.7737,
        "grad_norm": 2.3224940299987793,
        "learning_rate": 0.00013084383004713387,
        "epoch": 0.4685690653432589,
        "step": 3399
    },
    {
        "loss": 1.8664,
        "grad_norm": 2.122157096862793,
        "learning_rate": 0.00013071978579174545,
        "epoch": 0.46870692031982353,
        "step": 3400
    },
    {
        "loss": 1.7545,
        "grad_norm": 1.8148996829986572,
        "learning_rate": 0.00013059568932031768,
        "epoch": 0.4688447752963882,
        "step": 3401
    },
    {
        "loss": 2.3092,
        "grad_norm": 1.8517576456069946,
        "learning_rate": 0.00013047154084378393,
        "epoch": 0.46898263027295284,
        "step": 3402
    },
    {
        "loss": 1.6236,
        "grad_norm": 1.640025019645691,
        "learning_rate": 0.00013034734057316584,
        "epoch": 0.4691204852495175,
        "step": 3403
    },
    {
        "loss": 2.174,
        "grad_norm": 1.1834238767623901,
        "learning_rate": 0.00013022308871957325,
        "epoch": 0.46925834022608215,
        "step": 3404
    },
    {
        "loss": 1.3851,
        "grad_norm": 2.5025322437286377,
        "learning_rate": 0.00013009878549420341,
        "epoch": 0.4693961952026468,
        "step": 3405
    },
    {
        "loss": 1.8303,
        "grad_norm": 1.4832409620285034,
        "learning_rate": 0.00012997443110834123,
        "epoch": 0.46953405017921146,
        "step": 3406
    },
    {
        "loss": 1.6538,
        "grad_norm": 2.11665940284729,
        "learning_rate": 0.00012985002577335837,
        "epoch": 0.4696719051557761,
        "step": 3407
    },
    {
        "loss": 2.0804,
        "grad_norm": 1.785062313079834,
        "learning_rate": 0.00012972556970071317,
        "epoch": 0.46980976013234077,
        "step": 3408
    },
    {
        "loss": 1.3265,
        "grad_norm": 2.589233160018921,
        "learning_rate": 0.00012960106310195,
        "epoch": 0.4699476151089054,
        "step": 3409
    },
    {
        "loss": 1.0945,
        "grad_norm": 1.8931338787078857,
        "learning_rate": 0.00012947650618869958,
        "epoch": 0.4700854700854701,
        "step": 3410
    },
    {
        "loss": 2.0167,
        "grad_norm": 1.099704384803772,
        "learning_rate": 0.0001293518991726776,
        "epoch": 0.47022332506203474,
        "step": 3411
    },
    {
        "loss": 1.7888,
        "grad_norm": 2.0277047157287598,
        "learning_rate": 0.0001292272422656853,
        "epoch": 0.4703611800385994,
        "step": 3412
    },
    {
        "loss": 1.7846,
        "grad_norm": 2.3918869495391846,
        "learning_rate": 0.00012910253567960855,
        "epoch": 0.47049903501516405,
        "step": 3413
    },
    {
        "loss": 1.8978,
        "grad_norm": 2.563096046447754,
        "learning_rate": 0.00012897777962641782,
        "epoch": 0.4706368899917287,
        "step": 3414
    },
    {
        "loss": 1.8108,
        "grad_norm": 3.1148033142089844,
        "learning_rate": 0.0001288529743181674,
        "epoch": 0.47077474496829336,
        "step": 3415
    },
    {
        "loss": 1.9411,
        "grad_norm": 1.1765114068984985,
        "learning_rate": 0.00012872811996699553,
        "epoch": 0.470912599944858,
        "step": 3416
    },
    {
        "loss": 2.2672,
        "grad_norm": 1.8147319555282593,
        "learning_rate": 0.0001286032167851237,
        "epoch": 0.47105045492142267,
        "step": 3417
    },
    {
        "loss": 2.2259,
        "grad_norm": 2.0404810905456543,
        "learning_rate": 0.00012847826498485647,
        "epoch": 0.4711883098979873,
        "step": 3418
    },
    {
        "loss": 1.4027,
        "grad_norm": 2.4068644046783447,
        "learning_rate": 0.000128353264778581,
        "epoch": 0.471326164874552,
        "step": 3419
    },
    {
        "loss": 2.1647,
        "grad_norm": 1.9878960847854614,
        "learning_rate": 0.00012822821637876666,
        "epoch": 0.47146401985111663,
        "step": 3420
    },
    {
        "loss": 2.5461,
        "grad_norm": 1.7219672203063965,
        "learning_rate": 0.0001281031199979649,
        "epoch": 0.4716018748276813,
        "step": 3421
    },
    {
        "loss": 2.2615,
        "grad_norm": 1.0294791460037231,
        "learning_rate": 0.00012797797584880856,
        "epoch": 0.47173972980424594,
        "step": 3422
    },
    {
        "loss": 2.0736,
        "grad_norm": 1.3797868490219116,
        "learning_rate": 0.0001278527841440119,
        "epoch": 0.4718775847808106,
        "step": 3423
    },
    {
        "loss": 1.85,
        "grad_norm": 1.7684005498886108,
        "learning_rate": 0.00012772754509636958,
        "epoch": 0.47201543975737525,
        "step": 3424
    },
    {
        "loss": 2.1126,
        "grad_norm": 1.9464396238327026,
        "learning_rate": 0.00012760225891875722,
        "epoch": 0.4721532947339399,
        "step": 3425
    },
    {
        "loss": 2.5164,
        "grad_norm": 1.7194147109985352,
        "learning_rate": 0.00012747692582413032,
        "epoch": 0.47229114971050457,
        "step": 3426
    },
    {
        "loss": 1.8503,
        "grad_norm": 1.62581467628479,
        "learning_rate": 0.00012735154602552416,
        "epoch": 0.4724290046870692,
        "step": 3427
    },
    {
        "loss": 2.2754,
        "grad_norm": 1.0990095138549805,
        "learning_rate": 0.00012722611973605324,
        "epoch": 0.4725668596636339,
        "step": 3428
    },
    {
        "loss": 1.5299,
        "grad_norm": 2.3329029083251953,
        "learning_rate": 0.0001271006471689115,
        "epoch": 0.47270471464019853,
        "step": 3429
    },
    {
        "loss": 2.1335,
        "grad_norm": 1.763239860534668,
        "learning_rate": 0.00012697512853737102,
        "epoch": 0.4728425696167632,
        "step": 3430
    },
    {
        "loss": 1.9934,
        "grad_norm": 1.4635852575302124,
        "learning_rate": 0.00012684956405478257,
        "epoch": 0.47298042459332784,
        "step": 3431
    },
    {
        "loss": 2.0501,
        "grad_norm": 1.871982216835022,
        "learning_rate": 0.00012672395393457462,
        "epoch": 0.4731182795698925,
        "step": 3432
    },
    {
        "loss": 2.3446,
        "grad_norm": 1.2923895120620728,
        "learning_rate": 0.00012659829839025352,
        "epoch": 0.47325613454645715,
        "step": 3433
    },
    {
        "loss": 1.9777,
        "grad_norm": 1.8659065961837769,
        "learning_rate": 0.00012647259763540243,
        "epoch": 0.4733939895230218,
        "step": 3434
    },
    {
        "loss": 2.2768,
        "grad_norm": 1.4310222864151,
        "learning_rate": 0.00012634685188368157,
        "epoch": 0.4735318444995864,
        "step": 3435
    },
    {
        "loss": 2.2651,
        "grad_norm": 1.7184832096099854,
        "learning_rate": 0.00012622106134882772,
        "epoch": 0.47366969947615106,
        "step": 3436
    },
    {
        "loss": 1.952,
        "grad_norm": 2.4365077018737793,
        "learning_rate": 0.00012609522624465358,
        "epoch": 0.4738075544527157,
        "step": 3437
    },
    {
        "loss": 1.7353,
        "grad_norm": 2.475419521331787,
        "learning_rate": 0.00012596934678504775,
        "epoch": 0.4739454094292804,
        "step": 3438
    },
    {
        "loss": 1.6695,
        "grad_norm": 2.6093859672546387,
        "learning_rate": 0.00012584342318397419,
        "epoch": 0.47408326440584503,
        "step": 3439
    },
    {
        "loss": 1.0382,
        "grad_norm": 2.9600720405578613,
        "learning_rate": 0.00012571745565547192,
        "epoch": 0.4742211193824097,
        "step": 3440
    },
    {
        "loss": 2.4696,
        "grad_norm": 0.9918017387390137,
        "learning_rate": 0.00012559144441365452,
        "epoch": 0.47435897435897434,
        "step": 3441
    },
    {
        "loss": 2.7476,
        "grad_norm": 1.7672501802444458,
        "learning_rate": 0.00012546538967271,
        "epoch": 0.474496829335539,
        "step": 3442
    },
    {
        "loss": 1.7896,
        "grad_norm": 1.4609490633010864,
        "learning_rate": 0.00012533929164690015,
        "epoch": 0.47463468431210365,
        "step": 3443
    },
    {
        "loss": 2.3792,
        "grad_norm": 1.7685312032699585,
        "learning_rate": 0.00012521315055056054,
        "epoch": 0.4747725392886683,
        "step": 3444
    },
    {
        "loss": 1.6047,
        "grad_norm": 1.8697186708450317,
        "learning_rate": 0.0001250869665980998,
        "epoch": 0.47491039426523296,
        "step": 3445
    },
    {
        "loss": 2.0524,
        "grad_norm": 1.436415195465088,
        "learning_rate": 0.00012496074000399948,
        "epoch": 0.4750482492417976,
        "step": 3446
    },
    {
        "loss": 2.2622,
        "grad_norm": 1.4380836486816406,
        "learning_rate": 0.00012483447098281348,
        "epoch": 0.47518610421836227,
        "step": 3447
    },
    {
        "loss": 1.9368,
        "grad_norm": 2.1949117183685303,
        "learning_rate": 0.00012470815974916806,
        "epoch": 0.4753239591949269,
        "step": 3448
    },
    {
        "loss": 1.2772,
        "grad_norm": 2.223463296890259,
        "learning_rate": 0.00012458180651776093,
        "epoch": 0.4754618141714916,
        "step": 3449
    },
    {
        "loss": 2.0949,
        "grad_norm": 1.2641841173171997,
        "learning_rate": 0.00012445541150336132,
        "epoch": 0.47559966914805624,
        "step": 3450
    },
    {
        "loss": 1.3251,
        "grad_norm": 2.360976219177246,
        "learning_rate": 0.00012432897492080974,
        "epoch": 0.4757375241246209,
        "step": 3451
    },
    {
        "loss": 2.2602,
        "grad_norm": 1.6565325260162354,
        "learning_rate": 0.00012420249698501693,
        "epoch": 0.47587537910118555,
        "step": 3452
    },
    {
        "loss": 1.9863,
        "grad_norm": 1.7485405206680298,
        "learning_rate": 0.00012407597791096411,
        "epoch": 0.4760132340777502,
        "step": 3453
    },
    {
        "loss": 2.2335,
        "grad_norm": 1.792277216911316,
        "learning_rate": 0.00012394941791370244,
        "epoch": 0.47615108905431486,
        "step": 3454
    },
    {
        "loss": 2.412,
        "grad_norm": 1.1748859882354736,
        "learning_rate": 0.0001238228172083526,
        "epoch": 0.4762889440308795,
        "step": 3455
    },
    {
        "loss": 2.3947,
        "grad_norm": 1.1962045431137085,
        "learning_rate": 0.00012369617601010454,
        "epoch": 0.47642679900744417,
        "step": 3456
    },
    {
        "loss": 2.2206,
        "grad_norm": 2.2795701026916504,
        "learning_rate": 0.00012356949453421693,
        "epoch": 0.4765646539840088,
        "step": 3457
    },
    {
        "loss": 1.6856,
        "grad_norm": 2.033102512359619,
        "learning_rate": 0.00012344277299601697,
        "epoch": 0.4767025089605735,
        "step": 3458
    },
    {
        "loss": 2.0445,
        "grad_norm": 1.7354795932769775,
        "learning_rate": 0.00012331601161089993,
        "epoch": 0.47684036393713813,
        "step": 3459
    },
    {
        "loss": 2.3946,
        "grad_norm": 2.2794175148010254,
        "learning_rate": 0.00012318921059432882,
        "epoch": 0.4769782189137028,
        "step": 3460
    },
    {
        "loss": 2.5174,
        "grad_norm": 1.7203799486160278,
        "learning_rate": 0.0001230623701618341,
        "epoch": 0.47711607389026744,
        "step": 3461
    },
    {
        "loss": 2.0986,
        "grad_norm": 1.1813093423843384,
        "learning_rate": 0.00012293549052901293,
        "epoch": 0.4772539288668321,
        "step": 3462
    },
    {
        "loss": 1.5947,
        "grad_norm": 1.8277050256729126,
        "learning_rate": 0.0001228085719115295,
        "epoch": 0.47739178384339676,
        "step": 3463
    },
    {
        "loss": 2.0946,
        "grad_norm": 1.6043184995651245,
        "learning_rate": 0.000122681614525114,
        "epoch": 0.4775296388199614,
        "step": 3464
    },
    {
        "loss": 1.9049,
        "grad_norm": 3.186506748199463,
        "learning_rate": 0.0001225546185855627,
        "epoch": 0.47766749379652607,
        "step": 3465
    },
    {
        "loss": 1.7617,
        "grad_norm": 1.5473425388336182,
        "learning_rate": 0.00012242758430873704,
        "epoch": 0.4778053487730907,
        "step": 3466
    },
    {
        "loss": 2.2949,
        "grad_norm": 1.415166974067688,
        "learning_rate": 0.00012230051191056417,
        "epoch": 0.4779432037496554,
        "step": 3467
    },
    {
        "loss": 1.4455,
        "grad_norm": 2.0842995643615723,
        "learning_rate": 0.00012217340160703557,
        "epoch": 0.47808105872622003,
        "step": 3468
    },
    {
        "loss": 1.8081,
        "grad_norm": 2.1198806762695312,
        "learning_rate": 0.00012204625361420728,
        "epoch": 0.4782189137027847,
        "step": 3469
    },
    {
        "loss": 1.8259,
        "grad_norm": 2.5340023040771484,
        "learning_rate": 0.00012191906814819958,
        "epoch": 0.47835676867934934,
        "step": 3470
    },
    {
        "loss": 2.3078,
        "grad_norm": 1.2500650882720947,
        "learning_rate": 0.00012179184542519635,
        "epoch": 0.478494623655914,
        "step": 3471
    },
    {
        "loss": 2.044,
        "grad_norm": 2.258608818054199,
        "learning_rate": 0.00012166458566144459,
        "epoch": 0.47863247863247865,
        "step": 3472
    },
    {
        "loss": 1.0339,
        "grad_norm": 2.4500441551208496,
        "learning_rate": 0.00012153728907325452,
        "epoch": 0.4787703336090433,
        "step": 3473
    },
    {
        "loss": 1.8606,
        "grad_norm": 2.634988307952881,
        "learning_rate": 0.00012140995587699884,
        "epoch": 0.47890818858560796,
        "step": 3474
    },
    {
        "loss": 1.8994,
        "grad_norm": 1.5881881713867188,
        "learning_rate": 0.00012128258628911254,
        "epoch": 0.4790460435621726,
        "step": 3475
    },
    {
        "loss": 1.2527,
        "grad_norm": 1.4399670362472534,
        "learning_rate": 0.00012115518052609234,
        "epoch": 0.4791838985387373,
        "step": 3476
    },
    {
        "loss": 1.5486,
        "grad_norm": 2.2791900634765625,
        "learning_rate": 0.0001210277388044966,
        "epoch": 0.47932175351530193,
        "step": 3477
    },
    {
        "loss": 1.9614,
        "grad_norm": 1.5035743713378906,
        "learning_rate": 0.00012090026134094475,
        "epoch": 0.47945960849186653,
        "step": 3478
    },
    {
        "loss": 1.9716,
        "grad_norm": 2.13834285736084,
        "learning_rate": 0.00012077274835211688,
        "epoch": 0.4795974634684312,
        "step": 3479
    },
    {
        "loss": 1.885,
        "grad_norm": 2.0628974437713623,
        "learning_rate": 0.00012064520005475364,
        "epoch": 0.47973531844499584,
        "step": 3480
    },
    {
        "loss": 2.1169,
        "grad_norm": 1.644292950630188,
        "learning_rate": 0.0001205176166656554,
        "epoch": 0.4798731734215605,
        "step": 3481
    },
    {
        "loss": 2.0829,
        "grad_norm": 1.2126675844192505,
        "learning_rate": 0.00012038999840168259,
        "epoch": 0.48001102839812515,
        "step": 3482
    },
    {
        "loss": 1.973,
        "grad_norm": 3.181155204772949,
        "learning_rate": 0.00012026234547975461,
        "epoch": 0.4801488833746898,
        "step": 3483
    },
    {
        "loss": 2.1307,
        "grad_norm": 2.0687551498413086,
        "learning_rate": 0.00012013465811684991,
        "epoch": 0.48028673835125446,
        "step": 3484
    },
    {
        "loss": 1.6632,
        "grad_norm": 1.6661701202392578,
        "learning_rate": 0.0001200069365300053,
        "epoch": 0.4804245933278191,
        "step": 3485
    },
    {
        "loss": 2.1824,
        "grad_norm": 1.520206093788147,
        "learning_rate": 0.0001198791809363161,
        "epoch": 0.48056244830438377,
        "step": 3486
    },
    {
        "loss": 2.2671,
        "grad_norm": 1.2918914556503296,
        "learning_rate": 0.00011975139155293503,
        "epoch": 0.4807003032809484,
        "step": 3487
    },
    {
        "loss": 2.1947,
        "grad_norm": 1.2557659149169922,
        "learning_rate": 0.00011962356859707245,
        "epoch": 0.4808381582575131,
        "step": 3488
    },
    {
        "loss": 1.9993,
        "grad_norm": 1.4040369987487793,
        "learning_rate": 0.00011949571228599594,
        "epoch": 0.48097601323407774,
        "step": 3489
    },
    {
        "loss": 1.8397,
        "grad_norm": 1.8059430122375488,
        "learning_rate": 0.00011936782283702959,
        "epoch": 0.4811138682106424,
        "step": 3490
    },
    {
        "loss": 2.436,
        "grad_norm": 1.2897238731384277,
        "learning_rate": 0.00011923990046755367,
        "epoch": 0.48125172318720705,
        "step": 3491
    },
    {
        "loss": 2.1963,
        "grad_norm": 2.113556146621704,
        "learning_rate": 0.0001191119453950047,
        "epoch": 0.4813895781637717,
        "step": 3492
    },
    {
        "loss": 2.0894,
        "grad_norm": 2.0424931049346924,
        "learning_rate": 0.00011898395783687462,
        "epoch": 0.48152743314033636,
        "step": 3493
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.0901906490325928,
        "learning_rate": 0.00011885593801071065,
        "epoch": 0.481665288116901,
        "step": 3494
    },
    {
        "loss": 1.8522,
        "grad_norm": 1.3144657611846924,
        "learning_rate": 0.0001187278861341148,
        "epoch": 0.48180314309346567,
        "step": 3495
    },
    {
        "loss": 1.9042,
        "grad_norm": 1.8517333269119263,
        "learning_rate": 0.00011859980242474368,
        "epoch": 0.4819409980700303,
        "step": 3496
    },
    {
        "loss": 1.736,
        "grad_norm": 2.408689022064209,
        "learning_rate": 0.00011847168710030785,
        "epoch": 0.482078853046595,
        "step": 3497
    },
    {
        "loss": 1.795,
        "grad_norm": 1.3336056470870972,
        "learning_rate": 0.0001183435403785717,
        "epoch": 0.48221670802315963,
        "step": 3498
    },
    {
        "loss": 2.2776,
        "grad_norm": 1.9270267486572266,
        "learning_rate": 0.00011821536247735303,
        "epoch": 0.4823545629997243,
        "step": 3499
    },
    {
        "loss": 2.3654,
        "grad_norm": 1.4227535724639893,
        "learning_rate": 0.00011808715361452238,
        "epoch": 0.48249241797628895,
        "step": 3500
    },
    {
        "loss": 2.0633,
        "grad_norm": 2.476928949356079,
        "learning_rate": 0.00011795891400800331,
        "epoch": 0.4826302729528536,
        "step": 3501
    },
    {
        "loss": 2.3565,
        "grad_norm": 1.1550014019012451,
        "learning_rate": 0.00011783064387577139,
        "epoch": 0.48276812792941826,
        "step": 3502
    },
    {
        "loss": 2.2978,
        "grad_norm": 1.0800827741622925,
        "learning_rate": 0.00011770234343585418,
        "epoch": 0.4829059829059829,
        "step": 3503
    },
    {
        "loss": 2.0318,
        "grad_norm": 1.7908360958099365,
        "learning_rate": 0.0001175740129063305,
        "epoch": 0.48304383788254757,
        "step": 3504
    },
    {
        "loss": 2.427,
        "grad_norm": 2.7680113315582275,
        "learning_rate": 0.00011744565250533077,
        "epoch": 0.4831816928591122,
        "step": 3505
    },
    {
        "loss": 1.0126,
        "grad_norm": 1.7746201753616333,
        "learning_rate": 0.00011731726245103579,
        "epoch": 0.4833195478356769,
        "step": 3506
    },
    {
        "loss": 1.476,
        "grad_norm": 1.8030674457550049,
        "learning_rate": 0.0001171888429616768,
        "epoch": 0.48345740281224153,
        "step": 3507
    },
    {
        "loss": 2.1863,
        "grad_norm": 1.2016385793685913,
        "learning_rate": 0.00011706039425553537,
        "epoch": 0.4835952577888062,
        "step": 3508
    },
    {
        "loss": 1.6387,
        "grad_norm": 2.0237817764282227,
        "learning_rate": 0.00011693191655094257,
        "epoch": 0.48373311276537084,
        "step": 3509
    },
    {
        "loss": 1.9064,
        "grad_norm": 1.3395692110061646,
        "learning_rate": 0.00011680341006627853,
        "epoch": 0.4838709677419355,
        "step": 3510
    },
    {
        "loss": 1.3531,
        "grad_norm": 2.080019474029541,
        "learning_rate": 0.00011667487501997259,
        "epoch": 0.48400882271850015,
        "step": 3511
    },
    {
        "loss": 2.1435,
        "grad_norm": 3.313563108444214,
        "learning_rate": 0.00011654631163050256,
        "epoch": 0.4841466776950648,
        "step": 3512
    },
    {
        "loss": 1.9301,
        "grad_norm": 1.9250929355621338,
        "learning_rate": 0.0001164177201163944,
        "epoch": 0.48428453267162946,
        "step": 3513
    },
    {
        "loss": 1.6443,
        "grad_norm": 3.071957588195801,
        "learning_rate": 0.00011628910069622185,
        "epoch": 0.4844223876481941,
        "step": 3514
    },
    {
        "loss": 2.0877,
        "grad_norm": 1.3869810104370117,
        "learning_rate": 0.00011616045358860617,
        "epoch": 0.4845602426247588,
        "step": 3515
    },
    {
        "loss": 1.6361,
        "grad_norm": 1.082345962524414,
        "learning_rate": 0.0001160317790122156,
        "epoch": 0.48469809760132343,
        "step": 3516
    },
    {
        "loss": 2.1056,
        "grad_norm": 2.033151388168335,
        "learning_rate": 0.00011590307718576512,
        "epoch": 0.4848359525778881,
        "step": 3517
    },
    {
        "loss": 2.5967,
        "grad_norm": 1.5941227674484253,
        "learning_rate": 0.00011577434832801606,
        "epoch": 0.48497380755445274,
        "step": 3518
    },
    {
        "loss": 2.4834,
        "grad_norm": 1.560489296913147,
        "learning_rate": 0.00011564559265777546,
        "epoch": 0.4851116625310174,
        "step": 3519
    },
    {
        "loss": 0.4996,
        "grad_norm": 1.0750070810317993,
        "learning_rate": 0.00011551681039389633,
        "epoch": 0.48524951750758205,
        "step": 3520
    },
    {
        "loss": 1.559,
        "grad_norm": 1.8865406513214111,
        "learning_rate": 0.00011538800175527655,
        "epoch": 0.48538737248414665,
        "step": 3521
    },
    {
        "loss": 1.6605,
        "grad_norm": 1.8393014669418335,
        "learning_rate": 0.00011525916696085899,
        "epoch": 0.4855252274607113,
        "step": 3522
    },
    {
        "loss": 1.1817,
        "grad_norm": 1.8774811029434204,
        "learning_rate": 0.00011513030622963094,
        "epoch": 0.48566308243727596,
        "step": 3523
    },
    {
        "loss": 2.0116,
        "grad_norm": 1.349258542060852,
        "learning_rate": 0.00011500141978062391,
        "epoch": 0.4858009374138406,
        "step": 3524
    },
    {
        "loss": 2.1678,
        "grad_norm": 1.1390399932861328,
        "learning_rate": 0.00011487250783291275,
        "epoch": 0.48593879239040527,
        "step": 3525
    },
    {
        "loss": 1.8761,
        "grad_norm": 1.786025047302246,
        "learning_rate": 0.00011474357060561591,
        "epoch": 0.4860766473669699,
        "step": 3526
    },
    {
        "loss": 1.118,
        "grad_norm": 2.4182205200195312,
        "learning_rate": 0.00011461460831789501,
        "epoch": 0.4862145023435346,
        "step": 3527
    },
    {
        "loss": 2.7875,
        "grad_norm": 1.2332314252853394,
        "learning_rate": 0.00011448562118895397,
        "epoch": 0.48635235732009924,
        "step": 3528
    },
    {
        "loss": 1.9925,
        "grad_norm": 2.4051835536956787,
        "learning_rate": 0.0001143566094380389,
        "epoch": 0.4864902122966639,
        "step": 3529
    },
    {
        "loss": 2.1648,
        "grad_norm": 1.9306873083114624,
        "learning_rate": 0.00011422757328443796,
        "epoch": 0.48662806727322855,
        "step": 3530
    },
    {
        "loss": 2.4503,
        "grad_norm": 2.4778006076812744,
        "learning_rate": 0.00011409851294748066,
        "epoch": 0.4867659222497932,
        "step": 3531
    },
    {
        "loss": 1.6835,
        "grad_norm": 1.6857560873031616,
        "learning_rate": 0.0001139694286465377,
        "epoch": 0.48690377722635786,
        "step": 3532
    },
    {
        "loss": 1.6073,
        "grad_norm": 2.3567655086517334,
        "learning_rate": 0.00011384032060102046,
        "epoch": 0.4870416322029225,
        "step": 3533
    },
    {
        "loss": 2.0213,
        "grad_norm": 1.31294846534729,
        "learning_rate": 0.0001137111890303807,
        "epoch": 0.48717948717948717,
        "step": 3534
    },
    {
        "loss": 2.0053,
        "grad_norm": 1.7088825702667236,
        "learning_rate": 0.00011358203415411011,
        "epoch": 0.4873173421560518,
        "step": 3535
    },
    {
        "loss": 1.7526,
        "grad_norm": 1.4688735008239746,
        "learning_rate": 0.00011345285619174009,
        "epoch": 0.4874551971326165,
        "step": 3536
    },
    {
        "loss": 1.5365,
        "grad_norm": 1.6735508441925049,
        "learning_rate": 0.0001133236553628413,
        "epoch": 0.48759305210918114,
        "step": 3537
    },
    {
        "loss": 1.2318,
        "grad_norm": 3.037722587585449,
        "learning_rate": 0.00011319443188702297,
        "epoch": 0.4877309070857458,
        "step": 3538
    },
    {
        "loss": 2.0748,
        "grad_norm": 1.7636932134628296,
        "learning_rate": 0.00011306518598393328,
        "epoch": 0.48786876206231045,
        "step": 3539
    },
    {
        "loss": 2.274,
        "grad_norm": 1.6566801071166992,
        "learning_rate": 0.00011293591787325821,
        "epoch": 0.4880066170388751,
        "step": 3540
    },
    {
        "loss": 1.7687,
        "grad_norm": 1.436048984527588,
        "learning_rate": 0.00011280662777472164,
        "epoch": 0.48814447201543976,
        "step": 3541
    },
    {
        "loss": 1.4388,
        "grad_norm": 2.4410760402679443,
        "learning_rate": 0.00011267731590808468,
        "epoch": 0.4882823269920044,
        "step": 3542
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.870002269744873,
        "learning_rate": 0.00011254798249314567,
        "epoch": 0.48842018196856907,
        "step": 3543
    },
    {
        "loss": 0.9007,
        "grad_norm": 2.2456679344177246,
        "learning_rate": 0.00011241862774973926,
        "epoch": 0.4885580369451337,
        "step": 3544
    },
    {
        "loss": 2.5097,
        "grad_norm": 1.5748913288116455,
        "learning_rate": 0.00011228925189773654,
        "epoch": 0.4886958919216984,
        "step": 3545
    },
    {
        "loss": 2.1498,
        "grad_norm": 1.7428573369979858,
        "learning_rate": 0.00011215985515704456,
        "epoch": 0.48883374689826303,
        "step": 3546
    },
    {
        "loss": 2.1134,
        "grad_norm": 2.0628879070281982,
        "learning_rate": 0.00011203043774760585,
        "epoch": 0.4889716018748277,
        "step": 3547
    },
    {
        "loss": 2.077,
        "grad_norm": 1.638035774230957,
        "learning_rate": 0.00011190099988939784,
        "epoch": 0.48910945685139234,
        "step": 3548
    },
    {
        "loss": 1.5036,
        "grad_norm": 1.905371904373169,
        "learning_rate": 0.00011177154180243294,
        "epoch": 0.489247311827957,
        "step": 3549
    },
    {
        "loss": 1.4835,
        "grad_norm": 1.7596477270126343,
        "learning_rate": 0.00011164206370675787,
        "epoch": 0.48938516680452165,
        "step": 3550
    },
    {
        "loss": 1.3067,
        "grad_norm": 2.363186836242676,
        "learning_rate": 0.00011151256582245342,
        "epoch": 0.4895230217810863,
        "step": 3551
    },
    {
        "loss": 2.0517,
        "grad_norm": 2.7959537506103516,
        "learning_rate": 0.00011138304836963394,
        "epoch": 0.48966087675765096,
        "step": 3552
    },
    {
        "loss": 0.9821,
        "grad_norm": 2.295158863067627,
        "learning_rate": 0.00011125351156844705,
        "epoch": 0.4897987317342156,
        "step": 3553
    },
    {
        "loss": 2.1776,
        "grad_norm": 1.9665439128875732,
        "learning_rate": 0.00011112395563907331,
        "epoch": 0.4899365867107803,
        "step": 3554
    },
    {
        "loss": 2.2051,
        "grad_norm": 2.1837754249572754,
        "learning_rate": 0.00011099438080172579,
        "epoch": 0.49007444168734493,
        "step": 3555
    },
    {
        "loss": 1.0548,
        "grad_norm": 2.7030436992645264,
        "learning_rate": 0.00011086478727664969,
        "epoch": 0.4902122966639096,
        "step": 3556
    },
    {
        "loss": 1.2959,
        "grad_norm": 1.8098963499069214,
        "learning_rate": 0.00011073517528412176,
        "epoch": 0.49035015164047424,
        "step": 3557
    },
    {
        "loss": 2.3851,
        "grad_norm": 2.147853136062622,
        "learning_rate": 0.0001106055450444506,
        "epoch": 0.4904880066170389,
        "step": 3558
    },
    {
        "loss": 1.5978,
        "grad_norm": 2.114896297454834,
        "learning_rate": 0.00011047589677797538,
        "epoch": 0.49062586159360355,
        "step": 3559
    },
    {
        "loss": 2.0252,
        "grad_norm": 1.4846183061599731,
        "learning_rate": 0.0001103462307050662,
        "epoch": 0.4907637165701682,
        "step": 3560
    },
    {
        "loss": 2.4173,
        "grad_norm": 1.6236474514007568,
        "learning_rate": 0.00011021654704612325,
        "epoch": 0.49090157154673286,
        "step": 3561
    },
    {
        "loss": 1.8455,
        "grad_norm": 1.663053274154663,
        "learning_rate": 0.0001100868460215768,
        "epoch": 0.4910394265232975,
        "step": 3562
    },
    {
        "loss": 2.1148,
        "grad_norm": 2.2761473655700684,
        "learning_rate": 0.00010995712785188635,
        "epoch": 0.4911772814998621,
        "step": 3563
    },
    {
        "loss": 2.2839,
        "grad_norm": 1.8741577863693237,
        "learning_rate": 0.00010982739275754073,
        "epoch": 0.4913151364764268,
        "step": 3564
    },
    {
        "loss": 1.0289,
        "grad_norm": 2.0204849243164062,
        "learning_rate": 0.00010969764095905763,
        "epoch": 0.49145299145299143,
        "step": 3565
    },
    {
        "loss": 2.0712,
        "grad_norm": 2.6833462715148926,
        "learning_rate": 0.00010956787267698305,
        "epoch": 0.4915908464295561,
        "step": 3566
    },
    {
        "loss": 1.1893,
        "grad_norm": 2.315732717514038,
        "learning_rate": 0.00010943808813189083,
        "epoch": 0.49172870140612074,
        "step": 3567
    },
    {
        "loss": 1.5019,
        "grad_norm": 1.6538456678390503,
        "learning_rate": 0.00010930828754438266,
        "epoch": 0.4918665563826854,
        "step": 3568
    },
    {
        "loss": 2.5585,
        "grad_norm": 1.4871330261230469,
        "learning_rate": 0.0001091784711350875,
        "epoch": 0.49200441135925005,
        "step": 3569
    },
    {
        "loss": 2.1246,
        "grad_norm": 2.0388705730438232,
        "learning_rate": 0.00010904863912466106,
        "epoch": 0.4921422663358147,
        "step": 3570
    },
    {
        "loss": 2.6648,
        "grad_norm": 1.7532397508621216,
        "learning_rate": 0.0001089187917337857,
        "epoch": 0.49228012131237936,
        "step": 3571
    },
    {
        "loss": 1.3068,
        "grad_norm": 2.291536569595337,
        "learning_rate": 0.00010878892918316988,
        "epoch": 0.492417976288944,
        "step": 3572
    },
    {
        "loss": 1.882,
        "grad_norm": 1.7929059267044067,
        "learning_rate": 0.00010865905169354778,
        "epoch": 0.49255583126550867,
        "step": 3573
    },
    {
        "loss": 2.1792,
        "grad_norm": 2.5904510021209717,
        "learning_rate": 0.00010852915948567906,
        "epoch": 0.4926936862420733,
        "step": 3574
    },
    {
        "loss": 2.0872,
        "grad_norm": 1.9474319219589233,
        "learning_rate": 0.00010839925278034841,
        "epoch": 0.492831541218638,
        "step": 3575
    },
    {
        "loss": 2.3614,
        "grad_norm": 1.5092008113861084,
        "learning_rate": 0.0001082693317983649,
        "epoch": 0.49296939619520264,
        "step": 3576
    },
    {
        "loss": 2.3935,
        "grad_norm": 1.6283657550811768,
        "learning_rate": 0.00010813939676056223,
        "epoch": 0.4931072511717673,
        "step": 3577
    },
    {
        "loss": 1.92,
        "grad_norm": 1.6667594909667969,
        "learning_rate": 0.00010800944788779786,
        "epoch": 0.49324510614833195,
        "step": 3578
    },
    {
        "loss": 2.2695,
        "grad_norm": 2.1573708057403564,
        "learning_rate": 0.0001078794854009527,
        "epoch": 0.4933829611248966,
        "step": 3579
    },
    {
        "loss": 2.2877,
        "grad_norm": 1.6593291759490967,
        "learning_rate": 0.00010774950952093079,
        "epoch": 0.49352081610146126,
        "step": 3580
    },
    {
        "loss": 2.0205,
        "grad_norm": 1.4554246664047241,
        "learning_rate": 0.00010761952046865908,
        "epoch": 0.4936586710780259,
        "step": 3581
    },
    {
        "loss": 1.9453,
        "grad_norm": 1.9457768201828003,
        "learning_rate": 0.00010748951846508673,
        "epoch": 0.49379652605459057,
        "step": 3582
    },
    {
        "loss": 2.3411,
        "grad_norm": 1.3446972370147705,
        "learning_rate": 0.00010735950373118495,
        "epoch": 0.4939343810311552,
        "step": 3583
    },
    {
        "loss": 1.8305,
        "grad_norm": 2.222325325012207,
        "learning_rate": 0.00010722947648794677,
        "epoch": 0.4940722360077199,
        "step": 3584
    },
    {
        "loss": 2.6062,
        "grad_norm": 1.1282464265823364,
        "learning_rate": 0.00010709943695638636,
        "epoch": 0.49421009098428453,
        "step": 3585
    },
    {
        "loss": 1.3789,
        "grad_norm": 2.233330011367798,
        "learning_rate": 0.00010696938535753864,
        "epoch": 0.4943479459608492,
        "step": 3586
    },
    {
        "loss": 1.7277,
        "grad_norm": 1.7913053035736084,
        "learning_rate": 0.00010683932191245923,
        "epoch": 0.49448580093741384,
        "step": 3587
    },
    {
        "loss": 1.5643,
        "grad_norm": 2.4110302925109863,
        "learning_rate": 0.00010670924684222381,
        "epoch": 0.4946236559139785,
        "step": 3588
    },
    {
        "loss": 2.0073,
        "grad_norm": 2.034902811050415,
        "learning_rate": 0.00010657916036792788,
        "epoch": 0.49476151089054315,
        "step": 3589
    },
    {
        "loss": 1.3279,
        "grad_norm": 2.6690332889556885,
        "learning_rate": 0.00010644906271068622,
        "epoch": 0.4948993658671078,
        "step": 3590
    },
    {
        "loss": 1.5327,
        "grad_norm": 2.244774103164673,
        "learning_rate": 0.00010631895409163268,
        "epoch": 0.49503722084367247,
        "step": 3591
    },
    {
        "loss": 1.7759,
        "grad_norm": 1.3858057260513306,
        "learning_rate": 0.00010618883473191978,
        "epoch": 0.4951750758202371,
        "step": 3592
    },
    {
        "loss": 1.5842,
        "grad_norm": 2.445553779602051,
        "learning_rate": 0.00010605870485271827,
        "epoch": 0.4953129307968018,
        "step": 3593
    },
    {
        "loss": 2.2194,
        "grad_norm": 1.887163519859314,
        "learning_rate": 0.00010592856467521676,
        "epoch": 0.49545078577336643,
        "step": 3594
    },
    {
        "loss": 1.4759,
        "grad_norm": 2.021967887878418,
        "learning_rate": 0.00010579841442062121,
        "epoch": 0.4955886407499311,
        "step": 3595
    },
    {
        "loss": 1.5722,
        "grad_norm": 2.8630855083465576,
        "learning_rate": 0.00010566825431015505,
        "epoch": 0.49572649572649574,
        "step": 3596
    },
    {
        "loss": 2.0884,
        "grad_norm": 2.9427566528320312,
        "learning_rate": 0.00010553808456505828,
        "epoch": 0.4958643507030604,
        "step": 3597
    },
    {
        "loss": 0.691,
        "grad_norm": 1.2530590295791626,
        "learning_rate": 0.0001054079054065872,
        "epoch": 0.49600220567962505,
        "step": 3598
    },
    {
        "loss": 1.9783,
        "grad_norm": 1.496613621711731,
        "learning_rate": 0.00010527771705601422,
        "epoch": 0.4961400606561897,
        "step": 3599
    },
    {
        "loss": 2.344,
        "grad_norm": 2.0438239574432373,
        "learning_rate": 0.0001051475197346274,
        "epoch": 0.49627791563275436,
        "step": 3600
    },
    {
        "loss": 1.531,
        "grad_norm": 2.4254066944122314,
        "learning_rate": 0.00010501731366372986,
        "epoch": 0.496415770609319,
        "step": 3601
    },
    {
        "loss": 1.5926,
        "grad_norm": 1.763533353805542,
        "learning_rate": 0.00010488709906463974,
        "epoch": 0.4965536255858837,
        "step": 3602
    },
    {
        "loss": 0.7433,
        "grad_norm": 2.7721316814422607,
        "learning_rate": 0.00010475687615868976,
        "epoch": 0.49669148056244833,
        "step": 3603
    },
    {
        "loss": 2.1226,
        "grad_norm": 1.598998785018921,
        "learning_rate": 0.00010462664516722669,
        "epoch": 0.496829335539013,
        "step": 3604
    },
    {
        "loss": 2.1886,
        "grad_norm": 1.9003653526306152,
        "learning_rate": 0.00010449640631161086,
        "epoch": 0.49696719051557764,
        "step": 3605
    },
    {
        "loss": 1.935,
        "grad_norm": 1.8947709798812866,
        "learning_rate": 0.00010436615981321618,
        "epoch": 0.49710504549214224,
        "step": 3606
    },
    {
        "loss": 1.5132,
        "grad_norm": 3.0461020469665527,
        "learning_rate": 0.00010423590589342953,
        "epoch": 0.4972429004687069,
        "step": 3607
    },
    {
        "loss": 1.9271,
        "grad_norm": 2.489896297454834,
        "learning_rate": 0.0001041056447736503,
        "epoch": 0.49738075544527155,
        "step": 3608
    },
    {
        "loss": 1.6546,
        "grad_norm": 1.6087570190429688,
        "learning_rate": 0.00010397537667529022,
        "epoch": 0.4975186104218362,
        "step": 3609
    },
    {
        "loss": 1.6837,
        "grad_norm": 2.6241040229797363,
        "learning_rate": 0.00010384510181977287,
        "epoch": 0.49765646539840086,
        "step": 3610
    },
    {
        "loss": 2.6112,
        "grad_norm": 1.3077353239059448,
        "learning_rate": 0.00010371482042853322,
        "epoch": 0.4977943203749655,
        "step": 3611
    },
    {
        "loss": 1.9947,
        "grad_norm": 1.9358752965927124,
        "learning_rate": 0.00010358453272301746,
        "epoch": 0.49793217535153017,
        "step": 3612
    },
    {
        "loss": 2.0513,
        "grad_norm": 1.2677003145217896,
        "learning_rate": 0.00010345423892468246,
        "epoch": 0.4980700303280948,
        "step": 3613
    },
    {
        "loss": 2.0545,
        "grad_norm": 1.5070362091064453,
        "learning_rate": 0.0001033239392549955,
        "epoch": 0.4982078853046595,
        "step": 3614
    },
    {
        "loss": 2.2989,
        "grad_norm": 1.2853469848632812,
        "learning_rate": 0.00010319363393543376,
        "epoch": 0.49834574028122414,
        "step": 3615
    },
    {
        "loss": 1.5051,
        "grad_norm": 1.6923151016235352,
        "learning_rate": 0.00010306332318748404,
        "epoch": 0.4984835952577888,
        "step": 3616
    },
    {
        "loss": 2.2773,
        "grad_norm": 1.4871795177459717,
        "learning_rate": 0.00010293300723264241,
        "epoch": 0.49862145023435345,
        "step": 3617
    },
    {
        "loss": 2.2816,
        "grad_norm": 1.5839340686798096,
        "learning_rate": 0.0001028026862924138,
        "epoch": 0.4987593052109181,
        "step": 3618
    },
    {
        "loss": 2.3933,
        "grad_norm": 1.5073459148406982,
        "learning_rate": 0.0001026723605883116,
        "epoch": 0.49889716018748276,
        "step": 3619
    },
    {
        "loss": 1.0656,
        "grad_norm": 2.3486335277557373,
        "learning_rate": 0.00010254203034185717,
        "epoch": 0.4990350151640474,
        "step": 3620
    },
    {
        "loss": 2.0034,
        "grad_norm": 2.402909517288208,
        "learning_rate": 0.0001024116957745797,
        "epoch": 0.49917287014061207,
        "step": 3621
    },
    {
        "loss": 1.3385,
        "grad_norm": 2.5892324447631836,
        "learning_rate": 0.00010228135710801583,
        "epoch": 0.4993107251171767,
        "step": 3622
    },
    {
        "loss": 2.5897,
        "grad_norm": 1.1061934232711792,
        "learning_rate": 0.0001021510145637091,
        "epoch": 0.4994485800937414,
        "step": 3623
    },
    {
        "loss": 1.8708,
        "grad_norm": 1.4706053733825684,
        "learning_rate": 0.00010202066836320947,
        "epoch": 0.49958643507030603,
        "step": 3624
    },
    {
        "loss": 1.3804,
        "grad_norm": 2.6913962364196777,
        "learning_rate": 0.00010189031872807329,
        "epoch": 0.4997242900468707,
        "step": 3625
    },
    {
        "loss": 2.3123,
        "grad_norm": 1.598841905593872,
        "learning_rate": 0.00010175996587986269,
        "epoch": 0.49986214502343534,
        "step": 3626
    },
    {
        "loss": 2.1732,
        "grad_norm": 1.0302759408950806,
        "learning_rate": 0.00010162961004014532,
        "epoch": 0.5,
        "step": 3627
    },
    {
        "loss": 1.9278,
        "grad_norm": 1.874281644821167,
        "learning_rate": 0.0001014992514304939,
        "epoch": 0.5001378549765646,
        "step": 3628
    },
    {
        "loss": 1.7584,
        "grad_norm": 2.125558376312256,
        "learning_rate": 0.00010136889027248575,
        "epoch": 0.5002757099531293,
        "step": 3629
    },
    {
        "loss": 1.289,
        "grad_norm": 2.0363619327545166,
        "learning_rate": 0.00010123852678770265,
        "epoch": 0.5004135649296939,
        "step": 3630
    },
    {
        "loss": 1.8363,
        "grad_norm": 2.088616132736206,
        "learning_rate": 0.0001011081611977303,
        "epoch": 0.5005514199062586,
        "step": 3631
    },
    {
        "loss": 1.8302,
        "grad_norm": 2.043604612350464,
        "learning_rate": 0.00010097779372415792,
        "epoch": 0.5006892748828232,
        "step": 3632
    },
    {
        "loss": 1.0955,
        "grad_norm": 3.243300199508667,
        "learning_rate": 0.00010084742458857799,
        "epoch": 0.5008271298593879,
        "step": 3633
    },
    {
        "loss": 2.5205,
        "grad_norm": 1.3452883958816528,
        "learning_rate": 0.00010071705401258582,
        "epoch": 0.5009649848359525,
        "step": 3634
    },
    {
        "loss": 1.7304,
        "grad_norm": 1.793729543685913,
        "learning_rate": 0.00010058668221777918,
        "epoch": 0.5011028398125172,
        "step": 3635
    },
    {
        "loss": 1.9455,
        "grad_norm": 2.261373281478882,
        "learning_rate": 0.0001004563094257578,
        "epoch": 0.5012406947890818,
        "step": 3636
    },
    {
        "loss": 1.5234,
        "grad_norm": 2.151624917984009,
        "learning_rate": 0.00010032593585812327,
        "epoch": 0.5013785497656466,
        "step": 3637
    },
    {
        "loss": 1.9046,
        "grad_norm": 1.8379591703414917,
        "learning_rate": 0.00010019556173647844,
        "epoch": 0.5015164047422112,
        "step": 3638
    },
    {
        "loss": 1.4934,
        "grad_norm": 2.125446319580078,
        "learning_rate": 0.00010006518728242694,
        "epoch": 0.5016542597187759,
        "step": 3639
    },
    {
        "loss": 2.0006,
        "grad_norm": 2.168957471847534,
        "learning_rate": 9.993481271757315e-05,
        "epoch": 0.5017921146953405,
        "step": 3640
    },
    {
        "loss": 1.8625,
        "grad_norm": 1.4906110763549805,
        "learning_rate": 9.980443826352159e-05,
        "epoch": 0.5019299696719052,
        "step": 3641
    },
    {
        "loss": 2.5811,
        "grad_norm": 1.4277688264846802,
        "learning_rate": 9.96740641418767e-05,
        "epoch": 0.5020678246484698,
        "step": 3642
    },
    {
        "loss": 2.1035,
        "grad_norm": 1.8745462894439697,
        "learning_rate": 9.95436905742422e-05,
        "epoch": 0.5022056796250345,
        "step": 3643
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.7733429670333862,
        "learning_rate": 9.941331778222084e-05,
        "epoch": 0.5023435346015991,
        "step": 3644
    },
    {
        "loss": 1.8879,
        "grad_norm": 2.6785669326782227,
        "learning_rate": 9.928294598741427e-05,
        "epoch": 0.5024813895781638,
        "step": 3645
    },
    {
        "loss": 1.0419,
        "grad_norm": 2.829594373703003,
        "learning_rate": 9.915257541142203e-05,
        "epoch": 0.5026192445547284,
        "step": 3646
    },
    {
        "loss": 2.3872,
        "grad_norm": 2.2025399208068848,
        "learning_rate": 9.902220627584204e-05,
        "epoch": 0.5027570995312931,
        "step": 3647
    },
    {
        "loss": 1.7366,
        "grad_norm": 1.8422839641571045,
        "learning_rate": 9.889183880226973e-05,
        "epoch": 0.5028949545078577,
        "step": 3648
    },
    {
        "loss": 2.4669,
        "grad_norm": 1.6424769163131714,
        "learning_rate": 9.876147321229738e-05,
        "epoch": 0.5030328094844224,
        "step": 3649
    },
    {
        "loss": 0.9733,
        "grad_norm": 1.547461986541748,
        "learning_rate": 9.863110972751428e-05,
        "epoch": 0.503170664460987,
        "step": 3650
    },
    {
        "loss": 2.0886,
        "grad_norm": 1.037226676940918,
        "learning_rate": 9.850074856950618e-05,
        "epoch": 0.5033085194375517,
        "step": 3651
    },
    {
        "loss": 1.3123,
        "grad_norm": 2.3687899112701416,
        "learning_rate": 9.837038995985469e-05,
        "epoch": 0.5034463744141163,
        "step": 3652
    },
    {
        "loss": 2.1213,
        "grad_norm": 2.270092248916626,
        "learning_rate": 9.824003412013725e-05,
        "epoch": 0.503584229390681,
        "step": 3653
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.4353569746017456,
        "learning_rate": 9.810968127192675e-05,
        "epoch": 0.5037220843672456,
        "step": 3654
    },
    {
        "loss": 1.2536,
        "grad_norm": 2.829932928085327,
        "learning_rate": 9.797933163679054e-05,
        "epoch": 0.5038599393438103,
        "step": 3655
    },
    {
        "loss": 2.0213,
        "grad_norm": 1.7751872539520264,
        "learning_rate": 9.7848985436291e-05,
        "epoch": 0.503997794320375,
        "step": 3656
    },
    {
        "loss": 2.0546,
        "grad_norm": 1.4113492965698242,
        "learning_rate": 9.77186428919842e-05,
        "epoch": 0.5041356492969397,
        "step": 3657
    },
    {
        "loss": 1.5401,
        "grad_norm": 2.0369365215301514,
        "learning_rate": 9.758830422542027e-05,
        "epoch": 0.5042735042735043,
        "step": 3658
    },
    {
        "loss": 2.0234,
        "grad_norm": 1.0869072675704956,
        "learning_rate": 9.745796965814294e-05,
        "epoch": 0.504411359250069,
        "step": 3659
    },
    {
        "loss": 1.6636,
        "grad_norm": 1.7600483894348145,
        "learning_rate": 9.732763941168843e-05,
        "epoch": 0.5045492142266336,
        "step": 3660
    },
    {
        "loss": 2.4477,
        "grad_norm": 1.559396743774414,
        "learning_rate": 9.719731370758615e-05,
        "epoch": 0.5046870692031983,
        "step": 3661
    },
    {
        "loss": 1.7127,
        "grad_norm": 2.21812105178833,
        "learning_rate": 9.706699276735762e-05,
        "epoch": 0.5048249241797629,
        "step": 3662
    },
    {
        "loss": 2.0747,
        "grad_norm": 1.5061386823654175,
        "learning_rate": 9.693667681251599e-05,
        "epoch": 0.5049627791563276,
        "step": 3663
    },
    {
        "loss": 1.1322,
        "grad_norm": 2.025348663330078,
        "learning_rate": 9.680636606456634e-05,
        "epoch": 0.5051006341328922,
        "step": 3664
    },
    {
        "loss": 1.8031,
        "grad_norm": 1.851594090461731,
        "learning_rate": 9.667606074500452e-05,
        "epoch": 0.5052384891094569,
        "step": 3665
    },
    {
        "loss": 1.0379,
        "grad_norm": 1.9246305227279663,
        "learning_rate": 9.654576107531746e-05,
        "epoch": 0.5053763440860215,
        "step": 3666
    },
    {
        "loss": 2.1205,
        "grad_norm": 1.3156063556671143,
        "learning_rate": 9.641546727698256e-05,
        "epoch": 0.5055141990625862,
        "step": 3667
    },
    {
        "loss": 1.295,
        "grad_norm": 2.324965715408325,
        "learning_rate": 9.62851795714668e-05,
        "epoch": 0.5056520540391508,
        "step": 3668
    },
    {
        "loss": 1.9024,
        "grad_norm": 2.4267189502716064,
        "learning_rate": 9.615489818022716e-05,
        "epoch": 0.5057899090157155,
        "step": 3669
    },
    {
        "loss": 1.8915,
        "grad_norm": 1.708940029144287,
        "learning_rate": 9.602462332470987e-05,
        "epoch": 0.5059277639922801,
        "step": 3670
    },
    {
        "loss": 1.5136,
        "grad_norm": 2.39011287689209,
        "learning_rate": 9.589435522634972e-05,
        "epoch": 0.5060656189688447,
        "step": 3671
    },
    {
        "loss": 2.0489,
        "grad_norm": 1.3837579488754272,
        "learning_rate": 9.576409410657042e-05,
        "epoch": 0.5062034739454094,
        "step": 3672
    },
    {
        "loss": 2.6845,
        "grad_norm": 1.6859526634216309,
        "learning_rate": 9.563384018678384e-05,
        "epoch": 0.506341328921974,
        "step": 3673
    },
    {
        "loss": 2.1918,
        "grad_norm": 2.01631236076355,
        "learning_rate": 9.550359368838916e-05,
        "epoch": 0.5064791838985387,
        "step": 3674
    },
    {
        "loss": 1.2952,
        "grad_norm": 2.637829303741455,
        "learning_rate": 9.537335483277339e-05,
        "epoch": 0.5066170388751033,
        "step": 3675
    },
    {
        "loss": 1.6677,
        "grad_norm": 2.326449155807495,
        "learning_rate": 9.524312384131026e-05,
        "epoch": 0.506754893851668,
        "step": 3676
    },
    {
        "loss": 1.3786,
        "grad_norm": 2.5508275032043457,
        "learning_rate": 9.511290093536023e-05,
        "epoch": 0.5068927488282327,
        "step": 3677
    },
    {
        "loss": 2.6135,
        "grad_norm": 1.1592496633529663,
        "learning_rate": 9.498268633627023e-05,
        "epoch": 0.5070306038047974,
        "step": 3678
    },
    {
        "loss": 1.8163,
        "grad_norm": 1.9801350831985474,
        "learning_rate": 9.485248026537263e-05,
        "epoch": 0.507168458781362,
        "step": 3679
    },
    {
        "loss": 2.4556,
        "grad_norm": 1.193961262702942,
        "learning_rate": 9.472228294398575e-05,
        "epoch": 0.5073063137579267,
        "step": 3680
    },
    {
        "loss": 1.6475,
        "grad_norm": 1.7859992980957031,
        "learning_rate": 9.459209459341283e-05,
        "epoch": 0.5074441687344913,
        "step": 3681
    },
    {
        "loss": 2.1164,
        "grad_norm": 1.2727786302566528,
        "learning_rate": 9.446191543494173e-05,
        "epoch": 0.507582023711056,
        "step": 3682
    },
    {
        "loss": 1.4444,
        "grad_norm": 2.128899335861206,
        "learning_rate": 9.433174568984502e-05,
        "epoch": 0.5077198786876206,
        "step": 3683
    },
    {
        "loss": 1.2004,
        "grad_norm": 2.535632371902466,
        "learning_rate": 9.420158557937881e-05,
        "epoch": 0.5078577336641853,
        "step": 3684
    },
    {
        "loss": 1.7707,
        "grad_norm": 2.863954782485962,
        "learning_rate": 9.407143532478326e-05,
        "epoch": 0.5079955886407499,
        "step": 3685
    },
    {
        "loss": 2.2173,
        "grad_norm": 1.4517403841018677,
        "learning_rate": 9.394129514728176e-05,
        "epoch": 0.5081334436173146,
        "step": 3686
    },
    {
        "loss": 1.6736,
        "grad_norm": 2.4116766452789307,
        "learning_rate": 9.381116526808023e-05,
        "epoch": 0.5082712985938792,
        "step": 3687
    },
    {
        "loss": 2.1727,
        "grad_norm": 1.005518913269043,
        "learning_rate": 9.368104590836733e-05,
        "epoch": 0.5084091535704439,
        "step": 3688
    },
    {
        "loss": 2.0717,
        "grad_norm": 1.3211361169815063,
        "learning_rate": 9.355093728931388e-05,
        "epoch": 0.5085470085470085,
        "step": 3689
    },
    {
        "loss": 2.1724,
        "grad_norm": 2.06353497505188,
        "learning_rate": 9.342083963207215e-05,
        "epoch": 0.5086848635235732,
        "step": 3690
    },
    {
        "loss": 2.167,
        "grad_norm": 1.7321977615356445,
        "learning_rate": 9.329075315777615e-05,
        "epoch": 0.5088227185001378,
        "step": 3691
    },
    {
        "loss": 1.5569,
        "grad_norm": 1.7429707050323486,
        "learning_rate": 9.31606780875408e-05,
        "epoch": 0.5089605734767025,
        "step": 3692
    },
    {
        "loss": 1.5493,
        "grad_norm": 2.009223699569702,
        "learning_rate": 9.303061464246138e-05,
        "epoch": 0.5090984284532671,
        "step": 3693
    },
    {
        "loss": 2.2721,
        "grad_norm": 2.3824574947357178,
        "learning_rate": 9.290056304361375e-05,
        "epoch": 0.5092362834298318,
        "step": 3694
    },
    {
        "loss": 2.2372,
        "grad_norm": 1.988179326057434,
        "learning_rate": 9.277052351205327e-05,
        "epoch": 0.5093741384063964,
        "step": 3695
    },
    {
        "loss": 2.7006,
        "grad_norm": 1.291084885597229,
        "learning_rate": 9.264049626881502e-05,
        "epoch": 0.5095119933829612,
        "step": 3696
    },
    {
        "loss": 2.6267,
        "grad_norm": 1.4638906717300415,
        "learning_rate": 9.251048153491338e-05,
        "epoch": 0.5096498483595258,
        "step": 3697
    },
    {
        "loss": 1.0307,
        "grad_norm": 2.144702672958374,
        "learning_rate": 9.238047953134094e-05,
        "epoch": 0.5097877033360905,
        "step": 3698
    },
    {
        "loss": 1.4867,
        "grad_norm": 2.603302478790283,
        "learning_rate": 9.225049047906918e-05,
        "epoch": 0.5099255583126551,
        "step": 3699
    },
    {
        "loss": 1.3566,
        "grad_norm": 2.006479501724243,
        "learning_rate": 9.212051459904734e-05,
        "epoch": 0.5100634132892198,
        "step": 3700
    },
    {
        "loss": 2.0569,
        "grad_norm": 1.0569851398468018,
        "learning_rate": 9.199055211220217e-05,
        "epoch": 0.5102012682657844,
        "step": 3701
    },
    {
        "loss": 1.9378,
        "grad_norm": 1.734230399131775,
        "learning_rate": 9.186060323943783e-05,
        "epoch": 0.5103391232423491,
        "step": 3702
    },
    {
        "loss": 1.4111,
        "grad_norm": 2.187389850616455,
        "learning_rate": 9.173066820163514e-05,
        "epoch": 0.5104769782189137,
        "step": 3703
    },
    {
        "loss": 2.3904,
        "grad_norm": 1.3163787126541138,
        "learning_rate": 9.160074721965162e-05,
        "epoch": 0.5106148331954784,
        "step": 3704
    },
    {
        "loss": 1.1614,
        "grad_norm": 4.006869792938232,
        "learning_rate": 9.147084051432098e-05,
        "epoch": 0.510752688172043,
        "step": 3705
    },
    {
        "loss": 2.2301,
        "grad_norm": 1.9375557899475098,
        "learning_rate": 9.134094830645225e-05,
        "epoch": 0.5108905431486077,
        "step": 3706
    },
    {
        "loss": 1.7719,
        "grad_norm": 2.4009714126586914,
        "learning_rate": 9.121107081683016e-05,
        "epoch": 0.5110283981251723,
        "step": 3707
    },
    {
        "loss": 2.4043,
        "grad_norm": 2.320220947265625,
        "learning_rate": 9.10812082662144e-05,
        "epoch": 0.511166253101737,
        "step": 3708
    },
    {
        "loss": 1.4927,
        "grad_norm": 2.148972988128662,
        "learning_rate": 9.095136087533896e-05,
        "epoch": 0.5113041080783016,
        "step": 3709
    },
    {
        "loss": 2.0929,
        "grad_norm": 2.0867831707000732,
        "learning_rate": 9.082152886491247e-05,
        "epoch": 0.5114419630548663,
        "step": 3710
    },
    {
        "loss": 1.942,
        "grad_norm": 1.914778470993042,
        "learning_rate": 9.069171245561735e-05,
        "epoch": 0.5115798180314309,
        "step": 3711
    },
    {
        "loss": 2.5765,
        "grad_norm": 1.710230827331543,
        "learning_rate": 9.056191186810918e-05,
        "epoch": 0.5117176730079956,
        "step": 3712
    },
    {
        "loss": 1.9461,
        "grad_norm": 1.4323210716247559,
        "learning_rate": 9.043212732301703e-05,
        "epoch": 0.5118555279845602,
        "step": 3713
    },
    {
        "loss": 2.2025,
        "grad_norm": 2.2295994758605957,
        "learning_rate": 9.03023590409424e-05,
        "epoch": 0.5119933829611248,
        "step": 3714
    },
    {
        "loss": 2.2158,
        "grad_norm": 1.5150768756866455,
        "learning_rate": 9.017260724245924e-05,
        "epoch": 0.5121312379376896,
        "step": 3715
    },
    {
        "loss": 2.4038,
        "grad_norm": 1.6233770847320557,
        "learning_rate": 9.004287214811373e-05,
        "epoch": 0.5122690929142542,
        "step": 3716
    },
    {
        "loss": 1.9435,
        "grad_norm": 1.8500394821166992,
        "learning_rate": 8.991315397842321e-05,
        "epoch": 0.5124069478908189,
        "step": 3717
    },
    {
        "loss": 2.5702,
        "grad_norm": 1.8803430795669556,
        "learning_rate": 8.978345295387668e-05,
        "epoch": 0.5125448028673835,
        "step": 3718
    },
    {
        "loss": 1.8953,
        "grad_norm": 1.7206300497055054,
        "learning_rate": 8.965376929493383e-05,
        "epoch": 0.5126826578439482,
        "step": 3719
    },
    {
        "loss": 1.9896,
        "grad_norm": 1.824802041053772,
        "learning_rate": 8.952410322202463e-05,
        "epoch": 0.5128205128205128,
        "step": 3720
    },
    {
        "loss": 1.6172,
        "grad_norm": 1.6778486967086792,
        "learning_rate": 8.939445495554952e-05,
        "epoch": 0.5129583677970775,
        "step": 3721
    },
    {
        "loss": 1.8417,
        "grad_norm": 1.8115657567977905,
        "learning_rate": 8.926482471587825e-05,
        "epoch": 0.5130962227736421,
        "step": 3722
    },
    {
        "loss": 1.4922,
        "grad_norm": 2.672086477279663,
        "learning_rate": 8.913521272335032e-05,
        "epoch": 0.5132340777502068,
        "step": 3723
    },
    {
        "loss": 2.2907,
        "grad_norm": 1.0249910354614258,
        "learning_rate": 8.900561919827423e-05,
        "epoch": 0.5133719327267714,
        "step": 3724
    },
    {
        "loss": 1.9574,
        "grad_norm": 2.031097888946533,
        "learning_rate": 8.88760443609267e-05,
        "epoch": 0.5135097877033361,
        "step": 3725
    },
    {
        "loss": 1.7831,
        "grad_norm": 2.063469886779785,
        "learning_rate": 8.874648843155298e-05,
        "epoch": 0.5136476426799007,
        "step": 3726
    },
    {
        "loss": 1.6391,
        "grad_norm": 2.035926580429077,
        "learning_rate": 8.861695163036616e-05,
        "epoch": 0.5137854976564654,
        "step": 3727
    },
    {
        "loss": 1.6362,
        "grad_norm": 1.8963617086410522,
        "learning_rate": 8.84874341775466e-05,
        "epoch": 0.51392335263303,
        "step": 3728
    },
    {
        "loss": 1.9052,
        "grad_norm": 1.8571500778198242,
        "learning_rate": 8.835793629324208e-05,
        "epoch": 0.5140612076095947,
        "step": 3729
    },
    {
        "loss": 1.996,
        "grad_norm": 1.235823631286621,
        "learning_rate": 8.822845819756708e-05,
        "epoch": 0.5141990625861593,
        "step": 3730
    },
    {
        "loss": 1.5379,
        "grad_norm": 2.1076302528381348,
        "learning_rate": 8.809900011060217e-05,
        "epoch": 0.514336917562724,
        "step": 3731
    },
    {
        "loss": 1.9858,
        "grad_norm": 1.7337226867675781,
        "learning_rate": 8.796956225239423e-05,
        "epoch": 0.5144747725392886,
        "step": 3732
    },
    {
        "loss": 2.308,
        "grad_norm": 1.7116899490356445,
        "learning_rate": 8.784014484295545e-05,
        "epoch": 0.5146126275158533,
        "step": 3733
    },
    {
        "loss": 1.9023,
        "grad_norm": 2.217722177505493,
        "learning_rate": 8.771074810226343e-05,
        "epoch": 0.514750482492418,
        "step": 3734
    },
    {
        "loss": 1.0938,
        "grad_norm": 1.9910308122634888,
        "learning_rate": 8.758137225026085e-05,
        "epoch": 0.5148883374689827,
        "step": 3735
    },
    {
        "loss": 1.2285,
        "grad_norm": 2.692530632019043,
        "learning_rate": 8.745201750685437e-05,
        "epoch": 0.5150261924455473,
        "step": 3736
    },
    {
        "loss": 1.5386,
        "grad_norm": 1.7606995105743408,
        "learning_rate": 8.732268409191526e-05,
        "epoch": 0.515164047422112,
        "step": 3737
    },
    {
        "loss": 2.4596,
        "grad_norm": 1.822147250175476,
        "learning_rate": 8.71933722252784e-05,
        "epoch": 0.5153019023986766,
        "step": 3738
    },
    {
        "loss": 2.3012,
        "grad_norm": 1.7278175354003906,
        "learning_rate": 8.706408212674181e-05,
        "epoch": 0.5154397573752413,
        "step": 3739
    },
    {
        "loss": 1.8449,
        "grad_norm": 2.06758713722229,
        "learning_rate": 8.693481401606667e-05,
        "epoch": 0.5155776123518059,
        "step": 3740
    },
    {
        "loss": 1.6856,
        "grad_norm": 2.501875638961792,
        "learning_rate": 8.680556811297704e-05,
        "epoch": 0.5157154673283706,
        "step": 3741
    },
    {
        "loss": 2.1528,
        "grad_norm": 2.246297597885132,
        "learning_rate": 8.667634463715872e-05,
        "epoch": 0.5158533223049352,
        "step": 3742
    },
    {
        "loss": 2.1554,
        "grad_norm": 1.9250566959381104,
        "learning_rate": 8.654714380825992e-05,
        "epoch": 0.5159911772814999,
        "step": 3743
    },
    {
        "loss": 2.5243,
        "grad_norm": 1.5210148096084595,
        "learning_rate": 8.64179658458899e-05,
        "epoch": 0.5161290322580645,
        "step": 3744
    },
    {
        "loss": 2.4808,
        "grad_norm": 1.7408428192138672,
        "learning_rate": 8.628881096961932e-05,
        "epoch": 0.5162668872346292,
        "step": 3745
    },
    {
        "loss": 2.1847,
        "grad_norm": 1.761874794960022,
        "learning_rate": 8.615967939897963e-05,
        "epoch": 0.5164047422111938,
        "step": 3746
    },
    {
        "loss": 1.1152,
        "grad_norm": 2.714175224304199,
        "learning_rate": 8.603057135346232e-05,
        "epoch": 0.5165425971877585,
        "step": 3747
    },
    {
        "loss": 1.914,
        "grad_norm": 1.9331490993499756,
        "learning_rate": 8.590148705251928e-05,
        "epoch": 0.5166804521643231,
        "step": 3748
    },
    {
        "loss": 2.2213,
        "grad_norm": 1.270511507987976,
        "learning_rate": 8.577242671556206e-05,
        "epoch": 0.5168183071408878,
        "step": 3749
    },
    {
        "loss": 2.1598,
        "grad_norm": 1.197268009185791,
        "learning_rate": 8.564339056196113e-05,
        "epoch": 0.5169561621174524,
        "step": 3750
    },
    {
        "loss": 1.4105,
        "grad_norm": 2.8657567501068115,
        "learning_rate": 8.551437881104611e-05,
        "epoch": 0.5170940170940171,
        "step": 3751
    },
    {
        "loss": 2.41,
        "grad_norm": 1.254228949546814,
        "learning_rate": 8.538539168210501e-05,
        "epoch": 0.5172318720705817,
        "step": 3752
    },
    {
        "loss": 1.9,
        "grad_norm": 2.1267147064208984,
        "learning_rate": 8.525642939438403e-05,
        "epoch": 0.5173697270471465,
        "step": 3753
    },
    {
        "loss": 1.6818,
        "grad_norm": 1.4423999786376953,
        "learning_rate": 8.512749216708736e-05,
        "epoch": 0.517507582023711,
        "step": 3754
    },
    {
        "loss": 1.2783,
        "grad_norm": 2.479698896408081,
        "learning_rate": 8.499858021937613e-05,
        "epoch": 0.5176454370002758,
        "step": 3755
    },
    {
        "loss": 1.803,
        "grad_norm": 2.2598206996917725,
        "learning_rate": 8.486969377036897e-05,
        "epoch": 0.5177832919768404,
        "step": 3756
    },
    {
        "loss": 1.4659,
        "grad_norm": 2.3575801849365234,
        "learning_rate": 8.474083303914104e-05,
        "epoch": 0.517921146953405,
        "step": 3757
    },
    {
        "loss": 2.1893,
        "grad_norm": 1.6203155517578125,
        "learning_rate": 8.461199824472348e-05,
        "epoch": 0.5180590019299697,
        "step": 3758
    },
    {
        "loss": 1.4955,
        "grad_norm": 1.4696495532989502,
        "learning_rate": 8.448318960610364e-05,
        "epoch": 0.5181968569065343,
        "step": 3759
    },
    {
        "loss": 0.7628,
        "grad_norm": 2.4155197143554688,
        "learning_rate": 8.435440734222458e-05,
        "epoch": 0.518334711883099,
        "step": 3760
    },
    {
        "loss": 1.9315,
        "grad_norm": 2.3434994220733643,
        "learning_rate": 8.422565167198395e-05,
        "epoch": 0.5184725668596636,
        "step": 3761
    },
    {
        "loss": 2.308,
        "grad_norm": 1.2461576461791992,
        "learning_rate": 8.409692281423489e-05,
        "epoch": 0.5186104218362283,
        "step": 3762
    },
    {
        "loss": 2.2756,
        "grad_norm": 2.096247911453247,
        "learning_rate": 8.396822098778443e-05,
        "epoch": 0.5187482768127929,
        "step": 3763
    },
    {
        "loss": 2.3472,
        "grad_norm": 2.049974203109741,
        "learning_rate": 8.383954641139385e-05,
        "epoch": 0.5188861317893576,
        "step": 3764
    },
    {
        "loss": 2.156,
        "grad_norm": 1.7511701583862305,
        "learning_rate": 8.371089930377824e-05,
        "epoch": 0.5190239867659222,
        "step": 3765
    },
    {
        "loss": 2.4099,
        "grad_norm": 1.2030298709869385,
        "learning_rate": 8.358227988360563e-05,
        "epoch": 0.5191618417424869,
        "step": 3766
    },
    {
        "loss": 1.3627,
        "grad_norm": 3.101193428039551,
        "learning_rate": 8.345368836949738e-05,
        "epoch": 0.5192996967190515,
        "step": 3767
    },
    {
        "loss": 2.3118,
        "grad_norm": 2.173804521560669,
        "learning_rate": 8.332512498002742e-05,
        "epoch": 0.5194375516956162,
        "step": 3768
    },
    {
        "loss": 0.8034,
        "grad_norm": 1.8803906440734863,
        "learning_rate": 8.31965899337215e-05,
        "epoch": 0.5195754066721808,
        "step": 3769
    },
    {
        "loss": 1.827,
        "grad_norm": 1.071186900138855,
        "learning_rate": 8.306808344905754e-05,
        "epoch": 0.5197132616487455,
        "step": 3770
    },
    {
        "loss": 2.1445,
        "grad_norm": 2.0339627265930176,
        "learning_rate": 8.293960574446464e-05,
        "epoch": 0.5198511166253101,
        "step": 3771
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.7272322177886963,
        "learning_rate": 8.281115703832316e-05,
        "epoch": 0.5199889716018748,
        "step": 3772
    },
    {
        "loss": 1.4552,
        "grad_norm": 2.3317229747772217,
        "learning_rate": 8.268273754896432e-05,
        "epoch": 0.5201268265784394,
        "step": 3773
    },
    {
        "loss": 2.1453,
        "grad_norm": 1.523913860321045,
        "learning_rate": 8.255434749466925e-05,
        "epoch": 0.5202646815550042,
        "step": 3774
    },
    {
        "loss": 1.73,
        "grad_norm": 2.2208876609802246,
        "learning_rate": 8.242598709366945e-05,
        "epoch": 0.5204025365315688,
        "step": 3775
    },
    {
        "loss": 2.1907,
        "grad_norm": 1.6253330707550049,
        "learning_rate": 8.22976565641459e-05,
        "epoch": 0.5205403915081335,
        "step": 3776
    },
    {
        "loss": 2.0327,
        "grad_norm": 1.5971400737762451,
        "learning_rate": 8.216935612422864e-05,
        "epoch": 0.5206782464846981,
        "step": 3777
    },
    {
        "loss": 1.825,
        "grad_norm": 1.8392843008041382,
        "learning_rate": 8.204108599199664e-05,
        "epoch": 0.5208161014612628,
        "step": 3778
    },
    {
        "loss": 1.6547,
        "grad_norm": 1.9634089469909668,
        "learning_rate": 8.191284638547764e-05,
        "epoch": 0.5209539564378274,
        "step": 3779
    },
    {
        "loss": 2.3444,
        "grad_norm": 1.80423903465271,
        "learning_rate": 8.1784637522647e-05,
        "epoch": 0.5210918114143921,
        "step": 3780
    },
    {
        "loss": 2.3332,
        "grad_norm": 1.4153350591659546,
        "learning_rate": 8.165645962142834e-05,
        "epoch": 0.5212296663909567,
        "step": 3781
    },
    {
        "loss": 2.4645,
        "grad_norm": 1.5815895795822144,
        "learning_rate": 8.152831289969218e-05,
        "epoch": 0.5213675213675214,
        "step": 3782
    },
    {
        "loss": 1.3374,
        "grad_norm": 2.0439774990081787,
        "learning_rate": 8.140019757525634e-05,
        "epoch": 0.521505376344086,
        "step": 3783
    },
    {
        "loss": 1.5244,
        "grad_norm": 2.427730083465576,
        "learning_rate": 8.127211386588528e-05,
        "epoch": 0.5216432313206507,
        "step": 3784
    },
    {
        "loss": 2.3219,
        "grad_norm": 2.019970417022705,
        "learning_rate": 8.114406198928939e-05,
        "epoch": 0.5217810862972153,
        "step": 3785
    },
    {
        "loss": 2.355,
        "grad_norm": 1.0707679986953735,
        "learning_rate": 8.101604216312534e-05,
        "epoch": 0.52191894127378,
        "step": 3786
    },
    {
        "loss": 2.0026,
        "grad_norm": 1.753377914428711,
        "learning_rate": 8.088805460499532e-05,
        "epoch": 0.5220567962503446,
        "step": 3787
    },
    {
        "loss": 1.3509,
        "grad_norm": 2.202432155609131,
        "learning_rate": 8.076009953244636e-05,
        "epoch": 0.5221946512269093,
        "step": 3788
    },
    {
        "loss": 2.137,
        "grad_norm": 1.7088981866836548,
        "learning_rate": 8.063217716297052e-05,
        "epoch": 0.5223325062034739,
        "step": 3789
    },
    {
        "loss": 2.35,
        "grad_norm": 1.783664584159851,
        "learning_rate": 8.050428771400407e-05,
        "epoch": 0.5224703611800386,
        "step": 3790
    },
    {
        "loss": 2.1967,
        "grad_norm": 1.607466697692871,
        "learning_rate": 8.037643140292748e-05,
        "epoch": 0.5226082161566032,
        "step": 3791
    },
    {
        "loss": 1.8038,
        "grad_norm": 1.9064617156982422,
        "learning_rate": 8.024860844706505e-05,
        "epoch": 0.522746071133168,
        "step": 3792
    },
    {
        "loss": 1.9037,
        "grad_norm": 1.6712406873703003,
        "learning_rate": 8.012081906368393e-05,
        "epoch": 0.5228839261097326,
        "step": 3793
    },
    {
        "loss": 2.2091,
        "grad_norm": 1.470477819442749,
        "learning_rate": 7.999306346999465e-05,
        "epoch": 0.5230217810862973,
        "step": 3794
    },
    {
        "loss": 0.985,
        "grad_norm": 1.3962424993515015,
        "learning_rate": 7.986534188315016e-05,
        "epoch": 0.5231596360628619,
        "step": 3795
    },
    {
        "loss": 1.9326,
        "grad_norm": 1.5869135856628418,
        "learning_rate": 7.973765452024542e-05,
        "epoch": 0.5232974910394266,
        "step": 3796
    },
    {
        "loss": 2.1555,
        "grad_norm": 0.9841929078102112,
        "learning_rate": 7.961000159831736e-05,
        "epoch": 0.5234353460159912,
        "step": 3797
    },
    {
        "loss": 2.3616,
        "grad_norm": 1.791668176651001,
        "learning_rate": 7.948238333434462e-05,
        "epoch": 0.5235732009925558,
        "step": 3798
    },
    {
        "loss": 2.1035,
        "grad_norm": 2.105895757675171,
        "learning_rate": 7.93547999452464e-05,
        "epoch": 0.5237110559691205,
        "step": 3799
    },
    {
        "loss": 1.9517,
        "grad_norm": 2.5547194480895996,
        "learning_rate": 7.922725164788316e-05,
        "epoch": 0.5238489109456851,
        "step": 3800
    },
    {
        "loss": 2.2584,
        "grad_norm": 1.4795124530792236,
        "learning_rate": 7.909973865905529e-05,
        "epoch": 0.5239867659222498,
        "step": 3801
    },
    {
        "loss": 2.5319,
        "grad_norm": 1.3842717409133911,
        "learning_rate": 7.897226119550343e-05,
        "epoch": 0.5241246208988144,
        "step": 3802
    },
    {
        "loss": 2.0553,
        "grad_norm": 1.1846554279327393,
        "learning_rate": 7.884481947390775e-05,
        "epoch": 0.5242624758753791,
        "step": 3803
    },
    {
        "loss": 1.828,
        "grad_norm": 1.4749476909637451,
        "learning_rate": 7.87174137108875e-05,
        "epoch": 0.5244003308519437,
        "step": 3804
    },
    {
        "loss": 1.9829,
        "grad_norm": 1.5939116477966309,
        "learning_rate": 7.85900441230011e-05,
        "epoch": 0.5245381858285084,
        "step": 3805
    },
    {
        "loss": 1.5642,
        "grad_norm": 1.4811326265335083,
        "learning_rate": 7.846271092674551e-05,
        "epoch": 0.524676040805073,
        "step": 3806
    },
    {
        "loss": 2.0105,
        "grad_norm": 1.6226526498794556,
        "learning_rate": 7.833541433855542e-05,
        "epoch": 0.5248138957816377,
        "step": 3807
    },
    {
        "loss": 2.3667,
        "grad_norm": 1.4928898811340332,
        "learning_rate": 7.820815457480374e-05,
        "epoch": 0.5249517507582023,
        "step": 3808
    },
    {
        "loss": 1.3626,
        "grad_norm": 1.9257862567901611,
        "learning_rate": 7.808093185180045e-05,
        "epoch": 0.525089605734767,
        "step": 3809
    },
    {
        "loss": 1.9196,
        "grad_norm": 2.574519634246826,
        "learning_rate": 7.79537463857927e-05,
        "epoch": 0.5252274607113316,
        "step": 3810
    },
    {
        "loss": 2.3529,
        "grad_norm": 2.0523767471313477,
        "learning_rate": 7.782659839296452e-05,
        "epoch": 0.5253653156878964,
        "step": 3811
    },
    {
        "loss": 1.547,
        "grad_norm": 1.9421968460083008,
        "learning_rate": 7.769948808943586e-05,
        "epoch": 0.525503170664461,
        "step": 3812
    },
    {
        "loss": 2.2622,
        "grad_norm": 1.494721531867981,
        "learning_rate": 7.757241569126293e-05,
        "epoch": 0.5256410256410257,
        "step": 3813
    },
    {
        "loss": 1.4277,
        "grad_norm": 2.121164321899414,
        "learning_rate": 7.74453814144374e-05,
        "epoch": 0.5257788806175903,
        "step": 3814
    },
    {
        "loss": 0.7346,
        "grad_norm": 2.6007206439971924,
        "learning_rate": 7.7318385474886e-05,
        "epoch": 0.525916735594155,
        "step": 3815
    },
    {
        "loss": 2.0284,
        "grad_norm": 1.613946795463562,
        "learning_rate": 7.719142808847046e-05,
        "epoch": 0.5260545905707196,
        "step": 3816
    },
    {
        "loss": 0.8639,
        "grad_norm": 2.1105403900146484,
        "learning_rate": 7.706450947098709e-05,
        "epoch": 0.5261924455472843,
        "step": 3817
    },
    {
        "loss": 1.7237,
        "grad_norm": 1.4242420196533203,
        "learning_rate": 7.693762983816591e-05,
        "epoch": 0.5263303005238489,
        "step": 3818
    },
    {
        "loss": 2.1519,
        "grad_norm": 2.187483310699463,
        "learning_rate": 7.681078940567121e-05,
        "epoch": 0.5264681555004136,
        "step": 3819
    },
    {
        "loss": 2.4013,
        "grad_norm": 1.793604850769043,
        "learning_rate": 7.66839883891001e-05,
        "epoch": 0.5266060104769782,
        "step": 3820
    },
    {
        "loss": 2.0956,
        "grad_norm": 1.166098713874817,
        "learning_rate": 7.655722700398305e-05,
        "epoch": 0.5267438654535429,
        "step": 3821
    },
    {
        "loss": 1.8026,
        "grad_norm": 1.3198542594909668,
        "learning_rate": 7.643050546578317e-05,
        "epoch": 0.5268817204301075,
        "step": 3822
    },
    {
        "loss": 1.8505,
        "grad_norm": 1.39995276927948,
        "learning_rate": 7.63038239898955e-05,
        "epoch": 0.5270195754066722,
        "step": 3823
    },
    {
        "loss": 2.2511,
        "grad_norm": 1.6535670757293701,
        "learning_rate": 7.617718279164734e-05,
        "epoch": 0.5271574303832368,
        "step": 3824
    },
    {
        "loss": 2.0633,
        "grad_norm": 2.3054871559143066,
        "learning_rate": 7.605058208629758e-05,
        "epoch": 0.5272952853598015,
        "step": 3825
    },
    {
        "loss": 1.8864,
        "grad_norm": 3.899750232696533,
        "learning_rate": 7.592402208903592e-05,
        "epoch": 0.5274331403363661,
        "step": 3826
    },
    {
        "loss": 1.5928,
        "grad_norm": 2.3661324977874756,
        "learning_rate": 7.579750301498315e-05,
        "epoch": 0.5275709953129308,
        "step": 3827
    },
    {
        "loss": 1.7741,
        "grad_norm": 2.4026296138763428,
        "learning_rate": 7.56710250791903e-05,
        "epoch": 0.5277088502894954,
        "step": 3828
    },
    {
        "loss": 2.1446,
        "grad_norm": 1.9571887254714966,
        "learning_rate": 7.55445884966386e-05,
        "epoch": 0.5278467052660601,
        "step": 3829
    },
    {
        "loss": 2.0509,
        "grad_norm": 1.927011251449585,
        "learning_rate": 7.541819348223916e-05,
        "epoch": 0.5279845602426247,
        "step": 3830
    },
    {
        "loss": 2.3974,
        "grad_norm": 1.8908857107162476,
        "learning_rate": 7.529184025083197e-05,
        "epoch": 0.5281224152191895,
        "step": 3831
    },
    {
        "loss": 2.1573,
        "grad_norm": 4.343758583068848,
        "learning_rate": 7.516552901718649e-05,
        "epoch": 0.5282602701957541,
        "step": 3832
    },
    {
        "loss": 2.2638,
        "grad_norm": 1.5592190027236938,
        "learning_rate": 7.503925999600058e-05,
        "epoch": 0.5283981251723188,
        "step": 3833
    },
    {
        "loss": 1.498,
        "grad_norm": 2.295645236968994,
        "learning_rate": 7.491303340190023e-05,
        "epoch": 0.5285359801488834,
        "step": 3834
    },
    {
        "loss": 2.0072,
        "grad_norm": 1.5072952508926392,
        "learning_rate": 7.478684944943942e-05,
        "epoch": 0.5286738351254481,
        "step": 3835
    },
    {
        "loss": 1.4579,
        "grad_norm": 2.5027358531951904,
        "learning_rate": 7.466070835309988e-05,
        "epoch": 0.5288116901020127,
        "step": 3836
    },
    {
        "loss": 2.3447,
        "grad_norm": 1.0960097312927246,
        "learning_rate": 7.453461032729001e-05,
        "epoch": 0.5289495450785774,
        "step": 3837
    },
    {
        "loss": 1.6333,
        "grad_norm": 1.6214812994003296,
        "learning_rate": 7.440855558634552e-05,
        "epoch": 0.529087400055142,
        "step": 3838
    },
    {
        "loss": 2.1411,
        "grad_norm": 1.4082655906677246,
        "learning_rate": 7.42825443445281e-05,
        "epoch": 0.5292252550317067,
        "step": 3839
    },
    {
        "loss": 2.1325,
        "grad_norm": 1.5880823135375977,
        "learning_rate": 7.415657681602582e-05,
        "epoch": 0.5293631100082713,
        "step": 3840
    },
    {
        "loss": 1.7557,
        "grad_norm": 1.3261088132858276,
        "learning_rate": 7.403065321495232e-05,
        "epoch": 0.5295009649848359,
        "step": 3841
    },
    {
        "loss": 1.7528,
        "grad_norm": 2.1419084072113037,
        "learning_rate": 7.390477375534645e-05,
        "epoch": 0.5296388199614006,
        "step": 3842
    },
    {
        "loss": 1.7302,
        "grad_norm": 1.6237133741378784,
        "learning_rate": 7.377893865117225e-05,
        "epoch": 0.5297766749379652,
        "step": 3843
    },
    {
        "loss": 2.2249,
        "grad_norm": 1.0177152156829834,
        "learning_rate": 7.365314811631845e-05,
        "epoch": 0.5299145299145299,
        "step": 3844
    },
    {
        "loss": 1.6007,
        "grad_norm": 1.9418150186538696,
        "learning_rate": 7.35274023645976e-05,
        "epoch": 0.5300523848910945,
        "step": 3845
    },
    {
        "loss": 2.1001,
        "grad_norm": 2.22670316696167,
        "learning_rate": 7.340170160974655e-05,
        "epoch": 0.5301902398676592,
        "step": 3846
    },
    {
        "loss": 2.2875,
        "grad_norm": 2.1513562202453613,
        "learning_rate": 7.327604606542541e-05,
        "epoch": 0.5303280948442238,
        "step": 3847
    },
    {
        "loss": 1.5397,
        "grad_norm": 1.760389804840088,
        "learning_rate": 7.315043594521744e-05,
        "epoch": 0.5304659498207885,
        "step": 3848
    },
    {
        "loss": 1.9809,
        "grad_norm": 1.668059229850769,
        "learning_rate": 7.302487146262905e-05,
        "epoch": 0.5306038047973531,
        "step": 3849
    },
    {
        "loss": 1.5627,
        "grad_norm": 1.9802916049957275,
        "learning_rate": 7.289935283108853e-05,
        "epoch": 0.5307416597739179,
        "step": 3850
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.124021053314209,
        "learning_rate": 7.277388026394671e-05,
        "epoch": 0.5308795147504825,
        "step": 3851
    },
    {
        "loss": 1.9382,
        "grad_norm": 2.7639119625091553,
        "learning_rate": 7.264845397447592e-05,
        "epoch": 0.5310173697270472,
        "step": 3852
    },
    {
        "loss": 1.7206,
        "grad_norm": 1.8623803853988647,
        "learning_rate": 7.25230741758697e-05,
        "epoch": 0.5311552247036118,
        "step": 3853
    },
    {
        "loss": 1.8638,
        "grad_norm": 1.7536492347717285,
        "learning_rate": 7.239774108124274e-05,
        "epoch": 0.5312930796801765,
        "step": 3854
    },
    {
        "loss": 1.7282,
        "grad_norm": 3.0964138507843018,
        "learning_rate": 7.227245490363045e-05,
        "epoch": 0.5314309346567411,
        "step": 3855
    },
    {
        "loss": 0.4838,
        "grad_norm": 2.0186452865600586,
        "learning_rate": 7.214721585598813e-05,
        "epoch": 0.5315687896333058,
        "step": 3856
    },
    {
        "loss": 2.0867,
        "grad_norm": 1.2405933141708374,
        "learning_rate": 7.202202415119145e-05,
        "epoch": 0.5317066446098704,
        "step": 3857
    },
    {
        "loss": 2.2036,
        "grad_norm": 1.6522176265716553,
        "learning_rate": 7.189688000203511e-05,
        "epoch": 0.5318444995864351,
        "step": 3858
    },
    {
        "loss": 1.8726,
        "grad_norm": 1.539526343345642,
        "learning_rate": 7.177178362123335e-05,
        "epoch": 0.5319823545629997,
        "step": 3859
    },
    {
        "loss": 2.1701,
        "grad_norm": 1.4213682413101196,
        "learning_rate": 7.164673522141912e-05,
        "epoch": 0.5321202095395644,
        "step": 3860
    },
    {
        "loss": 2.1109,
        "grad_norm": 1.3372406959533691,
        "learning_rate": 7.152173501514354e-05,
        "epoch": 0.532258064516129,
        "step": 3861
    },
    {
        "loss": 2.2423,
        "grad_norm": 1.8120949268341064,
        "learning_rate": 7.139678321487623e-05,
        "epoch": 0.5323959194926937,
        "step": 3862
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.6436562538146973,
        "learning_rate": 7.127188003300449e-05,
        "epoch": 0.5325337744692583,
        "step": 3863
    },
    {
        "loss": 1.9217,
        "grad_norm": 2.3542444705963135,
        "learning_rate": 7.114702568183263e-05,
        "epoch": 0.532671629445823,
        "step": 3864
    },
    {
        "loss": 2.1222,
        "grad_norm": 1.935415267944336,
        "learning_rate": 7.102222037358227e-05,
        "epoch": 0.5328094844223876,
        "step": 3865
    },
    {
        "loss": 2.0843,
        "grad_norm": 2.8064768314361572,
        "learning_rate": 7.089746432039148e-05,
        "epoch": 0.5329473393989523,
        "step": 3866
    },
    {
        "loss": 1.9897,
        "grad_norm": 1.1157708168029785,
        "learning_rate": 7.077275773431469e-05,
        "epoch": 0.5330851943755169,
        "step": 3867
    },
    {
        "loss": 1.8302,
        "grad_norm": 1.025753378868103,
        "learning_rate": 7.064810082732248e-05,
        "epoch": 0.5332230493520816,
        "step": 3868
    },
    {
        "loss": 1.4485,
        "grad_norm": 2.17814302444458,
        "learning_rate": 7.052349381130046e-05,
        "epoch": 0.5333609043286462,
        "step": 3869
    },
    {
        "loss": 1.7998,
        "grad_norm": 1.4412561655044556,
        "learning_rate": 7.039893689804994e-05,
        "epoch": 0.533498759305211,
        "step": 3870
    },
    {
        "loss": 1.3421,
        "grad_norm": 2.447017192840576,
        "learning_rate": 7.027443029928694e-05,
        "epoch": 0.5336366142817756,
        "step": 3871
    },
    {
        "loss": 1.7545,
        "grad_norm": 1.5507640838623047,
        "learning_rate": 7.014997422664167e-05,
        "epoch": 0.5337744692583403,
        "step": 3872
    },
    {
        "loss": 2.0887,
        "grad_norm": 1.6227648258209229,
        "learning_rate": 7.002556889165872e-05,
        "epoch": 0.5339123242349049,
        "step": 3873
    },
    {
        "loss": 1.8085,
        "grad_norm": 2.987863540649414,
        "learning_rate": 6.990121450579661e-05,
        "epoch": 0.5340501792114696,
        "step": 3874
    },
    {
        "loss": 2.0658,
        "grad_norm": 1.168071985244751,
        "learning_rate": 6.977691128042676e-05,
        "epoch": 0.5341880341880342,
        "step": 3875
    },
    {
        "loss": 1.6716,
        "grad_norm": 2.025899887084961,
        "learning_rate": 6.965265942683417e-05,
        "epoch": 0.5343258891645989,
        "step": 3876
    },
    {
        "loss": 1.8378,
        "grad_norm": 1.4756734371185303,
        "learning_rate": 6.952845915621609e-05,
        "epoch": 0.5344637441411635,
        "step": 3877
    },
    {
        "loss": 2.3436,
        "grad_norm": 1.3992791175842285,
        "learning_rate": 6.940431067968231e-05,
        "epoch": 0.5346015991177282,
        "step": 3878
    },
    {
        "loss": 2.0835,
        "grad_norm": 1.3781589269638062,
        "learning_rate": 6.928021420825464e-05,
        "epoch": 0.5347394540942928,
        "step": 3879
    },
    {
        "loss": 2.1619,
        "grad_norm": 1.6141923666000366,
        "learning_rate": 6.915616995286615e-05,
        "epoch": 0.5348773090708575,
        "step": 3880
    },
    {
        "loss": 2.0931,
        "grad_norm": 1.4340271949768066,
        "learning_rate": 6.903217812436156e-05,
        "epoch": 0.5350151640474221,
        "step": 3881
    },
    {
        "loss": 1.9235,
        "grad_norm": 1.9668329954147339,
        "learning_rate": 6.890823893349647e-05,
        "epoch": 0.5351530190239868,
        "step": 3882
    },
    {
        "loss": 1.7378,
        "grad_norm": 2.1922786235809326,
        "learning_rate": 6.87843525909366e-05,
        "epoch": 0.5352908740005514,
        "step": 3883
    },
    {
        "loss": 2.7471,
        "grad_norm": 0.9838172197341919,
        "learning_rate": 6.866051930725828e-05,
        "epoch": 0.535428728977116,
        "step": 3884
    },
    {
        "loss": 1.5752,
        "grad_norm": 2.027116060256958,
        "learning_rate": 6.853673929294734e-05,
        "epoch": 0.5355665839536807,
        "step": 3885
    },
    {
        "loss": 1.9687,
        "grad_norm": 1.5501266717910767,
        "learning_rate": 6.841301275839917e-05,
        "epoch": 0.5357044389302453,
        "step": 3886
    },
    {
        "loss": 1.1269,
        "grad_norm": 2.6868529319763184,
        "learning_rate": 6.828933991391846e-05,
        "epoch": 0.53584229390681,
        "step": 3887
    },
    {
        "loss": 1.6292,
        "grad_norm": 1.919571042060852,
        "learning_rate": 6.816572096971825e-05,
        "epoch": 0.5359801488833746,
        "step": 3888
    },
    {
        "loss": 1.4984,
        "grad_norm": 2.0593557357788086,
        "learning_rate": 6.804215613592038e-05,
        "epoch": 0.5361180038599394,
        "step": 3889
    },
    {
        "loss": 2.0624,
        "grad_norm": 2.1258976459503174,
        "learning_rate": 6.791864562255444e-05,
        "epoch": 0.536255858836504,
        "step": 3890
    },
    {
        "loss": 2.2817,
        "grad_norm": 1.1977683305740356,
        "learning_rate": 6.779518963955768e-05,
        "epoch": 0.5363937138130687,
        "step": 3891
    },
    {
        "loss": 2.0956,
        "grad_norm": 2.114689588546753,
        "learning_rate": 6.767178839677474e-05,
        "epoch": 0.5365315687896333,
        "step": 3892
    },
    {
        "loss": 2.0022,
        "grad_norm": 1.1368051767349243,
        "learning_rate": 6.754844210395742e-05,
        "epoch": 0.536669423766198,
        "step": 3893
    },
    {
        "loss": 2.0891,
        "grad_norm": 1.4737193584442139,
        "learning_rate": 6.742515097076363e-05,
        "epoch": 0.5368072787427626,
        "step": 3894
    },
    {
        "loss": 2.4931,
        "grad_norm": 2.0378472805023193,
        "learning_rate": 6.730191520675809e-05,
        "epoch": 0.5369451337193273,
        "step": 3895
    },
    {
        "loss": 1.3988,
        "grad_norm": 2.224297285079956,
        "learning_rate": 6.717873502141095e-05,
        "epoch": 0.5370829886958919,
        "step": 3896
    },
    {
        "loss": 2.2513,
        "grad_norm": 1.8036669492721558,
        "learning_rate": 6.705561062409807e-05,
        "epoch": 0.5372208436724566,
        "step": 3897
    },
    {
        "loss": 1.3604,
        "grad_norm": 2.393181562423706,
        "learning_rate": 6.693254222410057e-05,
        "epoch": 0.5373586986490212,
        "step": 3898
    },
    {
        "loss": 2.2631,
        "grad_norm": 1.5140299797058105,
        "learning_rate": 6.680953003060413e-05,
        "epoch": 0.5374965536255859,
        "step": 3899
    },
    {
        "loss": 2.194,
        "grad_norm": 2.060865879058838,
        "learning_rate": 6.668657425269915e-05,
        "epoch": 0.5376344086021505,
        "step": 3900
    },
    {
        "loss": 2.0795,
        "grad_norm": 1.5991458892822266,
        "learning_rate": 6.65636750993802e-05,
        "epoch": 0.5377722635787152,
        "step": 3901
    },
    {
        "loss": 1.805,
        "grad_norm": 2.4134912490844727,
        "learning_rate": 6.644083277954524e-05,
        "epoch": 0.5379101185552798,
        "step": 3902
    },
    {
        "loss": 1.4485,
        "grad_norm": 2.5920612812042236,
        "learning_rate": 6.631804750199595e-05,
        "epoch": 0.5380479735318445,
        "step": 3903
    },
    {
        "loss": 1.6361,
        "grad_norm": 1.8542357683181763,
        "learning_rate": 6.619531947543694e-05,
        "epoch": 0.5381858285084091,
        "step": 3904
    },
    {
        "loss": 1.7886,
        "grad_norm": 1.8502449989318848,
        "learning_rate": 6.607264890847535e-05,
        "epoch": 0.5383236834849738,
        "step": 3905
    },
    {
        "loss": 2.2937,
        "grad_norm": 1.1946830749511719,
        "learning_rate": 6.595003600962106e-05,
        "epoch": 0.5384615384615384,
        "step": 3906
    },
    {
        "loss": 1.7998,
        "grad_norm": 1.2810873985290527,
        "learning_rate": 6.582748098728539e-05,
        "epoch": 0.5385993934381031,
        "step": 3907
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.4963053464889526,
        "learning_rate": 6.57049840497818e-05,
        "epoch": 0.5387372484146677,
        "step": 3908
    },
    {
        "loss": 1.8327,
        "grad_norm": 1.5145771503448486,
        "learning_rate": 6.558254540532481e-05,
        "epoch": 0.5388751033912325,
        "step": 3909
    },
    {
        "loss": 1.5092,
        "grad_norm": 2.3203043937683105,
        "learning_rate": 6.546016526202966e-05,
        "epoch": 0.5390129583677971,
        "step": 3910
    },
    {
        "loss": 2.3995,
        "grad_norm": 1.5448664426803589,
        "learning_rate": 6.533784382791234e-05,
        "epoch": 0.5391508133443618,
        "step": 3911
    },
    {
        "loss": 1.2833,
        "grad_norm": 1.572548508644104,
        "learning_rate": 6.521558131088927e-05,
        "epoch": 0.5392886683209264,
        "step": 3912
    },
    {
        "loss": 1.7783,
        "grad_norm": 2.305983543395996,
        "learning_rate": 6.509337791877624e-05,
        "epoch": 0.5394265232974911,
        "step": 3913
    },
    {
        "loss": 1.4893,
        "grad_norm": 2.0517232418060303,
        "learning_rate": 6.497123385928899e-05,
        "epoch": 0.5395643782740557,
        "step": 3914
    },
    {
        "loss": 1.9436,
        "grad_norm": 1.6289423704147339,
        "learning_rate": 6.484914934004207e-05,
        "epoch": 0.5397022332506204,
        "step": 3915
    },
    {
        "loss": 1.9651,
        "grad_norm": 2.456113576889038,
        "learning_rate": 6.472712456854903e-05,
        "epoch": 0.539840088227185,
        "step": 3916
    },
    {
        "loss": 2.5952,
        "grad_norm": 2.44467830657959,
        "learning_rate": 6.460515975222189e-05,
        "epoch": 0.5399779432037497,
        "step": 3917
    },
    {
        "loss": 1.8303,
        "grad_norm": 1.6578704118728638,
        "learning_rate": 6.448325509837046e-05,
        "epoch": 0.5401157981803143,
        "step": 3918
    },
    {
        "loss": 1.5848,
        "grad_norm": 1.760741114616394,
        "learning_rate": 6.436141081420263e-05,
        "epoch": 0.540253653156879,
        "step": 3919
    },
    {
        "loss": 1.3224,
        "grad_norm": 1.5898749828338623,
        "learning_rate": 6.423962710682372e-05,
        "epoch": 0.5403915081334436,
        "step": 3920
    },
    {
        "loss": 2.2592,
        "grad_norm": 2.4083995819091797,
        "learning_rate": 6.411790418323569e-05,
        "epoch": 0.5405293631100083,
        "step": 3921
    },
    {
        "loss": 2.022,
        "grad_norm": 1.2737464904785156,
        "learning_rate": 6.399624225033747e-05,
        "epoch": 0.5406672180865729,
        "step": 3922
    },
    {
        "loss": 2.0575,
        "grad_norm": 2.3777389526367188,
        "learning_rate": 6.387464151492434e-05,
        "epoch": 0.5408050730631376,
        "step": 3923
    },
    {
        "loss": 2.2999,
        "grad_norm": 1.502901315689087,
        "learning_rate": 6.37531021836873e-05,
        "epoch": 0.5409429280397022,
        "step": 3924
    },
    {
        "loss": 1.62,
        "grad_norm": 2.4654040336608887,
        "learning_rate": 6.363162446321339e-05,
        "epoch": 0.5410807830162669,
        "step": 3925
    },
    {
        "loss": 1.8809,
        "grad_norm": 1.8612011671066284,
        "learning_rate": 6.351020855998443e-05,
        "epoch": 0.5412186379928315,
        "step": 3926
    },
    {
        "loss": 1.9178,
        "grad_norm": 2.161051034927368,
        "learning_rate": 6.33888546803777e-05,
        "epoch": 0.5413564929693961,
        "step": 3927
    },
    {
        "loss": 1.5685,
        "grad_norm": 1.7544801235198975,
        "learning_rate": 6.32675630306647e-05,
        "epoch": 0.5414943479459609,
        "step": 3928
    },
    {
        "loss": 2.151,
        "grad_norm": 1.4807186126708984,
        "learning_rate": 6.314633381701111e-05,
        "epoch": 0.5416322029225255,
        "step": 3929
    },
    {
        "loss": 2.225,
        "grad_norm": 2.6037304401397705,
        "learning_rate": 6.302516724547669e-05,
        "epoch": 0.5417700578990902,
        "step": 3930
    },
    {
        "loss": 1.6741,
        "grad_norm": 2.5704030990600586,
        "learning_rate": 6.290406352201479e-05,
        "epoch": 0.5419079128756548,
        "step": 3931
    },
    {
        "loss": 2.076,
        "grad_norm": 1.3853975534439087,
        "learning_rate": 6.278302285247155e-05,
        "epoch": 0.5420457678522195,
        "step": 3932
    },
    {
        "loss": 2.3435,
        "grad_norm": 1.6409573554992676,
        "learning_rate": 6.26620454425864e-05,
        "epoch": 0.5421836228287841,
        "step": 3933
    },
    {
        "loss": 1.6172,
        "grad_norm": 1.5271782875061035,
        "learning_rate": 6.25411314979909e-05,
        "epoch": 0.5423214778053488,
        "step": 3934
    },
    {
        "loss": 2.2964,
        "grad_norm": 1.547113060951233,
        "learning_rate": 6.242028122420885e-05,
        "epoch": 0.5424593327819134,
        "step": 3935
    },
    {
        "loss": 1.7462,
        "grad_norm": 1.1872766017913818,
        "learning_rate": 6.229949482665594e-05,
        "epoch": 0.5425971877584781,
        "step": 3936
    },
    {
        "loss": 2.1089,
        "grad_norm": 1.4245426654815674,
        "learning_rate": 6.217877251063897e-05,
        "epoch": 0.5427350427350427,
        "step": 3937
    },
    {
        "loss": 2.1497,
        "grad_norm": 2.2583768367767334,
        "learning_rate": 6.205811448135612e-05,
        "epoch": 0.5428728977116074,
        "step": 3938
    },
    {
        "loss": 2.189,
        "grad_norm": 1.7703183889389038,
        "learning_rate": 6.193752094389638e-05,
        "epoch": 0.543010752688172,
        "step": 3939
    },
    {
        "loss": 1.7807,
        "grad_norm": 2.287778377532959,
        "learning_rate": 6.181699210323872e-05,
        "epoch": 0.5431486076647367,
        "step": 3940
    },
    {
        "loss": 1.7624,
        "grad_norm": 2.132718086242676,
        "learning_rate": 6.169652816425247e-05,
        "epoch": 0.5432864626413013,
        "step": 3941
    },
    {
        "loss": 2.1297,
        "grad_norm": 2.112764358520508,
        "learning_rate": 6.157612933169655e-05,
        "epoch": 0.543424317617866,
        "step": 3942
    },
    {
        "loss": 1.3567,
        "grad_norm": 2.631582736968994,
        "learning_rate": 6.145579581021903e-05,
        "epoch": 0.5435621725944306,
        "step": 3943
    },
    {
        "loss": 1.8904,
        "grad_norm": 1.6779342889785767,
        "learning_rate": 6.133552780435742e-05,
        "epoch": 0.5437000275709953,
        "step": 3944
    },
    {
        "loss": 2.3532,
        "grad_norm": 1.7901005744934082,
        "learning_rate": 6.12153255185373e-05,
        "epoch": 0.5438378825475599,
        "step": 3945
    },
    {
        "loss": 1.3714,
        "grad_norm": 2.3369500637054443,
        "learning_rate": 6.109518915707303e-05,
        "epoch": 0.5439757375241246,
        "step": 3946
    },
    {
        "loss": 2.2782,
        "grad_norm": 2.0159144401550293,
        "learning_rate": 6.0975118924166716e-05,
        "epoch": 0.5441135925006892,
        "step": 3947
    },
    {
        "loss": 2.1005,
        "grad_norm": 1.8727998733520508,
        "learning_rate": 6.085511502390797e-05,
        "epoch": 0.544251447477254,
        "step": 3948
    },
    {
        "loss": 1.7672,
        "grad_norm": 2.6333441734313965,
        "learning_rate": 6.07351776602737e-05,
        "epoch": 0.5443893024538186,
        "step": 3949
    },
    {
        "loss": 2.3146,
        "grad_norm": 1.871504545211792,
        "learning_rate": 6.061530703712797e-05,
        "epoch": 0.5445271574303833,
        "step": 3950
    },
    {
        "loss": 1.7229,
        "grad_norm": 1.4446158409118652,
        "learning_rate": 6.049550335822096e-05,
        "epoch": 0.5446650124069479,
        "step": 3951
    },
    {
        "loss": 1.7184,
        "grad_norm": 2.534860849380493,
        "learning_rate": 6.037576682718955e-05,
        "epoch": 0.5448028673835126,
        "step": 3952
    },
    {
        "loss": 1.4586,
        "grad_norm": 1.844813585281372,
        "learning_rate": 6.025609764755601e-05,
        "epoch": 0.5449407223600772,
        "step": 3953
    },
    {
        "loss": 1.8882,
        "grad_norm": 1.4795764684677124,
        "learning_rate": 6.013649602272845e-05,
        "epoch": 0.5450785773366419,
        "step": 3954
    },
    {
        "loss": 2.1409,
        "grad_norm": 1.0751309394836426,
        "learning_rate": 6.0016962156000126e-05,
        "epoch": 0.5452164323132065,
        "step": 3955
    },
    {
        "loss": 2.2886,
        "grad_norm": 1.3058186769485474,
        "learning_rate": 5.989749625054883e-05,
        "epoch": 0.5453542872897712,
        "step": 3956
    },
    {
        "loss": 2.0944,
        "grad_norm": 2.186936378479004,
        "learning_rate": 5.977809850943721e-05,
        "epoch": 0.5454921422663358,
        "step": 3957
    },
    {
        "loss": 2.0882,
        "grad_norm": 2.0057711601257324,
        "learning_rate": 5.9658769135612035e-05,
        "epoch": 0.5456299972429005,
        "step": 3958
    },
    {
        "loss": 1.101,
        "grad_norm": 2.0459911823272705,
        "learning_rate": 5.9539508331903514e-05,
        "epoch": 0.5457678522194651,
        "step": 3959
    },
    {
        "loss": 2.2466,
        "grad_norm": 1.9467676877975464,
        "learning_rate": 5.942031630102559e-05,
        "epoch": 0.5459057071960298,
        "step": 3960
    },
    {
        "loss": 1.6601,
        "grad_norm": 2.2831265926361084,
        "learning_rate": 5.930119324557534e-05,
        "epoch": 0.5460435621725944,
        "step": 3961
    },
    {
        "loss": 1.3498,
        "grad_norm": 1.808791995048523,
        "learning_rate": 5.9182139368032275e-05,
        "epoch": 0.5461814171491591,
        "step": 3962
    },
    {
        "loss": 1.4617,
        "grad_norm": 2.0844404697418213,
        "learning_rate": 5.9063154870758796e-05,
        "epoch": 0.5463192721257237,
        "step": 3963
    },
    {
        "loss": 2.497,
        "grad_norm": 1.4850575923919678,
        "learning_rate": 5.8944239955998895e-05,
        "epoch": 0.5464571271022884,
        "step": 3964
    },
    {
        "loss": 1.7284,
        "grad_norm": 2.1415603160858154,
        "learning_rate": 5.88253948258787e-05,
        "epoch": 0.546594982078853,
        "step": 3965
    },
    {
        "loss": 1.5652,
        "grad_norm": 1.835123062133789,
        "learning_rate": 5.870661968240556e-05,
        "epoch": 0.5467328370554178,
        "step": 3966
    },
    {
        "loss": 2.5894,
        "grad_norm": 1.197821855545044,
        "learning_rate": 5.858791472746769e-05,
        "epoch": 0.5468706920319824,
        "step": 3967
    },
    {
        "loss": 1.9433,
        "grad_norm": 2.5451347827911377,
        "learning_rate": 5.846928016283418e-05,
        "epoch": 0.5470085470085471,
        "step": 3968
    },
    {
        "loss": 2.3425,
        "grad_norm": 1.7518744468688965,
        "learning_rate": 5.835071619015466e-05,
        "epoch": 0.5471464019851117,
        "step": 3969
    },
    {
        "loss": 1.4254,
        "grad_norm": 2.579087495803833,
        "learning_rate": 5.823222301095827e-05,
        "epoch": 0.5472842569616763,
        "step": 3970
    },
    {
        "loss": 2.1904,
        "grad_norm": 1.1963741779327393,
        "learning_rate": 5.81138008266544e-05,
        "epoch": 0.547422111938241,
        "step": 3971
    },
    {
        "loss": 1.1627,
        "grad_norm": 2.500814914703369,
        "learning_rate": 5.799544983853129e-05,
        "epoch": 0.5475599669148056,
        "step": 3972
    },
    {
        "loss": 2.289,
        "grad_norm": 1.2299548387527466,
        "learning_rate": 5.7877170247756365e-05,
        "epoch": 0.5476978218913703,
        "step": 3973
    },
    {
        "loss": 1.9163,
        "grad_norm": 1.1412022113800049,
        "learning_rate": 5.775896225537583e-05,
        "epoch": 0.5478356768679349,
        "step": 3974
    },
    {
        "loss": 2.4168,
        "grad_norm": 2.1461181640625,
        "learning_rate": 5.764082606231378e-05,
        "epoch": 0.5479735318444996,
        "step": 3975
    },
    {
        "loss": 1.88,
        "grad_norm": 1.995036244392395,
        "learning_rate": 5.7522761869372687e-05,
        "epoch": 0.5481113868210642,
        "step": 3976
    },
    {
        "loss": 1.9689,
        "grad_norm": 2.4464914798736572,
        "learning_rate": 5.74047698772326e-05,
        "epoch": 0.5482492417976289,
        "step": 3977
    },
    {
        "loss": 1.4529,
        "grad_norm": 2.1336755752563477,
        "learning_rate": 5.7286850286450556e-05,
        "epoch": 0.5483870967741935,
        "step": 3978
    },
    {
        "loss": 1.5727,
        "grad_norm": 1.6614410877227783,
        "learning_rate": 5.716900329746071e-05,
        "epoch": 0.5485249517507582,
        "step": 3979
    },
    {
        "loss": 2.3061,
        "grad_norm": 1.1614900827407837,
        "learning_rate": 5.7051229110573923e-05,
        "epoch": 0.5486628067273228,
        "step": 3980
    },
    {
        "loss": 2.3822,
        "grad_norm": 2.2011446952819824,
        "learning_rate": 5.6933527925977035e-05,
        "epoch": 0.5488006617038875,
        "step": 3981
    },
    {
        "loss": 1.7148,
        "grad_norm": 2.345754384994507,
        "learning_rate": 5.68158999437331e-05,
        "epoch": 0.5489385166804521,
        "step": 3982
    },
    {
        "loss": 1.5156,
        "grad_norm": 2.2190933227539062,
        "learning_rate": 5.6698345363780405e-05,
        "epoch": 0.5490763716570168,
        "step": 3983
    },
    {
        "loss": 2.3508,
        "grad_norm": 1.5796396732330322,
        "learning_rate": 5.658086438593284e-05,
        "epoch": 0.5492142266335814,
        "step": 3984
    },
    {
        "loss": 0.9547,
        "grad_norm": 2.3099236488342285,
        "learning_rate": 5.6463457209879065e-05,
        "epoch": 0.5493520816101461,
        "step": 3985
    },
    {
        "loss": 1.899,
        "grad_norm": 1.503178358078003,
        "learning_rate": 5.634612403518205e-05,
        "epoch": 0.5494899365867107,
        "step": 3986
    },
    {
        "loss": 1.4499,
        "grad_norm": 1.917850375175476,
        "learning_rate": 5.6228865061279226e-05,
        "epoch": 0.5496277915632755,
        "step": 3987
    },
    {
        "loss": 2.4337,
        "grad_norm": 1.7676284313201904,
        "learning_rate": 5.6111680487482056e-05,
        "epoch": 0.5497656465398401,
        "step": 3988
    },
    {
        "loss": 2.4855,
        "grad_norm": 1.2127679586410522,
        "learning_rate": 5.599457051297502e-05,
        "epoch": 0.5499035015164048,
        "step": 3989
    },
    {
        "loss": 2.2091,
        "grad_norm": 2.280979633331299,
        "learning_rate": 5.587753533681639e-05,
        "epoch": 0.5500413564929694,
        "step": 3990
    },
    {
        "loss": 1.4213,
        "grad_norm": 2.326744556427002,
        "learning_rate": 5.576057515793689e-05,
        "epoch": 0.5501792114695341,
        "step": 3991
    },
    {
        "loss": 2.1286,
        "grad_norm": 2.269500970840454,
        "learning_rate": 5.5643690175139874e-05,
        "epoch": 0.5503170664460987,
        "step": 3992
    },
    {
        "loss": 1.8244,
        "grad_norm": 2.0643393993377686,
        "learning_rate": 5.552688058710102e-05,
        "epoch": 0.5504549214226634,
        "step": 3993
    },
    {
        "loss": 1.9639,
        "grad_norm": 1.3777464628219604,
        "learning_rate": 5.54101465923676e-05,
        "epoch": 0.550592776399228,
        "step": 3994
    },
    {
        "loss": 1.8622,
        "grad_norm": 1.2542262077331543,
        "learning_rate": 5.529348838935864e-05,
        "epoch": 0.5507306313757927,
        "step": 3995
    },
    {
        "loss": 1.6988,
        "grad_norm": 1.7991846799850464,
        "learning_rate": 5.517690617636429e-05,
        "epoch": 0.5508684863523573,
        "step": 3996
    },
    {
        "loss": 1.8583,
        "grad_norm": 2.066110372543335,
        "learning_rate": 5.506040015154532e-05,
        "epoch": 0.551006341328922,
        "step": 3997
    },
    {
        "loss": 0.8895,
        "grad_norm": 2.2198257446289062,
        "learning_rate": 5.4943970512933316e-05,
        "epoch": 0.5511441963054866,
        "step": 3998
    },
    {
        "loss": 0.9736,
        "grad_norm": 2.2649483680725098,
        "learning_rate": 5.482761745842983e-05,
        "epoch": 0.5512820512820513,
        "step": 3999
    },
    {
        "loss": 2.2488,
        "grad_norm": 1.8086442947387695,
        "learning_rate": 5.471134118580616e-05,
        "epoch": 0.5514199062586159,
        "step": 4000
    },
    {
        "loss": 1.4608,
        "grad_norm": 2.161121129989624,
        "learning_rate": 5.459514189270346e-05,
        "epoch": 0.5515577612351806,
        "step": 4001
    },
    {
        "loss": 2.2314,
        "grad_norm": 1.0761903524398804,
        "learning_rate": 5.44790197766315e-05,
        "epoch": 0.5516956162117452,
        "step": 4002
    },
    {
        "loss": 1.3628,
        "grad_norm": 2.6271209716796875,
        "learning_rate": 5.4362975034969355e-05,
        "epoch": 0.5518334711883099,
        "step": 4003
    },
    {
        "loss": 2.1579,
        "grad_norm": 1.3187233209609985,
        "learning_rate": 5.4247007864964415e-05,
        "epoch": 0.5519713261648745,
        "step": 4004
    },
    {
        "loss": 2.5869,
        "grad_norm": 1.0696972608566284,
        "learning_rate": 5.41311184637321e-05,
        "epoch": 0.5521091811414393,
        "step": 4005
    },
    {
        "loss": 2.6828,
        "grad_norm": 2.05679988861084,
        "learning_rate": 5.401530702825568e-05,
        "epoch": 0.5522470361180039,
        "step": 4006
    },
    {
        "loss": 2.0545,
        "grad_norm": 1.3344484567642212,
        "learning_rate": 5.3899573755386145e-05,
        "epoch": 0.5523848910945686,
        "step": 4007
    },
    {
        "loss": 2.0419,
        "grad_norm": 1.7975009679794312,
        "learning_rate": 5.378391884184122e-05,
        "epoch": 0.5525227460711332,
        "step": 4008
    },
    {
        "loss": 2.3429,
        "grad_norm": 1.6015617847442627,
        "learning_rate": 5.3668342484205934e-05,
        "epoch": 0.5526606010476979,
        "step": 4009
    },
    {
        "loss": 1.3759,
        "grad_norm": 5.484265327453613,
        "learning_rate": 5.355284487893122e-05,
        "epoch": 0.5527984560242625,
        "step": 4010
    },
    {
        "loss": 1.5807,
        "grad_norm": 2.0915684700012207,
        "learning_rate": 5.343742622233462e-05,
        "epoch": 0.5529363110008272,
        "step": 4011
    },
    {
        "loss": 1.3286,
        "grad_norm": 1.8022807836532593,
        "learning_rate": 5.332208671059931e-05,
        "epoch": 0.5530741659773918,
        "step": 4012
    },
    {
        "loss": 2.4767,
        "grad_norm": 1.461703896522522,
        "learning_rate": 5.3206826539773776e-05,
        "epoch": 0.5532120209539564,
        "step": 4013
    },
    {
        "loss": 1.365,
        "grad_norm": 2.5895204544067383,
        "learning_rate": 5.309164590577191e-05,
        "epoch": 0.5533498759305211,
        "step": 4014
    },
    {
        "loss": 1.6474,
        "grad_norm": 1.3377896547317505,
        "learning_rate": 5.297654500437246e-05,
        "epoch": 0.5534877309070857,
        "step": 4015
    },
    {
        "loss": 2.0747,
        "grad_norm": 1.2792690992355347,
        "learning_rate": 5.2861524031218315e-05,
        "epoch": 0.5536255858836504,
        "step": 4016
    },
    {
        "loss": 2.4069,
        "grad_norm": 1.466991662979126,
        "learning_rate": 5.2746583181816734e-05,
        "epoch": 0.553763440860215,
        "step": 4017
    },
    {
        "loss": 1.8943,
        "grad_norm": 1.670013427734375,
        "learning_rate": 5.263172265153876e-05,
        "epoch": 0.5539012958367797,
        "step": 4018
    },
    {
        "loss": 1.6961,
        "grad_norm": 1.5588711500167847,
        "learning_rate": 5.251694263561876e-05,
        "epoch": 0.5540391508133443,
        "step": 4019
    },
    {
        "loss": 1.8911,
        "grad_norm": 1.2222684621810913,
        "learning_rate": 5.24022433291546e-05,
        "epoch": 0.554177005789909,
        "step": 4020
    },
    {
        "loss": 2.2011,
        "grad_norm": 1.7984158992767334,
        "learning_rate": 5.228762492710649e-05,
        "epoch": 0.5543148607664736,
        "step": 4021
    },
    {
        "loss": 2.5158,
        "grad_norm": 1.7740232944488525,
        "learning_rate": 5.217308762429759e-05,
        "epoch": 0.5544527157430383,
        "step": 4022
    },
    {
        "loss": 2.0432,
        "grad_norm": 1.2163646221160889,
        "learning_rate": 5.2058631615412934e-05,
        "epoch": 0.5545905707196029,
        "step": 4023
    },
    {
        "loss": 2.1341,
        "grad_norm": 1.7239179611206055,
        "learning_rate": 5.194425709499937e-05,
        "epoch": 0.5547284256961676,
        "step": 4024
    },
    {
        "loss": 1.7541,
        "grad_norm": 2.9142961502075195,
        "learning_rate": 5.1829964257465244e-05,
        "epoch": 0.5548662806727322,
        "step": 4025
    },
    {
        "loss": 1.6757,
        "grad_norm": 2.1505143642425537,
        "learning_rate": 5.1715753297080335e-05,
        "epoch": 0.555004135649297,
        "step": 4026
    },
    {
        "loss": 1.3533,
        "grad_norm": 1.9652961492538452,
        "learning_rate": 5.160162440797478e-05,
        "epoch": 0.5551419906258616,
        "step": 4027
    },
    {
        "loss": 1.9579,
        "grad_norm": 2.165346145629883,
        "learning_rate": 5.148757778413971e-05,
        "epoch": 0.5552798456024263,
        "step": 4028
    },
    {
        "loss": 1.428,
        "grad_norm": 2.565664291381836,
        "learning_rate": 5.1373613619426034e-05,
        "epoch": 0.5554177005789909,
        "step": 4029
    },
    {
        "loss": 1.3129,
        "grad_norm": 2.2679905891418457,
        "learning_rate": 5.1259732107544664e-05,
        "epoch": 0.5555555555555556,
        "step": 4030
    },
    {
        "loss": 1.6142,
        "grad_norm": 1.9498870372772217,
        "learning_rate": 5.1145933442066087e-05,
        "epoch": 0.5556934105321202,
        "step": 4031
    },
    {
        "loss": 2.1273,
        "grad_norm": 2.0375053882598877,
        "learning_rate": 5.1032217816419695e-05,
        "epoch": 0.5558312655086849,
        "step": 4032
    },
    {
        "loss": 2.0855,
        "grad_norm": 1.2514649629592896,
        "learning_rate": 5.0918585423894084e-05,
        "epoch": 0.5559691204852495,
        "step": 4033
    },
    {
        "loss": 1.7015,
        "grad_norm": 1.9965884685516357,
        "learning_rate": 5.080503645763633e-05,
        "epoch": 0.5561069754618142,
        "step": 4034
    },
    {
        "loss": 1.8255,
        "grad_norm": 1.9381681680679321,
        "learning_rate": 5.069157111065141e-05,
        "epoch": 0.5562448304383788,
        "step": 4035
    },
    {
        "loss": 2.5998,
        "grad_norm": 1.3921680450439453,
        "learning_rate": 5.0578189575802406e-05,
        "epoch": 0.5563826854149435,
        "step": 4036
    },
    {
        "loss": 2.0649,
        "grad_norm": 1.3197689056396484,
        "learning_rate": 5.046489204581e-05,
        "epoch": 0.5565205403915081,
        "step": 4037
    },
    {
        "loss": 2.2592,
        "grad_norm": 1.9039103984832764,
        "learning_rate": 5.035167871325175e-05,
        "epoch": 0.5566583953680728,
        "step": 4038
    },
    {
        "loss": 1.8299,
        "grad_norm": 2.4399893283843994,
        "learning_rate": 5.0238549770562546e-05,
        "epoch": 0.5567962503446374,
        "step": 4039
    },
    {
        "loss": 1.8786,
        "grad_norm": 1.9644602537155151,
        "learning_rate": 5.0125505410033424e-05,
        "epoch": 0.5569341053212021,
        "step": 4040
    },
    {
        "loss": 1.7418,
        "grad_norm": 1.550167202949524,
        "learning_rate": 5.0012545823812026e-05,
        "epoch": 0.5570719602977667,
        "step": 4041
    },
    {
        "loss": 1.1846,
        "grad_norm": 2.9750332832336426,
        "learning_rate": 4.989967120390167e-05,
        "epoch": 0.5572098152743314,
        "step": 4042
    },
    {
        "loss": 2.0548,
        "grad_norm": 1.2765735387802124,
        "learning_rate": 4.978688174216122e-05,
        "epoch": 0.557347670250896,
        "step": 4043
    },
    {
        "loss": 1.566,
        "grad_norm": 2.0178205966949463,
        "learning_rate": 4.967417763030483e-05,
        "epoch": 0.5574855252274608,
        "step": 4044
    },
    {
        "loss": 2.3374,
        "grad_norm": 1.2427778244018555,
        "learning_rate": 4.9561559059901815e-05,
        "epoch": 0.5576233802040254,
        "step": 4045
    },
    {
        "loss": 1.9772,
        "grad_norm": 2.665131092071533,
        "learning_rate": 4.9449026222375685e-05,
        "epoch": 0.5577612351805901,
        "step": 4046
    },
    {
        "loss": 2.0423,
        "grad_norm": 1.5938678979873657,
        "learning_rate": 4.933657930900464e-05,
        "epoch": 0.5578990901571547,
        "step": 4047
    },
    {
        "loss": 2.274,
        "grad_norm": 2.0212347507476807,
        "learning_rate": 4.922421851092047e-05,
        "epoch": 0.5580369451337194,
        "step": 4048
    },
    {
        "loss": 2.1796,
        "grad_norm": 1.0775260925292969,
        "learning_rate": 4.911194401910878e-05,
        "epoch": 0.558174800110284,
        "step": 4049
    },
    {
        "loss": 1.9489,
        "grad_norm": 2.037782669067383,
        "learning_rate": 4.899975602440856e-05,
        "epoch": 0.5583126550868487,
        "step": 4050
    },
    {
        "loss": 1.976,
        "grad_norm": 1.5698648691177368,
        "learning_rate": 4.888765471751141e-05,
        "epoch": 0.5584505100634133,
        "step": 4051
    },
    {
        "loss": 1.5532,
        "grad_norm": 1.9311540126800537,
        "learning_rate": 4.8775640288961973e-05,
        "epoch": 0.558588365039978,
        "step": 4052
    },
    {
        "loss": 2.1065,
        "grad_norm": 2.433819532394409,
        "learning_rate": 4.8663712929157215e-05,
        "epoch": 0.5587262200165426,
        "step": 4053
    },
    {
        "loss": 1.5682,
        "grad_norm": 2.872647523880005,
        "learning_rate": 4.855187282834579e-05,
        "epoch": 0.5588640749931072,
        "step": 4054
    },
    {
        "loss": 2.0609,
        "grad_norm": 1.195542573928833,
        "learning_rate": 4.844012017662822e-05,
        "epoch": 0.5590019299696719,
        "step": 4055
    },
    {
        "loss": 1.4451,
        "grad_norm": 1.7571991682052612,
        "learning_rate": 4.83284551639565e-05,
        "epoch": 0.5591397849462365,
        "step": 4056
    },
    {
        "loss": 2.1515,
        "grad_norm": 2.1081392765045166,
        "learning_rate": 4.8216877980133346e-05,
        "epoch": 0.5592776399228012,
        "step": 4057
    },
    {
        "loss": 1.9625,
        "grad_norm": 2.2693514823913574,
        "learning_rate": 4.810538881481264e-05,
        "epoch": 0.5594154948993658,
        "step": 4058
    },
    {
        "loss": 1.6065,
        "grad_norm": 2.066617250442505,
        "learning_rate": 4.79939878574981e-05,
        "epoch": 0.5595533498759305,
        "step": 4059
    },
    {
        "loss": 2.1807,
        "grad_norm": 1.7094440460205078,
        "learning_rate": 4.7882675297543985e-05,
        "epoch": 0.5596912048524951,
        "step": 4060
    },
    {
        "loss": 2.4017,
        "grad_norm": 1.4548685550689697,
        "learning_rate": 4.777145132415422e-05,
        "epoch": 0.5598290598290598,
        "step": 4061
    },
    {
        "loss": 2.3389,
        "grad_norm": 2.1929309368133545,
        "learning_rate": 4.766031612638179e-05,
        "epoch": 0.5599669148056244,
        "step": 4062
    },
    {
        "loss": 1.3608,
        "grad_norm": 2.2398033142089844,
        "learning_rate": 4.754926989312914e-05,
        "epoch": 0.5601047697821891,
        "step": 4063
    },
    {
        "loss": 2.2526,
        "grad_norm": 1.7878241539001465,
        "learning_rate": 4.743831281314762e-05,
        "epoch": 0.5602426247587537,
        "step": 4064
    },
    {
        "loss": 2.2736,
        "grad_norm": 1.9659514427185059,
        "learning_rate": 4.73274450750365e-05,
        "epoch": 0.5603804797353185,
        "step": 4065
    },
    {
        "loss": 1.9495,
        "grad_norm": 1.7794122695922852,
        "learning_rate": 4.721666686724371e-05,
        "epoch": 0.5605183347118831,
        "step": 4066
    },
    {
        "loss": 0.8732,
        "grad_norm": 2.804126739501953,
        "learning_rate": 4.7105978378064854e-05,
        "epoch": 0.5606561896884478,
        "step": 4067
    },
    {
        "loss": 1.9153,
        "grad_norm": 1.9296962022781372,
        "learning_rate": 4.699537979564288e-05,
        "epoch": 0.5607940446650124,
        "step": 4068
    },
    {
        "loss": 2.1051,
        "grad_norm": 1.4304287433624268,
        "learning_rate": 4.688487130796809e-05,
        "epoch": 0.5609318996415771,
        "step": 4069
    },
    {
        "loss": 1.8272,
        "grad_norm": 2.3585829734802246,
        "learning_rate": 4.677445310287746e-05,
        "epoch": 0.5610697546181417,
        "step": 4070
    },
    {
        "loss": 2.0329,
        "grad_norm": 2.9458072185516357,
        "learning_rate": 4.666412536805472e-05,
        "epoch": 0.5612076095947064,
        "step": 4071
    },
    {
        "loss": 1.4418,
        "grad_norm": 2.0280308723449707,
        "learning_rate": 4.655388829102991e-05,
        "epoch": 0.561345464571271,
        "step": 4072
    },
    {
        "loss": 1.4884,
        "grad_norm": 2.722604513168335,
        "learning_rate": 4.644374205917853e-05,
        "epoch": 0.5614833195478357,
        "step": 4073
    },
    {
        "loss": 1.7077,
        "grad_norm": 1.7152587175369263,
        "learning_rate": 4.633368685972208e-05,
        "epoch": 0.5616211745244003,
        "step": 4074
    },
    {
        "loss": 2.3234,
        "grad_norm": 2.3819658756256104,
        "learning_rate": 4.6223722879727195e-05,
        "epoch": 0.561759029500965,
        "step": 4075
    },
    {
        "loss": 2.2614,
        "grad_norm": 1.762804627418518,
        "learning_rate": 4.611385030610529e-05,
        "epoch": 0.5618968844775296,
        "step": 4076
    },
    {
        "loss": 1.9585,
        "grad_norm": 1.6261554956436157,
        "learning_rate": 4.60040693256128e-05,
        "epoch": 0.5620347394540943,
        "step": 4077
    },
    {
        "loss": 2.5023,
        "grad_norm": 1.0557950735092163,
        "learning_rate": 4.589438012485001e-05,
        "epoch": 0.5621725944306589,
        "step": 4078
    },
    {
        "loss": 1.5279,
        "grad_norm": 1.7708183526992798,
        "learning_rate": 4.578478289026163e-05,
        "epoch": 0.5623104494072236,
        "step": 4079
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.6838066577911377,
        "learning_rate": 4.567527780813584e-05,
        "epoch": 0.5624483043837882,
        "step": 4080
    },
    {
        "loss": 0.9468,
        "grad_norm": 2.203495740890503,
        "learning_rate": 4.5565865064604105e-05,
        "epoch": 0.5625861593603529,
        "step": 4081
    },
    {
        "loss": 1.6581,
        "grad_norm": 2.05082368850708,
        "learning_rate": 4.545654484564099e-05,
        "epoch": 0.5627240143369175,
        "step": 4082
    },
    {
        "loss": 1.7487,
        "grad_norm": 2.2601747512817383,
        "learning_rate": 4.534731733706404e-05,
        "epoch": 0.5628618693134823,
        "step": 4083
    },
    {
        "loss": 2.248,
        "grad_norm": 1.2214457988739014,
        "learning_rate": 4.523818272453277e-05,
        "epoch": 0.5629997242900469,
        "step": 4084
    },
    {
        "loss": 1.959,
        "grad_norm": 1.7111188173294067,
        "learning_rate": 4.512914119354923e-05,
        "epoch": 0.5631375792666116,
        "step": 4085
    },
    {
        "loss": 1.5492,
        "grad_norm": 3.272606611251831,
        "learning_rate": 4.5020192929457e-05,
        "epoch": 0.5632754342431762,
        "step": 4086
    },
    {
        "loss": 2.1589,
        "grad_norm": 1.5042163133621216,
        "learning_rate": 4.491133811744116e-05,
        "epoch": 0.5634132892197409,
        "step": 4087
    },
    {
        "loss": 1.5765,
        "grad_norm": 1.3355181217193604,
        "learning_rate": 4.4802576942527995e-05,
        "epoch": 0.5635511441963055,
        "step": 4088
    },
    {
        "loss": 1.7298,
        "grad_norm": 1.3146435022354126,
        "learning_rate": 4.469390958958449e-05,
        "epoch": 0.5636889991728702,
        "step": 4089
    },
    {
        "loss": 2.1122,
        "grad_norm": 1.6765611171722412,
        "learning_rate": 4.458533624331838e-05,
        "epoch": 0.5638268541494348,
        "step": 4090
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.9121289253234863,
        "learning_rate": 4.44768570882776e-05,
        "epoch": 0.5639647091259995,
        "step": 4091
    },
    {
        "loss": 1.4764,
        "grad_norm": 3.021779775619507,
        "learning_rate": 4.4368472308849765e-05,
        "epoch": 0.5641025641025641,
        "step": 4092
    },
    {
        "loss": 1.6018,
        "grad_norm": 2.272904396057129,
        "learning_rate": 4.426018208926221e-05,
        "epoch": 0.5642404190791288,
        "step": 4093
    },
    {
        "loss": 2.455,
        "grad_norm": 1.1676899194717407,
        "learning_rate": 4.415198661358159e-05,
        "epoch": 0.5643782740556934,
        "step": 4094
    },
    {
        "loss": 1.5895,
        "grad_norm": 2.1676039695739746,
        "learning_rate": 4.404388606571329e-05,
        "epoch": 0.5645161290322581,
        "step": 4095
    },
    {
        "loss": 2.4936,
        "grad_norm": 1.212915062904358,
        "learning_rate": 4.393588062940175e-05,
        "epoch": 0.5646539840088227,
        "step": 4096
    },
    {
        "loss": 2.2794,
        "grad_norm": 2.3491835594177246,
        "learning_rate": 4.3827970488229275e-05,
        "epoch": 0.5647918389853873,
        "step": 4097
    },
    {
        "loss": 2.2964,
        "grad_norm": 1.161739468574524,
        "learning_rate": 4.3720155825616606e-05,
        "epoch": 0.564929693961952,
        "step": 4098
    },
    {
        "loss": 1.8847,
        "grad_norm": 2.643822431564331,
        "learning_rate": 4.3612436824821956e-05,
        "epoch": 0.5650675489385166,
        "step": 4099
    },
    {
        "loss": 1.8627,
        "grad_norm": 1.8337173461914062,
        "learning_rate": 4.350481366894095e-05,
        "epoch": 0.5652054039150813,
        "step": 4100
    },
    {
        "loss": 1.578,
        "grad_norm": 1.7980482578277588,
        "learning_rate": 4.339728654090627e-05,
        "epoch": 0.5653432588916459,
        "step": 4101
    },
    {
        "loss": 1.9029,
        "grad_norm": 1.327754020690918,
        "learning_rate": 4.328985562348764e-05,
        "epoch": 0.5654811138682106,
        "step": 4102
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.8734854459762573,
        "learning_rate": 4.318252109929083e-05,
        "epoch": 0.5656189688447752,
        "step": 4103
    },
    {
        "loss": 1.7548,
        "grad_norm": 2.443223237991333,
        "learning_rate": 4.307528315075818e-05,
        "epoch": 0.56575682382134,
        "step": 4104
    },
    {
        "loss": 1.5285,
        "grad_norm": 2.2431247234344482,
        "learning_rate": 4.296814196016764e-05,
        "epoch": 0.5658946787979046,
        "step": 4105
    },
    {
        "loss": 2.1452,
        "grad_norm": 1.3454402685165405,
        "learning_rate": 4.286109770963268e-05,
        "epoch": 0.5660325337744693,
        "step": 4106
    },
    {
        "loss": 1.3042,
        "grad_norm": 2.6845006942749023,
        "learning_rate": 4.27541505811021e-05,
        "epoch": 0.5661703887510339,
        "step": 4107
    },
    {
        "loss": 2.0344,
        "grad_norm": 1.7886872291564941,
        "learning_rate": 4.264730075635947e-05,
        "epoch": 0.5663082437275986,
        "step": 4108
    },
    {
        "loss": 2.2779,
        "grad_norm": 1.0885897874832153,
        "learning_rate": 4.254054841702313e-05,
        "epoch": 0.5664460987041632,
        "step": 4109
    },
    {
        "loss": 2.0652,
        "grad_norm": 1.4148788452148438,
        "learning_rate": 4.2433893744545804e-05,
        "epoch": 0.5665839536807279,
        "step": 4110
    },
    {
        "loss": 1.9043,
        "grad_norm": 1.5808067321777344,
        "learning_rate": 4.23273369202139e-05,
        "epoch": 0.5667218086572925,
        "step": 4111
    },
    {
        "loss": 1.602,
        "grad_norm": 1.704136848449707,
        "learning_rate": 4.2220878125147675e-05,
        "epoch": 0.5668596636338572,
        "step": 4112
    },
    {
        "loss": 2.0253,
        "grad_norm": 1.9448682069778442,
        "learning_rate": 4.21145175403009e-05,
        "epoch": 0.5669975186104218,
        "step": 4113
    },
    {
        "loss": 2.1747,
        "grad_norm": 2.1309525966644287,
        "learning_rate": 4.2008255346460015e-05,
        "epoch": 0.5671353735869865,
        "step": 4114
    },
    {
        "loss": 1.8517,
        "grad_norm": 1.7167861461639404,
        "learning_rate": 4.190209172424471e-05,
        "epoch": 0.5672732285635511,
        "step": 4115
    },
    {
        "loss": 2.272,
        "grad_norm": 1.56878662109375,
        "learning_rate": 4.17960268541067e-05,
        "epoch": 0.5674110835401158,
        "step": 4116
    },
    {
        "loss": 2.279,
        "grad_norm": 1.7140662670135498,
        "learning_rate": 4.1690060916330196e-05,
        "epoch": 0.5675489385166804,
        "step": 4117
    },
    {
        "loss": 1.2116,
        "grad_norm": 2.0019896030426025,
        "learning_rate": 4.158419409103107e-05,
        "epoch": 0.5676867934932451,
        "step": 4118
    },
    {
        "loss": 1.9175,
        "grad_norm": 2.8271045684814453,
        "learning_rate": 4.147842655815661e-05,
        "epoch": 0.5678246484698097,
        "step": 4119
    },
    {
        "loss": 2.0325,
        "grad_norm": 1.2656166553497314,
        "learning_rate": 4.1372758497485574e-05,
        "epoch": 0.5679625034463744,
        "step": 4120
    },
    {
        "loss": 1.8194,
        "grad_norm": 3.085869312286377,
        "learning_rate": 4.126719008862758e-05,
        "epoch": 0.568100358422939,
        "step": 4121
    },
    {
        "loss": 1.8088,
        "grad_norm": 2.4219799041748047,
        "learning_rate": 4.1161721511022655e-05,
        "epoch": 0.5682382133995038,
        "step": 4122
    },
    {
        "loss": 2.1191,
        "grad_norm": 1.7415653467178345,
        "learning_rate": 4.1056352943941435e-05,
        "epoch": 0.5683760683760684,
        "step": 4123
    },
    {
        "loss": 2.6262,
        "grad_norm": 1.2689383029937744,
        "learning_rate": 4.09510845664845e-05,
        "epoch": 0.5685139233526331,
        "step": 4124
    },
    {
        "loss": 1.8668,
        "grad_norm": 1.807196021080017,
        "learning_rate": 4.084591655758185e-05,
        "epoch": 0.5686517783291977,
        "step": 4125
    },
    {
        "loss": 1.1442,
        "grad_norm": 2.5474588871002197,
        "learning_rate": 4.0740849095993304e-05,
        "epoch": 0.5687896333057624,
        "step": 4126
    },
    {
        "loss": 1.8696,
        "grad_norm": 1.87035071849823,
        "learning_rate": 4.0635882360307323e-05,
        "epoch": 0.568927488282327,
        "step": 4127
    },
    {
        "loss": 2.5995,
        "grad_norm": 1.7254716157913208,
        "learning_rate": 4.053101652894152e-05,
        "epoch": 0.5690653432588917,
        "step": 4128
    },
    {
        "loss": 2.5748,
        "grad_norm": 1.6697325706481934,
        "learning_rate": 4.042625178014202e-05,
        "epoch": 0.5692031982354563,
        "step": 4129
    },
    {
        "loss": 1.7303,
        "grad_norm": 1.4471572637557983,
        "learning_rate": 4.032158829198279e-05,
        "epoch": 0.569341053212021,
        "step": 4130
    },
    {
        "loss": 1.664,
        "grad_norm": 2.386833667755127,
        "learning_rate": 4.0217026242365985e-05,
        "epoch": 0.5694789081885856,
        "step": 4131
    },
    {
        "loss": 1.3094,
        "grad_norm": 2.028980016708374,
        "learning_rate": 4.011256580902123e-05,
        "epoch": 0.5696167631651503,
        "step": 4132
    },
    {
        "loss": 1.8806,
        "grad_norm": 1.5436044931411743,
        "learning_rate": 4.00082071695053e-05,
        "epoch": 0.5697546181417149,
        "step": 4133
    },
    {
        "loss": 1.775,
        "grad_norm": 2.6214632987976074,
        "learning_rate": 3.9903950501202317e-05,
        "epoch": 0.5698924731182796,
        "step": 4134
    },
    {
        "loss": 1.7019,
        "grad_norm": 2.444267988204956,
        "learning_rate": 3.979979598132255e-05,
        "epoch": 0.5700303280948442,
        "step": 4135
    },
    {
        "loss": 2.0364,
        "grad_norm": 1.308165431022644,
        "learning_rate": 3.9695743786903106e-05,
        "epoch": 0.5701681830714089,
        "step": 4136
    },
    {
        "loss": 1.637,
        "grad_norm": 2.0225977897644043,
        "learning_rate": 3.959179409480707e-05,
        "epoch": 0.5703060380479735,
        "step": 4137
    },
    {
        "loss": 1.8782,
        "grad_norm": 1.882378101348877,
        "learning_rate": 3.948794708172297e-05,
        "epoch": 0.5704438930245382,
        "step": 4138
    },
    {
        "loss": 1.4469,
        "grad_norm": 2.4402449131011963,
        "learning_rate": 3.938420292416516e-05,
        "epoch": 0.5705817480011028,
        "step": 4139
    },
    {
        "loss": 1.7917,
        "grad_norm": 2.4719595909118652,
        "learning_rate": 3.9280561798473156e-05,
        "epoch": 0.5707196029776674,
        "step": 4140
    },
    {
        "loss": 2.2983,
        "grad_norm": 1.408994197845459,
        "learning_rate": 3.9177023880811094e-05,
        "epoch": 0.5708574579542322,
        "step": 4141
    },
    {
        "loss": 0.58,
        "grad_norm": 1.9739490747451782,
        "learning_rate": 3.907358934716788e-05,
        "epoch": 0.5709953129307968,
        "step": 4142
    },
    {
        "loss": 1.997,
        "grad_norm": 1.61068594455719,
        "learning_rate": 3.897025837335674e-05,
        "epoch": 0.5711331679073615,
        "step": 4143
    },
    {
        "loss": 2.2074,
        "grad_norm": 2.653829574584961,
        "learning_rate": 3.886703113501464e-05,
        "epoch": 0.5712710228839261,
        "step": 4144
    },
    {
        "loss": 1.727,
        "grad_norm": 1.6496938467025757,
        "learning_rate": 3.876390780760245e-05,
        "epoch": 0.5714088778604908,
        "step": 4145
    },
    {
        "loss": 1.9451,
        "grad_norm": 1.1913788318634033,
        "learning_rate": 3.8660888566404184e-05,
        "epoch": 0.5715467328370554,
        "step": 4146
    },
    {
        "loss": 1.3018,
        "grad_norm": 1.4950578212738037,
        "learning_rate": 3.8557973586527184e-05,
        "epoch": 0.5716845878136201,
        "step": 4147
    },
    {
        "loss": 1.4346,
        "grad_norm": 2.6119322776794434,
        "learning_rate": 3.84551630429016e-05,
        "epoch": 0.5718224427901847,
        "step": 4148
    },
    {
        "loss": 1.2004,
        "grad_norm": 2.211928606033325,
        "learning_rate": 3.835245711027968e-05,
        "epoch": 0.5719602977667494,
        "step": 4149
    },
    {
        "loss": 1.8544,
        "grad_norm": 2.270885705947876,
        "learning_rate": 3.824985596323627e-05,
        "epoch": 0.572098152743314,
        "step": 4150
    },
    {
        "loss": 2.368,
        "grad_norm": 1.217952013015747,
        "learning_rate": 3.8147359776167936e-05,
        "epoch": 0.5722360077198787,
        "step": 4151
    },
    {
        "loss": 1.3533,
        "grad_norm": 1.2942235469818115,
        "learning_rate": 3.8044968723292714e-05,
        "epoch": 0.5723738626964433,
        "step": 4152
    },
    {
        "loss": 1.7088,
        "grad_norm": Infinity,
        "learning_rate": 3.8044968723292714e-05,
        "epoch": 0.572511717673008,
        "step": 4153
    },
    {
        "loss": 2.2352,
        "grad_norm": 2.114461898803711,
        "learning_rate": 3.794268297865029e-05,
        "epoch": 0.5726495726495726,
        "step": 4154
    },
    {
        "loss": 1.9082,
        "grad_norm": 1.5801479816436768,
        "learning_rate": 3.784050271610091e-05,
        "epoch": 0.5727874276261373,
        "step": 4155
    },
    {
        "loss": 2.2241,
        "grad_norm": 1.9869481325149536,
        "learning_rate": 3.7738428109325954e-05,
        "epoch": 0.5729252826027019,
        "step": 4156
    },
    {
        "loss": 2.3479,
        "grad_norm": 1.6523189544677734,
        "learning_rate": 3.7636459331826945e-05,
        "epoch": 0.5730631375792666,
        "step": 4157
    },
    {
        "loss": 1.504,
        "grad_norm": 1.4667195081710815,
        "learning_rate": 3.753459655692555e-05,
        "epoch": 0.5732009925558312,
        "step": 4158
    },
    {
        "loss": 2.1764,
        "grad_norm": 1.758579134941101,
        "learning_rate": 3.743283995776323e-05,
        "epoch": 0.573338847532396,
        "step": 4159
    },
    {
        "loss": 1.5848,
        "grad_norm": 1.2936811447143555,
        "learning_rate": 3.733118970730125e-05,
        "epoch": 0.5734767025089605,
        "step": 4160
    },
    {
        "loss": 2.4817,
        "grad_norm": 1.414631962776184,
        "learning_rate": 3.722964597831967e-05,
        "epoch": 0.5736145574855253,
        "step": 4161
    },
    {
        "loss": 1.5657,
        "grad_norm": 2.473989963531494,
        "learning_rate": 3.71282089434179e-05,
        "epoch": 0.5737524124620899,
        "step": 4162
    },
    {
        "loss": 2.1709,
        "grad_norm": 2.287381410598755,
        "learning_rate": 3.702687877501377e-05,
        "epoch": 0.5738902674386546,
        "step": 4163
    },
    {
        "loss": 1.567,
        "grad_norm": 2.365778684616089,
        "learning_rate": 3.692565564534346e-05,
        "epoch": 0.5740281224152192,
        "step": 4164
    },
    {
        "loss": 1.0843,
        "grad_norm": 1.4531148672103882,
        "learning_rate": 3.682453972646132e-05,
        "epoch": 0.5741659773917839,
        "step": 4165
    },
    {
        "loss": 2.0571,
        "grad_norm": 1.886437177658081,
        "learning_rate": 3.672353119023925e-05,
        "epoch": 0.5743038323683485,
        "step": 4166
    },
    {
        "loss": 2.2134,
        "grad_norm": 1.8631160259246826,
        "learning_rate": 3.6622630208366906e-05,
        "epoch": 0.5744416873449132,
        "step": 4167
    },
    {
        "loss": 1.866,
        "grad_norm": 1.3170552253723145,
        "learning_rate": 3.6521836952351105e-05,
        "epoch": 0.5745795423214778,
        "step": 4168
    },
    {
        "loss": 2.0606,
        "grad_norm": 2.127563953399658,
        "learning_rate": 3.642115159351534e-05,
        "epoch": 0.5747173972980425,
        "step": 4169
    },
    {
        "loss": 1.9618,
        "grad_norm": 1.7555655241012573,
        "learning_rate": 3.632057430299982e-05,
        "epoch": 0.5748552522746071,
        "step": 4170
    },
    {
        "loss": 2.1723,
        "grad_norm": 1.3016432523727417,
        "learning_rate": 3.6220105251761126e-05,
        "epoch": 0.5749931072511718,
        "step": 4171
    },
    {
        "loss": 1.3614,
        "grad_norm": 2.164700984954834,
        "learning_rate": 3.611974461057166e-05,
        "epoch": 0.5751309622277364,
        "step": 4172
    },
    {
        "loss": 1.6956,
        "grad_norm": 2.31410551071167,
        "learning_rate": 3.60194925500199e-05,
        "epoch": 0.5752688172043011,
        "step": 4173
    },
    {
        "loss": 1.584,
        "grad_norm": 1.9669709205627441,
        "learning_rate": 3.591934924050935e-05,
        "epoch": 0.5754066721808657,
        "step": 4174
    },
    {
        "loss": 0.8706,
        "grad_norm": 1.7425897121429443,
        "learning_rate": 3.581931485225904e-05,
        "epoch": 0.5755445271574304,
        "step": 4175
    },
    {
        "loss": 1.8202,
        "grad_norm": 1.8870048522949219,
        "learning_rate": 3.5719389555302676e-05,
        "epoch": 0.575682382133995,
        "step": 4176
    },
    {
        "loss": 2.1381,
        "grad_norm": 2.2080602645874023,
        "learning_rate": 3.561957351948846e-05,
        "epoch": 0.5758202371105597,
        "step": 4177
    },
    {
        "loss": 2.472,
        "grad_norm": 1.3723889589309692,
        "learning_rate": 3.551986691447895e-05,
        "epoch": 0.5759580920871243,
        "step": 4178
    },
    {
        "loss": 2.2568,
        "grad_norm": 1.9564659595489502,
        "learning_rate": 3.542026990975089e-05,
        "epoch": 0.576095947063689,
        "step": 4179
    },
    {
        "loss": 2.2851,
        "grad_norm": 1.7309553623199463,
        "learning_rate": 3.532078267459435e-05,
        "epoch": 0.5762338020402537,
        "step": 4180
    },
    {
        "loss": 2.4196,
        "grad_norm": 1.60658597946167,
        "learning_rate": 3.522140537811321e-05,
        "epoch": 0.5763716570168184,
        "step": 4181
    },
    {
        "loss": 1.402,
        "grad_norm": 2.398249387741089,
        "learning_rate": 3.512213818922425e-05,
        "epoch": 0.576509511993383,
        "step": 4182
    },
    {
        "loss": 2.4516,
        "grad_norm": 1.9284747838974,
        "learning_rate": 3.502298127665705e-05,
        "epoch": 0.5766473669699476,
        "step": 4183
    },
    {
        "loss": 2.1688,
        "grad_norm": 2.163717269897461,
        "learning_rate": 3.492393480895402e-05,
        "epoch": 0.5767852219465123,
        "step": 4184
    },
    {
        "loss": 2.2828,
        "grad_norm": 2.0504133701324463,
        "learning_rate": 3.48249989544694e-05,
        "epoch": 0.5769230769230769,
        "step": 4185
    },
    {
        "loss": 1.982,
        "grad_norm": 1.5483794212341309,
        "learning_rate": 3.4726173881369826e-05,
        "epoch": 0.5770609318996416,
        "step": 4186
    },
    {
        "loss": 2.0651,
        "grad_norm": 1.7501331567764282,
        "learning_rate": 3.4627459757633595e-05,
        "epoch": 0.5771987868762062,
        "step": 4187
    },
    {
        "loss": 2.1994,
        "grad_norm": 1.6712502241134644,
        "learning_rate": 3.452885675105016e-05,
        "epoch": 0.5773366418527709,
        "step": 4188
    },
    {
        "loss": 2.3059,
        "grad_norm": 1.9659978151321411,
        "learning_rate": 3.443036502922027e-05,
        "epoch": 0.5774744968293355,
        "step": 4189
    },
    {
        "loss": 2.2856,
        "grad_norm": 1.477809190750122,
        "learning_rate": 3.433198475955559e-05,
        "epoch": 0.5776123518059002,
        "step": 4190
    },
    {
        "loss": 1.8905,
        "grad_norm": 2.7605831623077393,
        "learning_rate": 3.423371610927806e-05,
        "epoch": 0.5777502067824648,
        "step": 4191
    },
    {
        "loss": 2.3543,
        "grad_norm": 1.3264707326889038,
        "learning_rate": 3.413555924542029e-05,
        "epoch": 0.5778880617590295,
        "step": 4192
    },
    {
        "loss": 1.4002,
        "grad_norm": 2.368229627609253,
        "learning_rate": 3.4037514334824486e-05,
        "epoch": 0.5780259167355941,
        "step": 4193
    },
    {
        "loss": 2.5811,
        "grad_norm": 1.7858721017837524,
        "learning_rate": 3.393958154414292e-05,
        "epoch": 0.5781637717121588,
        "step": 4194
    },
    {
        "loss": 1.963,
        "grad_norm": 1.3722161054611206,
        "learning_rate": 3.384176103983708e-05,
        "epoch": 0.5783016266887234,
        "step": 4195
    },
    {
        "loss": 1.3662,
        "grad_norm": 3.1639773845672607,
        "learning_rate": 3.374405298817755e-05,
        "epoch": 0.5784394816652881,
        "step": 4196
    },
    {
        "loss": 1.5154,
        "grad_norm": 1.2502820491790771,
        "learning_rate": 3.364645755524386e-05,
        "epoch": 0.5785773366418527,
        "step": 4197
    },
    {
        "loss": 2.0414,
        "grad_norm": 2.328850746154785,
        "learning_rate": 3.3548974906924305e-05,
        "epoch": 0.5787151916184174,
        "step": 4198
    },
    {
        "loss": 1.5071,
        "grad_norm": 1.8558233976364136,
        "learning_rate": 3.3451605208914995e-05,
        "epoch": 0.578853046594982,
        "step": 4199
    },
    {
        "loss": 1.7251,
        "grad_norm": 2.3939783573150635,
        "learning_rate": 3.335434862672052e-05,
        "epoch": 0.5789909015715468,
        "step": 4200
    },
    {
        "loss": 2.0334,
        "grad_norm": 1.295686960220337,
        "learning_rate": 3.325720532565308e-05,
        "epoch": 0.5791287565481114,
        "step": 4201
    },
    {
        "loss": 1.9695,
        "grad_norm": 1.2836512327194214,
        "learning_rate": 3.3160175470832066e-05,
        "epoch": 0.5792666115246761,
        "step": 4202
    },
    {
        "loss": 1.9922,
        "grad_norm": 1.5974700450897217,
        "learning_rate": 3.3063259227184415e-05,
        "epoch": 0.5794044665012407,
        "step": 4203
    },
    {
        "loss": 1.6077,
        "grad_norm": 2.4302141666412354,
        "learning_rate": 3.2966456759443584e-05,
        "epoch": 0.5795423214778054,
        "step": 4204
    },
    {
        "loss": 2.1519,
        "grad_norm": 2.0995824337005615,
        "learning_rate": 3.2869768232149987e-05,
        "epoch": 0.57968017645437,
        "step": 4205
    },
    {
        "loss": 2.1973,
        "grad_norm": 1.0545376539230347,
        "learning_rate": 3.277319380965024e-05,
        "epoch": 0.5798180314309347,
        "step": 4206
    },
    {
        "loss": 1.9038,
        "grad_norm": 1.0543246269226074,
        "learning_rate": 3.2676733656096836e-05,
        "epoch": 0.5799558864074993,
        "step": 4207
    },
    {
        "loss": 2.3846,
        "grad_norm": 1.3706998825073242,
        "learning_rate": 3.258038793544829e-05,
        "epoch": 0.580093741384064,
        "step": 4208
    },
    {
        "loss": 1.5416,
        "grad_norm": 1.7001457214355469,
        "learning_rate": 3.248415681146849e-05,
        "epoch": 0.5802315963606286,
        "step": 4209
    },
    {
        "loss": 1.4291,
        "grad_norm": 1.9259607791900635,
        "learning_rate": 3.238804044772644e-05,
        "epoch": 0.5803694513371933,
        "step": 4210
    },
    {
        "loss": 1.9286,
        "grad_norm": 1.5964094400405884,
        "learning_rate": 3.229203900759641e-05,
        "epoch": 0.5805073063137579,
        "step": 4211
    },
    {
        "loss": 2.2039,
        "grad_norm": 1.220973253250122,
        "learning_rate": 3.219615265425684e-05,
        "epoch": 0.5806451612903226,
        "step": 4212
    },
    {
        "loss": 0.8904,
        "grad_norm": 1.757468342781067,
        "learning_rate": 3.210038155069099e-05,
        "epoch": 0.5807830162668872,
        "step": 4213
    },
    {
        "loss": 1.2564,
        "grad_norm": 2.461500644683838,
        "learning_rate": 3.200472585968609e-05,
        "epoch": 0.5809208712434519,
        "step": 4214
    },
    {
        "loss": 2.0236,
        "grad_norm": 1.509421944618225,
        "learning_rate": 3.190918574383306e-05,
        "epoch": 0.5810587262200165,
        "step": 4215
    },
    {
        "loss": 1.9213,
        "grad_norm": 1.5184859037399292,
        "learning_rate": 3.1813761365526426e-05,
        "epoch": 0.5811965811965812,
        "step": 4216
    },
    {
        "loss": 1.7651,
        "grad_norm": 2.163609266281128,
        "learning_rate": 3.171845288696422e-05,
        "epoch": 0.5813344361731458,
        "step": 4217
    },
    {
        "loss": 2.0713,
        "grad_norm": 1.4904781579971313,
        "learning_rate": 3.162326047014708e-05,
        "epoch": 0.5814722911497106,
        "step": 4218
    },
    {
        "loss": 2.026,
        "grad_norm": 1.850403904914856,
        "learning_rate": 3.152818427687874e-05,
        "epoch": 0.5816101461262752,
        "step": 4219
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.3718827962875366,
        "learning_rate": 3.1433224468765165e-05,
        "epoch": 0.5817480011028399,
        "step": 4220
    },
    {
        "loss": 2.1829,
        "grad_norm": 1.6458839178085327,
        "learning_rate": 3.133838120721452e-05,
        "epoch": 0.5818858560794045,
        "step": 4221
    },
    {
        "loss": 1.5898,
        "grad_norm": 2.131931781768799,
        "learning_rate": 3.124365465343693e-05,
        "epoch": 0.5820237110559692,
        "step": 4222
    },
    {
        "loss": 1.5545,
        "grad_norm": 2.789656400680542,
        "learning_rate": 3.114904496844398e-05,
        "epoch": 0.5821615660325338,
        "step": 4223
    },
    {
        "loss": 1.6065,
        "grad_norm": 2.2037980556488037,
        "learning_rate": 3.105455231304888e-05,
        "epoch": 0.5822994210090985,
        "step": 4224
    },
    {
        "loss": 1.8767,
        "grad_norm": 1.681172251701355,
        "learning_rate": 3.0960176847865855e-05,
        "epoch": 0.5824372759856631,
        "step": 4225
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.944950819015503,
        "learning_rate": 3.086591873330966e-05,
        "epoch": 0.5825751309622277,
        "step": 4226
    },
    {
        "loss": 1.1804,
        "grad_norm": 2.3687918186187744,
        "learning_rate": 3.077177812959591e-05,
        "epoch": 0.5827129859387924,
        "step": 4227
    },
    {
        "loss": 1.1242,
        "grad_norm": 2.461183547973633,
        "learning_rate": 3.067775519674033e-05,
        "epoch": 0.582850840915357,
        "step": 4228
    },
    {
        "loss": 1.4279,
        "grad_norm": 2.138127565383911,
        "learning_rate": 3.058385009455855e-05,
        "epoch": 0.5829886958919217,
        "step": 4229
    },
    {
        "loss": 2.3921,
        "grad_norm": 1.5693538188934326,
        "learning_rate": 3.0490062982666125e-05,
        "epoch": 0.5831265508684863,
        "step": 4230
    },
    {
        "loss": 2.3582,
        "grad_norm": 2.1142940521240234,
        "learning_rate": 3.0396394020478024e-05,
        "epoch": 0.583264405845051,
        "step": 4231
    },
    {
        "loss": 1.9822,
        "grad_norm": 1.7406208515167236,
        "learning_rate": 3.03028433672082e-05,
        "epoch": 0.5834022608216156,
        "step": 4232
    },
    {
        "loss": 2.0799,
        "grad_norm": 1.3953449726104736,
        "learning_rate": 3.0209411181869706e-05,
        "epoch": 0.5835401157981803,
        "step": 4233
    },
    {
        "loss": 1.6237,
        "grad_norm": 2.7817394733428955,
        "learning_rate": 3.0116097623274076e-05,
        "epoch": 0.5836779707747449,
        "step": 4234
    },
    {
        "loss": 1.9836,
        "grad_norm": 1.6430327892303467,
        "learning_rate": 3.0022902850031253e-05,
        "epoch": 0.5838158257513096,
        "step": 4235
    },
    {
        "loss": 2.2582,
        "grad_norm": 1.3039711713790894,
        "learning_rate": 2.9929827020549473e-05,
        "epoch": 0.5839536807278742,
        "step": 4236
    },
    {
        "loss": 1.949,
        "grad_norm": 2.5982754230499268,
        "learning_rate": 2.9836870293034447e-05,
        "epoch": 0.584091535704439,
        "step": 4237
    },
    {
        "loss": 0.8854,
        "grad_norm": 2.133326530456543,
        "learning_rate": 2.9744032825489755e-05,
        "epoch": 0.5842293906810035,
        "step": 4238
    },
    {
        "loss": 1.5846,
        "grad_norm": 2.5232882499694824,
        "learning_rate": 2.965131477571613e-05,
        "epoch": 0.5843672456575683,
        "step": 4239
    },
    {
        "loss": 2.2986,
        "grad_norm": 1.5321235656738281,
        "learning_rate": 2.9558716301311262e-05,
        "epoch": 0.5845051006341329,
        "step": 4240
    },
    {
        "loss": 1.6702,
        "grad_norm": 2.5359444618225098,
        "learning_rate": 2.946623755966971e-05,
        "epoch": 0.5846429556106976,
        "step": 4241
    },
    {
        "loss": 1.6561,
        "grad_norm": 3.345491409301758,
        "learning_rate": 2.9373878707982338e-05,
        "epoch": 0.5847808105872622,
        "step": 4242
    },
    {
        "loss": 2.6456,
        "grad_norm": 1.1569911241531372,
        "learning_rate": 2.9281639903236468e-05,
        "epoch": 0.5849186655638269,
        "step": 4243
    },
    {
        "loss": 2.0664,
        "grad_norm": 1.8382676839828491,
        "learning_rate": 2.9189521302215338e-05,
        "epoch": 0.5850565205403915,
        "step": 4244
    },
    {
        "loss": 2.1706,
        "grad_norm": 1.7160307168960571,
        "learning_rate": 2.909752306149768e-05,
        "epoch": 0.5851943755169562,
        "step": 4245
    },
    {
        "loss": 2.1153,
        "grad_norm": 1.114853858947754,
        "learning_rate": 2.9005645337457766e-05,
        "epoch": 0.5853322304935208,
        "step": 4246
    },
    {
        "loss": 1.9513,
        "grad_norm": 1.9022892713546753,
        "learning_rate": 2.8913888286265056e-05,
        "epoch": 0.5854700854700855,
        "step": 4247
    },
    {
        "loss": 2.1704,
        "grad_norm": 1.8191673755645752,
        "learning_rate": 2.8822252063883705e-05,
        "epoch": 0.5856079404466501,
        "step": 4248
    },
    {
        "loss": 1.1483,
        "grad_norm": 1.9628913402557373,
        "learning_rate": 2.8730736826072735e-05,
        "epoch": 0.5857457954232148,
        "step": 4249
    },
    {
        "loss": 1.8496,
        "grad_norm": 2.1190273761749268,
        "learning_rate": 2.863934272838551e-05,
        "epoch": 0.5858836503997794,
        "step": 4250
    },
    {
        "loss": 1.6822,
        "grad_norm": 2.765526294708252,
        "learning_rate": 2.8548069926169274e-05,
        "epoch": 0.5860215053763441,
        "step": 4251
    },
    {
        "loss": 0.7849,
        "grad_norm": 1.848114013671875,
        "learning_rate": 2.8456918574565282e-05,
        "epoch": 0.5861593603529087,
        "step": 4252
    },
    {
        "loss": 1.8774,
        "grad_norm": 1.7457325458526611,
        "learning_rate": 2.8365888828508234e-05,
        "epoch": 0.5862972153294734,
        "step": 4253
    },
    {
        "loss": 1.853,
        "grad_norm": 1.3354803323745728,
        "learning_rate": 2.827498084272614e-05,
        "epoch": 0.586435070306038,
        "step": 4254
    },
    {
        "loss": 1.8949,
        "grad_norm": 1.1355029344558716,
        "learning_rate": 2.818419477174027e-05,
        "epoch": 0.5865729252826027,
        "step": 4255
    },
    {
        "loss": 2.2352,
        "grad_norm": 2.1053409576416016,
        "learning_rate": 2.8093530769864305e-05,
        "epoch": 0.5867107802591673,
        "step": 4256
    },
    {
        "loss": 2.2449,
        "grad_norm": 1.76978600025177,
        "learning_rate": 2.8002988991204772e-05,
        "epoch": 0.586848635235732,
        "step": 4257
    },
    {
        "loss": 1.0746,
        "grad_norm": 2.955359935760498,
        "learning_rate": 2.791256958966029e-05,
        "epoch": 0.5869864902122967,
        "step": 4258
    },
    {
        "loss": 1.3805,
        "grad_norm": 2.1929779052734375,
        "learning_rate": 2.7822272718921372e-05,
        "epoch": 0.5871243451888614,
        "step": 4259
    },
    {
        "loss": 2.0594,
        "grad_norm": 1.8948490619659424,
        "learning_rate": 2.773209853247053e-05,
        "epoch": 0.587262200165426,
        "step": 4260
    },
    {
        "loss": 1.0742,
        "grad_norm": 2.6504569053649902,
        "learning_rate": 2.7642047183581377e-05,
        "epoch": 0.5874000551419907,
        "step": 4261
    },
    {
        "loss": 2.6323,
        "grad_norm": 2.522763967514038,
        "learning_rate": 2.755211882531906e-05,
        "epoch": 0.5875379101185553,
        "step": 4262
    },
    {
        "loss": 1.1753,
        "grad_norm": 1.8901283740997314,
        "learning_rate": 2.7462313610539626e-05,
        "epoch": 0.58767576509512,
        "step": 4263
    },
    {
        "loss": 1.633,
        "grad_norm": 1.7190157175064087,
        "learning_rate": 2.7372631691889628e-05,
        "epoch": 0.5878136200716846,
        "step": 4264
    },
    {
        "loss": 1.7325,
        "grad_norm": 3.7261533737182617,
        "learning_rate": 2.7283073221806134e-05,
        "epoch": 0.5879514750482493,
        "step": 4265
    },
    {
        "loss": 2.1296,
        "grad_norm": 1.8189855813980103,
        "learning_rate": 2.7193638352516502e-05,
        "epoch": 0.5880893300248139,
        "step": 4266
    },
    {
        "loss": 2.3784,
        "grad_norm": 1.9026540517807007,
        "learning_rate": 2.7104327236037763e-05,
        "epoch": 0.5882271850013786,
        "step": 4267
    },
    {
        "loss": 2.0736,
        "grad_norm": 1.531492829322815,
        "learning_rate": 2.7015140024176778e-05,
        "epoch": 0.5883650399779432,
        "step": 4268
    },
    {
        "loss": 1.9576,
        "grad_norm": 1.2546106576919556,
        "learning_rate": 2.6926076868529848e-05,
        "epoch": 0.5885028949545078,
        "step": 4269
    },
    {
        "loss": 1.5793,
        "grad_norm": 2.7080483436584473,
        "learning_rate": 2.683713792048218e-05,
        "epoch": 0.5886407499310725,
        "step": 4270
    },
    {
        "loss": 2.1496,
        "grad_norm": 1.294647216796875,
        "learning_rate": 2.6748323331208137e-05,
        "epoch": 0.5887786049076371,
        "step": 4271
    },
    {
        "loss": 1.8041,
        "grad_norm": 1.8323875665664673,
        "learning_rate": 2.665963325167038e-05,
        "epoch": 0.5889164598842018,
        "step": 4272
    },
    {
        "loss": 1.5282,
        "grad_norm": 3.1453168392181396,
        "learning_rate": 2.6571067832620167e-05,
        "epoch": 0.5890543148607664,
        "step": 4273
    },
    {
        "loss": 2.2736,
        "grad_norm": 1.717390775680542,
        "learning_rate": 2.6482627224596966e-05,
        "epoch": 0.5891921698373311,
        "step": 4274
    },
    {
        "loss": 1.878,
        "grad_norm": 1.3815099000930786,
        "learning_rate": 2.63943115779277e-05,
        "epoch": 0.5893300248138957,
        "step": 4275
    },
    {
        "loss": 2.1896,
        "grad_norm": 1.9715039730072021,
        "learning_rate": 2.6306121042727282e-05,
        "epoch": 0.5894678797904604,
        "step": 4276
    },
    {
        "loss": 1.9239,
        "grad_norm": 2.120795249938965,
        "learning_rate": 2.6218055768897864e-05,
        "epoch": 0.589605734767025,
        "step": 4277
    },
    {
        "loss": 1.105,
        "grad_norm": 2.1680757999420166,
        "learning_rate": 2.613011590612847e-05,
        "epoch": 0.5897435897435898,
        "step": 4278
    },
    {
        "loss": 1.3713,
        "grad_norm": 2.7979776859283447,
        "learning_rate": 2.6042301603895246e-05,
        "epoch": 0.5898814447201544,
        "step": 4279
    },
    {
        "loss": 2.1483,
        "grad_norm": 1.740860104560852,
        "learning_rate": 2.595461301146067e-05,
        "epoch": 0.5900192996967191,
        "step": 4280
    },
    {
        "loss": 1.9336,
        "grad_norm": 1.378705620765686,
        "learning_rate": 2.5867050277873737e-05,
        "epoch": 0.5901571546732837,
        "step": 4281
    },
    {
        "loss": 1.9472,
        "grad_norm": 2.022503137588501,
        "learning_rate": 2.577961355196956e-05,
        "epoch": 0.5902950096498484,
        "step": 4282
    },
    {
        "loss": 1.5182,
        "grad_norm": 2.1795296669006348,
        "learning_rate": 2.5692302982368755e-05,
        "epoch": 0.590432864626413,
        "step": 4283
    },
    {
        "loss": 1.6311,
        "grad_norm": 2.255636215209961,
        "learning_rate": 2.5605118717477804e-05,
        "epoch": 0.5905707196029777,
        "step": 4284
    },
    {
        "loss": 1.7802,
        "grad_norm": 2.2954702377319336,
        "learning_rate": 2.551806090548842e-05,
        "epoch": 0.5907085745795423,
        "step": 4285
    },
    {
        "loss": 2.4363,
        "grad_norm": 1.4063501358032227,
        "learning_rate": 2.5431129694377197e-05,
        "epoch": 0.590846429556107,
        "step": 4286
    },
    {
        "loss": 1.513,
        "grad_norm": 2.865668773651123,
        "learning_rate": 2.534432523190583e-05,
        "epoch": 0.5909842845326716,
        "step": 4287
    },
    {
        "loss": 2.1078,
        "grad_norm": 2.3244926929473877,
        "learning_rate": 2.5257647665620532e-05,
        "epoch": 0.5911221395092363,
        "step": 4288
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.5385355949401855,
        "learning_rate": 2.517109714285153e-05,
        "epoch": 0.5912599944858009,
        "step": 4289
    },
    {
        "loss": 1.0824,
        "grad_norm": 1.938768982887268,
        "learning_rate": 2.5084673810713465e-05,
        "epoch": 0.5913978494623656,
        "step": 4290
    },
    {
        "loss": 2.4897,
        "grad_norm": 1.2886415719985962,
        "learning_rate": 2.4998377816104533e-05,
        "epoch": 0.5915357044389302,
        "step": 4291
    },
    {
        "loss": 1.4729,
        "grad_norm": 1.7063827514648438,
        "learning_rate": 2.4912209305706567e-05,
        "epoch": 0.5916735594154949,
        "step": 4292
    },
    {
        "loss": 1.7362,
        "grad_norm": 1.5229732990264893,
        "learning_rate": 2.4826168425984865e-05,
        "epoch": 0.5918114143920595,
        "step": 4293
    },
    {
        "loss": 2.2431,
        "grad_norm": 1.3524706363677979,
        "learning_rate": 2.4740255323187465e-05,
        "epoch": 0.5919492693686242,
        "step": 4294
    },
    {
        "loss": 1.918,
        "grad_norm": 1.8926138877868652,
        "learning_rate": 2.4654470143345555e-05,
        "epoch": 0.5920871243451888,
        "step": 4295
    },
    {
        "loss": 0.8197,
        "grad_norm": 2.648022174835205,
        "learning_rate": 2.4568813032272687e-05,
        "epoch": 0.5922249793217536,
        "step": 4296
    },
    {
        "loss": 2.4273,
        "grad_norm": 1.4006245136260986,
        "learning_rate": 2.448328413556471e-05,
        "epoch": 0.5923628342983182,
        "step": 4297
    },
    {
        "loss": 2.3233,
        "grad_norm": 1.0616469383239746,
        "learning_rate": 2.439788359859968e-05,
        "epoch": 0.5925006892748829,
        "step": 4298
    },
    {
        "loss": 1.4132,
        "grad_norm": 4.191947937011719,
        "learning_rate": 2.4312611566537247e-05,
        "epoch": 0.5926385442514475,
        "step": 4299
    },
    {
        "loss": 1.9494,
        "grad_norm": 1.8669553995132446,
        "learning_rate": 2.422746818431887e-05,
        "epoch": 0.5927763992280122,
        "step": 4300
    },
    {
        "loss": 2.1576,
        "grad_norm": 2.6444907188415527,
        "learning_rate": 2.4142453596667348e-05,
        "epoch": 0.5929142542045768,
        "step": 4301
    },
    {
        "loss": 2.223,
        "grad_norm": 1.35531485080719,
        "learning_rate": 2.4057567948086324e-05,
        "epoch": 0.5930521091811415,
        "step": 4302
    },
    {
        "loss": 1.6207,
        "grad_norm": 1.8397811651229858,
        "learning_rate": 2.397281138286045e-05,
        "epoch": 0.5931899641577061,
        "step": 4303
    },
    {
        "loss": 1.7915,
        "grad_norm": 2.7459208965301514,
        "learning_rate": 2.3888184045054962e-05,
        "epoch": 0.5933278191342708,
        "step": 4304
    },
    {
        "loss": 2.4166,
        "grad_norm": 1.0366677045822144,
        "learning_rate": 2.3803686078515274e-05,
        "epoch": 0.5934656741108354,
        "step": 4305
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.3479276895523071,
        "learning_rate": 2.371931762686712e-05,
        "epoch": 0.5936035290874001,
        "step": 4306
    },
    {
        "loss": 2.3264,
        "grad_norm": 2.5978333950042725,
        "learning_rate": 2.363507883351611e-05,
        "epoch": 0.5937413840639647,
        "step": 4307
    },
    {
        "loss": 1.8658,
        "grad_norm": 2.180866003036499,
        "learning_rate": 2.355096984164724e-05,
        "epoch": 0.5938792390405294,
        "step": 4308
    },
    {
        "loss": 1.8909,
        "grad_norm": 2.157527446746826,
        "learning_rate": 2.346699079422505e-05,
        "epoch": 0.594017094017094,
        "step": 4309
    },
    {
        "loss": 1.6199,
        "grad_norm": 1.927591323852539,
        "learning_rate": 2.3383141833993094e-05,
        "epoch": 0.5941549489936586,
        "step": 4310
    },
    {
        "loss": 2.1917,
        "grad_norm": 2.8121602535247803,
        "learning_rate": 2.3299423103473862e-05,
        "epoch": 0.5942928039702233,
        "step": 4311
    },
    {
        "loss": 2.4098,
        "grad_norm": 1.4833450317382812,
        "learning_rate": 2.32158347449686e-05,
        "epoch": 0.5944306589467879,
        "step": 4312
    },
    {
        "loss": 1.5661,
        "grad_norm": 1.3455363512039185,
        "learning_rate": 2.313237690055671e-05,
        "epoch": 0.5945685139233526,
        "step": 4313
    },
    {
        "loss": 2.1214,
        "grad_norm": 1.3709335327148438,
        "learning_rate": 2.3049049712096014e-05,
        "epoch": 0.5947063688999172,
        "step": 4314
    },
    {
        "loss": 1.7352,
        "grad_norm": 2.230673313140869,
        "learning_rate": 2.2965853321222098e-05,
        "epoch": 0.594844223876482,
        "step": 4315
    },
    {
        "loss": 1.5392,
        "grad_norm": 2.0022811889648438,
        "learning_rate": 2.288278786934821e-05,
        "epoch": 0.5949820788530465,
        "step": 4316
    },
    {
        "loss": 1.8819,
        "grad_norm": 1.8209314346313477,
        "learning_rate": 2.2799853497665124e-05,
        "epoch": 0.5951199338296113,
        "step": 4317
    },
    {
        "loss": 1.1631,
        "grad_norm": 1.8702449798583984,
        "learning_rate": 2.2717050347140644e-05,
        "epoch": 0.5952577888061759,
        "step": 4318
    },
    {
        "loss": 1.3792,
        "grad_norm": 1.977431058883667,
        "learning_rate": 2.2634378558519754e-05,
        "epoch": 0.5953956437827406,
        "step": 4319
    },
    {
        "loss": 0.9035,
        "grad_norm": 1.985312581062317,
        "learning_rate": 2.2551838272324122e-05,
        "epoch": 0.5955334987593052,
        "step": 4320
    },
    {
        "loss": 2.1239,
        "grad_norm": 2.2767863273620605,
        "learning_rate": 2.2469429628851712e-05,
        "epoch": 0.5956713537358699,
        "step": 4321
    },
    {
        "loss": 1.4757,
        "grad_norm": 2.506216287612915,
        "learning_rate": 2.2387152768176843e-05,
        "epoch": 0.5958092087124345,
        "step": 4322
    },
    {
        "loss": 1.7372,
        "grad_norm": 1.5939960479736328,
        "learning_rate": 2.2305007830149947e-05,
        "epoch": 0.5959470636889992,
        "step": 4323
    },
    {
        "loss": 2.1239,
        "grad_norm": 1.7797303199768066,
        "learning_rate": 2.22229949543969e-05,
        "epoch": 0.5960849186655638,
        "step": 4324
    },
    {
        "loss": 2.5586,
        "grad_norm": 1.2213298082351685,
        "learning_rate": 2.214111428031943e-05,
        "epoch": 0.5962227736421285,
        "step": 4325
    },
    {
        "loss": 1.7003,
        "grad_norm": 2.0370371341705322,
        "learning_rate": 2.2059365947094523e-05,
        "epoch": 0.5963606286186931,
        "step": 4326
    },
    {
        "loss": 1.7978,
        "grad_norm": 1.9191398620605469,
        "learning_rate": 2.1977750093674e-05,
        "epoch": 0.5964984835952578,
        "step": 4327
    },
    {
        "loss": 2.3923,
        "grad_norm": 1.5693895816802979,
        "learning_rate": 2.189626685878471e-05,
        "epoch": 0.5966363385718224,
        "step": 4328
    },
    {
        "loss": 2.2572,
        "grad_norm": 1.3187870979309082,
        "learning_rate": 2.1814916380927898e-05,
        "epoch": 0.5967741935483871,
        "step": 4329
    },
    {
        "loss": 1.4285,
        "grad_norm": 2.1758475303649902,
        "learning_rate": 2.173369879837932e-05,
        "epoch": 0.5969120485249517,
        "step": 4330
    },
    {
        "loss": 1.7969,
        "grad_norm": 2.163055181503296,
        "learning_rate": 2.165261424918885e-05,
        "epoch": 0.5970499035015164,
        "step": 4331
    },
    {
        "loss": 1.0792,
        "grad_norm": 1.9895539283752441,
        "learning_rate": 2.1571662871180032e-05,
        "epoch": 0.597187758478081,
        "step": 4332
    },
    {
        "loss": 1.5899,
        "grad_norm": 2.181236505508423,
        "learning_rate": 2.1490844801950284e-05,
        "epoch": 0.5973256134546457,
        "step": 4333
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.5545517206192017,
        "learning_rate": 2.1410160178870408e-05,
        "epoch": 0.5974634684312103,
        "step": 4334
    },
    {
        "loss": 1.0621,
        "grad_norm": 2.3434321880340576,
        "learning_rate": 2.132960913908414e-05,
        "epoch": 0.597601323407775,
        "step": 4335
    },
    {
        "loss": 2.1836,
        "grad_norm": 1.7599364519119263,
        "learning_rate": 2.1249191819508495e-05,
        "epoch": 0.5977391783843397,
        "step": 4336
    },
    {
        "loss": 1.5632,
        "grad_norm": 1.8276246786117554,
        "learning_rate": 2.116890835683284e-05,
        "epoch": 0.5978770333609044,
        "step": 4337
    },
    {
        "loss": 1.2909,
        "grad_norm": 1.8479218482971191,
        "learning_rate": 2.1088758887519322e-05,
        "epoch": 0.598014888337469,
        "step": 4338
    },
    {
        "loss": 2.4076,
        "grad_norm": 1.4617642164230347,
        "learning_rate": 2.100874354780229e-05,
        "epoch": 0.5981527433140337,
        "step": 4339
    },
    {
        "loss": 2.2807,
        "grad_norm": 2.726461172103882,
        "learning_rate": 2.0928862473687927e-05,
        "epoch": 0.5982905982905983,
        "step": 4340
    },
    {
        "loss": 1.9201,
        "grad_norm": 1.4877876043319702,
        "learning_rate": 2.0849115800954322e-05,
        "epoch": 0.598428453267163,
        "step": 4341
    },
    {
        "loss": 1.5695,
        "grad_norm": 1.6495076417922974,
        "learning_rate": 2.0769503665151158e-05,
        "epoch": 0.5985663082437276,
        "step": 4342
    },
    {
        "loss": 2.5062,
        "grad_norm": 1.4611396789550781,
        "learning_rate": 2.0690026201599254e-05,
        "epoch": 0.5987041632202923,
        "step": 4343
    },
    {
        "loss": 2.2363,
        "grad_norm": 2.163343667984009,
        "learning_rate": 2.061068354539076e-05,
        "epoch": 0.5988420181968569,
        "step": 4344
    },
    {
        "loss": 2.3236,
        "grad_norm": 1.6687273979187012,
        "learning_rate": 2.053147583138859e-05,
        "epoch": 0.5989798731734216,
        "step": 4345
    },
    {
        "loss": 1.9997,
        "grad_norm": 1.5978220701217651,
        "learning_rate": 2.0452403194226165e-05,
        "epoch": 0.5991177281499862,
        "step": 4346
    },
    {
        "loss": 1.9871,
        "grad_norm": 2.7427923679351807,
        "learning_rate": 2.0373465768307552e-05,
        "epoch": 0.5992555831265509,
        "step": 4347
    },
    {
        "loss": 2.1378,
        "grad_norm": 1.890775442123413,
        "learning_rate": 2.029466368780669e-05,
        "epoch": 0.5993934381031155,
        "step": 4348
    },
    {
        "loss": 1.5048,
        "grad_norm": 2.6916022300720215,
        "learning_rate": 2.0215997086667682e-05,
        "epoch": 0.5995312930796802,
        "step": 4349
    },
    {
        "loss": 0.7052,
        "grad_norm": 2.4267361164093018,
        "learning_rate": 2.013746609860443e-05,
        "epoch": 0.5996691480562448,
        "step": 4350
    },
    {
        "loss": 2.0647,
        "grad_norm": 2.200582504272461,
        "learning_rate": 2.005907085709998e-05,
        "epoch": 0.5998070030328095,
        "step": 4351
    },
    {
        "loss": 1.0178,
        "grad_norm": 2.4358866214752197,
        "learning_rate": 1.9980811495407003e-05,
        "epoch": 0.5999448580093741,
        "step": 4352
    },
    {
        "loss": 2.5032,
        "grad_norm": 1.9350712299346924,
        "learning_rate": 1.9902688146547077e-05,
        "epoch": 0.6000827129859387,
        "step": 4353
    },
    {
        "loss": 1.4819,
        "grad_norm": 2.0659866333007812,
        "learning_rate": 1.9824700943310493e-05,
        "epoch": 0.6002205679625034,
        "step": 4354
    },
    {
        "loss": 1.9939,
        "grad_norm": 1.5165802240371704,
        "learning_rate": 1.974685001825627e-05,
        "epoch": 0.600358422939068,
        "step": 4355
    },
    {
        "loss": 0.7103,
        "grad_norm": 2.0854270458221436,
        "learning_rate": 1.9669135503711632e-05,
        "epoch": 0.6004962779156328,
        "step": 4356
    },
    {
        "loss": 1.4422,
        "grad_norm": 2.421570301055908,
        "learning_rate": 1.95915575317721e-05,
        "epoch": 0.6006341328921974,
        "step": 4357
    },
    {
        "loss": 1.7752,
        "grad_norm": 2.4661147594451904,
        "learning_rate": 1.951411623430115e-05,
        "epoch": 0.6007719878687621,
        "step": 4358
    },
    {
        "loss": 2.0753,
        "grad_norm": 1.9749364852905273,
        "learning_rate": 1.943681174292966e-05,
        "epoch": 0.6009098428453267,
        "step": 4359
    },
    {
        "loss": 1.1297,
        "grad_norm": 3.1044461727142334,
        "learning_rate": 1.935964418905626e-05,
        "epoch": 0.6010476978218914,
        "step": 4360
    },
    {
        "loss": 2.0922,
        "grad_norm": 2.355680227279663,
        "learning_rate": 1.9282613703846696e-05,
        "epoch": 0.601185552798456,
        "step": 4361
    },
    {
        "loss": 1.8969,
        "grad_norm": 1.4689972400665283,
        "learning_rate": 1.920572041823364e-05,
        "epoch": 0.6013234077750207,
        "step": 4362
    },
    {
        "loss": 1.4969,
        "grad_norm": 2.4497499465942383,
        "learning_rate": 1.9128964462916754e-05,
        "epoch": 0.6014612627515853,
        "step": 4363
    },
    {
        "loss": 1.4049,
        "grad_norm": 1.9823684692382812,
        "learning_rate": 1.9052345968362252e-05,
        "epoch": 0.60159911772815,
        "step": 4364
    },
    {
        "loss": 1.9836,
        "grad_norm": 2.5309817790985107,
        "learning_rate": 1.8975865064802523e-05,
        "epoch": 0.6017369727047146,
        "step": 4365
    },
    {
        "loss": 1.4043,
        "grad_norm": 3.0677967071533203,
        "learning_rate": 1.8899521882236248e-05,
        "epoch": 0.6018748276812793,
        "step": 4366
    },
    {
        "loss": 0.512,
        "grad_norm": 1.7475666999816895,
        "learning_rate": 1.8823316550427895e-05,
        "epoch": 0.6020126826578439,
        "step": 4367
    },
    {
        "loss": 1.9484,
        "grad_norm": 1.4072840213775635,
        "learning_rate": 1.874724919890768e-05,
        "epoch": 0.6021505376344086,
        "step": 4368
    },
    {
        "loss": 0.8559,
        "grad_norm": 2.0726230144500732,
        "learning_rate": 1.867131995697139e-05,
        "epoch": 0.6022883926109732,
        "step": 4369
    },
    {
        "loss": 1.4058,
        "grad_norm": 1.9382672309875488,
        "learning_rate": 1.859552895367983e-05,
        "epoch": 0.6024262475875379,
        "step": 4370
    },
    {
        "loss": 1.8029,
        "grad_norm": 1.5120790004730225,
        "learning_rate": 1.8519876317859088e-05,
        "epoch": 0.6025641025641025,
        "step": 4371
    },
    {
        "loss": 1.7725,
        "grad_norm": 1.8676587343215942,
        "learning_rate": 1.8444362178099894e-05,
        "epoch": 0.6027019575406672,
        "step": 4372
    },
    {
        "loss": 1.7086,
        "grad_norm": 2.1485376358032227,
        "learning_rate": 1.836898666275758e-05,
        "epoch": 0.6028398125172318,
        "step": 4373
    },
    {
        "loss": 1.6618,
        "grad_norm": 1.8715596199035645,
        "learning_rate": 1.829374989995193e-05,
        "epoch": 0.6029776674937966,
        "step": 4374
    },
    {
        "loss": 1.9836,
        "grad_norm": 3.2706592082977295,
        "learning_rate": 1.821865201756673e-05,
        "epoch": 0.6031155224703612,
        "step": 4375
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.6184849739074707,
        "learning_rate": 1.8143693143249885e-05,
        "epoch": 0.6032533774469259,
        "step": 4376
    },
    {
        "loss": 2.434,
        "grad_norm": 1.7437931299209595,
        "learning_rate": 1.8068873404413023e-05,
        "epoch": 0.6033912324234905,
        "step": 4377
    },
    {
        "loss": 0.5661,
        "grad_norm": 2.1633028984069824,
        "learning_rate": 1.79941929282311e-05,
        "epoch": 0.6035290874000552,
        "step": 4378
    },
    {
        "loss": 1.8013,
        "grad_norm": 2.569911241531372,
        "learning_rate": 1.7919651841642494e-05,
        "epoch": 0.6036669423766198,
        "step": 4379
    },
    {
        "loss": 1.8427,
        "grad_norm": 1.7890113592147827,
        "learning_rate": 1.7845250271348624e-05,
        "epoch": 0.6038047973531845,
        "step": 4380
    },
    {
        "loss": 2.5548,
        "grad_norm": 1.2766577005386353,
        "learning_rate": 1.7770988343813665e-05,
        "epoch": 0.6039426523297491,
        "step": 4381
    },
    {
        "loss": 1.1695,
        "grad_norm": 1.401640772819519,
        "learning_rate": 1.7696866185264605e-05,
        "epoch": 0.6040805073063138,
        "step": 4382
    },
    {
        "loss": 2.2731,
        "grad_norm": 1.7112592458724976,
        "learning_rate": 1.762288392169087e-05,
        "epoch": 0.6042183622828784,
        "step": 4383
    },
    {
        "loss": 2.2632,
        "grad_norm": 1.8812377452850342,
        "learning_rate": 1.7549041678843913e-05,
        "epoch": 0.6043562172594431,
        "step": 4384
    },
    {
        "loss": 2.0488,
        "grad_norm": 1.5684400796890259,
        "learning_rate": 1.7475339582237314e-05,
        "epoch": 0.6044940722360077,
        "step": 4385
    },
    {
        "loss": 1.2423,
        "grad_norm": 1.6754543781280518,
        "learning_rate": 1.7401777757146397e-05,
        "epoch": 0.6046319272125724,
        "step": 4386
    },
    {
        "loss": 1.7445,
        "grad_norm": 1.7818683385849,
        "learning_rate": 1.732835632860802e-05,
        "epoch": 0.604769782189137,
        "step": 4387
    },
    {
        "loss": 1.6903,
        "grad_norm": 2.9780631065368652,
        "learning_rate": 1.7255075421420597e-05,
        "epoch": 0.6049076371657017,
        "step": 4388
    },
    {
        "loss": 2.0429,
        "grad_norm": 1.5995466709136963,
        "learning_rate": 1.718193516014339e-05,
        "epoch": 0.6050454921422663,
        "step": 4389
    },
    {
        "loss": 2.2561,
        "grad_norm": 1.5806639194488525,
        "learning_rate": 1.7108935669096882e-05,
        "epoch": 0.605183347118831,
        "step": 4390
    },
    {
        "loss": 2.0694,
        "grad_norm": 1.5615944862365723,
        "learning_rate": 1.7036077072362145e-05,
        "epoch": 0.6053212020953956,
        "step": 4391
    },
    {
        "loss": 2.194,
        "grad_norm": 1.7351902723312378,
        "learning_rate": 1.6963359493780705e-05,
        "epoch": 0.6054590570719603,
        "step": 4392
    },
    {
        "loss": 2.2704,
        "grad_norm": 1.1940895318984985,
        "learning_rate": 1.6890783056954452e-05,
        "epoch": 0.605596912048525,
        "step": 4393
    },
    {
        "loss": 1.9052,
        "grad_norm": 2.5376949310302734,
        "learning_rate": 1.681834788524551e-05,
        "epoch": 0.6057347670250897,
        "step": 4394
    },
    {
        "loss": 1.6096,
        "grad_norm": 1.3948081731796265,
        "learning_rate": 1.67460541017756e-05,
        "epoch": 0.6058726220016543,
        "step": 4395
    },
    {
        "loss": 1.2755,
        "grad_norm": 1.714368462562561,
        "learning_rate": 1.6673901829426418e-05,
        "epoch": 0.6060104769782189,
        "step": 4396
    },
    {
        "loss": 1.6919,
        "grad_norm": 2.484703540802002,
        "learning_rate": 1.6601891190838915e-05,
        "epoch": 0.6061483319547836,
        "step": 4397
    },
    {
        "loss": 1.7139,
        "grad_norm": 1.4505972862243652,
        "learning_rate": 1.653002230841334e-05,
        "epoch": 0.6062861869313482,
        "step": 4398
    },
    {
        "loss": 1.6082,
        "grad_norm": 4.886074066162109,
        "learning_rate": 1.6458295304309134e-05,
        "epoch": 0.6064240419079129,
        "step": 4399
    },
    {
        "loss": 1.0571,
        "grad_norm": 2.302828073501587,
        "learning_rate": 1.6386710300444298e-05,
        "epoch": 0.6065618968844775,
        "step": 4400
    },
    {
        "loss": 2.0231,
        "grad_norm": 1.3561222553253174,
        "learning_rate": 1.631526741849573e-05,
        "epoch": 0.6066997518610422,
        "step": 4401
    },
    {
        "loss": 2.4552,
        "grad_norm": 1.6398041248321533,
        "learning_rate": 1.624396677989872e-05,
        "epoch": 0.6068376068376068,
        "step": 4402
    },
    {
        "loss": 2.0151,
        "grad_norm": 2.6880760192871094,
        "learning_rate": 1.617280850584666e-05,
        "epoch": 0.6069754618141715,
        "step": 4403
    },
    {
        "loss": 2.0988,
        "grad_norm": 1.5099246501922607,
        "learning_rate": 1.610179271729103e-05,
        "epoch": 0.6071133167907361,
        "step": 4404
    },
    {
        "loss": 2.1895,
        "grad_norm": 1.8198988437652588,
        "learning_rate": 1.603091953494107e-05,
        "epoch": 0.6072511717673008,
        "step": 4405
    },
    {
        "loss": 2.1647,
        "grad_norm": 2.2275450229644775,
        "learning_rate": 1.5960189079263666e-05,
        "epoch": 0.6073890267438654,
        "step": 4406
    },
    {
        "loss": 1.3615,
        "grad_norm": 2.4323692321777344,
        "learning_rate": 1.5889601470483228e-05,
        "epoch": 0.6075268817204301,
        "step": 4407
    },
    {
        "loss": 2.4463,
        "grad_norm": 2.481699228286743,
        "learning_rate": 1.5819156828581038e-05,
        "epoch": 0.6076647366969947,
        "step": 4408
    },
    {
        "loss": 1.1985,
        "grad_norm": 1.8682202100753784,
        "learning_rate": 1.5748855273295703e-05,
        "epoch": 0.6078025916735594,
        "step": 4409
    },
    {
        "loss": 1.6411,
        "grad_norm": 2.1011526584625244,
        "learning_rate": 1.5678696924122526e-05,
        "epoch": 0.607940446650124,
        "step": 4410
    },
    {
        "loss": 1.6662,
        "grad_norm": 1.9828546047210693,
        "learning_rate": 1.5608681900313203e-05,
        "epoch": 0.6080783016266887,
        "step": 4411
    },
    {
        "loss": 1.9548,
        "grad_norm": 1.7996424436569214,
        "learning_rate": 1.5538810320876008e-05,
        "epoch": 0.6082161566032533,
        "step": 4412
    },
    {
        "loss": 1.8294,
        "grad_norm": 1.9736961126327515,
        "learning_rate": 1.5469082304575465e-05,
        "epoch": 0.608354011579818,
        "step": 4413
    },
    {
        "loss": 1.3123,
        "grad_norm": 2.0395798683166504,
        "learning_rate": 1.5399497969931775e-05,
        "epoch": 0.6084918665563827,
        "step": 4414
    },
    {
        "loss": 1.93,
        "grad_norm": 1.4256130456924438,
        "learning_rate": 1.5330057435221256e-05,
        "epoch": 0.6086297215329474,
        "step": 4415
    },
    {
        "loss": 1.7536,
        "grad_norm": 2.589764356613159,
        "learning_rate": 1.5260760818475552e-05,
        "epoch": 0.608767576509512,
        "step": 4416
    },
    {
        "loss": 2.1037,
        "grad_norm": 2.073629856109619,
        "learning_rate": 1.5191608237481813e-05,
        "epoch": 0.6089054314860767,
        "step": 4417
    },
    {
        "loss": 2.1463,
        "grad_norm": 1.5598515272140503,
        "learning_rate": 1.5122599809782345e-05,
        "epoch": 0.6090432864626413,
        "step": 4418
    },
    {
        "loss": 1.9915,
        "grad_norm": 2.015165328979492,
        "learning_rate": 1.5053735652674318e-05,
        "epoch": 0.609181141439206,
        "step": 4419
    },
    {
        "loss": 1.8433,
        "grad_norm": 0.9498539566993713,
        "learning_rate": 1.498501588320984e-05,
        "epoch": 0.6093189964157706,
        "step": 4420
    },
    {
        "loss": 1.7327,
        "grad_norm": 2.394554376602173,
        "learning_rate": 1.4916440618195615e-05,
        "epoch": 0.6094568513923353,
        "step": 4421
    },
    {
        "loss": 2.1501,
        "grad_norm": 1.7002884149551392,
        "learning_rate": 1.4848009974192468e-05,
        "epoch": 0.6095947063688999,
        "step": 4422
    },
    {
        "loss": 2.221,
        "grad_norm": 1.858432412147522,
        "learning_rate": 1.477972406751571e-05,
        "epoch": 0.6097325613454646,
        "step": 4423
    },
    {
        "loss": 1.3886,
        "grad_norm": 1.5693210363388062,
        "learning_rate": 1.4711583014234375e-05,
        "epoch": 0.6098704163220292,
        "step": 4424
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.6893118619918823,
        "learning_rate": 1.4643586930171438e-05,
        "epoch": 0.6100082712985939,
        "step": 4425
    },
    {
        "loss": 1.9944,
        "grad_norm": 1.9097758531570435,
        "learning_rate": 1.4575735930903511e-05,
        "epoch": 0.6101461262751585,
        "step": 4426
    },
    {
        "loss": 1.9826,
        "grad_norm": 1.1898877620697021,
        "learning_rate": 1.450803013176042e-05,
        "epoch": 0.6102839812517232,
        "step": 4427
    },
    {
        "loss": 1.9649,
        "grad_norm": 1.932136058807373,
        "learning_rate": 1.4440469647825305e-05,
        "epoch": 0.6104218362282878,
        "step": 4428
    },
    {
        "loss": 0.7253,
        "grad_norm": 1.8523448705673218,
        "learning_rate": 1.4373054593934355e-05,
        "epoch": 0.6105596912048525,
        "step": 4429
    },
    {
        "loss": 1.9544,
        "grad_norm": 1.6727253198623657,
        "learning_rate": 1.4305785084676393e-05,
        "epoch": 0.6106975461814171,
        "step": 4430
    },
    {
        "loss": 1.6079,
        "grad_norm": 2.3808963298797607,
        "learning_rate": 1.4238661234392936e-05,
        "epoch": 0.6108354011579818,
        "step": 4431
    },
    {
        "loss": 2.4457,
        "grad_norm": 1.3368849754333496,
        "learning_rate": 1.4171683157178062e-05,
        "epoch": 0.6109732561345464,
        "step": 4432
    },
    {
        "loss": 1.7379,
        "grad_norm": 1.151100516319275,
        "learning_rate": 1.410485096687778e-05,
        "epoch": 0.6111111111111112,
        "step": 4433
    },
    {
        "loss": 1.6367,
        "grad_norm": 2.022146701812744,
        "learning_rate": 1.4038164777090457e-05,
        "epoch": 0.6112489660876758,
        "step": 4434
    },
    {
        "loss": 1.0642,
        "grad_norm": 1.413895606994629,
        "learning_rate": 1.3971624701165963e-05,
        "epoch": 0.6113868210642405,
        "step": 4435
    },
    {
        "loss": 1.8621,
        "grad_norm": 2.844464063644409,
        "learning_rate": 1.3905230852206075e-05,
        "epoch": 0.6115246760408051,
        "step": 4436
    },
    {
        "loss": 2.2436,
        "grad_norm": 1.1478875875473022,
        "learning_rate": 1.3838983343063905e-05,
        "epoch": 0.6116625310173698,
        "step": 4437
    },
    {
        "loss": 2.2065,
        "grad_norm": 1.052356243133545,
        "learning_rate": 1.3772882286343747e-05,
        "epoch": 0.6118003859939344,
        "step": 4438
    },
    {
        "loss": 1.6646,
        "grad_norm": 2.051438093185425,
        "learning_rate": 1.3706927794401125e-05,
        "epoch": 0.611938240970499,
        "step": 4439
    },
    {
        "loss": 1.8761,
        "grad_norm": 2.2890970706939697,
        "learning_rate": 1.3641119979342432e-05,
        "epoch": 0.6120760959470637,
        "step": 4440
    },
    {
        "loss": 1.5602,
        "grad_norm": 2.3254551887512207,
        "learning_rate": 1.357545895302459e-05,
        "epoch": 0.6122139509236283,
        "step": 4441
    },
    {
        "loss": 2.2276,
        "grad_norm": 2.1782681941986084,
        "learning_rate": 1.3509944827055155e-05,
        "epoch": 0.612351805900193,
        "step": 4442
    },
    {
        "loss": 1.5495,
        "grad_norm": 1.2356595993041992,
        "learning_rate": 1.3444577712791883e-05,
        "epoch": 0.6124896608767576,
        "step": 4443
    },
    {
        "loss": 1.7398,
        "grad_norm": 1.7408497333526611,
        "learning_rate": 1.3379357721342689e-05,
        "epoch": 0.6126275158533223,
        "step": 4444
    },
    {
        "loss": 1.5038,
        "grad_norm": 2.331897497177124,
        "learning_rate": 1.3314284963565548e-05,
        "epoch": 0.6127653708298869,
        "step": 4445
    },
    {
        "loss": 0.9776,
        "grad_norm": 1.720101237297058,
        "learning_rate": 1.3249359550067919e-05,
        "epoch": 0.6129032258064516,
        "step": 4446
    },
    {
        "loss": 2.2208,
        "grad_norm": 1.4596210718154907,
        "learning_rate": 1.3184581591207068e-05,
        "epoch": 0.6130410807830162,
        "step": 4447
    },
    {
        "loss": 1.6871,
        "grad_norm": 2.2266199588775635,
        "learning_rate": 1.3119951197089463e-05,
        "epoch": 0.6131789357595809,
        "step": 4448
    },
    {
        "loss": 1.9848,
        "grad_norm": 2.385652542114258,
        "learning_rate": 1.3055468477570775e-05,
        "epoch": 0.6133167907361455,
        "step": 4449
    },
    {
        "loss": 0.4007,
        "grad_norm": 1.5365777015686035,
        "learning_rate": 1.299113354225564e-05,
        "epoch": 0.6134546457127102,
        "step": 4450
    },
    {
        "loss": 2.2112,
        "grad_norm": 1.3166112899780273,
        "learning_rate": 1.292694650049765e-05,
        "epoch": 0.6135925006892748,
        "step": 4451
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.7221261262893677,
        "learning_rate": 1.2862907461398788e-05,
        "epoch": 0.6137303556658396,
        "step": 4452
    },
    {
        "loss": 2.0958,
        "grad_norm": 0.9486739039421082,
        "learning_rate": 1.2799016533809705e-05,
        "epoch": 0.6138682106424042,
        "step": 4453
    },
    {
        "loss": 1.2449,
        "grad_norm": 1.707046627998352,
        "learning_rate": 1.2735273826329086e-05,
        "epoch": 0.6140060656189689,
        "step": 4454
    },
    {
        "loss": 1.8765,
        "grad_norm": 1.7093676328659058,
        "learning_rate": 1.2671679447303797e-05,
        "epoch": 0.6141439205955335,
        "step": 4455
    },
    {
        "loss": 1.9935,
        "grad_norm": 2.062580108642578,
        "learning_rate": 1.2608233504828581e-05,
        "epoch": 0.6142817755720982,
        "step": 4456
    },
    {
        "loss": 2.0348,
        "grad_norm": 1.8928234577178955,
        "learning_rate": 1.2544936106745775e-05,
        "epoch": 0.6144196305486628,
        "step": 4457
    },
    {
        "loss": 2.2667,
        "grad_norm": 2.197305202484131,
        "learning_rate": 1.2481787360645347e-05,
        "epoch": 0.6145574855252275,
        "step": 4458
    },
    {
        "loss": 1.619,
        "grad_norm": 1.6805949211120605,
        "learning_rate": 1.2418787373864615e-05,
        "epoch": 0.6146953405017921,
        "step": 4459
    },
    {
        "loss": 1.9591,
        "grad_norm": 1.1521844863891602,
        "learning_rate": 1.235593625348791e-05,
        "epoch": 0.6148331954783568,
        "step": 4460
    },
    {
        "loss": 2.3528,
        "grad_norm": 3.2513110637664795,
        "learning_rate": 1.2293234106346629e-05,
        "epoch": 0.6149710504549214,
        "step": 4461
    },
    {
        "loss": 1.0359,
        "grad_norm": 2.4179306030273438,
        "learning_rate": 1.2230681039018877e-05,
        "epoch": 0.6151089054314861,
        "step": 4462
    },
    {
        "loss": 2.3795,
        "grad_norm": 1.363004446029663,
        "learning_rate": 1.2168277157829366e-05,
        "epoch": 0.6152467604080507,
        "step": 4463
    },
    {
        "loss": 1.8178,
        "grad_norm": 2.174631357192993,
        "learning_rate": 1.2106022568849385e-05,
        "epoch": 0.6153846153846154,
        "step": 4464
    },
    {
        "loss": 1.0397,
        "grad_norm": 1.9786410331726074,
        "learning_rate": 1.2043917377896197e-05,
        "epoch": 0.61552247036118,
        "step": 4465
    },
    {
        "loss": 2.0728,
        "grad_norm": 1.5004328489303589,
        "learning_rate": 1.1981961690533395e-05,
        "epoch": 0.6156603253377447,
        "step": 4466
    },
    {
        "loss": 2.3771,
        "grad_norm": 1.4245740175247192,
        "learning_rate": 1.1920155612070284e-05,
        "epoch": 0.6157981803143093,
        "step": 4467
    },
    {
        "loss": 2.4476,
        "grad_norm": 2.3055083751678467,
        "learning_rate": 1.1858499247561871e-05,
        "epoch": 0.615936035290874,
        "step": 4468
    },
    {
        "loss": 2.1206,
        "grad_norm": 2.3690731525421143,
        "learning_rate": 1.179699270180875e-05,
        "epoch": 0.6160738902674386,
        "step": 4469
    },
    {
        "loss": 1.8073,
        "grad_norm": 1.5051982402801514,
        "learning_rate": 1.1735636079356904e-05,
        "epoch": 0.6162117452440034,
        "step": 4470
    },
    {
        "loss": 1.5524,
        "grad_norm": 2.0169529914855957,
        "learning_rate": 1.1674429484497295e-05,
        "epoch": 0.616349600220568,
        "step": 4471
    },
    {
        "loss": 2.2534,
        "grad_norm": 1.9976507425308228,
        "learning_rate": 1.1613373021266139e-05,
        "epoch": 0.6164874551971327,
        "step": 4472
    },
    {
        "loss": 1.7801,
        "grad_norm": 2.1421241760253906,
        "learning_rate": 1.1552466793444227e-05,
        "epoch": 0.6166253101736973,
        "step": 4473
    },
    {
        "loss": 2.0047,
        "grad_norm": 2.4511585235595703,
        "learning_rate": 1.1491710904557095e-05,
        "epoch": 0.616763165150262,
        "step": 4474
    },
    {
        "loss": 0.9648,
        "grad_norm": 2.2714734077453613,
        "learning_rate": 1.1431105457874813e-05,
        "epoch": 0.6169010201268266,
        "step": 4475
    },
    {
        "loss": 2.4546,
        "grad_norm": 1.373996376991272,
        "learning_rate": 1.1370650556411499e-05,
        "epoch": 0.6170388751033913,
        "step": 4476
    },
    {
        "loss": 2.3793,
        "grad_norm": 1.2255593538284302,
        "learning_rate": 1.1310346302925635e-05,
        "epoch": 0.6171767300799559,
        "step": 4477
    },
    {
        "loss": 1.6577,
        "grad_norm": 1.2768193483352661,
        "learning_rate": 1.125019279991959e-05,
        "epoch": 0.6173145850565206,
        "step": 4478
    },
    {
        "loss": 1.9557,
        "grad_norm": 1.6371210813522339,
        "learning_rate": 1.1190190149639345e-05,
        "epoch": 0.6174524400330852,
        "step": 4479
    },
    {
        "loss": 2.2217,
        "grad_norm": 1.6463972330093384,
        "learning_rate": 1.1130338454074662e-05,
        "epoch": 0.6175902950096499,
        "step": 4480
    },
    {
        "loss": 1.6583,
        "grad_norm": 1.7745057344436646,
        "learning_rate": 1.1070637814958518e-05,
        "epoch": 0.6177281499862145,
        "step": 4481
    },
    {
        "loss": 1.8409,
        "grad_norm": 1.7454465627670288,
        "learning_rate": 1.101108833376726e-05,
        "epoch": 0.6178660049627791,
        "step": 4482
    },
    {
        "loss": 1.8096,
        "grad_norm": 2.1532838344573975,
        "learning_rate": 1.0951690111720391e-05,
        "epoch": 0.6180038599393438,
        "step": 4483
    },
    {
        "loss": 1.4414,
        "grad_norm": 2.2052652835845947,
        "learning_rate": 1.0892443249780038e-05,
        "epoch": 0.6181417149159084,
        "step": 4484
    },
    {
        "loss": 1.8465,
        "grad_norm": 2.1108222007751465,
        "learning_rate": 1.0833347848651309e-05,
        "epoch": 0.6182795698924731,
        "step": 4485
    },
    {
        "loss": 1.3283,
        "grad_norm": 1.8133502006530762,
        "learning_rate": 1.0774404008781825e-05,
        "epoch": 0.6184174248690377,
        "step": 4486
    },
    {
        "loss": 2.1367,
        "grad_norm": 1.322243094444275,
        "learning_rate": 1.0715611830361406e-05,
        "epoch": 0.6185552798456024,
        "step": 4487
    },
    {
        "loss": 1.5153,
        "grad_norm": 2.6953237056732178,
        "learning_rate": 1.065697141332227e-05,
        "epoch": 0.618693134822167,
        "step": 4488
    },
    {
        "loss": 2.0218,
        "grad_norm": 0.9736912250518799,
        "learning_rate": 1.0598482857338688e-05,
        "epoch": 0.6188309897987317,
        "step": 4489
    },
    {
        "loss": 0.9824,
        "grad_norm": 2.45759916305542,
        "learning_rate": 1.0540146261826656e-05,
        "epoch": 0.6189688447752963,
        "step": 4490
    },
    {
        "loss": 1.0005,
        "grad_norm": 2.038931369781494,
        "learning_rate": 1.0481961725944033e-05,
        "epoch": 0.6191066997518611,
        "step": 4491
    },
    {
        "loss": 1.101,
        "grad_norm": 2.4799845218658447,
        "learning_rate": 1.0423929348590066e-05,
        "epoch": 0.6192445547284257,
        "step": 4492
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.4593191146850586,
        "learning_rate": 1.0366049228405505e-05,
        "epoch": 0.6193824097049904,
        "step": 4493
    },
    {
        "loss": 1.993,
        "grad_norm": 1.958240270614624,
        "learning_rate": 1.0308321463772241e-05,
        "epoch": 0.619520264681555,
        "step": 4494
    },
    {
        "loss": 2.2266,
        "grad_norm": 1.3593645095825195,
        "learning_rate": 1.0250746152813118e-05,
        "epoch": 0.6196581196581197,
        "step": 4495
    },
    {
        "loss": 0.5574,
        "grad_norm": 2.569004774093628,
        "learning_rate": 1.0193323393391985e-05,
        "epoch": 0.6197959746346843,
        "step": 4496
    },
    {
        "loss": 1.7434,
        "grad_norm": 1.5502020120620728,
        "learning_rate": 1.0136053283113423e-05,
        "epoch": 0.619933829611249,
        "step": 4497
    },
    {
        "loss": 1.9403,
        "grad_norm": 3.1958727836608887,
        "learning_rate": 1.0078935919322308e-05,
        "epoch": 0.6200716845878136,
        "step": 4498
    },
    {
        "loss": 2.1674,
        "grad_norm": 1.6170498132705688,
        "learning_rate": 1.0021971399104124e-05,
        "epoch": 0.6202095395643783,
        "step": 4499
    },
    {
        "loss": 0.8802,
        "grad_norm": 3.0403592586517334,
        "learning_rate": 9.965159819284441e-06,
        "epoch": 0.6203473945409429,
        "step": 4500
    },
    {
        "loss": 1.9232,
        "grad_norm": 1.2834922075271606,
        "learning_rate": 9.90850127642885e-06,
        "epoch": 0.6204852495175076,
        "step": 4501
    },
    {
        "loss": 1.2859,
        "grad_norm": 2.3382792472839355,
        "learning_rate": 9.851995866842967e-06,
        "epoch": 0.6206231044940722,
        "step": 4502
    },
    {
        "loss": 1.6991,
        "grad_norm": 1.5364025831222534,
        "learning_rate": 9.795643686571897e-06,
        "epoch": 0.6207609594706369,
        "step": 4503
    },
    {
        "loss": 1.8558,
        "grad_norm": 1.4183645248413086,
        "learning_rate": 9.739444831400525e-06,
        "epoch": 0.6208988144472015,
        "step": 4504
    },
    {
        "loss": 2.1719,
        "grad_norm": 4.204155445098877,
        "learning_rate": 9.683399396852966e-06,
        "epoch": 0.6210366694237662,
        "step": 4505
    },
    {
        "loss": 1.3232,
        "grad_norm": 1.898341417312622,
        "learning_rate": 9.627507478192565e-06,
        "epoch": 0.6211745244003308,
        "step": 4506
    },
    {
        "loss": 2.4793,
        "grad_norm": 1.389915943145752,
        "learning_rate": 9.571769170421762e-06,
        "epoch": 0.6213123793768955,
        "step": 4507
    },
    {
        "loss": 2.0623,
        "grad_norm": 1.7389761209487915,
        "learning_rate": 9.516184568281972e-06,
        "epoch": 0.6214502343534601,
        "step": 4508
    },
    {
        "loss": 1.528,
        "grad_norm": 1.369121789932251,
        "learning_rate": 9.46075376625316e-06,
        "epoch": 0.6215880893300249,
        "step": 4509
    },
    {
        "loss": 1.7381,
        "grad_norm": 2.358088731765747,
        "learning_rate": 9.40547685855412e-06,
        "epoch": 0.6217259443065895,
        "step": 4510
    },
    {
        "loss": 1.1394,
        "grad_norm": 2.31475830078125,
        "learning_rate": 9.35035393914182e-06,
        "epoch": 0.6218637992831542,
        "step": 4511
    },
    {
        "loss": 2.2469,
        "grad_norm": 2.05159068107605,
        "learning_rate": 9.295385101711618e-06,
        "epoch": 0.6220016542597188,
        "step": 4512
    },
    {
        "loss": 1.9885,
        "grad_norm": 2.730334758758545,
        "learning_rate": 9.240570439696982e-06,
        "epoch": 0.6221395092362835,
        "step": 4513
    },
    {
        "loss": 2.0356,
        "grad_norm": 1.9451009035110474,
        "learning_rate": 9.185910046269187e-06,
        "epoch": 0.6222773642128481,
        "step": 4514
    },
    {
        "loss": 1.5535,
        "grad_norm": 2.8491342067718506,
        "learning_rate": 9.131404014337452e-06,
        "epoch": 0.6224152191894128,
        "step": 4515
    },
    {
        "loss": 1.9498,
        "grad_norm": 2.192141532897949,
        "learning_rate": 9.077052436548606e-06,
        "epoch": 0.6225530741659774,
        "step": 4516
    },
    {
        "loss": 1.9041,
        "grad_norm": 2.419283866882324,
        "learning_rate": 9.022855405286824e-06,
        "epoch": 0.6226909291425421,
        "step": 4517
    },
    {
        "loss": 2.0614,
        "grad_norm": 2.149796962738037,
        "learning_rate": 8.968813012673671e-06,
        "epoch": 0.6228287841191067,
        "step": 4518
    },
    {
        "loss": 2.0919,
        "grad_norm": 1.2312239408493042,
        "learning_rate": 8.914925350567838e-06,
        "epoch": 0.6229666390956714,
        "step": 4519
    },
    {
        "loss": 1.8094,
        "grad_norm": 1.710425615310669,
        "learning_rate": 8.861192510564996e-06,
        "epoch": 0.623104494072236,
        "step": 4520
    },
    {
        "loss": 1.7453,
        "grad_norm": 1.7115870714187622,
        "learning_rate": 8.80761458399778e-06,
        "epoch": 0.6232423490488007,
        "step": 4521
    },
    {
        "loss": 2.207,
        "grad_norm": 1.5721114873886108,
        "learning_rate": 8.754191661935307e-06,
        "epoch": 0.6233802040253653,
        "step": 4522
    },
    {
        "loss": 2.0787,
        "grad_norm": 1.435865044593811,
        "learning_rate": 8.700923835183427e-06,
        "epoch": 0.62351805900193,
        "step": 4523
    },
    {
        "loss": 1.8926,
        "grad_norm": 2.2228283882141113,
        "learning_rate": 8.64781119428425e-06,
        "epoch": 0.6236559139784946,
        "step": 4524
    },
    {
        "loss": 1.757,
        "grad_norm": 2.530381679534912,
        "learning_rate": 8.594853829516092e-06,
        "epoch": 0.6237937689550592,
        "step": 4525
    },
    {
        "loss": 1.8859,
        "grad_norm": 1.7922345399856567,
        "learning_rate": 8.542051830893371e-06,
        "epoch": 0.6239316239316239,
        "step": 4526
    },
    {
        "loss": 1.5707,
        "grad_norm": 2.5232949256896973,
        "learning_rate": 8.489405288166508e-06,
        "epoch": 0.6240694789081885,
        "step": 4527
    },
    {
        "loss": 1.531,
        "grad_norm": 1.5341603755950928,
        "learning_rate": 8.436914290821507e-06,
        "epoch": 0.6242073338847532,
        "step": 4528
    },
    {
        "loss": 1.9721,
        "grad_norm": 1.3126599788665771,
        "learning_rate": 8.384578928080178e-06,
        "epoch": 0.6243451888613178,
        "step": 4529
    },
    {
        "loss": 2.2974,
        "grad_norm": 1.1756494045257568,
        "learning_rate": 8.332399288899639e-06,
        "epoch": 0.6244830438378826,
        "step": 4530
    },
    {
        "loss": 1.7206,
        "grad_norm": 2.5056514739990234,
        "learning_rate": 8.280375461972367e-06,
        "epoch": 0.6246208988144472,
        "step": 4531
    },
    {
        "loss": 2.2317,
        "grad_norm": 1.2475807666778564,
        "learning_rate": 8.228507535726083e-06,
        "epoch": 0.6247587537910119,
        "step": 4532
    },
    {
        "loss": 2.0376,
        "grad_norm": 1.7534942626953125,
        "learning_rate": 8.176795598323295e-06,
        "epoch": 0.6248966087675765,
        "step": 4533
    },
    {
        "loss": 1.5637,
        "grad_norm": 1.7283544540405273,
        "learning_rate": 8.125239737661583e-06,
        "epoch": 0.6250344637441412,
        "step": 4534
    },
    {
        "loss": 2.1249,
        "grad_norm": 1.6155258417129517,
        "learning_rate": 8.073840041373226e-06,
        "epoch": 0.6251723187207058,
        "step": 4535
    },
    {
        "loss": 2.2147,
        "grad_norm": 2.1765191555023193,
        "learning_rate": 8.022596596824916e-06,
        "epoch": 0.6253101736972705,
        "step": 4536
    },
    {
        "loss": 2.2327,
        "grad_norm": 1.3026798963546753,
        "learning_rate": 7.97150949111788e-06,
        "epoch": 0.6254480286738351,
        "step": 4537
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.3559565544128418,
        "learning_rate": 7.92057881108752e-06,
        "epoch": 0.6255858836503998,
        "step": 4538
    },
    {
        "loss": 1.9298,
        "grad_norm": 2.012826681137085,
        "learning_rate": 7.869804643303392e-06,
        "epoch": 0.6257237386269644,
        "step": 4539
    },
    {
        "loss": 2.2262,
        "grad_norm": 1.9408760070800781,
        "learning_rate": 7.819187074069102e-06,
        "epoch": 0.6258615936035291,
        "step": 4540
    },
    {
        "loss": 1.7801,
        "grad_norm": 1.4914546012878418,
        "learning_rate": 7.768726189421904e-06,
        "epoch": 0.6259994485800937,
        "step": 4541
    },
    {
        "loss": 1.8265,
        "grad_norm": 2.2408902645111084,
        "learning_rate": 7.718422075132914e-06,
        "epoch": 0.6261373035566584,
        "step": 4542
    },
    {
        "loss": 1.9919,
        "grad_norm": 1.2422384023666382,
        "learning_rate": 7.668274816706655e-06,
        "epoch": 0.626275158533223,
        "step": 4543
    },
    {
        "loss": 1.6459,
        "grad_norm": 1.3745872974395752,
        "learning_rate": 7.6182844993810235e-06,
        "epoch": 0.6264130135097877,
        "step": 4544
    },
    {
        "loss": 1.8835,
        "grad_norm": 1.2515285015106201,
        "learning_rate": 7.568451208127214e-06,
        "epoch": 0.6265508684863523,
        "step": 4545
    },
    {
        "loss": 1.4862,
        "grad_norm": 2.489412307739258,
        "learning_rate": 7.518775027649583e-06,
        "epoch": 0.626688723462917,
        "step": 4546
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.39448881149292,
        "learning_rate": 7.46925604238522e-06,
        "epoch": 0.6268265784394816,
        "step": 4547
    },
    {
        "loss": 1.621,
        "grad_norm": 2.432013750076294,
        "learning_rate": 7.419894336504263e-06,
        "epoch": 0.6269644334160464,
        "step": 4548
    },
    {
        "loss": 1.8868,
        "grad_norm": 1.8909019231796265,
        "learning_rate": 7.370689993909352e-06,
        "epoch": 0.627102288392611,
        "step": 4549
    },
    {
        "loss": 1.7218,
        "grad_norm": 2.1561973094940186,
        "learning_rate": 7.321643098235698e-06,
        "epoch": 0.6272401433691757,
        "step": 4550
    },
    {
        "loss": 1.6484,
        "grad_norm": 2.291217088699341,
        "learning_rate": 7.272753732850946e-06,
        "epoch": 0.6273779983457403,
        "step": 4551
    },
    {
        "loss": 2.5809,
        "grad_norm": 1.3597303628921509,
        "learning_rate": 7.224021980854845e-06,
        "epoch": 0.627515853322305,
        "step": 4552
    },
    {
        "loss": 2.1422,
        "grad_norm": 1.6295245885849,
        "learning_rate": 7.175447925079359e-06,
        "epoch": 0.6276537082988696,
        "step": 4553
    },
    {
        "loss": 1.9463,
        "grad_norm": 1.268889307975769,
        "learning_rate": 7.127031648088434e-06,
        "epoch": 0.6277915632754343,
        "step": 4554
    },
    {
        "loss": 1.7806,
        "grad_norm": 2.429068088531494,
        "learning_rate": 7.078773232177682e-06,
        "epoch": 0.6279294182519989,
        "step": 4555
    },
    {
        "loss": 2.0663,
        "grad_norm": 1.5868116617202759,
        "learning_rate": 7.0306727593745235e-06,
        "epoch": 0.6280672732285636,
        "step": 4556
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.9899554252624512,
        "learning_rate": 6.982730311437868e-06,
        "epoch": 0.6282051282051282,
        "step": 4557
    },
    {
        "loss": 1.8123,
        "grad_norm": 2.360299587249756,
        "learning_rate": 6.934945969857964e-06,
        "epoch": 0.6283429831816929,
        "step": 4558
    },
    {
        "loss": 1.8447,
        "grad_norm": 1.3637348413467407,
        "learning_rate": 6.8873198158564855e-06,
        "epoch": 0.6284808381582575,
        "step": 4559
    },
    {
        "loss": 1.7061,
        "grad_norm": 1.2543290853500366,
        "learning_rate": 6.839851930385987e-06,
        "epoch": 0.6286186931348222,
        "step": 4560
    },
    {
        "loss": 1.4897,
        "grad_norm": 2.1261160373687744,
        "learning_rate": 6.792542394130208e-06,
        "epoch": 0.6287565481113868,
        "step": 4561
    },
    {
        "loss": 1.6729,
        "grad_norm": 2.051750659942627,
        "learning_rate": 6.745391287503699e-06,
        "epoch": 0.6288944030879515,
        "step": 4562
    },
    {
        "loss": 1.2581,
        "grad_norm": 1.872041940689087,
        "learning_rate": 6.698398690651608e-06,
        "epoch": 0.6290322580645161,
        "step": 4563
    },
    {
        "loss": 1.8437,
        "grad_norm": 1.685653567314148,
        "learning_rate": 6.6515646834497295e-06,
        "epoch": 0.6291701130410808,
        "step": 4564
    },
    {
        "loss": 1.7169,
        "grad_norm": 3.148844003677368,
        "learning_rate": 6.604889345504372e-06,
        "epoch": 0.6293079680176454,
        "step": 4565
    },
    {
        "loss": 1.8354,
        "grad_norm": 1.6806050539016724,
        "learning_rate": 6.558372756151976e-06,
        "epoch": 0.62944582299421,
        "step": 4566
    },
    {
        "loss": 2.8872,
        "grad_norm": 1.5115591287612915,
        "learning_rate": 6.512014994459337e-06,
        "epoch": 0.6295836779707747,
        "step": 4567
    },
    {
        "loss": 2.0196,
        "grad_norm": 1.9599920511245728,
        "learning_rate": 6.46581613922308e-06,
        "epoch": 0.6297215329473393,
        "step": 4568
    },
    {
        "loss": 2.0026,
        "grad_norm": 1.5335360765457153,
        "learning_rate": 6.419776268969913e-06,
        "epoch": 0.6298593879239041,
        "step": 4569
    },
    {
        "loss": 1.5173,
        "grad_norm": 2.3249878883361816,
        "learning_rate": 6.373895461956214e-06,
        "epoch": 0.6299972429004687,
        "step": 4570
    },
    {
        "loss": 1.9976,
        "grad_norm": 1.9542853832244873,
        "learning_rate": 6.328173796167957e-06,
        "epoch": 0.6301350978770334,
        "step": 4571
    },
    {
        "loss": 1.2717,
        "grad_norm": 2.056896686553955,
        "learning_rate": 6.282611349320711e-06,
        "epoch": 0.630272952853598,
        "step": 4572
    },
    {
        "loss": 2.1645,
        "grad_norm": 1.543005347251892,
        "learning_rate": 6.237208198859423e-06,
        "epoch": 0.6304108078301627,
        "step": 4573
    },
    {
        "loss": 1.8482,
        "grad_norm": 2.029080867767334,
        "learning_rate": 6.1919644219581295e-06,
        "epoch": 0.6305486628067273,
        "step": 4574
    },
    {
        "loss": 2.1082,
        "grad_norm": 1.276094675064087,
        "learning_rate": 6.146880095520114e-06,
        "epoch": 0.630686517783292,
        "step": 4575
    },
    {
        "loss": 1.5618,
        "grad_norm": 3.074326992034912,
        "learning_rate": 6.101955296177575e-06,
        "epoch": 0.6308243727598566,
        "step": 4576
    },
    {
        "loss": 1.0048,
        "grad_norm": 2.333746910095215,
        "learning_rate": 6.057190100291521e-06,
        "epoch": 0.6309622277364213,
        "step": 4577
    },
    {
        "loss": 2.2344,
        "grad_norm": 1.4104639291763306,
        "learning_rate": 6.012584583951786e-06,
        "epoch": 0.6311000827129859,
        "step": 4578
    },
    {
        "loss": 1.895,
        "grad_norm": 2.0489466190338135,
        "learning_rate": 5.9681388229766435e-06,
        "epoch": 0.6312379376895506,
        "step": 4579
    },
    {
        "loss": 1.8928,
        "grad_norm": 1.5378652811050415,
        "learning_rate": 5.9238528929129625e-06,
        "epoch": 0.6313757926661152,
        "step": 4580
    },
    {
        "loss": 2.2181,
        "grad_norm": 2.0962789058685303,
        "learning_rate": 5.879726869035862e-06,
        "epoch": 0.6315136476426799,
        "step": 4581
    },
    {
        "loss": 1.3478,
        "grad_norm": 3.5676822662353516,
        "learning_rate": 5.8357608263486355e-06,
        "epoch": 0.6316515026192445,
        "step": 4582
    },
    {
        "loss": 1.795,
        "grad_norm": 1.6402174234390259,
        "learning_rate": 5.79195483958267e-06,
        "epoch": 0.6317893575958092,
        "step": 4583
    },
    {
        "loss": 1.6224,
        "grad_norm": 2.801882743835449,
        "learning_rate": 5.748308983197392e-06,
        "epoch": 0.6319272125723738,
        "step": 4584
    },
    {
        "loss": 1.5246,
        "grad_norm": 2.1489686965942383,
        "learning_rate": 5.7048233313798714e-06,
        "epoch": 0.6320650675489385,
        "step": 4585
    },
    {
        "loss": 1.7843,
        "grad_norm": 1.885647177696228,
        "learning_rate": 5.661497958045048e-06,
        "epoch": 0.6322029225255031,
        "step": 4586
    },
    {
        "loss": 1.5307,
        "grad_norm": 2.340259552001953,
        "learning_rate": 5.618332936835291e-06,
        "epoch": 0.6323407775020679,
        "step": 4587
    },
    {
        "loss": 2.3347,
        "grad_norm": 1.5794960260391235,
        "learning_rate": 5.575328341120467e-06,
        "epoch": 0.6324786324786325,
        "step": 4588
    },
    {
        "loss": 1.4878,
        "grad_norm": 2.9746317863464355,
        "learning_rate": 5.532484243997782e-06,
        "epoch": 0.6326164874551972,
        "step": 4589
    },
    {
        "loss": 1.9716,
        "grad_norm": 1.493697166442871,
        "learning_rate": 5.4898007182915265e-06,
        "epoch": 0.6327543424317618,
        "step": 4590
    },
    {
        "loss": 1.4701,
        "grad_norm": 1.494682788848877,
        "learning_rate": 5.447277836553211e-06,
        "epoch": 0.6328921974083265,
        "step": 4591
    },
    {
        "loss": 2.489,
        "grad_norm": 2.215367078781128,
        "learning_rate": 5.40491567106125e-06,
        "epoch": 0.6330300523848911,
        "step": 4592
    },
    {
        "loss": 1.6477,
        "grad_norm": 1.425940752029419,
        "learning_rate": 5.36271429382078e-06,
        "epoch": 0.6331679073614558,
        "step": 4593
    },
    {
        "loss": 2.5049,
        "grad_norm": 1.7304928302764893,
        "learning_rate": 5.320673776563745e-06,
        "epoch": 0.6333057623380204,
        "step": 4594
    },
    {
        "loss": 1.5705,
        "grad_norm": 1.7556697130203247,
        "learning_rate": 5.278794190748615e-06,
        "epoch": 0.6334436173145851,
        "step": 4595
    },
    {
        "loss": 2.0014,
        "grad_norm": 1.416168451309204,
        "learning_rate": 5.237075607560293e-06,
        "epoch": 0.6335814722911497,
        "step": 4596
    },
    {
        "loss": 1.8457,
        "grad_norm": 1.6469215154647827,
        "learning_rate": 5.195518097910135e-06,
        "epoch": 0.6337193272677144,
        "step": 4597
    },
    {
        "loss": 1.9044,
        "grad_norm": 3.377485752105713,
        "learning_rate": 5.154121732435546e-06,
        "epoch": 0.633857182244279,
        "step": 4598
    },
    {
        "loss": 1.7631,
        "grad_norm": 2.1997478008270264,
        "learning_rate": 5.112886581500176e-06,
        "epoch": 0.6339950372208437,
        "step": 4599
    },
    {
        "loss": 1.537,
        "grad_norm": 2.6501357555389404,
        "learning_rate": 5.07181271519358e-06,
        "epoch": 0.6341328921974083,
        "step": 4600
    },
    {
        "loss": 1.7821,
        "grad_norm": 2.988075017929077,
        "learning_rate": 5.030900203331134e-06,
        "epoch": 0.634270747173973,
        "step": 4601
    },
    {
        "loss": 1.9896,
        "grad_norm": 2.0939996242523193,
        "learning_rate": 4.9901491154539794e-06,
        "epoch": 0.6344086021505376,
        "step": 4602
    },
    {
        "loss": 1.9482,
        "grad_norm": 1.8876495361328125,
        "learning_rate": 4.949559520828972e-06,
        "epoch": 0.6345464571271023,
        "step": 4603
    },
    {
        "loss": 1.7113,
        "grad_norm": 1.5187855958938599,
        "learning_rate": 4.909131488448282e-06,
        "epoch": 0.6346843121036669,
        "step": 4604
    },
    {
        "loss": 2.0265,
        "grad_norm": 1.5272350311279297,
        "learning_rate": 4.868865087029662e-06,
        "epoch": 0.6348221670802316,
        "step": 4605
    },
    {
        "loss": 2.1653,
        "grad_norm": 1.9843639135360718,
        "learning_rate": 4.828760385015984e-06,
        "epoch": 0.6349600220567962,
        "step": 4606
    },
    {
        "loss": 1.6164,
        "grad_norm": 1.940459966659546,
        "learning_rate": 4.788817450575323e-06,
        "epoch": 0.635097877033361,
        "step": 4607
    },
    {
        "loss": 2.5165,
        "grad_norm": 1.4272339344024658,
        "learning_rate": 4.749036351600844e-06,
        "epoch": 0.6352357320099256,
        "step": 4608
    },
    {
        "loss": 1.3424,
        "grad_norm": 2.714505910873413,
        "learning_rate": 4.709417155710472e-06,
        "epoch": 0.6353735869864902,
        "step": 4609
    },
    {
        "loss": 2.1431,
        "grad_norm": 2.7556989192962646,
        "learning_rate": 4.6699599302471145e-06,
        "epoch": 0.6355114419630549,
        "step": 4610
    },
    {
        "loss": 1.9857,
        "grad_norm": 1.4874910116195679,
        "learning_rate": 4.630664742278324e-06,
        "epoch": 0.6356492969396195,
        "step": 4611
    },
    {
        "loss": 2.2858,
        "grad_norm": 2.239318370819092,
        "learning_rate": 4.59153165859616e-06,
        "epoch": 0.6357871519161842,
        "step": 4612
    },
    {
        "loss": 1.4847,
        "grad_norm": 2.9568376541137695,
        "learning_rate": 4.5525607457171824e-06,
        "epoch": 0.6359250068927488,
        "step": 4613
    },
    {
        "loss": 2.1285,
        "grad_norm": 1.7015430927276611,
        "learning_rate": 4.513752069882349e-06,
        "epoch": 0.6360628618693135,
        "step": 4614
    },
    {
        "loss": 2.7426,
        "grad_norm": 1.4292285442352295,
        "learning_rate": 4.475105697056747e-06,
        "epoch": 0.6362007168458781,
        "step": 4615
    },
    {
        "loss": 1.8895,
        "grad_norm": 3.9495415687561035,
        "learning_rate": 4.4366216929297215e-06,
        "epoch": 0.6363385718224428,
        "step": 4616
    },
    {
        "loss": 2.617,
        "grad_norm": 1.2410292625427246,
        "learning_rate": 4.3983001229145e-06,
        "epoch": 0.6364764267990074,
        "step": 4617
    },
    {
        "loss": 2.024,
        "grad_norm": 1.8126884698867798,
        "learning_rate": 4.360141052148314e-06,
        "epoch": 0.6366142817755721,
        "step": 4618
    },
    {
        "loss": 2.0192,
        "grad_norm": 1.818920612335205,
        "learning_rate": 4.322144545492179e-06,
        "epoch": 0.6367521367521367,
        "step": 4619
    },
    {
        "loss": 1.4085,
        "grad_norm": 2.388278007507324,
        "learning_rate": 4.284310667530678e-06,
        "epoch": 0.6368899917287014,
        "step": 4620
    },
    {
        "loss": 2.3364,
        "grad_norm": 1.4739657640457153,
        "learning_rate": 4.246639482572079e-06,
        "epoch": 0.637027846705266,
        "step": 4621
    },
    {
        "loss": 1.9397,
        "grad_norm": 1.5734258890151978,
        "learning_rate": 4.2091310546481524e-06,
        "epoch": 0.6371657016818307,
        "step": 4622
    },
    {
        "loss": 1.7787,
        "grad_norm": 3.443512201309204,
        "learning_rate": 4.171785447513832e-06,
        "epoch": 0.6373035566583953,
        "step": 4623
    },
    {
        "loss": 1.6459,
        "grad_norm": 1.7264306545257568,
        "learning_rate": 4.134602724647518e-06,
        "epoch": 0.63744141163496,
        "step": 4624
    },
    {
        "loss": 2.3098,
        "grad_norm": 1.5306962728500366,
        "learning_rate": 4.097582949250589e-06,
        "epoch": 0.6375792666115246,
        "step": 4625
    },
    {
        "loss": 2.1198,
        "grad_norm": 1.0786123275756836,
        "learning_rate": 4.060726184247499e-06,
        "epoch": 0.6377171215880894,
        "step": 4626
    },
    {
        "loss": 1.5418,
        "grad_norm": 1.368463158607483,
        "learning_rate": 4.0240324922856894e-06,
        "epoch": 0.637854976564654,
        "step": 4627
    },
    {
        "loss": 2.098,
        "grad_norm": 1.3110393285751343,
        "learning_rate": 3.9875019357352935e-06,
        "epoch": 0.6379928315412187,
        "step": 4628
    },
    {
        "loss": 2.0763,
        "grad_norm": 1.8422468900680542,
        "learning_rate": 3.951134576689253e-06,
        "epoch": 0.6381306865177833,
        "step": 4629
    },
    {
        "loss": 1.8448,
        "grad_norm": 2.3553152084350586,
        "learning_rate": 3.914930476963152e-06,
        "epoch": 0.638268541494348,
        "step": 4630
    },
    {
        "loss": 1.6382,
        "grad_norm": 2.148380994796753,
        "learning_rate": 3.878889698094901e-06,
        "epoch": 0.6384063964709126,
        "step": 4631
    },
    {
        "loss": 1.7866,
        "grad_norm": 2.611187219619751,
        "learning_rate": 3.843012301344973e-06,
        "epoch": 0.6385442514474773,
        "step": 4632
    },
    {
        "loss": 1.3592,
        "grad_norm": 2.6699559688568115,
        "learning_rate": 3.8072983476960646e-06,
        "epoch": 0.6386821064240419,
        "step": 4633
    },
    {
        "loss": 1.901,
        "grad_norm": 1.9313760995864868,
        "learning_rate": 3.7717478978530285e-06,
        "epoch": 0.6388199614006066,
        "step": 4634
    },
    {
        "loss": 2.093,
        "grad_norm": 1.1545608043670654,
        "learning_rate": 3.736361012242895e-06,
        "epoch": 0.6389578163771712,
        "step": 4635
    },
    {
        "loss": 1.5013,
        "grad_norm": 1.8940329551696777,
        "learning_rate": 3.7011377510145718e-06,
        "epoch": 0.6390956713537359,
        "step": 4636
    },
    {
        "loss": 1.4383,
        "grad_norm": 1.5932157039642334,
        "learning_rate": 3.6660781740389116e-06,
        "epoch": 0.6392335263303005,
        "step": 4637
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.7050516605377197,
        "learning_rate": 3.6311823409085567e-06,
        "epoch": 0.6393713813068652,
        "step": 4638
    },
    {
        "loss": 1.5589,
        "grad_norm": 1.9486867189407349,
        "learning_rate": 3.596450310937738e-06,
        "epoch": 0.6395092362834298,
        "step": 4639
    },
    {
        "loss": 2.8757,
        "grad_norm": 1.6566998958587646,
        "learning_rate": 3.5618821431623316e-06,
        "epoch": 0.6396470912599945,
        "step": 4640
    },
    {
        "loss": 2.5335,
        "grad_norm": 1.599174976348877,
        "learning_rate": 3.527477896339715e-06,
        "epoch": 0.6397849462365591,
        "step": 4641
    },
    {
        "loss": 2.545,
        "grad_norm": 1.7282946109771729,
        "learning_rate": 3.493237628948531e-06,
        "epoch": 0.6399228012131238,
        "step": 4642
    },
    {
        "loss": 1.0677,
        "grad_norm": 1.8789856433868408,
        "learning_rate": 3.4591613991888592e-06,
        "epoch": 0.6400606561896884,
        "step": 4643
    },
    {
        "loss": 1.7282,
        "grad_norm": 1.693585991859436,
        "learning_rate": 3.425249264981778e-06,
        "epoch": 0.6401985111662531,
        "step": 4644
    },
    {
        "loss": 2.22,
        "grad_norm": 1.5445948839187622,
        "learning_rate": 3.391501283969567e-06,
        "epoch": 0.6403363661428177,
        "step": 4645
    },
    {
        "loss": 1.2888,
        "grad_norm": 2.2392241954803467,
        "learning_rate": 3.3579175135154737e-06,
        "epoch": 0.6404742211193825,
        "step": 4646
    },
    {
        "loss": 2.0812,
        "grad_norm": 1.777518391609192,
        "learning_rate": 3.324498010703525e-06,
        "epoch": 0.6406120760959471,
        "step": 4647
    },
    {
        "loss": 2.0781,
        "grad_norm": 1.9447733163833618,
        "learning_rate": 3.2912428323386703e-06,
        "epoch": 0.6407499310725118,
        "step": 4648
    },
    {
        "loss": 1.8035,
        "grad_norm": 1.5719339847564697,
        "learning_rate": 3.2581520349465045e-06,
        "epoch": 0.6408877860490764,
        "step": 4649
    },
    {
        "loss": 2.2521,
        "grad_norm": 1.699293851852417,
        "learning_rate": 3.225225674773169e-06,
        "epoch": 0.6410256410256411,
        "step": 4650
    },
    {
        "loss": 1.0773,
        "grad_norm": 2.2996411323547363,
        "learning_rate": 3.1924638077853398e-06,
        "epoch": 0.6411634960022057,
        "step": 4651
    },
    {
        "loss": 2.318,
        "grad_norm": 1.466739296913147,
        "learning_rate": 3.1598664896701046e-06,
        "epoch": 0.6413013509787703,
        "step": 4652
    },
    {
        "loss": 1.9303,
        "grad_norm": 1.235759973526001,
        "learning_rate": 3.127433775834798e-06,
        "epoch": 0.641439205955335,
        "step": 4653
    },
    {
        "loss": 2.1163,
        "grad_norm": 1.5833550691604614,
        "learning_rate": 3.095165721407078e-06,
        "epoch": 0.6415770609318996,
        "step": 4654
    },
    {
        "loss": 2.1512,
        "grad_norm": 1.7653162479400635,
        "learning_rate": 3.0630623812345826e-06,
        "epoch": 0.6417149159084643,
        "step": 4655
    },
    {
        "loss": 2.1099,
        "grad_norm": 2.4918835163116455,
        "learning_rate": 3.0311238098851168e-06,
        "epoch": 0.6418527708850289,
        "step": 4656
    },
    {
        "loss": 1.4026,
        "grad_norm": 2.36678409576416,
        "learning_rate": 2.999350061646344e-06,
        "epoch": 0.6419906258615936,
        "step": 4657
    },
    {
        "loss": 1.4995,
        "grad_norm": 1.4282829761505127,
        "learning_rate": 2.9677411905257636e-06,
        "epoch": 0.6421284808381582,
        "step": 4658
    },
    {
        "loss": 1.4058,
        "grad_norm": 2.801640272140503,
        "learning_rate": 2.936297250250608e-06,
        "epoch": 0.6422663358147229,
        "step": 4659
    },
    {
        "loss": 2.2616,
        "grad_norm": 1.8937594890594482,
        "learning_rate": 2.9050182942678693e-06,
        "epoch": 0.6424041907912875,
        "step": 4660
    },
    {
        "loss": 1.6796,
        "grad_norm": 1.788994550704956,
        "learning_rate": 2.8739043757439745e-06,
        "epoch": 0.6425420457678522,
        "step": 4661
    },
    {
        "loss": 2.0701,
        "grad_norm": 2.393991231918335,
        "learning_rate": 2.842955547564952e-06,
        "epoch": 0.6426799007444168,
        "step": 4662
    },
    {
        "loss": 2.2714,
        "grad_norm": 1.3874834775924683,
        "learning_rate": 2.8121718623361104e-06,
        "epoch": 0.6428177557209815,
        "step": 4663
    },
    {
        "loss": 1.2114,
        "grad_norm": 2.705594301223755,
        "learning_rate": 2.7815533723821173e-06,
        "epoch": 0.6429556106975461,
        "step": 4664
    },
    {
        "loss": 1.9092,
        "grad_norm": 1.7016804218292236,
        "learning_rate": 2.751100129746842e-06,
        "epoch": 0.6430934656741109,
        "step": 4665
    },
    {
        "loss": 2.094,
        "grad_norm": 1.7372902631759644,
        "learning_rate": 2.7208121861932224e-06,
        "epoch": 0.6432313206506755,
        "step": 4666
    },
    {
        "loss": 2.2875,
        "grad_norm": 1.8234530687332153,
        "learning_rate": 2.6906895932033105e-06,
        "epoch": 0.6433691756272402,
        "step": 4667
    },
    {
        "loss": 1.8599,
        "grad_norm": 1.738329529762268,
        "learning_rate": 2.6607324019780943e-06,
        "epoch": 0.6435070306038048,
        "step": 4668
    },
    {
        "loss": 1.2753,
        "grad_norm": 2.289418935775757,
        "learning_rate": 2.6309406634373425e-06,
        "epoch": 0.6436448855803695,
        "step": 4669
    },
    {
        "loss": 1.9555,
        "grad_norm": 2.3591809272766113,
        "learning_rate": 2.6013144282196587e-06,
        "epoch": 0.6437827405569341,
        "step": 4670
    },
    {
        "loss": 1.3017,
        "grad_norm": 1.8500430583953857,
        "learning_rate": 2.57185374668234e-06,
        "epoch": 0.6439205955334988,
        "step": 4671
    },
    {
        "loss": 1.5951,
        "grad_norm": 1.3084672689437866,
        "learning_rate": 2.542558668901207e-06,
        "epoch": 0.6440584505100634,
        "step": 4672
    },
    {
        "loss": 1.8883,
        "grad_norm": 2.069394588470459,
        "learning_rate": 2.5134292446707062e-06,
        "epoch": 0.6441963054866281,
        "step": 4673
    },
    {
        "loss": 2.0157,
        "grad_norm": 2.249171257019043,
        "learning_rate": 2.4844655235035984e-06,
        "epoch": 0.6443341604631927,
        "step": 4674
    },
    {
        "loss": 1.2915,
        "grad_norm": 2.653517484664917,
        "learning_rate": 2.4556675546310915e-06,
        "epoch": 0.6444720154397574,
        "step": 4675
    },
    {
        "loss": 2.2663,
        "grad_norm": 1.2426788806915283,
        "learning_rate": 2.4270353870026075e-06,
        "epoch": 0.644609870416322,
        "step": 4676
    },
    {
        "loss": 2.247,
        "grad_norm": 1.6637176275253296,
        "learning_rate": 2.3985690692857387e-06,
        "epoch": 0.6447477253928867,
        "step": 4677
    },
    {
        "loss": 1.784,
        "grad_norm": 2.32865571975708,
        "learning_rate": 2.3702686498661586e-06,
        "epoch": 0.6448855803694513,
        "step": 4678
    },
    {
        "loss": 1.9139,
        "grad_norm": 2.569523334503174,
        "learning_rate": 2.3421341768476435e-06,
        "epoch": 0.645023435346016,
        "step": 4679
    },
    {
        "loss": 2.0292,
        "grad_norm": 1.9143054485321045,
        "learning_rate": 2.3141656980517847e-06,
        "epoch": 0.6451612903225806,
        "step": 4680
    },
    {
        "loss": 1.5165,
        "grad_norm": 1.565990924835205,
        "learning_rate": 2.286363261018132e-06,
        "epoch": 0.6452991452991453,
        "step": 4681
    },
    {
        "loss": 2.2293,
        "grad_norm": 1.5635701417922974,
        "learning_rate": 2.258726913003906e-06,
        "epoch": 0.6454370002757099,
        "step": 4682
    },
    {
        "loss": 1.2518,
        "grad_norm": 2.3950419425964355,
        "learning_rate": 2.2312567009840746e-06,
        "epoch": 0.6455748552522746,
        "step": 4683
    },
    {
        "loss": 1.791,
        "grad_norm": 1.3358005285263062,
        "learning_rate": 2.2039526716512438e-06,
        "epoch": 0.6457127102288392,
        "step": 4684
    },
    {
        "loss": 2.0609,
        "grad_norm": 1.6818370819091797,
        "learning_rate": 2.1768148714154446e-06,
        "epoch": 0.645850565205404,
        "step": 4685
    },
    {
        "loss": 2.0309,
        "grad_norm": 1.5519092082977295,
        "learning_rate": 2.1498433464042677e-06,
        "epoch": 0.6459884201819686,
        "step": 4686
    },
    {
        "loss": 1.8773,
        "grad_norm": 2.2830095291137695,
        "learning_rate": 2.1230381424626523e-06,
        "epoch": 0.6461262751585333,
        "step": 4687
    },
    {
        "loss": 2.1416,
        "grad_norm": 1.778189778327942,
        "learning_rate": 2.0963993051527966e-06,
        "epoch": 0.6462641301350979,
        "step": 4688
    },
    {
        "loss": 2.0161,
        "grad_norm": 1.6552237272262573,
        "learning_rate": 2.069926879754114e-06,
        "epoch": 0.6464019851116626,
        "step": 4689
    },
    {
        "loss": 1.533,
        "grad_norm": 1.6093122959136963,
        "learning_rate": 2.0436209112632332e-06,
        "epoch": 0.6465398400882272,
        "step": 4690
    },
    {
        "loss": 2.0885,
        "grad_norm": 1.6099886894226074,
        "learning_rate": 2.0174814443937316e-06,
        "epoch": 0.6466776950647919,
        "step": 4691
    },
    {
        "loss": 1.8904,
        "grad_norm": 1.9247734546661377,
        "learning_rate": 1.991508523576313e-06,
        "epoch": 0.6468155500413565,
        "step": 4692
    },
    {
        "loss": 2.1485,
        "grad_norm": 2.205688714981079,
        "learning_rate": 1.9657021929584406e-06,
        "epoch": 0.6469534050179212,
        "step": 4693
    },
    {
        "loss": 1.5812,
        "grad_norm": 2.3161909580230713,
        "learning_rate": 1.940062496404549e-06,
        "epoch": 0.6470912599944858,
        "step": 4694
    },
    {
        "loss": 1.2424,
        "grad_norm": 1.7448939085006714,
        "learning_rate": 1.914589477495787e-06,
        "epoch": 0.6472291149710504,
        "step": 4695
    },
    {
        "loss": 1.6374,
        "grad_norm": 1.9308642148971558,
        "learning_rate": 1.8892831795299548e-06,
        "epoch": 0.6473669699476151,
        "step": 4696
    },
    {
        "loss": 0.9101,
        "grad_norm": 2.5876142978668213,
        "learning_rate": 1.8641436455214988e-06,
        "epoch": 0.6475048249241797,
        "step": 4697
    },
    {
        "loss": 1.7332,
        "grad_norm": 1.9345991611480713,
        "learning_rate": 1.8391709182014493e-06,
        "epoch": 0.6476426799007444,
        "step": 4698
    },
    {
        "loss": 1.6888,
        "grad_norm": 2.7890563011169434,
        "learning_rate": 1.8143650400172297e-06,
        "epoch": 0.647780534877309,
        "step": 4699
    },
    {
        "loss": 0.9126,
        "grad_norm": 1.928372859954834,
        "learning_rate": 1.789726053132723e-06,
        "epoch": 0.6479183898538737,
        "step": 4700
    },
    {
        "loss": 1.441,
        "grad_norm": 1.7429217100143433,
        "learning_rate": 1.765253999428096e-06,
        "epoch": 0.6480562448304383,
        "step": 4701
    },
    {
        "loss": 2.4108,
        "grad_norm": 1.080804705619812,
        "learning_rate": 1.7409489204998076e-06,
        "epoch": 0.648194099807003,
        "step": 4702
    },
    {
        "loss": 1.6198,
        "grad_norm": 1.7247390747070312,
        "learning_rate": 1.716810857660478e-06,
        "epoch": 0.6483319547835676,
        "step": 4703
    },
    {
        "loss": 2.0051,
        "grad_norm": 1.9796428680419922,
        "learning_rate": 1.69283985193881e-06,
        "epoch": 0.6484698097601324,
        "step": 4704
    },
    {
        "loss": 1.931,
        "grad_norm": 1.261449933052063,
        "learning_rate": 1.6690359440796111e-06,
        "epoch": 0.648607664736697,
        "step": 4705
    },
    {
        "loss": 2.2233,
        "grad_norm": 2.1634390354156494,
        "learning_rate": 1.6453991745436826e-06,
        "epoch": 0.6487455197132617,
        "step": 4706
    },
    {
        "loss": 2.3777,
        "grad_norm": 1.784676194190979,
        "learning_rate": 1.6219295835076088e-06,
        "epoch": 0.6488833746898263,
        "step": 4707
    },
    {
        "loss": 1.8776,
        "grad_norm": 1.8480862379074097,
        "learning_rate": 1.5986272108639455e-06,
        "epoch": 0.649021229666391,
        "step": 4708
    },
    {
        "loss": 2.194,
        "grad_norm": 1.2684295177459717,
        "learning_rate": 1.575492096220954e-06,
        "epoch": 0.6491590846429556,
        "step": 4709
    },
    {
        "loss": 2.0847,
        "grad_norm": 1.600976824760437,
        "learning_rate": 1.5525242789025786e-06,
        "epoch": 0.6492969396195203,
        "step": 4710
    },
    {
        "loss": 1.689,
        "grad_norm": 2.717254400253296,
        "learning_rate": 1.529723797948468e-06,
        "epoch": 0.6494347945960849,
        "step": 4711
    },
    {
        "loss": 1.42,
        "grad_norm": 2.2592978477478027,
        "learning_rate": 1.5070906921137661e-06,
        "epoch": 0.6495726495726496,
        "step": 4712
    },
    {
        "loss": 1.7825,
        "grad_norm": 2.205864429473877,
        "learning_rate": 1.4846249998691997e-06,
        "epoch": 0.6497105045492142,
        "step": 4713
    },
    {
        "loss": 2.0104,
        "grad_norm": 2.120347023010254,
        "learning_rate": 1.4623267594008562e-06,
        "epoch": 0.6498483595257789,
        "step": 4714
    },
    {
        "loss": 2.16,
        "grad_norm": 1.301878571510315,
        "learning_rate": 1.4401960086102395e-06,
        "epoch": 0.6499862145023435,
        "step": 4715
    },
    {
        "loss": 1.7569,
        "grad_norm": 1.77056086063385,
        "learning_rate": 1.4182327851141375e-06,
        "epoch": 0.6501240694789082,
        "step": 4716
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.7254202365875244,
        "learning_rate": 1.396437126244643e-06,
        "epoch": 0.6502619244554728,
        "step": 4717
    },
    {
        "loss": 2.0397,
        "grad_norm": 2.4197633266448975,
        "learning_rate": 1.3748090690489324e-06,
        "epoch": 0.6503997794320375,
        "step": 4718
    },
    {
        "loss": 1.5946,
        "grad_norm": 1.7525982856750488,
        "learning_rate": 1.3533486502893989e-06,
        "epoch": 0.6505376344086021,
        "step": 4719
    },
    {
        "loss": 1.513,
        "grad_norm": 1.889915943145752,
        "learning_rate": 1.33205590644343e-06,
        "epoch": 0.6506754893851668,
        "step": 4720
    },
    {
        "loss": 1.9217,
        "grad_norm": 2.16611909866333,
        "learning_rate": 1.3109308737034199e-06,
        "epoch": 0.6508133443617314,
        "step": 4721
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.5007591247558594,
        "learning_rate": 1.2899735879767117e-06,
        "epoch": 0.6509511993382961,
        "step": 4722
    },
    {
        "loss": 1.8628,
        "grad_norm": 2.419175863265991,
        "learning_rate": 1.2691840848854774e-06,
        "epoch": 0.6510890543148607,
        "step": 4723
    },
    {
        "loss": 2.3656,
        "grad_norm": 1.3446040153503418,
        "learning_rate": 1.2485623997667616e-06,
        "epoch": 0.6512269092914255,
        "step": 4724
    },
    {
        "loss": 2.0602,
        "grad_norm": 2.39804744720459,
        "learning_rate": 1.228108567672337e-06,
        "epoch": 0.6513647642679901,
        "step": 4725
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.454987049102783,
        "learning_rate": 1.2078226233686485e-06,
        "epoch": 0.6515026192445548,
        "step": 4726
    },
    {
        "loss": 1.1891,
        "grad_norm": 2.0703587532043457,
        "learning_rate": 1.1877046013367809e-06,
        "epoch": 0.6516404742211194,
        "step": 4727
    },
    {
        "loss": 1.4932,
        "grad_norm": 1.8884730339050293,
        "learning_rate": 1.167754535772403e-06,
        "epoch": 0.6517783291976841,
        "step": 4728
    },
    {
        "loss": 2.0159,
        "grad_norm": 1.7886747121810913,
        "learning_rate": 1.1479724605856669e-06,
        "epoch": 0.6519161841742487,
        "step": 4729
    },
    {
        "loss": 2.0122,
        "grad_norm": 2.4777989387512207,
        "learning_rate": 1.1283584094012646e-06,
        "epoch": 0.6520540391508134,
        "step": 4730
    },
    {
        "loss": 0.8743,
        "grad_norm": 2.806159019470215,
        "learning_rate": 1.108912415558161e-06,
        "epoch": 0.652191894127378,
        "step": 4731
    },
    {
        "loss": 2.0377,
        "grad_norm": 2.4662373065948486,
        "learning_rate": 1.089634512109805e-06,
        "epoch": 0.6523297491039427,
        "step": 4732
    },
    {
        "loss": 2.3843,
        "grad_norm": 1.5787959098815918,
        "learning_rate": 1.0705247318238297e-06,
        "epoch": 0.6524676040805073,
        "step": 4733
    },
    {
        "loss": 2.1755,
        "grad_norm": 2.167717218399048,
        "learning_rate": 1.0515831071821414e-06,
        "epoch": 0.652605459057072,
        "step": 4734
    },
    {
        "loss": 1.9704,
        "grad_norm": 2.3053317070007324,
        "learning_rate": 1.0328096703808076e-06,
        "epoch": 0.6527433140336366,
        "step": 4735
    },
    {
        "loss": 1.5434,
        "grad_norm": 3.3557119369506836,
        "learning_rate": 1.0142044533300476e-06,
        "epoch": 0.6528811690102013,
        "step": 4736
    },
    {
        "loss": 2.206,
        "grad_norm": 2.1117756366729736,
        "learning_rate": 9.95767487654109e-07,
        "epoch": 0.6530190239867659,
        "step": 4737
    },
    {
        "loss": 2.217,
        "grad_norm": 1.0614902973175049,
        "learning_rate": 9.774988046913013e-07,
        "epoch": 0.6531568789633305,
        "step": 4738
    },
    {
        "loss": 2.2607,
        "grad_norm": 2.1211729049682617,
        "learning_rate": 9.593984354938634e-07,
        "epoch": 0.6532947339398952,
        "step": 4739
    },
    {
        "loss": 1.8708,
        "grad_norm": 3.403512954711914,
        "learning_rate": 9.414664108279292e-07,
        "epoch": 0.6534325889164598,
        "step": 4740
    },
    {
        "loss": 1.2865,
        "grad_norm": 2.862438440322876,
        "learning_rate": 9.237027611735171e-07,
        "epoch": 0.6535704438930245,
        "step": 4741
    },
    {
        "loss": 1.724,
        "grad_norm": 2.045774459838867,
        "learning_rate": 9.061075167244193e-07,
        "epoch": 0.6537082988695891,
        "step": 4742
    },
    {
        "loss": 2.155,
        "grad_norm": 1.6265000104904175,
        "learning_rate": 8.88680707388212e-07,
        "epoch": 0.6538461538461539,
        "step": 4743
    },
    {
        "loss": 2.2616,
        "grad_norm": 1.2866686582565308,
        "learning_rate": 8.714223627861895e-07,
        "epoch": 0.6539840088227185,
        "step": 4744
    },
    {
        "loss": 1.6084,
        "grad_norm": 2.141443967819214,
        "learning_rate": 8.54332512253242e-07,
        "epoch": 0.6541218637992832,
        "step": 4745
    },
    {
        "loss": 1.176,
        "grad_norm": 1.8507143259048462,
        "learning_rate": 8.374111848378774e-07,
        "epoch": 0.6542597187758478,
        "step": 4746
    },
    {
        "loss": 1.9101,
        "grad_norm": 1.887302279472351,
        "learning_rate": 8.206584093022107e-07,
        "epoch": 0.6543975737524125,
        "step": 4747
    },
    {
        "loss": 2.2335,
        "grad_norm": 1.4569486379623413,
        "learning_rate": 8.04074214121775e-07,
        "epoch": 0.6545354287289771,
        "step": 4748
    },
    {
        "loss": 1.4698,
        "grad_norm": 3.2127866744995117,
        "learning_rate": 7.876586274856323e-07,
        "epoch": 0.6546732837055418,
        "step": 4749
    },
    {
        "loss": 2.0791,
        "grad_norm": 1.6179057359695435,
        "learning_rate": 7.714116772961854e-07,
        "epoch": 0.6548111386821064,
        "step": 4750
    },
    {
        "loss": 1.5606,
        "grad_norm": 1.2539055347442627,
        "learning_rate": 7.553333911692662e-07,
        "epoch": 0.6549489936586711,
        "step": 4751
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.3401789665222168,
        "learning_rate": 7.39423796433969e-07,
        "epoch": 0.6550868486352357,
        "step": 4752
    },
    {
        "loss": 1.6362,
        "grad_norm": 1.9839868545532227,
        "learning_rate": 7.236829201326733e-07,
        "epoch": 0.6552247036118004,
        "step": 4753
    },
    {
        "loss": 1.711,
        "grad_norm": 1.8660451173782349,
        "learning_rate": 7.08110789020966e-07,
        "epoch": 0.655362558588365,
        "step": 4754
    },
    {
        "loss": 0.9621,
        "grad_norm": 2.109755516052246,
        "learning_rate": 6.927074295676516e-07,
        "epoch": 0.6555004135649297,
        "step": 4755
    },
    {
        "loss": 2.3867,
        "grad_norm": 1.9330512285232544,
        "learning_rate": 6.774728679545983e-07,
        "epoch": 0.6556382685414943,
        "step": 4756
    },
    {
        "loss": 1.6521,
        "grad_norm": 1.5923184156417847,
        "learning_rate": 6.624071300768032e-07,
        "epoch": 0.655776123518059,
        "step": 4757
    },
    {
        "loss": 2.5072,
        "grad_norm": 1.7529833316802979,
        "learning_rate": 6.47510241542304e-07,
        "epoch": 0.6559139784946236,
        "step": 4758
    },
    {
        "loss": 2.0009,
        "grad_norm": 1.9466160535812378,
        "learning_rate": 6.32782227672124e-07,
        "epoch": 0.6560518334711883,
        "step": 4759
    },
    {
        "loss": 1.9243,
        "grad_norm": 1.6599712371826172,
        "learning_rate": 6.182231135002381e-07,
        "epoch": 0.6561896884477529,
        "step": 4760
    },
    {
        "loss": 1.1621,
        "grad_norm": 2.2606215476989746,
        "learning_rate": 6.038329237735396e-07,
        "epoch": 0.6563275434243176,
        "step": 4761
    },
    {
        "loss": 1.7544,
        "grad_norm": 2.006455421447754,
        "learning_rate": 5.896116829517962e-07,
        "epoch": 0.6564653984008822,
        "step": 4762
    },
    {
        "loss": 1.7575,
        "grad_norm": 2.362807273864746,
        "learning_rate": 5.755594152076049e-07,
        "epoch": 0.656603253377447,
        "step": 4763
    },
    {
        "loss": 1.1687,
        "grad_norm": 1.701156497001648,
        "learning_rate": 5.616761444263374e-07,
        "epoch": 0.6567411083540116,
        "step": 4764
    },
    {
        "loss": 1.8992,
        "grad_norm": 1.8706519603729248,
        "learning_rate": 5.479618942061393e-07,
        "epoch": 0.6568789633305763,
        "step": 4765
    },
    {
        "loss": 1.5986,
        "grad_norm": 1.9458469152450562,
        "learning_rate": 5.344166878578194e-07,
        "epoch": 0.6570168183071409,
        "step": 4766
    },
    {
        "loss": 1.6147,
        "grad_norm": 2.2557249069213867,
        "learning_rate": 5.210405484048941e-07,
        "epoch": 0.6571546732837056,
        "step": 4767
    },
    {
        "loss": 1.3049,
        "grad_norm": 3.4788219928741455,
        "learning_rate": 5.078334985834988e-07,
        "epoch": 0.6572925282602702,
        "step": 4768
    },
    {
        "loss": 2.1474,
        "grad_norm": 1.4462651014328003,
        "learning_rate": 4.947955608423205e-07,
        "epoch": 0.6574303832368349,
        "step": 4769
    },
    {
        "loss": 2.0379,
        "grad_norm": 1.7561978101730347,
        "learning_rate": 4.819267573426656e-07,
        "epoch": 0.6575682382133995,
        "step": 4770
    },
    {
        "loss": 1.6756,
        "grad_norm": 1.989399790763855,
        "learning_rate": 4.692271099583145e-07,
        "epoch": 0.6577060931899642,
        "step": 4771
    },
    {
        "loss": 0.986,
        "grad_norm": 3.4333276748657227,
        "learning_rate": 4.566966402755002e-07,
        "epoch": 0.6578439481665288,
        "step": 4772
    },
    {
        "loss": 2.1951,
        "grad_norm": 1.7359694242477417,
        "learning_rate": 4.4433536959294085e-07,
        "epoch": 0.6579818031430935,
        "step": 4773
    },
    {
        "loss": 0.9668,
        "grad_norm": 1.9482346773147583,
        "learning_rate": 4.3214331892175165e-07,
        "epoch": 0.6581196581196581,
        "step": 4774
    },
    {
        "loss": 2.4111,
        "grad_norm": 1.2252509593963623,
        "learning_rate": 4.201205089853777e-07,
        "epoch": 0.6582575130962228,
        "step": 4775
    },
    {
        "loss": 2.123,
        "grad_norm": 1.72624671459198,
        "learning_rate": 4.082669602196276e-07,
        "epoch": 0.6583953680727874,
        "step": 4776
    },
    {
        "loss": 2.1447,
        "grad_norm": 1.543220043182373,
        "learning_rate": 3.9658269277262904e-07,
        "epoch": 0.6585332230493521,
        "step": 4777
    },
    {
        "loss": 1.5249,
        "grad_norm": 2.500481605529785,
        "learning_rate": 3.8506772650472867e-07,
        "epoch": 0.6586710780259167,
        "step": 4778
    },
    {
        "loss": 1.2303,
        "grad_norm": 1.4543181657791138,
        "learning_rate": 3.737220809885256e-07,
        "epoch": 0.6588089330024814,
        "step": 4779
    },
    {
        "loss": 2.3639,
        "grad_norm": 1.6585273742675781,
        "learning_rate": 3.6254577550879353e-07,
        "epoch": 0.658946787979046,
        "step": 4780
    },
    {
        "loss": 2.2354,
        "grad_norm": 1.5700479745864868,
        "learning_rate": 3.5153882906249216e-07,
        "epoch": 0.6590846429556106,
        "step": 4781
    },
    {
        "loss": 2.005,
        "grad_norm": 1.2803094387054443,
        "learning_rate": 3.407012603587445e-07,
        "epoch": 0.6592224979321754,
        "step": 4782
    },
    {
        "loss": 2.1419,
        "grad_norm": 1.1909310817718506,
        "learning_rate": 3.300330878187041e-07,
        "epoch": 0.65936035290874,
        "step": 4783
    },
    {
        "loss": 2.2111,
        "grad_norm": 1.3206439018249512,
        "learning_rate": 3.1953432957563256e-07,
        "epoch": 0.6594982078853047,
        "step": 4784
    },
    {
        "loss": 1.5966,
        "grad_norm": 1.8912919759750366,
        "learning_rate": 3.0920500347483283e-07,
        "epoch": 0.6596360628618693,
        "step": 4785
    },
    {
        "loss": 1.487,
        "grad_norm": 2.2459466457366943,
        "learning_rate": 2.990451270735828e-07,
        "epoch": 0.659773917838434,
        "step": 4786
    },
    {
        "loss": 2.4877,
        "grad_norm": 2.2897744178771973,
        "learning_rate": 2.890547176411906e-07,
        "epoch": 0.6599117728149986,
        "step": 4787
    },
    {
        "loss": 1.8872,
        "grad_norm": 2.705108165740967,
        "learning_rate": 2.792337921588506e-07,
        "epoch": 0.6600496277915633,
        "step": 4788
    },
    {
        "loss": 1.8436,
        "grad_norm": 2.2323222160339355,
        "learning_rate": 2.6958236731972065e-07,
        "epoch": 0.6601874827681279,
        "step": 4789
    },
    {
        "loss": 1.9376,
        "grad_norm": 1.3491042852401733,
        "learning_rate": 2.6010045952885585e-07,
        "epoch": 0.6603253377446926,
        "step": 4790
    },
    {
        "loss": 1.8491,
        "grad_norm": 1.4534709453582764,
        "learning_rate": 2.507880849031197e-07,
        "epoch": 0.6604631927212572,
        "step": 4791
    },
    {
        "loss": 1.9432,
        "grad_norm": 1.6314717531204224,
        "learning_rate": 2.4164525927126146e-07,
        "epoch": 0.6606010476978219,
        "step": 4792
    },
    {
        "loss": 0.4087,
        "grad_norm": 1.9904361963272095,
        "learning_rate": 2.32671998173839e-07,
        "epoch": 0.6607389026743865,
        "step": 4793
    },
    {
        "loss": 2.2026,
        "grad_norm": 1.5864274501800537,
        "learning_rate": 2.2386831686315168e-07,
        "epoch": 0.6608767576509512,
        "step": 4794
    },
    {
        "loss": 2.3572,
        "grad_norm": 1.5013377666473389,
        "learning_rate": 2.1523423030329616e-07,
        "epoch": 0.6610146126275158,
        "step": 4795
    },
    {
        "loss": 1.6441,
        "grad_norm": 1.3703837394714355,
        "learning_rate": 2.067697531700885e-07,
        "epoch": 0.6611524676040805,
        "step": 4796
    },
    {
        "loss": 2.1712,
        "grad_norm": 1.2334363460540771,
        "learning_rate": 1.9847489985103106e-07,
        "epoch": 0.6612903225806451,
        "step": 4797
    },
    {
        "loss": 2.1145,
        "grad_norm": 1.294029951095581,
        "learning_rate": 1.9034968444533453e-07,
        "epoch": 0.6614281775572098,
        "step": 4798
    },
    {
        "loss": 2.0972,
        "grad_norm": 1.865610957145691,
        "learning_rate": 1.8239412076385132e-07,
        "epoch": 0.6615660325337744,
        "step": 4799
    },
    {
        "loss": 1.7571,
        "grad_norm": 3.592599868774414,
        "learning_rate": 1.746082223290757e-07,
        "epoch": 0.6617038875103392,
        "step": 4800
    },
    {
        "loss": 1.4217,
        "grad_norm": 2.1203701496124268,
        "learning_rate": 1.6699200237511037e-07,
        "epoch": 0.6618417424869038,
        "step": 4801
    },
    {
        "loss": 2.2652,
        "grad_norm": 1.2037391662597656,
        "learning_rate": 1.595454738476332e-07,
        "epoch": 0.6619795974634685,
        "step": 4802
    },
    {
        "loss": 2.1067,
        "grad_norm": 1.659609079360962,
        "learning_rate": 1.5226864940391938e-07,
        "epoch": 0.6621174524400331,
        "step": 4803
    },
    {
        "loss": 2.4693,
        "grad_norm": 1.2897673845291138,
        "learning_rate": 1.4516154141277495e-07,
        "epoch": 0.6622553074165978,
        "step": 4804
    },
    {
        "loss": 2.0004,
        "grad_norm": 1.2636537551879883,
        "learning_rate": 1.3822416195449216e-07,
        "epoch": 0.6623931623931624,
        "step": 4805
    },
    {
        "loss": 1.8807,
        "grad_norm": 2.273059844970703,
        "learning_rate": 1.3145652282093856e-07,
        "epoch": 0.6625310173697271,
        "step": 4806
    },
    {
        "loss": 1.2562,
        "grad_norm": 2.382689952850342,
        "learning_rate": 1.2485863551540133e-07,
        "epoch": 0.6626688723462917,
        "step": 4807
    },
    {
        "loss": 1.2569,
        "grad_norm": 2.1089284420013428,
        "learning_rate": 1.1843051125266513e-07,
        "epoch": 0.6628067273228564,
        "step": 4808
    },
    {
        "loss": 1.9493,
        "grad_norm": 2.854473352432251,
        "learning_rate": 1.1217216095896765e-07,
        "epoch": 0.662944582299421,
        "step": 4809
    },
    {
        "loss": 2.0814,
        "grad_norm": 1.5458300113677979,
        "learning_rate": 1.0608359527193302e-07,
        "epoch": 0.6630824372759857,
        "step": 4810
    },
    {
        "loss": 2.6598,
        "grad_norm": 1.528306007385254,
        "learning_rate": 1.0016482454062726e-07,
        "epoch": 0.6632202922525503,
        "step": 4811
    },
    {
        "loss": 1.7524,
        "grad_norm": 1.8892143964767456,
        "learning_rate": 9.441585882550286e-08,
        "epoch": 0.663358147229115,
        "step": 4812
    },
    {
        "loss": 2.2111,
        "grad_norm": 2.1325314044952393,
        "learning_rate": 8.88367078983654e-08,
        "epoch": 0.6634960022056796,
        "step": 4813
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.487729787826538,
        "learning_rate": 8.34273812424069e-08,
        "epoch": 0.6636338571822443,
        "step": 4814
    },
    {
        "loss": 1.4613,
        "grad_norm": 1.9821672439575195,
        "learning_rate": 7.818788805213917e-08,
        "epoch": 0.6637717121588089,
        "step": 4815
    },
    {
        "loss": 2.1718,
        "grad_norm": 1.8198508024215698,
        "learning_rate": 7.311823723340494e-08,
        "epoch": 0.6639095671353736,
        "step": 4816
    },
    {
        "loss": 1.3063,
        "grad_norm": 1.6787275075912476,
        "learning_rate": 6.821843740335565e-08,
        "epoch": 0.6640474221119382,
        "step": 4817
    },
    {
        "loss": 2.137,
        "grad_norm": 1.5933130979537964,
        "learning_rate": 6.348849689044035e-08,
        "epoch": 0.664185277088503,
        "step": 4818
    },
    {
        "loss": 1.8096,
        "grad_norm": 1.6632870435714722,
        "learning_rate": 5.892842373438345e-08,
        "epoch": 0.6643231320650675,
        "step": 4819
    },
    {
        "loss": 1.0271,
        "grad_norm": 2.9362666606903076,
        "learning_rate": 5.453822568618483e-08,
        "epoch": 0.6644609870416323,
        "step": 4820
    },
    {
        "loss": 1.9252,
        "grad_norm": 1.5750592947006226,
        "learning_rate": 5.031791020810861e-08,
        "epoch": 0.6645988420181969,
        "step": 4821
    },
    {
        "loss": 2.4044,
        "grad_norm": 2.72204852104187,
        "learning_rate": 4.626748447362772e-08,
        "epoch": 0.6647366969947615,
        "step": 4822
    },
    {
        "loss": 1.1465,
        "grad_norm": 2.5591695308685303,
        "learning_rate": 4.238695536746829e-08,
        "epoch": 0.6648745519713262,
        "step": 4823
    },
    {
        "loss": 1.7217,
        "grad_norm": 2.0075037479400635,
        "learning_rate": 3.8676329485587416e-08,
        "epoch": 0.6650124069478908,
        "step": 4824
    },
    {
        "loss": 1.7431,
        "grad_norm": 2.1504099369049072,
        "learning_rate": 3.51356131351066e-08,
        "epoch": 0.6651502619244555,
        "step": 4825
    },
    {
        "loss": 2.1621,
        "grad_norm": 1.9652613401412964,
        "learning_rate": 3.176481233437833e-08,
        "epoch": 0.6652881169010201,
        "step": 4826
    },
    {
        "loss": 2.2048,
        "grad_norm": 1.819385290145874,
        "learning_rate": 2.8563932812941673e-08,
        "epoch": 0.6654259718775848,
        "step": 4827
    },
    {
        "loss": 1.7588,
        "grad_norm": 2.903623580932617,
        "learning_rate": 2.553298001148896e-08,
        "epoch": 0.6655638268541494,
        "step": 4828
    },
    {
        "loss": 1.3998,
        "grad_norm": 2.0185935497283936,
        "learning_rate": 2.2671959081888018e-08,
        "epoch": 0.6657016818307141,
        "step": 4829
    },
    {
        "loss": 2.2963,
        "grad_norm": 1.8817033767700195,
        "learning_rate": 1.998087488717104e-08,
        "epoch": 0.6658395368072787,
        "step": 4830
    },
    {
        "loss": 2.288,
        "grad_norm": 2.203706741333008,
        "learning_rate": 1.745973200152351e-08,
        "epoch": 0.6659773917838434,
        "step": 4831
    },
    {
        "loss": 2.313,
        "grad_norm": 1.3313690423965454,
        "learning_rate": 1.5108534710250864e-08,
        "epoch": 0.666115246760408,
        "step": 4832
    },
    {
        "loss": 2.221,
        "grad_norm": 1.1856218576431274,
        "learning_rate": 1.2927287009811829e-08,
        "epoch": 0.6662531017369727,
        "step": 4833
    },
    {
        "loss": 2.0494,
        "grad_norm": 2.1089928150177,
        "learning_rate": 1.0915992607796188e-08,
        "epoch": 0.6663909567135373,
        "step": 4834
    },
    {
        "loss": 1.3888,
        "grad_norm": 1.9848570823669434,
        "learning_rate": 9.074654922891502e-09,
        "epoch": 0.666528811690102,
        "step": 4835
    },
    {
        "loss": 1.1939,
        "grad_norm": 2.652231216430664,
        "learning_rate": 7.403277084927495e-09,
        "epoch": 0.6666666666666666,
        "step": 4836
    },
    {
        "loss": 2.0977,
        "grad_norm": 2.3617782592773438,
        "learning_rate": 5.901861934842768e-09,
        "epoch": 0.6668045216432313,
        "step": 4837
    },
    {
        "loss": 1.6141,
        "grad_norm": 1.9855732917785645,
        "learning_rate": 4.570412024640369e-09,
        "epoch": 0.6669423766197959,
        "step": 4838
    },
    {
        "loss": 1.859,
        "grad_norm": 1.5569267272949219,
        "learning_rate": 3.4089296174877326e-09,
        "epoch": 0.6670802315963607,
        "step": 4839
    },
    {
        "loss": 2.4688,
        "grad_norm": 2.0831522941589355,
        "learning_rate": 2.4174166876056447e-09,
        "epoch": 0.6672180865729253,
        "step": 4840
    },
    {
        "loss": 2.118,
        "grad_norm": 1.8297384977340698,
        "learning_rate": 1.5958749203237589e-09,
        "epoch": 0.66735594154949,
        "step": 4841
    },
    {
        "loss": 1.1506,
        "grad_norm": 2.392627239227295,
        "learning_rate": 9.44305712058391e-10,
        "epoch": 0.6674937965260546,
        "step": 4842
    },
    {
        "loss": 2.1905,
        "grad_norm": 1.0613923072814941,
        "learning_rate": 4.6271017030141694e-10,
        "epoch": 0.6676316515026193,
        "step": 4843
    },
    {
        "loss": 1.8424,
        "grad_norm": 1.8833473920822144,
        "learning_rate": 1.5108911367578417e-10,
        "epoch": 0.6677695064791839,
        "step": 4844
    },
    {
        "loss": 2.3028,
        "grad_norm": 1.6070820093154907,
        "learning_rate": 9.4430718355909e-12,
        "epoch": 0.6679073614557486,
        "step": 4845
    },
    {
        "loss": 1.2738,
        "grad_norm": 1.8694559335708618,
        "learning_rate": 0.00019999996222771444,
        "epoch": 0.6680452164323132,
        "step": 4846
    },
    {
        "loss": 2.294,
        "grad_norm": 1.6716282367706299,
        "learning_rate": 0.00019999976392329335,
        "epoch": 0.6681830714088779,
        "step": 4847
    },
    {
        "loss": 2.0925,
        "grad_norm": 1.5031532049179077,
        "learning_rate": 0.00019999939564400187,
        "epoch": 0.6683209263854425,
        "step": 4848
    },
    {
        "loss": 1.9312,
        "grad_norm": 1.206821322441101,
        "learning_rate": 0.00019999885739046606,
        "epoch": 0.6684587813620072,
        "step": 4849
    },
    {
        "loss": 2.4211,
        "grad_norm": 1.5951101779937744,
        "learning_rate": 0.0001999981491636008,
        "epoch": 0.6685966363385718,
        "step": 4850
    },
    {
        "loss": 1.7912,
        "grad_norm": 2.1044411659240723,
        "learning_rate": 0.0001999972709646099,
        "epoch": 0.6687344913151365,
        "step": 4851
    },
    {
        "loss": 1.8078,
        "grad_norm": 2.2937722206115723,
        "learning_rate": 0.00019999622279498605,
        "epoch": 0.6688723462917011,
        "step": 4852
    },
    {
        "loss": 1.9313,
        "grad_norm": 1.6474814414978027,
        "learning_rate": 0.00019999500465651092,
        "epoch": 0.6690102012682658,
        "step": 4853
    },
    {
        "loss": 2.1154,
        "grad_norm": 1.4172346591949463,
        "learning_rate": 0.00019999361655125505,
        "epoch": 0.6691480562448304,
        "step": 4854
    },
    {
        "loss": 1.8798,
        "grad_norm": 3.179356575012207,
        "learning_rate": 0.00019999205848157784,
        "epoch": 0.6692859112213951,
        "step": 4855
    },
    {
        "loss": 1.8855,
        "grad_norm": 1.406338095664978,
        "learning_rate": 0.00019999033045012764,
        "epoch": 0.6694237661979597,
        "step": 4856
    },
    {
        "loss": 2.4305,
        "grad_norm": 2.0438947677612305,
        "learning_rate": 0.00019998843245984168,
        "epoch": 0.6695616211745244,
        "step": 4857
    },
    {
        "loss": 1.7309,
        "grad_norm": 1.1816965341567993,
        "learning_rate": 0.00019998636451394605,
        "epoch": 0.669699476151089,
        "step": 4858
    },
    {
        "loss": 1.3737,
        "grad_norm": 1.5594124794006348,
        "learning_rate": 0.0001999841266159558,
        "epoch": 0.6698373311276538,
        "step": 4859
    },
    {
        "loss": 1.6487,
        "grad_norm": 2.3378407955169678,
        "learning_rate": 0.00019998171876967473,
        "epoch": 0.6699751861042184,
        "step": 4860
    },
    {
        "loss": 1.8349,
        "grad_norm": 2.390042543411255,
        "learning_rate": 0.00019997914097919566,
        "epoch": 0.6701130410807831,
        "step": 4861
    },
    {
        "loss": 2.2332,
        "grad_norm": 1.1955229043960571,
        "learning_rate": 0.00019997639324890014,
        "epoch": 0.6702508960573477,
        "step": 4862
    },
    {
        "loss": 1.3166,
        "grad_norm": 1.8574528694152832,
        "learning_rate": 0.00019997347558345867,
        "epoch": 0.6703887510339124,
        "step": 4863
    },
    {
        "loss": 1.8987,
        "grad_norm": 2.0441765785217285,
        "learning_rate": 0.00019997038798783053,
        "epoch": 0.670526606010477,
        "step": 4864
    },
    {
        "loss": 1.5521,
        "grad_norm": 2.9341976642608643,
        "learning_rate": 0.00019996713046726384,
        "epoch": 0.6706644609870416,
        "step": 4865
    },
    {
        "loss": 1.8671,
        "grad_norm": 1.9669419527053833,
        "learning_rate": 0.0001999637030272957,
        "epoch": 0.6708023159636063,
        "step": 4866
    },
    {
        "loss": 1.5972,
        "grad_norm": 2.2277355194091797,
        "learning_rate": 0.00019996010567375176,
        "epoch": 0.6709401709401709,
        "step": 4867
    },
    {
        "loss": 2.0471,
        "grad_norm": 2.7846436500549316,
        "learning_rate": 0.00019995633841274674,
        "epoch": 0.6710780259167356,
        "step": 4868
    },
    {
        "loss": 1.68,
        "grad_norm": 2.289337396621704,
        "learning_rate": 0.000199952401250684,
        "epoch": 0.6712158808933002,
        "step": 4869
    },
    {
        "loss": 1.8907,
        "grad_norm": 2.566026210784912,
        "learning_rate": 0.00019994829419425574,
        "epoch": 0.6713537358698649,
        "step": 4870
    },
    {
        "loss": 2.0318,
        "grad_norm": 2.2719838619232178,
        "learning_rate": 0.00019994401725044299,
        "epoch": 0.6714915908464295,
        "step": 4871
    },
    {
        "loss": 1.991,
        "grad_norm": 1.408551812171936,
        "learning_rate": 0.00019993957042651546,
        "epoch": 0.6716294458229942,
        "step": 4872
    },
    {
        "loss": 0.7288,
        "grad_norm": 1.8027297258377075,
        "learning_rate": 0.0001999349537300316,
        "epoch": 0.6717673007995588,
        "step": 4873
    },
    {
        "loss": 1.839,
        "grad_norm": 2.0228238105773926,
        "learning_rate": 0.00019993016716883877,
        "epoch": 0.6719051557761235,
        "step": 4874
    },
    {
        "loss": 2.1893,
        "grad_norm": 2.2237532138824463,
        "learning_rate": 0.0001999252107510728,
        "epoch": 0.6720430107526881,
        "step": 4875
    },
    {
        "loss": 1.9338,
        "grad_norm": 2.196018934249878,
        "learning_rate": 0.00019992008448515853,
        "epoch": 0.6721808657292528,
        "step": 4876
    },
    {
        "loss": 2.4198,
        "grad_norm": 1.4481462240219116,
        "learning_rate": 0.00019991478837980923,
        "epoch": 0.6723187207058174,
        "step": 4877
    },
    {
        "loss": 2.2731,
        "grad_norm": 1.5903478860855103,
        "learning_rate": 0.00019990932244402703,
        "epoch": 0.6724565756823822,
        "step": 4878
    },
    {
        "loss": 1.8967,
        "grad_norm": 1.8310065269470215,
        "learning_rate": 0.00019990368668710263,
        "epoch": 0.6725944306589468,
        "step": 4879
    },
    {
        "loss": 1.5616,
        "grad_norm": 2.1155846118927,
        "learning_rate": 0.00019989788111861542,
        "epoch": 0.6727322856355115,
        "step": 4880
    },
    {
        "loss": 1.6883,
        "grad_norm": 1.5249744653701782,
        "learning_rate": 0.00019989190574843345,
        "epoch": 0.6728701406120761,
        "step": 4881
    },
    {
        "loss": 2.1215,
        "grad_norm": 1.3605921268463135,
        "learning_rate": 0.0001998857605867134,
        "epoch": 0.6730079955886408,
        "step": 4882
    },
    {
        "loss": 1.9689,
        "grad_norm": 1.3162051439285278,
        "learning_rate": 0.00019987944564390047,
        "epoch": 0.6731458505652054,
        "step": 4883
    },
    {
        "loss": 1.854,
        "grad_norm": 2.4421656131744385,
        "learning_rate": 0.00019987296093072854,
        "epoch": 0.6732837055417701,
        "step": 4884
    },
    {
        "loss": 1.9468,
        "grad_norm": 2.139503002166748,
        "learning_rate": 0.00019986630645821998,
        "epoch": 0.6734215605183347,
        "step": 4885
    },
    {
        "loss": 2.2687,
        "grad_norm": 2.067690849304199,
        "learning_rate": 0.0001998594822376858,
        "epoch": 0.6735594154948994,
        "step": 4886
    },
    {
        "loss": 2.35,
        "grad_norm": 2.482239246368408,
        "learning_rate": 0.00019985248828072546,
        "epoch": 0.673697270471464,
        "step": 4887
    },
    {
        "loss": 1.7557,
        "grad_norm": 1.796364665031433,
        "learning_rate": 0.00019984532459922693,
        "epoch": 0.6738351254480287,
        "step": 4888
    },
    {
        "loss": 1.4448,
        "grad_norm": 2.4765994548797607,
        "learning_rate": 0.00019983799120536672,
        "epoch": 0.6739729804245933,
        "step": 4889
    },
    {
        "loss": 1.4846,
        "grad_norm": 2.67671799659729,
        "learning_rate": 0.00019983048811160982,
        "epoch": 0.674110835401158,
        "step": 4890
    },
    {
        "loss": 2.35,
        "grad_norm": 1.5083140134811401,
        "learning_rate": 0.0001998228153307096,
        "epoch": 0.6742486903777226,
        "step": 4891
    },
    {
        "loss": 2.0714,
        "grad_norm": 1.7756686210632324,
        "learning_rate": 0.0001998149728757079,
        "epoch": 0.6743865453542873,
        "step": 4892
    },
    {
        "loss": 2.2916,
        "grad_norm": 1.93329656124115,
        "learning_rate": 0.00019980696075993495,
        "epoch": 0.6745244003308519,
        "step": 4893
    },
    {
        "loss": 2.5211,
        "grad_norm": 2.052332878112793,
        "learning_rate": 0.00019979877899700934,
        "epoch": 0.6746622553074166,
        "step": 4894
    },
    {
        "loss": 1.3942,
        "grad_norm": 2.292072296142578,
        "learning_rate": 0.00019979042760083808,
        "epoch": 0.6748001102839812,
        "step": 4895
    },
    {
        "loss": 1.8555,
        "grad_norm": 1.931663155555725,
        "learning_rate": 0.00019978190658561648,
        "epoch": 0.674937965260546,
        "step": 4896
    },
    {
        "loss": 2.2356,
        "grad_norm": 1.2581040859222412,
        "learning_rate": 0.00019977321596582813,
        "epoch": 0.6750758202371105,
        "step": 4897
    },
    {
        "loss": 1.7776,
        "grad_norm": 1.851932168006897,
        "learning_rate": 0.00019976435575624497,
        "epoch": 0.6752136752136753,
        "step": 4898
    },
    {
        "loss": 1.8632,
        "grad_norm": 2.634828805923462,
        "learning_rate": 0.00019975532597192713,
        "epoch": 0.6753515301902399,
        "step": 4899
    },
    {
        "loss": 2.432,
        "grad_norm": 1.7753928899765015,
        "learning_rate": 0.000199746126628223,
        "epoch": 0.6754893851668046,
        "step": 4900
    },
    {
        "loss": 1.9536,
        "grad_norm": 1.340438961982727,
        "learning_rate": 0.00019973675774076925,
        "epoch": 0.6756272401433692,
        "step": 4901
    },
    {
        "loss": 1.8648,
        "grad_norm": 2.0425753593444824,
        "learning_rate": 0.00019972721932549062,
        "epoch": 0.6757650951199339,
        "step": 4902
    },
    {
        "loss": 1.8372,
        "grad_norm": 1.6858445405960083,
        "learning_rate": 0.0001997175113986001,
        "epoch": 0.6759029500964985,
        "step": 4903
    },
    {
        "loss": 2.3999,
        "grad_norm": 1.926125168800354,
        "learning_rate": 0.0001997076339765987,
        "epoch": 0.6760408050730632,
        "step": 4904
    },
    {
        "loss": 1.536,
        "grad_norm": 2.451357364654541,
        "learning_rate": 0.00019969758707627565,
        "epoch": 0.6761786600496278,
        "step": 4905
    },
    {
        "loss": 1.827,
        "grad_norm": 1.796795129776001,
        "learning_rate": 0.0001996873707147082,
        "epoch": 0.6763165150261925,
        "step": 4906
    },
    {
        "loss": 2.2455,
        "grad_norm": 1.5389801263809204,
        "learning_rate": 0.0001996769849092616,
        "epoch": 0.6764543700027571,
        "step": 4907
    },
    {
        "loss": 2.1051,
        "grad_norm": 1.4237500429153442,
        "learning_rate": 0.0001996664296775892,
        "epoch": 0.6765922249793217,
        "step": 4908
    },
    {
        "loss": 1.538,
        "grad_norm": 1.9316962957382202,
        "learning_rate": 0.0001996557050376322,
        "epoch": 0.6767300799558864,
        "step": 4909
    },
    {
        "loss": 1.7886,
        "grad_norm": 2.6362059116363525,
        "learning_rate": 0.00019964481100761994,
        "epoch": 0.676867934932451,
        "step": 4910
    },
    {
        "loss": 1.9324,
        "grad_norm": 2.279958486557007,
        "learning_rate": 0.0001996337476060695,
        "epoch": 0.6770057899090157,
        "step": 4911
    },
    {
        "loss": 1.5923,
        "grad_norm": 1.5294936895370483,
        "learning_rate": 0.00019962251485178595,
        "epoch": 0.6771436448855803,
        "step": 4912
    },
    {
        "loss": 2.0583,
        "grad_norm": 1.8115694522857666,
        "learning_rate": 0.0001996111127638622,
        "epoch": 0.677281499862145,
        "step": 4913
    },
    {
        "loss": 1.8233,
        "grad_norm": 2.163996458053589,
        "learning_rate": 0.000199599541361679,
        "epoch": 0.6774193548387096,
        "step": 4914
    },
    {
        "loss": 2.1117,
        "grad_norm": 2.177969217300415,
        "learning_rate": 0.00019958780066490483,
        "epoch": 0.6775572098152743,
        "step": 4915
    },
    {
        "loss": 2.3632,
        "grad_norm": 1.975529432296753,
        "learning_rate": 0.000199575890693496,
        "epoch": 0.6776950647918389,
        "step": 4916
    },
    {
        "loss": 1.3391,
        "grad_norm": 2.267655372619629,
        "learning_rate": 0.0001995638114676965,
        "epoch": 0.6778329197684037,
        "step": 4917
    },
    {
        "loss": 2.0893,
        "grad_norm": 1.5490676164627075,
        "learning_rate": 0.00019955156300803805,
        "epoch": 0.6779707747449683,
        "step": 4918
    },
    {
        "loss": 2.1072,
        "grad_norm": 1.833888292312622,
        "learning_rate": 0.00019953914533533994,
        "epoch": 0.678108629721533,
        "step": 4919
    },
    {
        "loss": 1.3491,
        "grad_norm": 2.5586395263671875,
        "learning_rate": 0.0001995265584707092,
        "epoch": 0.6782464846980976,
        "step": 4920
    },
    {
        "loss": 2.0026,
        "grad_norm": 3.6079325675964355,
        "learning_rate": 0.00019951380243554044,
        "epoch": 0.6783843396746623,
        "step": 4921
    },
    {
        "loss": 1.7256,
        "grad_norm": 1.7712316513061523,
        "learning_rate": 0.00019950087725151568,
        "epoch": 0.6785221946512269,
        "step": 4922
    },
    {
        "loss": 1.489,
        "grad_norm": 1.5557520389556885,
        "learning_rate": 0.00019948778294060456,
        "epoch": 0.6786600496277916,
        "step": 4923
    },
    {
        "loss": 2.177,
        "grad_norm": 1.7486772537231445,
        "learning_rate": 0.0001994745195250641,
        "epoch": 0.6787979046043562,
        "step": 4924
    },
    {
        "loss": 1.3592,
        "grad_norm": 2.5009701251983643,
        "learning_rate": 0.00019946108702743897,
        "epoch": 0.6789357595809209,
        "step": 4925
    },
    {
        "loss": 1.6571,
        "grad_norm": 3.012699604034424,
        "learning_rate": 0.000199447485470561,
        "epoch": 0.6790736145574855,
        "step": 4926
    },
    {
        "loss": 1.8666,
        "grad_norm": 2.583711624145508,
        "learning_rate": 0.0001994337148775495,
        "epoch": 0.6792114695340502,
        "step": 4927
    },
    {
        "loss": 1.5908,
        "grad_norm": 1.6904089450836182,
        "learning_rate": 0.00019941977527181105,
        "epoch": 0.6793493245106148,
        "step": 4928
    },
    {
        "loss": 0.9562,
        "grad_norm": 1.9464237689971924,
        "learning_rate": 0.00019940566667703955,
        "epoch": 0.6794871794871795,
        "step": 4929
    },
    {
        "loss": 1.9686,
        "grad_norm": 1.3604387044906616,
        "learning_rate": 0.00019939138911721617,
        "epoch": 0.6796250344637441,
        "step": 4930
    },
    {
        "loss": 1.5966,
        "grad_norm": 2.0367937088012695,
        "learning_rate": 0.00019937694261660914,
        "epoch": 0.6797628894403088,
        "step": 4931
    },
    {
        "loss": 2.0448,
        "grad_norm": 1.8432786464691162,
        "learning_rate": 0.00019936232719977397,
        "epoch": 0.6799007444168734,
        "step": 4932
    },
    {
        "loss": 1.6403,
        "grad_norm": 2.4964404106140137,
        "learning_rate": 0.00019934754289155327,
        "epoch": 0.6800385993934381,
        "step": 4933
    },
    {
        "loss": 1.8057,
        "grad_norm": 2.3923988342285156,
        "learning_rate": 0.00019933258971707672,
        "epoch": 0.6801764543700027,
        "step": 4934
    },
    {
        "loss": 1.8466,
        "grad_norm": 2.066378355026245,
        "learning_rate": 0.00019931746770176102,
        "epoch": 0.6803143093465674,
        "step": 4935
    },
    {
        "loss": 2.3325,
        "grad_norm": 1.4269778728485107,
        "learning_rate": 0.0001993021768713098,
        "epoch": 0.680452164323132,
        "step": 4936
    },
    {
        "loss": 0.9619,
        "grad_norm": 2.165198564529419,
        "learning_rate": 0.00019928671725171374,
        "epoch": 0.6805900192996968,
        "step": 4937
    },
    {
        "loss": 2.2634,
        "grad_norm": 1.8577758073806763,
        "learning_rate": 0.0001992710888692504,
        "epoch": 0.6807278742762614,
        "step": 4938
    },
    {
        "loss": 1.5482,
        "grad_norm": 3.7201220989227295,
        "learning_rate": 0.00019925529175048408,
        "epoch": 0.6808657292528261,
        "step": 4939
    },
    {
        "loss": 1.4918,
        "grad_norm": 2.332202911376953,
        "learning_rate": 0.00019923932592226604,
        "epoch": 0.6810035842293907,
        "step": 4940
    },
    {
        "loss": 1.5689,
        "grad_norm": 2.0453481674194336,
        "learning_rate": 0.00019922319141173423,
        "epoch": 0.6811414392059554,
        "step": 4941
    },
    {
        "loss": 1.5487,
        "grad_norm": 2.570070743560791,
        "learning_rate": 0.0001992068882463133,
        "epoch": 0.68127929418252,
        "step": 4942
    },
    {
        "loss": 2.0051,
        "grad_norm": 1.952850580215454,
        "learning_rate": 0.00019919041645371466,
        "epoch": 0.6814171491590847,
        "step": 4943
    },
    {
        "loss": 1.7075,
        "grad_norm": 2.112278699874878,
        "learning_rate": 0.0001991737760619362,
        "epoch": 0.6815550041356493,
        "step": 4944
    },
    {
        "loss": 1.6882,
        "grad_norm": 2.2521286010742188,
        "learning_rate": 0.00019915696709926252,
        "epoch": 0.681692859112214,
        "step": 4945
    },
    {
        "loss": 1.8093,
        "grad_norm": 1.4342390298843384,
        "learning_rate": 0.0001991399895942647,
        "epoch": 0.6818307140887786,
        "step": 4946
    },
    {
        "loss": 1.0882,
        "grad_norm": 2.428497552871704,
        "learning_rate": 0.00019912284357580031,
        "epoch": 0.6819685690653433,
        "step": 4947
    },
    {
        "loss": 1.8945,
        "grad_norm": 1.9030641317367554,
        "learning_rate": 0.00019910552907301333,
        "epoch": 0.6821064240419079,
        "step": 4948
    },
    {
        "loss": 1.4975,
        "grad_norm": 1.7144958972930908,
        "learning_rate": 0.0001990880461153341,
        "epoch": 0.6822442790184726,
        "step": 4949
    },
    {
        "loss": 1.83,
        "grad_norm": 2.062974452972412,
        "learning_rate": 0.00019907039473247937,
        "epoch": 0.6823821339950372,
        "step": 4950
    },
    {
        "loss": 0.9095,
        "grad_norm": 2.1743054389953613,
        "learning_rate": 0.00019905257495445212,
        "epoch": 0.6825199889716018,
        "step": 4951
    },
    {
        "loss": 2.5868,
        "grad_norm": 2.105125665664673,
        "learning_rate": 0.00019903458681154153,
        "epoch": 0.6826578439481665,
        "step": 4952
    },
    {
        "loss": 1.5958,
        "grad_norm": 2.51055908203125,
        "learning_rate": 0.00019901643033432302,
        "epoch": 0.6827956989247311,
        "step": 4953
    },
    {
        "loss": 2.2576,
        "grad_norm": 2.2339978218078613,
        "learning_rate": 0.0001989981055536581,
        "epoch": 0.6829335539012958,
        "step": 4954
    },
    {
        "loss": 1.792,
        "grad_norm": 2.3748788833618164,
        "learning_rate": 0.00019897961250069443,
        "epoch": 0.6830714088778604,
        "step": 4955
    },
    {
        "loss": 2.0544,
        "grad_norm": 2.2943100929260254,
        "learning_rate": 0.00019896095120686559,
        "epoch": 0.6832092638544252,
        "step": 4956
    },
    {
        "loss": 1.4471,
        "grad_norm": 2.636669158935547,
        "learning_rate": 0.00019894212170389107,
        "epoch": 0.6833471188309898,
        "step": 4957
    },
    {
        "loss": 2.0077,
        "grad_norm": 1.917288064956665,
        "learning_rate": 0.0001989231240237765,
        "epoch": 0.6834849738075545,
        "step": 4958
    },
    {
        "loss": 1.0963,
        "grad_norm": 2.2310054302215576,
        "learning_rate": 0.00019890395819881318,
        "epoch": 0.6836228287841191,
        "step": 4959
    },
    {
        "loss": 1.317,
        "grad_norm": 2.682112216949463,
        "learning_rate": 0.00019888462426157828,
        "epoch": 0.6837606837606838,
        "step": 4960
    },
    {
        "loss": 2.13,
        "grad_norm": 3.333379030227661,
        "learning_rate": 0.00019886512224493474,
        "epoch": 0.6838985387372484,
        "step": 4961
    },
    {
        "loss": 2.3916,
        "grad_norm": 1.5980087518692017,
        "learning_rate": 0.0001988454521820311,
        "epoch": 0.6840363937138131,
        "step": 4962
    },
    {
        "loss": 2.0269,
        "grad_norm": 2.1648342609405518,
        "learning_rate": 0.00019882561410630164,
        "epoch": 0.6841742486903777,
        "step": 4963
    },
    {
        "loss": 2.369,
        "grad_norm": 1.7440142631530762,
        "learning_rate": 0.0001988056080514662,
        "epoch": 0.6843121036669424,
        "step": 4964
    },
    {
        "loss": 2.1478,
        "grad_norm": 2.1922519207000732,
        "learning_rate": 0.00019878543405153009,
        "epoch": 0.684449958643507,
        "step": 4965
    },
    {
        "loss": 2.0219,
        "grad_norm": 1.2612321376800537,
        "learning_rate": 0.00019876509214078415,
        "epoch": 0.6845878136200717,
        "step": 4966
    },
    {
        "loss": 2.0373,
        "grad_norm": 2.252506971359253,
        "learning_rate": 0.0001987445823538046,
        "epoch": 0.6847256685966363,
        "step": 4967
    },
    {
        "loss": 2.4151,
        "grad_norm": 1.1052824258804321,
        "learning_rate": 0.00019872390472545302,
        "epoch": 0.684863523573201,
        "step": 4968
    },
    {
        "loss": 1.2517,
        "grad_norm": 2.6764650344848633,
        "learning_rate": 0.00019870305929087622,
        "epoch": 0.6850013785497656,
        "step": 4969
    },
    {
        "loss": 2.393,
        "grad_norm": 2.789885997772217,
        "learning_rate": 0.00019868204608550628,
        "epoch": 0.6851392335263303,
        "step": 4970
    },
    {
        "loss": 2.4283,
        "grad_norm": 1.4055160284042358,
        "learning_rate": 0.00019866086514506052,
        "epoch": 0.6852770885028949,
        "step": 4971
    },
    {
        "loss": 1.8778,
        "grad_norm": 2.1801576614379883,
        "learning_rate": 0.0001986395165055412,
        "epoch": 0.6854149434794596,
        "step": 4972
    },
    {
        "loss": 1.5753,
        "grad_norm": 2.3844664096832275,
        "learning_rate": 0.00019861800020323585,
        "epoch": 0.6855527984560242,
        "step": 4973
    },
    {
        "loss": 1.3547,
        "grad_norm": 1.7532639503479004,
        "learning_rate": 0.00019859631627471678,
        "epoch": 0.685690653432589,
        "step": 4974
    },
    {
        "loss": 1.0929,
        "grad_norm": 2.4795970916748047,
        "learning_rate": 0.00019857446475684133,
        "epoch": 0.6858285084091535,
        "step": 4975
    },
    {
        "loss": 2.2138,
        "grad_norm": 1.8285996913909912,
        "learning_rate": 0.00019855244568675164,
        "epoch": 0.6859663633857183,
        "step": 4976
    },
    {
        "loss": 2.2098,
        "grad_norm": 1.8013038635253906,
        "learning_rate": 0.00019853025910187473,
        "epoch": 0.6861042183622829,
        "step": 4977
    },
    {
        "loss": 1.1281,
        "grad_norm": 2.1387572288513184,
        "learning_rate": 0.00019850790503992232,
        "epoch": 0.6862420733388476,
        "step": 4978
    },
    {
        "loss": 1.1355,
        "grad_norm": 3.382009506225586,
        "learning_rate": 0.00019848538353889073,
        "epoch": 0.6863799283154122,
        "step": 4979
    },
    {
        "loss": 1.5226,
        "grad_norm": 2.580085515975952,
        "learning_rate": 0.000198462694637061,
        "epoch": 0.6865177832919769,
        "step": 4980
    },
    {
        "loss": 1.7904,
        "grad_norm": 2.5722246170043945,
        "learning_rate": 0.00019843983837299864,
        "epoch": 0.6866556382685415,
        "step": 4981
    },
    {
        "loss": 0.9619,
        "grad_norm": 3.1039774417877197,
        "learning_rate": 0.0001984168147855536,
        "epoch": 0.6867934932451062,
        "step": 4982
    },
    {
        "loss": 2.1789,
        "grad_norm": 2.249058723449707,
        "learning_rate": 0.00019839362391386035,
        "epoch": 0.6869313482216708,
        "step": 4983
    },
    {
        "loss": 2.2042,
        "grad_norm": 1.7108426094055176,
        "learning_rate": 0.0001983702657973376,
        "epoch": 0.6870692031982355,
        "step": 4984
    },
    {
        "loss": 1.8331,
        "grad_norm": 1.903977870941162,
        "learning_rate": 0.00019834674047568838,
        "epoch": 0.6872070581748001,
        "step": 4985
    },
    {
        "loss": 1.834,
        "grad_norm": 2.193225383758545,
        "learning_rate": 0.00019832304798889991,
        "epoch": 0.6873449131513648,
        "step": 4986
    },
    {
        "loss": 2.7431,
        "grad_norm": 1.6411389112472534,
        "learning_rate": 0.00019829918837724358,
        "epoch": 0.6874827681279294,
        "step": 4987
    },
    {
        "loss": 2.2544,
        "grad_norm": 2.4177746772766113,
        "learning_rate": 0.0001982751616812748,
        "epoch": 0.6876206231044941,
        "step": 4988
    },
    {
        "loss": 2.1812,
        "grad_norm": 1.4985014200210571,
        "learning_rate": 0.00019825096794183304,
        "epoch": 0.6877584780810587,
        "step": 4989
    },
    {
        "loss": 1.9161,
        "grad_norm": 1.9095337390899658,
        "learning_rate": 0.00019822660720004165,
        "epoch": 0.6878963330576234,
        "step": 4990
    },
    {
        "loss": 2.3777,
        "grad_norm": 1.2663217782974243,
        "learning_rate": 0.0001982020794973079,
        "epoch": 0.688034188034188,
        "step": 4991
    },
    {
        "loss": 1.9119,
        "grad_norm": 1.483559489250183,
        "learning_rate": 0.00019817738487532278,
        "epoch": 0.6881720430107527,
        "step": 4992
    },
    {
        "loss": 2.1984,
        "grad_norm": 1.5846388339996338,
        "learning_rate": 0.0001981525233760611,
        "epoch": 0.6883098979873173,
        "step": 4993
    },
    {
        "loss": 2.2324,
        "grad_norm": 2.2500247955322266,
        "learning_rate": 0.00019812749504178118,
        "epoch": 0.6884477529638819,
        "step": 4994
    },
    {
        "loss": 1.8591,
        "grad_norm": 2.9401888847351074,
        "learning_rate": 0.00019810229991502505,
        "epoch": 0.6885856079404467,
        "step": 4995
    },
    {
        "loss": 1.5846,
        "grad_norm": 2.153674840927124,
        "learning_rate": 0.00019807693803861818,
        "epoch": 0.6887234629170113,
        "step": 4996
    },
    {
        "loss": 1.9896,
        "grad_norm": 2.150005340576172,
        "learning_rate": 0.00019805140945566952,
        "epoch": 0.688861317893576,
        "step": 4997
    },
    {
        "loss": 1.8573,
        "grad_norm": 1.7645905017852783,
        "learning_rate": 0.0001980257142095713,
        "epoch": 0.6889991728701406,
        "step": 4998
    },
    {
        "loss": 2.479,
        "grad_norm": 1.6420648097991943,
        "learning_rate": 0.00019799985234399912,
        "epoch": 0.6891370278467053,
        "step": 4999
    },
    {
        "loss": 0.8753,
        "grad_norm": 2.5705223083496094,
        "learning_rate": 0.00019797382390291174,
        "epoch": 0.6892748828232699,
        "step": 5000
    },
    {
        "loss": 1.9719,
        "grad_norm": 2.0413095951080322,
        "learning_rate": 0.00019794762893055102,
        "epoch": 0.6894127377998346,
        "step": 5001
    },
    {
        "loss": 2.0231,
        "grad_norm": 2.6214799880981445,
        "learning_rate": 0.0001979212674714421,
        "epoch": 0.6895505927763992,
        "step": 5002
    },
    {
        "loss": 2.618,
        "grad_norm": 1.7834703922271729,
        "learning_rate": 0.00019789473957039272,
        "epoch": 0.6896884477529639,
        "step": 5003
    },
    {
        "loss": 2.0738,
        "grad_norm": 1.946670413017273,
        "learning_rate": 0.00019786804527249392,
        "epoch": 0.6898263027295285,
        "step": 5004
    },
    {
        "loss": 1.8421,
        "grad_norm": 3.579663038253784,
        "learning_rate": 0.00019784118462311937,
        "epoch": 0.6899641577060932,
        "step": 5005
    },
    {
        "loss": 2.0259,
        "grad_norm": 1.529373288154602,
        "learning_rate": 0.0001978141576679255,
        "epoch": 0.6901020126826578,
        "step": 5006
    },
    {
        "loss": 1.6002,
        "grad_norm": 2.0893847942352295,
        "learning_rate": 0.00019778696445285146,
        "epoch": 0.6902398676592225,
        "step": 5007
    },
    {
        "loss": 1.5282,
        "grad_norm": 1.881770133972168,
        "learning_rate": 0.000197759605024119,
        "epoch": 0.6903777226357871,
        "step": 5008
    },
    {
        "loss": 1.2269,
        "grad_norm": 2.8692262172698975,
        "learning_rate": 0.00019773207942823235,
        "epoch": 0.6905155776123518,
        "step": 5009
    },
    {
        "loss": 2.0169,
        "grad_norm": 2.686994791030884,
        "learning_rate": 0.0001977043877119783,
        "epoch": 0.6906534325889164,
        "step": 5010
    },
    {
        "loss": 2.2831,
        "grad_norm": 1.663606882095337,
        "learning_rate": 0.00019767652992242583,
        "epoch": 0.6907912875654811,
        "step": 5011
    },
    {
        "loss": 1.8477,
        "grad_norm": 2.0450146198272705,
        "learning_rate": 0.00019764850610692628,
        "epoch": 0.6909291425420457,
        "step": 5012
    },
    {
        "loss": 2.4659,
        "grad_norm": 1.3697655200958252,
        "learning_rate": 0.0001976203163131133,
        "epoch": 0.6910669975186104,
        "step": 5013
    },
    {
        "loss": 1.5441,
        "grad_norm": 2.695517063140869,
        "learning_rate": 0.00019759196058890253,
        "epoch": 0.691204852495175,
        "step": 5014
    },
    {
        "loss": 1.9315,
        "grad_norm": 2.34123158454895,
        "learning_rate": 0.00019756343898249163,
        "epoch": 0.6913427074717398,
        "step": 5015
    },
    {
        "loss": 2.2302,
        "grad_norm": 2.4560561180114746,
        "learning_rate": 0.0001975347515423604,
        "epoch": 0.6914805624483044,
        "step": 5016
    },
    {
        "loss": 1.7793,
        "grad_norm": 2.3971171379089355,
        "learning_rate": 0.00019750589831727026,
        "epoch": 0.6916184174248691,
        "step": 5017
    },
    {
        "loss": 2.2465,
        "grad_norm": 2.4048008918762207,
        "learning_rate": 0.00019747687935626468,
        "epoch": 0.6917562724014337,
        "step": 5018
    },
    {
        "loss": 1.7733,
        "grad_norm": 1.9688708782196045,
        "learning_rate": 0.00019744769470866867,
        "epoch": 0.6918941273779984,
        "step": 5019
    },
    {
        "loss": 1.4295,
        "grad_norm": 2.1585733890533447,
        "learning_rate": 0.00019741834442408882,
        "epoch": 0.692031982354563,
        "step": 5020
    },
    {
        "loss": 1.4106,
        "grad_norm": 2.0979390144348145,
        "learning_rate": 0.00019738882855241353,
        "epoch": 0.6921698373311277,
        "step": 5021
    },
    {
        "loss": 2.1317,
        "grad_norm": 2.3412368297576904,
        "learning_rate": 0.0001973591471438123,
        "epoch": 0.6923076923076923,
        "step": 5022
    },
    {
        "loss": 1.9046,
        "grad_norm": 1.9718141555786133,
        "learning_rate": 0.00019732930024873635,
        "epoch": 0.692445547284257,
        "step": 5023
    },
    {
        "loss": 2.2525,
        "grad_norm": 2.246213674545288,
        "learning_rate": 0.00019729928791791795,
        "epoch": 0.6925834022608216,
        "step": 5024
    },
    {
        "loss": 2.1785,
        "grad_norm": 1.7244139909744263,
        "learning_rate": 0.00019726911020237058,
        "epoch": 0.6927212572373863,
        "step": 5025
    },
    {
        "loss": 2.1616,
        "grad_norm": 2.0807042121887207,
        "learning_rate": 0.00019723876715338902,
        "epoch": 0.6928591122139509,
        "step": 5026
    },
    {
        "loss": 1.7766,
        "grad_norm": 2.160437822341919,
        "learning_rate": 0.0001972082588225488,
        "epoch": 0.6929969671905156,
        "step": 5027
    },
    {
        "loss": 1.9792,
        "grad_norm": 1.3134697675704956,
        "learning_rate": 0.00019717758526170668,
        "epoch": 0.6931348221670802,
        "step": 5028
    },
    {
        "loss": 2.3399,
        "grad_norm": 1.7292673587799072,
        "learning_rate": 0.00019714674652300006,
        "epoch": 0.6932726771436449,
        "step": 5029
    },
    {
        "loss": 1.9722,
        "grad_norm": 1.8650519847869873,
        "learning_rate": 0.00019711574265884717,
        "epoch": 0.6934105321202095,
        "step": 5030
    },
    {
        "loss": 1.9562,
        "grad_norm": 2.5728888511657715,
        "learning_rate": 0.00019708457372194694,
        "epoch": 0.6935483870967742,
        "step": 5031
    },
    {
        "loss": 2.3465,
        "grad_norm": 1.973766803741455,
        "learning_rate": 0.0001970532397652788,
        "epoch": 0.6936862420733388,
        "step": 5032
    },
    {
        "loss": 1.6156,
        "grad_norm": 2.3148534297943115,
        "learning_rate": 0.0001970217408421028,
        "epoch": 0.6938240970499036,
        "step": 5033
    },
    {
        "loss": 1.7231,
        "grad_norm": 2.434673309326172,
        "learning_rate": 0.00019699007700595925,
        "epoch": 0.6939619520264682,
        "step": 5034
    },
    {
        "loss": 2.287,
        "grad_norm": 2.2283785343170166,
        "learning_rate": 0.0001969582483106689,
        "epoch": 0.6940998070030329,
        "step": 5035
    },
    {
        "loss": 2.1459,
        "grad_norm": 1.345559000968933,
        "learning_rate": 0.00019692625481033266,
        "epoch": 0.6942376619795975,
        "step": 5036
    },
    {
        "loss": 1.4591,
        "grad_norm": 2.5426149368286133,
        "learning_rate": 0.00019689409655933155,
        "epoch": 0.6943755169561621,
        "step": 5037
    },
    {
        "loss": 1.4081,
        "grad_norm": 3.2488739490509033,
        "learning_rate": 0.0001968617736123266,
        "epoch": 0.6945133719327268,
        "step": 5038
    },
    {
        "loss": 2.063,
        "grad_norm": 1.1773574352264404,
        "learning_rate": 0.00019682928602425887,
        "epoch": 0.6946512269092914,
        "step": 5039
    },
    {
        "loss": 2.0784,
        "grad_norm": 1.971341848373413,
        "learning_rate": 0.00019679663385034925,
        "epoch": 0.6947890818858561,
        "step": 5040
    },
    {
        "loss": 2.0111,
        "grad_norm": 1.9229168891906738,
        "learning_rate": 0.0001967638171460983,
        "epoch": 0.6949269368624207,
        "step": 5041
    },
    {
        "loss": 1.9931,
        "grad_norm": 1.613852620124817,
        "learning_rate": 0.00019673083596728633,
        "epoch": 0.6950647918389854,
        "step": 5042
    },
    {
        "loss": 1.9869,
        "grad_norm": 1.3499764204025269,
        "learning_rate": 0.00019669769036997326,
        "epoch": 0.69520264681555,
        "step": 5043
    },
    {
        "loss": 2.8335,
        "grad_norm": 1.6963540315628052,
        "learning_rate": 0.0001966643804104983,
        "epoch": 0.6953405017921147,
        "step": 5044
    },
    {
        "loss": 1.5275,
        "grad_norm": 3.274221658706665,
        "learning_rate": 0.0001966309061454802,
        "epoch": 0.6954783567686793,
        "step": 5045
    },
    {
        "loss": 1.4157,
        "grad_norm": 2.667372703552246,
        "learning_rate": 0.00019659726763181695,
        "epoch": 0.695616211745244,
        "step": 5046
    },
    {
        "loss": 1.0935,
        "grad_norm": 2.303345203399658,
        "learning_rate": 0.0001965634649266856,
        "epoch": 0.6957540667218086,
        "step": 5047
    },
    {
        "loss": 2.7607,
        "grad_norm": 1.5327173471450806,
        "learning_rate": 0.00019652949808754254,
        "epoch": 0.6958919216983733,
        "step": 5048
    },
    {
        "loss": 2.265,
        "grad_norm": 1.3827203512191772,
        "learning_rate": 0.00019649536717212293,
        "epoch": 0.6960297766749379,
        "step": 5049
    },
    {
        "loss": 2.2321,
        "grad_norm": 1.3548835515975952,
        "learning_rate": 0.00019646107223844083,
        "epoch": 0.6961676316515026,
        "step": 5050
    },
    {
        "loss": 2.4138,
        "grad_norm": 2.7668590545654297,
        "learning_rate": 0.00019642661334478922,
        "epoch": 0.6963054866280672,
        "step": 5051
    },
    {
        "loss": 2.0587,
        "grad_norm": 1.7994576692581177,
        "learning_rate": 0.00019639199054973964,
        "epoch": 0.696443341604632,
        "step": 5052
    },
    {
        "loss": 2.5163,
        "grad_norm": 1.7692688703536987,
        "learning_rate": 0.00019635720391214233,
        "epoch": 0.6965811965811965,
        "step": 5053
    },
    {
        "loss": 1.7798,
        "grad_norm": 2.180823564529419,
        "learning_rate": 0.00019632225349112598,
        "epoch": 0.6967190515577613,
        "step": 5054
    },
    {
        "loss": 1.9788,
        "grad_norm": 2.9497392177581787,
        "learning_rate": 0.0001962871393460976,
        "epoch": 0.6968569065343259,
        "step": 5055
    },
    {
        "loss": 2.3519,
        "grad_norm": 1.4891940355300903,
        "learning_rate": 0.00019625186153674268,
        "epoch": 0.6969947615108906,
        "step": 5056
    },
    {
        "loss": 2.0515,
        "grad_norm": 2.0578317642211914,
        "learning_rate": 0.00019621642012302465,
        "epoch": 0.6971326164874552,
        "step": 5057
    },
    {
        "loss": 1.7712,
        "grad_norm": 2.1824631690979004,
        "learning_rate": 0.00019618081516518512,
        "epoch": 0.6972704714640199,
        "step": 5058
    },
    {
        "loss": 2.0624,
        "grad_norm": 1.6674147844314575,
        "learning_rate": 0.00019614504672374385,
        "epoch": 0.6974083264405845,
        "step": 5059
    },
    {
        "loss": 1.6664,
        "grad_norm": 2.6782631874084473,
        "learning_rate": 0.00019610911485949824,
        "epoch": 0.6975461814171492,
        "step": 5060
    },
    {
        "loss": 1.7894,
        "grad_norm": 1.5845848321914673,
        "learning_rate": 0.0001960730196335236,
        "epoch": 0.6976840363937138,
        "step": 5061
    },
    {
        "loss": 1.6623,
        "grad_norm": 2.2833995819091797,
        "learning_rate": 0.00019603676110717294,
        "epoch": 0.6978218913702785,
        "step": 5062
    },
    {
        "loss": 1.7019,
        "grad_norm": 2.468616008758545,
        "learning_rate": 0.00019600033934207668,
        "epoch": 0.6979597463468431,
        "step": 5063
    },
    {
        "loss": 2.5424,
        "grad_norm": 1.3455771207809448,
        "learning_rate": 0.00019596375440014292,
        "epoch": 0.6980976013234078,
        "step": 5064
    },
    {
        "loss": 2.2955,
        "grad_norm": 2.427802324295044,
        "learning_rate": 0.00019592700634355696,
        "epoch": 0.6982354562999724,
        "step": 5065
    },
    {
        "loss": 2.1391,
        "grad_norm": 2.0259101390838623,
        "learning_rate": 0.0001958900952347814,
        "epoch": 0.6983733112765371,
        "step": 5066
    },
    {
        "loss": 1.9282,
        "grad_norm": 2.6833789348602295,
        "learning_rate": 0.00019585302113655608,
        "epoch": 0.6985111662531017,
        "step": 5067
    },
    {
        "loss": 1.839,
        "grad_norm": 1.9434244632720947,
        "learning_rate": 0.00019581578411189768,
        "epoch": 0.6986490212296664,
        "step": 5068
    },
    {
        "loss": 1.8279,
        "grad_norm": 2.302079200744629,
        "learning_rate": 0.00019577838422410001,
        "epoch": 0.698786876206231,
        "step": 5069
    },
    {
        "loss": 2.3672,
        "grad_norm": 2.2463698387145996,
        "learning_rate": 0.00019574082153673365,
        "epoch": 0.6989247311827957,
        "step": 5070
    },
    {
        "loss": 2.3653,
        "grad_norm": 1.1086273193359375,
        "learning_rate": 0.0001957030961136458,
        "epoch": 0.6990625861593603,
        "step": 5071
    },
    {
        "loss": 0.9789,
        "grad_norm": 2.920408248901367,
        "learning_rate": 0.0001956652080189604,
        "epoch": 0.699200441135925,
        "step": 5072
    },
    {
        "loss": 2.0024,
        "grad_norm": 1.701332688331604,
        "learning_rate": 0.00019562715731707785,
        "epoch": 0.6993382961124897,
        "step": 5073
    },
    {
        "loss": 2.1734,
        "grad_norm": 1.6276988983154297,
        "learning_rate": 0.00019558894407267495,
        "epoch": 0.6994761510890544,
        "step": 5074
    },
    {
        "loss": 2.0982,
        "grad_norm": 2.528754472732544,
        "learning_rate": 0.0001955505683507047,
        "epoch": 0.699614006065619,
        "step": 5075
    },
    {
        "loss": 1.2682,
        "grad_norm": 2.1418120861053467,
        "learning_rate": 0.00019551203021639634,
        "epoch": 0.6997518610421837,
        "step": 5076
    },
    {
        "loss": 2.0152,
        "grad_norm": 2.164170026779175,
        "learning_rate": 0.0001954733297352552,
        "epoch": 0.6998897160187483,
        "step": 5077
    },
    {
        "loss": 2.1382,
        "grad_norm": 1.0903300046920776,
        "learning_rate": 0.0001954344669730626,
        "epoch": 0.7000275709953129,
        "step": 5078
    },
    {
        "loss": 1.5381,
        "grad_norm": 1.8170806169509888,
        "learning_rate": 0.00019539544199587548,
        "epoch": 0.7001654259718776,
        "step": 5079
    },
    {
        "loss": 2.4153,
        "grad_norm": 2.053029775619507,
        "learning_rate": 0.00019535625487002674,
        "epoch": 0.7003032809484422,
        "step": 5080
    },
    {
        "loss": 2.1921,
        "grad_norm": 1.7634843587875366,
        "learning_rate": 0.00019531690566212484,
        "epoch": 0.7004411359250069,
        "step": 5081
    },
    {
        "loss": 2.3892,
        "grad_norm": 1.5361298322677612,
        "learning_rate": 0.00019527739443905365,
        "epoch": 0.7005789909015715,
        "step": 5082
    },
    {
        "loss": 2.2321,
        "grad_norm": 1.65645170211792,
        "learning_rate": 0.00019523772126797242,
        "epoch": 0.7007168458781362,
        "step": 5083
    },
    {
        "loss": 1.2382,
        "grad_norm": 2.2616653442382812,
        "learning_rate": 0.00019519788621631586,
        "epoch": 0.7008547008547008,
        "step": 5084
    },
    {
        "loss": 1.8236,
        "grad_norm": 1.9031996726989746,
        "learning_rate": 0.00019515788935179357,
        "epoch": 0.7009925558312655,
        "step": 5085
    },
    {
        "loss": 1.7411,
        "grad_norm": 2.6979146003723145,
        "learning_rate": 0.00019511773074239045,
        "epoch": 0.7011304108078301,
        "step": 5086
    },
    {
        "loss": 2.1073,
        "grad_norm": 1.934177041053772,
        "learning_rate": 0.00019507741045636615,
        "epoch": 0.7012682657843948,
        "step": 5087
    },
    {
        "loss": 1.8232,
        "grad_norm": 2.8160650730133057,
        "learning_rate": 0.00019503692856225516,
        "epoch": 0.7014061207609594,
        "step": 5088
    },
    {
        "loss": 2.0875,
        "grad_norm": 1.8247026205062866,
        "learning_rate": 0.00019499628512886674,
        "epoch": 0.7015439757375241,
        "step": 5089
    },
    {
        "loss": 1.0124,
        "grad_norm": 2.329709768295288,
        "learning_rate": 0.00019495548022528463,
        "epoch": 0.7016818307140887,
        "step": 5090
    },
    {
        "loss": 2.1753,
        "grad_norm": 1.2031306028366089,
        "learning_rate": 0.0001949145139208671,
        "epoch": 0.7018196856906534,
        "step": 5091
    },
    {
        "loss": 2.3727,
        "grad_norm": 1.0883958339691162,
        "learning_rate": 0.00019487338628524677,
        "epoch": 0.701957540667218,
        "step": 5092
    },
    {
        "loss": 1.823,
        "grad_norm": 2.133866548538208,
        "learning_rate": 0.00019483209738833037,
        "epoch": 0.7020953956437828,
        "step": 5093
    },
    {
        "loss": 2.0404,
        "grad_norm": 1.87700617313385,
        "learning_rate": 0.00019479064730029892,
        "epoch": 0.7022332506203474,
        "step": 5094
    },
    {
        "loss": 2.2005,
        "grad_norm": 2.321122646331787,
        "learning_rate": 0.00019474903609160727,
        "epoch": 0.7023711055969121,
        "step": 5095
    },
    {
        "loss": 1.2454,
        "grad_norm": 1.824600100517273,
        "learning_rate": 0.0001947072638329841,
        "epoch": 0.7025089605734767,
        "step": 5096
    },
    {
        "loss": 1.8622,
        "grad_norm": 1.1708871126174927,
        "learning_rate": 0.00019466533059543206,
        "epoch": 0.7026468155500414,
        "step": 5097
    },
    {
        "loss": 2.6527,
        "grad_norm": 1.698655366897583,
        "learning_rate": 0.0001946232364502272,
        "epoch": 0.702784670526606,
        "step": 5098
    },
    {
        "loss": 1.7304,
        "grad_norm": 2.030729055404663,
        "learning_rate": 0.00019458098146891918,
        "epoch": 0.7029225255031707,
        "step": 5099
    },
    {
        "loss": 1.9765,
        "grad_norm": 1.75700843334198,
        "learning_rate": 0.00019453856572333104,
        "epoch": 0.7030603804797353,
        "step": 5100
    },
    {
        "loss": 1.7459,
        "grad_norm": 2.5860679149627686,
        "learning_rate": 0.000194495989285559,
        "epoch": 0.7031982354563,
        "step": 5101
    },
    {
        "loss": 1.7553,
        "grad_norm": 2.397744655609131,
        "learning_rate": 0.00019445325222797253,
        "epoch": 0.7033360904328646,
        "step": 5102
    },
    {
        "loss": 2.098,
        "grad_norm": 2.300532579421997,
        "learning_rate": 0.00019441035462321407,
        "epoch": 0.7034739454094293,
        "step": 5103
    },
    {
        "loss": 2.1192,
        "grad_norm": 2.5920028686523438,
        "learning_rate": 0.00019436729654419884,
        "epoch": 0.7036118003859939,
        "step": 5104
    },
    {
        "loss": 2.6649,
        "grad_norm": 1.1843864917755127,
        "learning_rate": 0.00019432407806411507,
        "epoch": 0.7037496553625586,
        "step": 5105
    },
    {
        "loss": 1.8211,
        "grad_norm": 1.8178281784057617,
        "learning_rate": 0.00019428069925642337,
        "epoch": 0.7038875103391232,
        "step": 5106
    },
    {
        "loss": 2.2422,
        "grad_norm": 2.078806161880493,
        "learning_rate": 0.0001942371601948571,
        "epoch": 0.7040253653156879,
        "step": 5107
    },
    {
        "loss": 2.3611,
        "grad_norm": 1.2751699686050415,
        "learning_rate": 0.0001941934609534218,
        "epoch": 0.7041632202922525,
        "step": 5108
    },
    {
        "loss": 0.8655,
        "grad_norm": 2.412365674972534,
        "learning_rate": 0.00019414960160639538,
        "epoch": 0.7043010752688172,
        "step": 5109
    },
    {
        "loss": 1.1709,
        "grad_norm": 2.113990306854248,
        "learning_rate": 0.00019410558222832794,
        "epoch": 0.7044389302453818,
        "step": 5110
    },
    {
        "loss": 1.8632,
        "grad_norm": 2.233393430709839,
        "learning_rate": 0.00019406140289404152,
        "epoch": 0.7045767852219466,
        "step": 5111
    },
    {
        "loss": 2.0125,
        "grad_norm": 2.089203119277954,
        "learning_rate": 0.00019401706367863005,
        "epoch": 0.7047146401985112,
        "step": 5112
    },
    {
        "loss": 2.2697,
        "grad_norm": 1.9672737121582031,
        "learning_rate": 0.00019397256465745924,
        "epoch": 0.7048524951750759,
        "step": 5113
    },
    {
        "loss": 1.3493,
        "grad_norm": 2.3776919841766357,
        "learning_rate": 0.00019392790590616637,
        "epoch": 0.7049903501516405,
        "step": 5114
    },
    {
        "loss": 1.6521,
        "grad_norm": 1.7678556442260742,
        "learning_rate": 0.0001938830875006603,
        "epoch": 0.7051282051282052,
        "step": 5115
    },
    {
        "loss": 2.3238,
        "grad_norm": 1.3677232265472412,
        "learning_rate": 0.00019383810951712133,
        "epoch": 0.7052660601047698,
        "step": 5116
    },
    {
        "loss": 1.8362,
        "grad_norm": 2.4759390354156494,
        "learning_rate": 0.0001937929720320008,
        "epoch": 0.7054039150813345,
        "step": 5117
    },
    {
        "loss": 2.476,
        "grad_norm": 1.267724633216858,
        "learning_rate": 0.0001937476751220213,
        "epoch": 0.7055417700578991,
        "step": 5118
    },
    {
        "loss": 1.1255,
        "grad_norm": 3.076934814453125,
        "learning_rate": 0.0001937022188641764,
        "epoch": 0.7056796250344638,
        "step": 5119
    },
    {
        "loss": 1.8427,
        "grad_norm": 2.757423162460327,
        "learning_rate": 0.00019365660333573053,
        "epoch": 0.7058174800110284,
        "step": 5120
    },
    {
        "loss": 2.2775,
        "grad_norm": 1.61699378490448,
        "learning_rate": 0.00019361082861421867,
        "epoch": 0.705955334987593,
        "step": 5121
    },
    {
        "loss": 2.0421,
        "grad_norm": 1.9069204330444336,
        "learning_rate": 0.0001935648947774467,
        "epoch": 0.7060931899641577,
        "step": 5122
    },
    {
        "loss": 1.894,
        "grad_norm": 1.4961835145950317,
        "learning_rate": 0.00019351880190349063,
        "epoch": 0.7062310449407223,
        "step": 5123
    },
    {
        "loss": 2.0907,
        "grad_norm": 1.474769115447998,
        "learning_rate": 0.0001934725500706971,
        "epoch": 0.706368899917287,
        "step": 5124
    },
    {
        "loss": 2.1207,
        "grad_norm": 1.3830910921096802,
        "learning_rate": 0.0001934261393576827,
        "epoch": 0.7065067548938516,
        "step": 5125
    },
    {
        "loss": 2.0339,
        "grad_norm": 1.8329793214797974,
        "learning_rate": 0.0001933795698433341,
        "epoch": 0.7066446098704163,
        "step": 5126
    },
    {
        "loss": 2.3651,
        "grad_norm": 1.8267673254013062,
        "learning_rate": 0.0001933328416068081,
        "epoch": 0.7067824648469809,
        "step": 5127
    },
    {
        "loss": 1.5692,
        "grad_norm": 2.2327566146850586,
        "learning_rate": 0.00019328595472753102,
        "epoch": 0.7069203198235456,
        "step": 5128
    },
    {
        "loss": 1.1843,
        "grad_norm": 2.877513885498047,
        "learning_rate": 0.00019323890928519904,
        "epoch": 0.7070581748001102,
        "step": 5129
    },
    {
        "loss": 2.0571,
        "grad_norm": 2.4361140727996826,
        "learning_rate": 0.00019319170535977778,
        "epoch": 0.707196029776675,
        "step": 5130
    },
    {
        "loss": 2.4949,
        "grad_norm": 1.48103928565979,
        "learning_rate": 0.00019314434303150218,
        "epoch": 0.7073338847532396,
        "step": 5131
    },
    {
        "loss": 2.056,
        "grad_norm": 1.6042892932891846,
        "learning_rate": 0.00019309682238087658,
        "epoch": 0.7074717397298043,
        "step": 5132
    },
    {
        "loss": 0.7253,
        "grad_norm": 3.2419116497039795,
        "learning_rate": 0.00019304914348867427,
        "epoch": 0.7076095947063689,
        "step": 5133
    },
    {
        "loss": 2.2766,
        "grad_norm": 1.1818431615829468,
        "learning_rate": 0.00019300130643593754,
        "epoch": 0.7077474496829336,
        "step": 5134
    },
    {
        "loss": 2.2399,
        "grad_norm": 1.5722540616989136,
        "learning_rate": 0.00019295331130397765,
        "epoch": 0.7078853046594982,
        "step": 5135
    },
    {
        "loss": 1.8829,
        "grad_norm": 1.4897228479385376,
        "learning_rate": 0.00019290515817437437,
        "epoch": 0.7080231596360629,
        "step": 5136
    },
    {
        "loss": 2.1183,
        "grad_norm": 2.0123348236083984,
        "learning_rate": 0.0001928568471289762,
        "epoch": 0.7081610146126275,
        "step": 5137
    },
    {
        "loss": 1.3048,
        "grad_norm": 2.383939027786255,
        "learning_rate": 0.00019280837824989987,
        "epoch": 0.7082988695891922,
        "step": 5138
    },
    {
        "loss": 1.6483,
        "grad_norm": 1.9913965463638306,
        "learning_rate": 0.00019275975161953048,
        "epoch": 0.7084367245657568,
        "step": 5139
    },
    {
        "loss": 1.9385,
        "grad_norm": 1.7640453577041626,
        "learning_rate": 0.00019271096732052136,
        "epoch": 0.7085745795423215,
        "step": 5140
    },
    {
        "loss": 2.1988,
        "grad_norm": 1.5157873630523682,
        "learning_rate": 0.0001926620254357938,
        "epoch": 0.7087124345188861,
        "step": 5141
    },
    {
        "loss": 1.6654,
        "grad_norm": 2.711554527282715,
        "learning_rate": 0.00019261292604853675,
        "epoch": 0.7088502894954508,
        "step": 5142
    },
    {
        "loss": 2.1024,
        "grad_norm": 1.906184196472168,
        "learning_rate": 0.00019256366924220718,
        "epoch": 0.7089881444720154,
        "step": 5143
    },
    {
        "loss": 1.9313,
        "grad_norm": 2.130840301513672,
        "learning_rate": 0.00019251425510052932,
        "epoch": 0.7091259994485801,
        "step": 5144
    },
    {
        "loss": 2.6481,
        "grad_norm": 2.7249324321746826,
        "learning_rate": 0.00019246468370749513,
        "epoch": 0.7092638544251447,
        "step": 5145
    },
    {
        "loss": 1.6363,
        "grad_norm": 2.2625010013580322,
        "learning_rate": 0.00019241495514736367,
        "epoch": 0.7094017094017094,
        "step": 5146
    },
    {
        "loss": 2.2855,
        "grad_norm": 1.7093833684921265,
        "learning_rate": 0.00019236506950466115,
        "epoch": 0.709539564378274,
        "step": 5147
    },
    {
        "loss": 2.0806,
        "grad_norm": 1.4489011764526367,
        "learning_rate": 0.00019231502686418086,
        "epoch": 0.7096774193548387,
        "step": 5148
    },
    {
        "loss": 1.7025,
        "grad_norm": 2.435500383377075,
        "learning_rate": 0.000192264827310983,
        "epoch": 0.7098152743314033,
        "step": 5149
    },
    {
        "loss": 1.8367,
        "grad_norm": 1.905218243598938,
        "learning_rate": 0.00019221447093039428,
        "epoch": 0.7099531293079681,
        "step": 5150
    },
    {
        "loss": 1.7107,
        "grad_norm": 2.4007723331451416,
        "learning_rate": 0.00019216395780800814,
        "epoch": 0.7100909842845327,
        "step": 5151
    },
    {
        "loss": 2.1999,
        "grad_norm": 1.627180576324463,
        "learning_rate": 0.00019211328802968435,
        "epoch": 0.7102288392610974,
        "step": 5152
    },
    {
        "loss": 2.6404,
        "grad_norm": 2.2968132495880127,
        "learning_rate": 0.00019206246168154906,
        "epoch": 0.710366694237662,
        "step": 5153
    },
    {
        "loss": 2.2702,
        "grad_norm": 1.17201828956604,
        "learning_rate": 0.00019201147884999453,
        "epoch": 0.7105045492142267,
        "step": 5154
    },
    {
        "loss": 2.12,
        "grad_norm": 1.2669427394866943,
        "learning_rate": 0.00019196033962167884,
        "epoch": 0.7106424041907913,
        "step": 5155
    },
    {
        "loss": 2.1884,
        "grad_norm": 1.387109398841858,
        "learning_rate": 0.00019190904408352612,
        "epoch": 0.710780259167356,
        "step": 5156
    },
    {
        "loss": 1.3079,
        "grad_norm": 1.9301493167877197,
        "learning_rate": 0.00019185759232272614,
        "epoch": 0.7109181141439206,
        "step": 5157
    },
    {
        "loss": 2.1337,
        "grad_norm": 1.2437658309936523,
        "learning_rate": 0.0001918059844267341,
        "epoch": 0.7110559691204853,
        "step": 5158
    },
    {
        "loss": 1.1527,
        "grad_norm": 2.6427958011627197,
        "learning_rate": 0.00019175422048327065,
        "epoch": 0.7111938240970499,
        "step": 5159
    },
    {
        "loss": 2.3802,
        "grad_norm": 1.6049795150756836,
        "learning_rate": 0.00019170230058032176,
        "epoch": 0.7113316790736146,
        "step": 5160
    },
    {
        "loss": 1.9811,
        "grad_norm": 1.245545506477356,
        "learning_rate": 0.00019165022480613835,
        "epoch": 0.7114695340501792,
        "step": 5161
    },
    {
        "loss": 2.0872,
        "grad_norm": 1.423828363418579,
        "learning_rate": 0.00019159799324923641,
        "epoch": 0.7116073890267439,
        "step": 5162
    },
    {
        "loss": 2.1765,
        "grad_norm": 1.7219409942626953,
        "learning_rate": 0.0001915456059983967,
        "epoch": 0.7117452440033085,
        "step": 5163
    },
    {
        "loss": 2.0726,
        "grad_norm": 2.048285961151123,
        "learning_rate": 0.00019149306314266448,
        "epoch": 0.7118830989798731,
        "step": 5164
    },
    {
        "loss": 2.2573,
        "grad_norm": 1.8937534093856812,
        "learning_rate": 0.00019144036477134973,
        "epoch": 0.7120209539564378,
        "step": 5165
    },
    {
        "loss": 2.2627,
        "grad_norm": 1.2580252885818481,
        "learning_rate": 0.00019138751097402655,
        "epoch": 0.7121588089330024,
        "step": 5166
    },
    {
        "loss": 2.4401,
        "grad_norm": 1.7085591554641724,
        "learning_rate": 0.00019133450184053336,
        "epoch": 0.7122966639095671,
        "step": 5167
    },
    {
        "loss": 1.8431,
        "grad_norm": 2.381579637527466,
        "learning_rate": 0.00019128133746097267,
        "epoch": 0.7124345188861317,
        "step": 5168
    },
    {
        "loss": 1.9316,
        "grad_norm": 2.0630910396575928,
        "learning_rate": 0.00019122801792571066,
        "epoch": 0.7125723738626965,
        "step": 5169
    },
    {
        "loss": 1.7736,
        "grad_norm": 2.354273557662964,
        "learning_rate": 0.0001911745433253774,
        "epoch": 0.712710228839261,
        "step": 5170
    },
    {
        "loss": 1.9385,
        "grad_norm": 1.4119617938995361,
        "learning_rate": 0.00019112091375086653,
        "epoch": 0.7128480838158258,
        "step": 5171
    },
    {
        "loss": 1.7612,
        "grad_norm": 1.846236228942871,
        "learning_rate": 0.00019106712929333497,
        "epoch": 0.7129859387923904,
        "step": 5172
    },
    {
        "loss": 1.8453,
        "grad_norm": 2.6742050647735596,
        "learning_rate": 0.0001910131900442031,
        "epoch": 0.7131237937689551,
        "step": 5173
    },
    {
        "loss": 1.8785,
        "grad_norm": 2.2817797660827637,
        "learning_rate": 0.00019095909609515417,
        "epoch": 0.7132616487455197,
        "step": 5174
    },
    {
        "loss": 1.0677,
        "grad_norm": 2.6206419467926025,
        "learning_rate": 0.00019090484753813466,
        "epoch": 0.7133995037220844,
        "step": 5175
    },
    {
        "loss": 2.5423,
        "grad_norm": 1.5485209226608276,
        "learning_rate": 0.00019085044446535369,
        "epoch": 0.713537358698649,
        "step": 5176
    },
    {
        "loss": 2.086,
        "grad_norm": 1.395622730255127,
        "learning_rate": 0.00019079588696928288,
        "epoch": 0.7136752136752137,
        "step": 5177
    },
    {
        "loss": 2.1961,
        "grad_norm": 1.5959221124649048,
        "learning_rate": 0.00019074117514265659,
        "epoch": 0.7138130686517783,
        "step": 5178
    },
    {
        "loss": 1.936,
        "grad_norm": 1.785502314567566,
        "learning_rate": 0.00019068630907847145,
        "epoch": 0.713950923628343,
        "step": 5179
    },
    {
        "loss": 1.8761,
        "grad_norm": 1.628820538520813,
        "learning_rate": 0.00019063128886998612,
        "epoch": 0.7140887786049076,
        "step": 5180
    },
    {
        "loss": 1.9933,
        "grad_norm": 1.911376953125,
        "learning_rate": 0.00019057611461072138,
        "epoch": 0.7142266335814723,
        "step": 5181
    },
    {
        "loss": 1.6321,
        "grad_norm": 2.2106339931488037,
        "learning_rate": 0.00019052078639445976,
        "epoch": 0.7143644885580369,
        "step": 5182
    },
    {
        "loss": 2.2461,
        "grad_norm": 1.328933835029602,
        "learning_rate": 0.0001904653043152457,
        "epoch": 0.7145023435346016,
        "step": 5183
    },
    {
        "loss": 1.6218,
        "grad_norm": 1.8954411745071411,
        "learning_rate": 0.0001904096684673849,
        "epoch": 0.7146401985111662,
        "step": 5184
    },
    {
        "loss": 1.7976,
        "grad_norm": 1.8214174509048462,
        "learning_rate": 0.00019035387894544452,
        "epoch": 0.7147780534877309,
        "step": 5185
    },
    {
        "loss": 1.6378,
        "grad_norm": 2.5540409088134766,
        "learning_rate": 0.00019029793584425297,
        "epoch": 0.7149159084642955,
        "step": 5186
    },
    {
        "loss": 1.1964,
        "grad_norm": 2.474997043609619,
        "learning_rate": 0.00019024183925889977,
        "epoch": 0.7150537634408602,
        "step": 5187
    },
    {
        "loss": 2.0852,
        "grad_norm": 2.4655508995056152,
        "learning_rate": 0.0001901855892847352,
        "epoch": 0.7151916184174248,
        "step": 5188
    },
    {
        "loss": 2.1425,
        "grad_norm": 2.0888233184814453,
        "learning_rate": 0.0001901291860173703,
        "epoch": 0.7153294733939896,
        "step": 5189
    },
    {
        "loss": 0.85,
        "grad_norm": 4.011177062988281,
        "learning_rate": 0.0001900726295526766,
        "epoch": 0.7154673283705542,
        "step": 5190
    },
    {
        "loss": 1.6586,
        "grad_norm": 2.835242748260498,
        "learning_rate": 0.00019001591998678616,
        "epoch": 0.7156051833471189,
        "step": 5191
    },
    {
        "loss": 2.1205,
        "grad_norm": 3.0209054946899414,
        "learning_rate": 0.0001899590574160913,
        "epoch": 0.7157430383236835,
        "step": 5192
    },
    {
        "loss": 1.0696,
        "grad_norm": 2.128300428390503,
        "learning_rate": 0.0001899020419372442,
        "epoch": 0.7158808933002482,
        "step": 5193
    },
    {
        "loss": 1.8229,
        "grad_norm": 2.181474208831787,
        "learning_rate": 0.00018984487364715716,
        "epoch": 0.7160187482768128,
        "step": 5194
    },
    {
        "loss": 1.8321,
        "grad_norm": 2.9263086318969727,
        "learning_rate": 0.00018978755264300215,
        "epoch": 0.7161566032533775,
        "step": 5195
    },
    {
        "loss": 1.6586,
        "grad_norm": 1.5407739877700806,
        "learning_rate": 0.00018973007902221064,
        "epoch": 0.7162944582299421,
        "step": 5196
    },
    {
        "loss": 1.5342,
        "grad_norm": 1.4946482181549072,
        "learning_rate": 0.00018967245288247357,
        "epoch": 0.7164323132065068,
        "step": 5197
    },
    {
        "loss": 1.4856,
        "grad_norm": 2.6030213832855225,
        "learning_rate": 0.0001896146743217412,
        "epoch": 0.7165701681830714,
        "step": 5198
    },
    {
        "loss": 2.0486,
        "grad_norm": 2.61202073097229,
        "learning_rate": 0.00018955674343822273,
        "epoch": 0.7167080231596361,
        "step": 5199
    },
    {
        "loss": 1.886,
        "grad_norm": 1.9144577980041504,
        "learning_rate": 0.00018949866033038637,
        "epoch": 0.7168458781362007,
        "step": 5200
    },
    {
        "loss": 1.8086,
        "grad_norm": 1.9702421426773071,
        "learning_rate": 0.00018944042509695905,
        "epoch": 0.7169837331127654,
        "step": 5201
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.3102647066116333,
        "learning_rate": 0.00018938203783692618,
        "epoch": 0.71712158808933,
        "step": 5202
    },
    {
        "loss": 1.9126,
        "grad_norm": 2.398360013961792,
        "learning_rate": 0.00018932349864953177,
        "epoch": 0.7172594430658947,
        "step": 5203
    },
    {
        "loss": 1.8418,
        "grad_norm": 1.2493243217468262,
        "learning_rate": 0.0001892648076342779,
        "epoch": 0.7173972980424593,
        "step": 5204
    },
    {
        "loss": 1.9609,
        "grad_norm": 1.2923693656921387,
        "learning_rate": 0.00018920596489092477,
        "epoch": 0.717535153019024,
        "step": 5205
    },
    {
        "loss": 1.5538,
        "grad_norm": 2.1089954376220703,
        "learning_rate": 0.0001891469705194906,
        "epoch": 0.7176730079955886,
        "step": 5206
    },
    {
        "loss": 1.5395,
        "grad_norm": 2.345244884490967,
        "learning_rate": 0.0001890878246202511,
        "epoch": 0.7178108629721532,
        "step": 5207
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.1580514907836914,
        "learning_rate": 0.00018902852729373978,
        "epoch": 0.717948717948718,
        "step": 5208
    },
    {
        "loss": 2.1037,
        "grad_norm": 2.7996697425842285,
        "learning_rate": 0.0001889690786407474,
        "epoch": 0.7180865729252826,
        "step": 5209
    },
    {
        "loss": 0.9977,
        "grad_norm": 3.112461805343628,
        "learning_rate": 0.00018890947876232188,
        "epoch": 0.7182244279018473,
        "step": 5210
    },
    {
        "loss": 1.6779,
        "grad_norm": 2.098750352859497,
        "learning_rate": 0.0001888497277597684,
        "epoch": 0.7183622828784119,
        "step": 5211
    },
    {
        "loss": 1.4963,
        "grad_norm": 1.8050298690795898,
        "learning_rate": 0.00018878982573464881,
        "epoch": 0.7185001378549766,
        "step": 5212
    },
    {
        "loss": 1.7615,
        "grad_norm": 1.6263049840927124,
        "learning_rate": 0.0001887297727887818,
        "epoch": 0.7186379928315412,
        "step": 5213
    },
    {
        "loss": 1.5632,
        "grad_norm": 2.761681079864502,
        "learning_rate": 0.0001886695690242425,
        "epoch": 0.7187758478081059,
        "step": 5214
    },
    {
        "loss": 1.6829,
        "grad_norm": 2.0763845443725586,
        "learning_rate": 0.0001886092145433624,
        "epoch": 0.7189137027846705,
        "step": 5215
    },
    {
        "loss": 1.8626,
        "grad_norm": 2.2637858390808105,
        "learning_rate": 0.00018854870944872915,
        "epoch": 0.7190515577612352,
        "step": 5216
    },
    {
        "loss": 2.3765,
        "grad_norm": 2.3410210609436035,
        "learning_rate": 0.00018848805384318658,
        "epoch": 0.7191894127377998,
        "step": 5217
    },
    {
        "loss": 1.8712,
        "grad_norm": 2.098632335662842,
        "learning_rate": 0.00018842724782983412,
        "epoch": 0.7193272677143645,
        "step": 5218
    },
    {
        "loss": 2.4952,
        "grad_norm": 2.0475001335144043,
        "learning_rate": 0.000188366291512027,
        "epoch": 0.7194651226909291,
        "step": 5219
    },
    {
        "loss": 1.6638,
        "grad_norm": 2.3670566082000732,
        "learning_rate": 0.00018830518499337586,
        "epoch": 0.7196029776674938,
        "step": 5220
    },
    {
        "loss": 2.3264,
        "grad_norm": 2.5912137031555176,
        "learning_rate": 0.00018824392837774671,
        "epoch": 0.7197408326440584,
        "step": 5221
    },
    {
        "loss": 2.3113,
        "grad_norm": 1.9002031087875366,
        "learning_rate": 0.00018818252176926065,
        "epoch": 0.7198786876206231,
        "step": 5222
    },
    {
        "loss": 2.0961,
        "grad_norm": 1.7365928888320923,
        "learning_rate": 0.0001881209652722936,
        "epoch": 0.7200165425971877,
        "step": 5223
    },
    {
        "loss": 1.9576,
        "grad_norm": 2.2471840381622314,
        "learning_rate": 0.0001880592589914765,
        "epoch": 0.7201543975737524,
        "step": 5224
    },
    {
        "loss": 1.3921,
        "grad_norm": 2.7672431468963623,
        "learning_rate": 0.00018799740303169485,
        "epoch": 0.720292252550317,
        "step": 5225
    },
    {
        "loss": 1.9878,
        "grad_norm": 1.8714131116867065,
        "learning_rate": 0.00018793539749808837,
        "epoch": 0.7204301075268817,
        "step": 5226
    },
    {
        "loss": 0.9224,
        "grad_norm": 2.0710434913635254,
        "learning_rate": 0.00018787324249605107,
        "epoch": 0.7205679625034463,
        "step": 5227
    },
    {
        "loss": 1.5595,
        "grad_norm": 2.3505680561065674,
        "learning_rate": 0.00018781093813123126,
        "epoch": 0.7207058174800111,
        "step": 5228
    },
    {
        "loss": 1.4574,
        "grad_norm": 2.7073781490325928,
        "learning_rate": 0.00018774848450953078,
        "epoch": 0.7208436724565757,
        "step": 5229
    },
    {
        "loss": 1.4968,
        "grad_norm": 2.2011935710906982,
        "learning_rate": 0.0001876858817371055,
        "epoch": 0.7209815274331404,
        "step": 5230
    },
    {
        "loss": 2.6973,
        "grad_norm": 1.6858679056167603,
        "learning_rate": 0.0001876231299203645,
        "epoch": 0.721119382409705,
        "step": 5231
    },
    {
        "loss": 2.4762,
        "grad_norm": 1.1666227579116821,
        "learning_rate": 0.0001875602291659704,
        "epoch": 0.7212572373862697,
        "step": 5232
    },
    {
        "loss": 2.3686,
        "grad_norm": 1.6855850219726562,
        "learning_rate": 0.00018749717958083903,
        "epoch": 0.7213950923628343,
        "step": 5233
    },
    {
        "loss": 1.7879,
        "grad_norm": 1.413230061531067,
        "learning_rate": 0.00018743398127213904,
        "epoch": 0.721532947339399,
        "step": 5234
    },
    {
        "loss": 1.9617,
        "grad_norm": 2.565920829772949,
        "learning_rate": 0.00018737063434729186,
        "epoch": 0.7216708023159636,
        "step": 5235
    },
    {
        "loss": 1.8272,
        "grad_norm": 1.8377526998519897,
        "learning_rate": 0.00018730713891397164,
        "epoch": 0.7218086572925283,
        "step": 5236
    },
    {
        "loss": 1.8202,
        "grad_norm": 1.8822520971298218,
        "learning_rate": 0.00018724349508010492,
        "epoch": 0.7219465122690929,
        "step": 5237
    },
    {
        "loss": 1.3286,
        "grad_norm": 1.580586552619934,
        "learning_rate": 0.0001871797029538705,
        "epoch": 0.7220843672456576,
        "step": 5238
    },
    {
        "loss": 1.9755,
        "grad_norm": 2.1910696029663086,
        "learning_rate": 0.0001871157626436992,
        "epoch": 0.7222222222222222,
        "step": 5239
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.3453153371810913,
        "learning_rate": 0.0001870516742582737,
        "epoch": 0.7223600771987869,
        "step": 5240
    },
    {
        "loss": 1.8374,
        "grad_norm": 2.957561731338501,
        "learning_rate": 0.00018698743790652848,
        "epoch": 0.7224979321753515,
        "step": 5241
    },
    {
        "loss": 2.225,
        "grad_norm": 1.9696300029754639,
        "learning_rate": 0.00018692305369764935,
        "epoch": 0.7226357871519162,
        "step": 5242
    },
    {
        "loss": 1.363,
        "grad_norm": 2.8693253993988037,
        "learning_rate": 0.0001868585217410736,
        "epoch": 0.7227736421284808,
        "step": 5243
    },
    {
        "loss": 2.129,
        "grad_norm": 2.0837690830230713,
        "learning_rate": 0.00018679384214648968,
        "epoch": 0.7229114971050455,
        "step": 5244
    },
    {
        "loss": 1.2462,
        "grad_norm": 2.502774715423584,
        "learning_rate": 0.00018672901502383678,
        "epoch": 0.7230493520816101,
        "step": 5245
    },
    {
        "loss": 1.5295,
        "grad_norm": 2.5410208702087402,
        "learning_rate": 0.0001866640404833051,
        "epoch": 0.7231872070581749,
        "step": 5246
    },
    {
        "loss": 1.5099,
        "grad_norm": 4.21516752243042,
        "learning_rate": 0.00018659891863533518,
        "epoch": 0.7233250620347395,
        "step": 5247
    },
    {
        "loss": 2.2316,
        "grad_norm": 1.456662893295288,
        "learning_rate": 0.00018653364959061807,
        "epoch": 0.7234629170113042,
        "step": 5248
    },
    {
        "loss": 1.8736,
        "grad_norm": 2.720806360244751,
        "learning_rate": 0.0001864682334600951,
        "epoch": 0.7236007719878688,
        "step": 5249
    },
    {
        "loss": 1.0039,
        "grad_norm": 3.573317050933838,
        "learning_rate": 0.00018640267035495737,
        "epoch": 0.7237386269644334,
        "step": 5250
    },
    {
        "loss": 1.6822,
        "grad_norm": 2.5541369915008545,
        "learning_rate": 0.0001863369603866461,
        "epoch": 0.7238764819409981,
        "step": 5251
    },
    {
        "loss": 2.1701,
        "grad_norm": 1.6527091264724731,
        "learning_rate": 0.0001862711036668519,
        "epoch": 0.7240143369175627,
        "step": 5252
    },
    {
        "loss": 1.7607,
        "grad_norm": 2.3141982555389404,
        "learning_rate": 0.00018620510030751487,
        "epoch": 0.7241521918941274,
        "step": 5253
    },
    {
        "loss": 1.2204,
        "grad_norm": 2.957904815673828,
        "learning_rate": 0.00018613895042082445,
        "epoch": 0.724290046870692,
        "step": 5254
    },
    {
        "loss": 2.0494,
        "grad_norm": 1.091637134552002,
        "learning_rate": 0.0001860726541192192,
        "epoch": 0.7244279018472567,
        "step": 5255
    },
    {
        "loss": 1.7352,
        "grad_norm": 1.6502888202667236,
        "learning_rate": 0.00018600621151538621,
        "epoch": 0.7245657568238213,
        "step": 5256
    },
    {
        "loss": 1.5609,
        "grad_norm": 2.0524168014526367,
        "learning_rate": 0.00018593962272226168,
        "epoch": 0.724703611800386,
        "step": 5257
    },
    {
        "loss": 1.5566,
        "grad_norm": 2.275120258331299,
        "learning_rate": 0.00018587288785302993,
        "epoch": 0.7248414667769506,
        "step": 5258
    },
    {
        "loss": 1.6787,
        "grad_norm": 2.4509871006011963,
        "learning_rate": 0.00018580600702112392,
        "epoch": 0.7249793217535153,
        "step": 5259
    },
    {
        "loss": 1.7972,
        "grad_norm": 1.804679274559021,
        "learning_rate": 0.00018573898034022438,
        "epoch": 0.7251171767300799,
        "step": 5260
    },
    {
        "loss": 2.1346,
        "grad_norm": 2.116637706756592,
        "learning_rate": 0.0001856718079242601,
        "epoch": 0.7252550317066446,
        "step": 5261
    },
    {
        "loss": 1.5384,
        "grad_norm": 2.1445472240448,
        "learning_rate": 0.00018560448988740757,
        "epoch": 0.7253928866832092,
        "step": 5262
    },
    {
        "loss": 1.2046,
        "grad_norm": 1.9740060567855835,
        "learning_rate": 0.00018553702634409094,
        "epoch": 0.7255307416597739,
        "step": 5263
    },
    {
        "loss": 1.9063,
        "grad_norm": 2.581174612045288,
        "learning_rate": 0.00018546941740898144,
        "epoch": 0.7256685966363385,
        "step": 5264
    },
    {
        "loss": 2.0648,
        "grad_norm": 2.0062904357910156,
        "learning_rate": 0.0001854016631969975,
        "epoch": 0.7258064516129032,
        "step": 5265
    },
    {
        "loss": 1.7475,
        "grad_norm": 2.521212339401245,
        "learning_rate": 0.00018533376382330466,
        "epoch": 0.7259443065894678,
        "step": 5266
    },
    {
        "loss": 1.6968,
        "grad_norm": 1.6903094053268433,
        "learning_rate": 0.0001852657194033149,
        "epoch": 0.7260821615660326,
        "step": 5267
    },
    {
        "loss": 1.7816,
        "grad_norm": 2.7099037170410156,
        "learning_rate": 0.00018519753005268708,
        "epoch": 0.7262200165425972,
        "step": 5268
    },
    {
        "loss": 1.9796,
        "grad_norm": 2.2480509281158447,
        "learning_rate": 0.00018512919588732608,
        "epoch": 0.7263578715191619,
        "step": 5269
    },
    {
        "loss": 2.1661,
        "grad_norm": 2.0577967166900635,
        "learning_rate": 0.00018506071702338313,
        "epoch": 0.7264957264957265,
        "step": 5270
    },
    {
        "loss": 1.9518,
        "grad_norm": 1.6027472019195557,
        "learning_rate": 0.00018499209357725546,
        "epoch": 0.7266335814722912,
        "step": 5271
    },
    {
        "loss": 2.2064,
        "grad_norm": 2.0056190490722656,
        "learning_rate": 0.0001849233256655859,
        "epoch": 0.7267714364488558,
        "step": 5272
    },
    {
        "loss": 1.5575,
        "grad_norm": 2.4466350078582764,
        "learning_rate": 0.00018485441340526282,
        "epoch": 0.7269092914254205,
        "step": 5273
    },
    {
        "loss": 2.2707,
        "grad_norm": 1.4558501243591309,
        "learning_rate": 0.00018478535691342015,
        "epoch": 0.7270471464019851,
        "step": 5274
    },
    {
        "loss": 1.9876,
        "grad_norm": 2.4108781814575195,
        "learning_rate": 0.00018471615630743672,
        "epoch": 0.7271850013785498,
        "step": 5275
    },
    {
        "loss": 1.9172,
        "grad_norm": 2.1023426055908203,
        "learning_rate": 0.00018464681170493654,
        "epoch": 0.7273228563551144,
        "step": 5276
    },
    {
        "loss": 2.4602,
        "grad_norm": 2.164353132247925,
        "learning_rate": 0.0001845773232237883,
        "epoch": 0.7274607113316791,
        "step": 5277
    },
    {
        "loss": 1.6933,
        "grad_norm": 1.4673950672149658,
        "learning_rate": 0.00018450769098210507,
        "epoch": 0.7275985663082437,
        "step": 5278
    },
    {
        "loss": 2.1599,
        "grad_norm": 1.3389558792114258,
        "learning_rate": 0.00018443791509824468,
        "epoch": 0.7277364212848084,
        "step": 5279
    },
    {
        "loss": 2.2902,
        "grad_norm": 1.3552557229995728,
        "learning_rate": 0.0001843679956908087,
        "epoch": 0.727874276261373,
        "step": 5280
    },
    {
        "loss": 1.9105,
        "grad_norm": 1.4411420822143555,
        "learning_rate": 0.00018429793287864287,
        "epoch": 0.7280121312379377,
        "step": 5281
    },
    {
        "loss": 1.8734,
        "grad_norm": 2.4454665184020996,
        "learning_rate": 0.00018422772678083674,
        "epoch": 0.7281499862145023,
        "step": 5282
    },
    {
        "loss": 2.3867,
        "grad_norm": 1.0750523805618286,
        "learning_rate": 0.00018415737751672318,
        "epoch": 0.728287841191067,
        "step": 5283
    },
    {
        "loss": 1.0362,
        "grad_norm": 2.7689900398254395,
        "learning_rate": 0.00018408688520587868,
        "epoch": 0.7284256961676316,
        "step": 5284
    },
    {
        "loss": 1.5195,
        "grad_norm": 2.8000967502593994,
        "learning_rate": 0.00018401624996812265,
        "epoch": 0.7285635511441964,
        "step": 5285
    },
    {
        "loss": 1.0836,
        "grad_norm": 2.4885475635528564,
        "learning_rate": 0.0001839454719235175,
        "epoch": 0.728701406120761,
        "step": 5286
    },
    {
        "loss": 2.2997,
        "grad_norm": 1.8702685832977295,
        "learning_rate": 0.0001838745511923685,
        "epoch": 0.7288392610973257,
        "step": 5287
    },
    {
        "loss": 1.864,
        "grad_norm": 2.6202516555786133,
        "learning_rate": 0.0001838034878952232,
        "epoch": 0.7289771160738903,
        "step": 5288
    },
    {
        "loss": 1.3804,
        "grad_norm": 2.44795823097229,
        "learning_rate": 0.00018373228215287185,
        "epoch": 0.729114971050455,
        "step": 5289
    },
    {
        "loss": 1.9671,
        "grad_norm": 2.006051540374756,
        "learning_rate": 0.00018366093408634648,
        "epoch": 0.7292528260270196,
        "step": 5290
    },
    {
        "loss": 2.6117,
        "grad_norm": 1.7754608392715454,
        "learning_rate": 0.00018358944381692107,
        "epoch": 0.7293906810035843,
        "step": 5291
    },
    {
        "loss": 1.4757,
        "grad_norm": 2.368027687072754,
        "learning_rate": 0.00018351781146611152,
        "epoch": 0.7295285359801489,
        "step": 5292
    },
    {
        "loss": 1.286,
        "grad_norm": 2.528452157974243,
        "learning_rate": 0.00018344603715567513,
        "epoch": 0.7296663909567135,
        "step": 5293
    },
    {
        "loss": 1.4019,
        "grad_norm": 2.523242235183716,
        "learning_rate": 0.0001833741210076104,
        "epoch": 0.7298042459332782,
        "step": 5294
    },
    {
        "loss": 2.1074,
        "grad_norm": 2.2518043518066406,
        "learning_rate": 0.0001833020631441571,
        "epoch": 0.7299421009098428,
        "step": 5295
    },
    {
        "loss": 2.1118,
        "grad_norm": 1.4636552333831787,
        "learning_rate": 0.00018322986368779566,
        "epoch": 0.7300799558864075,
        "step": 5296
    },
    {
        "loss": 1.6365,
        "grad_norm": 3.014402151107788,
        "learning_rate": 0.00018315752276124738,
        "epoch": 0.7302178108629721,
        "step": 5297
    },
    {
        "loss": 1.8712,
        "grad_norm": 2.188936710357666,
        "learning_rate": 0.00018308504048747398,
        "epoch": 0.7303556658395368,
        "step": 5298
    },
    {
        "loss": 1.2499,
        "grad_norm": 2.926213026046753,
        "learning_rate": 0.00018301241698967725,
        "epoch": 0.7304935208161014,
        "step": 5299
    },
    {
        "loss": 2.4238,
        "grad_norm": 2.6464147567749023,
        "learning_rate": 0.00018293965239129926,
        "epoch": 0.7306313757926661,
        "step": 5300
    },
    {
        "loss": 2.0844,
        "grad_norm": 1.542457938194275,
        "learning_rate": 0.00018286674681602193,
        "epoch": 0.7307692307692307,
        "step": 5301
    },
    {
        "loss": 0.7267,
        "grad_norm": 2.946707010269165,
        "learning_rate": 0.00018279370038776663,
        "epoch": 0.7309070857457954,
        "step": 5302
    },
    {
        "loss": 1.6786,
        "grad_norm": 1.985182285308838,
        "learning_rate": 0.00018272051323069415,
        "epoch": 0.73104494072236,
        "step": 5303
    },
    {
        "loss": 1.9834,
        "grad_norm": 1.9074044227600098,
        "learning_rate": 0.00018264718546920472,
        "epoch": 0.7311827956989247,
        "step": 5304
    },
    {
        "loss": 1.05,
        "grad_norm": 2.5514280796051025,
        "learning_rate": 0.0001825737172279372,
        "epoch": 0.7313206506754893,
        "step": 5305
    },
    {
        "loss": 1.8074,
        "grad_norm": 2.002401113510132,
        "learning_rate": 0.00018250010863176964,
        "epoch": 0.7314585056520541,
        "step": 5306
    },
    {
        "loss": 1.9646,
        "grad_norm": 1.4869863986968994,
        "learning_rate": 0.00018242635980581827,
        "epoch": 0.7315963606286187,
        "step": 5307
    },
    {
        "loss": 2.0815,
        "grad_norm": 2.002406597137451,
        "learning_rate": 0.00018235247087543797,
        "epoch": 0.7317342156051834,
        "step": 5308
    },
    {
        "loss": 2.2382,
        "grad_norm": 2.8394935131073,
        "learning_rate": 0.00018227844196622167,
        "epoch": 0.731872070581748,
        "step": 5309
    },
    {
        "loss": 1.7831,
        "grad_norm": 1.5695937871932983,
        "learning_rate": 0.0001822042732040002,
        "epoch": 0.7320099255583127,
        "step": 5310
    },
    {
        "loss": 2.4515,
        "grad_norm": 2.2793586254119873,
        "learning_rate": 0.00018212996471484195,
        "epoch": 0.7321477805348773,
        "step": 5311
    },
    {
        "loss": 2.5616,
        "grad_norm": 2.3497824668884277,
        "learning_rate": 0.00018205551662505323,
        "epoch": 0.732285635511442,
        "step": 5312
    },
    {
        "loss": 1.4499,
        "grad_norm": 2.748490810394287,
        "learning_rate": 0.00018198092906117713,
        "epoch": 0.7324234904880066,
        "step": 5313
    },
    {
        "loss": 2.494,
        "grad_norm": 1.608511209487915,
        "learning_rate": 0.0001819062021499943,
        "epoch": 0.7325613454645713,
        "step": 5314
    },
    {
        "loss": 2.2124,
        "grad_norm": 2.0632200241088867,
        "learning_rate": 0.00018183133601852193,
        "epoch": 0.7326992004411359,
        "step": 5315
    },
    {
        "loss": 1.9421,
        "grad_norm": 2.5433244705200195,
        "learning_rate": 0.00018175633079401378,
        "epoch": 0.7328370554177006,
        "step": 5316
    },
    {
        "loss": 1.5475,
        "grad_norm": 2.01662278175354,
        "learning_rate": 0.0001816811866039604,
        "epoch": 0.7329749103942652,
        "step": 5317
    },
    {
        "loss": 2.1735,
        "grad_norm": 2.0365121364593506,
        "learning_rate": 0.00018160590357608816,
        "epoch": 0.7331127653708299,
        "step": 5318
    },
    {
        "loss": 1.6641,
        "grad_norm": 2.781959056854248,
        "learning_rate": 0.00018153048183835966,
        "epoch": 0.7332506203473945,
        "step": 5319
    },
    {
        "loss": 2.228,
        "grad_norm": 1.1570310592651367,
        "learning_rate": 0.00018145492151897327,
        "epoch": 0.7333884753239592,
        "step": 5320
    },
    {
        "loss": 1.612,
        "grad_norm": 3.595771074295044,
        "learning_rate": 0.00018137922274636274,
        "epoch": 0.7335263303005238,
        "step": 5321
    },
    {
        "loss": 2.1814,
        "grad_norm": 1.2777206897735596,
        "learning_rate": 0.0001813033856491974,
        "epoch": 0.7336641852770885,
        "step": 5322
    },
    {
        "loss": 2.1891,
        "grad_norm": 1.1214951276779175,
        "learning_rate": 0.00018122741035638148,
        "epoch": 0.7338020402536531,
        "step": 5323
    },
    {
        "loss": 2.2075,
        "grad_norm": 1.985625982284546,
        "learning_rate": 0.0001811512969970541,
        "epoch": 0.7339398952302179,
        "step": 5324
    },
    {
        "loss": 2.1801,
        "grad_norm": 1.694583773612976,
        "learning_rate": 0.00018107504570058928,
        "epoch": 0.7340777502067825,
        "step": 5325
    },
    {
        "loss": 1.8414,
        "grad_norm": 2.328284502029419,
        "learning_rate": 0.00018099865659659527,
        "epoch": 0.7342156051833472,
        "step": 5326
    },
    {
        "loss": 2.2741,
        "grad_norm": 2.078418016433716,
        "learning_rate": 0.0001809221298149148,
        "epoch": 0.7343534601599118,
        "step": 5327
    },
    {
        "loss": 2.0644,
        "grad_norm": 1.1847560405731201,
        "learning_rate": 0.00018084546548562436,
        "epoch": 0.7344913151364765,
        "step": 5328
    },
    {
        "loss": 1.9972,
        "grad_norm": 1.5832632780075073,
        "learning_rate": 0.00018076866373903434,
        "epoch": 0.7346291701130411,
        "step": 5329
    },
    {
        "loss": 1.8391,
        "grad_norm": 2.670849084854126,
        "learning_rate": 0.00018069172470568874,
        "epoch": 0.7347670250896058,
        "step": 5330
    },
    {
        "loss": 1.1257,
        "grad_norm": 1.9004980325698853,
        "learning_rate": 0.000180614648516365,
        "epoch": 0.7349048800661704,
        "step": 5331
    },
    {
        "loss": 1.9489,
        "grad_norm": 1.9628328084945679,
        "learning_rate": 0.00018053743530207337,
        "epoch": 0.7350427350427351,
        "step": 5332
    },
    {
        "loss": 2.2628,
        "grad_norm": 1.7344022989273071,
        "learning_rate": 0.00018046008519405745,
        "epoch": 0.7351805900192997,
        "step": 5333
    },
    {
        "loss": 1.8832,
        "grad_norm": 1.5544872283935547,
        "learning_rate": 0.00018038259832379308,
        "epoch": 0.7353184449958643,
        "step": 5334
    },
    {
        "loss": 2.4437,
        "grad_norm": 1.6005128622055054,
        "learning_rate": 0.000180304974822989,
        "epoch": 0.735456299972429,
        "step": 5335
    },
    {
        "loss": 2.4772,
        "grad_norm": 1.5737851858139038,
        "learning_rate": 0.0001802272148235858,
        "epoch": 0.7355941549489936,
        "step": 5336
    },
    {
        "loss": 2.5702,
        "grad_norm": 1.7482340335845947,
        "learning_rate": 0.00018014931845775623,
        "epoch": 0.7357320099255583,
        "step": 5337
    },
    {
        "loss": 1.2606,
        "grad_norm": 2.267397880554199,
        "learning_rate": 0.00018007128585790496,
        "epoch": 0.7358698649021229,
        "step": 5338
    },
    {
        "loss": 1.782,
        "grad_norm": 1.3100342750549316,
        "learning_rate": 0.0001799931171566681,
        "epoch": 0.7360077198786876,
        "step": 5339
    },
    {
        "loss": 2.1382,
        "grad_norm": 1.3216214179992676,
        "learning_rate": 0.00017991481248691313,
        "epoch": 0.7361455748552522,
        "step": 5340
    },
    {
        "loss": 1.5508,
        "grad_norm": 1.8059674501419067,
        "learning_rate": 0.0001798363719817385,
        "epoch": 0.7362834298318169,
        "step": 5341
    },
    {
        "loss": 1.7927,
        "grad_norm": 1.5697669982910156,
        "learning_rate": 0.00017975779577447378,
        "epoch": 0.7364212848083815,
        "step": 5342
    },
    {
        "loss": 2.0827,
        "grad_norm": 1.76773202419281,
        "learning_rate": 0.00017967908399867902,
        "epoch": 0.7365591397849462,
        "step": 5343
    },
    {
        "loss": 2.5194,
        "grad_norm": 1.9496080875396729,
        "learning_rate": 0.0001796002367881449,
        "epoch": 0.7366969947615108,
        "step": 5344
    },
    {
        "loss": 1.7714,
        "grad_norm": 2.526775360107422,
        "learning_rate": 0.00017952125427689203,
        "epoch": 0.7368348497380756,
        "step": 5345
    },
    {
        "loss": 0.8996,
        "grad_norm": 4.191336631774902,
        "learning_rate": 0.0001794421365991712,
        "epoch": 0.7369727047146402,
        "step": 5346
    },
    {
        "loss": 2.0348,
        "grad_norm": 1.4361542463302612,
        "learning_rate": 0.00017936288388946297,
        "epoch": 0.7371105596912049,
        "step": 5347
    },
    {
        "loss": 1.8048,
        "grad_norm": 2.287494421005249,
        "learning_rate": 0.00017928349628247732,
        "epoch": 0.7372484146677695,
        "step": 5348
    },
    {
        "loss": 1.5762,
        "grad_norm": 2.456434488296509,
        "learning_rate": 0.00017920397391315348,
        "epoch": 0.7373862696443342,
        "step": 5349
    },
    {
        "loss": 2.7271,
        "grad_norm": 1.2302097082138062,
        "learning_rate": 0.0001791243169166599,
        "epoch": 0.7375241246208988,
        "step": 5350
    },
    {
        "loss": 1.6245,
        "grad_norm": 1.683671236038208,
        "learning_rate": 0.00017904452542839364,
        "epoch": 0.7376619795974635,
        "step": 5351
    },
    {
        "loss": 2.3665,
        "grad_norm": 1.1270897388458252,
        "learning_rate": 0.00017896459958398068,
        "epoch": 0.7377998345740281,
        "step": 5352
    },
    {
        "loss": 2.2588,
        "grad_norm": 2.438673496246338,
        "learning_rate": 0.00017888453951927513,
        "epoch": 0.7379376895505928,
        "step": 5353
    },
    {
        "loss": 1.8467,
        "grad_norm": 1.9229624271392822,
        "learning_rate": 0.00017880434537035917,
        "epoch": 0.7380755445271574,
        "step": 5354
    },
    {
        "loss": 1.9089,
        "grad_norm": 2.4298250675201416,
        "learning_rate": 0.0001787240172735432,
        "epoch": 0.7382133995037221,
        "step": 5355
    },
    {
        "loss": 2.4532,
        "grad_norm": 1.331182837486267,
        "learning_rate": 0.00017864355536536497,
        "epoch": 0.7383512544802867,
        "step": 5356
    },
    {
        "loss": 1.7346,
        "grad_norm": 1.438880205154419,
        "learning_rate": 0.00017856295978258991,
        "epoch": 0.7384891094568514,
        "step": 5357
    },
    {
        "loss": 1.1688,
        "grad_norm": 2.956892728805542,
        "learning_rate": 0.00017848223066221068,
        "epoch": 0.738626964433416,
        "step": 5358
    },
    {
        "loss": 2.0905,
        "grad_norm": 1.713688611984253,
        "learning_rate": 0.00017840136814144662,
        "epoch": 0.7387648194099807,
        "step": 5359
    },
    {
        "loss": 2.3993,
        "grad_norm": 1.7477259635925293,
        "learning_rate": 0.00017832037235774418,
        "epoch": 0.7389026743865453,
        "step": 5360
    },
    {
        "loss": 2.3335,
        "grad_norm": 2.3600375652313232,
        "learning_rate": 0.00017823924344877615,
        "epoch": 0.73904052936311,
        "step": 5361
    },
    {
        "loss": 2.2397,
        "grad_norm": 2.5385186672210693,
        "learning_rate": 0.00017815798155244143,
        "epoch": 0.7391783843396746,
        "step": 5362
    },
    {
        "loss": 2.0749,
        "grad_norm": 2.4452688694000244,
        "learning_rate": 0.00017807658680686538,
        "epoch": 0.7393162393162394,
        "step": 5363
    },
    {
        "loss": 1.8642,
        "grad_norm": 2.7238287925720215,
        "learning_rate": 0.00017799505935039877,
        "epoch": 0.739454094292804,
        "step": 5364
    },
    {
        "loss": 2.0692,
        "grad_norm": 1.9885531663894653,
        "learning_rate": 0.00017791339932161828,
        "epoch": 0.7395919492693687,
        "step": 5365
    },
    {
        "loss": 2.3518,
        "grad_norm": 1.7559384107589722,
        "learning_rate": 0.00017783160685932566,
        "epoch": 0.7397298042459333,
        "step": 5366
    },
    {
        "loss": 0.8912,
        "grad_norm": 2.95058012008667,
        "learning_rate": 0.0001777496821025478,
        "epoch": 0.739867659222498,
        "step": 5367
    },
    {
        "loss": 1.9944,
        "grad_norm": 3.427537679672241,
        "learning_rate": 0.0001776676251905366,
        "epoch": 0.7400055141990626,
        "step": 5368
    },
    {
        "loss": 2.0014,
        "grad_norm": 1.877485990524292,
        "learning_rate": 0.00017758543626276866,
        "epoch": 0.7401433691756273,
        "step": 5369
    },
    {
        "loss": 2.1735,
        "grad_norm": 2.1563730239868164,
        "learning_rate": 0.00017750311545894457,
        "epoch": 0.7402812241521919,
        "step": 5370
    },
    {
        "loss": 1.763,
        "grad_norm": 1.7052842378616333,
        "learning_rate": 0.0001774206629189895,
        "epoch": 0.7404190791287566,
        "step": 5371
    },
    {
        "loss": 1.9586,
        "grad_norm": 2.1210439205169678,
        "learning_rate": 0.0001773380787830524,
        "epoch": 0.7405569341053212,
        "step": 5372
    },
    {
        "loss": 2.0444,
        "grad_norm": 1.451649785041809,
        "learning_rate": 0.00017725536319150586,
        "epoch": 0.7406947890818859,
        "step": 5373
    },
    {
        "loss": 1.0709,
        "grad_norm": 2.140934467315674,
        "learning_rate": 0.0001771725162849459,
        "epoch": 0.7408326440584505,
        "step": 5374
    },
    {
        "loss": 2.1426,
        "grad_norm": 1.8575429916381836,
        "learning_rate": 0.00017708953820419173,
        "epoch": 0.7409704990350152,
        "step": 5375
    },
    {
        "loss": 1.9833,
        "grad_norm": 2.5729072093963623,
        "learning_rate": 0.00017700642909028555,
        "epoch": 0.7411083540115798,
        "step": 5376
    },
    {
        "loss": 1.9029,
        "grad_norm": 2.215696096420288,
        "learning_rate": 0.0001769231890844925,
        "epoch": 0.7412462089881444,
        "step": 5377
    },
    {
        "loss": 1.8071,
        "grad_norm": 1.1439054012298584,
        "learning_rate": 0.00017683981832829985,
        "epoch": 0.7413840639647091,
        "step": 5378
    },
    {
        "loss": 1.0785,
        "grad_norm": 2.759894371032715,
        "learning_rate": 0.0001767563169634172,
        "epoch": 0.7415219189412737,
        "step": 5379
    },
    {
        "loss": 1.2143,
        "grad_norm": 1.9728273153305054,
        "learning_rate": 0.00017667268513177643,
        "epoch": 0.7416597739178384,
        "step": 5380
    },
    {
        "loss": 1.4224,
        "grad_norm": 1.8105396032333374,
        "learning_rate": 0.00017658892297553079,
        "epoch": 0.741797628894403,
        "step": 5381
    },
    {
        "loss": 1.8381,
        "grad_norm": 2.3041908740997314,
        "learning_rate": 0.00017650503063705534,
        "epoch": 0.7419354838709677,
        "step": 5382
    },
    {
        "loss": 1.9899,
        "grad_norm": 2.0527563095092773,
        "learning_rate": 0.00017642100825894622,
        "epoch": 0.7420733388475323,
        "step": 5383
    },
    {
        "loss": 1.8311,
        "grad_norm": 2.7053375244140625,
        "learning_rate": 0.0001763368559840207,
        "epoch": 0.7422111938240971,
        "step": 5384
    },
    {
        "loss": 1.2697,
        "grad_norm": 2.938058614730835,
        "learning_rate": 0.000176252573955317,
        "epoch": 0.7423490488006617,
        "step": 5385
    },
    {
        "loss": 2.1612,
        "grad_norm": 2.382436513900757,
        "learning_rate": 0.0001761681623160936,
        "epoch": 0.7424869037772264,
        "step": 5386
    },
    {
        "loss": 1.8494,
        "grad_norm": 2.0239360332489014,
        "learning_rate": 0.00017608362120982937,
        "epoch": 0.742624758753791,
        "step": 5387
    },
    {
        "loss": 1.8393,
        "grad_norm": 2.745420217514038,
        "learning_rate": 0.00017599895078022342,
        "epoch": 0.7427626137303557,
        "step": 5388
    },
    {
        "loss": 1.4597,
        "grad_norm": 3.284693717956543,
        "learning_rate": 0.00017591415117119435,
        "epoch": 0.7429004687069203,
        "step": 5389
    },
    {
        "loss": 1.6769,
        "grad_norm": 1.531630039215088,
        "learning_rate": 0.00017582922252688072,
        "epoch": 0.743038323683485,
        "step": 5390
    },
    {
        "loss": 1.9918,
        "grad_norm": 2.3604321479797363,
        "learning_rate": 0.00017574416499164017,
        "epoch": 0.7431761786600496,
        "step": 5391
    },
    {
        "loss": 2.2447,
        "grad_norm": 2.0579776763916016,
        "learning_rate": 0.00017565897871004932,
        "epoch": 0.7433140336366143,
        "step": 5392
    },
    {
        "loss": 1.8967,
        "grad_norm": 0.9783039689064026,
        "learning_rate": 0.00017557366382690406,
        "epoch": 0.7434518886131789,
        "step": 5393
    },
    {
        "loss": 1.9228,
        "grad_norm": 1.8807239532470703,
        "learning_rate": 0.0001754882204872183,
        "epoch": 0.7435897435897436,
        "step": 5394
    },
    {
        "loss": 1.3916,
        "grad_norm": 2.209351062774658,
        "learning_rate": 0.00017540264883622478,
        "epoch": 0.7437275985663082,
        "step": 5395
    },
    {
        "loss": 2.0283,
        "grad_norm": 2.997241973876953,
        "learning_rate": 0.00017531694901937414,
        "epoch": 0.7438654535428729,
        "step": 5396
    },
    {
        "loss": 2.0712,
        "grad_norm": 1.80516517162323,
        "learning_rate": 0.00017523112118233481,
        "epoch": 0.7440033085194375,
        "step": 5397
    },
    {
        "loss": 1.5894,
        "grad_norm": 1.9243968725204468,
        "learning_rate": 0.00017514516547099295,
        "epoch": 0.7441411634960022,
        "step": 5398
    },
    {
        "loss": 1.928,
        "grad_norm": 2.180276870727539,
        "learning_rate": 0.000175059082031452,
        "epoch": 0.7442790184725668,
        "step": 5399
    },
    {
        "loss": 1.941,
        "grad_norm": 1.3812271356582642,
        "learning_rate": 0.0001749728710100324,
        "epoch": 0.7444168734491315,
        "step": 5400
    },
    {
        "loss": 1.3976,
        "grad_norm": 2.605548620223999,
        "learning_rate": 0.00017488653255327182,
        "epoch": 0.7445547284256961,
        "step": 5401
    },
    {
        "loss": 1.8997,
        "grad_norm": 1.7993427515029907,
        "learning_rate": 0.000174800066807924,
        "epoch": 0.7446925834022609,
        "step": 5402
    },
    {
        "loss": 2.282,
        "grad_norm": 1.575376033782959,
        "learning_rate": 0.0001747134739209595,
        "epoch": 0.7448304383788255,
        "step": 5403
    },
    {
        "loss": 1.3929,
        "grad_norm": 3.2472288608551025,
        "learning_rate": 0.00017462675403956483,
        "epoch": 0.7449682933553902,
        "step": 5404
    },
    {
        "loss": 1.7237,
        "grad_norm": 1.2256898880004883,
        "learning_rate": 0.0001745399073111422,
        "epoch": 0.7451061483319548,
        "step": 5405
    },
    {
        "loss": 1.5552,
        "grad_norm": 2.2015061378479004,
        "learning_rate": 0.00017445293388330965,
        "epoch": 0.7452440033085195,
        "step": 5406
    },
    {
        "loss": 1.9436,
        "grad_norm": 2.4691708087921143,
        "learning_rate": 0.00017436583390390063,
        "epoch": 0.7453818582850841,
        "step": 5407
    },
    {
        "loss": 1.4448,
        "grad_norm": 1.9184521436691284,
        "learning_rate": 0.0001742786075209633,
        "epoch": 0.7455197132616488,
        "step": 5408
    },
    {
        "loss": 2.0217,
        "grad_norm": 2.138150215148926,
        "learning_rate": 0.0001741912548827611,
        "epoch": 0.7456575682382134,
        "step": 5409
    },
    {
        "loss": 2.0493,
        "grad_norm": 2.0950703620910645,
        "learning_rate": 0.00017410377613777205,
        "epoch": 0.7457954232147781,
        "step": 5410
    },
    {
        "loss": 2.0013,
        "grad_norm": 2.630530834197998,
        "learning_rate": 0.0001740161714346882,
        "epoch": 0.7459332781913427,
        "step": 5411
    },
    {
        "loss": 2.5664,
        "grad_norm": 1.5100401639938354,
        "learning_rate": 0.0001739284409224159,
        "epoch": 0.7460711331679074,
        "step": 5412
    },
    {
        "loss": 1.811,
        "grad_norm": 2.397765874862671,
        "learning_rate": 0.0001738405847500753,
        "epoch": 0.746208988144472,
        "step": 5413
    },
    {
        "loss": 1.9708,
        "grad_norm": 2.815730571746826,
        "learning_rate": 0.0001737526030670002,
        "epoch": 0.7463468431210367,
        "step": 5414
    },
    {
        "loss": 2.0314,
        "grad_norm": 1.8476430177688599,
        "learning_rate": 0.0001736644960227378,
        "epoch": 0.7464846980976013,
        "step": 5415
    },
    {
        "loss": 2.2788,
        "grad_norm": 2.6448609828948975,
        "learning_rate": 0.0001735762637670482,
        "epoch": 0.746622553074166,
        "step": 5416
    },
    {
        "loss": 2.1935,
        "grad_norm": 1.9399514198303223,
        "learning_rate": 0.00017348790644990434,
        "epoch": 0.7467604080507306,
        "step": 5417
    },
    {
        "loss": 2.1754,
        "grad_norm": 2.1655006408691406,
        "learning_rate": 0.000173399424221492,
        "epoch": 0.7468982630272953,
        "step": 5418
    },
    {
        "loss": 2.0732,
        "grad_norm": 2.268573522567749,
        "learning_rate": 0.00017331081723220889,
        "epoch": 0.7470361180038599,
        "step": 5419
    },
    {
        "loss": 1.4344,
        "grad_norm": 2.5078587532043457,
        "learning_rate": 0.00017322208563266513,
        "epoch": 0.7471739729804245,
        "step": 5420
    },
    {
        "loss": 2.1466,
        "grad_norm": 2.315661907196045,
        "learning_rate": 0.00017313322957368237,
        "epoch": 0.7473118279569892,
        "step": 5421
    },
    {
        "loss": 1.4361,
        "grad_norm": 2.260956287384033,
        "learning_rate": 0.000173044249206294,
        "epoch": 0.7474496829335538,
        "step": 5422
    },
    {
        "loss": 2.227,
        "grad_norm": 1.7528003454208374,
        "learning_rate": 0.00017295514468174475,
        "epoch": 0.7475875379101186,
        "step": 5423
    },
    {
        "loss": 2.3006,
        "grad_norm": 1.2414660453796387,
        "learning_rate": 0.00017286591615149014,
        "epoch": 0.7477253928866832,
        "step": 5424
    },
    {
        "loss": 2.2046,
        "grad_norm": 0.9159204959869385,
        "learning_rate": 0.0001727765637671966,
        "epoch": 0.7478632478632479,
        "step": 5425
    },
    {
        "loss": 1.9919,
        "grad_norm": 2.5598692893981934,
        "learning_rate": 0.0001726870876807412,
        "epoch": 0.7480011028398125,
        "step": 5426
    },
    {
        "loss": 1.6942,
        "grad_norm": 1.4761849641799927,
        "learning_rate": 0.00017259748804421098,
        "epoch": 0.7481389578163772,
        "step": 5427
    },
    {
        "loss": 2.1908,
        "grad_norm": 1.966903567314148,
        "learning_rate": 0.0001725077650099034,
        "epoch": 0.7482768127929418,
        "step": 5428
    },
    {
        "loss": 1.8279,
        "grad_norm": 1.5716739892959595,
        "learning_rate": 0.00017241791873032528,
        "epoch": 0.7484146677695065,
        "step": 5429
    },
    {
        "loss": 1.762,
        "grad_norm": 1.6029877662658691,
        "learning_rate": 0.00017232794935819303,
        "epoch": 0.7485525227460711,
        "step": 5430
    },
    {
        "loss": 1.4963,
        "grad_norm": 1.9482412338256836,
        "learning_rate": 0.00017223785704643252,
        "epoch": 0.7486903777226358,
        "step": 5431
    },
    {
        "loss": 1.7791,
        "grad_norm": 2.080496072769165,
        "learning_rate": 0.00017214764194817814,
        "epoch": 0.7488282326992004,
        "step": 5432
    },
    {
        "loss": 1.6365,
        "grad_norm": 2.5650148391723633,
        "learning_rate": 0.00017205730421677345,
        "epoch": 0.7489660876757651,
        "step": 5433
    },
    {
        "loss": 1.6236,
        "grad_norm": 2.0026121139526367,
        "learning_rate": 0.0001719668440057703,
        "epoch": 0.7491039426523297,
        "step": 5434
    },
    {
        "loss": 2.2976,
        "grad_norm": 1.8473392724990845,
        "learning_rate": 0.00017187626146892846,
        "epoch": 0.7492417976288944,
        "step": 5435
    },
    {
        "loss": 2.2474,
        "grad_norm": 2.836318016052246,
        "learning_rate": 0.00017178555676021607,
        "epoch": 0.749379652605459,
        "step": 5436
    },
    {
        "loss": 1.8031,
        "grad_norm": 2.098548650741577,
        "learning_rate": 0.0001716947300338086,
        "epoch": 0.7495175075820237,
        "step": 5437
    },
    {
        "loss": 2.2311,
        "grad_norm": 1.6306222677230835,
        "learning_rate": 0.00017160378144408895,
        "epoch": 0.7496553625585883,
        "step": 5438
    },
    {
        "loss": 2.2286,
        "grad_norm": 1.867017388343811,
        "learning_rate": 0.00017151271114564738,
        "epoch": 0.749793217535153,
        "step": 5439
    },
    {
        "loss": 2.3963,
        "grad_norm": 2.4900567531585693,
        "learning_rate": 0.0001714215192932807,
        "epoch": 0.7499310725117176,
        "step": 5440
    },
    {
        "loss": 1.8683,
        "grad_norm": 1.9814233779907227,
        "learning_rate": 0.00017133020604199274,
        "epoch": 0.7500689274882824,
        "step": 5441
    },
    {
        "loss": 2.1978,
        "grad_norm": 1.3847476243972778,
        "learning_rate": 0.00017123877154699333,
        "epoch": 0.750206782464847,
        "step": 5442
    },
    {
        "loss": 2.0429,
        "grad_norm": 1.8964532613754272,
        "learning_rate": 0.0001711472159636984,
        "epoch": 0.7503446374414117,
        "step": 5443
    },
    {
        "loss": 1.7403,
        "grad_norm": 2.0563888549804688,
        "learning_rate": 0.00017105553944772985,
        "epoch": 0.7504824924179763,
        "step": 5444
    },
    {
        "loss": 2.1053,
        "grad_norm": 2.896638870239258,
        "learning_rate": 0.00017096374215491524,
        "epoch": 0.750620347394541,
        "step": 5445
    },
    {
        "loss": 2.2496,
        "grad_norm": 1.7646913528442383,
        "learning_rate": 0.0001708718242412871,
        "epoch": 0.7507582023711056,
        "step": 5446
    },
    {
        "loss": 1.7311,
        "grad_norm": 2.639738082885742,
        "learning_rate": 0.00017077978586308316,
        "epoch": 0.7508960573476703,
        "step": 5447
    },
    {
        "loss": 1.2088,
        "grad_norm": 3.2652506828308105,
        "learning_rate": 0.0001706876271767461,
        "epoch": 0.7510339123242349,
        "step": 5448
    },
    {
        "loss": 1.7377,
        "grad_norm": 2.9106462001800537,
        "learning_rate": 0.00017059534833892273,
        "epoch": 0.7511717673007996,
        "step": 5449
    },
    {
        "loss": 1.5543,
        "grad_norm": 2.167760133743286,
        "learning_rate": 0.0001705029495064643,
        "epoch": 0.7513096222773642,
        "step": 5450
    },
    {
        "loss": 1.6721,
        "grad_norm": 2.986670970916748,
        "learning_rate": 0.0001704104308364258,
        "epoch": 0.7514474772539289,
        "step": 5451
    },
    {
        "loss": 1.8006,
        "grad_norm": 1.6618117094039917,
        "learning_rate": 0.0001703177924860663,
        "epoch": 0.7515853322304935,
        "step": 5452
    },
    {
        "loss": 1.5069,
        "grad_norm": 2.670149564743042,
        "learning_rate": 0.0001702250346128481,
        "epoch": 0.7517231872070582,
        "step": 5453
    },
    {
        "loss": 1.8137,
        "grad_norm": 1.6374045610427856,
        "learning_rate": 0.0001701321573744366,
        "epoch": 0.7518610421836228,
        "step": 5454
    },
    {
        "loss": 1.8538,
        "grad_norm": 2.527923822402954,
        "learning_rate": 0.00017003916092870003,
        "epoch": 0.7519988971601875,
        "step": 5455
    },
    {
        "loss": 1.1019,
        "grad_norm": 2.645651340484619,
        "learning_rate": 0.00016994604543370948,
        "epoch": 0.7521367521367521,
        "step": 5456
    },
    {
        "loss": 1.859,
        "grad_norm": 1.6037108898162842,
        "learning_rate": 0.0001698528110477382,
        "epoch": 0.7522746071133168,
        "step": 5457
    },
    {
        "loss": 1.7245,
        "grad_norm": 2.374154806137085,
        "learning_rate": 0.00016975945792926166,
        "epoch": 0.7524124620898814,
        "step": 5458
    },
    {
        "loss": 1.8388,
        "grad_norm": 2.8812482357025146,
        "learning_rate": 0.00016966598623695695,
        "epoch": 0.7525503170664462,
        "step": 5459
    },
    {
        "loss": 2.2459,
        "grad_norm": 1.9094998836517334,
        "learning_rate": 0.00016957239612970293,
        "epoch": 0.7526881720430108,
        "step": 5460
    },
    {
        "loss": 2.34,
        "grad_norm": 1.8571943044662476,
        "learning_rate": 0.00016947868776657972,
        "epoch": 0.7528260270195755,
        "step": 5461
    },
    {
        "loss": 2.641,
        "grad_norm": 4.919154644012451,
        "learning_rate": 0.00016938486130686826,
        "epoch": 0.7529638819961401,
        "step": 5462
    },
    {
        "loss": 2.0667,
        "grad_norm": 2.3810997009277344,
        "learning_rate": 0.00016929091691005025,
        "epoch": 0.7531017369727047,
        "step": 5463
    },
    {
        "loss": 1.8087,
        "grad_norm": 2.097839832305908,
        "learning_rate": 0.0001691968547358081,
        "epoch": 0.7532395919492694,
        "step": 5464
    },
    {
        "loss": 2.2193,
        "grad_norm": 1.9778835773468018,
        "learning_rate": 0.0001691026749440241,
        "epoch": 0.753377446925834,
        "step": 5465
    },
    {
        "loss": 1.9457,
        "grad_norm": 2.512498140335083,
        "learning_rate": 0.00016900837769478076,
        "epoch": 0.7535153019023987,
        "step": 5466
    },
    {
        "loss": 1.9201,
        "grad_norm": 2.881723642349243,
        "learning_rate": 0.00016891396314835998,
        "epoch": 0.7536531568789633,
        "step": 5467
    },
    {
        "loss": 1.9602,
        "grad_norm": 2.1781063079833984,
        "learning_rate": 0.0001688194314652431,
        "epoch": 0.753791011855528,
        "step": 5468
    },
    {
        "loss": 2.2959,
        "grad_norm": 1.538892149925232,
        "learning_rate": 0.00016872478280611074,
        "epoch": 0.7539288668320926,
        "step": 5469
    },
    {
        "loss": 1.6076,
        "grad_norm": 3.100879430770874,
        "learning_rate": 0.00016863001733184206,
        "epoch": 0.7540667218086573,
        "step": 5470
    },
    {
        "loss": 2.4336,
        "grad_norm": 1.8884040117263794,
        "learning_rate": 0.000168535135203515,
        "epoch": 0.7542045767852219,
        "step": 5471
    },
    {
        "loss": 2.239,
        "grad_norm": 1.1812719106674194,
        "learning_rate": 0.00016844013658240585,
        "epoch": 0.7543424317617866,
        "step": 5472
    },
    {
        "loss": 2.0027,
        "grad_norm": 1.5656075477600098,
        "learning_rate": 0.00016834502162998852,
        "epoch": 0.7544802867383512,
        "step": 5473
    },
    {
        "loss": 1.2819,
        "grad_norm": 1.7308800220489502,
        "learning_rate": 0.0001682497905079352,
        "epoch": 0.7546181417149159,
        "step": 5474
    },
    {
        "loss": 1.8073,
        "grad_norm": 2.244046688079834,
        "learning_rate": 0.00016815444337811507,
        "epoch": 0.7547559966914805,
        "step": 5475
    },
    {
        "loss": 1.8219,
        "grad_norm": 1.7462267875671387,
        "learning_rate": 0.00016805898040259457,
        "epoch": 0.7548938516680452,
        "step": 5476
    },
    {
        "loss": 0.7485,
        "grad_norm": 2.1242663860321045,
        "learning_rate": 0.00016796340174363738,
        "epoch": 0.7550317066446098,
        "step": 5477
    },
    {
        "loss": 2.1277,
        "grad_norm": 2.025144100189209,
        "learning_rate": 0.0001678677075637034,
        "epoch": 0.7551695616211745,
        "step": 5478
    },
    {
        "loss": 1.1201,
        "grad_norm": 2.484980583190918,
        "learning_rate": 0.00016777189802544927,
        "epoch": 0.7553074165977391,
        "step": 5479
    },
    {
        "loss": 1.2834,
        "grad_norm": 2.672109842300415,
        "learning_rate": 0.00016767597329172737,
        "epoch": 0.7554452715743039,
        "step": 5480
    },
    {
        "loss": 1.4751,
        "grad_norm": 2.5875632762908936,
        "learning_rate": 0.00016757993352558596,
        "epoch": 0.7555831265508685,
        "step": 5481
    },
    {
        "loss": 2.6845,
        "grad_norm": 1.535477876663208,
        "learning_rate": 0.000167483778890269,
        "epoch": 0.7557209815274332,
        "step": 5482
    },
    {
        "loss": 2.3098,
        "grad_norm": 1.6150527000427246,
        "learning_rate": 0.00016738750954921564,
        "epoch": 0.7558588365039978,
        "step": 5483
    },
    {
        "loss": 1.7581,
        "grad_norm": 1.9650448560714722,
        "learning_rate": 0.00016729112566605983,
        "epoch": 0.7559966914805625,
        "step": 5484
    },
    {
        "loss": 1.9562,
        "grad_norm": 2.5874717235565186,
        "learning_rate": 0.0001671946274046304,
        "epoch": 0.7561345464571271,
        "step": 5485
    },
    {
        "loss": 1.5677,
        "grad_norm": 1.466313123703003,
        "learning_rate": 0.00016709801492895062,
        "epoch": 0.7562724014336918,
        "step": 5486
    },
    {
        "loss": 2.0062,
        "grad_norm": 1.6745445728302002,
        "learning_rate": 0.00016700128840323774,
        "epoch": 0.7564102564102564,
        "step": 5487
    },
    {
        "loss": 2.0598,
        "grad_norm": 1.2544969320297241,
        "learning_rate": 0.0001669044479919029,
        "epoch": 0.7565481113868211,
        "step": 5488
    },
    {
        "loss": 2.295,
        "grad_norm": 1.7482929229736328,
        "learning_rate": 0.00016680749385955077,
        "epoch": 0.7566859663633857,
        "step": 5489
    },
    {
        "loss": 1.8867,
        "grad_norm": 2.2802953720092773,
        "learning_rate": 0.00016671042617097948,
        "epoch": 0.7568238213399504,
        "step": 5490
    },
    {
        "loss": 1.962,
        "grad_norm": 2.9562759399414062,
        "learning_rate": 0.00016661324509118023,
        "epoch": 0.756961676316515,
        "step": 5491
    },
    {
        "loss": 2.0113,
        "grad_norm": 1.587457537651062,
        "learning_rate": 0.00016651595078533675,
        "epoch": 0.7570995312930797,
        "step": 5492
    },
    {
        "loss": 1.8581,
        "grad_norm": 2.8413937091827393,
        "learning_rate": 0.0001664185434188251,
        "epoch": 0.7572373862696443,
        "step": 5493
    },
    {
        "loss": 2.3833,
        "grad_norm": 2.3794963359832764,
        "learning_rate": 0.000166321023157214,
        "epoch": 0.757375241246209,
        "step": 5494
    },
    {
        "loss": 1.1652,
        "grad_norm": 3.1759355068206787,
        "learning_rate": 0.00016622339016626354,
        "epoch": 0.7575130962227736,
        "step": 5495
    },
    {
        "loss": 1.9291,
        "grad_norm": 1.607526421546936,
        "learning_rate": 0.0001661256446119259,
        "epoch": 0.7576509511993383,
        "step": 5496
    },
    {
        "loss": 1.9548,
        "grad_norm": 1.348920226097107,
        "learning_rate": 0.00016602778666034414,
        "epoch": 0.7577888061759029,
        "step": 5497
    },
    {
        "loss": 2.1084,
        "grad_norm": 1.9434423446655273,
        "learning_rate": 0.00016592981647785263,
        "epoch": 0.7579266611524677,
        "step": 5498
    },
    {
        "loss": 1.6701,
        "grad_norm": 1.8481788635253906,
        "learning_rate": 0.0001658317342309766,
        "epoch": 0.7580645161290323,
        "step": 5499
    },
    {
        "loss": 2.0401,
        "grad_norm": 1.374768853187561,
        "learning_rate": 0.00016573354008643152,
        "epoch": 0.758202371105597,
        "step": 5500
    },
    {
        "loss": 2.5043,
        "grad_norm": 1.6175017356872559,
        "learning_rate": 0.000165635234211123,
        "epoch": 0.7583402260821616,
        "step": 5501
    },
    {
        "loss": 2.3002,
        "grad_norm": 2.346924066543579,
        "learning_rate": 0.00016553681677214702,
        "epoch": 0.7584780810587263,
        "step": 5502
    },
    {
        "loss": 1.6808,
        "grad_norm": 2.6469295024871826,
        "learning_rate": 0.00016543828793678863,
        "epoch": 0.7586159360352909,
        "step": 5503
    },
    {
        "loss": 2.2507,
        "grad_norm": 1.2123223543167114,
        "learning_rate": 0.00016533964787252264,
        "epoch": 0.7587537910118556,
        "step": 5504
    },
    {
        "loss": 2.2904,
        "grad_norm": 1.6378395557403564,
        "learning_rate": 0.0001652408967470127,
        "epoch": 0.7588916459884202,
        "step": 5505
    },
    {
        "loss": 1.8613,
        "grad_norm": 2.2545835971832275,
        "learning_rate": 0.00016514203472811127,
        "epoch": 0.7590295009649848,
        "step": 5506
    },
    {
        "loss": 2.1763,
        "grad_norm": 2.0981431007385254,
        "learning_rate": 0.00016504306198385945,
        "epoch": 0.7591673559415495,
        "step": 5507
    },
    {
        "loss": 1.42,
        "grad_norm": 2.3965299129486084,
        "learning_rate": 0.00016494397868248626,
        "epoch": 0.7593052109181141,
        "step": 5508
    },
    {
        "loss": 1.8595,
        "grad_norm": 2.223555564880371,
        "learning_rate": 0.0001648447849924089,
        "epoch": 0.7594430658946788,
        "step": 5509
    },
    {
        "loss": 1.2705,
        "grad_norm": 2.137578010559082,
        "learning_rate": 0.00016474548108223223,
        "epoch": 0.7595809208712434,
        "step": 5510
    },
    {
        "loss": 2.2775,
        "grad_norm": 1.9450018405914307,
        "learning_rate": 0.00016464606712074815,
        "epoch": 0.7597187758478081,
        "step": 5511
    },
    {
        "loss": 2.3286,
        "grad_norm": 1.5482317209243774,
        "learning_rate": 0.000164546543276936,
        "epoch": 0.7598566308243727,
        "step": 5512
    },
    {
        "loss": 1.1216,
        "grad_norm": 2.748528480529785,
        "learning_rate": 0.00016444690971996158,
        "epoch": 0.7599944858009374,
        "step": 5513
    },
    {
        "loss": 1.54,
        "grad_norm": 2.129727840423584,
        "learning_rate": 0.00016434716661917724,
        "epoch": 0.760132340777502,
        "step": 5514
    },
    {
        "loss": 2.5626,
        "grad_norm": 1.9255331754684448,
        "learning_rate": 0.00016424731414412178,
        "epoch": 0.7602701957540667,
        "step": 5515
    },
    {
        "loss": 1.1289,
        "grad_norm": 2.6029484272003174,
        "learning_rate": 0.0001641473524645195,
        "epoch": 0.7604080507306313,
        "step": 5516
    },
    {
        "loss": 1.5301,
        "grad_norm": 2.063325881958008,
        "learning_rate": 0.0001640472817502807,
        "epoch": 0.760545905707196,
        "step": 5517
    },
    {
        "loss": 2.1829,
        "grad_norm": 1.4298471212387085,
        "learning_rate": 0.0001639471021715008,
        "epoch": 0.7606837606837606,
        "step": 5518
    },
    {
        "loss": 1.4708,
        "grad_norm": 1.8785617351531982,
        "learning_rate": 0.00016384681389846023,
        "epoch": 0.7608216156603254,
        "step": 5519
    },
    {
        "loss": 2.5567,
        "grad_norm": 1.3140161037445068,
        "learning_rate": 0.00016374641710162428,
        "epoch": 0.76095947063689,
        "step": 5520
    },
    {
        "loss": 1.5803,
        "grad_norm": 2.2855982780456543,
        "learning_rate": 0.00016364591195164286,
        "epoch": 0.7610973256134547,
        "step": 5521
    },
    {
        "loss": 1.5686,
        "grad_norm": 2.2606828212738037,
        "learning_rate": 0.00016354529861934962,
        "epoch": 0.7612351805900193,
        "step": 5522
    },
    {
        "loss": 1.9638,
        "grad_norm": 1.5938290357589722,
        "learning_rate": 0.00016344457727576243,
        "epoch": 0.761373035566584,
        "step": 5523
    },
    {
        "loss": 1.9921,
        "grad_norm": 1.9945247173309326,
        "learning_rate": 0.00016334374809208284,
        "epoch": 0.7615108905431486,
        "step": 5524
    },
    {
        "loss": 2.3785,
        "grad_norm": 1.539499282836914,
        "learning_rate": 0.00016324281123969537,
        "epoch": 0.7616487455197133,
        "step": 5525
    },
    {
        "loss": 2.3316,
        "grad_norm": 1.8234422206878662,
        "learning_rate": 0.00016314176689016772,
        "epoch": 0.7617866004962779,
        "step": 5526
    },
    {
        "loss": 2.1547,
        "grad_norm": 1.9361011981964111,
        "learning_rate": 0.00016304061521525017,
        "epoch": 0.7619244554728426,
        "step": 5527
    },
    {
        "loss": 1.0058,
        "grad_norm": 2.141014337539673,
        "learning_rate": 0.0001629393563868757,
        "epoch": 0.7620623104494072,
        "step": 5528
    },
    {
        "loss": 2.0806,
        "grad_norm": 1.2438355684280396,
        "learning_rate": 0.00016283799057715937,
        "epoch": 0.7622001654259719,
        "step": 5529
    },
    {
        "loss": 0.7859,
        "grad_norm": 2.661661386489868,
        "learning_rate": 0.0001627365179583979,
        "epoch": 0.7623380204025365,
        "step": 5530
    },
    {
        "loss": 2.2155,
        "grad_norm": 1.611979603767395,
        "learning_rate": 0.00016263493870306953,
        "epoch": 0.7624758753791012,
        "step": 5531
    },
    {
        "loss": 1.5395,
        "grad_norm": 1.611609697341919,
        "learning_rate": 0.00016253325298383407,
        "epoch": 0.7626137303556658,
        "step": 5532
    },
    {
        "loss": 2.3224,
        "grad_norm": 1.4546701908111572,
        "learning_rate": 0.00016243146097353188,
        "epoch": 0.7627515853322305,
        "step": 5533
    },
    {
        "loss": 2.2356,
        "grad_norm": 1.5685396194458008,
        "learning_rate": 0.00016232956284518437,
        "epoch": 0.7628894403087951,
        "step": 5534
    },
    {
        "loss": 1.9089,
        "grad_norm": 2.0960099697113037,
        "learning_rate": 0.0001622275587719932,
        "epoch": 0.7630272952853598,
        "step": 5535
    },
    {
        "loss": 1.5555,
        "grad_norm": 1.9252899885177612,
        "learning_rate": 0.00016212544892733988,
        "epoch": 0.7631651502619244,
        "step": 5536
    },
    {
        "loss": 1.8158,
        "grad_norm": 2.0366835594177246,
        "learning_rate": 0.00016202323348478613,
        "epoch": 0.7633030052384892,
        "step": 5537
    },
    {
        "loss": 2.5028,
        "grad_norm": 1.3014378547668457,
        "learning_rate": 0.00016192091261807282,
        "epoch": 0.7634408602150538,
        "step": 5538
    },
    {
        "loss": 1.802,
        "grad_norm": 1.6472810506820679,
        "learning_rate": 0.00016181848650111998,
        "epoch": 0.7635787151916185,
        "step": 5539
    },
    {
        "loss": 1.279,
        "grad_norm": 2.6460957527160645,
        "learning_rate": 0.00016171595530802693,
        "epoch": 0.7637165701681831,
        "step": 5540
    },
    {
        "loss": 2.1154,
        "grad_norm": 1.9893224239349365,
        "learning_rate": 0.00016161331921307106,
        "epoch": 0.7638544251447478,
        "step": 5541
    },
    {
        "loss": 1.8731,
        "grad_norm": 3.262098550796509,
        "learning_rate": 0.00016151057839070866,
        "epoch": 0.7639922801213124,
        "step": 5542
    },
    {
        "loss": 1.9718,
        "grad_norm": 1.7104215621948242,
        "learning_rate": 0.00016140773301557358,
        "epoch": 0.7641301350978771,
        "step": 5543
    },
    {
        "loss": 1.758,
        "grad_norm": 1.8990474939346313,
        "learning_rate": 0.0001613047832624774,
        "epoch": 0.7642679900744417,
        "step": 5544
    },
    {
        "loss": 2.0262,
        "grad_norm": 2.172665596008301,
        "learning_rate": 0.00016120172930640946,
        "epoch": 0.7644058450510064,
        "step": 5545
    },
    {
        "loss": 1.6941,
        "grad_norm": 2.5039658546447754,
        "learning_rate": 0.0001610985713225358,
        "epoch": 0.764543700027571,
        "step": 5546
    },
    {
        "loss": 1.7955,
        "grad_norm": 1.9818930625915527,
        "learning_rate": 0.00016099530948619962,
        "epoch": 0.7646815550041357,
        "step": 5547
    },
    {
        "loss": 2.2203,
        "grad_norm": 1.5250555276870728,
        "learning_rate": 0.00016089194397292053,
        "epoch": 0.7648194099807003,
        "step": 5548
    },
    {
        "loss": 1.7552,
        "grad_norm": 2.052398681640625,
        "learning_rate": 0.0001607884749583942,
        "epoch": 0.7649572649572649,
        "step": 5549
    },
    {
        "loss": 1.8346,
        "grad_norm": 1.3825534582138062,
        "learning_rate": 0.00016068490261849253,
        "epoch": 0.7650951199338296,
        "step": 5550
    },
    {
        "loss": 1.8526,
        "grad_norm": 2.048259973526001,
        "learning_rate": 0.00016058122712926285,
        "epoch": 0.7652329749103942,
        "step": 5551
    },
    {
        "loss": 2.4499,
        "grad_norm": 1.596239447593689,
        "learning_rate": 0.00016047744866692768,
        "epoch": 0.7653708298869589,
        "step": 5552
    },
    {
        "loss": 2.4192,
        "grad_norm": 1.906354308128357,
        "learning_rate": 0.00016037356740788487,
        "epoch": 0.7655086848635235,
        "step": 5553
    },
    {
        "loss": 1.8383,
        "grad_norm": 2.0304415225982666,
        "learning_rate": 0.00016026958352870704,
        "epoch": 0.7656465398400882,
        "step": 5554
    },
    {
        "loss": 2.4291,
        "grad_norm": 1.2696305513381958,
        "learning_rate": 0.00016016549720614088,
        "epoch": 0.7657843948166528,
        "step": 5555
    },
    {
        "loss": 1.6568,
        "grad_norm": 1.8532568216323853,
        "learning_rate": 0.0001600613086171074,
        "epoch": 0.7659222497932175,
        "step": 5556
    },
    {
        "loss": 2.0206,
        "grad_norm": 1.9114147424697876,
        "learning_rate": 0.0001599570179387014,
        "epoch": 0.7660601047697821,
        "step": 5557
    },
    {
        "loss": 1.6639,
        "grad_norm": 1.3792897462844849,
        "learning_rate": 0.00015985262534819127,
        "epoch": 0.7661979597463469,
        "step": 5558
    },
    {
        "loss": 2.3132,
        "grad_norm": 1.2616875171661377,
        "learning_rate": 0.00015974813102301877,
        "epoch": 0.7663358147229115,
        "step": 5559
    },
    {
        "loss": 2.1873,
        "grad_norm": 2.7984766960144043,
        "learning_rate": 0.0001596435351407982,
        "epoch": 0.7664736696994762,
        "step": 5560
    },
    {
        "loss": 1.9189,
        "grad_norm": 2.4966588020324707,
        "learning_rate": 0.00015953883787931674,
        "epoch": 0.7666115246760408,
        "step": 5561
    },
    {
        "loss": 1.9988,
        "grad_norm": 1.8981434106826782,
        "learning_rate": 0.000159434039416534,
        "epoch": 0.7667493796526055,
        "step": 5562
    },
    {
        "loss": 2.4009,
        "grad_norm": 1.5055515766143799,
        "learning_rate": 0.00015932913993058133,
        "epoch": 0.7668872346291701,
        "step": 5563
    },
    {
        "loss": 2.1611,
        "grad_norm": 1.9492428302764893,
        "learning_rate": 0.00015922413959976195,
        "epoch": 0.7670250896057348,
        "step": 5564
    },
    {
        "loss": 1.7638,
        "grad_norm": 1.9153386354446411,
        "learning_rate": 0.0001591190386025503,
        "epoch": 0.7671629445822994,
        "step": 5565
    },
    {
        "loss": 2.3276,
        "grad_norm": 1.776396632194519,
        "learning_rate": 0.0001590138371175922,
        "epoch": 0.7673007995588641,
        "step": 5566
    },
    {
        "loss": 1.3682,
        "grad_norm": 2.703662633895874,
        "learning_rate": 0.00015890853532370431,
        "epoch": 0.7674386545354287,
        "step": 5567
    },
    {
        "loss": 1.1659,
        "grad_norm": 2.8111777305603027,
        "learning_rate": 0.00015880313339987348,
        "epoch": 0.7675765095119934,
        "step": 5568
    },
    {
        "loss": 2.3459,
        "grad_norm": 1.2949860095977783,
        "learning_rate": 0.00015869763152525683,
        "epoch": 0.767714364488558,
        "step": 5569
    },
    {
        "loss": 2.0287,
        "grad_norm": 1.3421909809112549,
        "learning_rate": 0.0001585920298791817,
        "epoch": 0.7678522194651227,
        "step": 5570
    },
    {
        "loss": 1.7393,
        "grad_norm": 2.315270185470581,
        "learning_rate": 0.0001584863286411445,
        "epoch": 0.7679900744416873,
        "step": 5571
    },
    {
        "loss": 2.1618,
        "grad_norm": 2.0214169025421143,
        "learning_rate": 0.00015838052799081135,
        "epoch": 0.768127929418252,
        "step": 5572
    },
    {
        "loss": 1.3517,
        "grad_norm": 2.1201016902923584,
        "learning_rate": 0.0001582746281080173,
        "epoch": 0.7682657843948166,
        "step": 5573
    },
    {
        "loss": 2.2057,
        "grad_norm": 1.508166790008545,
        "learning_rate": 0.0001581686291727657,
        "epoch": 0.7684036393713813,
        "step": 5574
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.9109349250793457,
        "learning_rate": 0.00015806253136522883,
        "epoch": 0.7685414943479459,
        "step": 5575
    },
    {
        "loss": 2.2245,
        "grad_norm": 2.3517863750457764,
        "learning_rate": 0.00015795633486574654,
        "epoch": 0.7686793493245107,
        "step": 5576
    },
    {
        "loss": 2.4234,
        "grad_norm": 1.3369462490081787,
        "learning_rate": 0.00015785003985482657,
        "epoch": 0.7688172043010753,
        "step": 5577
    },
    {
        "loss": 2.3407,
        "grad_norm": 1.5517475605010986,
        "learning_rate": 0.00015774364651314433,
        "epoch": 0.76895505927764,
        "step": 5578
    },
    {
        "loss": 0.7094,
        "grad_norm": 3.2732579708099365,
        "learning_rate": 0.00015763715502154196,
        "epoch": 0.7690929142542046,
        "step": 5579
    },
    {
        "loss": 2.6865,
        "grad_norm": 1.918203592300415,
        "learning_rate": 0.00015753056556102894,
        "epoch": 0.7692307692307693,
        "step": 5580
    },
    {
        "loss": 2.245,
        "grad_norm": 1.3665030002593994,
        "learning_rate": 0.00015742387831278084,
        "epoch": 0.7693686242073339,
        "step": 5581
    },
    {
        "loss": 1.8512,
        "grad_norm": 1.5602166652679443,
        "learning_rate": 0.00015731709345813947,
        "epoch": 0.7695064791838986,
        "step": 5582
    },
    {
        "loss": 1.0443,
        "grad_norm": 2.2866930961608887,
        "learning_rate": 0.00015721021117861295,
        "epoch": 0.7696443341604632,
        "step": 5583
    },
    {
        "loss": 0.9132,
        "grad_norm": 1.8340392112731934,
        "learning_rate": 0.00015710323165587449,
        "epoch": 0.7697821891370279,
        "step": 5584
    },
    {
        "loss": 1.6165,
        "grad_norm": 2.262967348098755,
        "learning_rate": 0.0001569961550717629,
        "epoch": 0.7699200441135925,
        "step": 5585
    },
    {
        "loss": 2.1084,
        "grad_norm": 1.9293266534805298,
        "learning_rate": 0.00015688898160828204,
        "epoch": 0.7700578990901572,
        "step": 5586
    },
    {
        "loss": 2.3142,
        "grad_norm": 1.8123797178268433,
        "learning_rate": 0.00015678171144760007,
        "epoch": 0.7701957540667218,
        "step": 5587
    },
    {
        "loss": 1.4866,
        "grad_norm": 2.8542535305023193,
        "learning_rate": 0.00015667434477204995,
        "epoch": 0.7703336090432865,
        "step": 5588
    },
    {
        "loss": 1.6391,
        "grad_norm": 2.1734726428985596,
        "learning_rate": 0.00015656688176412837,
        "epoch": 0.7704714640198511,
        "step": 5589
    },
    {
        "loss": 1.8386,
        "grad_norm": 1.691403865814209,
        "learning_rate": 0.00015645932260649579,
        "epoch": 0.7706093189964157,
        "step": 5590
    },
    {
        "loss": 2.1675,
        "grad_norm": 1.884189248085022,
        "learning_rate": 0.00015635166748197623,
        "epoch": 0.7707471739729804,
        "step": 5591
    },
    {
        "loss": 1.9187,
        "grad_norm": 2.458768367767334,
        "learning_rate": 0.00015624391657355694,
        "epoch": 0.770885028949545,
        "step": 5592
    },
    {
        "loss": 1.6443,
        "grad_norm": 2.5855984687805176,
        "learning_rate": 0.0001561360700643877,
        "epoch": 0.7710228839261097,
        "step": 5593
    },
    {
        "loss": 1.882,
        "grad_norm": 2.369112491607666,
        "learning_rate": 0.00015602812813778085,
        "epoch": 0.7711607389026743,
        "step": 5594
    },
    {
        "loss": 1.9731,
        "grad_norm": 1.529314637184143,
        "learning_rate": 0.00015592009097721094,
        "epoch": 0.771298593879239,
        "step": 5595
    },
    {
        "loss": 1.6882,
        "grad_norm": 2.4837849140167236,
        "learning_rate": 0.0001558119587663145,
        "epoch": 0.7714364488558036,
        "step": 5596
    },
    {
        "loss": 1.5921,
        "grad_norm": 2.056993246078491,
        "learning_rate": 0.00015570373168888966,
        "epoch": 0.7715743038323684,
        "step": 5597
    },
    {
        "loss": 1.6476,
        "grad_norm": 2.2480368614196777,
        "learning_rate": 0.0001555954099288955,
        "epoch": 0.771712158808933,
        "step": 5598
    },
    {
        "loss": 1.5098,
        "grad_norm": 2.00148868560791,
        "learning_rate": 0.0001554869936704523,
        "epoch": 0.7718500137854977,
        "step": 5599
    },
    {
        "loss": 2.4955,
        "grad_norm": 1.2115001678466797,
        "learning_rate": 0.00015537848309784104,
        "epoch": 0.7719878687620623,
        "step": 5600
    },
    {
        "loss": 2.3503,
        "grad_norm": 2.1494057178497314,
        "learning_rate": 0.0001552698783955027,
        "epoch": 0.772125723738627,
        "step": 5601
    },
    {
        "loss": 2.3116,
        "grad_norm": 1.792649507522583,
        "learning_rate": 0.00015516117974803843,
        "epoch": 0.7722635787151916,
        "step": 5602
    },
    {
        "loss": 1.274,
        "grad_norm": 2.483635425567627,
        "learning_rate": 0.00015505238734020896,
        "epoch": 0.7724014336917563,
        "step": 5603
    },
    {
        "loss": 1.9492,
        "grad_norm": 2.1925432682037354,
        "learning_rate": 0.00015494350135693456,
        "epoch": 0.7725392886683209,
        "step": 5604
    },
    {
        "loss": 1.8985,
        "grad_norm": 2.455152988433838,
        "learning_rate": 0.00015483452198329466,
        "epoch": 0.7726771436448856,
        "step": 5605
    },
    {
        "loss": 1.5424,
        "grad_norm": 2.5816240310668945,
        "learning_rate": 0.00015472544940452703,
        "epoch": 0.7728149986214502,
        "step": 5606
    },
    {
        "loss": 2.4721,
        "grad_norm": 1.805451512336731,
        "learning_rate": 0.00015461628380602807,
        "epoch": 0.7729528535980149,
        "step": 5607
    },
    {
        "loss": 1.7151,
        "grad_norm": 2.1667563915252686,
        "learning_rate": 0.00015450702537335244,
        "epoch": 0.7730907085745795,
        "step": 5608
    },
    {
        "loss": 2.3149,
        "grad_norm": 1.470564365386963,
        "learning_rate": 0.00015439767429221228,
        "epoch": 0.7732285635511442,
        "step": 5609
    },
    {
        "loss": 1.5345,
        "grad_norm": 2.4285776615142822,
        "learning_rate": 0.00015428823074847752,
        "epoch": 0.7733664185277088,
        "step": 5610
    },
    {
        "loss": 2.5975,
        "grad_norm": 2.382824182510376,
        "learning_rate": 0.00015417869492817519,
        "epoch": 0.7735042735042735,
        "step": 5611
    },
    {
        "loss": 1.2648,
        "grad_norm": 1.723343014717102,
        "learning_rate": 0.00015406906701748887,
        "epoch": 0.7736421284808381,
        "step": 5612
    },
    {
        "loss": 1.4379,
        "grad_norm": 2.833479642868042,
        "learning_rate": 0.00015395934720275917,
        "epoch": 0.7737799834574028,
        "step": 5613
    },
    {
        "loss": 1.5263,
        "grad_norm": 1.6372778415679932,
        "learning_rate": 0.0001538495356704825,
        "epoch": 0.7739178384339674,
        "step": 5614
    },
    {
        "loss": 2.1372,
        "grad_norm": 1.0987095832824707,
        "learning_rate": 0.00015373963260731122,
        "epoch": 0.7740556934105322,
        "step": 5615
    },
    {
        "loss": 2.7829,
        "grad_norm": 1.4613192081451416,
        "learning_rate": 0.00015362963820005353,
        "epoch": 0.7741935483870968,
        "step": 5616
    },
    {
        "loss": 1.6228,
        "grad_norm": 1.8127793073654175,
        "learning_rate": 0.00015351955263567254,
        "epoch": 0.7743314033636615,
        "step": 5617
    },
    {
        "loss": 2.1051,
        "grad_norm": 1.952489972114563,
        "learning_rate": 0.00015340937610128672,
        "epoch": 0.7744692583402261,
        "step": 5618
    },
    {
        "loss": 2.15,
        "grad_norm": 2.3201985359191895,
        "learning_rate": 0.00015329910878416882,
        "epoch": 0.7746071133167908,
        "step": 5619
    },
    {
        "loss": 2.3943,
        "grad_norm": 1.4269490242004395,
        "learning_rate": 0.00015318875087174587,
        "epoch": 0.7747449682933554,
        "step": 5620
    },
    {
        "loss": 2.3707,
        "grad_norm": 1.489020586013794,
        "learning_rate": 0.00015307830255159924,
        "epoch": 0.7748828232699201,
        "step": 5621
    },
    {
        "loss": 1.9118,
        "grad_norm": 1.278061866760254,
        "learning_rate": 0.00015296776401146363,
        "epoch": 0.7750206782464847,
        "step": 5622
    },
    {
        "loss": 1.8914,
        "grad_norm": 1.2884465456008911,
        "learning_rate": 0.00015285713543922727,
        "epoch": 0.7751585332230494,
        "step": 5623
    },
    {
        "loss": 2.0953,
        "grad_norm": 2.024700880050659,
        "learning_rate": 0.00015274641702293148,
        "epoch": 0.775296388199614,
        "step": 5624
    },
    {
        "loss": 0.8157,
        "grad_norm": 1.844387412071228,
        "learning_rate": 0.00015263560895076997,
        "epoch": 0.7754342431761787,
        "step": 5625
    },
    {
        "loss": 1.8838,
        "grad_norm": 1.566173791885376,
        "learning_rate": 0.00015252471141108932,
        "epoch": 0.7755720981527433,
        "step": 5626
    },
    {
        "loss": 1.7448,
        "grad_norm": 1.6745675802230835,
        "learning_rate": 0.00015241372459238775,
        "epoch": 0.775709953129308,
        "step": 5627
    },
    {
        "loss": 2.3438,
        "grad_norm": 2.4499075412750244,
        "learning_rate": 0.00015230264868331533,
        "epoch": 0.7758478081058726,
        "step": 5628
    },
    {
        "loss": 1.4384,
        "grad_norm": 2.216797113418579,
        "learning_rate": 0.0001521914838726738,
        "epoch": 0.7759856630824373,
        "step": 5629
    },
    {
        "loss": 1.147,
        "grad_norm": 3.531628370285034,
        "learning_rate": 0.0001520802303494159,
        "epoch": 0.7761235180590019,
        "step": 5630
    },
    {
        "loss": 2.3379,
        "grad_norm": 1.4815354347229004,
        "learning_rate": 0.00015196888830264498,
        "epoch": 0.7762613730355666,
        "step": 5631
    },
    {
        "loss": 2.3702,
        "grad_norm": 2.175206422805786,
        "learning_rate": 0.00015185745792161505,
        "epoch": 0.7763992280121312,
        "step": 5632
    },
    {
        "loss": 2.2015,
        "grad_norm": 1.9531856775283813,
        "learning_rate": 0.00015174593939573002,
        "epoch": 0.7765370829886958,
        "step": 5633
    },
    {
        "loss": 1.9835,
        "grad_norm": 2.364868640899658,
        "learning_rate": 0.000151634332914544,
        "epoch": 0.7766749379652605,
        "step": 5634
    },
    {
        "loss": 2.1338,
        "grad_norm": 1.491316795349121,
        "learning_rate": 0.00015152263866776043,
        "epoch": 0.7768127929418251,
        "step": 5635
    },
    {
        "loss": 2.104,
        "grad_norm": 2.16290283203125,
        "learning_rate": 0.00015141085684523177,
        "epoch": 0.7769506479183899,
        "step": 5636
    },
    {
        "loss": 1.5843,
        "grad_norm": 2.2505977153778076,
        "learning_rate": 0.00015129898763695953,
        "epoch": 0.7770885028949545,
        "step": 5637
    },
    {
        "loss": 2.3968,
        "grad_norm": 1.2577025890350342,
        "learning_rate": 0.00015118703123309386,
        "epoch": 0.7772263578715192,
        "step": 5638
    },
    {
        "loss": 1.7357,
        "grad_norm": 1.4376423358917236,
        "learning_rate": 0.00015107498782393282,
        "epoch": 0.7773642128480838,
        "step": 5639
    },
    {
        "loss": 2.2837,
        "grad_norm": 1.4598461389541626,
        "learning_rate": 0.00015096285759992246,
        "epoch": 0.7775020678246485,
        "step": 5640
    },
    {
        "loss": 1.8087,
        "grad_norm": 1.8980259895324707,
        "learning_rate": 0.00015085064075165638,
        "epoch": 0.7776399228012131,
        "step": 5641
    },
    {
        "loss": 2.0548,
        "grad_norm": 1.3460543155670166,
        "learning_rate": 0.0001507383374698756,
        "epoch": 0.7777777777777778,
        "step": 5642
    },
    {
        "loss": 1.7255,
        "grad_norm": 2.2841553688049316,
        "learning_rate": 0.000150625947945468,
        "epoch": 0.7779156327543424,
        "step": 5643
    },
    {
        "loss": 2.1566,
        "grad_norm": 2.5152530670166016,
        "learning_rate": 0.00015051347236946786,
        "epoch": 0.7780534877309071,
        "step": 5644
    },
    {
        "loss": 2.6496,
        "grad_norm": 1.5976001024246216,
        "learning_rate": 0.00015040091093305574,
        "epoch": 0.7781913427074717,
        "step": 5645
    },
    {
        "loss": 1.9515,
        "grad_norm": 2.192671298980713,
        "learning_rate": 0.00015028826382755848,
        "epoch": 0.7783291976840364,
        "step": 5646
    },
    {
        "loss": 2.0937,
        "grad_norm": 1.8117868900299072,
        "learning_rate": 0.0001501755312444481,
        "epoch": 0.778467052660601,
        "step": 5647
    },
    {
        "loss": 1.8002,
        "grad_norm": 1.886155366897583,
        "learning_rate": 0.00015006271337534218,
        "epoch": 0.7786049076371657,
        "step": 5648
    },
    {
        "loss": 2.0179,
        "grad_norm": 2.288888454437256,
        "learning_rate": 0.00014994981041200334,
        "epoch": 0.7787427626137303,
        "step": 5649
    },
    {
        "loss": 2.3392,
        "grad_norm": 1.6241320371627808,
        "learning_rate": 0.0001498368225463385,
        "epoch": 0.778880617590295,
        "step": 5650
    },
    {
        "loss": 1.8591,
        "grad_norm": 1.1747372150421143,
        "learning_rate": 0.0001497237499703993,
        "epoch": 0.7790184725668596,
        "step": 5651
    },
    {
        "loss": 2.0094,
        "grad_norm": 1.5779305696487427,
        "learning_rate": 0.00014961059287638102,
        "epoch": 0.7791563275434243,
        "step": 5652
    },
    {
        "loss": 2.1436,
        "grad_norm": 2.1025519371032715,
        "learning_rate": 0.0001494973514566226,
        "epoch": 0.7792941825199889,
        "step": 5653
    },
    {
        "loss": 2.1579,
        "grad_norm": 1.7059985399246216,
        "learning_rate": 0.00014938402590360678,
        "epoch": 0.7794320374965537,
        "step": 5654
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.830578327178955,
        "learning_rate": 0.00014927061640995865,
        "epoch": 0.7795698924731183,
        "step": 5655
    },
    {
        "loss": 1.1248,
        "grad_norm": 2.931699514389038,
        "learning_rate": 0.00014915712316844657,
        "epoch": 0.779707747449683,
        "step": 5656
    },
    {
        "loss": 2.2243,
        "grad_norm": 1.22976815700531,
        "learning_rate": 0.00014904354637198088,
        "epoch": 0.7798456024262476,
        "step": 5657
    },
    {
        "loss": 0.8567,
        "grad_norm": 2.388136148452759,
        "learning_rate": 0.0001489298862136139,
        "epoch": 0.7799834574028123,
        "step": 5658
    },
    {
        "loss": 1.67,
        "grad_norm": 2.0594706535339355,
        "learning_rate": 0.00014881614288654008,
        "epoch": 0.7801213123793769,
        "step": 5659
    },
    {
        "loss": 1.5519,
        "grad_norm": 2.346010208129883,
        "learning_rate": 0.00014870231658409465,
        "epoch": 0.7802591673559416,
        "step": 5660
    },
    {
        "loss": 1.6362,
        "grad_norm": 2.205775737762451,
        "learning_rate": 0.00014858840749975433,
        "epoch": 0.7803970223325062,
        "step": 5661
    },
    {
        "loss": 2.0416,
        "grad_norm": 2.3617193698883057,
        "learning_rate": 0.00014847441582713653,
        "epoch": 0.7805348773090709,
        "step": 5662
    },
    {
        "loss": 1.6993,
        "grad_norm": 1.9781237840652466,
        "learning_rate": 0.00014836034175999863,
        "epoch": 0.7806727322856355,
        "step": 5663
    },
    {
        "loss": 1.9068,
        "grad_norm": 1.7538093328475952,
        "learning_rate": 0.0001482461854922386,
        "epoch": 0.7808105872622002,
        "step": 5664
    },
    {
        "loss": 2.5825,
        "grad_norm": 1.2792582511901855,
        "learning_rate": 0.00014813194721789374,
        "epoch": 0.7809484422387648,
        "step": 5665
    },
    {
        "loss": 2.6689,
        "grad_norm": 1.4795746803283691,
        "learning_rate": 0.00014801762713114076,
        "epoch": 0.7810862972153295,
        "step": 5666
    },
    {
        "loss": 1.0954,
        "grad_norm": 2.7102627754211426,
        "learning_rate": 0.00014790322542629561,
        "epoch": 0.7812241521918941,
        "step": 5667
    },
    {
        "loss": 1.9437,
        "grad_norm": 2.586683988571167,
        "learning_rate": 0.0001477887422978131,
        "epoch": 0.7813620071684588,
        "step": 5668
    },
    {
        "loss": 1.9303,
        "grad_norm": 2.135270357131958,
        "learning_rate": 0.00014767417794028605,
        "epoch": 0.7814998621450234,
        "step": 5669
    },
    {
        "loss": 1.6811,
        "grad_norm": 2.317054271697998,
        "learning_rate": 0.00014755953254844557,
        "epoch": 0.7816377171215881,
        "step": 5670
    },
    {
        "loss": 2.6642,
        "grad_norm": 1.6962157487869263,
        "learning_rate": 0.0001474448063171603,
        "epoch": 0.7817755720981527,
        "step": 5671
    },
    {
        "loss": 1.9875,
        "grad_norm": 2.6258184909820557,
        "learning_rate": 0.00014732999944143667,
        "epoch": 0.7819134270747174,
        "step": 5672
    },
    {
        "loss": 1.2602,
        "grad_norm": 2.4917187690734863,
        "learning_rate": 0.00014721511211641803,
        "epoch": 0.782051282051282,
        "step": 5673
    },
    {
        "loss": 2.0997,
        "grad_norm": 1.4301496744155884,
        "learning_rate": 0.0001471001445373842,
        "epoch": 0.7821891370278468,
        "step": 5674
    },
    {
        "loss": 1.9393,
        "grad_norm": 1.2763988971710205,
        "learning_rate": 0.00014698509689975184,
        "epoch": 0.7823269920044114,
        "step": 5675
    },
    {
        "loss": 2.3274,
        "grad_norm": 1.8320366144180298,
        "learning_rate": 0.00014686996939907346,
        "epoch": 0.782464846980976,
        "step": 5676
    },
    {
        "loss": 1.5596,
        "grad_norm": 2.4300622940063477,
        "learning_rate": 0.0001467547622310373,
        "epoch": 0.7826027019575407,
        "step": 5677
    },
    {
        "loss": 2.0606,
        "grad_norm": 1.9671272039413452,
        "learning_rate": 0.00014663947559146706,
        "epoch": 0.7827405569341053,
        "step": 5678
    },
    {
        "loss": 1.74,
        "grad_norm": 2.547495126724243,
        "learning_rate": 0.0001465241096763214,
        "epoch": 0.78287841191067,
        "step": 5679
    },
    {
        "loss": 2.0856,
        "grad_norm": 1.9680362939834595,
        "learning_rate": 0.00014640866468169393,
        "epoch": 0.7830162668872346,
        "step": 5680
    },
    {
        "loss": 2.2037,
        "grad_norm": 1.4077478647232056,
        "learning_rate": 0.00014629314080381276,
        "epoch": 0.7831541218637993,
        "step": 5681
    },
    {
        "loss": 0.9886,
        "grad_norm": 2.671898365020752,
        "learning_rate": 0.00014617753823903976,
        "epoch": 0.7832919768403639,
        "step": 5682
    },
    {
        "loss": 1.6923,
        "grad_norm": 2.483398914337158,
        "learning_rate": 0.0001460618571838706,
        "epoch": 0.7834298318169286,
        "step": 5683
    },
    {
        "loss": 1.5551,
        "grad_norm": 1.5502816438674927,
        "learning_rate": 0.0001459460978349347,
        "epoch": 0.7835676867934932,
        "step": 5684
    },
    {
        "loss": 1.8459,
        "grad_norm": 1.3770368099212646,
        "learning_rate": 0.0001458302603889941,
        "epoch": 0.7837055417700579,
        "step": 5685
    },
    {
        "loss": 1.8519,
        "grad_norm": 2.3179914951324463,
        "learning_rate": 0.0001457143450429439,
        "epoch": 0.7838433967466225,
        "step": 5686
    },
    {
        "loss": 1.3073,
        "grad_norm": 2.413020133972168,
        "learning_rate": 0.00014559835199381169,
        "epoch": 0.7839812517231872,
        "step": 5687
    },
    {
        "loss": 1.7231,
        "grad_norm": 2.3388900756835938,
        "learning_rate": 0.00014548228143875669,
        "epoch": 0.7841191066997518,
        "step": 5688
    },
    {
        "loss": 1.4284,
        "grad_norm": 2.9372506141662598,
        "learning_rate": 0.00014536613357507046,
        "epoch": 0.7842569616763165,
        "step": 5689
    },
    {
        "loss": 1.866,
        "grad_norm": 1.8344194889068604,
        "learning_rate": 0.0001452499086001754,
        "epoch": 0.7843948166528811,
        "step": 5690
    },
    {
        "loss": 1.0009,
        "grad_norm": 2.755225896835327,
        "learning_rate": 0.0001451336067116252,
        "epoch": 0.7845326716294458,
        "step": 5691
    },
    {
        "loss": 2.2164,
        "grad_norm": 2.4914584159851074,
        "learning_rate": 0.00014501722810710455,
        "epoch": 0.7846705266060104,
        "step": 5692
    },
    {
        "loss": 2.103,
        "grad_norm": 2.1245484352111816,
        "learning_rate": 0.00014490077298442798,
        "epoch": 0.7848083815825752,
        "step": 5693
    },
    {
        "loss": 2.163,
        "grad_norm": 1.7297487258911133,
        "learning_rate": 0.00014478424154154074,
        "epoch": 0.7849462365591398,
        "step": 5694
    },
    {
        "loss": 1.4384,
        "grad_norm": 2.6659042835235596,
        "learning_rate": 0.00014466763397651727,
        "epoch": 0.7850840915357045,
        "step": 5695
    },
    {
        "loss": 2.0044,
        "grad_norm": 1.2872661352157593,
        "learning_rate": 0.00014455095048756153,
        "epoch": 0.7852219465122691,
        "step": 5696
    },
    {
        "loss": 2.4094,
        "grad_norm": 1.414473295211792,
        "learning_rate": 0.0001444341912730067,
        "epoch": 0.7853598014888338,
        "step": 5697
    },
    {
        "loss": 2.1226,
        "grad_norm": 1.7438607215881348,
        "learning_rate": 0.00014431735653131465,
        "epoch": 0.7854976564653984,
        "step": 5698
    },
    {
        "loss": 1.8547,
        "grad_norm": 1.5986530780792236,
        "learning_rate": 0.0001442004464610754,
        "epoch": 0.7856355114419631,
        "step": 5699
    },
    {
        "loss": 1.4288,
        "grad_norm": 2.0622456073760986,
        "learning_rate": 0.00014408346126100732,
        "epoch": 0.7857733664185277,
        "step": 5700
    },
    {
        "loss": 2.1785,
        "grad_norm": 2.576136589050293,
        "learning_rate": 0.00014396640112995608,
        "epoch": 0.7859112213950924,
        "step": 5701
    },
    {
        "loss": 2.4341,
        "grad_norm": 1.4290521144866943,
        "learning_rate": 0.0001438492662668953,
        "epoch": 0.786049076371657,
        "step": 5702
    },
    {
        "loss": 2.2249,
        "grad_norm": 1.8396190404891968,
        "learning_rate": 0.000143732056870925,
        "epoch": 0.7861869313482217,
        "step": 5703
    },
    {
        "loss": 1.9521,
        "grad_norm": 1.634744644165039,
        "learning_rate": 0.00014361477314127217,
        "epoch": 0.7863247863247863,
        "step": 5704
    },
    {
        "loss": 2.4802,
        "grad_norm": 1.8171635866165161,
        "learning_rate": 0.0001434974152772902,
        "epoch": 0.786462641301351,
        "step": 5705
    },
    {
        "loss": 1.79,
        "grad_norm": 1.3585450649261475,
        "learning_rate": 0.00014337998347845865,
        "epoch": 0.7866004962779156,
        "step": 5706
    },
    {
        "loss": 2.5164,
        "grad_norm": 1.1639559268951416,
        "learning_rate": 0.00014326247794438232,
        "epoch": 0.7867383512544803,
        "step": 5707
    },
    {
        "loss": 1.2363,
        "grad_norm": 2.3929808139801025,
        "learning_rate": 0.00014314489887479163,
        "epoch": 0.7868762062310449,
        "step": 5708
    },
    {
        "loss": 1.699,
        "grad_norm": 2.0757603645324707,
        "learning_rate": 0.00014302724646954174,
        "epoch": 0.7870140612076096,
        "step": 5709
    },
    {
        "loss": 2.0131,
        "grad_norm": 2.0449538230895996,
        "learning_rate": 0.0001429095209286128,
        "epoch": 0.7871519161841742,
        "step": 5710
    },
    {
        "loss": 2.0621,
        "grad_norm": 1.4498813152313232,
        "learning_rate": 0.00014279172245210925,
        "epoch": 0.787289771160739,
        "step": 5711
    },
    {
        "loss": 2.0853,
        "grad_norm": 1.2589010000228882,
        "learning_rate": 0.0001426738512402591,
        "epoch": 0.7874276261373035,
        "step": 5712
    },
    {
        "loss": 1.6646,
        "grad_norm": 1.9691380262374878,
        "learning_rate": 0.00014255590749341442,
        "epoch": 0.7875654811138683,
        "step": 5713
    },
    {
        "loss": 2.0158,
        "grad_norm": 1.7945835590362549,
        "learning_rate": 0.00014243789141205052,
        "epoch": 0.7877033360904329,
        "step": 5714
    },
    {
        "loss": 1.425,
        "grad_norm": 1.5568208694458008,
        "learning_rate": 0.00014231980319676538,
        "epoch": 0.7878411910669976,
        "step": 5715
    },
    {
        "loss": 1.3853,
        "grad_norm": 2.4770281314849854,
        "learning_rate": 0.00014220164304827975,
        "epoch": 0.7879790460435622,
        "step": 5716
    },
    {
        "loss": 2.4036,
        "grad_norm": 1.5547940731048584,
        "learning_rate": 0.0001420834111674368,
        "epoch": 0.7881169010201269,
        "step": 5717
    },
    {
        "loss": 1.998,
        "grad_norm": 1.2203480005264282,
        "learning_rate": 0.00014196510775520133,
        "epoch": 0.7882547559966915,
        "step": 5718
    },
    {
        "loss": 1.8083,
        "grad_norm": 1.551721453666687,
        "learning_rate": 0.00014184673301266006,
        "epoch": 0.7883926109732561,
        "step": 5719
    },
    {
        "loss": 2.298,
        "grad_norm": 1.7254377603530884,
        "learning_rate": 0.00014172828714102074,
        "epoch": 0.7885304659498208,
        "step": 5720
    },
    {
        "loss": 1.5118,
        "grad_norm": 1.8772624731063843,
        "learning_rate": 0.00014160977034161185,
        "epoch": 0.7886683209263854,
        "step": 5721
    },
    {
        "loss": 1.4911,
        "grad_norm": 2.5398483276367188,
        "learning_rate": 0.00014149118281588292,
        "epoch": 0.7888061759029501,
        "step": 5722
    },
    {
        "loss": 1.9019,
        "grad_norm": 2.7818386554718018,
        "learning_rate": 0.00014137252476540324,
        "epoch": 0.7889440308795147,
        "step": 5723
    },
    {
        "loss": 1.151,
        "grad_norm": 2.841493844985962,
        "learning_rate": 0.00014125379639186216,
        "epoch": 0.7890818858560794,
        "step": 5724
    },
    {
        "loss": 1.1316,
        "grad_norm": 2.5464484691619873,
        "learning_rate": 0.00014113499789706878,
        "epoch": 0.789219740832644,
        "step": 5725
    },
    {
        "loss": 2.014,
        "grad_norm": 2.1126370429992676,
        "learning_rate": 0.00014101612948295083,
        "epoch": 0.7893575958092087,
        "step": 5726
    },
    {
        "loss": 1.9959,
        "grad_norm": 1.1884706020355225,
        "learning_rate": 0.00014089719135155555,
        "epoch": 0.7894954507857733,
        "step": 5727
    },
    {
        "loss": 1.6908,
        "grad_norm": 2.252633571624756,
        "learning_rate": 0.00014077818370504818,
        "epoch": 0.789633305762338,
        "step": 5728
    },
    {
        "loss": 1.9169,
        "grad_norm": 2.2519376277923584,
        "learning_rate": 0.00014065910674571216,
        "epoch": 0.7897711607389026,
        "step": 5729
    },
    {
        "loss": 2.4772,
        "grad_norm": 2.1283130645751953,
        "learning_rate": 0.00014053996067594915,
        "epoch": 0.7899090157154673,
        "step": 5730
    },
    {
        "loss": 1.8018,
        "grad_norm": 2.403319835662842,
        "learning_rate": 0.00014042074569827777,
        "epoch": 0.7900468706920319,
        "step": 5731
    },
    {
        "loss": 2.0166,
        "grad_norm": 1.758263111114502,
        "learning_rate": 0.00014030146201533418,
        "epoch": 0.7901847256685967,
        "step": 5732
    },
    {
        "loss": 2.0494,
        "grad_norm": 1.7973179817199707,
        "learning_rate": 0.00014018210982987112,
        "epoch": 0.7903225806451613,
        "step": 5733
    },
    {
        "loss": 1.6566,
        "grad_norm": 1.912711262702942,
        "learning_rate": 0.0001400626893447576,
        "epoch": 0.790460435621726,
        "step": 5734
    },
    {
        "loss": 2.033,
        "grad_norm": 1.6628016233444214,
        "learning_rate": 0.00013994320076297903,
        "epoch": 0.7905982905982906,
        "step": 5735
    },
    {
        "loss": 2.1244,
        "grad_norm": 2.317387819290161,
        "learning_rate": 0.0001398236442876366,
        "epoch": 0.7907361455748553,
        "step": 5736
    },
    {
        "loss": 1.0324,
        "grad_norm": 2.556626081466675,
        "learning_rate": 0.00013970402012194655,
        "epoch": 0.7908740005514199,
        "step": 5737
    },
    {
        "loss": 1.5723,
        "grad_norm": 1.7077027559280396,
        "learning_rate": 0.00013958432846924052,
        "epoch": 0.7910118555279846,
        "step": 5738
    },
    {
        "loss": 2.2039,
        "grad_norm": 2.027291774749756,
        "learning_rate": 0.00013946456953296455,
        "epoch": 0.7911497105045492,
        "step": 5739
    },
    {
        "loss": 2.5527,
        "grad_norm": 1.3637923002243042,
        "learning_rate": 0.0001393447435166795,
        "epoch": 0.7912875654811139,
        "step": 5740
    },
    {
        "loss": 1.2828,
        "grad_norm": 3.5494041442871094,
        "learning_rate": 0.00013922485062405974,
        "epoch": 0.7914254204576785,
        "step": 5741
    },
    {
        "loss": 2.043,
        "grad_norm": 2.8870534896850586,
        "learning_rate": 0.0001391048910588935,
        "epoch": 0.7915632754342432,
        "step": 5742
    },
    {
        "loss": 2.3283,
        "grad_norm": 1.816846251487732,
        "learning_rate": 0.00013898486502508244,
        "epoch": 0.7917011304108078,
        "step": 5743
    },
    {
        "loss": 2.2286,
        "grad_norm": 1.4960542917251587,
        "learning_rate": 0.00013886477272664134,
        "epoch": 0.7918389853873725,
        "step": 5744
    },
    {
        "loss": 1.6237,
        "grad_norm": 2.188640832901001,
        "learning_rate": 0.00013874461436769718,
        "epoch": 0.7919768403639371,
        "step": 5745
    },
    {
        "loss": 1.627,
        "grad_norm": 2.0954549312591553,
        "learning_rate": 0.00013862439015248956,
        "epoch": 0.7921146953405018,
        "step": 5746
    },
    {
        "loss": 1.9968,
        "grad_norm": 1.289538025856018,
        "learning_rate": 0.00013850410028536976,
        "epoch": 0.7922525503170664,
        "step": 5747
    },
    {
        "loss": 1.716,
        "grad_norm": 2.1172502040863037,
        "learning_rate": 0.00013838374497080087,
        "epoch": 0.7923904052936311,
        "step": 5748
    },
    {
        "loss": 1.8332,
        "grad_norm": 1.981358528137207,
        "learning_rate": 0.00013826332441335733,
        "epoch": 0.7925282602701957,
        "step": 5749
    },
    {
        "loss": 1.6818,
        "grad_norm": 2.5088675022125244,
        "learning_rate": 0.0001381428388177241,
        "epoch": 0.7926661152467604,
        "step": 5750
    },
    {
        "loss": 1.865,
        "grad_norm": 2.3412461280822754,
        "learning_rate": 0.000138022288388697,
        "epoch": 0.792803970223325,
        "step": 5751
    },
    {
        "loss": 2.704,
        "grad_norm": 1.3164838552474976,
        "learning_rate": 0.00013790167333118208,
        "epoch": 0.7929418251998898,
        "step": 5752
    },
    {
        "loss": 2.3961,
        "grad_norm": 1.2763391733169556,
        "learning_rate": 0.00013778099385019493,
        "epoch": 0.7930796801764544,
        "step": 5753
    },
    {
        "loss": 1.1264,
        "grad_norm": 2.450453996658325,
        "learning_rate": 0.00013766025015086073,
        "epoch": 0.7932175351530191,
        "step": 5754
    },
    {
        "loss": 2.0422,
        "grad_norm": 1.7308987379074097,
        "learning_rate": 0.00013753944243841422,
        "epoch": 0.7933553901295837,
        "step": 5755
    },
    {
        "loss": 1.6461,
        "grad_norm": 2.385554075241089,
        "learning_rate": 0.0001374185709181983,
        "epoch": 0.7934932451061484,
        "step": 5756
    },
    {
        "loss": 1.8077,
        "grad_norm": 1.8961275815963745,
        "learning_rate": 0.00013729763579566496,
        "epoch": 0.793631100082713,
        "step": 5757
    },
    {
        "loss": 2.0413,
        "grad_norm": 2.279458522796631,
        "learning_rate": 0.0001371766372763739,
        "epoch": 0.7937689550592777,
        "step": 5758
    },
    {
        "loss": 1.2751,
        "grad_norm": 1.7139432430267334,
        "learning_rate": 0.0001370555755659925,
        "epoch": 0.7939068100358423,
        "step": 5759
    },
    {
        "loss": 2.2431,
        "grad_norm": 2.0833945274353027,
        "learning_rate": 0.00013693445087029603,
        "epoch": 0.794044665012407,
        "step": 5760
    },
    {
        "loss": 1.8906,
        "grad_norm": 1.9341349601745605,
        "learning_rate": 0.00013681326339516628,
        "epoch": 0.7941825199889716,
        "step": 5761
    },
    {
        "loss": 0.8176,
        "grad_norm": 2.6188857555389404,
        "learning_rate": 0.00013669201334659206,
        "epoch": 0.7943203749655362,
        "step": 5762
    },
    {
        "loss": 2.0001,
        "grad_norm": 1.9395670890808105,
        "learning_rate": 0.0001365707009306686,
        "epoch": 0.7944582299421009,
        "step": 5763
    },
    {
        "loss": 2.0023,
        "grad_norm": 2.249474287033081,
        "learning_rate": 0.00013644932635359678,
        "epoch": 0.7945960849186655,
        "step": 5764
    },
    {
        "loss": 1.6057,
        "grad_norm": 2.031571388244629,
        "learning_rate": 0.00013632788982168362,
        "epoch": 0.7947339398952302,
        "step": 5765
    },
    {
        "loss": 1.6485,
        "grad_norm": 2.1042373180389404,
        "learning_rate": 0.0001362063915413411,
        "epoch": 0.7948717948717948,
        "step": 5766
    },
    {
        "loss": 2.2555,
        "grad_norm": 1.3296915292739868,
        "learning_rate": 0.00013608483171908607,
        "epoch": 0.7950096498483595,
        "step": 5767
    },
    {
        "loss": 2.4354,
        "grad_norm": 1.120603322982788,
        "learning_rate": 0.00013596321056154044,
        "epoch": 0.7951475048249241,
        "step": 5768
    },
    {
        "loss": 2.2502,
        "grad_norm": 2.0962488651275635,
        "learning_rate": 0.00013584152827542985,
        "epoch": 0.7952853598014888,
        "step": 5769
    },
    {
        "loss": 2.2974,
        "grad_norm": 1.2513258457183838,
        "learning_rate": 0.00013571978506758434,
        "epoch": 0.7954232147780534,
        "step": 5770
    },
    {
        "loss": 2.3968,
        "grad_norm": 2.4235408306121826,
        "learning_rate": 0.00013559798114493718,
        "epoch": 0.7955610697546182,
        "step": 5771
    },
    {
        "loss": 1.3266,
        "grad_norm": 2.1135733127593994,
        "learning_rate": 0.00013547611671452472,
        "epoch": 0.7956989247311828,
        "step": 5772
    },
    {
        "loss": 1.5998,
        "grad_norm": 1.8102174997329712,
        "learning_rate": 0.00013535419198348653,
        "epoch": 0.7958367797077475,
        "step": 5773
    },
    {
        "loss": 1.485,
        "grad_norm": 2.0325355529785156,
        "learning_rate": 0.0001352322071590646,
        "epoch": 0.7959746346843121,
        "step": 5774
    },
    {
        "loss": 2.5248,
        "grad_norm": 1.9972811937332153,
        "learning_rate": 0.00013511016244860275,
        "epoch": 0.7961124896608768,
        "step": 5775
    },
    {
        "loss": 1.0529,
        "grad_norm": 3.3147425651550293,
        "learning_rate": 0.00013498805805954705,
        "epoch": 0.7962503446374414,
        "step": 5776
    },
    {
        "loss": 2.4284,
        "grad_norm": 1.4968513250350952,
        "learning_rate": 0.00013486589419944451,
        "epoch": 0.7963881996140061,
        "step": 5777
    },
    {
        "loss": 2.1226,
        "grad_norm": 2.3206334114074707,
        "learning_rate": 0.00013474367107594372,
        "epoch": 0.7965260545905707,
        "step": 5778
    },
    {
        "loss": 1.6184,
        "grad_norm": 2.375783920288086,
        "learning_rate": 0.00013462138889679364,
        "epoch": 0.7966639095671354,
        "step": 5779
    },
    {
        "loss": 1.7898,
        "grad_norm": 1.6823177337646484,
        "learning_rate": 0.00013449904786984367,
        "epoch": 0.7968017645437,
        "step": 5780
    },
    {
        "loss": 1.0698,
        "grad_norm": 2.3332509994506836,
        "learning_rate": 0.0001343766482030433,
        "epoch": 0.7969396195202647,
        "step": 5781
    },
    {
        "loss": 1.3269,
        "grad_norm": 2.208017587661743,
        "learning_rate": 0.00013425419010444197,
        "epoch": 0.7970774744968293,
        "step": 5782
    },
    {
        "loss": 2.5362,
        "grad_norm": 1.2076441049575806,
        "learning_rate": 0.00013413167378218796,
        "epoch": 0.797215329473394,
        "step": 5783
    },
    {
        "loss": 1.7034,
        "grad_norm": 2.2248008251190186,
        "learning_rate": 0.00013400909944452874,
        "epoch": 0.7973531844499586,
        "step": 5784
    },
    {
        "loss": 2.2865,
        "grad_norm": 2.1272003650665283,
        "learning_rate": 0.00013388646729981023,
        "epoch": 0.7974910394265233,
        "step": 5785
    },
    {
        "loss": 1.9821,
        "grad_norm": 1.9106923341751099,
        "learning_rate": 0.0001337637775564769,
        "epoch": 0.7976288944030879,
        "step": 5786
    },
    {
        "loss": 1.7851,
        "grad_norm": 1.724881649017334,
        "learning_rate": 0.00013364103042307106,
        "epoch": 0.7977667493796526,
        "step": 5787
    },
    {
        "loss": 1.8629,
        "grad_norm": 2.6147191524505615,
        "learning_rate": 0.00013351822610823227,
        "epoch": 0.7979046043562172,
        "step": 5788
    },
    {
        "loss": 1.5493,
        "grad_norm": 2.189356565475464,
        "learning_rate": 0.0001333953648206976,
        "epoch": 0.798042459332782,
        "step": 5789
    },
    {
        "loss": 1.6431,
        "grad_norm": 3.353602886199951,
        "learning_rate": 0.00013327244676930103,
        "epoch": 0.7981803143093466,
        "step": 5790
    },
    {
        "loss": 1.7943,
        "grad_norm": 2.978363037109375,
        "learning_rate": 0.0001331494721629727,
        "epoch": 0.7983181692859113,
        "step": 5791
    },
    {
        "loss": 2.0181,
        "grad_norm": 1.5742130279541016,
        "learning_rate": 0.00013302644121073888,
        "epoch": 0.7984560242624759,
        "step": 5792
    },
    {
        "loss": 2.1187,
        "grad_norm": 2.1730077266693115,
        "learning_rate": 0.00013290335412172204,
        "epoch": 0.7985938792390406,
        "step": 5793
    },
    {
        "loss": 2.0301,
        "grad_norm": 1.9063640832901,
        "learning_rate": 0.00013278021110513954,
        "epoch": 0.7987317342156052,
        "step": 5794
    },
    {
        "loss": 1.8386,
        "grad_norm": 2.1249799728393555,
        "learning_rate": 0.00013265701237030427,
        "epoch": 0.7988695891921699,
        "step": 5795
    },
    {
        "loss": 2.0966,
        "grad_norm": 1.415987491607666,
        "learning_rate": 0.0001325337581266236,
        "epoch": 0.7990074441687345,
        "step": 5796
    },
    {
        "loss": 2.3323,
        "grad_norm": 2.2654433250427246,
        "learning_rate": 0.000132410448583599,
        "epoch": 0.7991452991452992,
        "step": 5797
    },
    {
        "loss": 1.7854,
        "grad_norm": 4.287849426269531,
        "learning_rate": 0.00013228708395082654,
        "epoch": 0.7992831541218638,
        "step": 5798
    },
    {
        "loss": 1.7132,
        "grad_norm": 2.097919464111328,
        "learning_rate": 0.0001321636644379953,
        "epoch": 0.7994210090984285,
        "step": 5799
    },
    {
        "loss": 1.7414,
        "grad_norm": 3.3900482654571533,
        "learning_rate": 0.0001320401902548881,
        "epoch": 0.7995588640749931,
        "step": 5800
    },
    {
        "loss": 2.0142,
        "grad_norm": 1.502451777458191,
        "learning_rate": 0.00013191666161138058,
        "epoch": 0.7996967190515578,
        "step": 5801
    },
    {
        "loss": 1.4694,
        "grad_norm": 1.533700704574585,
        "learning_rate": 0.0001317930787174407,
        "epoch": 0.7998345740281224,
        "step": 5802
    },
    {
        "loss": 1.5345,
        "grad_norm": 3.1728806495666504,
        "learning_rate": 0.00013166944178312904,
        "epoch": 0.7999724290046871,
        "step": 5803
    },
    {
        "loss": 1.8691,
        "grad_norm": 2.2837107181549072,
        "learning_rate": 0.0001315457510185977,
        "epoch": 0.8001102839812517,
        "step": 5804
    },
    {
        "loss": 1.7521,
        "grad_norm": 2.3289544582366943,
        "learning_rate": 0.00013142200663409024,
        "epoch": 0.8002481389578163,
        "step": 5805
    },
    {
        "loss": 2.1332,
        "grad_norm": 1.4228726625442505,
        "learning_rate": 0.0001312982088399418,
        "epoch": 0.800385993934381,
        "step": 5806
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.3579199314117432,
        "learning_rate": 0.00013117435784657772,
        "epoch": 0.8005238489109456,
        "step": 5807
    },
    {
        "loss": 2.2751,
        "grad_norm": 1.047024130821228,
        "learning_rate": 0.00013105045386451435,
        "epoch": 0.8006617038875103,
        "step": 5808
    },
    {
        "loss": 2.4557,
        "grad_norm": 1.800242304801941,
        "learning_rate": 0.00013092649710435774,
        "epoch": 0.800799558864075,
        "step": 5809
    },
    {
        "loss": 2.0619,
        "grad_norm": 2.1204559803009033,
        "learning_rate": 0.0001308024877768035,
        "epoch": 0.8009374138406397,
        "step": 5810
    },
    {
        "loss": 1.8459,
        "grad_norm": 2.626762866973877,
        "learning_rate": 0.00013067842609263702,
        "epoch": 0.8010752688172043,
        "step": 5811
    },
    {
        "loss": 2.3665,
        "grad_norm": 2.408032178878784,
        "learning_rate": 0.00013055431226273258,
        "epoch": 0.801213123793769,
        "step": 5812
    },
    {
        "loss": 1.7781,
        "grad_norm": 2.593337297439575,
        "learning_rate": 0.00013043014649805274,
        "epoch": 0.8013509787703336,
        "step": 5813
    },
    {
        "loss": 1.2478,
        "grad_norm": 1.7992513179779053,
        "learning_rate": 0.0001303059290096489,
        "epoch": 0.8014888337468983,
        "step": 5814
    },
    {
        "loss": 1.9103,
        "grad_norm": 1.913511037826538,
        "learning_rate": 0.00013018166000865978,
        "epoch": 0.8016266887234629,
        "step": 5815
    },
    {
        "loss": 1.4454,
        "grad_norm": 2.3046858310699463,
        "learning_rate": 0.0001300573397063122,
        "epoch": 0.8017645437000276,
        "step": 5816
    },
    {
        "loss": 2.0821,
        "grad_norm": 2.481773853302002,
        "learning_rate": 0.00012993296831391983,
        "epoch": 0.8019023986765922,
        "step": 5817
    },
    {
        "loss": 2.2088,
        "grad_norm": 1.6539180278778076,
        "learning_rate": 0.00012980854604288314,
        "epoch": 0.8020402536531569,
        "step": 5818
    },
    {
        "loss": 1.5367,
        "grad_norm": 2.026554822921753,
        "learning_rate": 0.00012968407310468932,
        "epoch": 0.8021781086297215,
        "step": 5819
    },
    {
        "loss": 2.3633,
        "grad_norm": 1.6902754306793213,
        "learning_rate": 0.00012955954971091176,
        "epoch": 0.8023159636062862,
        "step": 5820
    },
    {
        "loss": 1.5584,
        "grad_norm": 2.2999722957611084,
        "learning_rate": 0.0001294349760732093,
        "epoch": 0.8024538185828508,
        "step": 5821
    },
    {
        "loss": 1.7659,
        "grad_norm": 1.176668643951416,
        "learning_rate": 0.00012931035240332632,
        "epoch": 0.8025916735594155,
        "step": 5822
    },
    {
        "loss": 2.5007,
        "grad_norm": 2.024003267288208,
        "learning_rate": 0.0001291856789130921,
        "epoch": 0.8027295285359801,
        "step": 5823
    },
    {
        "loss": 2.0197,
        "grad_norm": 1.3197776079177856,
        "learning_rate": 0.00012906095581442087,
        "epoch": 0.8028673835125448,
        "step": 5824
    },
    {
        "loss": 1.7738,
        "grad_norm": 2.3929555416107178,
        "learning_rate": 0.0001289361833193112,
        "epoch": 0.8030052384891094,
        "step": 5825
    },
    {
        "loss": 0.9557,
        "grad_norm": 2.812640428543091,
        "learning_rate": 0.00012881136163984518,
        "epoch": 0.8031430934656741,
        "step": 5826
    },
    {
        "loss": 1.2678,
        "grad_norm": 2.9382922649383545,
        "learning_rate": 0.000128686490988189,
        "epoch": 0.8032809484422387,
        "step": 5827
    },
    {
        "loss": 2.0074,
        "grad_norm": 1.5025914907455444,
        "learning_rate": 0.00012856157157659198,
        "epoch": 0.8034188034188035,
        "step": 5828
    },
    {
        "loss": 2.1932,
        "grad_norm": 1.1635594367980957,
        "learning_rate": 0.00012843660361738608,
        "epoch": 0.803556658395368,
        "step": 5829
    },
    {
        "loss": 1.6619,
        "grad_norm": 3.373674154281616,
        "learning_rate": 0.00012831158732298578,
        "epoch": 0.8036945133719328,
        "step": 5830
    },
    {
        "loss": 2.3093,
        "grad_norm": 1.9267165660858154,
        "learning_rate": 0.0001281865229058882,
        "epoch": 0.8038323683484974,
        "step": 5831
    },
    {
        "loss": 1.8327,
        "grad_norm": 2.597008466720581,
        "learning_rate": 0.00012806141057867163,
        "epoch": 0.8039702233250621,
        "step": 5832
    },
    {
        "loss": 1.7606,
        "grad_norm": 2.250166177749634,
        "learning_rate": 0.00012793625055399635,
        "epoch": 0.8041080783016267,
        "step": 5833
    },
    {
        "loss": 2.17,
        "grad_norm": 2.0973446369171143,
        "learning_rate": 0.0001278110430446033,
        "epoch": 0.8042459332781914,
        "step": 5834
    },
    {
        "loss": 2.0569,
        "grad_norm": 2.750886917114258,
        "learning_rate": 0.00012768578826331415,
        "epoch": 0.804383788254756,
        "step": 5835
    },
    {
        "loss": 1.3887,
        "grad_norm": 2.2795157432556152,
        "learning_rate": 0.00012756048642303134,
        "epoch": 0.8045216432313207,
        "step": 5836
    },
    {
        "loss": 1.5657,
        "grad_norm": 2.516493082046509,
        "learning_rate": 0.00012743513773673675,
        "epoch": 0.8046594982078853,
        "step": 5837
    },
    {
        "loss": 2.1618,
        "grad_norm": 2.2249720096588135,
        "learning_rate": 0.00012730974241749227,
        "epoch": 0.80479735318445,
        "step": 5838
    },
    {
        "loss": 2.2604,
        "grad_norm": 2.180936336517334,
        "learning_rate": 0.0001271843006784391,
        "epoch": 0.8049352081610146,
        "step": 5839
    },
    {
        "loss": 1.7205,
        "grad_norm": 2.906069755554199,
        "learning_rate": 0.00012705881273279685,
        "epoch": 0.8050730631375793,
        "step": 5840
    },
    {
        "loss": 1.5715,
        "grad_norm": 2.537135601043701,
        "learning_rate": 0.00012693327879386432,
        "epoch": 0.8052109181141439,
        "step": 5841
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.7595049142837524,
        "learning_rate": 0.000126807699075018,
        "epoch": 0.8053487730907086,
        "step": 5842
    },
    {
        "loss": 1.3982,
        "grad_norm": 2.2369959354400635,
        "learning_rate": 0.00012668207378971222,
        "epoch": 0.8054866280672732,
        "step": 5843
    },
    {
        "loss": 2.1206,
        "grad_norm": 2.4487242698669434,
        "learning_rate": 0.00012655640315147916,
        "epoch": 0.8056244830438379,
        "step": 5844
    },
    {
        "loss": 1.5394,
        "grad_norm": 2.6516900062561035,
        "learning_rate": 0.00012643068737392763,
        "epoch": 0.8057623380204025,
        "step": 5845
    },
    {
        "loss": 2.1632,
        "grad_norm": 2.501528024673462,
        "learning_rate": 0.00012630492667074357,
        "epoch": 0.8059001929969671,
        "step": 5846
    },
    {
        "loss": 2.0002,
        "grad_norm": 1.2179628610610962,
        "learning_rate": 0.00012617912125568892,
        "epoch": 0.8060380479735318,
        "step": 5847
    },
    {
        "loss": 2.0382,
        "grad_norm": 2.487496852874756,
        "learning_rate": 0.00012605327134260173,
        "epoch": 0.8061759029500964,
        "step": 5848
    },
    {
        "loss": 2.3152,
        "grad_norm": 1.5123252868652344,
        "learning_rate": 0.00012592737714539577,
        "epoch": 0.8063137579266612,
        "step": 5849
    },
    {
        "loss": 2.2771,
        "grad_norm": 2.162415027618408,
        "learning_rate": 0.0001258014388780602,
        "epoch": 0.8064516129032258,
        "step": 5850
    },
    {
        "loss": 1.4269,
        "grad_norm": 1.719827651977539,
        "learning_rate": 0.00012567545675465876,
        "epoch": 0.8065894678797905,
        "step": 5851
    },
    {
        "loss": 1.7969,
        "grad_norm": 2.6406490802764893,
        "learning_rate": 0.00012554943098933005,
        "epoch": 0.8067273228563551,
        "step": 5852
    },
    {
        "loss": 2.161,
        "grad_norm": 1.7265266180038452,
        "learning_rate": 0.0001254233617962865,
        "epoch": 0.8068651778329198,
        "step": 5853
    },
    {
        "loss": 2.5467,
        "grad_norm": 1.115370273590088,
        "learning_rate": 0.0001252972493898148,
        "epoch": 0.8070030328094844,
        "step": 5854
    },
    {
        "loss": 1.7086,
        "grad_norm": 2.395618200302124,
        "learning_rate": 0.00012517109398427477,
        "epoch": 0.8071408877860491,
        "step": 5855
    },
    {
        "loss": 0.9575,
        "grad_norm": 1.3592461347579956,
        "learning_rate": 0.0001250448957940992,
        "epoch": 0.8072787427626137,
        "step": 5856
    },
    {
        "loss": 1.4342,
        "grad_norm": 1.8928004503250122,
        "learning_rate": 0.000124918655033794,
        "epoch": 0.8074165977391784,
        "step": 5857
    },
    {
        "loss": 1.1877,
        "grad_norm": 2.2855916023254395,
        "learning_rate": 0.00012479237191793737,
        "epoch": 0.807554452715743,
        "step": 5858
    },
    {
        "loss": 1.6768,
        "grad_norm": 2.563711404800415,
        "learning_rate": 0.00012466604666117924,
        "epoch": 0.8076923076923077,
        "step": 5859
    },
    {
        "loss": 1.6286,
        "grad_norm": 2.238638401031494,
        "learning_rate": 0.00012453967947824118,
        "epoch": 0.8078301626688723,
        "step": 5860
    },
    {
        "loss": 1.3955,
        "grad_norm": 2.0816729068756104,
        "learning_rate": 0.00012441327058391636,
        "epoch": 0.807968017645437,
        "step": 5861
    },
    {
        "loss": 2.2561,
        "grad_norm": 2.130143165588379,
        "learning_rate": 0.00012428682019306845,
        "epoch": 0.8081058726220016,
        "step": 5862
    },
    {
        "loss": 2.1373,
        "grad_norm": 1.7007008790969849,
        "learning_rate": 0.0001241603285206321,
        "epoch": 0.8082437275985663,
        "step": 5863
    },
    {
        "loss": 1.7151,
        "grad_norm": 2.153329372406006,
        "learning_rate": 0.00012403379578161157,
        "epoch": 0.8083815825751309,
        "step": 5864
    },
    {
        "loss": 1.1544,
        "grad_norm": 1.6341285705566406,
        "learning_rate": 0.00012390722219108138,
        "epoch": 0.8085194375516956,
        "step": 5865
    },
    {
        "loss": 2.2211,
        "grad_norm": 1.1833645105361938,
        "learning_rate": 0.0001237806079641855,
        "epoch": 0.8086572925282602,
        "step": 5866
    },
    {
        "loss": 2.2002,
        "grad_norm": 1.4273806810379028,
        "learning_rate": 0.0001236539533161366,
        "epoch": 0.808795147504825,
        "step": 5867
    },
    {
        "loss": 1.4026,
        "grad_norm": 1.9380863904953003,
        "learning_rate": 0.0001235272584622162,
        "epoch": 0.8089330024813896,
        "step": 5868
    },
    {
        "loss": 1.8587,
        "grad_norm": 2.1591954231262207,
        "learning_rate": 0.00012340052361777446,
        "epoch": 0.8090708574579543,
        "step": 5869
    },
    {
        "loss": 1.2978,
        "grad_norm": 2.1341195106506348,
        "learning_rate": 0.000123273748998229,
        "epoch": 0.8092087124345189,
        "step": 5870
    },
    {
        "loss": 1.904,
        "grad_norm": 2.544917106628418,
        "learning_rate": 0.00012314693481906558,
        "epoch": 0.8093465674110836,
        "step": 5871
    },
    {
        "loss": 1.9191,
        "grad_norm": 1.2881406545639038,
        "learning_rate": 0.00012302008129583687,
        "epoch": 0.8094844223876482,
        "step": 5872
    },
    {
        "loss": 1.7687,
        "grad_norm": 2.449746608734131,
        "learning_rate": 0.0001228931886441623,
        "epoch": 0.8096222773642129,
        "step": 5873
    },
    {
        "loss": 2.1067,
        "grad_norm": 2.0544159412384033,
        "learning_rate": 0.00012276625707972828,
        "epoch": 0.8097601323407775,
        "step": 5874
    },
    {
        "loss": 1.5014,
        "grad_norm": 3.4993343353271484,
        "learning_rate": 0.00012263928681828683,
        "epoch": 0.8098979873173422,
        "step": 5875
    },
    {
        "loss": 0.9526,
        "grad_norm": 2.9658987522125244,
        "learning_rate": 0.00012251227807565614,
        "epoch": 0.8100358422939068,
        "step": 5876
    },
    {
        "loss": 0.8909,
        "grad_norm": 7.778652667999268,
        "learning_rate": 0.00012238523106771976,
        "epoch": 0.8101736972704715,
        "step": 5877
    },
    {
        "loss": 2.1387,
        "grad_norm": 1.9713640213012695,
        "learning_rate": 0.00012225814601042593,
        "epoch": 0.8103115522470361,
        "step": 5878
    },
    {
        "loss": 1.4845,
        "grad_norm": 2.5251073837280273,
        "learning_rate": 0.0001221310231197881,
        "epoch": 0.8104494072236008,
        "step": 5879
    },
    {
        "loss": 1.9547,
        "grad_norm": 2.3766467571258545,
        "learning_rate": 0.00012200386261188363,
        "epoch": 0.8105872622001654,
        "step": 5880
    },
    {
        "loss": 0.8964,
        "grad_norm": 1.9134043455123901,
        "learning_rate": 0.00012187666470285378,
        "epoch": 0.8107251171767301,
        "step": 5881
    },
    {
        "loss": 1.2188,
        "grad_norm": 2.892256259918213,
        "learning_rate": 0.0001217494296089038,
        "epoch": 0.8108629721532947,
        "step": 5882
    },
    {
        "loss": 1.2596,
        "grad_norm": 2.913788318634033,
        "learning_rate": 0.00012162215754630166,
        "epoch": 0.8110008271298594,
        "step": 5883
    },
    {
        "loss": 1.98,
        "grad_norm": 1.3526602983474731,
        "learning_rate": 0.00012149484873137862,
        "epoch": 0.811138682106424,
        "step": 5884
    },
    {
        "loss": 1.6775,
        "grad_norm": 3.0175371170043945,
        "learning_rate": 0.00012136750338052808,
        "epoch": 0.8112765370829887,
        "step": 5885
    },
    {
        "loss": 1.5743,
        "grad_norm": 1.5410841703414917,
        "learning_rate": 0.00012124012171020548,
        "epoch": 0.8114143920595533,
        "step": 5886
    },
    {
        "loss": 1.8308,
        "grad_norm": 2.3858602046966553,
        "learning_rate": 0.00012111270393692834,
        "epoch": 0.8115522470361181,
        "step": 5887
    },
    {
        "loss": 1.9109,
        "grad_norm": 1.6966307163238525,
        "learning_rate": 0.00012098525027727545,
        "epoch": 0.8116901020126827,
        "step": 5888
    },
    {
        "loss": 1.9967,
        "grad_norm": 1.6353421211242676,
        "learning_rate": 0.00012085776094788628,
        "epoch": 0.8118279569892473,
        "step": 5889
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.4260200262069702,
        "learning_rate": 0.00012073023616546142,
        "epoch": 0.811965811965812,
        "step": 5890
    },
    {
        "loss": 1.2675,
        "grad_norm": 2.7329115867614746,
        "learning_rate": 0.0001206026761467612,
        "epoch": 0.8121036669423766,
        "step": 5891
    },
    {
        "loss": 2.3555,
        "grad_norm": 1.4765350818634033,
        "learning_rate": 0.00012047508110860636,
        "epoch": 0.8122415219189413,
        "step": 5892
    },
    {
        "loss": 1.918,
        "grad_norm": 2.078866958618164,
        "learning_rate": 0.00012034745126787683,
        "epoch": 0.8123793768955059,
        "step": 5893
    },
    {
        "loss": 1.9742,
        "grad_norm": 1.732591152191162,
        "learning_rate": 0.00012021978684151158,
        "epoch": 0.8125172318720706,
        "step": 5894
    },
    {
        "loss": 0.8317,
        "grad_norm": 1.9193435907363892,
        "learning_rate": 0.00012009208804650874,
        "epoch": 0.8126550868486352,
        "step": 5895
    },
    {
        "loss": 2.0686,
        "grad_norm": 1.4061836004257202,
        "learning_rate": 0.00011996435509992478,
        "epoch": 0.8127929418251999,
        "step": 5896
    },
    {
        "loss": 2.4488,
        "grad_norm": 1.1723634004592896,
        "learning_rate": 0.00011983658821887405,
        "epoch": 0.8129307968017645,
        "step": 5897
    },
    {
        "loss": 2.0558,
        "grad_norm": 1.2719707489013672,
        "learning_rate": 0.00011970878762052846,
        "epoch": 0.8130686517783292,
        "step": 5898
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.4812651872634888,
        "learning_rate": 0.00011958095352211771,
        "epoch": 0.8132065067548938,
        "step": 5899
    },
    {
        "loss": 2.1468,
        "grad_norm": 2.024868965148926,
        "learning_rate": 0.0001194530861409279,
        "epoch": 0.8133443617314585,
        "step": 5900
    },
    {
        "loss": 2.1623,
        "grad_norm": 2.1105566024780273,
        "learning_rate": 0.00011932518569430225,
        "epoch": 0.8134822167080231,
        "step": 5901
    },
    {
        "loss": 0.6898,
        "grad_norm": 1.0030015707015991,
        "learning_rate": 0.00011919725239963955,
        "epoch": 0.8136200716845878,
        "step": 5902
    },
    {
        "loss": 0.8162,
        "grad_norm": 1.2559245824813843,
        "learning_rate": 0.00011906928647439494,
        "epoch": 0.8137579266611524,
        "step": 5903
    },
    {
        "loss": 1.0067,
        "grad_norm": 1.8393118381500244,
        "learning_rate": 0.00011894128813607903,
        "epoch": 0.8138957816377171,
        "step": 5904
    },
    {
        "loss": 2.2147,
        "grad_norm": 1.2660858631134033,
        "learning_rate": 0.00011881325760225713,
        "epoch": 0.8140336366142817,
        "step": 5905
    },
    {
        "loss": 2.3407,
        "grad_norm": 1.7280771732330322,
        "learning_rate": 0.00011868519509054938,
        "epoch": 0.8141714915908465,
        "step": 5906
    },
    {
        "loss": 2.0394,
        "grad_norm": 2.4981000423431396,
        "learning_rate": 0.00011855710081863069,
        "epoch": 0.814309346567411,
        "step": 5907
    },
    {
        "loss": 2.2552,
        "grad_norm": 1.5615832805633545,
        "learning_rate": 0.00011842897500422934,
        "epoch": 0.8144472015439758,
        "step": 5908
    },
    {
        "loss": 2.5455,
        "grad_norm": 1.2785277366638184,
        "learning_rate": 0.00011830081786512778,
        "epoch": 0.8145850565205404,
        "step": 5909
    },
    {
        "loss": 1.5244,
        "grad_norm": 1.6106717586517334,
        "learning_rate": 0.0001181726296191614,
        "epoch": 0.8147229114971051,
        "step": 5910
    },
    {
        "loss": 2.3387,
        "grad_norm": 1.8221646547317505,
        "learning_rate": 0.00011804441048421828,
        "epoch": 0.8148607664736697,
        "step": 5911
    },
    {
        "loss": 2.0412,
        "grad_norm": 2.7566046714782715,
        "learning_rate": 0.00011791616067823958,
        "epoch": 0.8149986214502344,
        "step": 5912
    },
    {
        "loss": 1.8177,
        "grad_norm": 1.2231957912445068,
        "learning_rate": 0.00011778788041921797,
        "epoch": 0.815136476426799,
        "step": 5913
    },
    {
        "loss": 2.3495,
        "grad_norm": 1.4897993803024292,
        "learning_rate": 0.00011765956992519831,
        "epoch": 0.8152743314033637,
        "step": 5914
    },
    {
        "loss": 2.3311,
        "grad_norm": 2.2090260982513428,
        "learning_rate": 0.00011753122941427685,
        "epoch": 0.8154121863799283,
        "step": 5915
    },
    {
        "loss": 2.0713,
        "grad_norm": 1.8523530960083008,
        "learning_rate": 0.00011740285910460043,
        "epoch": 0.815550041356493,
        "step": 5916
    },
    {
        "loss": 1.9289,
        "grad_norm": 2.315918445587158,
        "learning_rate": 0.00011727445921436711,
        "epoch": 0.8156878963330576,
        "step": 5917
    },
    {
        "loss": 2.2699,
        "grad_norm": 1.2718830108642578,
        "learning_rate": 0.00011714602996182482,
        "epoch": 0.8158257513096223,
        "step": 5918
    },
    {
        "loss": 1.8022,
        "grad_norm": 1.8480435609817505,
        "learning_rate": 0.0001170175715652714,
        "epoch": 0.8159636062861869,
        "step": 5919
    },
    {
        "loss": 1.9477,
        "grad_norm": 2.0193333625793457,
        "learning_rate": 0.00011688908424305463,
        "epoch": 0.8161014612627516,
        "step": 5920
    },
    {
        "loss": 1.6043,
        "grad_norm": 2.529123544692993,
        "learning_rate": 0.00011676056821357088,
        "epoch": 0.8162393162393162,
        "step": 5921
    },
    {
        "loss": 1.9181,
        "grad_norm": 1.7349121570587158,
        "learning_rate": 0.00011663202369526598,
        "epoch": 0.8163771712158809,
        "step": 5922
    },
    {
        "loss": 2.0912,
        "grad_norm": 1.9621059894561768,
        "learning_rate": 0.0001165034509066336,
        "epoch": 0.8165150261924455,
        "step": 5923
    },
    {
        "loss": 1.8872,
        "grad_norm": 1.9521435499191284,
        "learning_rate": 0.00011637485006621557,
        "epoch": 0.8166528811690102,
        "step": 5924
    },
    {
        "loss": 1.6433,
        "grad_norm": 2.5101513862609863,
        "learning_rate": 0.00011624622139260168,
        "epoch": 0.8167907361455748,
        "step": 5925
    },
    {
        "loss": 1.526,
        "grad_norm": 1.7943809032440186,
        "learning_rate": 0.00011611756510442899,
        "epoch": 0.8169285911221396,
        "step": 5926
    },
    {
        "loss": 1.5949,
        "grad_norm": 2.2522754669189453,
        "learning_rate": 0.00011598888142038109,
        "epoch": 0.8170664460987042,
        "step": 5927
    },
    {
        "loss": 2.5068,
        "grad_norm": 2.2266592979431152,
        "learning_rate": 0.00011586017055918871,
        "epoch": 0.8172043010752689,
        "step": 5928
    },
    {
        "loss": 2.1173,
        "grad_norm": 1.6497939825057983,
        "learning_rate": 0.00011573143273962821,
        "epoch": 0.8173421560518335,
        "step": 5929
    },
    {
        "loss": 2.1596,
        "grad_norm": 1.7987141609191895,
        "learning_rate": 0.00011560266818052233,
        "epoch": 0.8174800110283982,
        "step": 5930
    },
    {
        "loss": 2.4444,
        "grad_norm": 1.2360247373580933,
        "learning_rate": 0.00011547387710073882,
        "epoch": 0.8176178660049628,
        "step": 5931
    },
    {
        "loss": 1.379,
        "grad_norm": 3.203641653060913,
        "learning_rate": 0.00011534505971919054,
        "epoch": 0.8177557209815274,
        "step": 5932
    },
    {
        "loss": 1.0997,
        "grad_norm": 3.2441606521606445,
        "learning_rate": 0.00011521621625483532,
        "epoch": 0.8178935759580921,
        "step": 5933
    },
    {
        "loss": 1.1439,
        "grad_norm": 3.2206077575683594,
        "learning_rate": 0.00011508734692667533,
        "epoch": 0.8180314309346567,
        "step": 5934
    },
    {
        "loss": 1.5009,
        "grad_norm": 1.622201681137085,
        "learning_rate": 0.00011495845195375644,
        "epoch": 0.8181692859112214,
        "step": 5935
    },
    {
        "loss": 1.9905,
        "grad_norm": 3.1748039722442627,
        "learning_rate": 0.00011482953155516804,
        "epoch": 0.818307140887786,
        "step": 5936
    },
    {
        "loss": 1.2771,
        "grad_norm": 2.936502456665039,
        "learning_rate": 0.00011470058595004327,
        "epoch": 0.8184449958643507,
        "step": 5937
    },
    {
        "loss": 2.6164,
        "grad_norm": 1.8571012020111084,
        "learning_rate": 0.00011457161535755746,
        "epoch": 0.8185828508409153,
        "step": 5938
    },
    {
        "loss": 2.1562,
        "grad_norm": 1.5902897119522095,
        "learning_rate": 0.000114442619996929,
        "epoch": 0.81872070581748,
        "step": 5939
    },
    {
        "loss": 2.3917,
        "grad_norm": 1.573325276374817,
        "learning_rate": 0.00011431360008741786,
        "epoch": 0.8188585607940446,
        "step": 5940
    },
    {
        "loss": 2.0315,
        "grad_norm": 1.9269341230392456,
        "learning_rate": 0.00011418455584832606,
        "epoch": 0.8189964157706093,
        "step": 5941
    },
    {
        "loss": 1.2404,
        "grad_norm": 3.016584634780884,
        "learning_rate": 0.00011405548749899705,
        "epoch": 0.8191342707471739,
        "step": 5942
    },
    {
        "loss": 2.3102,
        "grad_norm": 1.6629555225372314,
        "learning_rate": 0.00011392639525881505,
        "epoch": 0.8192721257237386,
        "step": 5943
    },
    {
        "loss": 1.41,
        "grad_norm": 1.6666275262832642,
        "learning_rate": 0.00011379727934720472,
        "epoch": 0.8194099807003032,
        "step": 5944
    },
    {
        "loss": 2.2233,
        "grad_norm": 1.316741943359375,
        "learning_rate": 0.00011366813998363147,
        "epoch": 0.819547835676868,
        "step": 5945
    },
    {
        "loss": 2.0047,
        "grad_norm": 2.5721352100372314,
        "learning_rate": 0.00011353897738760005,
        "epoch": 0.8196856906534326,
        "step": 5946
    },
    {
        "loss": 0.9422,
        "grad_norm": 0.960699737071991,
        "learning_rate": 0.00011340979177865514,
        "epoch": 0.8198235456299973,
        "step": 5947
    },
    {
        "loss": 0.7106,
        "grad_norm": 0.9590499401092529,
        "learning_rate": 0.00011328058337638026,
        "epoch": 0.8199614006065619,
        "step": 5948
    },
    {
        "loss": 2.0798,
        "grad_norm": 1.4145152568817139,
        "learning_rate": 0.00011315135240039759,
        "epoch": 0.8200992555831266,
        "step": 5949
    },
    {
        "loss": 1.9389,
        "grad_norm": 1.3175362348556519,
        "learning_rate": 0.00011302209907036804,
        "epoch": 0.8202371105596912,
        "step": 5950
    },
    {
        "loss": 2.482,
        "grad_norm": 1.8113932609558105,
        "learning_rate": 0.00011289282360599013,
        "epoch": 0.8203749655362559,
        "step": 5951
    },
    {
        "loss": 1.1904,
        "grad_norm": 2.6456899642944336,
        "learning_rate": 0.00011276352622700027,
        "epoch": 0.8205128205128205,
        "step": 5952
    },
    {
        "loss": 2.4824,
        "grad_norm": 1.2208222150802612,
        "learning_rate": 0.00011263420715317219,
        "epoch": 0.8206506754893852,
        "step": 5953
    },
    {
        "loss": 1.6567,
        "grad_norm": 2.494028329849243,
        "learning_rate": 0.00011250486660431602,
        "epoch": 0.8207885304659498,
        "step": 5954
    },
    {
        "loss": 2.0056,
        "grad_norm": 1.9397382736206055,
        "learning_rate": 0.00011237550480027903,
        "epoch": 0.8209263854425145,
        "step": 5955
    },
    {
        "loss": 1.046,
        "grad_norm": 2.2752060890197754,
        "learning_rate": 0.00011224612196094415,
        "epoch": 0.8210642404190791,
        "step": 5956
    },
    {
        "loss": 2.4592,
        "grad_norm": 1.481068730354309,
        "learning_rate": 0.00011211671830623006,
        "epoch": 0.8212020953956438,
        "step": 5957
    },
    {
        "loss": 1.3858,
        "grad_norm": 2.495800733566284,
        "learning_rate": 0.00011198729405609125,
        "epoch": 0.8213399503722084,
        "step": 5958
    },
    {
        "loss": 1.7642,
        "grad_norm": 1.6736518144607544,
        "learning_rate": 0.00011185784943051667,
        "epoch": 0.8214778053487731,
        "step": 5959
    },
    {
        "loss": 1.5014,
        "grad_norm": 2.474339485168457,
        "learning_rate": 0.00011172838464953046,
        "epoch": 0.8216156603253377,
        "step": 5960
    },
    {
        "loss": 2.1771,
        "grad_norm": 1.4795645475387573,
        "learning_rate": 0.00011159889993319059,
        "epoch": 0.8217535153019024,
        "step": 5961
    },
    {
        "loss": 1.9284,
        "grad_norm": 1.4003266096115112,
        "learning_rate": 0.0001114693955015889,
        "epoch": 0.821891370278467,
        "step": 5962
    },
    {
        "loss": 1.6346,
        "grad_norm": 1.4478269815444946,
        "learning_rate": 0.00011133987157485108,
        "epoch": 0.8220292252550317,
        "step": 5963
    },
    {
        "loss": 2.2467,
        "grad_norm": 1.4267003536224365,
        "learning_rate": 0.00011121032837313588,
        "epoch": 0.8221670802315963,
        "step": 5964
    },
    {
        "loss": 2.2719,
        "grad_norm": 1.579904556274414,
        "learning_rate": 0.00011108076611663443,
        "epoch": 0.8223049352081611,
        "step": 5965
    },
    {
        "loss": 1.2065,
        "grad_norm": 2.1487834453582764,
        "learning_rate": 0.00011095118502557085,
        "epoch": 0.8224427901847257,
        "step": 5966
    },
    {
        "loss": 1.852,
        "grad_norm": 1.7619080543518066,
        "learning_rate": 0.0001108215853202006,
        "epoch": 0.8225806451612904,
        "step": 5967
    },
    {
        "loss": 1.4731,
        "grad_norm": 2.274674654006958,
        "learning_rate": 0.00011069196722081146,
        "epoch": 0.822718500137855,
        "step": 5968
    },
    {
        "loss": 2.4238,
        "grad_norm": 1.434734582901001,
        "learning_rate": 0.00011056233094772198,
        "epoch": 0.8228563551144197,
        "step": 5969
    },
    {
        "loss": 1.363,
        "grad_norm": 3.377750873565674,
        "learning_rate": 0.00011043267672128168,
        "epoch": 0.8229942100909843,
        "step": 5970
    },
    {
        "loss": 1.8266,
        "grad_norm": 1.7249890565872192,
        "learning_rate": 0.00011030300476187077,
        "epoch": 0.823132065067549,
        "step": 5971
    },
    {
        "loss": 2.0935,
        "grad_norm": 1.9371615648269653,
        "learning_rate": 0.00011017331528989965,
        "epoch": 0.8232699200441136,
        "step": 5972
    },
    {
        "loss": 1.8176,
        "grad_norm": 3.207895278930664,
        "learning_rate": 0.00011004360852580826,
        "epoch": 0.8234077750206783,
        "step": 5973
    },
    {
        "loss": 2.128,
        "grad_norm": 1.816343069076538,
        "learning_rate": 0.00010991388469006585,
        "epoch": 0.8235456299972429,
        "step": 5974
    },
    {
        "loss": 2.0791,
        "grad_norm": 2.206773042678833,
        "learning_rate": 0.00010978414400317117,
        "epoch": 0.8236834849738075,
        "step": 5975
    },
    {
        "loss": 1.8027,
        "grad_norm": 2.026533603668213,
        "learning_rate": 0.00010965438668565098,
        "epoch": 0.8238213399503722,
        "step": 5976
    },
    {
        "loss": 1.7004,
        "grad_norm": 1.6194016933441162,
        "learning_rate": 0.00010952461295806095,
        "epoch": 0.8239591949269368,
        "step": 5977
    },
    {
        "loss": 1.231,
        "grad_norm": 2.460125684738159,
        "learning_rate": 0.00010939482304098403,
        "epoch": 0.8240970499035015,
        "step": 5978
    },
    {
        "loss": 2.2709,
        "grad_norm": 1.9236903190612793,
        "learning_rate": 0.00010926501715503114,
        "epoch": 0.8242349048800661,
        "step": 5979
    },
    {
        "loss": 1.6373,
        "grad_norm": 2.9288575649261475,
        "learning_rate": 0.0001091351955208403,
        "epoch": 0.8243727598566308,
        "step": 5980
    },
    {
        "loss": 1.5862,
        "grad_norm": 2.8869147300720215,
        "learning_rate": 0.00010900535835907609,
        "epoch": 0.8245106148331954,
        "step": 5981
    },
    {
        "loss": 2.4396,
        "grad_norm": 1.6532596349716187,
        "learning_rate": 0.00010887550589042939,
        "epoch": 0.8246484698097601,
        "step": 5982
    },
    {
        "loss": 2.0524,
        "grad_norm": 1.932997703552246,
        "learning_rate": 0.00010874563833561758,
        "epoch": 0.8247863247863247,
        "step": 5983
    },
    {
        "loss": 1.8646,
        "grad_norm": 2.0051424503326416,
        "learning_rate": 0.00010861575591538313,
        "epoch": 0.8249241797628895,
        "step": 5984
    },
    {
        "loss": 2.0375,
        "grad_norm": 1.977396845817566,
        "learning_rate": 0.00010848585885049429,
        "epoch": 0.825062034739454,
        "step": 5985
    },
    {
        "loss": 1.0442,
        "grad_norm": 2.0101242065429688,
        "learning_rate": 0.00010835594736174381,
        "epoch": 0.8251998897160188,
        "step": 5986
    },
    {
        "loss": 2.1773,
        "grad_norm": 1.899355411529541,
        "learning_rate": 0.000108226021669949,
        "epoch": 0.8253377446925834,
        "step": 5987
    },
    {
        "loss": 1.7441,
        "grad_norm": 2.4007647037506104,
        "learning_rate": 0.00010809608199595163,
        "epoch": 0.8254755996691481,
        "step": 5988
    },
    {
        "loss": 2.1265,
        "grad_norm": 1.777192234992981,
        "learning_rate": 0.00010796612856061676,
        "epoch": 0.8256134546457127,
        "step": 5989
    },
    {
        "loss": 1.7506,
        "grad_norm": 1.6920326948165894,
        "learning_rate": 0.00010783616158483323,
        "epoch": 0.8257513096222774,
        "step": 5990
    },
    {
        "loss": 1.772,
        "grad_norm": 2.3313050270080566,
        "learning_rate": 0.00010770618128951293,
        "epoch": 0.825889164598842,
        "step": 5991
    },
    {
        "loss": 2.5648,
        "grad_norm": 2.12546706199646,
        "learning_rate": 0.00010757618789558988,
        "epoch": 0.8260270195754067,
        "step": 5992
    },
    {
        "loss": 2.0377,
        "grad_norm": 2.8951661586761475,
        "learning_rate": 0.00010744618162402103,
        "epoch": 0.8261648745519713,
        "step": 5993
    },
    {
        "loss": 2.0215,
        "grad_norm": 1.3446602821350098,
        "learning_rate": 0.00010731616269578473,
        "epoch": 0.826302729528536,
        "step": 5994
    },
    {
        "loss": 2.002,
        "grad_norm": 2.598515748977661,
        "learning_rate": 0.00010718613133188089,
        "epoch": 0.8264405845051006,
        "step": 5995
    },
    {
        "loss": 1.0591,
        "grad_norm": 2.399541139602661,
        "learning_rate": 0.00010705608775333094,
        "epoch": 0.8265784394816653,
        "step": 5996
    },
    {
        "loss": 2.434,
        "grad_norm": 1.2598844766616821,
        "learning_rate": 0.00010692603218117652,
        "epoch": 0.8267162944582299,
        "step": 5997
    },
    {
        "loss": 1.4646,
        "grad_norm": 2.3096628189086914,
        "learning_rate": 0.00010679596483648022,
        "epoch": 0.8268541494347946,
        "step": 5998
    },
    {
        "loss": 2.0281,
        "grad_norm": 2.060173749923706,
        "learning_rate": 0.00010666588594032423,
        "epoch": 0.8269920044113592,
        "step": 5999
    },
    {
        "loss": 2.4303,
        "grad_norm": 2.132906436920166,
        "learning_rate": 0.00010653579571381038,
        "epoch": 0.8271298593879239,
        "step": 6000
    },
    {
        "loss": 0.9401,
        "grad_norm": 2.0012218952178955,
        "learning_rate": 0.00010640569437806001,
        "epoch": 0.8272677143644885,
        "step": 6001
    },
    {
        "loss": 2.2683,
        "grad_norm": 1.581494927406311,
        "learning_rate": 0.00010627558215421338,
        "epoch": 0.8274055693410532,
        "step": 6002
    },
    {
        "loss": 2.0284,
        "grad_norm": 1.9677635431289673,
        "learning_rate": 0.00010614545926342878,
        "epoch": 0.8275434243176178,
        "step": 6003
    },
    {
        "loss": 2.1046,
        "grad_norm": 1.5516818761825562,
        "learning_rate": 0.00010601532592688324,
        "epoch": 0.8276812792941826,
        "step": 6004
    },
    {
        "loss": 2.0446,
        "grad_norm": 2.059633493423462,
        "learning_rate": 0.00010588518236577101,
        "epoch": 0.8278191342707472,
        "step": 6005
    },
    {
        "loss": 1.7502,
        "grad_norm": 2.206695556640625,
        "learning_rate": 0.00010575502880130416,
        "epoch": 0.8279569892473119,
        "step": 6006
    },
    {
        "loss": 2.088,
        "grad_norm": 1.4778773784637451,
        "learning_rate": 0.00010562486545471151,
        "epoch": 0.8280948442238765,
        "step": 6007
    },
    {
        "loss": 1.4503,
        "grad_norm": 2.5511016845703125,
        "learning_rate": 0.00010549469254723838,
        "epoch": 0.8282326992004412,
        "step": 6008
    },
    {
        "loss": 2.038,
        "grad_norm": 2.560326099395752,
        "learning_rate": 0.00010536451030014666,
        "epoch": 0.8283705541770058,
        "step": 6009
    },
    {
        "loss": 1.6265,
        "grad_norm": 1.7387616634368896,
        "learning_rate": 0.00010523431893471412,
        "epoch": 0.8285084091535705,
        "step": 6010
    },
    {
        "loss": 1.9518,
        "grad_norm": 1.9928404092788696,
        "learning_rate": 0.00010510411867223377,
        "epoch": 0.8286462641301351,
        "step": 6011
    },
    {
        "loss": 2.2776,
        "grad_norm": 1.530229926109314,
        "learning_rate": 0.0001049739097340137,
        "epoch": 0.8287841191066998,
        "step": 6012
    },
    {
        "loss": 1.916,
        "grad_norm": 2.1739866733551025,
        "learning_rate": 0.00010484369234137718,
        "epoch": 0.8289219740832644,
        "step": 6013
    },
    {
        "loss": 2.0724,
        "grad_norm": 2.3537673950195312,
        "learning_rate": 0.00010471346671566132,
        "epoch": 0.8290598290598291,
        "step": 6014
    },
    {
        "loss": 1.4067,
        "grad_norm": 1.7794142961502075,
        "learning_rate": 0.00010458323307821776,
        "epoch": 0.8291976840363937,
        "step": 6015
    },
    {
        "loss": 2.2953,
        "grad_norm": 1.6313034296035767,
        "learning_rate": 0.00010445299165041115,
        "epoch": 0.8293355390129584,
        "step": 6016
    },
    {
        "loss": 2.3639,
        "grad_norm": 1.3794420957565308,
        "learning_rate": 0.00010432274265361992,
        "epoch": 0.829473393989523,
        "step": 6017
    },
    {
        "loss": 2.0658,
        "grad_norm": 1.3238191604614258,
        "learning_rate": 0.00010419248630923524,
        "epoch": 0.8296112489660876,
        "step": 6018
    },
    {
        "loss": 2.3739,
        "grad_norm": 1.3396447896957397,
        "learning_rate": 0.00010406222283866058,
        "epoch": 0.8297491039426523,
        "step": 6019
    },
    {
        "loss": 1.9608,
        "grad_norm": 1.7567089796066284,
        "learning_rate": 0.00010393195246331149,
        "epoch": 0.8298869589192169,
        "step": 6020
    },
    {
        "loss": 2.0895,
        "grad_norm": 1.260525107383728,
        "learning_rate": 0.00010380167540461562,
        "epoch": 0.8300248138957816,
        "step": 6021
    },
    {
        "loss": 2.3333,
        "grad_norm": 1.86933434009552,
        "learning_rate": 0.00010367139188401156,
        "epoch": 0.8301626688723462,
        "step": 6022
    },
    {
        "loss": 1.1134,
        "grad_norm": 1.129221796989441,
        "learning_rate": 0.0001035411021229493,
        "epoch": 0.830300523848911,
        "step": 6023
    },
    {
        "loss": 1.9567,
        "grad_norm": 2.2361457347869873,
        "learning_rate": 0.00010341080634288907,
        "epoch": 0.8304383788254756,
        "step": 6024
    },
    {
        "loss": 2.1769,
        "grad_norm": 1.5605063438415527,
        "learning_rate": 0.00010328050476530138,
        "epoch": 0.8305762338020403,
        "step": 6025
    },
    {
        "loss": 1.7934,
        "grad_norm": 2.3186779022216797,
        "learning_rate": 0.00010315019761166694,
        "epoch": 0.8307140887786049,
        "step": 6026
    },
    {
        "loss": 1.4764,
        "grad_norm": 2.615497350692749,
        "learning_rate": 0.0001030198851034754,
        "epoch": 0.8308519437551696,
        "step": 6027
    },
    {
        "loss": 2.0125,
        "grad_norm": 2.191171884536743,
        "learning_rate": 0.00010288956746222596,
        "epoch": 0.8309897987317342,
        "step": 6028
    },
    {
        "loss": 2.3173,
        "grad_norm": 1.2274067401885986,
        "learning_rate": 0.00010275924490942652,
        "epoch": 0.8311276537082989,
        "step": 6029
    },
    {
        "loss": 1.8146,
        "grad_norm": 2.1905133724212646,
        "learning_rate": 0.00010262891766659292,
        "epoch": 0.8312655086848635,
        "step": 6030
    },
    {
        "loss": 1.6437,
        "grad_norm": 3.1563851833343506,
        "learning_rate": 0.00010249858595524948,
        "epoch": 0.8314033636614282,
        "step": 6031
    },
    {
        "loss": 2.0029,
        "grad_norm": 1.450574517250061,
        "learning_rate": 0.00010236824999692783,
        "epoch": 0.8315412186379928,
        "step": 6032
    },
    {
        "loss": 2.0726,
        "grad_norm": 2.0034420490264893,
        "learning_rate": 0.00010223791001316665,
        "epoch": 0.8316790736145575,
        "step": 6033
    },
    {
        "loss": 1.6779,
        "grad_norm": 2.146054267883301,
        "learning_rate": 0.00010210756622551197,
        "epoch": 0.8318169285911221,
        "step": 6034
    },
    {
        "loss": 1.7539,
        "grad_norm": 1.8832528591156006,
        "learning_rate": 0.00010197721885551574,
        "epoch": 0.8319547835676868,
        "step": 6035
    },
    {
        "loss": 2.4243,
        "grad_norm": 2.5594401359558105,
        "learning_rate": 0.00010184686812473654,
        "epoch": 0.8320926385442514,
        "step": 6036
    },
    {
        "loss": 2.0488,
        "grad_norm": 1.570438265800476,
        "learning_rate": 0.00010171651425473823,
        "epoch": 0.8322304935208161,
        "step": 6037
    },
    {
        "loss": 1.8517,
        "grad_norm": 1.949961543083191,
        "learning_rate": 0.00010158615746709,
        "epoch": 0.8323683484973807,
        "step": 6038
    },
    {
        "loss": 2.6922,
        "grad_norm": 1.4049904346466064,
        "learning_rate": 0.00010145579798336631,
        "epoch": 0.8325062034739454,
        "step": 6039
    },
    {
        "loss": 1.5243,
        "grad_norm": 1.87328040599823,
        "learning_rate": 0.00010132543602514622,
        "epoch": 0.83264405845051,
        "step": 6040
    },
    {
        "loss": 1.0675,
        "grad_norm": 1.6696555614471436,
        "learning_rate": 0.00010119507181401247,
        "epoch": 0.8327819134270747,
        "step": 6041
    },
    {
        "loss": 2.1601,
        "grad_norm": 2.063538074493408,
        "learning_rate": 0.00010106470557155216,
        "epoch": 0.8329197684036393,
        "step": 6042
    },
    {
        "loss": 2.7957,
        "grad_norm": 1.5005712509155273,
        "learning_rate": 0.00010093433751935579,
        "epoch": 0.8330576233802041,
        "step": 6043
    },
    {
        "loss": 1.9666,
        "grad_norm": 2.411585807800293,
        "learning_rate": 0.00010080396787901671,
        "epoch": 0.8331954783567687,
        "step": 6044
    },
    {
        "loss": 2.0957,
        "grad_norm": 1.115411639213562,
        "learning_rate": 0.00010067359687213103,
        "epoch": 0.8333333333333334,
        "step": 6045
    },
    {
        "loss": 1.6588,
        "grad_norm": 2.0939018726348877,
        "learning_rate": 0.00010054322472029712,
        "epoch": 0.833471188309898,
        "step": 6046
    },
    {
        "loss": 1.852,
        "grad_norm": 3.0451722145080566,
        "learning_rate": 0.00010041285164511551,
        "epoch": 0.8336090432864627,
        "step": 6047
    },
    {
        "loss": 2.2574,
        "grad_norm": 2.0156166553497314,
        "learning_rate": 0.0001002824778681883,
        "epoch": 0.8337468982630273,
        "step": 6048
    },
    {
        "loss": 1.3031,
        "grad_norm": 2.241023540496826,
        "learning_rate": 0.0001001521036111186,
        "epoch": 0.833884753239592,
        "step": 6049
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.578320860862732,
        "learning_rate": 0.00010002172909551024,
        "epoch": 0.8340226082161566,
        "step": 6050
    },
    {
        "loss": 2.2036,
        "grad_norm": 1.9704383611679077,
        "learning_rate": 9.98913545429679e-05,
        "epoch": 0.8341604631927213,
        "step": 6051
    },
    {
        "loss": 2.2705,
        "grad_norm": 1.5840634107589722,
        "learning_rate": 9.976098017509584e-05,
        "epoch": 0.8342983181692859,
        "step": 6052
    },
    {
        "loss": 1.4005,
        "grad_norm": 1.8754081726074219,
        "learning_rate": 9.963060621349853e-05,
        "epoch": 0.8344361731458506,
        "step": 6053
    },
    {
        "loss": 2.3955,
        "grad_norm": 1.426396369934082,
        "learning_rate": 9.950023287977916e-05,
        "epoch": 0.8345740281224152,
        "step": 6054
    },
    {
        "loss": 2.1003,
        "grad_norm": 1.664553165435791,
        "learning_rate": 9.936986039554031e-05,
        "epoch": 0.8347118830989799,
        "step": 6055
    },
    {
        "loss": 1.2213,
        "grad_norm": 2.3339459896087646,
        "learning_rate": 9.92394889823831e-05,
        "epoch": 0.8348497380755445,
        "step": 6056
    },
    {
        "loss": 1.8922,
        "grad_norm": 2.262115955352783,
        "learning_rate": 9.910911886190658e-05,
        "epoch": 0.8349875930521092,
        "step": 6057
    },
    {
        "loss": 1.3523,
        "grad_norm": 2.4888525009155273,
        "learning_rate": 9.897875025570754e-05,
        "epoch": 0.8351254480286738,
        "step": 6058
    },
    {
        "loss": 1.5424,
        "grad_norm": 1.636872410774231,
        "learning_rate": 9.884838338538067e-05,
        "epoch": 0.8352633030052385,
        "step": 6059
    },
    {
        "loss": 1.2522,
        "grad_norm": 2.300288200378418,
        "learning_rate": 9.871801847251712e-05,
        "epoch": 0.8354011579818031,
        "step": 6060
    },
    {
        "loss": 1.9646,
        "grad_norm": 2.0679104328155518,
        "learning_rate": 9.85876557387053e-05,
        "epoch": 0.8355390129583677,
        "step": 6061
    },
    {
        "loss": 1.8981,
        "grad_norm": 1.6443110704421997,
        "learning_rate": 9.845729540552947e-05,
        "epoch": 0.8356768679349325,
        "step": 6062
    },
    {
        "loss": 2.0301,
        "grad_norm": 2.621696710586548,
        "learning_rate": 9.832693769456983e-05,
        "epoch": 0.835814722911497,
        "step": 6063
    },
    {
        "loss": 1.54,
        "grad_norm": 1.829299807548523,
        "learning_rate": 9.819658282740253e-05,
        "epoch": 0.8359525778880618,
        "step": 6064
    },
    {
        "loss": 1.8839,
        "grad_norm": 1.7934845685958862,
        "learning_rate": 9.806623102559832e-05,
        "epoch": 0.8360904328646264,
        "step": 6065
    },
    {
        "loss": 2.0514,
        "grad_norm": 1.2585779428482056,
        "learning_rate": 9.793588251072318e-05,
        "epoch": 0.8362282878411911,
        "step": 6066
    },
    {
        "loss": 1.5997,
        "grad_norm": 2.9802820682525635,
        "learning_rate": 9.780553750433747e-05,
        "epoch": 0.8363661428177557,
        "step": 6067
    },
    {
        "loss": 1.8973,
        "grad_norm": 2.736732006072998,
        "learning_rate": 9.767519622799524e-05,
        "epoch": 0.8365039977943204,
        "step": 6068
    },
    {
        "loss": 1.9459,
        "grad_norm": 1.9083507061004639,
        "learning_rate": 9.754485890324466e-05,
        "epoch": 0.836641852770885,
        "step": 6069
    },
    {
        "loss": 0.8613,
        "grad_norm": 1.4575490951538086,
        "learning_rate": 9.741452575162682e-05,
        "epoch": 0.8367797077474497,
        "step": 6070
    },
    {
        "loss": 1.533,
        "grad_norm": 1.6807385683059692,
        "learning_rate": 9.728419699467575e-05,
        "epoch": 0.8369175627240143,
        "step": 6071
    },
    {
        "loss": 1.1351,
        "grad_norm": 3.154862403869629,
        "learning_rate": 9.715387285391832e-05,
        "epoch": 0.837055417700579,
        "step": 6072
    },
    {
        "loss": 2.4007,
        "grad_norm": 2.8859283924102783,
        "learning_rate": 9.702355355087309e-05,
        "epoch": 0.8371932726771436,
        "step": 6073
    },
    {
        "loss": 1.633,
        "grad_norm": 3.184845447540283,
        "learning_rate": 9.689323930705096e-05,
        "epoch": 0.8373311276537083,
        "step": 6074
    },
    {
        "loss": 0.9101,
        "grad_norm": 3.7663497924804688,
        "learning_rate": 9.676293034395375e-05,
        "epoch": 0.8374689826302729,
        "step": 6075
    },
    {
        "loss": 2.226,
        "grad_norm": 1.4650205373764038,
        "learning_rate": 9.663262688307433e-05,
        "epoch": 0.8376068376068376,
        "step": 6076
    },
    {
        "loss": 1.2799,
        "grad_norm": 1.9684749841690063,
        "learning_rate": 9.650232914589652e-05,
        "epoch": 0.8377446925834022,
        "step": 6077
    },
    {
        "loss": 1.3857,
        "grad_norm": 2.593975305557251,
        "learning_rate": 9.63720373538944e-05,
        "epoch": 0.8378825475599669,
        "step": 6078
    },
    {
        "loss": 1.9168,
        "grad_norm": 2.23349928855896,
        "learning_rate": 9.624175172853148e-05,
        "epoch": 0.8380204025365315,
        "step": 6079
    },
    {
        "loss": 1.6252,
        "grad_norm": 2.099148988723755,
        "learning_rate": 9.611147249126127e-05,
        "epoch": 0.8381582575130962,
        "step": 6080
    },
    {
        "loss": 1.8752,
        "grad_norm": 1.5938961505889893,
        "learning_rate": 9.598119986352639e-05,
        "epoch": 0.8382961124896608,
        "step": 6081
    },
    {
        "loss": 1.1008,
        "grad_norm": 2.223764181137085,
        "learning_rate": 9.585093406675798e-05,
        "epoch": 0.8384339674662256,
        "step": 6082
    },
    {
        "loss": 1.4818,
        "grad_norm": 2.417849540710449,
        "learning_rate": 9.572067532237563e-05,
        "epoch": 0.8385718224427902,
        "step": 6083
    },
    {
        "loss": 2.3526,
        "grad_norm": 1.695769190788269,
        "learning_rate": 9.559042385178689e-05,
        "epoch": 0.8387096774193549,
        "step": 6084
    },
    {
        "loss": 1.8561,
        "grad_norm": 2.0543861389160156,
        "learning_rate": 9.546017987638722e-05,
        "epoch": 0.8388475323959195,
        "step": 6085
    },
    {
        "loss": 1.3607,
        "grad_norm": 1.3615565299987793,
        "learning_rate": 9.532994361755923e-05,
        "epoch": 0.8389853873724842,
        "step": 6086
    },
    {
        "loss": 1.7131,
        "grad_norm": 1.9390103816986084,
        "learning_rate": 9.519971529667227e-05,
        "epoch": 0.8391232423490488,
        "step": 6087
    },
    {
        "loss": 1.8636,
        "grad_norm": 1.6052933931350708,
        "learning_rate": 9.506949513508217e-05,
        "epoch": 0.8392610973256135,
        "step": 6088
    },
    {
        "loss": 1.7199,
        "grad_norm": 2.125807523727417,
        "learning_rate": 9.493928335413121e-05,
        "epoch": 0.8393989523021781,
        "step": 6089
    },
    {
        "loss": 2.5484,
        "grad_norm": 1.1004716157913208,
        "learning_rate": 9.480908017514696e-05,
        "epoch": 0.8395368072787428,
        "step": 6090
    },
    {
        "loss": 1.5763,
        "grad_norm": 3.0265603065490723,
        "learning_rate": 9.467888581944296e-05,
        "epoch": 0.8396746622553074,
        "step": 6091
    },
    {
        "loss": 2.3752,
        "grad_norm": 1.9228883981704712,
        "learning_rate": 9.454870050831709e-05,
        "epoch": 0.8398125172318721,
        "step": 6092
    },
    {
        "loss": 1.3971,
        "grad_norm": 2.138087272644043,
        "learning_rate": 9.441852446305232e-05,
        "epoch": 0.8399503722084367,
        "step": 6093
    },
    {
        "loss": 2.09,
        "grad_norm": 2.3231513500213623,
        "learning_rate": 9.42883579049159e-05,
        "epoch": 0.8400882271850014,
        "step": 6094
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.618485689163208,
        "learning_rate": 9.415820105515866e-05,
        "epoch": 0.840226082161566,
        "step": 6095
    },
    {
        "loss": 1.4154,
        "grad_norm": 2.619065046310425,
        "learning_rate": 9.402805413501489e-05,
        "epoch": 0.8403639371381307,
        "step": 6096
    },
    {
        "loss": 2.0852,
        "grad_norm": 2.6186957359313965,
        "learning_rate": 9.389791736570244e-05,
        "epoch": 0.8405017921146953,
        "step": 6097
    },
    {
        "loss": 2.3778,
        "grad_norm": 1.689136028289795,
        "learning_rate": 9.376779096842141e-05,
        "epoch": 0.84063964709126,
        "step": 6098
    },
    {
        "loss": 1.8441,
        "grad_norm": 1.7621116638183594,
        "learning_rate": 9.363767516435476e-05,
        "epoch": 0.8407775020678246,
        "step": 6099
    },
    {
        "loss": 2.2876,
        "grad_norm": 2.093416929244995,
        "learning_rate": 9.350757017466701e-05,
        "epoch": 0.8409153570443894,
        "step": 6100
    },
    {
        "loss": 2.0594,
        "grad_norm": 2.434572696685791,
        "learning_rate": 9.337747622050435e-05,
        "epoch": 0.841053212020954,
        "step": 6101
    },
    {
        "loss": 1.5489,
        "grad_norm": 1.762651801109314,
        "learning_rate": 9.324739352299464e-05,
        "epoch": 0.8411910669975186,
        "step": 6102
    },
    {
        "loss": 2.096,
        "grad_norm": 1.5053133964538574,
        "learning_rate": 9.311732230324595e-05,
        "epoch": 0.8413289219740833,
        "step": 6103
    },
    {
        "loss": 1.6307,
        "grad_norm": 2.624579429626465,
        "learning_rate": 9.298726278234739e-05,
        "epoch": 0.8414667769506479,
        "step": 6104
    },
    {
        "loss": 2.1128,
        "grad_norm": 2.213625907897949,
        "learning_rate": 9.285721518136808e-05,
        "epoch": 0.8416046319272126,
        "step": 6105
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.6920145750045776,
        "learning_rate": 9.272717972135657e-05,
        "epoch": 0.8417424869037772,
        "step": 6106
    },
    {
        "loss": 1.5715,
        "grad_norm": 1.8805214166641235,
        "learning_rate": 9.259715662334119e-05,
        "epoch": 0.8418803418803419,
        "step": 6107
    },
    {
        "loss": 1.5459,
        "grad_norm": 2.797869920730591,
        "learning_rate": 9.246714610832893e-05,
        "epoch": 0.8420181968569065,
        "step": 6108
    },
    {
        "loss": 1.1131,
        "grad_norm": 2.167962074279785,
        "learning_rate": 9.233714839730545e-05,
        "epoch": 0.8421560518334712,
        "step": 6109
    },
    {
        "loss": 1.7082,
        "grad_norm": 1.3070045709609985,
        "learning_rate": 9.22071637112349e-05,
        "epoch": 0.8422939068100358,
        "step": 6110
    },
    {
        "loss": 1.3064,
        "grad_norm": 2.4565818309783936,
        "learning_rate": 9.207719227105885e-05,
        "epoch": 0.8424317617866005,
        "step": 6111
    },
    {
        "loss": 1.5287,
        "grad_norm": 2.755383014678955,
        "learning_rate": 9.19472342976969e-05,
        "epoch": 0.8425696167631651,
        "step": 6112
    },
    {
        "loss": 1.8906,
        "grad_norm": 1.5314269065856934,
        "learning_rate": 9.181729001204534e-05,
        "epoch": 0.8427074717397298,
        "step": 6113
    },
    {
        "loss": 1.7506,
        "grad_norm": 2.5387258529663086,
        "learning_rate": 9.168735963497714e-05,
        "epoch": 0.8428453267162944,
        "step": 6114
    },
    {
        "loss": 2.0768,
        "grad_norm": 1.4052294492721558,
        "learning_rate": 9.1557443387342e-05,
        "epoch": 0.8429831816928591,
        "step": 6115
    },
    {
        "loss": 1.4576,
        "grad_norm": 1.9420111179351807,
        "learning_rate": 9.142754148996553e-05,
        "epoch": 0.8431210366694237,
        "step": 6116
    },
    {
        "loss": 1.9577,
        "grad_norm": 1.8126046657562256,
        "learning_rate": 9.129765416364855e-05,
        "epoch": 0.8432588916459884,
        "step": 6117
    },
    {
        "loss": 2.2027,
        "grad_norm": 1.1867132186889648,
        "learning_rate": 9.116778162916753e-05,
        "epoch": 0.843396746622553,
        "step": 6118
    },
    {
        "loss": 2.1744,
        "grad_norm": 1.8086920976638794,
        "learning_rate": 9.103792410727383e-05,
        "epoch": 0.8435346015991178,
        "step": 6119
    },
    {
        "loss": 1.9732,
        "grad_norm": 1.722468614578247,
        "learning_rate": 9.090808181869298e-05,
        "epoch": 0.8436724565756824,
        "step": 6120
    },
    {
        "loss": 2.1115,
        "grad_norm": 1.4853167533874512,
        "learning_rate": 9.077825498412473e-05,
        "epoch": 0.8438103115522471,
        "step": 6121
    },
    {
        "loss": 1.5325,
        "grad_norm": 2.6322622299194336,
        "learning_rate": 9.064844382424248e-05,
        "epoch": 0.8439481665288117,
        "step": 6122
    },
    {
        "loss": 1.7271,
        "grad_norm": 2.3575122356414795,
        "learning_rate": 9.051864855969326e-05,
        "epoch": 0.8440860215053764,
        "step": 6123
    },
    {
        "loss": 1.9495,
        "grad_norm": 1.8986022472381592,
        "learning_rate": 9.038886941109705e-05,
        "epoch": 0.844223876481941,
        "step": 6124
    },
    {
        "loss": 0.8833,
        "grad_norm": 2.0684149265289307,
        "learning_rate": 9.025910659904612e-05,
        "epoch": 0.8443617314585057,
        "step": 6125
    },
    {
        "loss": 1.9029,
        "grad_norm": 1.6186127662658691,
        "learning_rate": 9.012936034410509e-05,
        "epoch": 0.8444995864350703,
        "step": 6126
    },
    {
        "loss": 1.4588,
        "grad_norm": 2.437549591064453,
        "learning_rate": 8.999963086681078e-05,
        "epoch": 0.844637441411635,
        "step": 6127
    },
    {
        "loss": 1.9038,
        "grad_norm": 1.7040356397628784,
        "learning_rate": 8.986991838767091e-05,
        "epoch": 0.8447752963881996,
        "step": 6128
    },
    {
        "loss": 1.5007,
        "grad_norm": 2.300128221511841,
        "learning_rate": 8.97402231271649e-05,
        "epoch": 0.8449131513647643,
        "step": 6129
    },
    {
        "loss": 1.4197,
        "grad_norm": 3.47725248336792,
        "learning_rate": 8.96105453057424e-05,
        "epoch": 0.8450510063413289,
        "step": 6130
    },
    {
        "loss": 1.8953,
        "grad_norm": 2.115234136581421,
        "learning_rate": 8.94808851438237e-05,
        "epoch": 0.8451888613178936,
        "step": 6131
    },
    {
        "loss": 1.9657,
        "grad_norm": 2.038125991821289,
        "learning_rate": 8.935124286179919e-05,
        "epoch": 0.8453267162944582,
        "step": 6132
    },
    {
        "loss": 2.4518,
        "grad_norm": 1.6428378820419312,
        "learning_rate": 8.922161868002856e-05,
        "epoch": 0.8454645712710229,
        "step": 6133
    },
    {
        "loss": 2.0413,
        "grad_norm": 2.076348066329956,
        "learning_rate": 8.90920128188407e-05,
        "epoch": 0.8456024262475875,
        "step": 6134
    },
    {
        "loss": 1.437,
        "grad_norm": 1.7852383852005005,
        "learning_rate": 8.896242549853382e-05,
        "epoch": 0.8457402812241522,
        "step": 6135
    },
    {
        "loss": 2.0498,
        "grad_norm": 1.7629780769348145,
        "learning_rate": 8.883285693937404e-05,
        "epoch": 0.8458781362007168,
        "step": 6136
    },
    {
        "loss": 1.5723,
        "grad_norm": 1.7788112163543701,
        "learning_rate": 8.870330736159616e-05,
        "epoch": 0.8460159911772815,
        "step": 6137
    },
    {
        "loss": 1.9485,
        "grad_norm": 2.2175159454345703,
        "learning_rate": 8.857377698540225e-05,
        "epoch": 0.8461538461538461,
        "step": 6138
    },
    {
        "loss": 1.3835,
        "grad_norm": 2.473487377166748,
        "learning_rate": 8.844426603096182e-05,
        "epoch": 0.8462917011304109,
        "step": 6139
    },
    {
        "loss": 1.9938,
        "grad_norm": 1.34749174118042,
        "learning_rate": 8.831477471841175e-05,
        "epoch": 0.8464295561069755,
        "step": 6140
    },
    {
        "loss": 1.254,
        "grad_norm": 2.413701295852661,
        "learning_rate": 8.818530326785493e-05,
        "epoch": 0.8465674110835402,
        "step": 6141
    },
    {
        "loss": 2.3897,
        "grad_norm": 2.6045401096343994,
        "learning_rate": 8.805585189936096e-05,
        "epoch": 0.8467052660601048,
        "step": 6142
    },
    {
        "loss": 2.0666,
        "grad_norm": 2.484795331954956,
        "learning_rate": 8.79264208329653e-05,
        "epoch": 0.8468431210366695,
        "step": 6143
    },
    {
        "loss": 2.0114,
        "grad_norm": 1.5996938943862915,
        "learning_rate": 8.77970102886685e-05,
        "epoch": 0.8469809760132341,
        "step": 6144
    },
    {
        "loss": 1.4061,
        "grad_norm": 2.064796209335327,
        "learning_rate": 8.766762048643675e-05,
        "epoch": 0.8471188309897987,
        "step": 6145
    },
    {
        "loss": 1.9811,
        "grad_norm": 2.3197195529937744,
        "learning_rate": 8.753825164620057e-05,
        "epoch": 0.8472566859663634,
        "step": 6146
    },
    {
        "loss": 1.3209,
        "grad_norm": 1.71043062210083,
        "learning_rate": 8.740890398785484e-05,
        "epoch": 0.847394540942928,
        "step": 6147
    },
    {
        "loss": 1.9091,
        "grad_norm": 2.6542885303497314,
        "learning_rate": 8.727957773125893e-05,
        "epoch": 0.8475323959194927,
        "step": 6148
    },
    {
        "loss": 1.3103,
        "grad_norm": 2.076395273208618,
        "learning_rate": 8.715027309623515e-05,
        "epoch": 0.8476702508960573,
        "step": 6149
    },
    {
        "loss": 1.1165,
        "grad_norm": 2.7406439781188965,
        "learning_rate": 8.702099030256974e-05,
        "epoch": 0.847808105872622,
        "step": 6150
    },
    {
        "loss": 2.0338,
        "grad_norm": 1.6923903226852417,
        "learning_rate": 8.689172957001132e-05,
        "epoch": 0.8479459608491866,
        "step": 6151
    },
    {
        "loss": 2.1615,
        "grad_norm": 2.014296770095825,
        "learning_rate": 8.6762491118271e-05,
        "epoch": 0.8480838158257513,
        "step": 6152
    },
    {
        "loss": 1.6073,
        "grad_norm": 3.193413734436035,
        "learning_rate": 8.663327516702237e-05,
        "epoch": 0.8482216708023159,
        "step": 6153
    },
    {
        "loss": 2.3831,
        "grad_norm": 1.1597132682800293,
        "learning_rate": 8.650408193590073e-05,
        "epoch": 0.8483595257788806,
        "step": 6154
    },
    {
        "loss": 1.2148,
        "grad_norm": 3.109192132949829,
        "learning_rate": 8.637491164450232e-05,
        "epoch": 0.8484973807554452,
        "step": 6155
    },
    {
        "loss": 1.897,
        "grad_norm": 2.1525766849517822,
        "learning_rate": 8.624576451238485e-05,
        "epoch": 0.8486352357320099,
        "step": 6156
    },
    {
        "loss": 2.2503,
        "grad_norm": 1.561716914176941,
        "learning_rate": 8.611664075906664e-05,
        "epoch": 0.8487730907085745,
        "step": 6157
    },
    {
        "loss": 2.0893,
        "grad_norm": 2.008356809616089,
        "learning_rate": 8.598754060402602e-05,
        "epoch": 0.8489109456851393,
        "step": 6158
    },
    {
        "loss": 2.5297,
        "grad_norm": 1.9891828298568726,
        "learning_rate": 8.585846426670131e-05,
        "epoch": 0.8490488006617039,
        "step": 6159
    },
    {
        "loss": 1.9911,
        "grad_norm": 1.413017988204956,
        "learning_rate": 8.572941196649024e-05,
        "epoch": 0.8491866556382686,
        "step": 6160
    },
    {
        "loss": 2.4233,
        "grad_norm": 2.2267513275146484,
        "learning_rate": 8.560038392274994e-05,
        "epoch": 0.8493245106148332,
        "step": 6161
    },
    {
        "loss": 1.8792,
        "grad_norm": 1.8835889101028442,
        "learning_rate": 8.547138035479632e-05,
        "epoch": 0.8494623655913979,
        "step": 6162
    },
    {
        "loss": 2.0339,
        "grad_norm": 1.8171597719192505,
        "learning_rate": 8.534240148190338e-05,
        "epoch": 0.8496002205679625,
        "step": 6163
    },
    {
        "loss": 2.2121,
        "grad_norm": 1.451171636581421,
        "learning_rate": 8.521344752330322e-05,
        "epoch": 0.8497380755445272,
        "step": 6164
    },
    {
        "loss": 0.7014,
        "grad_norm": 2.12896728515625,
        "learning_rate": 8.508451869818594e-05,
        "epoch": 0.8498759305210918,
        "step": 6165
    },
    {
        "loss": 1.9638,
        "grad_norm": 2.1001815795898438,
        "learning_rate": 8.495561522569835e-05,
        "epoch": 0.8500137854976565,
        "step": 6166
    },
    {
        "loss": 1.7348,
        "grad_norm": 1.911023736000061,
        "learning_rate": 8.482673732494478e-05,
        "epoch": 0.8501516404742211,
        "step": 6167
    },
    {
        "loss": 2.2255,
        "grad_norm": 1.7642565965652466,
        "learning_rate": 8.469788521498552e-05,
        "epoch": 0.8502894954507858,
        "step": 6168
    },
    {
        "loss": 2.231,
        "grad_norm": 2.377243995666504,
        "learning_rate": 8.456905911483745e-05,
        "epoch": 0.8504273504273504,
        "step": 6169
    },
    {
        "loss": 2.2041,
        "grad_norm": 1.9916534423828125,
        "learning_rate": 8.444025924347318e-05,
        "epoch": 0.8505652054039151,
        "step": 6170
    },
    {
        "loss": 1.0773,
        "grad_norm": 3.043450355529785,
        "learning_rate": 8.431148581982052e-05,
        "epoch": 0.8507030603804797,
        "step": 6171
    },
    {
        "loss": 2.2127,
        "grad_norm": 1.3489726781845093,
        "learning_rate": 8.418273906276233e-05,
        "epoch": 0.8508409153570444,
        "step": 6172
    },
    {
        "loss": 1.3424,
        "grad_norm": 1.333207368850708,
        "learning_rate": 8.405401919113649e-05,
        "epoch": 0.850978770333609,
        "step": 6173
    },
    {
        "loss": 1.9353,
        "grad_norm": 1.3956996202468872,
        "learning_rate": 8.392532642373472e-05,
        "epoch": 0.8511166253101737,
        "step": 6174
    },
    {
        "loss": 2.3046,
        "grad_norm": 1.1599029302597046,
        "learning_rate": 8.379666097930316e-05,
        "epoch": 0.8512544802867383,
        "step": 6175
    },
    {
        "loss": 1.6976,
        "grad_norm": 2.3140807151794434,
        "learning_rate": 8.36680230765411e-05,
        "epoch": 0.851392335263303,
        "step": 6176
    },
    {
        "loss": 2.283,
        "grad_norm": 2.458434581756592,
        "learning_rate": 8.353941293410103e-05,
        "epoch": 0.8515301902398676,
        "step": 6177
    },
    {
        "loss": 0.6862,
        "grad_norm": 2.717538356781006,
        "learning_rate": 8.34108307705887e-05,
        "epoch": 0.8516680452164324,
        "step": 6178
    },
    {
        "loss": 1.8693,
        "grad_norm": 2.0178797245025635,
        "learning_rate": 8.328227680456166e-05,
        "epoch": 0.851805900192997,
        "step": 6179
    },
    {
        "loss": 2.0454,
        "grad_norm": 2.8862557411193848,
        "learning_rate": 8.315375125453004e-05,
        "epoch": 0.8519437551695617,
        "step": 6180
    },
    {
        "loss": 1.705,
        "grad_norm": 2.96824312210083,
        "learning_rate": 8.30252543389556e-05,
        "epoch": 0.8520816101461263,
        "step": 6181
    },
    {
        "loss": 2.0622,
        "grad_norm": 1.4841164350509644,
        "learning_rate": 8.289678627625109e-05,
        "epoch": 0.852219465122691,
        "step": 6182
    },
    {
        "loss": 1.7334,
        "grad_norm": 1.2375400066375732,
        "learning_rate": 8.27683472847807e-05,
        "epoch": 0.8523573200992556,
        "step": 6183
    },
    {
        "loss": 2.3188,
        "grad_norm": 1.5172464847564697,
        "learning_rate": 8.26399375828588e-05,
        "epoch": 0.8524951750758203,
        "step": 6184
    },
    {
        "loss": 1.8864,
        "grad_norm": 2.2202444076538086,
        "learning_rate": 8.251155738875e-05,
        "epoch": 0.8526330300523849,
        "step": 6185
    },
    {
        "loss": 2.3472,
        "grad_norm": 1.763817548751831,
        "learning_rate": 8.23832069206692e-05,
        "epoch": 0.8527708850289496,
        "step": 6186
    },
    {
        "loss": 1.7876,
        "grad_norm": 2.522472381591797,
        "learning_rate": 8.225488639678011e-05,
        "epoch": 0.8529087400055142,
        "step": 6187
    },
    {
        "loss": 1.9176,
        "grad_norm": 2.489985704421997,
        "learning_rate": 8.212659603519621e-05,
        "epoch": 0.8530465949820788,
        "step": 6188
    },
    {
        "loss": 1.7022,
        "grad_norm": 2.998051881790161,
        "learning_rate": 8.199833605397926e-05,
        "epoch": 0.8531844499586435,
        "step": 6189
    },
    {
        "loss": 1.8676,
        "grad_norm": 3.354257106781006,
        "learning_rate": 8.187010667113936e-05,
        "epoch": 0.8533223049352081,
        "step": 6190
    },
    {
        "loss": 1.6962,
        "grad_norm": 1.9224354028701782,
        "learning_rate": 8.174190810463487e-05,
        "epoch": 0.8534601599117728,
        "step": 6191
    },
    {
        "loss": 2.3725,
        "grad_norm": 2.030885696411133,
        "learning_rate": 8.161374057237181e-05,
        "epoch": 0.8535980148883374,
        "step": 6192
    },
    {
        "loss": 2.3156,
        "grad_norm": 1.1964741945266724,
        "learning_rate": 8.148560429220304e-05,
        "epoch": 0.8537358698649021,
        "step": 6193
    },
    {
        "loss": 2.4411,
        "grad_norm": 1.6394621133804321,
        "learning_rate": 8.135749948192864e-05,
        "epoch": 0.8538737248414667,
        "step": 6194
    },
    {
        "loss": 2.1059,
        "grad_norm": 1.6313564777374268,
        "learning_rate": 8.122942635929526e-05,
        "epoch": 0.8540115798180314,
        "step": 6195
    },
    {
        "loss": 1.4491,
        "grad_norm": 2.1176936626434326,
        "learning_rate": 8.110138514199543e-05,
        "epoch": 0.854149434794596,
        "step": 6196
    },
    {
        "loss": 2.2537,
        "grad_norm": 1.1770819425582886,
        "learning_rate": 8.09733760476675e-05,
        "epoch": 0.8542872897711608,
        "step": 6197
    },
    {
        "loss": 1.7722,
        "grad_norm": 1.7850191593170166,
        "learning_rate": 8.084539929389519e-05,
        "epoch": 0.8544251447477254,
        "step": 6198
    },
    {
        "loss": 1.8838,
        "grad_norm": 2.081540822982788,
        "learning_rate": 8.071745509820746e-05,
        "epoch": 0.8545629997242901,
        "step": 6199
    },
    {
        "loss": 1.3819,
        "grad_norm": 2.507622718811035,
        "learning_rate": 8.058954367807795e-05,
        "epoch": 0.8547008547008547,
        "step": 6200
    },
    {
        "loss": 1.3825,
        "grad_norm": 1.9357702732086182,
        "learning_rate": 8.046166525092427e-05,
        "epoch": 0.8548387096774194,
        "step": 6201
    },
    {
        "loss": 2.1426,
        "grad_norm": 1.8467978239059448,
        "learning_rate": 8.033382003410802e-05,
        "epoch": 0.854976564653984,
        "step": 6202
    },
    {
        "loss": 1.8013,
        "grad_norm": 1.901219129562378,
        "learning_rate": 8.020600824493473e-05,
        "epoch": 0.8551144196305487,
        "step": 6203
    },
    {
        "loss": 2.1022,
        "grad_norm": 1.757650375366211,
        "learning_rate": 8.007823010065254e-05,
        "epoch": 0.8552522746071133,
        "step": 6204
    },
    {
        "loss": 1.9917,
        "grad_norm": 2.4195406436920166,
        "learning_rate": 7.995048581845284e-05,
        "epoch": 0.855390129583678,
        "step": 6205
    },
    {
        "loss": 1.7901,
        "grad_norm": 1.4205392599105835,
        "learning_rate": 7.982277561546947e-05,
        "epoch": 0.8555279845602426,
        "step": 6206
    },
    {
        "loss": 1.6553,
        "grad_norm": 2.2694082260131836,
        "learning_rate": 7.969509970877792e-05,
        "epoch": 0.8556658395368073,
        "step": 6207
    },
    {
        "loss": 1.4725,
        "grad_norm": 1.9146416187286377,
        "learning_rate": 7.956745831539592e-05,
        "epoch": 0.8558036945133719,
        "step": 6208
    },
    {
        "loss": 2.2151,
        "grad_norm": 1.169539451599121,
        "learning_rate": 7.943985165228214e-05,
        "epoch": 0.8559415494899366,
        "step": 6209
    },
    {
        "loss": 2.0627,
        "grad_norm": 1.7174831628799438,
        "learning_rate": 7.931227993633623e-05,
        "epoch": 0.8560794044665012,
        "step": 6210
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.6414457559585571,
        "learning_rate": 7.918474338439882e-05,
        "epoch": 0.8562172594430659,
        "step": 6211
    },
    {
        "loss": 0.9942,
        "grad_norm": 3.884317398071289,
        "learning_rate": 7.905724221325024e-05,
        "epoch": 0.8563551144196305,
        "step": 6212
    },
    {
        "loss": 2.5526,
        "grad_norm": 2.1400322914123535,
        "learning_rate": 7.892977663961123e-05,
        "epoch": 0.8564929693961952,
        "step": 6213
    },
    {
        "loss": 2.0109,
        "grad_norm": 1.988053798675537,
        "learning_rate": 7.88023468801416e-05,
        "epoch": 0.8566308243727598,
        "step": 6214
    },
    {
        "loss": 2.1541,
        "grad_norm": 1.510039210319519,
        "learning_rate": 7.86749531514403e-05,
        "epoch": 0.8567686793493245,
        "step": 6215
    },
    {
        "loss": 1.978,
        "grad_norm": 2.1442463397979736,
        "learning_rate": 7.854759567004545e-05,
        "epoch": 0.8569065343258891,
        "step": 6216
    },
    {
        "loss": 1.3807,
        "grad_norm": 2.7930803298950195,
        "learning_rate": 7.842027465243298e-05,
        "epoch": 0.8570443893024539,
        "step": 6217
    },
    {
        "loss": 1.1927,
        "grad_norm": 2.769040822982788,
        "learning_rate": 7.829299031501726e-05,
        "epoch": 0.8571822442790185,
        "step": 6218
    },
    {
        "loss": 2.1882,
        "grad_norm": 1.1822012662887573,
        "learning_rate": 7.816574287415037e-05,
        "epoch": 0.8573200992555832,
        "step": 6219
    },
    {
        "loss": 1.1403,
        "grad_norm": 2.365725517272949,
        "learning_rate": 7.803853254612119e-05,
        "epoch": 0.8574579542321478,
        "step": 6220
    },
    {
        "loss": 2.2837,
        "grad_norm": 2.061187267303467,
        "learning_rate": 7.791135954715612e-05,
        "epoch": 0.8575958092087125,
        "step": 6221
    },
    {
        "loss": 1.8485,
        "grad_norm": 1.8670793771743774,
        "learning_rate": 7.778422409341764e-05,
        "epoch": 0.8577336641852771,
        "step": 6222
    },
    {
        "loss": 1.4147,
        "grad_norm": 2.1013729572296143,
        "learning_rate": 7.765712640100444e-05,
        "epoch": 0.8578715191618418,
        "step": 6223
    },
    {
        "loss": 1.7835,
        "grad_norm": 1.6405366659164429,
        "learning_rate": 7.753006668595136e-05,
        "epoch": 0.8580093741384064,
        "step": 6224
    },
    {
        "loss": 2.3575,
        "grad_norm": 1.5659195184707642,
        "learning_rate": 7.740304516422858e-05,
        "epoch": 0.8581472291149711,
        "step": 6225
    },
    {
        "loss": 1.7671,
        "grad_norm": 2.4373137950897217,
        "learning_rate": 7.727606205174114e-05,
        "epoch": 0.8582850840915357,
        "step": 6226
    },
    {
        "loss": 2.0642,
        "grad_norm": 1.8721781969070435,
        "learning_rate": 7.714911756432899e-05,
        "epoch": 0.8584229390681004,
        "step": 6227
    },
    {
        "loss": 1.7645,
        "grad_norm": 1.6422005891799927,
        "learning_rate": 7.702221191776617e-05,
        "epoch": 0.858560794044665,
        "step": 6228
    },
    {
        "loss": 1.7708,
        "grad_norm": 1.47156822681427,
        "learning_rate": 7.689534532776103e-05,
        "epoch": 0.8586986490212297,
        "step": 6229
    },
    {
        "loss": 2.0777,
        "grad_norm": 2.1363658905029297,
        "learning_rate": 7.676851800995558e-05,
        "epoch": 0.8588365039977943,
        "step": 6230
    },
    {
        "loss": 1.9364,
        "grad_norm": 1.7220741510391235,
        "learning_rate": 7.664173017992462e-05,
        "epoch": 0.8589743589743589,
        "step": 6231
    },
    {
        "loss": 2.1638,
        "grad_norm": 2.6970348358154297,
        "learning_rate": 7.65149820531762e-05,
        "epoch": 0.8591122139509236,
        "step": 6232
    },
    {
        "loss": 1.3875,
        "grad_norm": 3.2699086666107178,
        "learning_rate": 7.638827384515096e-05,
        "epoch": 0.8592500689274882,
        "step": 6233
    },
    {
        "loss": 1.817,
        "grad_norm": 1.8405438661575317,
        "learning_rate": 7.626160577122141e-05,
        "epoch": 0.8593879239040529,
        "step": 6234
    },
    {
        "loss": 1.0383,
        "grad_norm": 2.748913049697876,
        "learning_rate": 7.61349780466919e-05,
        "epoch": 0.8595257788806175,
        "step": 6235
    },
    {
        "loss": 2.3162,
        "grad_norm": 1.8770499229431152,
        "learning_rate": 7.600839088679814e-05,
        "epoch": 0.8596636338571823,
        "step": 6236
    },
    {
        "loss": 1.0436,
        "grad_norm": 1.1847000122070312,
        "learning_rate": 7.588184450670715e-05,
        "epoch": 0.8598014888337469,
        "step": 6237
    },
    {
        "loss": 1.4717,
        "grad_norm": 1.9663845300674438,
        "learning_rate": 7.57553391215166e-05,
        "epoch": 0.8599393438103116,
        "step": 6238
    },
    {
        "loss": 1.9497,
        "grad_norm": 2.8729076385498047,
        "learning_rate": 7.56288749462542e-05,
        "epoch": 0.8600771987868762,
        "step": 6239
    },
    {
        "loss": 2.264,
        "grad_norm": 1.8830937147140503,
        "learning_rate": 7.550245219587769e-05,
        "epoch": 0.8602150537634409,
        "step": 6240
    },
    {
        "loss": 1.9507,
        "grad_norm": 1.694060206413269,
        "learning_rate": 7.537607108527474e-05,
        "epoch": 0.8603529087400055,
        "step": 6241
    },
    {
        "loss": 1.4148,
        "grad_norm": 3.136168956756592,
        "learning_rate": 7.52497318292617e-05,
        "epoch": 0.8604907637165702,
        "step": 6242
    },
    {
        "loss": 2.1094,
        "grad_norm": 2.2483866214752197,
        "learning_rate": 7.512343464258422e-05,
        "epoch": 0.8606286186931348,
        "step": 6243
    },
    {
        "loss": 2.0946,
        "grad_norm": 1.7129994630813599,
        "learning_rate": 7.499717973991638e-05,
        "epoch": 0.8607664736696995,
        "step": 6244
    },
    {
        "loss": 1.7002,
        "grad_norm": 2.724532127380371,
        "learning_rate": 7.487096733586005e-05,
        "epoch": 0.8609043286462641,
        "step": 6245
    },
    {
        "loss": 2.0873,
        "grad_norm": 2.5200071334838867,
        "learning_rate": 7.474479764494542e-05,
        "epoch": 0.8610421836228288,
        "step": 6246
    },
    {
        "loss": 0.8732,
        "grad_norm": 2.3412978649139404,
        "learning_rate": 7.461867088162954e-05,
        "epoch": 0.8611800385993934,
        "step": 6247
    },
    {
        "loss": 2.2194,
        "grad_norm": 1.2369015216827393,
        "learning_rate": 7.449258726029667e-05,
        "epoch": 0.8613178935759581,
        "step": 6248
    },
    {
        "loss": 1.8437,
        "grad_norm": 1.7277477979660034,
        "learning_rate": 7.436654699525801e-05,
        "epoch": 0.8614557485525227,
        "step": 6249
    },
    {
        "loss": 2.0085,
        "grad_norm": 2.18831205368042,
        "learning_rate": 7.424055030075057e-05,
        "epoch": 0.8615936035290874,
        "step": 6250
    },
    {
        "loss": 1.7535,
        "grad_norm": 3.283907890319824,
        "learning_rate": 7.411459739093787e-05,
        "epoch": 0.861731458505652,
        "step": 6251
    },
    {
        "loss": 2.0543,
        "grad_norm": 1.825727105140686,
        "learning_rate": 7.398868847990857e-05,
        "epoch": 0.8618693134822167,
        "step": 6252
    },
    {
        "loss": 1.9122,
        "grad_norm": 1.7567368745803833,
        "learning_rate": 7.38628237816765e-05,
        "epoch": 0.8620071684587813,
        "step": 6253
    },
    {
        "loss": 1.8771,
        "grad_norm": 1.9615854024887085,
        "learning_rate": 7.373700351018086e-05,
        "epoch": 0.862145023435346,
        "step": 6254
    },
    {
        "loss": 1.8218,
        "grad_norm": 1.8266804218292236,
        "learning_rate": 7.361122787928472e-05,
        "epoch": 0.8622828784119106,
        "step": 6255
    },
    {
        "loss": 2.2664,
        "grad_norm": 1.6768375635147095,
        "learning_rate": 7.348549710277564e-05,
        "epoch": 0.8624207333884754,
        "step": 6256
    },
    {
        "loss": 2.0044,
        "grad_norm": 2.75531268119812,
        "learning_rate": 7.335981139436501e-05,
        "epoch": 0.86255858836504,
        "step": 6257
    },
    {
        "loss": 2.6083,
        "grad_norm": 1.3720650672912598,
        "learning_rate": 7.323417096768719e-05,
        "epoch": 0.8626964433416047,
        "step": 6258
    },
    {
        "loss": 2.1389,
        "grad_norm": 2.007977247238159,
        "learning_rate": 7.31085760363001e-05,
        "epoch": 0.8628342983181693,
        "step": 6259
    },
    {
        "loss": 1.9332,
        "grad_norm": 2.3119313716888428,
        "learning_rate": 7.298302681368393e-05,
        "epoch": 0.862972153294734,
        "step": 6260
    },
    {
        "loss": 0.9957,
        "grad_norm": 2.3682518005371094,
        "learning_rate": 7.285752351324118e-05,
        "epoch": 0.8631100082712986,
        "step": 6261
    },
    {
        "loss": 1.8348,
        "grad_norm": 2.2640492916107178,
        "learning_rate": 7.273206634829656e-05,
        "epoch": 0.8632478632478633,
        "step": 6262
    },
    {
        "loss": 1.7804,
        "grad_norm": 3.0932278633117676,
        "learning_rate": 7.260665553209637e-05,
        "epoch": 0.8633857182244279,
        "step": 6263
    },
    {
        "loss": 1.2939,
        "grad_norm": 4.5371174812316895,
        "learning_rate": 7.248129127780782e-05,
        "epoch": 0.8635235732009926,
        "step": 6264
    },
    {
        "loss": 2.1569,
        "grad_norm": 2.740661382675171,
        "learning_rate": 7.235597379851919e-05,
        "epoch": 0.8636614281775572,
        "step": 6265
    },
    {
        "loss": 1.5546,
        "grad_norm": 1.8156129121780396,
        "learning_rate": 7.223070330723903e-05,
        "epoch": 0.8637992831541219,
        "step": 6266
    },
    {
        "loss": 1.1605,
        "grad_norm": 3.8772132396698,
        "learning_rate": 7.210548001689634e-05,
        "epoch": 0.8639371381306865,
        "step": 6267
    },
    {
        "loss": 2.4261,
        "grad_norm": 1.1418476104736328,
        "learning_rate": 7.198030414033989e-05,
        "epoch": 0.8640749931072512,
        "step": 6268
    },
    {
        "loss": 1.6161,
        "grad_norm": 3.1187901496887207,
        "learning_rate": 7.185517589033746e-05,
        "epoch": 0.8642128480838158,
        "step": 6269
    },
    {
        "loss": 1.7729,
        "grad_norm": 2.1483848094940186,
        "learning_rate": 7.173009547957626e-05,
        "epoch": 0.8643507030603805,
        "step": 6270
    },
    {
        "loss": 0.9288,
        "grad_norm": 2.5149762630462646,
        "learning_rate": 7.160506312066217e-05,
        "epoch": 0.8644885580369451,
        "step": 6271
    },
    {
        "loss": 1.5685,
        "grad_norm": 3.2260515689849854,
        "learning_rate": 7.148007902611916e-05,
        "epoch": 0.8646264130135098,
        "step": 6272
    },
    {
        "loss": 2.4559,
        "grad_norm": 1.9507135152816772,
        "learning_rate": 7.135514340838931e-05,
        "epoch": 0.8647642679900744,
        "step": 6273
    },
    {
        "loss": 1.8185,
        "grad_norm": 2.4874937534332275,
        "learning_rate": 7.12302564798321e-05,
        "epoch": 0.864902122966639,
        "step": 6274
    },
    {
        "loss": 2.0423,
        "grad_norm": 1.5218487977981567,
        "learning_rate": 7.110541845272458e-05,
        "epoch": 0.8650399779432038,
        "step": 6275
    },
    {
        "loss": 2.0509,
        "grad_norm": 2.045515537261963,
        "learning_rate": 7.098062953926068e-05,
        "epoch": 0.8651778329197684,
        "step": 6276
    },
    {
        "loss": 2.3649,
        "grad_norm": 1.4181785583496094,
        "learning_rate": 7.085588995155052e-05,
        "epoch": 0.8653156878963331,
        "step": 6277
    },
    {
        "loss": 0.9676,
        "grad_norm": 1.734820008277893,
        "learning_rate": 7.073119990162044e-05,
        "epoch": 0.8654535428728977,
        "step": 6278
    },
    {
        "loss": 2.2249,
        "grad_norm": 1.156178593635559,
        "learning_rate": 7.060655960141297e-05,
        "epoch": 0.8655913978494624,
        "step": 6279
    },
    {
        "loss": 1.9398,
        "grad_norm": 1.6175552606582642,
        "learning_rate": 7.04819692627855e-05,
        "epoch": 0.865729252826027,
        "step": 6280
    },
    {
        "loss": 1.2527,
        "grad_norm": 2.345109462738037,
        "learning_rate": 7.035742909751096e-05,
        "epoch": 0.8658671078025917,
        "step": 6281
    },
    {
        "loss": 2.0876,
        "grad_norm": 2.224614381790161,
        "learning_rate": 7.023293931727694e-05,
        "epoch": 0.8660049627791563,
        "step": 6282
    },
    {
        "loss": 1.7712,
        "grad_norm": 2.0586047172546387,
        "learning_rate": 7.010850013368505e-05,
        "epoch": 0.866142817755721,
        "step": 6283
    },
    {
        "loss": 2.1279,
        "grad_norm": 1.0532225370407104,
        "learning_rate": 6.998411175825139e-05,
        "epoch": 0.8662806727322856,
        "step": 6284
    },
    {
        "loss": 1.5784,
        "grad_norm": 2.56594181060791,
        "learning_rate": 6.985977440240528e-05,
        "epoch": 0.8664185277088503,
        "step": 6285
    },
    {
        "loss": 1.7159,
        "grad_norm": 1.7900457382202148,
        "learning_rate": 6.973548827748935e-05,
        "epoch": 0.8665563826854149,
        "step": 6286
    },
    {
        "loss": 2.1637,
        "grad_norm": 1.3766597509384155,
        "learning_rate": 6.961125359475958e-05,
        "epoch": 0.8666942376619796,
        "step": 6287
    },
    {
        "loss": 1.3611,
        "grad_norm": 2.278360366821289,
        "learning_rate": 6.948707056538388e-05,
        "epoch": 0.8668320926385442,
        "step": 6288
    },
    {
        "loss": 1.9714,
        "grad_norm": 1.770548701286316,
        "learning_rate": 6.936293940044302e-05,
        "epoch": 0.8669699476151089,
        "step": 6289
    },
    {
        "loss": 2.619,
        "grad_norm": 1.417128086090088,
        "learning_rate": 6.923886031092913e-05,
        "epoch": 0.8671078025916735,
        "step": 6290
    },
    {
        "loss": 1.7696,
        "grad_norm": 2.403568983078003,
        "learning_rate": 6.911483350774583e-05,
        "epoch": 0.8672456575682382,
        "step": 6291
    },
    {
        "loss": 2.1591,
        "grad_norm": 1.3256051540374756,
        "learning_rate": 6.899085920170829e-05,
        "epoch": 0.8673835125448028,
        "step": 6292
    },
    {
        "loss": 2.3167,
        "grad_norm": 1.8011056184768677,
        "learning_rate": 6.886693760354193e-05,
        "epoch": 0.8675213675213675,
        "step": 6293
    },
    {
        "loss": 1.5637,
        "grad_norm": 1.6643226146697998,
        "learning_rate": 6.874306892388295e-05,
        "epoch": 0.8676592224979321,
        "step": 6294
    },
    {
        "loss": 1.0036,
        "grad_norm": 2.6218276023864746,
        "learning_rate": 6.861925337327757e-05,
        "epoch": 0.8677970774744969,
        "step": 6295
    },
    {
        "loss": 1.4581,
        "grad_norm": 3.1338911056518555,
        "learning_rate": 6.849549116218138e-05,
        "epoch": 0.8679349324510615,
        "step": 6296
    },
    {
        "loss": 1.5653,
        "grad_norm": 1.5785831212997437,
        "learning_rate": 6.83717825009598e-05,
        "epoch": 0.8680727874276262,
        "step": 6297
    },
    {
        "loss": 1.292,
        "grad_norm": 1.7835334539413452,
        "learning_rate": 6.824812759988678e-05,
        "epoch": 0.8682106424041908,
        "step": 6298
    },
    {
        "loss": 1.8634,
        "grad_norm": 1.3793178796768188,
        "learning_rate": 6.8124526669145e-05,
        "epoch": 0.8683484973807555,
        "step": 6299
    },
    {
        "loss": 2.2039,
        "grad_norm": 2.387605905532837,
        "learning_rate": 6.80009799188256e-05,
        "epoch": 0.8684863523573201,
        "step": 6300
    },
    {
        "loss": 2.8165,
        "grad_norm": 2.294654130935669,
        "learning_rate": 6.787748755892763e-05,
        "epoch": 0.8686242073338848,
        "step": 6301
    },
    {
        "loss": 2.1064,
        "grad_norm": 1.6190454959869385,
        "learning_rate": 6.77540497993574e-05,
        "epoch": 0.8687620623104494,
        "step": 6302
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.830709457397461,
        "learning_rate": 6.763066684992864e-05,
        "epoch": 0.8688999172870141,
        "step": 6303
    },
    {
        "loss": 1.6064,
        "grad_norm": 1.22306489944458,
        "learning_rate": 6.750733892036167e-05,
        "epoch": 0.8690377722635787,
        "step": 6304
    },
    {
        "loss": 1.8441,
        "grad_norm": 2.0782408714294434,
        "learning_rate": 6.738406622028363e-05,
        "epoch": 0.8691756272401434,
        "step": 6305
    },
    {
        "loss": 1.4971,
        "grad_norm": 1.4509752988815308,
        "learning_rate": 6.726084895922772e-05,
        "epoch": 0.869313482216708,
        "step": 6306
    },
    {
        "loss": 1.2612,
        "grad_norm": 2.515136957168579,
        "learning_rate": 6.713768734663262e-05,
        "epoch": 0.8694513371932727,
        "step": 6307
    },
    {
        "loss": 1.9375,
        "grad_norm": 2.871978521347046,
        "learning_rate": 6.701458159184273e-05,
        "epoch": 0.8695891921698373,
        "step": 6308
    },
    {
        "loss": 2.4574,
        "grad_norm": 1.2753639221191406,
        "learning_rate": 6.689153190410745e-05,
        "epoch": 0.869727047146402,
        "step": 6309
    },
    {
        "loss": 2.131,
        "grad_norm": 1.8251007795333862,
        "learning_rate": 6.676853849258076e-05,
        "epoch": 0.8698649021229666,
        "step": 6310
    },
    {
        "loss": 2.1197,
        "grad_norm": 1.2235506772994995,
        "learning_rate": 6.664560156632103e-05,
        "epoch": 0.8700027570995313,
        "step": 6311
    },
    {
        "loss": 2.3149,
        "grad_norm": 2.156099557876587,
        "learning_rate": 6.65227213342905e-05,
        "epoch": 0.8701406120760959,
        "step": 6312
    },
    {
        "loss": 2.1197,
        "grad_norm": 1.866911768913269,
        "learning_rate": 6.639989800535531e-05,
        "epoch": 0.8702784670526607,
        "step": 6313
    },
    {
        "loss": 1.9239,
        "grad_norm": 1.6978248357772827,
        "learning_rate": 6.627713178828489e-05,
        "epoch": 0.8704163220292253,
        "step": 6314
    },
    {
        "loss": 1.3843,
        "grad_norm": 2.8397650718688965,
        "learning_rate": 6.615442289175128e-05,
        "epoch": 0.87055417700579,
        "step": 6315
    },
    {
        "loss": 2.099,
        "grad_norm": 2.00046706199646,
        "learning_rate": 6.603177152432916e-05,
        "epoch": 0.8706920319823546,
        "step": 6316
    },
    {
        "loss": 1.5574,
        "grad_norm": 1.7955964803695679,
        "learning_rate": 6.590917789449579e-05,
        "epoch": 0.8708298869589192,
        "step": 6317
    },
    {
        "loss": 1.8977,
        "grad_norm": 3.0371510982513428,
        "learning_rate": 6.578664221062978e-05,
        "epoch": 0.8709677419354839,
        "step": 6318
    },
    {
        "loss": 1.6321,
        "grad_norm": 1.8705875873565674,
        "learning_rate": 6.566416468101156e-05,
        "epoch": 0.8711055969120485,
        "step": 6319
    },
    {
        "loss": 1.5484,
        "grad_norm": 2.3604085445404053,
        "learning_rate": 6.554174551382282e-05,
        "epoch": 0.8712434518886132,
        "step": 6320
    },
    {
        "loss": 1.8223,
        "grad_norm": 1.4059629440307617,
        "learning_rate": 6.541938491714561e-05,
        "epoch": 0.8713813068651778,
        "step": 6321
    },
    {
        "loss": 0.9283,
        "grad_norm": 2.343372106552124,
        "learning_rate": 6.529708309896297e-05,
        "epoch": 0.8715191618417425,
        "step": 6322
    },
    {
        "loss": 2.2127,
        "grad_norm": 2.1129915714263916,
        "learning_rate": 6.517484026715754e-05,
        "epoch": 0.8716570168183071,
        "step": 6323
    },
    {
        "loss": 1.8133,
        "grad_norm": 2.8383877277374268,
        "learning_rate": 6.505265662951184e-05,
        "epoch": 0.8717948717948718,
        "step": 6324
    },
    {
        "loss": 1.542,
        "grad_norm": 2.4730734825134277,
        "learning_rate": 6.49305323937081e-05,
        "epoch": 0.8719327267714364,
        "step": 6325
    },
    {
        "loss": 2.1224,
        "grad_norm": 1.3707644939422607,
        "learning_rate": 6.480846776732704e-05,
        "epoch": 0.8720705817480011,
        "step": 6326
    },
    {
        "loss": 2.4024,
        "grad_norm": 1.2442355155944824,
        "learning_rate": 6.46864629578486e-05,
        "epoch": 0.8722084367245657,
        "step": 6327
    },
    {
        "loss": 1.1811,
        "grad_norm": 1.6654155254364014,
        "learning_rate": 6.456451817265066e-05,
        "epoch": 0.8723462917011304,
        "step": 6328
    },
    {
        "loss": 1.2562,
        "grad_norm": 2.441136598587036,
        "learning_rate": 6.444263361900907e-05,
        "epoch": 0.872484146677695,
        "step": 6329
    },
    {
        "loss": 2.2568,
        "grad_norm": 2.1017539501190186,
        "learning_rate": 6.432080950409769e-05,
        "epoch": 0.8726220016542597,
        "step": 6330
    },
    {
        "loss": 2.2294,
        "grad_norm": 1.3532568216323853,
        "learning_rate": 6.419904603498714e-05,
        "epoch": 0.8727598566308243,
        "step": 6331
    },
    {
        "loss": 1.6007,
        "grad_norm": 2.832019090652466,
        "learning_rate": 6.407734341864536e-05,
        "epoch": 0.872897711607389,
        "step": 6332
    },
    {
        "loss": 2.5213,
        "grad_norm": 1.3652793169021606,
        "learning_rate": 6.395570186193675e-05,
        "epoch": 0.8730355665839536,
        "step": 6333
    },
    {
        "loss": 1.832,
        "grad_norm": 1.9010785818099976,
        "learning_rate": 6.383412157162169e-05,
        "epoch": 0.8731734215605184,
        "step": 6334
    },
    {
        "loss": 1.7508,
        "grad_norm": 1.3493151664733887,
        "learning_rate": 6.371260275435686e-05,
        "epoch": 0.873311276537083,
        "step": 6335
    },
    {
        "loss": 1.9042,
        "grad_norm": 1.9267311096191406,
        "learning_rate": 6.359114561669403e-05,
        "epoch": 0.8734491315136477,
        "step": 6336
    },
    {
        "loss": 2.1525,
        "grad_norm": 1.8183326721191406,
        "learning_rate": 6.346975036508022e-05,
        "epoch": 0.8735869864902123,
        "step": 6337
    },
    {
        "loss": 1.8359,
        "grad_norm": 1.8996448516845703,
        "learning_rate": 6.334841720585744e-05,
        "epoch": 0.873724841466777,
        "step": 6338
    },
    {
        "loss": 1.3886,
        "grad_norm": 2.10160231590271,
        "learning_rate": 6.322714634526221e-05,
        "epoch": 0.8738626964433416,
        "step": 6339
    },
    {
        "loss": 1.7555,
        "grad_norm": 1.257656455039978,
        "learning_rate": 6.310593798942483e-05,
        "epoch": 0.8740005514199063,
        "step": 6340
    },
    {
        "loss": 0.7585,
        "grad_norm": Infinity,
        "learning_rate": 6.310593798942483e-05,
        "epoch": 0.8741384063964709,
        "step": 6341
    },
    {
        "loss": 1.1928,
        "grad_norm": 2.6796746253967285,
        "learning_rate": 6.298479234436959e-05,
        "epoch": 0.8742762613730356,
        "step": 6342
    },
    {
        "loss": 1.4842,
        "grad_norm": 2.3814685344696045,
        "learning_rate": 6.286370961601395e-05,
        "epoch": 0.8744141163496002,
        "step": 6343
    },
    {
        "loss": 2.0839,
        "grad_norm": 1.5604784488677979,
        "learning_rate": 6.274269001016874e-05,
        "epoch": 0.8745519713261649,
        "step": 6344
    },
    {
        "loss": 1.4126,
        "grad_norm": 3.107142925262451,
        "learning_rate": 6.262173373253752e-05,
        "epoch": 0.8746898263027295,
        "step": 6345
    },
    {
        "loss": 2.0224,
        "grad_norm": 2.1873512268066406,
        "learning_rate": 6.250084098871579e-05,
        "epoch": 0.8748276812792942,
        "step": 6346
    },
    {
        "loss": 2.1565,
        "grad_norm": 2.2273497581481934,
        "learning_rate": 6.238001198419144e-05,
        "epoch": 0.8749655362558588,
        "step": 6347
    },
    {
        "loss": 2.4877,
        "grad_norm": 2.0872344970703125,
        "learning_rate": 6.225924692434404e-05,
        "epoch": 0.8751033912324235,
        "step": 6348
    },
    {
        "loss": 1.1167,
        "grad_norm": 2.5813331604003906,
        "learning_rate": 6.213854601444416e-05,
        "epoch": 0.8752412462089881,
        "step": 6349
    },
    {
        "loss": 1.9703,
        "grad_norm": 1.567731261253357,
        "learning_rate": 6.201790945965354e-05,
        "epoch": 0.8753791011855528,
        "step": 6350
    },
    {
        "loss": 1.8353,
        "grad_norm": 1.977535605430603,
        "learning_rate": 6.189733746502436e-05,
        "epoch": 0.8755169561621174,
        "step": 6351
    },
    {
        "loss": 2.1161,
        "grad_norm": 2.590590000152588,
        "learning_rate": 6.17768302354993e-05,
        "epoch": 0.8756548111386822,
        "step": 6352
    },
    {
        "loss": 2.3394,
        "grad_norm": 2.1680002212524414,
        "learning_rate": 6.165638797591098e-05,
        "epoch": 0.8757926661152468,
        "step": 6353
    },
    {
        "loss": 1.8996,
        "grad_norm": 1.2340636253356934,
        "learning_rate": 6.153601089098137e-05,
        "epoch": 0.8759305210918115,
        "step": 6354
    },
    {
        "loss": 1.7759,
        "grad_norm": 1.4185019731521606,
        "learning_rate": 6.141569918532157e-05,
        "epoch": 0.8760683760683761,
        "step": 6355
    },
    {
        "loss": 2.0586,
        "grad_norm": 2.005340099334717,
        "learning_rate": 6.1295453063432e-05,
        "epoch": 0.8762062310449408,
        "step": 6356
    },
    {
        "loss": 1.6611,
        "grad_norm": 2.1240148544311523,
        "learning_rate": 6.117527272970111e-05,
        "epoch": 0.8763440860215054,
        "step": 6357
    },
    {
        "loss": 2.0975,
        "grad_norm": 2.0737087726593018,
        "learning_rate": 6.105515838840584e-05,
        "epoch": 0.87648194099807,
        "step": 6358
    },
    {
        "loss": 0.9646,
        "grad_norm": 3.670657157897949,
        "learning_rate": 6.093511024371099e-05,
        "epoch": 0.8766197959746347,
        "step": 6359
    },
    {
        "loss": 1.442,
        "grad_norm": 1.9727423191070557,
        "learning_rate": 6.0815128499668486e-05,
        "epoch": 0.8767576509511993,
        "step": 6360
    },
    {
        "loss": 2.5123,
        "grad_norm": 1.7742691040039062,
        "learning_rate": 6.069521336021788e-05,
        "epoch": 0.876895505927764,
        "step": 6361
    },
    {
        "loss": 1.8733,
        "grad_norm": 1.8901337385177612,
        "learning_rate": 6.05753650291851e-05,
        "epoch": 0.8770333609043286,
        "step": 6362
    },
    {
        "loss": 2.0652,
        "grad_norm": 1.8759604692459106,
        "learning_rate": 6.045558371028259e-05,
        "epoch": 0.8771712158808933,
        "step": 6363
    },
    {
        "loss": 2.1755,
        "grad_norm": 1.684410572052002,
        "learning_rate": 6.03358696071092e-05,
        "epoch": 0.8773090708574579,
        "step": 6364
    },
    {
        "loss": 1.8621,
        "grad_norm": 1.3736910820007324,
        "learning_rate": 6.021622292314904e-05,
        "epoch": 0.8774469258340226,
        "step": 6365
    },
    {
        "loss": 1.705,
        "grad_norm": 2.7993650436401367,
        "learning_rate": 6.009664386177214e-05,
        "epoch": 0.8775847808105872,
        "step": 6366
    },
    {
        "loss": 0.8525,
        "grad_norm": 2.725010871887207,
        "learning_rate": 5.9977132626233236e-05,
        "epoch": 0.8777226357871519,
        "step": 6367
    },
    {
        "loss": 2.1909,
        "grad_norm": 1.4118207693099976,
        "learning_rate": 5.9857689419671714e-05,
        "epoch": 0.8778604907637165,
        "step": 6368
    },
    {
        "loss": 2.3824,
        "grad_norm": 3.151737928390503,
        "learning_rate": 5.9738314445111635e-05,
        "epoch": 0.8779983457402812,
        "step": 6369
    },
    {
        "loss": 1.9005,
        "grad_norm": 2.9501354694366455,
        "learning_rate": 5.961900790546104e-05,
        "epoch": 0.8781362007168458,
        "step": 6370
    },
    {
        "loss": 2.1086,
        "grad_norm": 1.4686208963394165,
        "learning_rate": 5.9499770003511325e-05,
        "epoch": 0.8782740556934105,
        "step": 6371
    },
    {
        "loss": 1.3071,
        "grad_norm": 2.0451624393463135,
        "learning_rate": 5.938060094193769e-05,
        "epoch": 0.8784119106699751,
        "step": 6372
    },
    {
        "loss": 1.3619,
        "grad_norm": 1.7668681144714355,
        "learning_rate": 5.926150092329778e-05,
        "epoch": 0.8785497656465399,
        "step": 6373
    },
    {
        "loss": 1.7843,
        "grad_norm": 2.486384153366089,
        "learning_rate": 5.914247015003247e-05,
        "epoch": 0.8786876206231045,
        "step": 6374
    },
    {
        "loss": 2.131,
        "grad_norm": 2.1000940799713135,
        "learning_rate": 5.90235088244645e-05,
        "epoch": 0.8788254755996692,
        "step": 6375
    },
    {
        "loss": 1.6062,
        "grad_norm": 2.187955856323242,
        "learning_rate": 5.890461714879856e-05,
        "epoch": 0.8789633305762338,
        "step": 6376
    },
    {
        "loss": 1.66,
        "grad_norm": 2.5239789485931396,
        "learning_rate": 5.878579532512123e-05,
        "epoch": 0.8791011855527985,
        "step": 6377
    },
    {
        "loss": 2.1513,
        "grad_norm": 1.9288533926010132,
        "learning_rate": 5.866704355540038e-05,
        "epoch": 0.8792390405293631,
        "step": 6378
    },
    {
        "loss": 1.8942,
        "grad_norm": 1.3846882581710815,
        "learning_rate": 5.854836204148451e-05,
        "epoch": 0.8793768955059278,
        "step": 6379
    },
    {
        "loss": 1.975,
        "grad_norm": 2.2125275135040283,
        "learning_rate": 5.8429750985102825e-05,
        "epoch": 0.8795147504824924,
        "step": 6380
    },
    {
        "loss": 2.5508,
        "grad_norm": 1.2124879360198975,
        "learning_rate": 5.8311210587864697e-05,
        "epoch": 0.8796526054590571,
        "step": 6381
    },
    {
        "loss": 2.0437,
        "grad_norm": 1.2871357202529907,
        "learning_rate": 5.81927410512596e-05,
        "epoch": 0.8797904604356217,
        "step": 6382
    },
    {
        "loss": 1.7843,
        "grad_norm": 2.515641212463379,
        "learning_rate": 5.8074342576656624e-05,
        "epoch": 0.8799283154121864,
        "step": 6383
    },
    {
        "loss": 1.0655,
        "grad_norm": 2.1094918251037598,
        "learning_rate": 5.795601536530355e-05,
        "epoch": 0.880066170388751,
        "step": 6384
    },
    {
        "loss": 1.8271,
        "grad_norm": 1.7970105409622192,
        "learning_rate": 5.783775961832757e-05,
        "epoch": 0.8802040253653157,
        "step": 6385
    },
    {
        "loss": 2.1596,
        "grad_norm": 2.724588632583618,
        "learning_rate": 5.7719575536734306e-05,
        "epoch": 0.8803418803418803,
        "step": 6386
    },
    {
        "loss": 2.1097,
        "grad_norm": 1.7984763383865356,
        "learning_rate": 5.760146332140737e-05,
        "epoch": 0.880479735318445,
        "step": 6387
    },
    {
        "loss": 1.6778,
        "grad_norm": 2.6129555702209473,
        "learning_rate": 5.748342317310817e-05,
        "epoch": 0.8806175902950096,
        "step": 6388
    },
    {
        "loss": 2.0608,
        "grad_norm": 2.368549108505249,
        "learning_rate": 5.7365455292476054e-05,
        "epoch": 0.8807554452715743,
        "step": 6389
    },
    {
        "loss": 0.4872,
        "grad_norm": 2.0744006633758545,
        "learning_rate": 5.7247559880026966e-05,
        "epoch": 0.8808933002481389,
        "step": 6390
    },
    {
        "loss": 0.9468,
        "grad_norm": 2.8145267963409424,
        "learning_rate": 5.712973713615426e-05,
        "epoch": 0.8810311552247037,
        "step": 6391
    },
    {
        "loss": 2.4922,
        "grad_norm": 1.9541223049163818,
        "learning_rate": 5.701198726112727e-05,
        "epoch": 0.8811690102012683,
        "step": 6392
    },
    {
        "loss": 2.088,
        "grad_norm": 1.7699706554412842,
        "learning_rate": 5.689431045509155e-05,
        "epoch": 0.881306865177833,
        "step": 6393
    },
    {
        "loss": 2.3717,
        "grad_norm": 1.721599817276001,
        "learning_rate": 5.677670691806882e-05,
        "epoch": 0.8814447201543976,
        "step": 6394
    },
    {
        "loss": 2.0012,
        "grad_norm": 2.312221050262451,
        "learning_rate": 5.665917684995581e-05,
        "epoch": 0.8815825751309623,
        "step": 6395
    },
    {
        "loss": 2.0359,
        "grad_norm": 1.5561022758483887,
        "learning_rate": 5.654172045052466e-05,
        "epoch": 0.8817204301075269,
        "step": 6396
    },
    {
        "loss": 1.8607,
        "grad_norm": 2.8627915382385254,
        "learning_rate": 5.6424337919422364e-05,
        "epoch": 0.8818582850840916,
        "step": 6397
    },
    {
        "loss": 1.3672,
        "grad_norm": 3.302950143814087,
        "learning_rate": 5.63070294561699e-05,
        "epoch": 0.8819961400606562,
        "step": 6398
    },
    {
        "loss": 2.1234,
        "grad_norm": 1.327457070350647,
        "learning_rate": 5.6189795260162905e-05,
        "epoch": 0.8821339950372209,
        "step": 6399
    },
    {
        "loss": 2.0472,
        "grad_norm": 1.64436674118042,
        "learning_rate": 5.6072635530670395e-05,
        "epoch": 0.8822718500137855,
        "step": 6400
    },
    {
        "loss": 1.8883,
        "grad_norm": 2.0210647583007812,
        "learning_rate": 5.595555046683485e-05,
        "epoch": 0.8824097049903501,
        "step": 6401
    },
    {
        "loss": 0.6409,
        "grad_norm": 2.756803512573242,
        "learning_rate": 5.5838540267672135e-05,
        "epoch": 0.8825475599669148,
        "step": 6402
    },
    {
        "loss": 1.9762,
        "grad_norm": 1.443886399269104,
        "learning_rate": 5.572160513207043e-05,
        "epoch": 0.8826854149434794,
        "step": 6403
    },
    {
        "loss": 1.0144,
        "grad_norm": 1.5885605812072754,
        "learning_rate": 5.560474525879082e-05,
        "epoch": 0.8828232699200441,
        "step": 6404
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.8923124074935913,
        "learning_rate": 5.5487960846466016e-05,
        "epoch": 0.8829611248966087,
        "step": 6405
    },
    {
        "loss": 1.9429,
        "grad_norm": 1.355766773223877,
        "learning_rate": 5.537125209360052e-05,
        "epoch": 0.8830989798731734,
        "step": 6406
    },
    {
        "loss": 1.1338,
        "grad_norm": 2.2295615673065186,
        "learning_rate": 5.52546191985705e-05,
        "epoch": 0.883236834849738,
        "step": 6407
    },
    {
        "loss": 1.5521,
        "grad_norm": 2.2259011268615723,
        "learning_rate": 5.5138062359623155e-05,
        "epoch": 0.8833746898263027,
        "step": 6408
    },
    {
        "loss": 2.1265,
        "grad_norm": 1.6423791646957397,
        "learning_rate": 5.502158177487604e-05,
        "epoch": 0.8835125448028673,
        "step": 6409
    },
    {
        "loss": 1.2346,
        "grad_norm": 2.2893588542938232,
        "learning_rate": 5.490517764231759e-05,
        "epoch": 0.883650399779432,
        "step": 6410
    },
    {
        "loss": 1.9266,
        "grad_norm": 1.1118710041046143,
        "learning_rate": 5.478885015980579e-05,
        "epoch": 0.8837882547559966,
        "step": 6411
    },
    {
        "loss": 1.763,
        "grad_norm": 1.049669861793518,
        "learning_rate": 5.46725995250689e-05,
        "epoch": 0.8839261097325614,
        "step": 6412
    },
    {
        "loss": 2.1685,
        "grad_norm": 2.120469331741333,
        "learning_rate": 5.455642593570405e-05,
        "epoch": 0.884063964709126,
        "step": 6413
    },
    {
        "loss": 2.2416,
        "grad_norm": 1.4544708728790283,
        "learning_rate": 5.444032958917751e-05,
        "epoch": 0.8842018196856907,
        "step": 6414
    },
    {
        "loss": 1.9901,
        "grad_norm": 2.0744965076446533,
        "learning_rate": 5.432431068282449e-05,
        "epoch": 0.8843396746622553,
        "step": 6415
    },
    {
        "loss": 2.1622,
        "grad_norm": 2.010867118835449,
        "learning_rate": 5.420836941384857e-05,
        "epoch": 0.88447752963882,
        "step": 6416
    },
    {
        "loss": 1.0219,
        "grad_norm": 2.641223430633545,
        "learning_rate": 5.4092505979321114e-05,
        "epoch": 0.8846153846153846,
        "step": 6417
    },
    {
        "loss": 1.8906,
        "grad_norm": 1.6037609577178955,
        "learning_rate": 5.397672057618135e-05,
        "epoch": 0.8847532395919493,
        "step": 6418
    },
    {
        "loss": 1.6065,
        "grad_norm": 2.1254260540008545,
        "learning_rate": 5.386101340123564e-05,
        "epoch": 0.8848910945685139,
        "step": 6419
    },
    {
        "loss": 1.3,
        "grad_norm": 2.3508994579315186,
        "learning_rate": 5.37453846511577e-05,
        "epoch": 0.8850289495450786,
        "step": 6420
    },
    {
        "loss": 1.5252,
        "grad_norm": 2.6135880947113037,
        "learning_rate": 5.3629834522487956e-05,
        "epoch": 0.8851668045216432,
        "step": 6421
    },
    {
        "loss": 0.9902,
        "grad_norm": 3.640134334564209,
        "learning_rate": 5.351436321163281e-05,
        "epoch": 0.8853046594982079,
        "step": 6422
    },
    {
        "loss": 1.7226,
        "grad_norm": 1.4893180131912231,
        "learning_rate": 5.339897091486505e-05,
        "epoch": 0.8854425144747725,
        "step": 6423
    },
    {
        "loss": 2.0807,
        "grad_norm": 2.719247817993164,
        "learning_rate": 5.3283657828323184e-05,
        "epoch": 0.8855803694513372,
        "step": 6424
    },
    {
        "loss": 1.6137,
        "grad_norm": 2.9564192295074463,
        "learning_rate": 5.3168424148010845e-05,
        "epoch": 0.8857182244279018,
        "step": 6425
    },
    {
        "loss": 1.906,
        "grad_norm": 2.2678544521331787,
        "learning_rate": 5.305327006979659e-05,
        "epoch": 0.8858560794044665,
        "step": 6426
    },
    {
        "loss": 2.5753,
        "grad_norm": 1.5335019826889038,
        "learning_rate": 5.293819578941417e-05,
        "epoch": 0.8859939343810311,
        "step": 6427
    },
    {
        "loss": 1.9962,
        "grad_norm": 1.4867823123931885,
        "learning_rate": 5.282320150246116e-05,
        "epoch": 0.8861317893575958,
        "step": 6428
    },
    {
        "loss": 2.0426,
        "grad_norm": 2.1320741176605225,
        "learning_rate": 5.2708287404399634e-05,
        "epoch": 0.8862696443341604,
        "step": 6429
    },
    {
        "loss": 1.9149,
        "grad_norm": 2.1690430641174316,
        "learning_rate": 5.2593453690555064e-05,
        "epoch": 0.8864074993107252,
        "step": 6430
    },
    {
        "loss": 2.044,
        "grad_norm": 1.8418983221054077,
        "learning_rate": 5.247870055611621e-05,
        "epoch": 0.8865453542872898,
        "step": 6431
    },
    {
        "loss": 2.4334,
        "grad_norm": 2.5302209854125977,
        "learning_rate": 5.236402819613533e-05,
        "epoch": 0.8866832092638545,
        "step": 6432
    },
    {
        "loss": 1.8938,
        "grad_norm": 3.653809070587158,
        "learning_rate": 5.224943680552674e-05,
        "epoch": 0.8868210642404191,
        "step": 6433
    },
    {
        "loss": 1.8309,
        "grad_norm": 1.9647506475448608,
        "learning_rate": 5.213492657906764e-05,
        "epoch": 0.8869589192169838,
        "step": 6434
    },
    {
        "loss": 1.5318,
        "grad_norm": 2.6224660873413086,
        "learning_rate": 5.202049771139721e-05,
        "epoch": 0.8870967741935484,
        "step": 6435
    },
    {
        "loss": 1.5396,
        "grad_norm": 1.4434770345687866,
        "learning_rate": 5.190615039701598e-05,
        "epoch": 0.8872346291701131,
        "step": 6436
    },
    {
        "loss": 2.3091,
        "grad_norm": 1.7666144371032715,
        "learning_rate": 5.179188483028637e-05,
        "epoch": 0.8873724841466777,
        "step": 6437
    },
    {
        "loss": 2.5678,
        "grad_norm": 1.2268803119659424,
        "learning_rate": 5.167770120543145e-05,
        "epoch": 0.8875103391232424,
        "step": 6438
    },
    {
        "loss": 1.6925,
        "grad_norm": 1.8576154708862305,
        "learning_rate": 5.156359971653508e-05,
        "epoch": 0.887648194099807,
        "step": 6439
    },
    {
        "loss": 1.9689,
        "grad_norm": 1.6730002164840698,
        "learning_rate": 5.144958055754173e-05,
        "epoch": 0.8877860490763717,
        "step": 6440
    },
    {
        "loss": 2.0503,
        "grad_norm": 1.9581232070922852,
        "learning_rate": 5.133564392225557e-05,
        "epoch": 0.8879239040529363,
        "step": 6441
    },
    {
        "loss": 1.7595,
        "grad_norm": 2.8707728385925293,
        "learning_rate": 5.122179000434095e-05,
        "epoch": 0.888061759029501,
        "step": 6442
    },
    {
        "loss": 2.5956,
        "grad_norm": 1.25973641872406,
        "learning_rate": 5.110801899732126e-05,
        "epoch": 0.8881996140060656,
        "step": 6443
    },
    {
        "loss": 2.2744,
        "grad_norm": 1.1737054586410522,
        "learning_rate": 5.099433109457895e-05,
        "epoch": 0.8883374689826302,
        "step": 6444
    },
    {
        "loss": 0.9767,
        "grad_norm": 2.7292912006378174,
        "learning_rate": 5.088072648935547e-05,
        "epoch": 0.8884753239591949,
        "step": 6445
    },
    {
        "loss": 1.8973,
        "grad_norm": 2.6303956508636475,
        "learning_rate": 5.0767205374750704e-05,
        "epoch": 0.8886131789357595,
        "step": 6446
    },
    {
        "loss": 1.8818,
        "grad_norm": 1.2335935831069946,
        "learning_rate": 5.065376794372216e-05,
        "epoch": 0.8887510339123242,
        "step": 6447
    },
    {
        "loss": 0.9495,
        "grad_norm": 2.0396931171417236,
        "learning_rate": 5.054041438908572e-05,
        "epoch": 0.8888888888888888,
        "step": 6448
    },
    {
        "loss": 1.8878,
        "grad_norm": 2.19149112701416,
        "learning_rate": 5.0427144903514125e-05,
        "epoch": 0.8890267438654536,
        "step": 6449
    },
    {
        "loss": 2.2502,
        "grad_norm": 2.201262950897217,
        "learning_rate": 5.031395967953774e-05,
        "epoch": 0.8891645988420182,
        "step": 6450
    },
    {
        "loss": 1.5908,
        "grad_norm": 1.9900422096252441,
        "learning_rate": 5.020085890954335e-05,
        "epoch": 0.8893024538185829,
        "step": 6451
    },
    {
        "loss": 1.268,
        "grad_norm": 2.1675963401794434,
        "learning_rate": 5.008784278577416e-05,
        "epoch": 0.8894403087951475,
        "step": 6452
    },
    {
        "loss": 0.873,
        "grad_norm": 1.7156293392181396,
        "learning_rate": 4.9974911500329804e-05,
        "epoch": 0.8895781637717122,
        "step": 6453
    },
    {
        "loss": 1.5787,
        "grad_norm": 1.5759034156799316,
        "learning_rate": 4.986206524516558e-05,
        "epoch": 0.8897160187482768,
        "step": 6454
    },
    {
        "loss": 1.4462,
        "grad_norm": 2.084908962249756,
        "learning_rate": 4.974930421209213e-05,
        "epoch": 0.8898538737248415,
        "step": 6455
    },
    {
        "loss": 1.7258,
        "grad_norm": 1.50522780418396,
        "learning_rate": 4.963662859277532e-05,
        "epoch": 0.8899917287014061,
        "step": 6456
    },
    {
        "loss": 1.4233,
        "grad_norm": 2.2176027297973633,
        "learning_rate": 4.9524038578735755e-05,
        "epoch": 0.8901295836779708,
        "step": 6457
    },
    {
        "loss": 2.2974,
        "grad_norm": 1.3313357830047607,
        "learning_rate": 4.94115343613487e-05,
        "epoch": 0.8902674386545354,
        "step": 6458
    },
    {
        "loss": 1.5835,
        "grad_norm": 2.0718138217926025,
        "learning_rate": 4.9299116131843633e-05,
        "epoch": 0.8904052936311001,
        "step": 6459
    },
    {
        "loss": 1.6844,
        "grad_norm": 2.0720927715301514,
        "learning_rate": 4.918678408130355e-05,
        "epoch": 0.8905431486076647,
        "step": 6460
    },
    {
        "loss": 1.8941,
        "grad_norm": 1.6863640546798706,
        "learning_rate": 4.90745384006652e-05,
        "epoch": 0.8906810035842294,
        "step": 6461
    },
    {
        "loss": 1.8073,
        "grad_norm": 2.3375964164733887,
        "learning_rate": 4.896237928071864e-05,
        "epoch": 0.890818858560794,
        "step": 6462
    },
    {
        "loss": 0.6439,
        "grad_norm": 1.7001985311508179,
        "learning_rate": 4.885030691210652e-05,
        "epoch": 0.8909567135373587,
        "step": 6463
    },
    {
        "loss": 2.0514,
        "grad_norm": 2.3962583541870117,
        "learning_rate": 4.873832148532403e-05,
        "epoch": 0.8910945685139233,
        "step": 6464
    },
    {
        "loss": 2.0603,
        "grad_norm": 2.003528118133545,
        "learning_rate": 4.8626423190718964e-05,
        "epoch": 0.891232423490488,
        "step": 6465
    },
    {
        "loss": 2.0029,
        "grad_norm": 2.727238893508911,
        "learning_rate": 4.851461221849047e-05,
        "epoch": 0.8913702784670526,
        "step": 6466
    },
    {
        "loss": 1.8594,
        "grad_norm": 1.7588562965393066,
        "learning_rate": 4.840288875868987e-05,
        "epoch": 0.8915081334436173,
        "step": 6467
    },
    {
        "loss": 1.77,
        "grad_norm": 1.8134679794311523,
        "learning_rate": 4.829125300121919e-05,
        "epoch": 0.891645988420182,
        "step": 6468
    },
    {
        "loss": 1.914,
        "grad_norm": 2.1896209716796875,
        "learning_rate": 4.817970513583157e-05,
        "epoch": 0.8917838433967467,
        "step": 6469
    },
    {
        "loss": 1.1413,
        "grad_norm": 2.576789140701294,
        "learning_rate": 4.8068245352130995e-05,
        "epoch": 0.8919216983733113,
        "step": 6470
    },
    {
        "loss": 1.2779,
        "grad_norm": 2.3942067623138428,
        "learning_rate": 4.795687383957133e-05,
        "epoch": 0.892059553349876,
        "step": 6471
    },
    {
        "loss": 1.7363,
        "grad_norm": 2.0827882289886475,
        "learning_rate": 4.7845590787456696e-05,
        "epoch": 0.8921974083264406,
        "step": 6472
    },
    {
        "loss": 2.0595,
        "grad_norm": 1.8888999223709106,
        "learning_rate": 4.7734396384940896e-05,
        "epoch": 0.8923352633030053,
        "step": 6473
    },
    {
        "loss": 2.0476,
        "grad_norm": 2.351975679397583,
        "learning_rate": 4.762329082102672e-05,
        "epoch": 0.8924731182795699,
        "step": 6474
    },
    {
        "loss": 1.7319,
        "grad_norm": 1.6508305072784424,
        "learning_rate": 4.751227428456631e-05,
        "epoch": 0.8926109732561346,
        "step": 6475
    },
    {
        "loss": 1.6035,
        "grad_norm": 2.759737014770508,
        "learning_rate": 4.740134696426024e-05,
        "epoch": 0.8927488282326992,
        "step": 6476
    },
    {
        "loss": 2.0823,
        "grad_norm": 1.590478539466858,
        "learning_rate": 4.729050904865743e-05,
        "epoch": 0.8928866832092639,
        "step": 6477
    },
    {
        "loss": 1.56,
        "grad_norm": 2.3337244987487793,
        "learning_rate": 4.717976072615512e-05,
        "epoch": 0.8930245381858285,
        "step": 6478
    },
    {
        "loss": 2.0987,
        "grad_norm": 1.6402534246444702,
        "learning_rate": 4.706910218499788e-05,
        "epoch": 0.8931623931623932,
        "step": 6479
    },
    {
        "loss": 1.9442,
        "grad_norm": 1.2775356769561768,
        "learning_rate": 4.695853361327812e-05,
        "epoch": 0.8933002481389578,
        "step": 6480
    },
    {
        "loss": 2.1178,
        "grad_norm": 2.4693052768707275,
        "learning_rate": 4.684805519893487e-05,
        "epoch": 0.8934381031155225,
        "step": 6481
    },
    {
        "loss": 1.7816,
        "grad_norm": 2.258026599884033,
        "learning_rate": 4.673766712975405e-05,
        "epoch": 0.8935759580920871,
        "step": 6482
    },
    {
        "loss": 1.933,
        "grad_norm": 2.3723983764648438,
        "learning_rate": 4.66273695933682e-05,
        "epoch": 0.8937138130686518,
        "step": 6483
    },
    {
        "loss": 1.7817,
        "grad_norm": 1.9565268754959106,
        "learning_rate": 4.6517162777255944e-05,
        "epoch": 0.8938516680452164,
        "step": 6484
    },
    {
        "loss": 2.0831,
        "grad_norm": 1.6958914995193481,
        "learning_rate": 4.640704686874142e-05,
        "epoch": 0.8939895230217811,
        "step": 6485
    },
    {
        "loss": 2.2571,
        "grad_norm": 2.536020517349243,
        "learning_rate": 4.629702205499465e-05,
        "epoch": 0.8941273779983457,
        "step": 6486
    },
    {
        "loss": 1.022,
        "grad_norm": 1.8318895101547241,
        "learning_rate": 4.6187088523030394e-05,
        "epoch": 0.8942652329749103,
        "step": 6487
    },
    {
        "loss": 1.8451,
        "grad_norm": 2.4212992191314697,
        "learning_rate": 4.6077246459708734e-05,
        "epoch": 0.894403087951475,
        "step": 6488
    },
    {
        "loss": 1.7464,
        "grad_norm": 1.581923007965088,
        "learning_rate": 4.596749605173382e-05,
        "epoch": 0.8945409429280397,
        "step": 6489
    },
    {
        "loss": 2.0268,
        "grad_norm": 2.127514362335205,
        "learning_rate": 4.585783748565414e-05,
        "epoch": 0.8946787979046044,
        "step": 6490
    },
    {
        "loss": 2.4485,
        "grad_norm": 1.7982786893844604,
        "learning_rate": 4.574827094786225e-05,
        "epoch": 0.894816652881169,
        "step": 6491
    },
    {
        "loss": 0.9138,
        "grad_norm": 2.355437994003296,
        "learning_rate": 4.563879662459427e-05,
        "epoch": 0.8949545078577337,
        "step": 6492
    },
    {
        "loss": 1.7658,
        "grad_norm": 2.5337674617767334,
        "learning_rate": 4.552941470192938e-05,
        "epoch": 0.8950923628342983,
        "step": 6493
    },
    {
        "loss": 1.0015,
        "grad_norm": 2.1720571517944336,
        "learning_rate": 4.542012536578985e-05,
        "epoch": 0.895230217810863,
        "step": 6494
    },
    {
        "loss": 1.6205,
        "grad_norm": 1.4109560251235962,
        "learning_rate": 4.531092880194031e-05,
        "epoch": 0.8953680727874276,
        "step": 6495
    },
    {
        "loss": 2.4373,
        "grad_norm": 2.3009042739868164,
        "learning_rate": 4.5201825195988076e-05,
        "epoch": 0.8955059277639923,
        "step": 6496
    },
    {
        "loss": 1.6163,
        "grad_norm": 2.0343737602233887,
        "learning_rate": 4.509281473338242e-05,
        "epoch": 0.8956437827405569,
        "step": 6497
    },
    {
        "loss": 1.4762,
        "grad_norm": 1.923537015914917,
        "learning_rate": 4.498389759941394e-05,
        "epoch": 0.8957816377171216,
        "step": 6498
    },
    {
        "loss": 1.3303,
        "grad_norm": 2.372600555419922,
        "learning_rate": 4.4875073979214945e-05,
        "epoch": 0.8959194926936862,
        "step": 6499
    },
    {
        "loss": 1.6486,
        "grad_norm": 1.158693790435791,
        "learning_rate": 4.476634405775879e-05,
        "epoch": 0.8960573476702509,
        "step": 6500
    },
    {
        "loss": 0.8814,
        "grad_norm": 1.9109008312225342,
        "learning_rate": 4.4657708019859365e-05,
        "epoch": 0.8961952026468155,
        "step": 6501
    },
    {
        "loss": 1.8132,
        "grad_norm": 1.6154578924179077,
        "learning_rate": 4.454916605017092e-05,
        "epoch": 0.8963330576233802,
        "step": 6502
    },
    {
        "loss": 1.6137,
        "grad_norm": 2.4433538913726807,
        "learning_rate": 4.4440718333188195e-05,
        "epoch": 0.8964709125999448,
        "step": 6503
    },
    {
        "loss": 1.2685,
        "grad_norm": 1.8951860666275024,
        "learning_rate": 4.4332365053245264e-05,
        "epoch": 0.8966087675765095,
        "step": 6504
    },
    {
        "loss": 2.061,
        "grad_norm": 1.4821170568466187,
        "learning_rate": 4.422410639451615e-05,
        "epoch": 0.8967466225530741,
        "step": 6505
    },
    {
        "loss": 1.9946,
        "grad_norm": 1.4404385089874268,
        "learning_rate": 4.411594254101369e-05,
        "epoch": 0.8968844775296388,
        "step": 6506
    },
    {
        "loss": 1.7537,
        "grad_norm": 2.597963809967041,
        "learning_rate": 4.4007873676589564e-05,
        "epoch": 0.8970223325062034,
        "step": 6507
    },
    {
        "loss": 1.9884,
        "grad_norm": 2.8713157176971436,
        "learning_rate": 4.389989998493438e-05,
        "epoch": 0.8971601874827682,
        "step": 6508
    },
    {
        "loss": 2.0068,
        "grad_norm": 2.0161020755767822,
        "learning_rate": 4.379202164957643e-05,
        "epoch": 0.8972980424593328,
        "step": 6509
    },
    {
        "loss": 1.2701,
        "grad_norm": 2.533108949661255,
        "learning_rate": 4.368423885388234e-05,
        "epoch": 0.8974358974358975,
        "step": 6510
    },
    {
        "loss": 1.4881,
        "grad_norm": 2.4215147495269775,
        "learning_rate": 4.357655178105633e-05,
        "epoch": 0.8975737524124621,
        "step": 6511
    },
    {
        "loss": 2.0336,
        "grad_norm": 2.381972312927246,
        "learning_rate": 4.346896061413956e-05,
        "epoch": 0.8977116073890268,
        "step": 6512
    },
    {
        "loss": 2.4345,
        "grad_norm": 2.401057004928589,
        "learning_rate": 4.336146553601066e-05,
        "epoch": 0.8978494623655914,
        "step": 6513
    },
    {
        "loss": 0.9749,
        "grad_norm": 2.1696043014526367,
        "learning_rate": 4.325406672938453e-05,
        "epoch": 0.8979873173421561,
        "step": 6514
    },
    {
        "loss": 2.1927,
        "grad_norm": 1.7557411193847656,
        "learning_rate": 4.3146764376812507e-05,
        "epoch": 0.8981251723187207,
        "step": 6515
    },
    {
        "loss": 2.364,
        "grad_norm": 2.9508485794067383,
        "learning_rate": 4.303955866068229e-05,
        "epoch": 0.8982630272952854,
        "step": 6516
    },
    {
        "loss": 1.2724,
        "grad_norm": 2.3808929920196533,
        "learning_rate": 4.293244976321681e-05,
        "epoch": 0.89840088227185,
        "step": 6517
    },
    {
        "loss": 0.974,
        "grad_norm": 1.5896433591842651,
        "learning_rate": 4.282543786647495e-05,
        "epoch": 0.8985387372484147,
        "step": 6518
    },
    {
        "loss": 2.0448,
        "grad_norm": 2.822709083557129,
        "learning_rate": 4.2718523152350365e-05,
        "epoch": 0.8986765922249793,
        "step": 6519
    },
    {
        "loss": 1.4985,
        "grad_norm": 2.5086405277252197,
        "learning_rate": 4.2611705802571535e-05,
        "epoch": 0.898814447201544,
        "step": 6520
    },
    {
        "loss": 1.846,
        "grad_norm": 1.7527730464935303,
        "learning_rate": 4.250498599870163e-05,
        "epoch": 0.8989523021781086,
        "step": 6521
    },
    {
        "loss": 0.8503,
        "grad_norm": 4.169467926025391,
        "learning_rate": 4.2398363922138074e-05,
        "epoch": 0.8990901571546733,
        "step": 6522
    },
    {
        "loss": 1.8774,
        "grad_norm": 2.0908889770507812,
        "learning_rate": 4.229183975411175e-05,
        "epoch": 0.8992280121312379,
        "step": 6523
    },
    {
        "loss": 1.7996,
        "grad_norm": 2.3995602130889893,
        "learning_rate": 4.2185413675687655e-05,
        "epoch": 0.8993658671078026,
        "step": 6524
    },
    {
        "loss": 1.8697,
        "grad_norm": 2.195261001586914,
        "learning_rate": 4.207908586776359e-05,
        "epoch": 0.8995037220843672,
        "step": 6525
    },
    {
        "loss": 2.1999,
        "grad_norm": 2.714738607406616,
        "learning_rate": 4.1972856511070745e-05,
        "epoch": 0.899641577060932,
        "step": 6526
    },
    {
        "loss": 1.9649,
        "grad_norm": 2.344099521636963,
        "learning_rate": 4.1866725786172646e-05,
        "epoch": 0.8997794320374966,
        "step": 6527
    },
    {
        "loss": 1.8025,
        "grad_norm": 2.3603038787841797,
        "learning_rate": 4.176069387346518e-05,
        "epoch": 0.8999172870140613,
        "step": 6528
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.605706214904785,
        "learning_rate": 4.16547609531765e-05,
        "epoch": 0.9000551419906259,
        "step": 6529
    },
    {
        "loss": 2.0031,
        "grad_norm": 1.1382826566696167,
        "learning_rate": 4.1548927205366484e-05,
        "epoch": 0.9001929969671905,
        "step": 6530
    },
    {
        "loss": 2.0115,
        "grad_norm": 1.8917491436004639,
        "learning_rate": 4.144319280992618e-05,
        "epoch": 0.9003308519437552,
        "step": 6531
    },
    {
        "loss": 1.8859,
        "grad_norm": 1.4597454071044922,
        "learning_rate": 4.133755794657783e-05,
        "epoch": 0.9004687069203198,
        "step": 6532
    },
    {
        "loss": 2.1215,
        "grad_norm": 1.6895347833633423,
        "learning_rate": 4.1232022794874814e-05,
        "epoch": 0.9006065618968845,
        "step": 6533
    },
    {
        "loss": 1.761,
        "grad_norm": 5.240668773651123,
        "learning_rate": 4.112658753420056e-05,
        "epoch": 0.9007444168734491,
        "step": 6534
    },
    {
        "loss": 2.2571,
        "grad_norm": 1.4865177869796753,
        "learning_rate": 4.102125234376914e-05,
        "epoch": 0.9008822718500138,
        "step": 6535
    },
    {
        "loss": 1.8667,
        "grad_norm": 2.0402371883392334,
        "learning_rate": 4.091601740262414e-05,
        "epoch": 0.9010201268265784,
        "step": 6536
    },
    {
        "loss": 2.0756,
        "grad_norm": 1.7218836545944214,
        "learning_rate": 4.081088288963908e-05,
        "epoch": 0.9011579818031431,
        "step": 6537
    },
    {
        "loss": 1.8912,
        "grad_norm": 2.5535614490509033,
        "learning_rate": 4.070584898351666e-05,
        "epoch": 0.9012958367797077,
        "step": 6538
    },
    {
        "loss": 2.3269,
        "grad_norm": 1.6555380821228027,
        "learning_rate": 4.060091586278848e-05,
        "epoch": 0.9014336917562724,
        "step": 6539
    },
    {
        "loss": 2.1285,
        "grad_norm": 1.557558298110962,
        "learning_rate": 4.049608370581478e-05,
        "epoch": 0.901571546732837,
        "step": 6540
    },
    {
        "loss": 2.0871,
        "grad_norm": 1.888002634048462,
        "learning_rate": 4.0391352690784537e-05,
        "epoch": 0.9017094017094017,
        "step": 6541
    },
    {
        "loss": 1.3132,
        "grad_norm": 2.7572021484375,
        "learning_rate": 4.028672299571434e-05,
        "epoch": 0.9018472566859663,
        "step": 6542
    },
    {
        "loss": 1.8129,
        "grad_norm": 2.1948745250701904,
        "learning_rate": 4.018219479844907e-05,
        "epoch": 0.901985111662531,
        "step": 6543
    },
    {
        "loss": 1.389,
        "grad_norm": 1.7392098903656006,
        "learning_rate": 4.007776827666062e-05,
        "epoch": 0.9021229666390956,
        "step": 6544
    },
    {
        "loss": 1.0347,
        "grad_norm": 2.020848274230957,
        "learning_rate": 3.997344360784818e-05,
        "epoch": 0.9022608216156603,
        "step": 6545
    },
    {
        "loss": 1.8636,
        "grad_norm": 2.0615930557250977,
        "learning_rate": 3.986922096933813e-05,
        "epoch": 0.902398676592225,
        "step": 6546
    },
    {
        "loss": 1.4807,
        "grad_norm": 2.0935795307159424,
        "learning_rate": 3.976510053828294e-05,
        "epoch": 0.9025365315687897,
        "step": 6547
    },
    {
        "loss": 1.3569,
        "grad_norm": 1.9407531023025513,
        "learning_rate": 3.9661082491661714e-05,
        "epoch": 0.9026743865453543,
        "step": 6548
    },
    {
        "loss": 1.762,
        "grad_norm": 2.2125425338745117,
        "learning_rate": 3.9557167006279514e-05,
        "epoch": 0.902812241521919,
        "step": 6549
    },
    {
        "loss": 2.3908,
        "grad_norm": 2.3052098751068115,
        "learning_rate": 3.945335425876676e-05,
        "epoch": 0.9029500964984836,
        "step": 6550
    },
    {
        "loss": 1.8297,
        "grad_norm": 1.81651771068573,
        "learning_rate": 3.934964442557969e-05,
        "epoch": 0.9030879514750483,
        "step": 6551
    },
    {
        "loss": 2.1068,
        "grad_norm": 1.900674819946289,
        "learning_rate": 3.92460376829992e-05,
        "epoch": 0.9032258064516129,
        "step": 6552
    },
    {
        "loss": 2.2144,
        "grad_norm": 1.1945425271987915,
        "learning_rate": 3.914253420713108e-05,
        "epoch": 0.9033636614281776,
        "step": 6553
    },
    {
        "loss": 1.7702,
        "grad_norm": 2.227337121963501,
        "learning_rate": 3.9039134173905835e-05,
        "epoch": 0.9035015164047422,
        "step": 6554
    },
    {
        "loss": 1.1614,
        "grad_norm": 2.293837308883667,
        "learning_rate": 3.893583775907774e-05,
        "epoch": 0.9036393713813069,
        "step": 6555
    },
    {
        "loss": 1.7399,
        "grad_norm": 2.067197322845459,
        "learning_rate": 3.883264513822538e-05,
        "epoch": 0.9037772263578715,
        "step": 6556
    },
    {
        "loss": 2.177,
        "grad_norm": 2.350733518600464,
        "learning_rate": 3.8729556486750616e-05,
        "epoch": 0.9039150813344362,
        "step": 6557
    },
    {
        "loss": 1.2779,
        "grad_norm": 2.576359987258911,
        "learning_rate": 3.862657197987849e-05,
        "epoch": 0.9040529363110008,
        "step": 6558
    },
    {
        "loss": 2.4493,
        "grad_norm": 1.8389226198196411,
        "learning_rate": 3.8523691792657336e-05,
        "epoch": 0.9041907912875655,
        "step": 6559
    },
    {
        "loss": 0.949,
        "grad_norm": 3.1631736755371094,
        "learning_rate": 3.8420916099958135e-05,
        "epoch": 0.9043286462641301,
        "step": 6560
    },
    {
        "loss": 0.3992,
        "grad_norm": 1.7641044855117798,
        "learning_rate": 3.831824507647395e-05,
        "epoch": 0.9044665012406948,
        "step": 6561
    },
    {
        "loss": 1.7957,
        "grad_norm": 2.216430902481079,
        "learning_rate": 3.821567889672032e-05,
        "epoch": 0.9046043562172594,
        "step": 6562
    },
    {
        "loss": 1.2314,
        "grad_norm": 3.3021605014801025,
        "learning_rate": 3.81132177350342e-05,
        "epoch": 0.9047422111938241,
        "step": 6563
    },
    {
        "loss": 1.8876,
        "grad_norm": 1.5725759267807007,
        "learning_rate": 3.801086176557446e-05,
        "epoch": 0.9048800661703887,
        "step": 6564
    },
    {
        "loss": 1.7297,
        "grad_norm": 1.8411011695861816,
        "learning_rate": 3.7908611162320774e-05,
        "epoch": 0.9050179211469535,
        "step": 6565
    },
    {
        "loss": 2.4102,
        "grad_norm": 1.7205274105072021,
        "learning_rate": 3.78064660990738e-05,
        "epoch": 0.905155776123518,
        "step": 6566
    },
    {
        "loss": 1.7279,
        "grad_norm": 3.4789459705352783,
        "learning_rate": 3.7704426749455005e-05,
        "epoch": 0.9052936311000828,
        "step": 6567
    },
    {
        "loss": 1.4829,
        "grad_norm": 3.881077766418457,
        "learning_rate": 3.7602493286906124e-05,
        "epoch": 0.9054314860766474,
        "step": 6568
    },
    {
        "loss": 2.1944,
        "grad_norm": 1.767337441444397,
        "learning_rate": 3.750066588468878e-05,
        "epoch": 0.9055693410532121,
        "step": 6569
    },
    {
        "loss": 1.6393,
        "grad_norm": 1.925503134727478,
        "learning_rate": 3.739894471588423e-05,
        "epoch": 0.9057071960297767,
        "step": 6570
    },
    {
        "loss": 1.8644,
        "grad_norm": 1.4793853759765625,
        "learning_rate": 3.729732995339358e-05,
        "epoch": 0.9058450510063414,
        "step": 6571
    },
    {
        "loss": 2.2365,
        "grad_norm": 2.025214195251465,
        "learning_rate": 3.71958217699365e-05,
        "epoch": 0.905982905982906,
        "step": 6572
    },
    {
        "loss": 1.4204,
        "grad_norm": 2.800450563430786,
        "learning_rate": 3.709442033805206e-05,
        "epoch": 0.9061207609594706,
        "step": 6573
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.7458962202072144,
        "learning_rate": 3.6993125830097396e-05,
        "epoch": 0.9062586159360353,
        "step": 6574
    },
    {
        "loss": 1.7707,
        "grad_norm": 2.399428367614746,
        "learning_rate": 3.689193841824824e-05,
        "epoch": 0.9063964709125999,
        "step": 6575
    },
    {
        "loss": 1.3986,
        "grad_norm": 1.6444859504699707,
        "learning_rate": 3.6790858274498264e-05,
        "epoch": 0.9065343258891646,
        "step": 6576
    },
    {
        "loss": 2.0262,
        "grad_norm": 1.9590116739273071,
        "learning_rate": 3.668988557065862e-05,
        "epoch": 0.9066721808657292,
        "step": 6577
    },
    {
        "loss": 1.7564,
        "grad_norm": 2.0929665565490723,
        "learning_rate": 3.658902047835786e-05,
        "epoch": 0.9068100358422939,
        "step": 6578
    },
    {
        "loss": 1.8007,
        "grad_norm": 1.288041353225708,
        "learning_rate": 3.648826316904185e-05,
        "epoch": 0.9069478908188585,
        "step": 6579
    },
    {
        "loss": 2.1631,
        "grad_norm": 2.242643117904663,
        "learning_rate": 3.638761381397291e-05,
        "epoch": 0.9070857457954232,
        "step": 6580
    },
    {
        "loss": 0.7158,
        "grad_norm": 2.1600685119628906,
        "learning_rate": 3.6287072584230266e-05,
        "epoch": 0.9072236007719878,
        "step": 6581
    },
    {
        "loss": 1.445,
        "grad_norm": 1.5770946741104126,
        "learning_rate": 3.618663965070904e-05,
        "epoch": 0.9073614557485525,
        "step": 6582
    },
    {
        "loss": 2.416,
        "grad_norm": 1.8886420726776123,
        "learning_rate": 3.60863151841203e-05,
        "epoch": 0.9074993107251171,
        "step": 6583
    },
    {
        "loss": 1.8151,
        "grad_norm": 2.3784713745117188,
        "learning_rate": 3.598609935499102e-05,
        "epoch": 0.9076371657016818,
        "step": 6584
    },
    {
        "loss": 1.2321,
        "grad_norm": 2.5337374210357666,
        "learning_rate": 3.5885992333663164e-05,
        "epoch": 0.9077750206782464,
        "step": 6585
    },
    {
        "loss": 1.6704,
        "grad_norm": 2.3709304332733154,
        "learning_rate": 3.5785994290293935e-05,
        "epoch": 0.9079128756548112,
        "step": 6586
    },
    {
        "loss": 2.1687,
        "grad_norm": 3.850454092025757,
        "learning_rate": 3.568610539485543e-05,
        "epoch": 0.9080507306313758,
        "step": 6587
    },
    {
        "loss": 2.12,
        "grad_norm": 1.7895805835723877,
        "learning_rate": 3.558632581713386e-05,
        "epoch": 0.9081885856079405,
        "step": 6588
    },
    {
        "loss": 2.3888,
        "grad_norm": 1.2546051740646362,
        "learning_rate": 3.5486655726730034e-05,
        "epoch": 0.9083264405845051,
        "step": 6589
    },
    {
        "loss": 2.3398,
        "grad_norm": 1.770263433456421,
        "learning_rate": 3.538709529305836e-05,
        "epoch": 0.9084642955610698,
        "step": 6590
    },
    {
        "loss": 1.111,
        "grad_norm": 2.2831737995147705,
        "learning_rate": 3.528764468534686e-05,
        "epoch": 0.9086021505376344,
        "step": 6591
    },
    {
        "loss": 1.7771,
        "grad_norm": 2.965642213821411,
        "learning_rate": 3.518830407263719e-05,
        "epoch": 0.9087400055141991,
        "step": 6592
    },
    {
        "loss": 1.845,
        "grad_norm": 2.688048839569092,
        "learning_rate": 3.5089073623783586e-05,
        "epoch": 0.9088778604907637,
        "step": 6593
    },
    {
        "loss": 1.5086,
        "grad_norm": 1.5427062511444092,
        "learning_rate": 3.498995350745349e-05,
        "epoch": 0.9090157154673284,
        "step": 6594
    },
    {
        "loss": 2.0452,
        "grad_norm": 1.618306040763855,
        "learning_rate": 3.4890943892126494e-05,
        "epoch": 0.909153570443893,
        "step": 6595
    },
    {
        "loss": 1.0765,
        "grad_norm": 2.809108018875122,
        "learning_rate": 3.479204494609435e-05,
        "epoch": 0.9092914254204577,
        "step": 6596
    },
    {
        "loss": 1.7547,
        "grad_norm": 2.359246253967285,
        "learning_rate": 3.469325683746093e-05,
        "epoch": 0.9094292803970223,
        "step": 6597
    },
    {
        "loss": 1.8428,
        "grad_norm": 1.3173714876174927,
        "learning_rate": 3.4594579734141675e-05,
        "epoch": 0.909567135373587,
        "step": 6598
    },
    {
        "loss": 2.1155,
        "grad_norm": 7.112726211547852,
        "learning_rate": 3.449601380386309e-05,
        "epoch": 0.9097049903501516,
        "step": 6599
    },
    {
        "loss": 1.5324,
        "grad_norm": 1.3226125240325928,
        "learning_rate": 3.4397559214163037e-05,
        "epoch": 0.9098428453267163,
        "step": 6600
    },
    {
        "loss": 1.5143,
        "grad_norm": 1.5502222776412964,
        "learning_rate": 3.4299216132389775e-05,
        "epoch": 0.9099807003032809,
        "step": 6601
    },
    {
        "loss": 1.9202,
        "grad_norm": 1.7522406578063965,
        "learning_rate": 3.4200984725702447e-05,
        "epoch": 0.9101185552798456,
        "step": 6602
    },
    {
        "loss": 1.5323,
        "grad_norm": 2.0967373847961426,
        "learning_rate": 3.410286516107005e-05,
        "epoch": 0.9102564102564102,
        "step": 6603
    },
    {
        "loss": 1.4849,
        "grad_norm": Infinity,
        "learning_rate": 3.410286516107005e-05,
        "epoch": 0.910394265232975,
        "step": 6604
    },
    {
        "loss": 2.178,
        "grad_norm": 1.8868173360824585,
        "learning_rate": 3.400485760527148e-05,
        "epoch": 0.9105321202095396,
        "step": 6605
    },
    {
        "loss": 1.846,
        "grad_norm": 1.913336157798767,
        "learning_rate": 3.3906962224895435e-05,
        "epoch": 0.9106699751861043,
        "step": 6606
    },
    {
        "loss": 1.7173,
        "grad_norm": 2.1353533267974854,
        "learning_rate": 3.380917918634e-05,
        "epoch": 0.9108078301626689,
        "step": 6607
    },
    {
        "loss": 1.7617,
        "grad_norm": 1.6482142210006714,
        "learning_rate": 3.3711508655811986e-05,
        "epoch": 0.9109456851392336,
        "step": 6608
    },
    {
        "loss": 1.6627,
        "grad_norm": 2.4645957946777344,
        "learning_rate": 3.3613950799327086e-05,
        "epoch": 0.9110835401157982,
        "step": 6609
    },
    {
        "loss": 1.7005,
        "grad_norm": 2.2347145080566406,
        "learning_rate": 3.3516505782709716e-05,
        "epoch": 0.9112213950923629,
        "step": 6610
    },
    {
        "loss": 2.0799,
        "grad_norm": 2.1898984909057617,
        "learning_rate": 3.341917377159213e-05,
        "epoch": 0.9113592500689275,
        "step": 6611
    },
    {
        "loss": 2.4515,
        "grad_norm": 1.8384989500045776,
        "learning_rate": 3.3321954931414855e-05,
        "epoch": 0.9114971050454922,
        "step": 6612
    },
    {
        "loss": 1.6373,
        "grad_norm": 1.704759955406189,
        "learning_rate": 3.322484942742569e-05,
        "epoch": 0.9116349600220568,
        "step": 6613
    },
    {
        "loss": 0.5402,
        "grad_norm": 3.7268779277801514,
        "learning_rate": 3.3127857424680074e-05,
        "epoch": 0.9117728149986214,
        "step": 6614
    },
    {
        "loss": 1.1554,
        "grad_norm": 1.2000129222869873,
        "learning_rate": 3.3030979088040556e-05,
        "epoch": 0.9119106699751861,
        "step": 6615
    },
    {
        "loss": 1.9319,
        "grad_norm": 1.6739908456802368,
        "learning_rate": 3.293421458217621e-05,
        "epoch": 0.9120485249517507,
        "step": 6616
    },
    {
        "loss": 1.6104,
        "grad_norm": 2.242765426635742,
        "learning_rate": 3.283756407156268e-05,
        "epoch": 0.9121863799283154,
        "step": 6617
    },
    {
        "loss": 2.2254,
        "grad_norm": 2.105933666229248,
        "learning_rate": 3.274102772048213e-05,
        "epoch": 0.91232423490488,
        "step": 6618
    },
    {
        "loss": 2.1743,
        "grad_norm": 1.8857601881027222,
        "learning_rate": 3.26446056930223e-05,
        "epoch": 0.9124620898814447,
        "step": 6619
    },
    {
        "loss": 2.4515,
        "grad_norm": 1.1792010068893433,
        "learning_rate": 3.254829815307698e-05,
        "epoch": 0.9125999448580093,
        "step": 6620
    },
    {
        "loss": 1.0932,
        "grad_norm": 3.8317198753356934,
        "learning_rate": 3.245210526434512e-05,
        "epoch": 0.912737799834574,
        "step": 6621
    },
    {
        "loss": 1.7484,
        "grad_norm": 2.080296516418457,
        "learning_rate": 3.235602719033066e-05,
        "epoch": 0.9128756548111386,
        "step": 6622
    },
    {
        "loss": 2.2898,
        "grad_norm": 1.6113699674606323,
        "learning_rate": 3.226006409434282e-05,
        "epoch": 0.9130135097877033,
        "step": 6623
    },
    {
        "loss": 1.3784,
        "grad_norm": 2.6652848720550537,
        "learning_rate": 3.216421613949492e-05,
        "epoch": 0.913151364764268,
        "step": 6624
    },
    {
        "loss": 1.5447,
        "grad_norm": 2.2154581546783447,
        "learning_rate": 3.206848348870488e-05,
        "epoch": 0.9132892197408327,
        "step": 6625
    },
    {
        "loss": 1.3305,
        "grad_norm": 2.652360439300537,
        "learning_rate": 3.197286630469464e-05,
        "epoch": 0.9134270747173973,
        "step": 6626
    },
    {
        "loss": 1.0451,
        "grad_norm": 2.474332571029663,
        "learning_rate": 3.187736474998957e-05,
        "epoch": 0.913564929693962,
        "step": 6627
    },
    {
        "loss": 2.0333,
        "grad_norm": 1.7141731977462769,
        "learning_rate": 3.1781978986918915e-05,
        "epoch": 0.9137027846705266,
        "step": 6628
    },
    {
        "loss": 1.8344,
        "grad_norm": 2.8651576042175293,
        "learning_rate": 3.168670917761474e-05,
        "epoch": 0.9138406396470913,
        "step": 6629
    },
    {
        "loss": 1.0089,
        "grad_norm": 2.7936959266662598,
        "learning_rate": 3.159155548401208e-05,
        "epoch": 0.9139784946236559,
        "step": 6630
    },
    {
        "loss": 1.912,
        "grad_norm": 2.073232412338257,
        "learning_rate": 3.149651806784889e-05,
        "epoch": 0.9141163496002206,
        "step": 6631
    },
    {
        "loss": 2.0772,
        "grad_norm": 1.7800530195236206,
        "learning_rate": 3.140159709066508e-05,
        "epoch": 0.9142542045767852,
        "step": 6632
    },
    {
        "loss": 0.8884,
        "grad_norm": 1.6553577184677124,
        "learning_rate": 3.130679271380302e-05,
        "epoch": 0.9143920595533499,
        "step": 6633
    },
    {
        "loss": 2.1252,
        "grad_norm": 2.3522539138793945,
        "learning_rate": 3.121210509840666e-05,
        "epoch": 0.9145299145299145,
        "step": 6634
    },
    {
        "loss": 0.8302,
        "grad_norm": 1.4499897956848145,
        "learning_rate": 3.1117534405421364e-05,
        "epoch": 0.9146677695064792,
        "step": 6635
    },
    {
        "loss": 2.1503,
        "grad_norm": 2.3662681579589844,
        "learning_rate": 3.102308079559405e-05,
        "epoch": 0.9148056244830438,
        "step": 6636
    },
    {
        "loss": 1.6012,
        "grad_norm": 1.518886685371399,
        "learning_rate": 3.09287444294726e-05,
        "epoch": 0.9149434794596085,
        "step": 6637
    },
    {
        "loss": 1.7782,
        "grad_norm": 1.511883020401001,
        "learning_rate": 3.0834525467405316e-05,
        "epoch": 0.9150813344361731,
        "step": 6638
    },
    {
        "loss": 0.5127,
        "grad_norm": 1.101639986038208,
        "learning_rate": 3.074042406954132e-05,
        "epoch": 0.9152191894127378,
        "step": 6639
    },
    {
        "loss": 1.7074,
        "grad_norm": 2.1060359477996826,
        "learning_rate": 3.0646440395829536e-05,
        "epoch": 0.9153570443893024,
        "step": 6640
    },
    {
        "loss": 1.6128,
        "grad_norm": 1.9638547897338867,
        "learning_rate": 3.0552574606019146e-05,
        "epoch": 0.9154948993658671,
        "step": 6641
    },
    {
        "loss": 1.6298,
        "grad_norm": 1.8955836296081543,
        "learning_rate": 3.045882685965873e-05,
        "epoch": 0.9156327543424317,
        "step": 6642
    },
    {
        "loss": 2.0015,
        "grad_norm": 1.6921340227127075,
        "learning_rate": 3.0365197316096105e-05,
        "epoch": 0.9157706093189965,
        "step": 6643
    },
    {
        "loss": 1.9919,
        "grad_norm": 2.6135668754577637,
        "learning_rate": 3.0271686134478483e-05,
        "epoch": 0.915908464295561,
        "step": 6644
    },
    {
        "loss": 1.3108,
        "grad_norm": 2.000493288040161,
        "learning_rate": 3.017829347375184e-05,
        "epoch": 0.9160463192721258,
        "step": 6645
    },
    {
        "loss": 2.2937,
        "grad_norm": 1.4026259183883667,
        "learning_rate": 3.0085019492660504e-05,
        "epoch": 0.9161841742486904,
        "step": 6646
    },
    {
        "loss": 2.6259,
        "grad_norm": 1.5317972898483276,
        "learning_rate": 2.9991864349747078e-05,
        "epoch": 0.9163220292252551,
        "step": 6647
    },
    {
        "loss": 1.4486,
        "grad_norm": 1.8752593994140625,
        "learning_rate": 2.98988282033525e-05,
        "epoch": 0.9164598842018197,
        "step": 6648
    },
    {
        "loss": 1.886,
        "grad_norm": 1.7601816654205322,
        "learning_rate": 2.980591121161497e-05,
        "epoch": 0.9165977391783844,
        "step": 6649
    },
    {
        "loss": 2.0781,
        "grad_norm": 1.6922924518585205,
        "learning_rate": 2.9713113532470593e-05,
        "epoch": 0.916735594154949,
        "step": 6650
    },
    {
        "loss": 2.0468,
        "grad_norm": 1.773339033126831,
        "learning_rate": 2.9620435323652307e-05,
        "epoch": 0.9168734491315137,
        "step": 6651
    },
    {
        "loss": 1.7153,
        "grad_norm": 2.02292537689209,
        "learning_rate": 2.952787674269023e-05,
        "epoch": 0.9170113041080783,
        "step": 6652
    },
    {
        "loss": 1.352,
        "grad_norm": 2.9569931030273438,
        "learning_rate": 2.943543794691116e-05,
        "epoch": 0.917149159084643,
        "step": 6653
    },
    {
        "loss": 0.7761,
        "grad_norm": 1.9318002462387085,
        "learning_rate": 2.9343119093438097e-05,
        "epoch": 0.9172870140612076,
        "step": 6654
    },
    {
        "loss": 1.9925,
        "grad_norm": 2.269578695297241,
        "learning_rate": 2.925092033919018e-05,
        "epoch": 0.9174248690377723,
        "step": 6655
    },
    {
        "loss": 2.0885,
        "grad_norm": 1.862587571144104,
        "learning_rate": 2.91588418408827e-05,
        "epoch": 0.9175627240143369,
        "step": 6656
    },
    {
        "loss": 2.2159,
        "grad_norm": 2.6580662727355957,
        "learning_rate": 2.9066883755026085e-05,
        "epoch": 0.9177005789909015,
        "step": 6657
    },
    {
        "loss": 1.8855,
        "grad_norm": 2.200777530670166,
        "learning_rate": 2.8975046237926563e-05,
        "epoch": 0.9178384339674662,
        "step": 6658
    },
    {
        "loss": 2.1932,
        "grad_norm": 1.5250325202941895,
        "learning_rate": 2.8883329445685102e-05,
        "epoch": 0.9179762889440308,
        "step": 6659
    },
    {
        "loss": 1.6019,
        "grad_norm": 2.3817856311798096,
        "learning_rate": 2.8791733534197484e-05,
        "epoch": 0.9181141439205955,
        "step": 6660
    },
    {
        "loss": 2.3658,
        "grad_norm": 1.7539483308792114,
        "learning_rate": 2.870025865915429e-05,
        "epoch": 0.9182519988971601,
        "step": 6661
    },
    {
        "loss": 1.9551,
        "grad_norm": 2.1593945026397705,
        "learning_rate": 2.8608904976040006e-05,
        "epoch": 0.9183898538737248,
        "step": 6662
    },
    {
        "loss": 2.1048,
        "grad_norm": 1.7253856658935547,
        "learning_rate": 2.851767264013342e-05,
        "epoch": 0.9185277088502894,
        "step": 6663
    },
    {
        "loss": 1.3851,
        "grad_norm": 2.1973655223846436,
        "learning_rate": 2.842656180650698e-05,
        "epoch": 0.9186655638268542,
        "step": 6664
    },
    {
        "loss": 2.3604,
        "grad_norm": 1.9695979356765747,
        "learning_rate": 2.833557263002642e-05,
        "epoch": 0.9188034188034188,
        "step": 6665
    },
    {
        "loss": 2.2953,
        "grad_norm": 2.1264777183532715,
        "learning_rate": 2.8244705265351012e-05,
        "epoch": 0.9189412737799835,
        "step": 6666
    },
    {
        "loss": 1.4382,
        "grad_norm": 2.1518092155456543,
        "learning_rate": 2.8153959866932732e-05,
        "epoch": 0.9190791287565481,
        "step": 6667
    },
    {
        "loss": 1.3763,
        "grad_norm": 2.553239107131958,
        "learning_rate": 2.806333658901621e-05,
        "epoch": 0.9192169837331128,
        "step": 6668
    },
    {
        "loss": 1.0263,
        "grad_norm": 3.00726056098938,
        "learning_rate": 2.7972835585638824e-05,
        "epoch": 0.9193548387096774,
        "step": 6669
    },
    {
        "loss": 2.0725,
        "grad_norm": 1.781754732131958,
        "learning_rate": 2.788245701062967e-05,
        "epoch": 0.9194926936862421,
        "step": 6670
    },
    {
        "loss": 1.5145,
        "grad_norm": 2.514697551727295,
        "learning_rate": 2.7792201017610175e-05,
        "epoch": 0.9196305486628067,
        "step": 6671
    },
    {
        "loss": 2.1213,
        "grad_norm": 1.5956543684005737,
        "learning_rate": 2.7702067759993122e-05,
        "epoch": 0.9197684036393714,
        "step": 6672
    },
    {
        "loss": 1.5127,
        "grad_norm": 1.5032676458358765,
        "learning_rate": 2.7612057390982682e-05,
        "epoch": 0.919906258615936,
        "step": 6673
    },
    {
        "loss": 1.1885,
        "grad_norm": 3.2174956798553467,
        "learning_rate": 2.7522170063574292e-05,
        "epoch": 0.9200441135925007,
        "step": 6674
    },
    {
        "loss": 2.0022,
        "grad_norm": 2.4922423362731934,
        "learning_rate": 2.7432405930554284e-05,
        "epoch": 0.9201819685690653,
        "step": 6675
    },
    {
        "loss": 1.9056,
        "grad_norm": 1.9382245540618896,
        "learning_rate": 2.7342765144499304e-05,
        "epoch": 0.92031982354563,
        "step": 6676
    },
    {
        "loss": 2.1164,
        "grad_norm": 2.4594972133636475,
        "learning_rate": 2.725324785777673e-05,
        "epoch": 0.9204576785221946,
        "step": 6677
    },
    {
        "loss": 1.6951,
        "grad_norm": 1.701662302017212,
        "learning_rate": 2.7163854222543582e-05,
        "epoch": 0.9205955334987593,
        "step": 6678
    },
    {
        "loss": 1.5168,
        "grad_norm": 2.5967612266540527,
        "learning_rate": 2.7074584390747137e-05,
        "epoch": 0.9207333884753239,
        "step": 6679
    },
    {
        "loss": 2.1129,
        "grad_norm": 1.690706491470337,
        "learning_rate": 2.698543851412394e-05,
        "epoch": 0.9208712434518886,
        "step": 6680
    },
    {
        "loss": 1.7075,
        "grad_norm": 1.8430410623550415,
        "learning_rate": 2.689641674419986e-05,
        "epoch": 0.9210090984284532,
        "step": 6681
    },
    {
        "loss": 1.0599,
        "grad_norm": 3.4154021739959717,
        "learning_rate": 2.680751923228997e-05,
        "epoch": 0.921146953405018,
        "step": 6682
    },
    {
        "loss": 0.4823,
        "grad_norm": 2.764345169067383,
        "learning_rate": 2.6718746129498152e-05,
        "epoch": 0.9212848083815826,
        "step": 6683
    },
    {
        "loss": 0.5604,
        "grad_norm": 2.3049933910369873,
        "learning_rate": 2.6630097586716662e-05,
        "epoch": 0.9214226633581473,
        "step": 6684
    },
    {
        "loss": 2.5375,
        "grad_norm": 1.5930956602096558,
        "learning_rate": 2.654157375462597e-05,
        "epoch": 0.9215605183347119,
        "step": 6685
    },
    {
        "loss": 2.3845,
        "grad_norm": 1.1872931718826294,
        "learning_rate": 2.6453174783694888e-05,
        "epoch": 0.9216983733112766,
        "step": 6686
    },
    {
        "loss": 2.1666,
        "grad_norm": 1.5386624336242676,
        "learning_rate": 2.6364900824179638e-05,
        "epoch": 0.9218362282878412,
        "step": 6687
    },
    {
        "loss": 2.2156,
        "grad_norm": 1.658882975578308,
        "learning_rate": 2.6276752026124306e-05,
        "epoch": 0.9219740832644059,
        "step": 6688
    },
    {
        "loss": 1.6591,
        "grad_norm": 1.5390515327453613,
        "learning_rate": 2.6188728539359863e-05,
        "epoch": 0.9221119382409705,
        "step": 6689
    },
    {
        "loss": 2.2357,
        "grad_norm": 1.3586760759353638,
        "learning_rate": 2.610083051350456e-05,
        "epoch": 0.9222497932175352,
        "step": 6690
    },
    {
        "loss": 1.0098,
        "grad_norm": 2.2529380321502686,
        "learning_rate": 2.6013058097963428e-05,
        "epoch": 0.9223876481940998,
        "step": 6691
    },
    {
        "loss": 2.3721,
        "grad_norm": 1.429896593093872,
        "learning_rate": 2.592541144192773e-05,
        "epoch": 0.9225255031706645,
        "step": 6692
    },
    {
        "loss": 1.987,
        "grad_norm": 1.3660356998443604,
        "learning_rate": 2.5837890694375023e-05,
        "epoch": 0.9226633581472291,
        "step": 6693
    },
    {
        "loss": 0.5237,
        "grad_norm": 1.9766608476638794,
        "learning_rate": 2.5750496004069135e-05,
        "epoch": 0.9228012131237938,
        "step": 6694
    },
    {
        "loss": 2.4631,
        "grad_norm": 3.0463521480560303,
        "learning_rate": 2.566322751955923e-05,
        "epoch": 0.9229390681003584,
        "step": 6695
    },
    {
        "loss": 2.2209,
        "grad_norm": 2.2955496311187744,
        "learning_rate": 2.557608538918036e-05,
        "epoch": 0.9230769230769231,
        "step": 6696
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.864814281463623,
        "learning_rate": 2.5489069761052508e-05,
        "epoch": 0.9232147780534877,
        "step": 6697
    },
    {
        "loss": 1.8609,
        "grad_norm": 2.340747356414795,
        "learning_rate": 2.54021807830806e-05,
        "epoch": 0.9233526330300524,
        "step": 6698
    },
    {
        "loss": 1.6667,
        "grad_norm": 2.230775833129883,
        "learning_rate": 2.53154186029546e-05,
        "epoch": 0.923490488006617,
        "step": 6699
    },
    {
        "loss": 2.097,
        "grad_norm": 1.8921599388122559,
        "learning_rate": 2.522878336814859e-05,
        "epoch": 0.9236283429831816,
        "step": 6700
    },
    {
        "loss": 2.1582,
        "grad_norm": 1.8902947902679443,
        "learning_rate": 2.514227522592113e-05,
        "epoch": 0.9237661979597463,
        "step": 6701
    },
    {
        "loss": 2.0074,
        "grad_norm": 1.8730957508087158,
        "learning_rate": 2.505589432331478e-05,
        "epoch": 0.923904052936311,
        "step": 6702
    },
    {
        "loss": 1.6287,
        "grad_norm": 2.3720343112945557,
        "learning_rate": 2.4969640807155502e-05,
        "epoch": 0.9240419079128757,
        "step": 6703
    },
    {
        "loss": 1.506,
        "grad_norm": 2.180607557296753,
        "learning_rate": 2.488351482405318e-05,
        "epoch": 0.9241797628894403,
        "step": 6704
    },
    {
        "loss": 1.8448,
        "grad_norm": 1.4661675691604614,
        "learning_rate": 2.479751652040052e-05,
        "epoch": 0.924317617866005,
        "step": 6705
    },
    {
        "loss": 1.2038,
        "grad_norm": 3.071518898010254,
        "learning_rate": 2.4711646042373326e-05,
        "epoch": 0.9244554728425696,
        "step": 6706
    },
    {
        "loss": 1.1415,
        "grad_norm": 1.7210227251052856,
        "learning_rate": 2.462590353593035e-05,
        "epoch": 0.9245933278191343,
        "step": 6707
    },
    {
        "loss": 1.4423,
        "grad_norm": 3.0157880783081055,
        "learning_rate": 2.4540289146812466e-05,
        "epoch": 0.9247311827956989,
        "step": 6708
    },
    {
        "loss": 1.4805,
        "grad_norm": 1.9129478931427002,
        "learning_rate": 2.445480302054316e-05,
        "epoch": 0.9248690377722636,
        "step": 6709
    },
    {
        "loss": 1.2552,
        "grad_norm": 2.195418357849121,
        "learning_rate": 2.4369445302427595e-05,
        "epoch": 0.9250068927488282,
        "step": 6710
    },
    {
        "loss": 1.4849,
        "grad_norm": 2.223839521408081,
        "learning_rate": 2.428421613755275e-05,
        "epoch": 0.9251447477253929,
        "step": 6711
    },
    {
        "loss": 2.323,
        "grad_norm": 2.076262950897217,
        "learning_rate": 2.419911567078713e-05,
        "epoch": 0.9252826027019575,
        "step": 6712
    },
    {
        "loss": 1.8867,
        "grad_norm": 2.145498275756836,
        "learning_rate": 2.411414404678063e-05,
        "epoch": 0.9254204576785222,
        "step": 6713
    },
    {
        "loss": 2.1242,
        "grad_norm": 1.7909979820251465,
        "learning_rate": 2.402930140996381e-05,
        "epoch": 0.9255583126550868,
        "step": 6714
    },
    {
        "loss": 1.9807,
        "grad_norm": 1.606162667274475,
        "learning_rate": 2.3944587904548255e-05,
        "epoch": 0.9256961676316515,
        "step": 6715
    },
    {
        "loss": 1.8513,
        "grad_norm": 2.3700900077819824,
        "learning_rate": 2.386000367452608e-05,
        "epoch": 0.9258340226082161,
        "step": 6716
    },
    {
        "loss": 1.9635,
        "grad_norm": 2.684638261795044,
        "learning_rate": 2.377554886366945e-05,
        "epoch": 0.9259718775847808,
        "step": 6717
    },
    {
        "loss": 0.8184,
        "grad_norm": 1.9548635482788086,
        "learning_rate": 2.36912236155307e-05,
        "epoch": 0.9261097325613454,
        "step": 6718
    },
    {
        "loss": 2.1581,
        "grad_norm": 1.9666329622268677,
        "learning_rate": 2.360702807344176e-05,
        "epoch": 0.9262475875379101,
        "step": 6719
    },
    {
        "loss": 1.7166,
        "grad_norm": 3.370784044265747,
        "learning_rate": 2.3522962380514347e-05,
        "epoch": 0.9263854425144747,
        "step": 6720
    },
    {
        "loss": 1.3809,
        "grad_norm": 2.3897907733917236,
        "learning_rate": 2.343902667963942e-05,
        "epoch": 0.9265232974910395,
        "step": 6721
    },
    {
        "loss": 1.366,
        "grad_norm": 1.8535616397857666,
        "learning_rate": 2.3355221113486847e-05,
        "epoch": 0.926661152467604,
        "step": 6722
    },
    {
        "loss": 2.2099,
        "grad_norm": 1.2902486324310303,
        "learning_rate": 2.327154582450527e-05,
        "epoch": 0.9267990074441688,
        "step": 6723
    },
    {
        "loss": 1.8223,
        "grad_norm": 2.6026477813720703,
        "learning_rate": 2.3188000954922162e-05,
        "epoch": 0.9269368624207334,
        "step": 6724
    },
    {
        "loss": 1.4108,
        "grad_norm": 1.8868104219436646,
        "learning_rate": 2.310458664674301e-05,
        "epoch": 0.9270747173972981,
        "step": 6725
    },
    {
        "loss": 1.9527,
        "grad_norm": 1.681978464126587,
        "learning_rate": 2.3021303041751607e-05,
        "epoch": 0.9272125723738627,
        "step": 6726
    },
    {
        "loss": 0.9287,
        "grad_norm": 2.4723875522613525,
        "learning_rate": 2.2938150281509364e-05,
        "epoch": 0.9273504273504274,
        "step": 6727
    },
    {
        "loss": 1.3268,
        "grad_norm": 2.7580502033233643,
        "learning_rate": 2.2855128507355484e-05,
        "epoch": 0.927488282326992,
        "step": 6728
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.6364715099334717,
        "learning_rate": 2.277223786040653e-05,
        "epoch": 0.9276261373035567,
        "step": 6729
    },
    {
        "loss": 1.3582,
        "grad_norm": 1.6155208349227905,
        "learning_rate": 2.2689478481556035e-05,
        "epoch": 0.9277639922801213,
        "step": 6730
    },
    {
        "loss": 1.3754,
        "grad_norm": 2.7837982177734375,
        "learning_rate": 2.2606850511474396e-05,
        "epoch": 0.927901847256686,
        "step": 6731
    },
    {
        "loss": 1.9359,
        "grad_norm": 1.899288535118103,
        "learning_rate": 2.252435409060891e-05,
        "epoch": 0.9280397022332506,
        "step": 6732
    },
    {
        "loss": 2.1231,
        "grad_norm": 1.6992173194885254,
        "learning_rate": 2.244198935918288e-05,
        "epoch": 0.9281775572098153,
        "step": 6733
    },
    {
        "loss": 1.7314,
        "grad_norm": 2.1342079639434814,
        "learning_rate": 2.235975645719617e-05,
        "epoch": 0.9283154121863799,
        "step": 6734
    },
    {
        "loss": 1.6762,
        "grad_norm": 1.9733664989471436,
        "learning_rate": 2.22776555244243e-05,
        "epoch": 0.9284532671629446,
        "step": 6735
    },
    {
        "loss": 1.5196,
        "grad_norm": 2.638698101043701,
        "learning_rate": 2.2195686700418473e-05,
        "epoch": 0.9285911221395092,
        "step": 6736
    },
    {
        "loss": 1.4773,
        "grad_norm": 1.2838375568389893,
        "learning_rate": 2.2113850124505585e-05,
        "epoch": 0.9287289771160739,
        "step": 6737
    },
    {
        "loss": 1.698,
        "grad_norm": 1.7378877401351929,
        "learning_rate": 2.2032145935787406e-05,
        "epoch": 0.9288668320926385,
        "step": 6738
    },
    {
        "loss": 1.5893,
        "grad_norm": 1.6229794025421143,
        "learning_rate": 2.1950574273140934e-05,
        "epoch": 0.9290046870692032,
        "step": 6739
    },
    {
        "loss": 1.896,
        "grad_norm": 1.2367724180221558,
        "learning_rate": 2.1869135275217945e-05,
        "epoch": 0.9291425420457678,
        "step": 6740
    },
    {
        "loss": 2.0274,
        "grad_norm": 2.1085147857666016,
        "learning_rate": 2.1787829080444377e-05,
        "epoch": 0.9292803970223326,
        "step": 6741
    },
    {
        "loss": 1.2742,
        "grad_norm": 1.7247141599655151,
        "learning_rate": 2.170665582702084e-05,
        "epoch": 0.9294182519988972,
        "step": 6742
    },
    {
        "loss": 2.2835,
        "grad_norm": 2.5764598846435547,
        "learning_rate": 2.1625615652921706e-05,
        "epoch": 0.9295561069754618,
        "step": 6743
    },
    {
        "loss": 1.4093,
        "grad_norm": 1.9653552770614624,
        "learning_rate": 2.1544708695895153e-05,
        "epoch": 0.9296939619520265,
        "step": 6744
    },
    {
        "loss": 1.7264,
        "grad_norm": 2.938498020172119,
        "learning_rate": 2.1463935093463183e-05,
        "epoch": 0.9298318169285911,
        "step": 6745
    },
    {
        "loss": 2.0411,
        "grad_norm": 2.430079698562622,
        "learning_rate": 2.1383294982920775e-05,
        "epoch": 0.9299696719051558,
        "step": 6746
    },
    {
        "loss": 2.3261,
        "grad_norm": 1.3030840158462524,
        "learning_rate": 2.1302788501336314e-05,
        "epoch": 0.9301075268817204,
        "step": 6747
    },
    {
        "loss": 1.8497,
        "grad_norm": 2.321153163909912,
        "learning_rate": 2.1222415785550844e-05,
        "epoch": 0.9302453818582851,
        "step": 6748
    },
    {
        "loss": 1.6803,
        "grad_norm": 2.559420347213745,
        "learning_rate": 2.114217697217803e-05,
        "epoch": 0.9303832368348497,
        "step": 6749
    },
    {
        "loss": 1.7752,
        "grad_norm": 2.3665645122528076,
        "learning_rate": 2.106207219760409e-05,
        "epoch": 0.9305210918114144,
        "step": 6750
    },
    {
        "loss": 2.1558,
        "grad_norm": 1.71527898311615,
        "learning_rate": 2.0982101597987415e-05,
        "epoch": 0.930658946787979,
        "step": 6751
    },
    {
        "loss": 0.6044,
        "grad_norm": 1.6638715267181396,
        "learning_rate": 2.0902265309258084e-05,
        "epoch": 0.9307968017645437,
        "step": 6752
    },
    {
        "loss": 1.9788,
        "grad_norm": 1.350179672241211,
        "learning_rate": 2.0822563467118126e-05,
        "epoch": 0.9309346567411083,
        "step": 6753
    },
    {
        "loss": 1.6476,
        "grad_norm": 2.7795345783233643,
        "learning_rate": 2.0742996207041087e-05,
        "epoch": 0.931072511717673,
        "step": 6754
    },
    {
        "loss": 1.995,
        "grad_norm": 2.759661912918091,
        "learning_rate": 2.066356366427148e-05,
        "epoch": 0.9312103666942376,
        "step": 6755
    },
    {
        "loss": 1.0296,
        "grad_norm": 2.325098752975464,
        "learning_rate": 2.058426597382501e-05,
        "epoch": 0.9313482216708023,
        "step": 6756
    },
    {
        "loss": 0.5881,
        "grad_norm": 1.9240862131118774,
        "learning_rate": 2.050510327048808e-05,
        "epoch": 0.9314860766473669,
        "step": 6757
    },
    {
        "loss": 2.1621,
        "grad_norm": 1.1181966066360474,
        "learning_rate": 2.042607568881778e-05,
        "epoch": 0.9316239316239316,
        "step": 6758
    },
    {
        "loss": 1.6519,
        "grad_norm": 1.3847397565841675,
        "learning_rate": 2.034718336314154e-05,
        "epoch": 0.9317617866004962,
        "step": 6759
    },
    {
        "loss": 1.3608,
        "grad_norm": 1.7884862422943115,
        "learning_rate": 2.026842642755674e-05,
        "epoch": 0.931899641577061,
        "step": 6760
    },
    {
        "loss": 1.559,
        "grad_norm": 2.2741916179656982,
        "learning_rate": 2.0189805015930574e-05,
        "epoch": 0.9320374965536256,
        "step": 6761
    },
    {
        "loss": 1.8929,
        "grad_norm": 2.5292556285858154,
        "learning_rate": 2.0111319261900185e-05,
        "epoch": 0.9321753515301903,
        "step": 6762
    },
    {
        "loss": 1.759,
        "grad_norm": 2.2229115962982178,
        "learning_rate": 2.0032969298871785e-05,
        "epoch": 0.9323132065067549,
        "step": 6763
    },
    {
        "loss": 2.0248,
        "grad_norm": 2.0234267711639404,
        "learning_rate": 1.9954755260021098e-05,
        "epoch": 0.9324510614833196,
        "step": 6764
    },
    {
        "loss": 1.461,
        "grad_norm": 1.7975454330444336,
        "learning_rate": 1.9876677278292488e-05,
        "epoch": 0.9325889164598842,
        "step": 6765
    },
    {
        "loss": 1.963,
        "grad_norm": 1.4063475131988525,
        "learning_rate": 1.9798735486399288e-05,
        "epoch": 0.9327267714364489,
        "step": 6766
    },
    {
        "loss": 1.3916,
        "grad_norm": 1.5750281810760498,
        "learning_rate": 1.972093001682339e-05,
        "epoch": 0.9328646264130135,
        "step": 6767
    },
    {
        "loss": 1.979,
        "grad_norm": 1.5953269004821777,
        "learning_rate": 1.9643261001814716e-05,
        "epoch": 0.9330024813895782,
        "step": 6768
    },
    {
        "loss": 1.7476,
        "grad_norm": 2.4333934783935547,
        "learning_rate": 1.9565728573391318e-05,
        "epoch": 0.9331403363661428,
        "step": 6769
    },
    {
        "loss": 1.6488,
        "grad_norm": 2.1903176307678223,
        "learning_rate": 1.9488332863339333e-05,
        "epoch": 0.9332781913427075,
        "step": 6770
    },
    {
        "loss": 1.0197,
        "grad_norm": 2.5140492916107178,
        "learning_rate": 1.941107400321216e-05,
        "epoch": 0.9334160463192721,
        "step": 6771
    },
    {
        "loss": 1.5285,
        "grad_norm": 2.861621141433716,
        "learning_rate": 1.933395212433091e-05,
        "epoch": 0.9335539012958368,
        "step": 6772
    },
    {
        "loss": 1.1304,
        "grad_norm": 1.7786304950714111,
        "learning_rate": 1.9256967357783663e-05,
        "epoch": 0.9336917562724014,
        "step": 6773
    },
    {
        "loss": 2.0254,
        "grad_norm": 2.432219982147217,
        "learning_rate": 1.9180119834425348e-05,
        "epoch": 0.9338296112489661,
        "step": 6774
    },
    {
        "loss": 1.4604,
        "grad_norm": 2.3737709522247314,
        "learning_rate": 1.9103409684877938e-05,
        "epoch": 0.9339674662255307,
        "step": 6775
    },
    {
        "loss": 1.6504,
        "grad_norm": 2.0910158157348633,
        "learning_rate": 1.9026837039529554e-05,
        "epoch": 0.9341053212020954,
        "step": 6776
    },
    {
        "loss": 1.6373,
        "grad_norm": 2.867399215698242,
        "learning_rate": 1.8950402028534852e-05,
        "epoch": 0.93424317617866,
        "step": 6777
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.6133548021316528,
        "learning_rate": 1.8874104781814504e-05,
        "epoch": 0.9343810311552248,
        "step": 6778
    },
    {
        "loss": 1.5502,
        "grad_norm": 2.97690486907959,
        "learning_rate": 1.879794542905483e-05,
        "epoch": 0.9345188861317894,
        "step": 6779
    },
    {
        "loss": 2.1575,
        "grad_norm": 2.9696338176727295,
        "learning_rate": 1.8721924099708043e-05,
        "epoch": 0.9346567411083541,
        "step": 6780
    },
    {
        "loss": 1.6479,
        "grad_norm": 1.0500597953796387,
        "learning_rate": 1.864604092299155e-05,
        "epoch": 0.9347945960849187,
        "step": 6781
    },
    {
        "loss": 2.1717,
        "grad_norm": 1.4283887147903442,
        "learning_rate": 1.8570296027887858e-05,
        "epoch": 0.9349324510614834,
        "step": 6782
    },
    {
        "loss": 1.982,
        "grad_norm": 1.6075924634933472,
        "learning_rate": 1.8494689543144727e-05,
        "epoch": 0.935070306038048,
        "step": 6783
    },
    {
        "loss": 0.9955,
        "grad_norm": 3.0314853191375732,
        "learning_rate": 1.841922159727436e-05,
        "epoch": 0.9352081610146127,
        "step": 6784
    },
    {
        "loss": 2.2945,
        "grad_norm": 1.7596001625061035,
        "learning_rate": 1.8343892318553734e-05,
        "epoch": 0.9353460159911773,
        "step": 6785
    },
    {
        "loss": 1.3455,
        "grad_norm": 2.7720296382904053,
        "learning_rate": 1.8268701835023917e-05,
        "epoch": 0.9354838709677419,
        "step": 6786
    },
    {
        "loss": 2.161,
        "grad_norm": 1.8730473518371582,
        "learning_rate": 1.819365027449008e-05,
        "epoch": 0.9356217259443066,
        "step": 6787
    },
    {
        "loss": 2.4954,
        "grad_norm": 2.204120397567749,
        "learning_rate": 1.81187377645214e-05,
        "epoch": 0.9357595809208712,
        "step": 6788
    },
    {
        "loss": 2.0597,
        "grad_norm": 2.535203456878662,
        "learning_rate": 1.8043964432450656e-05,
        "epoch": 0.9358974358974359,
        "step": 6789
    },
    {
        "loss": 2.2963,
        "grad_norm": 2.154966354370117,
        "learning_rate": 1.7969330405373885e-05,
        "epoch": 0.9360352908740005,
        "step": 6790
    },
    {
        "loss": 2.5324,
        "grad_norm": 2.1088123321533203,
        "learning_rate": 1.7894835810150557e-05,
        "epoch": 0.9361731458505652,
        "step": 6791
    },
    {
        "loss": 1.7708,
        "grad_norm": 1.7717456817626953,
        "learning_rate": 1.7820480773403147e-05,
        "epoch": 0.9363110008271298,
        "step": 6792
    },
    {
        "loss": 1.9906,
        "grad_norm": 1.526877760887146,
        "learning_rate": 1.774626542151675e-05,
        "epoch": 0.9364488558036945,
        "step": 6793
    },
    {
        "loss": 1.3901,
        "grad_norm": 2.9801206588745117,
        "learning_rate": 1.7672189880639135e-05,
        "epoch": 0.9365867107802591,
        "step": 6794
    },
    {
        "loss": 2.1073,
        "grad_norm": 2.5661680698394775,
        "learning_rate": 1.759825427668028e-05,
        "epoch": 0.9367245657568238,
        "step": 6795
    },
    {
        "loss": 2.1481,
        "grad_norm": 2.0010015964508057,
        "learning_rate": 1.752445873531252e-05,
        "epoch": 0.9368624207333884,
        "step": 6796
    },
    {
        "loss": 2.2636,
        "grad_norm": 1.3841819763183594,
        "learning_rate": 1.7450803381970115e-05,
        "epoch": 0.9370002757099531,
        "step": 6797
    },
    {
        "loss": 1.9443,
        "grad_norm": 1.9482877254486084,
        "learning_rate": 1.7377288341848875e-05,
        "epoch": 0.9371381306865177,
        "step": 6798
    },
    {
        "loss": 1.1069,
        "grad_norm": 2.468445301055908,
        "learning_rate": 1.7303913739906106e-05,
        "epoch": 0.9372759856630825,
        "step": 6799
    },
    {
        "loss": 1.8722,
        "grad_norm": 2.16103196144104,
        "learning_rate": 1.723067970086064e-05,
        "epoch": 0.9374138406396471,
        "step": 6800
    },
    {
        "loss": 1.5965,
        "grad_norm": 2.2474706172943115,
        "learning_rate": 1.7157586349192102e-05,
        "epoch": 0.9375516956162118,
        "step": 6801
    },
    {
        "loss": 1.5618,
        "grad_norm": 1.594836711883545,
        "learning_rate": 1.708463380914126e-05,
        "epoch": 0.9376895505927764,
        "step": 6802
    },
    {
        "loss": 1.1533,
        "grad_norm": 2.332695960998535,
        "learning_rate": 1.7011822204709204e-05,
        "epoch": 0.9378274055693411,
        "step": 6803
    },
    {
        "loss": 1.7934,
        "grad_norm": 1.8032172918319702,
        "learning_rate": 1.6939151659657758e-05,
        "epoch": 0.9379652605459057,
        "step": 6804
    },
    {
        "loss": 0.8601,
        "grad_norm": 2.05777645111084,
        "learning_rate": 1.686662229750896e-05,
        "epoch": 0.9381031155224704,
        "step": 6805
    },
    {
        "loss": 1.9843,
        "grad_norm": 2.8659565448760986,
        "learning_rate": 1.6794234241544705e-05,
        "epoch": 0.938240970499035,
        "step": 6806
    },
    {
        "loss": 1.4985,
        "grad_norm": 2.2372488975524902,
        "learning_rate": 1.6721987614806713e-05,
        "epoch": 0.9383788254755997,
        "step": 6807
    },
    {
        "loss": 1.7591,
        "grad_norm": 2.07733154296875,
        "learning_rate": 1.664988254009654e-05,
        "epoch": 0.9385166804521643,
        "step": 6808
    },
    {
        "loss": 1.9564,
        "grad_norm": 2.054042339324951,
        "learning_rate": 1.6577919139974862e-05,
        "epoch": 0.938654535428729,
        "step": 6809
    },
    {
        "loss": 2.1269,
        "grad_norm": 2.124704599380493,
        "learning_rate": 1.6506097536761746e-05,
        "epoch": 0.9387923904052936,
        "step": 6810
    },
    {
        "loss": 1.7878,
        "grad_norm": 2.474813222885132,
        "learning_rate": 1.643441785253612e-05,
        "epoch": 0.9389302453818583,
        "step": 6811
    },
    {
        "loss": 2.2902,
        "grad_norm": 1.379014253616333,
        "learning_rate": 1.6362880209135658e-05,
        "epoch": 0.9390681003584229,
        "step": 6812
    },
    {
        "loss": 0.8209,
        "grad_norm": 1.8855490684509277,
        "learning_rate": 1.629148472815678e-05,
        "epoch": 0.9392059553349876,
        "step": 6813
    },
    {
        "loss": 1.5971,
        "grad_norm": 2.2945377826690674,
        "learning_rate": 1.6220231530954034e-05,
        "epoch": 0.9393438103115522,
        "step": 6814
    },
    {
        "loss": 1.7983,
        "grad_norm": 2.796290159225464,
        "learning_rate": 1.6149120738640298e-05,
        "epoch": 0.9394816652881169,
        "step": 6815
    },
    {
        "loss": 1.9403,
        "grad_norm": 1.7210478782653809,
        "learning_rate": 1.6078152472086427e-05,
        "epoch": 0.9396195202646815,
        "step": 6816
    },
    {
        "loss": 1.7342,
        "grad_norm": 1.9996064901351929,
        "learning_rate": 1.6007326851920723e-05,
        "epoch": 0.9397573752412463,
        "step": 6817
    },
    {
        "loss": 2.8283,
        "grad_norm": 1.4026564359664917,
        "learning_rate": 1.593664399852942e-05,
        "epoch": 0.9398952302178109,
        "step": 6818
    },
    {
        "loss": 1.6244,
        "grad_norm": 2.066765546798706,
        "learning_rate": 1.5866104032055794e-05,
        "epoch": 0.9400330851943756,
        "step": 6819
    },
    {
        "loss": 2.1145,
        "grad_norm": 1.797536015510559,
        "learning_rate": 1.579570707240029e-05,
        "epoch": 0.9401709401709402,
        "step": 6820
    },
    {
        "loss": 1.2965,
        "grad_norm": 3.129715919494629,
        "learning_rate": 1.5725453239220456e-05,
        "epoch": 0.9403087951475049,
        "step": 6821
    },
    {
        "loss": 2.1675,
        "grad_norm": 2.3486528396606445,
        "learning_rate": 1.5655342651930316e-05,
        "epoch": 0.9404466501240695,
        "step": 6822
    },
    {
        "loss": 1.8377,
        "grad_norm": 1.2303094863891602,
        "learning_rate": 1.5585375429700678e-05,
        "epoch": 0.9405845051006342,
        "step": 6823
    },
    {
        "loss": 1.3099,
        "grad_norm": 2.1006197929382324,
        "learning_rate": 1.5515551691458406e-05,
        "epoch": 0.9407223600771988,
        "step": 6824
    },
    {
        "loss": 2.3169,
        "grad_norm": 1.3573737144470215,
        "learning_rate": 1.544587155588654e-05,
        "epoch": 0.9408602150537635,
        "step": 6825
    },
    {
        "loss": 0.9677,
        "grad_norm": 1.642296314239502,
        "learning_rate": 1.5376335141424158e-05,
        "epoch": 0.9409980700303281,
        "step": 6826
    },
    {
        "loss": 1.5219,
        "grad_norm": 1.7051396369934082,
        "learning_rate": 1.5306942566266037e-05,
        "epoch": 0.9411359250068928,
        "step": 6827
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.963376760482788,
        "learning_rate": 1.5237693948362242e-05,
        "epoch": 0.9412737799834574,
        "step": 6828
    },
    {
        "loss": 2.2104,
        "grad_norm": 1.6988362073898315,
        "learning_rate": 1.5168589405418387e-05,
        "epoch": 0.941411634960022,
        "step": 6829
    },
    {
        "loss": 1.3773,
        "grad_norm": 4.9470295906066895,
        "learning_rate": 1.5099629054895192e-05,
        "epoch": 0.9415494899365867,
        "step": 6830
    },
    {
        "loss": 1.8725,
        "grad_norm": 2.4794037342071533,
        "learning_rate": 1.5030813014008083e-05,
        "epoch": 0.9416873449131513,
        "step": 6831
    },
    {
        "loss": 2.1753,
        "grad_norm": 1.716508388519287,
        "learning_rate": 1.4962141399727336e-05,
        "epoch": 0.941825199889716,
        "step": 6832
    },
    {
        "loss": 1.7521,
        "grad_norm": 1.67653226852417,
        "learning_rate": 1.489361432877765e-05,
        "epoch": 0.9419630548662806,
        "step": 6833
    },
    {
        "loss": 1.7652,
        "grad_norm": 2.2207634449005127,
        "learning_rate": 1.4825231917638149e-05,
        "epoch": 0.9421009098428453,
        "step": 6834
    },
    {
        "loss": 0.9035,
        "grad_norm": 1.8960518836975098,
        "learning_rate": 1.4756994282542125e-05,
        "epoch": 0.9422387648194099,
        "step": 6835
    },
    {
        "loss": 1.0625,
        "grad_norm": 2.342406988143921,
        "learning_rate": 1.4688901539476608e-05,
        "epoch": 0.9423766197959746,
        "step": 6836
    },
    {
        "loss": 2.4318,
        "grad_norm": 1.3047394752502441,
        "learning_rate": 1.4620953804182324e-05,
        "epoch": 0.9425144747725392,
        "step": 6837
    },
    {
        "loss": 1.4416,
        "grad_norm": 1.8102904558181763,
        "learning_rate": 1.455315119215378e-05,
        "epoch": 0.942652329749104,
        "step": 6838
    },
    {
        "loss": 2.0018,
        "grad_norm": 1.4741051197052002,
        "learning_rate": 1.4485493818638497e-05,
        "epoch": 0.9427901847256686,
        "step": 6839
    },
    {
        "loss": 1.7693,
        "grad_norm": 2.991626739501953,
        "learning_rate": 1.4417981798637436e-05,
        "epoch": 0.9429280397022333,
        "step": 6840
    },
    {
        "loss": 2.3278,
        "grad_norm": 1.9536243677139282,
        "learning_rate": 1.4350615246904197e-05,
        "epoch": 0.9430658946787979,
        "step": 6841
    },
    {
        "loss": 2.3535,
        "grad_norm": 1.916909098625183,
        "learning_rate": 1.4283394277945317e-05,
        "epoch": 0.9432037496553626,
        "step": 6842
    },
    {
        "loss": 1.7416,
        "grad_norm": 1.5666275024414062,
        "learning_rate": 1.4216319006019895e-05,
        "epoch": 0.9433416046319272,
        "step": 6843
    },
    {
        "loss": 1.3073,
        "grad_norm": 2.356762409210205,
        "learning_rate": 1.4149389545139258e-05,
        "epoch": 0.9434794596084919,
        "step": 6844
    },
    {
        "loss": 1.7466,
        "grad_norm": 1.9291889667510986,
        "learning_rate": 1.4082606009066835e-05,
        "epoch": 0.9436173145850565,
        "step": 6845
    },
    {
        "loss": 2.0891,
        "grad_norm": 1.4353426694869995,
        "learning_rate": 1.4015968511318267e-05,
        "epoch": 0.9437551695616212,
        "step": 6846
    },
    {
        "loss": 1.7223,
        "grad_norm": 2.3665711879730225,
        "learning_rate": 1.3949477165160696e-05,
        "epoch": 0.9438930245381858,
        "step": 6847
    },
    {
        "loss": 1.8821,
        "grad_norm": 2.0976405143737793,
        "learning_rate": 1.3883132083613104e-05,
        "epoch": 0.9440308795147505,
        "step": 6848
    },
    {
        "loss": 2.2381,
        "grad_norm": 2.3563790321350098,
        "learning_rate": 1.3816933379445651e-05,
        "epoch": 0.9441687344913151,
        "step": 6849
    },
    {
        "loss": 1.848,
        "grad_norm": 1.6556525230407715,
        "learning_rate": 1.375088116517972e-05,
        "epoch": 0.9443065894678798,
        "step": 6850
    },
    {
        "loss": 1.9229,
        "grad_norm": 1.8746367692947388,
        "learning_rate": 1.3684975553087842e-05,
        "epoch": 0.9444444444444444,
        "step": 6851
    },
    {
        "loss": 1.2262,
        "grad_norm": 1.2543216943740845,
        "learning_rate": 1.3619216655193123e-05,
        "epoch": 0.9445822994210091,
        "step": 6852
    },
    {
        "loss": 2.4494,
        "grad_norm": 1.412648320198059,
        "learning_rate": 1.355360458326953e-05,
        "epoch": 0.9447201543975737,
        "step": 6853
    },
    {
        "loss": 1.6264,
        "grad_norm": 2.1555891036987305,
        "learning_rate": 1.3488139448841397e-05,
        "epoch": 0.9448580093741384,
        "step": 6854
    },
    {
        "loss": 0.7348,
        "grad_norm": 2.3581981658935547,
        "learning_rate": 1.3422821363183147e-05,
        "epoch": 0.944995864350703,
        "step": 6855
    },
    {
        "loss": 2.6945,
        "grad_norm": 1.3070083856582642,
        "learning_rate": 1.3357650437319491e-05,
        "epoch": 0.9451337193272678,
        "step": 6856
    },
    {
        "loss": 1.902,
        "grad_norm": 2.3580574989318848,
        "learning_rate": 1.3292626782024864e-05,
        "epoch": 0.9452715743038324,
        "step": 6857
    },
    {
        "loss": 0.736,
        "grad_norm": 1.4752795696258545,
        "learning_rate": 1.3227750507823278e-05,
        "epoch": 0.9454094292803971,
        "step": 6858
    },
    {
        "loss": 0.6152,
        "grad_norm": 1.4347970485687256,
        "learning_rate": 1.3163021724988456e-05,
        "epoch": 0.9455472842569617,
        "step": 6859
    },
    {
        "loss": 2.2128,
        "grad_norm": 1.2071174383163452,
        "learning_rate": 1.3098440543543366e-05,
        "epoch": 0.9456851392335264,
        "step": 6860
    },
    {
        "loss": 1.926,
        "grad_norm": 1.9843636751174927,
        "learning_rate": 1.3034007073260002e-05,
        "epoch": 0.945822994210091,
        "step": 6861
    },
    {
        "loss": 1.2603,
        "grad_norm": 1.7640235424041748,
        "learning_rate": 1.296972142365932e-05,
        "epoch": 0.9459608491866557,
        "step": 6862
    },
    {
        "loss": 2.3344,
        "grad_norm": 1.784261703491211,
        "learning_rate": 1.2905583704010959e-05,
        "epoch": 0.9460987041632203,
        "step": 6863
    },
    {
        "loss": 1.4897,
        "grad_norm": 2.9516286849975586,
        "learning_rate": 1.2841594023333258e-05,
        "epoch": 0.946236559139785,
        "step": 6864
    },
    {
        "loss": 0.7834,
        "grad_norm": 2.8070755004882812,
        "learning_rate": 1.2777752490392925e-05,
        "epoch": 0.9463744141163496,
        "step": 6865
    },
    {
        "loss": 2.36,
        "grad_norm": 1.223839282989502,
        "learning_rate": 1.271405921370462e-05,
        "epoch": 0.9465122690929143,
        "step": 6866
    },
    {
        "loss": 1.9816,
        "grad_norm": 1.2664649486541748,
        "learning_rate": 1.2650514301531236e-05,
        "epoch": 0.9466501240694789,
        "step": 6867
    },
    {
        "loss": 1.9213,
        "grad_norm": 1.6451313495635986,
        "learning_rate": 1.2587117861883491e-05,
        "epoch": 0.9467879790460436,
        "step": 6868
    },
    {
        "loss": 1.4501,
        "grad_norm": 1.459951639175415,
        "learning_rate": 1.2523870002519567e-05,
        "epoch": 0.9469258340226082,
        "step": 6869
    },
    {
        "loss": 2.3727,
        "grad_norm": 1.6778695583343506,
        "learning_rate": 1.2460770830945212e-05,
        "epoch": 0.9470636889991728,
        "step": 6870
    },
    {
        "loss": 1.4239,
        "grad_norm": 2.193138360977173,
        "learning_rate": 1.2397820454413345e-05,
        "epoch": 0.9472015439757375,
        "step": 6871
    },
    {
        "loss": 2.4903,
        "grad_norm": 2.1944992542266846,
        "learning_rate": 1.2335018979924084e-05,
        "epoch": 0.9473393989523021,
        "step": 6872
    },
    {
        "loss": 1.482,
        "grad_norm": 2.8414461612701416,
        "learning_rate": 1.2272366514224476e-05,
        "epoch": 0.9474772539288668,
        "step": 6873
    },
    {
        "loss": 1.3367,
        "grad_norm": 3.3864519596099854,
        "learning_rate": 1.2209863163808166e-05,
        "epoch": 0.9476151089054314,
        "step": 6874
    },
    {
        "loss": 1.8718,
        "grad_norm": 2.3452062606811523,
        "learning_rate": 1.2147509034915316e-05,
        "epoch": 0.9477529638819961,
        "step": 6875
    },
    {
        "loss": 1.9134,
        "grad_norm": 1.8260577917099,
        "learning_rate": 1.2085304233532669e-05,
        "epoch": 0.9478908188585607,
        "step": 6876
    },
    {
        "loss": 1.4971,
        "grad_norm": 1.3544018268585205,
        "learning_rate": 1.2023248865392866e-05,
        "epoch": 0.9480286738351255,
        "step": 6877
    },
    {
        "loss": 2.1281,
        "grad_norm": 1.0582140684127808,
        "learning_rate": 1.196134303597476e-05,
        "epoch": 0.9481665288116901,
        "step": 6878
    },
    {
        "loss": 2.023,
        "grad_norm": 1.627144455909729,
        "learning_rate": 1.189958685050303e-05,
        "epoch": 0.9483043837882548,
        "step": 6879
    },
    {
        "loss": 1.3545,
        "grad_norm": 3.6516525745391846,
        "learning_rate": 1.1837980413947759e-05,
        "epoch": 0.9484422387648194,
        "step": 6880
    },
    {
        "loss": 1.0894,
        "grad_norm": 2.1159727573394775,
        "learning_rate": 1.1776523831024788e-05,
        "epoch": 0.9485800937413841,
        "step": 6881
    },
    {
        "loss": 1.1272,
        "grad_norm": 2.4479854106903076,
        "learning_rate": 1.1715217206195073e-05,
        "epoch": 0.9487179487179487,
        "step": 6882
    },
    {
        "loss": 1.7005,
        "grad_norm": 2.585223913192749,
        "learning_rate": 1.1654060643664622e-05,
        "epoch": 0.9488558036945134,
        "step": 6883
    },
    {
        "loss": 1.91,
        "grad_norm": 2.3195226192474365,
        "learning_rate": 1.1593054247384615e-05,
        "epoch": 0.948993658671078,
        "step": 6884
    },
    {
        "loss": 2.5222,
        "grad_norm": 1.314884901046753,
        "learning_rate": 1.153219812105072e-05,
        "epoch": 0.9491315136476427,
        "step": 6885
    },
    {
        "loss": 1.6306,
        "grad_norm": 1.3159880638122559,
        "learning_rate": 1.147149236810342e-05,
        "epoch": 0.9492693686242073,
        "step": 6886
    },
    {
        "loss": 2.5145,
        "grad_norm": 1.374200463294983,
        "learning_rate": 1.1410937091727403e-05,
        "epoch": 0.949407223600772,
        "step": 6887
    },
    {
        "loss": 2.2222,
        "grad_norm": 1.484704613685608,
        "learning_rate": 1.135053239485162e-05,
        "epoch": 0.9495450785773366,
        "step": 6888
    },
    {
        "loss": 1.8681,
        "grad_norm": 1.7411139011383057,
        "learning_rate": 1.1290278380149245e-05,
        "epoch": 0.9496829335539013,
        "step": 6889
    },
    {
        "loss": 1.5513,
        "grad_norm": 3.120421886444092,
        "learning_rate": 1.123017515003706e-05,
        "epoch": 0.9498207885304659,
        "step": 6890
    },
    {
        "loss": 1.501,
        "grad_norm": 2.131704092025757,
        "learning_rate": 1.1170222806675778e-05,
        "epoch": 0.9499586435070306,
        "step": 6891
    },
    {
        "loss": 1.2906,
        "grad_norm": 2.6517722606658936,
        "learning_rate": 1.1110421451969632e-05,
        "epoch": 0.9500964984835952,
        "step": 6892
    },
    {
        "loss": 1.8703,
        "grad_norm": 3.520719528198242,
        "learning_rate": 1.1050771187565966e-05,
        "epoch": 0.9502343534601599,
        "step": 6893
    },
    {
        "loss": 1.2551,
        "grad_norm": 1.8969804048538208,
        "learning_rate": 1.0991272114855644e-05,
        "epoch": 0.9503722084367245,
        "step": 6894
    },
    {
        "loss": 0.8665,
        "grad_norm": 2.4648640155792236,
        "learning_rate": 1.0931924334972288e-05,
        "epoch": 0.9505100634132893,
        "step": 6895
    },
    {
        "loss": 1.4808,
        "grad_norm": 2.31003737449646,
        "learning_rate": 1.0872727948792395e-05,
        "epoch": 0.9506479183898539,
        "step": 6896
    },
    {
        "loss": 1.3148,
        "grad_norm": 2.1798675060272217,
        "learning_rate": 1.0813683056935254e-05,
        "epoch": 0.9507857733664186,
        "step": 6897
    },
    {
        "loss": 1.1742,
        "grad_norm": 2.4539315700531006,
        "learning_rate": 1.075478975976264e-05,
        "epoch": 0.9509236283429832,
        "step": 6898
    },
    {
        "loss": 2.0497,
        "grad_norm": 2.7776482105255127,
        "learning_rate": 1.0696048157378535e-05,
        "epoch": 0.9510614833195479,
        "step": 6899
    },
    {
        "loss": 2.0959,
        "grad_norm": 1.1894299983978271,
        "learning_rate": 1.0637458349629125e-05,
        "epoch": 0.9511993382961125,
        "step": 6900
    },
    {
        "loss": 2.2348,
        "grad_norm": 1.1668329238891602,
        "learning_rate": 1.0579020436102527e-05,
        "epoch": 0.9513371932726772,
        "step": 6901
    },
    {
        "loss": 1.9705,
        "grad_norm": 2.11456561088562,
        "learning_rate": 1.0520734516128827e-05,
        "epoch": 0.9514750482492418,
        "step": 6902
    },
    {
        "loss": 2.1396,
        "grad_norm": 1.7978051900863647,
        "learning_rate": 1.0462600688779722e-05,
        "epoch": 0.9516129032258065,
        "step": 6903
    },
    {
        "loss": 2.0134,
        "grad_norm": 2.2229228019714355,
        "learning_rate": 1.0404619052868236e-05,
        "epoch": 0.9517507582023711,
        "step": 6904
    },
    {
        "loss": 1.9667,
        "grad_norm": 1.3974311351776123,
        "learning_rate": 1.0346789706948857e-05,
        "epoch": 0.9518886131789358,
        "step": 6905
    },
    {
        "loss": 1.9917,
        "grad_norm": 2.8441665172576904,
        "learning_rate": 1.0289112749317254e-05,
        "epoch": 0.9520264681555004,
        "step": 6906
    },
    {
        "loss": 1.9142,
        "grad_norm": 1.0262527465820312,
        "learning_rate": 1.0231588278009907e-05,
        "epoch": 0.9521643231320651,
        "step": 6907
    },
    {
        "loss": 1.3901,
        "grad_norm": 2.2981746196746826,
        "learning_rate": 1.0174216390804192e-05,
        "epoch": 0.9523021781086297,
        "step": 6908
    },
    {
        "loss": 1.2076,
        "grad_norm": 2.5419304370880127,
        "learning_rate": 1.011699718521809e-05,
        "epoch": 0.9524400330851944,
        "step": 6909
    },
    {
        "loss": 1.451,
        "grad_norm": 2.4704692363739014,
        "learning_rate": 1.005993075851015e-05,
        "epoch": 0.952577888061759,
        "step": 6910
    },
    {
        "loss": 1.2528,
        "grad_norm": 2.3406875133514404,
        "learning_rate": 1.0003017207679243e-05,
        "epoch": 0.9527157430383237,
        "step": 6911
    },
    {
        "loss": 1.6158,
        "grad_norm": 1.9667965173721313,
        "learning_rate": 9.94625662946428e-06,
        "epoch": 0.9528535980148883,
        "step": 6912
    },
    {
        "loss": 2.2521,
        "grad_norm": 2.8359663486480713,
        "learning_rate": 9.88964912034417e-06,
        "epoch": 0.9529914529914529,
        "step": 6913
    },
    {
        "loss": 1.7455,
        "grad_norm": 2.320998191833496,
        "learning_rate": 9.833194776537747e-06,
        "epoch": 0.9531293079680176,
        "step": 6914
    },
    {
        "loss": 1.6355,
        "grad_norm": 1.4864983558654785,
        "learning_rate": 9.776893694003342e-06,
        "epoch": 0.9532671629445822,
        "step": 6915
    },
    {
        "loss": 1.3507,
        "grad_norm": 2.6796867847442627,
        "learning_rate": 9.720745968438938e-06,
        "epoch": 0.953405017921147,
        "step": 6916
    },
    {
        "loss": 2.3945,
        "grad_norm": 1.8052929639816284,
        "learning_rate": 9.664751695281837e-06,
        "epoch": 0.9535428728977116,
        "step": 6917
    },
    {
        "loss": 2.1859,
        "grad_norm": 2.3222763538360596,
        "learning_rate": 9.60891096970834e-06,
        "epoch": 0.9536807278742763,
        "step": 6918
    },
    {
        "loss": 0.5047,
        "grad_norm": 1.2531908750534058,
        "learning_rate": 9.553223886633989e-06,
        "epoch": 0.9538185828508409,
        "step": 6919
    },
    {
        "loss": 1.3142,
        "grad_norm": 2.514042615890503,
        "learning_rate": 9.497690540713023e-06,
        "epoch": 0.9539564378274056,
        "step": 6920
    },
    {
        "loss": 2.4246,
        "grad_norm": 2.2372725009918213,
        "learning_rate": 9.44231102633829e-06,
        "epoch": 0.9540942928039702,
        "step": 6921
    },
    {
        "loss": 1.3359,
        "grad_norm": 2.4534857273101807,
        "learning_rate": 9.387085437641408e-06,
        "epoch": 0.9542321477805349,
        "step": 6922
    },
    {
        "loss": 1.276,
        "grad_norm": 2.4017512798309326,
        "learning_rate": 9.332013868492117e-06,
        "epoch": 0.9543700027570995,
        "step": 6923
    },
    {
        "loss": 2.1933,
        "grad_norm": 1.2195656299591064,
        "learning_rate": 9.277096412498564e-06,
        "epoch": 0.9545078577336642,
        "step": 6924
    },
    {
        "loss": 1.5979,
        "grad_norm": 2.779756784439087,
        "learning_rate": 9.222333163006791e-06,
        "epoch": 0.9546457127102288,
        "step": 6925
    },
    {
        "loss": 1.4561,
        "grad_norm": 2.093622922897339,
        "learning_rate": 9.167724213100738e-06,
        "epoch": 0.9547835676867935,
        "step": 6926
    },
    {
        "loss": 1.2687,
        "grad_norm": 2.4670324325561523,
        "learning_rate": 9.11326965560223e-06,
        "epoch": 0.9549214226633581,
        "step": 6927
    },
    {
        "loss": 1.7968,
        "grad_norm": 2.009441614151001,
        "learning_rate": 9.058969583070397e-06,
        "epoch": 0.9550592776399228,
        "step": 6928
    },
    {
        "loss": 0.7507,
        "grad_norm": 2.042090892791748,
        "learning_rate": 9.004824087802032e-06,
        "epoch": 0.9551971326164874,
        "step": 6929
    },
    {
        "loss": 1.7388,
        "grad_norm": 1.8244725465774536,
        "learning_rate": 8.950833261831126e-06,
        "epoch": 0.9553349875930521,
        "step": 6930
    },
    {
        "loss": 1.186,
        "grad_norm": 2.712406873703003,
        "learning_rate": 8.896997196928624e-06,
        "epoch": 0.9554728425696167,
        "step": 6931
    },
    {
        "loss": 1.5861,
        "grad_norm": 1.5707110166549683,
        "learning_rate": 8.843315984602641e-06,
        "epoch": 0.9556106975461814,
        "step": 6932
    },
    {
        "loss": 2.5661,
        "grad_norm": 1.2825233936309814,
        "learning_rate": 8.789789716097896e-06,
        "epoch": 0.955748552522746,
        "step": 6933
    },
    {
        "loss": 1.9173,
        "grad_norm": 2.754213809967041,
        "learning_rate": 8.73641848239577e-06,
        "epoch": 0.9558864074993108,
        "step": 6934
    },
    {
        "loss": 2.1488,
        "grad_norm": 1.5624972581863403,
        "learning_rate": 8.683202374214205e-06,
        "epoch": 0.9560242624758754,
        "step": 6935
    },
    {
        "loss": 2.0254,
        "grad_norm": 1.437442660331726,
        "learning_rate": 8.63014148200747e-06,
        "epoch": 0.9561621174524401,
        "step": 6936
    },
    {
        "loss": 0.9428,
        "grad_norm": 2.216428756713867,
        "learning_rate": 8.57723589596593e-06,
        "epoch": 0.9562999724290047,
        "step": 6937
    },
    {
        "loss": 1.719,
        "grad_norm": 1.8860783576965332,
        "learning_rate": 8.524485706015983e-06,
        "epoch": 0.9564378274055694,
        "step": 6938
    },
    {
        "loss": 2.2275,
        "grad_norm": 1.704559087753296,
        "learning_rate": 8.471891001819853e-06,
        "epoch": 0.956575682382134,
        "step": 6939
    },
    {
        "loss": 1.9283,
        "grad_norm": 1.8988593816757202,
        "learning_rate": 8.419451872775597e-06,
        "epoch": 0.9567135373586987,
        "step": 6940
    },
    {
        "loss": 1.6436,
        "grad_norm": 1.4198864698410034,
        "learning_rate": 8.367168408016834e-06,
        "epoch": 0.9568513923352633,
        "step": 6941
    },
    {
        "loss": 1.6558,
        "grad_norm": 1.8606603145599365,
        "learning_rate": 8.315040696412368e-06,
        "epoch": 0.956989247311828,
        "step": 6942
    },
    {
        "loss": 2.0293,
        "grad_norm": 2.1481029987335205,
        "learning_rate": 8.263068826566512e-06,
        "epoch": 0.9571271022883926,
        "step": 6943
    },
    {
        "loss": 1.6046,
        "grad_norm": 1.9471640586853027,
        "learning_rate": 8.211252886818633e-06,
        "epoch": 0.9572649572649573,
        "step": 6944
    },
    {
        "loss": 1.8509,
        "grad_norm": 1.7636082172393799,
        "learning_rate": 8.159592965242968e-06,
        "epoch": 0.9574028122415219,
        "step": 6945
    },
    {
        "loss": 1.9809,
        "grad_norm": 1.8377448320388794,
        "learning_rate": 8.108089149648612e-06,
        "epoch": 0.9575406672180866,
        "step": 6946
    },
    {
        "loss": 1.7216,
        "grad_norm": 1.646488904953003,
        "learning_rate": 8.056741527579248e-06,
        "epoch": 0.9576785221946512,
        "step": 6947
    },
    {
        "loss": 1.5977,
        "grad_norm": 3.485581159591675,
        "learning_rate": 8.005550186313215e-06,
        "epoch": 0.9578163771712159,
        "step": 6948
    },
    {
        "loss": 0.9279,
        "grad_norm": 2.096768379211426,
        "learning_rate": 7.954515212863157e-06,
        "epoch": 0.9579542321477805,
        "step": 6949
    },
    {
        "loss": 1.5285,
        "grad_norm": 2.162832736968994,
        "learning_rate": 7.903636693975857e-06,
        "epoch": 0.9580920871243452,
        "step": 6950
    },
    {
        "loss": 1.7121,
        "grad_norm": 1.7966760396957397,
        "learning_rate": 7.852914716132187e-06,
        "epoch": 0.9582299421009098,
        "step": 6951
    },
    {
        "loss": 1.9257,
        "grad_norm": 2.1118531227111816,
        "learning_rate": 7.802349365547057e-06,
        "epoch": 0.9583677970774745,
        "step": 6952
    },
    {
        "loss": 2.1401,
        "grad_norm": 2.7634944915771484,
        "learning_rate": 7.75194072816896e-06,
        "epoch": 0.9585056520540391,
        "step": 6953
    },
    {
        "loss": 2.1031,
        "grad_norm": 1.6775397062301636,
        "learning_rate": 7.701688889680158e-06,
        "epoch": 0.9586435070306039,
        "step": 6954
    },
    {
        "loss": 1.6377,
        "grad_norm": 2.4630939960479736,
        "learning_rate": 7.651593935496427e-06,
        "epoch": 0.9587813620071685,
        "step": 6955
    },
    {
        "loss": 1.9524,
        "grad_norm": 2.2067794799804688,
        "learning_rate": 7.6016559507666615e-06,
        "epoch": 0.9589192169837331,
        "step": 6956
    },
    {
        "loss": 2.1865,
        "grad_norm": 2.1467809677124023,
        "learning_rate": 7.551875020373211e-06,
        "epoch": 0.9590570719602978,
        "step": 6957
    },
    {
        "loss": 1.8217,
        "grad_norm": 1.9882688522338867,
        "learning_rate": 7.502251228931279e-06,
        "epoch": 0.9591949269368624,
        "step": 6958
    },
    {
        "loss": 2.0235,
        "grad_norm": 1.6250182390213013,
        "learning_rate": 7.452784660789014e-06,
        "epoch": 0.9593327819134271,
        "step": 6959
    },
    {
        "loss": 2.4393,
        "grad_norm": 1.555025339126587,
        "learning_rate": 7.403475400027426e-06,
        "epoch": 0.9594706368899917,
        "step": 6960
    },
    {
        "loss": 1.5073,
        "grad_norm": 2.9450058937072754,
        "learning_rate": 7.354323530459983e-06,
        "epoch": 0.9596084918665564,
        "step": 6961
    },
    {
        "loss": 2.2094,
        "grad_norm": 2.025970697402954,
        "learning_rate": 7.305329135632821e-06,
        "epoch": 0.959746346843121,
        "step": 6962
    },
    {
        "loss": 1.2432,
        "grad_norm": 1.7540552616119385,
        "learning_rate": 7.2564922988242026e-06,
        "epoch": 0.9598842018196857,
        "step": 6963
    },
    {
        "loss": 1.977,
        "grad_norm": 1.1352739334106445,
        "learning_rate": 7.207813103044659e-06,
        "epoch": 0.9600220567962503,
        "step": 6964
    },
    {
        "loss": 1.8857,
        "grad_norm": 1.891561508178711,
        "learning_rate": 7.159291631036869e-06,
        "epoch": 0.960159911772815,
        "step": 6965
    },
    {
        "loss": 0.8555,
        "grad_norm": 2.2104556560516357,
        "learning_rate": 7.110927965275238e-06,
        "epoch": 0.9602977667493796,
        "step": 6966
    },
    {
        "loss": 1.9921,
        "grad_norm": 2.651228427886963,
        "learning_rate": 7.062722187966097e-06,
        "epoch": 0.9604356217259443,
        "step": 6967
    },
    {
        "loss": 1.8607,
        "grad_norm": 1.6023588180541992,
        "learning_rate": 7.014674381047392e-06,
        "epoch": 0.9605734767025089,
        "step": 6968
    },
    {
        "loss": 1.6873,
        "grad_norm": 1.678282618522644,
        "learning_rate": 6.9667846261884274e-06,
        "epoch": 0.9607113316790736,
        "step": 6969
    },
    {
        "loss": 1.3978,
        "grad_norm": 1.957445502281189,
        "learning_rate": 6.9190530047900214e-06,
        "epoch": 0.9608491866556382,
        "step": 6970
    },
    {
        "loss": 0.8415,
        "grad_norm": 3.3023157119750977,
        "learning_rate": 6.871479597984076e-06,
        "epoch": 0.9609870416322029,
        "step": 6971
    },
    {
        "loss": 2.5994,
        "grad_norm": 1.8322935104370117,
        "learning_rate": 6.8240644866335855e-06,
        "epoch": 0.9611248966087675,
        "step": 6972
    },
    {
        "loss": 2.4496,
        "grad_norm": 1.3349859714508057,
        "learning_rate": 6.776807751332548e-06,
        "epoch": 0.9612627515853323,
        "step": 6973
    },
    {
        "loss": 1.8372,
        "grad_norm": 2.4644505977630615,
        "learning_rate": 6.729709472405799e-06,
        "epoch": 0.9614006065618969,
        "step": 6974
    },
    {
        "loss": 1.5141,
        "grad_norm": 2.1787405014038086,
        "learning_rate": 6.682769729908711e-06,
        "epoch": 0.9615384615384616,
        "step": 6975
    },
    {
        "loss": 2.2268,
        "grad_norm": 1.3586070537567139,
        "learning_rate": 6.635988603627219e-06,
        "epoch": 0.9616763165150262,
        "step": 6976
    },
    {
        "loss": 1.3995,
        "grad_norm": 2.158463478088379,
        "learning_rate": 6.589366173077627e-06,
        "epoch": 0.9618141714915909,
        "step": 6977
    },
    {
        "loss": 1.4665,
        "grad_norm": 1.9988094568252563,
        "learning_rate": 6.5429025175066e-06,
        "epoch": 0.9619520264681555,
        "step": 6978
    },
    {
        "loss": 1.8694,
        "grad_norm": 1.6446506977081299,
        "learning_rate": 6.496597715890907e-06,
        "epoch": 0.9620898814447202,
        "step": 6979
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.7828691005706787,
        "learning_rate": 6.450451846937156e-06,
        "epoch": 0.9622277364212848,
        "step": 6980
    },
    {
        "loss": 1.5812,
        "grad_norm": 2.484870433807373,
        "learning_rate": 6.40446498908196e-06,
        "epoch": 0.9623655913978495,
        "step": 6981
    },
    {
        "loss": 1.4812,
        "grad_norm": 1.4355653524398804,
        "learning_rate": 6.35863722049167e-06,
        "epoch": 0.9625034463744141,
        "step": 6982
    },
    {
        "loss": 0.5959,
        "grad_norm": 2.607975482940674,
        "learning_rate": 6.31296861906211e-06,
        "epoch": 0.9626413013509788,
        "step": 6983
    },
    {
        "loss": 1.262,
        "grad_norm": 3.4468133449554443,
        "learning_rate": 6.267459262418596e-06,
        "epoch": 0.9627791563275434,
        "step": 6984
    },
    {
        "loss": 0.9882,
        "grad_norm": 1.1156792640686035,
        "learning_rate": 6.222109227915729e-06,
        "epoch": 0.9629170113041081,
        "step": 6985
    },
    {
        "loss": 2.0635,
        "grad_norm": 1.3534064292907715,
        "learning_rate": 6.176918592637415e-06,
        "epoch": 0.9630548662806727,
        "step": 6986
    },
    {
        "loss": 2.0054,
        "grad_norm": 2.570053815841675,
        "learning_rate": 6.1318874333966e-06,
        "epoch": 0.9631927212572374,
        "step": 6987
    },
    {
        "loss": 2.1566,
        "grad_norm": 1.7601603269577026,
        "learning_rate": 6.087015826735076e-06,
        "epoch": 0.963330576233802,
        "step": 6988
    },
    {
        "loss": 2.1721,
        "grad_norm": 2.1049740314483643,
        "learning_rate": 6.042303848923425e-06,
        "epoch": 0.9634684312103667,
        "step": 6989
    },
    {
        "loss": 2.2476,
        "grad_norm": 1.3739901781082153,
        "learning_rate": 5.997751575961064e-06,
        "epoch": 0.9636062861869313,
        "step": 6990
    },
    {
        "loss": 2.038,
        "grad_norm": 2.4644365310668945,
        "learning_rate": 5.953359083575727e-06,
        "epoch": 0.963744141163496,
        "step": 6991
    },
    {
        "loss": 2.0841,
        "grad_norm": 1.2824052572250366,
        "learning_rate": 5.909126447223712e-06,
        "epoch": 0.9638819961400606,
        "step": 6992
    },
    {
        "loss": 1.8862,
        "grad_norm": 2.681502103805542,
        "learning_rate": 5.865053742089632e-06,
        "epoch": 0.9640198511166254,
        "step": 6993
    },
    {
        "loss": 1.1463,
        "grad_norm": 1.5585894584655762,
        "learning_rate": 5.8211410430860865e-06,
        "epoch": 0.96415770609319,
        "step": 6994
    },
    {
        "loss": 1.6341,
        "grad_norm": 2.4098777770996094,
        "learning_rate": 5.777388424853891e-06,
        "epoch": 0.9642955610697547,
        "step": 6995
    },
    {
        "loss": 1.1353,
        "grad_norm": 1.9747262001037598,
        "learning_rate": 5.733795961761656e-06,
        "epoch": 0.9644334160463193,
        "step": 6996
    },
    {
        "loss": 1.6158,
        "grad_norm": 2.717773914337158,
        "learning_rate": 5.690363727905712e-06,
        "epoch": 0.964571271022884,
        "step": 6997
    },
    {
        "loss": 1.8303,
        "grad_norm": 1.7084088325500488,
        "learning_rate": 5.647091797110215e-06,
        "epoch": 0.9647091259994486,
        "step": 6998
    },
    {
        "loss": 2.0772,
        "grad_norm": 1.7459360361099243,
        "learning_rate": 5.603980242926654e-06,
        "epoch": 0.9648469809760132,
        "step": 6999
    },
    {
        "loss": 1.9468,
        "grad_norm": 2.3468849658966064,
        "learning_rate": 5.561029138634122e-06,
        "epoch": 0.9649848359525779,
        "step": 7000
    },
    {
        "loss": 1.6045,
        "grad_norm": 2.361194610595703,
        "learning_rate": 5.518238557238797e-06,
        "epoch": 0.9651226909291425,
        "step": 7001
    },
    {
        "loss": 2.0838,
        "grad_norm": 1.8735185861587524,
        "learning_rate": 5.475608571474078e-06,
        "epoch": 0.9652605459057072,
        "step": 7002
    },
    {
        "loss": 0.734,
        "grad_norm": 1.7980635166168213,
        "learning_rate": 5.433139253800479e-06,
        "epoch": 0.9653984008822718,
        "step": 7003
    },
    {
        "loss": 2.0564,
        "grad_norm": 2.1976771354675293,
        "learning_rate": 5.390830676405223e-06,
        "epoch": 0.9655362558588365,
        "step": 7004
    },
    {
        "loss": 0.4707,
        "grad_norm": 2.177762508392334,
        "learning_rate": 5.348682911202485e-06,
        "epoch": 0.9656741108354011,
        "step": 7005
    },
    {
        "loss": 1.9679,
        "grad_norm": 2.101285934448242,
        "learning_rate": 5.306696029833103e-06,
        "epoch": 0.9658119658119658,
        "step": 7006
    },
    {
        "loss": 1.6291,
        "grad_norm": 2.6827433109283447,
        "learning_rate": 5.264870103664288e-06,
        "epoch": 0.9659498207885304,
        "step": 7007
    },
    {
        "loss": 1.5091,
        "grad_norm": 2.432543992996216,
        "learning_rate": 5.2232052037898715e-06,
        "epoch": 0.9660876757650951,
        "step": 7008
    },
    {
        "loss": 1.8362,
        "grad_norm": 1.3670889139175415,
        "learning_rate": 5.181701401029837e-06,
        "epoch": 0.9662255307416597,
        "step": 7009
    },
    {
        "loss": 2.179,
        "grad_norm": 1.3263814449310303,
        "learning_rate": 5.140358765930331e-06,
        "epoch": 0.9663633857182244,
        "step": 7010
    },
    {
        "loss": 1.5625,
        "grad_norm": 1.2232052087783813,
        "learning_rate": 5.099177368763663e-06,
        "epoch": 0.966501240694789,
        "step": 7011
    },
    {
        "loss": 1.9704,
        "grad_norm": 2.017606258392334,
        "learning_rate": 5.058157279528064e-06,
        "epoch": 0.9666390956713538,
        "step": 7012
    },
    {
        "loss": 0.7596,
        "grad_norm": 1.7615313529968262,
        "learning_rate": 5.0172985679474925e-06,
        "epoch": 0.9667769506479184,
        "step": 7013
    },
    {
        "loss": 0.9983,
        "grad_norm": 1.9964654445648193,
        "learning_rate": 4.976601303471662e-06,
        "epoch": 0.9669148056244831,
        "step": 7014
    },
    {
        "loss": 1.9894,
        "grad_norm": 2.3158228397369385,
        "learning_rate": 4.936065555275815e-06,
        "epoch": 0.9670526606010477,
        "step": 7015
    },
    {
        "loss": 2.3019,
        "grad_norm": 1.215701937675476,
        "learning_rate": 4.895691392260738e-06,
        "epoch": 0.9671905155776124,
        "step": 7016
    },
    {
        "loss": 1.4632,
        "grad_norm": 2.0368776321411133,
        "learning_rate": 4.855478883052589e-06,
        "epoch": 0.967328370554177,
        "step": 7017
    },
    {
        "loss": 1.8332,
        "grad_norm": 1.9177905321121216,
        "learning_rate": 4.815428096002572e-06,
        "epoch": 0.9674662255307417,
        "step": 7018
    },
    {
        "loss": 1.8162,
        "grad_norm": 2.486227512359619,
        "learning_rate": 4.775539099187176e-06,
        "epoch": 0.9676040805073063,
        "step": 7019
    },
    {
        "loss": 1.8855,
        "grad_norm": 3.040588855743408,
        "learning_rate": 4.735811960407877e-06,
        "epoch": 0.967741935483871,
        "step": 7020
    },
    {
        "loss": 2.2468,
        "grad_norm": 2.0691914558410645,
        "learning_rate": 4.696246747190947e-06,
        "epoch": 0.9678797904604356,
        "step": 7021
    },
    {
        "loss": 0.8209,
        "grad_norm": 1.9839109182357788,
        "learning_rate": 4.6568435267874134e-06,
        "epoch": 0.9680176454370003,
        "step": 7022
    },
    {
        "loss": 1.7962,
        "grad_norm": 2.1358420848846436,
        "learning_rate": 4.61760236617309e-06,
        "epoch": 0.9681555004135649,
        "step": 7023
    },
    {
        "loss": 2.338,
        "grad_norm": 1.979472279548645,
        "learning_rate": 4.578523332048179e-06,
        "epoch": 0.9682933553901296,
        "step": 7024
    },
    {
        "loss": 1.7286,
        "grad_norm": 1.9022362232208252,
        "learning_rate": 4.539606490837467e-06,
        "epoch": 0.9684312103666942,
        "step": 7025
    },
    {
        "loss": 1.943,
        "grad_norm": 1.6765042543411255,
        "learning_rate": 4.500851908689874e-06,
        "epoch": 0.9685690653432589,
        "step": 7026
    },
    {
        "loss": 1.5533,
        "grad_norm": 2.053739070892334,
        "learning_rate": 4.4622596514785955e-06,
        "epoch": 0.9687069203198235,
        "step": 7027
    },
    {
        "loss": 1.9038,
        "grad_norm": 1.8476444482803345,
        "learning_rate": 4.423829784801015e-06,
        "epoch": 0.9688447752963882,
        "step": 7028
    },
    {
        "loss": 1.398,
        "grad_norm": 2.465719223022461,
        "learning_rate": 4.3855623739782915e-06,
        "epoch": 0.9689826302729528,
        "step": 7029
    },
    {
        "loss": 1.028,
        "grad_norm": 2.7119710445404053,
        "learning_rate": 4.347457484055639e-06,
        "epoch": 0.9691204852495175,
        "step": 7030
    },
    {
        "loss": 1.4635,
        "grad_norm": 2.3284482955932617,
        "learning_rate": 4.30951517980196e-06,
        "epoch": 0.9692583402260821,
        "step": 7031
    },
    {
        "loss": 1.9562,
        "grad_norm": 1.328665852546692,
        "learning_rate": 4.271735525709719e-06,
        "epoch": 0.9693961952026469,
        "step": 7032
    },
    {
        "loss": 1.3966,
        "grad_norm": 1.7247079610824585,
        "learning_rate": 4.234118585995062e-06,
        "epoch": 0.9695340501792115,
        "step": 7033
    },
    {
        "loss": 0.4513,
        "grad_norm": 1.7231687307357788,
        "learning_rate": 4.196664424597441e-06,
        "epoch": 0.9696719051557762,
        "step": 7034
    },
    {
        "loss": 1.1557,
        "grad_norm": 1.7559727430343628,
        "learning_rate": 4.159373105179643e-06,
        "epoch": 0.9698097601323408,
        "step": 7035
    },
    {
        "loss": 1.6757,
        "grad_norm": 2.0321385860443115,
        "learning_rate": 4.122244691127753e-06,
        "epoch": 0.9699476151089055,
        "step": 7036
    },
    {
        "loss": 1.6234,
        "grad_norm": 1.880234956741333,
        "learning_rate": 4.085279245550822e-06,
        "epoch": 0.9700854700854701,
        "step": 7037
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.3297306299209595,
        "learning_rate": 4.048476831281056e-06,
        "epoch": 0.9702233250620348,
        "step": 7038
    },
    {
        "loss": 1.9195,
        "grad_norm": 2.0634665489196777,
        "learning_rate": 4.011837510873395e-06,
        "epoch": 0.9703611800385994,
        "step": 7039
    },
    {
        "loss": 1.7067,
        "grad_norm": 2.1601388454437256,
        "learning_rate": 3.975361346605599e-06,
        "epoch": 0.9704990350151641,
        "step": 7040
    },
    {
        "loss": 1.6452,
        "grad_norm": 2.3206679821014404,
        "learning_rate": 3.939048400478162e-06,
        "epoch": 0.9706368899917287,
        "step": 7041
    },
    {
        "loss": 1.8221,
        "grad_norm": 1.8266165256500244,
        "learning_rate": 3.9028987342141666e-06,
        "epoch": 0.9707747449682933,
        "step": 7042
    },
    {
        "loss": 1.5922,
        "grad_norm": 3.0943403244018555,
        "learning_rate": 3.866912409259005e-06,
        "epoch": 0.970912599944858,
        "step": 7043
    },
    {
        "loss": 1.8896,
        "grad_norm": 1.5655450820922852,
        "learning_rate": 3.831089486780625e-06,
        "epoch": 0.9710504549214226,
        "step": 7044
    },
    {
        "loss": 1.3639,
        "grad_norm": 1.8462228775024414,
        "learning_rate": 3.7954300276690445e-06,
        "epoch": 0.9711883098979873,
        "step": 7045
    },
    {
        "loss": 1.3739,
        "grad_norm": 1.998381495475769,
        "learning_rate": 3.759934092536632e-06,
        "epoch": 0.9713261648745519,
        "step": 7046
    },
    {
        "loss": 1.6455,
        "grad_norm": 1.7342482805252075,
        "learning_rate": 3.724601741717626e-06,
        "epoch": 0.9714640198511166,
        "step": 7047
    },
    {
        "loss": 1.3189,
        "grad_norm": 2.280512809753418,
        "learning_rate": 3.6894330352682747e-06,
        "epoch": 0.9716018748276812,
        "step": 7048
    },
    {
        "loss": 1.4795,
        "grad_norm": 2.918440818786621,
        "learning_rate": 3.6544280329666947e-06,
        "epoch": 0.9717397298042459,
        "step": 7049
    },
    {
        "loss": 0.5725,
        "grad_norm": 1.419463038444519,
        "learning_rate": 3.6195867943127904e-06,
        "epoch": 0.9718775847808105,
        "step": 7050
    },
    {
        "loss": 2.3674,
        "grad_norm": 1.367026448249817,
        "learning_rate": 3.584909378528012e-06,
        "epoch": 0.9720154397573753,
        "step": 7051
    },
    {
        "loss": 1.9104,
        "grad_norm": 2.1761651039123535,
        "learning_rate": 3.5503958445553987e-06,
        "epoch": 0.9721532947339399,
        "step": 7052
    },
    {
        "loss": 1.9276,
        "grad_norm": 1.7592517137527466,
        "learning_rate": 3.516046251059368e-06,
        "epoch": 0.9722911497105046,
        "step": 7053
    },
    {
        "loss": 2.0013,
        "grad_norm": 2.0438175201416016,
        "learning_rate": 3.4818606564257726e-06,
        "epoch": 0.9724290046870692,
        "step": 7054
    },
    {
        "loss": 2.0164,
        "grad_norm": 2.579885721206665,
        "learning_rate": 3.44783911876172e-06,
        "epoch": 0.9725668596636339,
        "step": 7055
    },
    {
        "loss": 1.8008,
        "grad_norm": 2.073887348175049,
        "learning_rate": 3.4139816958953295e-06,
        "epoch": 0.9727047146401985,
        "step": 7056
    },
    {
        "loss": 2.0222,
        "grad_norm": 1.518534541130066,
        "learning_rate": 3.38028844537589e-06,
        "epoch": 0.9728425696167632,
        "step": 7057
    },
    {
        "loss": 2.5758,
        "grad_norm": 2.1771655082702637,
        "learning_rate": 3.3467594244736224e-06,
        "epoch": 0.9729804245933278,
        "step": 7058
    },
    {
        "loss": 0.9447,
        "grad_norm": 1.7384889125823975,
        "learning_rate": 3.313394690179561e-06,
        "epoch": 0.9731182795698925,
        "step": 7059
    },
    {
        "loss": 1.8532,
        "grad_norm": 1.4130131006240845,
        "learning_rate": 3.28019429920543e-06,
        "epoch": 0.9732561345464571,
        "step": 7060
    },
    {
        "loss": 1.0129,
        "grad_norm": 1.8036831617355347,
        "learning_rate": 3.2471583079837776e-06,
        "epoch": 0.9733939895230218,
        "step": 7061
    },
    {
        "loss": 2.2475,
        "grad_norm": 1.046549677848816,
        "learning_rate": 3.214286772667541e-06,
        "epoch": 0.9735318444995864,
        "step": 7062
    },
    {
        "loss": 1.2378,
        "grad_norm": 3.1889758110046387,
        "learning_rate": 3.18157974913027e-06,
        "epoch": 0.9736696994761511,
        "step": 7063
    },
    {
        "loss": 2.1367,
        "grad_norm": 1.7994486093521118,
        "learning_rate": 3.149037292965795e-06,
        "epoch": 0.9738075544527157,
        "step": 7064
    },
    {
        "loss": 1.5404,
        "grad_norm": 1.8657715320587158,
        "learning_rate": 3.1166594594882024e-06,
        "epoch": 0.9739454094292804,
        "step": 7065
    },
    {
        "loss": 1.9799,
        "grad_norm": 2.052650213241577,
        "learning_rate": 3.0844463037318805e-06,
        "epoch": 0.974083264405845,
        "step": 7066
    },
    {
        "loss": 1.5169,
        "grad_norm": 2.2559025287628174,
        "learning_rate": 3.052397880451141e-06,
        "epoch": 0.9742211193824097,
        "step": 7067
    },
    {
        "loss": 1.4813,
        "grad_norm": 1.761670470237732,
        "learning_rate": 3.0205142441204203e-06,
        "epoch": 0.9743589743589743,
        "step": 7068
    },
    {
        "loss": 1.1174,
        "grad_norm": 2.485790967941284,
        "learning_rate": 2.9887954489340453e-06,
        "epoch": 0.974496829335539,
        "step": 7069
    },
    {
        "loss": 1.7008,
        "grad_norm": 1.7824774980545044,
        "learning_rate": 2.957241548806067e-06,
        "epoch": 0.9746346843121036,
        "step": 7070
    },
    {
        "loss": 2.0694,
        "grad_norm": 2.6146864891052246,
        "learning_rate": 2.9258525973703708e-06,
        "epoch": 0.9747725392886684,
        "step": 7071
    },
    {
        "loss": 1.5555,
        "grad_norm": 2.694117546081543,
        "learning_rate": 2.8946286479803907e-06,
        "epoch": 0.974910394265233,
        "step": 7072
    },
    {
        "loss": 1.4748,
        "grad_norm": 2.8259713649749756,
        "learning_rate": 2.8635697537090833e-06,
        "epoch": 0.9750482492417977,
        "step": 7073
    },
    {
        "loss": 1.0562,
        "grad_norm": 2.560178756713867,
        "learning_rate": 2.8326759673489523e-06,
        "epoch": 0.9751861042183623,
        "step": 7074
    },
    {
        "loss": 2.3007,
        "grad_norm": 1.9026105403900146,
        "learning_rate": 2.8019473414117147e-06,
        "epoch": 0.975323959194927,
        "step": 7075
    },
    {
        "loss": 1.532,
        "grad_norm": 2.3929226398468018,
        "learning_rate": 2.7713839281285127e-06,
        "epoch": 0.9754618141714916,
        "step": 7076
    },
    {
        "loss": 1.602,
        "grad_norm": 3.150477409362793,
        "learning_rate": 2.7409857794495673e-06,
        "epoch": 0.9755996691480563,
        "step": 7077
    },
    {
        "loss": 1.9511,
        "grad_norm": 1.4987282752990723,
        "learning_rate": 2.7107529470441707e-06,
        "epoch": 0.9757375241246209,
        "step": 7078
    },
    {
        "loss": 0.9968,
        "grad_norm": 2.343306541442871,
        "learning_rate": 2.6806854823007044e-06,
        "epoch": 0.9758753791011856,
        "step": 7079
    },
    {
        "loss": 1.9821,
        "grad_norm": 1.9955049753189087,
        "learning_rate": 2.650783436326454e-06,
        "epoch": 0.9760132340777502,
        "step": 7080
    },
    {
        "loss": 2.6331,
        "grad_norm": 2.9331741333007812,
        "learning_rate": 2.621046859947418e-06,
        "epoch": 0.9761510890543149,
        "step": 7081
    },
    {
        "loss": 1.7163,
        "grad_norm": 1.3625622987747192,
        "learning_rate": 2.5914758037085097e-06,
        "epoch": 0.9762889440308795,
        "step": 7082
    },
    {
        "loss": 1.9972,
        "grad_norm": 1.3672475814819336,
        "learning_rate": 2.5620703178731442e-06,
        "epoch": 0.9764267990074442,
        "step": 7083
    },
    {
        "loss": 1.4764,
        "grad_norm": 2.7735674381256104,
        "learning_rate": 2.5328304524234293e-06,
        "epoch": 0.9765646539840088,
        "step": 7084
    },
    {
        "loss": 1.5402,
        "grad_norm": 2.938797950744629,
        "learning_rate": 2.503756257059886e-06,
        "epoch": 0.9767025089605734,
        "step": 7085
    },
    {
        "loss": 2.038,
        "grad_norm": 1.3919966220855713,
        "learning_rate": 2.474847781201417e-06,
        "epoch": 0.9768403639371381,
        "step": 7086
    },
    {
        "loss": 2.4153,
        "grad_norm": 1.567853569984436,
        "learning_rate": 2.4461050739853274e-06,
        "epoch": 0.9769782189137027,
        "step": 7087
    },
    {
        "loss": 1.2396,
        "grad_norm": 2.86847186088562,
        "learning_rate": 2.417528184267137e-06,
        "epoch": 0.9771160738902674,
        "step": 7088
    },
    {
        "loss": 2.0659,
        "grad_norm": 1.6649749279022217,
        "learning_rate": 2.3891171606204467e-06,
        "epoch": 0.977253928866832,
        "step": 7089
    },
    {
        "loss": 2.3652,
        "grad_norm": 1.998177170753479,
        "learning_rate": 2.360872051336982e-06,
        "epoch": 0.9773917838433968,
        "step": 7090
    },
    {
        "loss": 1.4824,
        "grad_norm": 2.9447853565216064,
        "learning_rate": 2.332792904426406e-06,
        "epoch": 0.9775296388199614,
        "step": 7091
    },
    {
        "loss": 2.0619,
        "grad_norm": 1.737046241760254,
        "learning_rate": 2.3048797676163746e-06,
        "epoch": 0.9776674937965261,
        "step": 7092
    },
    {
        "loss": 1.8105,
        "grad_norm": 1.8757221698760986,
        "learning_rate": 2.2771326883523345e-06,
        "epoch": 0.9778053487730907,
        "step": 7093
    },
    {
        "loss": 1.9074,
        "grad_norm": 1.4178664684295654,
        "learning_rate": 2.249551713797404e-06,
        "epoch": 0.9779432037496554,
        "step": 7094
    },
    {
        "loss": 1.842,
        "grad_norm": 1.664105772972107,
        "learning_rate": 2.2221368908324492e-06,
        "epoch": 0.97808105872622,
        "step": 7095
    },
    {
        "loss": 2.0228,
        "grad_norm": 1.217934012413025,
        "learning_rate": 2.1948882660559054e-06,
        "epoch": 0.9782189137027847,
        "step": 7096
    },
    {
        "loss": 1.8794,
        "grad_norm": 1.6862314939498901,
        "learning_rate": 2.1678058857836803e-06,
        "epoch": 0.9783567686793493,
        "step": 7097
    },
    {
        "loss": 1.8146,
        "grad_norm": 2.145352840423584,
        "learning_rate": 2.140889796049084e-06,
        "epoch": 0.978494623655914,
        "step": 7098
    },
    {
        "loss": 1.7022,
        "grad_norm": 2.3028106689453125,
        "learning_rate": 2.114140042602886e-06,
        "epoch": 0.9786324786324786,
        "step": 7099
    },
    {
        "loss": 2.2222,
        "grad_norm": 3.2726898193359375,
        "learning_rate": 2.0875566709129934e-06,
        "epoch": 0.9787703336090433,
        "step": 7100
    },
    {
        "loss": 2.0446,
        "grad_norm": 1.8886945247650146,
        "learning_rate": 2.0611397261646183e-06,
        "epoch": 0.9789081885856079,
        "step": 7101
    },
    {
        "loss": 1.3861,
        "grad_norm": 2.5688560009002686,
        "learning_rate": 2.0348892532599973e-06,
        "epoch": 0.9790460435621726,
        "step": 7102
    },
    {
        "loss": 2.2828,
        "grad_norm": 1.5313975811004639,
        "learning_rate": 2.0088052968184167e-06,
        "epoch": 0.9791838985387372,
        "step": 7103
    },
    {
        "loss": 1.4619,
        "grad_norm": 1.3634103536605835,
        "learning_rate": 1.9828879011762114e-06,
        "epoch": 0.9793217535153019,
        "step": 7104
    },
    {
        "loss": 2.5199,
        "grad_norm": 1.6493966579437256,
        "learning_rate": 1.9571371103864978e-06,
        "epoch": 0.9794596084918665,
        "step": 7105
    },
    {
        "loss": 1.8128,
        "grad_norm": 2.3480403423309326,
        "learning_rate": 1.9315529682192747e-06,
        "epoch": 0.9795974634684312,
        "step": 7106
    },
    {
        "loss": 1.6221,
        "grad_norm": 2.317861795425415,
        "learning_rate": 1.9061355181612784e-06,
        "epoch": 0.9797353184449958,
        "step": 7107
    },
    {
        "loss": 0.8458,
        "grad_norm": 4.0058794021606445,
        "learning_rate": 1.8808848034158389e-06,
        "epoch": 0.9798731734215606,
        "step": 7108
    },
    {
        "loss": 2.0199,
        "grad_norm": 2.298888921737671,
        "learning_rate": 1.8558008669029903e-06,
        "epoch": 0.9800110283981252,
        "step": 7109
    },
    {
        "loss": 2.1756,
        "grad_norm": 1.884054183959961,
        "learning_rate": 1.8308837512591825e-06,
        "epoch": 0.9801488833746899,
        "step": 7110
    },
    {
        "loss": 1.1521,
        "grad_norm": 1.7986056804656982,
        "learning_rate": 1.8061334988373146e-06,
        "epoch": 0.9802867383512545,
        "step": 7111
    },
    {
        "loss": 2.0248,
        "grad_norm": 1.3890472650527954,
        "learning_rate": 1.7815501517067678e-06,
        "epoch": 0.9804245933278192,
        "step": 7112
    },
    {
        "loss": 1.6291,
        "grad_norm": 2.209146022796631,
        "learning_rate": 1.757133751653095e-06,
        "epoch": 0.9805624483043838,
        "step": 7113
    },
    {
        "loss": 2.3537,
        "grad_norm": 1.3871169090270996,
        "learning_rate": 1.7328843401781868e-06,
        "epoch": 0.9807003032809485,
        "step": 7114
    },
    {
        "loss": 1.8831,
        "grad_norm": 1.7669275999069214,
        "learning_rate": 1.7088019585000393e-06,
        "epoch": 0.9808381582575131,
        "step": 7115
    },
    {
        "loss": 1.9269,
        "grad_norm": 2.300164222717285,
        "learning_rate": 1.684886647552686e-06,
        "epoch": 0.9809760132340778,
        "step": 7116
    },
    {
        "loss": 1.9044,
        "grad_norm": 1.8433318138122559,
        "learning_rate": 1.661138447986288e-06,
        "epoch": 0.9811138682106424,
        "step": 7117
    },
    {
        "loss": 1.4005,
        "grad_norm": 2.7115890979766846,
        "learning_rate": 1.6375574001669335e-06,
        "epoch": 0.9812517231872071,
        "step": 7118
    },
    {
        "loss": 1.3128,
        "grad_norm": 2.695183515548706,
        "learning_rate": 1.6141435441765273e-06,
        "epoch": 0.9813895781637717,
        "step": 7119
    },
    {
        "loss": 1.7602,
        "grad_norm": 3.217127799987793,
        "learning_rate": 1.5908969198128787e-06,
        "epoch": 0.9815274331403364,
        "step": 7120
    },
    {
        "loss": 1.4346,
        "grad_norm": 2.8231818675994873,
        "learning_rate": 1.5678175665894469e-06,
        "epoch": 0.981665288116901,
        "step": 7121
    },
    {
        "loss": 1.8181,
        "grad_norm": 1.6925877332687378,
        "learning_rate": 1.544905523735496e-06,
        "epoch": 0.9818031430934657,
        "step": 7122
    },
    {
        "loss": 2.3331,
        "grad_norm": 1.9035604000091553,
        "learning_rate": 1.5221608301957847e-06,
        "epoch": 0.9819409980700303,
        "step": 7123
    },
    {
        "loss": 1.9764,
        "grad_norm": 2.472212076187134,
        "learning_rate": 1.4995835246306545e-06,
        "epoch": 0.982078853046595,
        "step": 7124
    },
    {
        "loss": 1.8882,
        "grad_norm": 2.537395715713501,
        "learning_rate": 1.4771736454159523e-06,
        "epoch": 0.9822167080231596,
        "step": 7125
    },
    {
        "loss": 2.2776,
        "grad_norm": 1.392822265625,
        "learning_rate": 1.4549312306429751e-06,
        "epoch": 0.9823545629997242,
        "step": 7126
    },
    {
        "loss": 1.5599,
        "grad_norm": 2.9690661430358887,
        "learning_rate": 1.432856318118292e-06,
        "epoch": 0.982492417976289,
        "step": 7127
    },
    {
        "loss": 0.9971,
        "grad_norm": 2.601888656616211,
        "learning_rate": 1.4109489453638103e-06,
        "epoch": 0.9826302729528535,
        "step": 7128
    },
    {
        "loss": 1.7085,
        "grad_norm": 3.312487840652466,
        "learning_rate": 1.3892091496165993e-06,
        "epoch": 0.9827681279294183,
        "step": 7129
    },
    {
        "loss": 2.1297,
        "grad_norm": 2.1229684352874756,
        "learning_rate": 1.3676369678289668e-06,
        "epoch": 0.9829059829059829,
        "step": 7130
    },
    {
        "loss": 2.592,
        "grad_norm": 1.706664800643921,
        "learning_rate": 1.3462324366683266e-06,
        "epoch": 0.9830438378825476,
        "step": 7131
    },
    {
        "loss": 1.1615,
        "grad_norm": 2.316049337387085,
        "learning_rate": 1.32499559251702e-06,
        "epoch": 0.9831816928591122,
        "step": 7132
    },
    {
        "loss": 2.1302,
        "grad_norm": 1.832629680633545,
        "learning_rate": 1.303926471472472e-06,
        "epoch": 0.9833195478356769,
        "step": 7133
    },
    {
        "loss": 1.575,
        "grad_norm": 2.1622653007507324,
        "learning_rate": 1.2830251093469915e-06,
        "epoch": 0.9834574028122415,
        "step": 7134
    },
    {
        "loss": 1.4826,
        "grad_norm": 2.404439687728882,
        "learning_rate": 1.2622915416677039e-06,
        "epoch": 0.9835952577888062,
        "step": 7135
    },
    {
        "loss": 0.5194,
        "grad_norm": 2.4998016357421875,
        "learning_rate": 1.24172580367653e-06,
        "epoch": 0.9837331127653708,
        "step": 7136
    },
    {
        "loss": 1.3934,
        "grad_norm": 1.255975365638733,
        "learning_rate": 1.2213279303301738e-06,
        "epoch": 0.9838709677419355,
        "step": 7137
    },
    {
        "loss": 1.7589,
        "grad_norm": 1.7218464612960815,
        "learning_rate": 1.2010979562999348e-06,
        "epoch": 0.9840088227185001,
        "step": 7138
    },
    {
        "loss": 2.4431,
        "grad_norm": 1.4586617946624756,
        "learning_rate": 1.1810359159718176e-06,
        "epoch": 0.9841466776950648,
        "step": 7139
    },
    {
        "loss": 1.6056,
        "grad_norm": 2.080707550048828,
        "learning_rate": 1.1611418434462894e-06,
        "epoch": 0.9842845326716294,
        "step": 7140
    },
    {
        "loss": 2.0601,
        "grad_norm": 2.522937536239624,
        "learning_rate": 1.1414157725383568e-06,
        "epoch": 0.9844223876481941,
        "step": 7141
    },
    {
        "loss": 1.3147,
        "grad_norm": 2.463057518005371,
        "learning_rate": 1.1218577367774985e-06,
        "epoch": 0.9845602426247587,
        "step": 7142
    },
    {
        "loss": 1.2876,
        "grad_norm": 2.417151689529419,
        "learning_rate": 1.1024677694074892e-06,
        "epoch": 0.9846980976013234,
        "step": 7143
    },
    {
        "loss": 1.5415,
        "grad_norm": 1.4727883338928223,
        "learning_rate": 1.0832459033864984e-06,
        "epoch": 0.984835952577888,
        "step": 7144
    },
    {
        "loss": 1.1502,
        "grad_norm": 2.395362377166748,
        "learning_rate": 1.0641921713869685e-06,
        "epoch": 0.9849738075544527,
        "step": 7145
    },
    {
        "loss": 1.3852,
        "grad_norm": 2.339123249053955,
        "learning_rate": 1.045306605795504e-06,
        "epoch": 0.9851116625310173,
        "step": 7146
    },
    {
        "loss": 2.292,
        "grad_norm": 1.5696016550064087,
        "learning_rate": 1.026589238712916e-06,
        "epoch": 0.985249517507582,
        "step": 7147
    },
    {
        "loss": 1.5004,
        "grad_norm": 1.8877567052841187,
        "learning_rate": 1.0080401019540886e-06,
        "epoch": 0.9853873724841467,
        "step": 7148
    },
    {
        "loss": 2.221,
        "grad_norm": 1.4510959386825562,
        "learning_rate": 9.896592270479455e-07,
        "epoch": 0.9855252274607114,
        "step": 7149
    },
    {
        "loss": 2.0767,
        "grad_norm": 1.9274705648422241,
        "learning_rate": 9.714466452374837e-07,
        "epoch": 0.985663082437276,
        "step": 7150
    },
    {
        "loss": 1.9133,
        "grad_norm": 1.7128112316131592,
        "learning_rate": 9.534023874795295e-07,
        "epoch": 0.9858009374138407,
        "step": 7151
    },
    {
        "loss": 1.5139,
        "grad_norm": 2.349073648452759,
        "learning_rate": 9.35526484444893e-07,
        "epoch": 0.9859387923904053,
        "step": 7152
    },
    {
        "loss": 2.2097,
        "grad_norm": 1.9770174026489258,
        "learning_rate": 9.178189665181914e-07,
        "epoch": 0.98607664736697,
        "step": 7153
    },
    {
        "loss": 1.6278,
        "grad_norm": 1.6602956056594849,
        "learning_rate": 9.002798637978038e-07,
        "epoch": 0.9862145023435346,
        "step": 7154
    },
    {
        "loss": 2.3802,
        "grad_norm": 1.6680721044540405,
        "learning_rate": 8.829092060958722e-07,
        "epoch": 0.9863523573200993,
        "step": 7155
    },
    {
        "loss": 2.0567,
        "grad_norm": 2.1299476623535156,
        "learning_rate": 8.657070229382557e-07,
        "epoch": 0.9864902122966639,
        "step": 7156
    },
    {
        "loss": 2.0789,
        "grad_norm": 1.922310471534729,
        "learning_rate": 8.486733435643545e-07,
        "epoch": 0.9866280672732286,
        "step": 7157
    },
    {
        "loss": 1.6249,
        "grad_norm": 2.45654034614563,
        "learning_rate": 8.31808196927264e-07,
        "epoch": 0.9867659222497932,
        "step": 7158
    },
    {
        "loss": 1.6861,
        "grad_norm": 2.3238658905029297,
        "learning_rate": 8.151116116935198e-07,
        "epoch": 0.9869037772263579,
        "step": 7159
    },
    {
        "loss": 1.6647,
        "grad_norm": 1.3069993257522583,
        "learning_rate": 7.985836162432314e-07,
        "epoch": 0.9870416322029225,
        "step": 7160
    },
    {
        "loss": 1.2842,
        "grad_norm": 1.825224757194519,
        "learning_rate": 7.822242386698709e-07,
        "epoch": 0.9871794871794872,
        "step": 7161
    },
    {
        "loss": 1.6808,
        "grad_norm": 2.749074935913086,
        "learning_rate": 7.660335067803282e-07,
        "epoch": 0.9873173421560518,
        "step": 7162
    },
    {
        "loss": 1.9406,
        "grad_norm": 1.959831714630127,
        "learning_rate": 7.500114480948672e-07,
        "epoch": 0.9874551971326165,
        "step": 7163
    },
    {
        "loss": 1.3178,
        "grad_norm": 2.8790018558502197,
        "learning_rate": 7.341580898470369e-07,
        "epoch": 0.9875930521091811,
        "step": 7164
    },
    {
        "loss": 1.6839,
        "grad_norm": 2.4972994327545166,
        "learning_rate": 7.184734589836039e-07,
        "epoch": 0.9877309070857458,
        "step": 7165
    },
    {
        "loss": 2.5318,
        "grad_norm": 1.08822762966156,
        "learning_rate": 7.02957582164554e-07,
        "epoch": 0.9878687620623104,
        "step": 7166
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.149475336074829,
        "learning_rate": 6.876104857630461e-07,
        "epoch": 0.9880066170388752,
        "step": 7167
    },
    {
        "loss": 1.921,
        "grad_norm": 2.727041482925415,
        "learning_rate": 6.724321958653579e-07,
        "epoch": 0.9881444720154398,
        "step": 7168
    },
    {
        "loss": 1.5138,
        "grad_norm": 1.46058189868927,
        "learning_rate": 6.574227382708408e-07,
        "epoch": 0.9882823269920044,
        "step": 7169
    },
    {
        "loss": 2.1396,
        "grad_norm": 2.448364734649658,
        "learning_rate": 6.425821384918318e-07,
        "epoch": 0.9884201819685691,
        "step": 7170
    },
    {
        "loss": 1.4898,
        "grad_norm": 2.642401933670044,
        "learning_rate": 6.279104217536969e-07,
        "epoch": 0.9885580369451337,
        "step": 7171
    },
    {
        "loss": 1.4911,
        "grad_norm": 2.245331287384033,
        "learning_rate": 6.134076129947319e-07,
        "epoch": 0.9886958919216984,
        "step": 7172
    },
    {
        "loss": 1.2524,
        "grad_norm": 2.0792551040649414,
        "learning_rate": 5.990737368661292e-07,
        "epoch": 0.988833746898263,
        "step": 7173
    },
    {
        "loss": 2.5062,
        "grad_norm": 1.6537209749221802,
        "learning_rate": 5.849088177318996e-07,
        "epoch": 0.9889716018748277,
        "step": 7174
    },
    {
        "loss": 2.2982,
        "grad_norm": 1.3253850936889648,
        "learning_rate": 5.709128796689389e-07,
        "epoch": 0.9891094568513923,
        "step": 7175
    },
    {
        "loss": 2.6849,
        "grad_norm": 1.7726633548736572,
        "learning_rate": 5.570859464668621e-07,
        "epoch": 0.989247311827957,
        "step": 7176
    },
    {
        "loss": 1.6449,
        "grad_norm": 2.096463918685913,
        "learning_rate": 5.434280416280579e-07,
        "epoch": 0.9893851668045216,
        "step": 7177
    },
    {
        "loss": 1.1022,
        "grad_norm": 2.6071221828460693,
        "learning_rate": 5.299391883675675e-07,
        "epoch": 0.9895230217810863,
        "step": 7178
    },
    {
        "loss": 2.0573,
        "grad_norm": 2.2876579761505127,
        "learning_rate": 5.16619409613095e-07,
        "epoch": 0.9896608767576509,
        "step": 7179
    },
    {
        "loss": 2.2551,
        "grad_norm": 1.4363881349563599,
        "learning_rate": 5.034687280050076e-07,
        "epoch": 0.9897987317342156,
        "step": 7180
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.175914764404297,
        "learning_rate": 4.904871658961696e-07,
        "epoch": 0.9899365867107802,
        "step": 7181
    },
    {
        "loss": 2.0156,
        "grad_norm": 1.2226134538650513,
        "learning_rate": 4.776747453520525e-07,
        "epoch": 0.9900744416873449,
        "step": 7182
    },
    {
        "loss": 1.5427,
        "grad_norm": 1.528888463973999,
        "learning_rate": 4.650314881506024e-07,
        "epoch": 0.9902122966639095,
        "step": 7183
    },
    {
        "loss": 1.5986,
        "grad_norm": 1.219764232635498,
        "learning_rate": 4.525574157822177e-07,
        "epoch": 0.9903501516404742,
        "step": 7184
    },
    {
        "loss": 2.4232,
        "grad_norm": 1.6468831300735474,
        "learning_rate": 4.4025254944976e-07,
        "epoch": 0.9904880066170388,
        "step": 7185
    },
    {
        "loss": 1.8114,
        "grad_norm": 2.3107118606567383,
        "learning_rate": 4.2811691006844345e-07,
        "epoch": 0.9906258615936036,
        "step": 7186
    },
    {
        "loss": 1.1773,
        "grad_norm": 1.7793298959732056,
        "learning_rate": 4.161505182658343e-07,
        "epoch": 0.9907637165701682,
        "step": 7187
    },
    {
        "loss": 1.98,
        "grad_norm": 2.6215343475341797,
        "learning_rate": 4.0435339438187334e-07,
        "epoch": 0.9909015715467329,
        "step": 7188
    },
    {
        "loss": 1.7185,
        "grad_norm": 1.4667123556137085,
        "learning_rate": 3.927255584687317e-07,
        "epoch": 0.9910394265232975,
        "step": 7189
    },
    {
        "loss": 1.2487,
        "grad_norm": 2.7215120792388916,
        "learning_rate": 3.812670302908772e-07,
        "epoch": 0.9911772814998622,
        "step": 7190
    },
    {
        "loss": 1.3115,
        "grad_norm": 2.0897159576416016,
        "learning_rate": 3.699778293249634e-07,
        "epoch": 0.9913151364764268,
        "step": 7191
    },
    {
        "loss": 2.0475,
        "grad_norm": 2.4928956031799316,
        "learning_rate": 3.5885797475981866e-07,
        "epoch": 0.9914529914529915,
        "step": 7192
    },
    {
        "loss": 0.3868,
        "grad_norm": 1.4928927421569824,
        "learning_rate": 3.479074854964681e-07,
        "epoch": 0.9915908464295561,
        "step": 7193
    },
    {
        "loss": 2.1431,
        "grad_norm": 1.5468463897705078,
        "learning_rate": 3.371263801480451e-07,
        "epoch": 0.9917287014061208,
        "step": 7194
    },
    {
        "loss": 1.6016,
        "grad_norm": 2.1540393829345703,
        "learning_rate": 3.265146770397354e-07,
        "epoch": 0.9918665563826854,
        "step": 7195
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.5177083015441895,
        "learning_rate": 3.1607239420883285e-07,
        "epoch": 0.9920044113592501,
        "step": 7196
    },
    {
        "loss": 2.2854,
        "grad_norm": 1.9274369478225708,
        "learning_rate": 3.057995494046173e-07,
        "epoch": 0.9921422663358147,
        "step": 7197
    },
    {
        "loss": 1.9126,
        "grad_norm": 2.2513515949249268,
        "learning_rate": 2.9569616008841004e-07,
        "epoch": 0.9922801213123794,
        "step": 7198
    },
    {
        "loss": 2.0375,
        "grad_norm": 1.7221976518630981,
        "learning_rate": 2.857622434334628e-07,
        "epoch": 0.992417976288944,
        "step": 7199
    },
    {
        "loss": 1.8585,
        "grad_norm": 1.2998909950256348,
        "learning_rate": 2.759978163249577e-07,
        "epoch": 0.9925558312655087,
        "step": 7200
    },
    {
        "loss": 2.2595,
        "grad_norm": 1.3894459009170532,
        "learning_rate": 2.664028953600184e-07,
        "epoch": 0.9926936862420733,
        "step": 7201
    },
    {
        "loss": 2.4598,
        "grad_norm": 3.1264538764953613,
        "learning_rate": 2.569774968476546e-07,
        "epoch": 0.992831541218638,
        "step": 7202
    },
    {
        "loss": 1.7895,
        "grad_norm": 4.194496154785156,
        "learning_rate": 2.477216368087065e-07,
        "epoch": 0.9929693961952026,
        "step": 7203
    },
    {
        "loss": 1.4686,
        "grad_norm": 2.896911144256592,
        "learning_rate": 2.386353309758338e-07,
        "epoch": 0.9931072511717673,
        "step": 7204
    },
    {
        "loss": 1.6656,
        "grad_norm": 2.797621011734009,
        "learning_rate": 2.2971859479351542e-07,
        "epoch": 0.993245106148332,
        "step": 7205
    },
    {
        "loss": 1.918,
        "grad_norm": 3.1673479080200195,
        "learning_rate": 2.2097144341800546e-07,
        "epoch": 0.9933829611248967,
        "step": 7206
    },
    {
        "loss": 1.2798,
        "grad_norm": 2.008471727371216,
        "learning_rate": 2.123938917172885e-07,
        "epoch": 0.9935208161014613,
        "step": 7207
    },
    {
        "loss": 1.9309,
        "grad_norm": 1.4939078092575073,
        "learning_rate": 2.039859542710798e-07,
        "epoch": 0.993658671078026,
        "step": 7208
    },
    {
        "loss": 0.9688,
        "grad_norm": 1.8648279905319214,
        "learning_rate": 1.9574764537080294e-07,
        "epoch": 0.9937965260545906,
        "step": 7209
    },
    {
        "loss": 1.9393,
        "grad_norm": 1.8642725944519043,
        "learning_rate": 1.8767897901954544e-07,
        "epoch": 0.9939343810311553,
        "step": 7210
    },
    {
        "loss": 2.0487,
        "grad_norm": 3.0549561977386475,
        "learning_rate": 1.7977996893204785e-07,
        "epoch": 0.9940722360077199,
        "step": 7211
    },
    {
        "loss": 2.3852,
        "grad_norm": 1.844630479812622,
        "learning_rate": 1.7205062853464792e-07,
        "epoch": 0.9942100909842845,
        "step": 7212
    },
    {
        "loss": 1.6023,
        "grad_norm": 2.2222251892089844,
        "learning_rate": 1.644909709653586e-07,
        "epoch": 0.9943479459608492,
        "step": 7213
    },
    {
        "loss": 2.1987,
        "grad_norm": 1.6137539148330688,
        "learning_rate": 1.5710100907367909e-07,
        "epoch": 0.9944858009374138,
        "step": 7214
    },
    {
        "loss": 1.7992,
        "grad_norm": 3.632765054702759,
        "learning_rate": 1.4988075542076153e-07,
        "epoch": 0.9946236559139785,
        "step": 7215
    },
    {
        "loss": 2.0069,
        "grad_norm": 2.381199836730957,
        "learning_rate": 1.4283022227922216e-07,
        "epoch": 0.9947615108905431,
        "step": 7216
    },
    {
        "loss": 2.1308,
        "grad_norm": 2.001075506210327,
        "learning_rate": 1.3594942163323022e-07,
        "epoch": 0.9948993658671078,
        "step": 7217
    },
    {
        "loss": 1.6492,
        "grad_norm": 2.301409959793091,
        "learning_rate": 1.2923836517845233e-07,
        "epoch": 0.9950372208436724,
        "step": 7218
    },
    {
        "loss": 2.4433,
        "grad_norm": 1.7453618049621582,
        "learning_rate": 1.2269706432200823e-07,
        "epoch": 0.9951750758202371,
        "step": 7219
    },
    {
        "loss": 2.3884,
        "grad_norm": 1.1937059164047241,
        "learning_rate": 1.1632553018251501e-07,
        "epoch": 0.9953129307968017,
        "step": 7220
    },
    {
        "loss": 1.6231,
        "grad_norm": 2.812883138656616,
        "learning_rate": 1.1012377358998738e-07,
        "epoch": 0.9954507857733664,
        "step": 7221
    },
    {
        "loss": 1.6027,
        "grad_norm": 3.0658414363861084,
        "learning_rate": 1.040918050858819e-07,
        "epoch": 0.995588640749931,
        "step": 7222
    },
    {
        "loss": 1.7191,
        "grad_norm": 2.4376068115234375,
        "learning_rate": 9.82296349230749e-08,
        "epoch": 0.9957264957264957,
        "step": 7223
    },
    {
        "loss": 1.7026,
        "grad_norm": 2.3777990341186523,
        "learning_rate": 9.253727306577364e-08,
        "epoch": 0.9958643507030603,
        "step": 7224
    },
    {
        "loss": 0.272,
        "grad_norm": 1.106067180633545,
        "learning_rate": 8.701472918959396e-08,
        "epoch": 0.996002205679625,
        "step": 7225
    },
    {
        "loss": 1.9466,
        "grad_norm": 1.3611356019973755,
        "learning_rate": 8.166201268151597e-08,
        "epoch": 0.9961400606561897,
        "step": 7226
    },
    {
        "loss": 1.2806,
        "grad_norm": 2.452623128890991,
        "learning_rate": 7.647913263980621e-08,
        "epoch": 0.9962779156327544,
        "step": 7227
    },
    {
        "loss": 2.1399,
        "grad_norm": 2.3056747913360596,
        "learning_rate": 7.146609787409552e-08,
        "epoch": 0.996415770609319,
        "step": 7228
    },
    {
        "loss": 0.9237,
        "grad_norm": 2.7186641693115234,
        "learning_rate": 6.66229169052901e-08,
        "epoch": 0.9965536255858837,
        "step": 7229
    },
    {
        "loss": 2.2915,
        "grad_norm": 1.2506756782531738,
        "learning_rate": 6.194959796561595e-08,
        "epoch": 0.9966914805624483,
        "step": 7230
    },
    {
        "loss": 2.1344,
        "grad_norm": 1.4569308757781982,
        "learning_rate": 5.7446148998552277e-08,
        "epoch": 0.996829335539013,
        "step": 7231
    },
    {
        "loss": 2.374,
        "grad_norm": 1.2457284927368164,
        "learning_rate": 5.3112577658853694e-08,
        "epoch": 0.9969671905155776,
        "step": 7232
    },
    {
        "loss": 1.5566,
        "grad_norm": 2.2764275074005127,
        "learning_rate": 4.89488913125058e-08,
        "epoch": 0.9971050454921423,
        "step": 7233
    },
    {
        "loss": 1.7913,
        "grad_norm": 2.3102712631225586,
        "learning_rate": 4.4955097036758486e-08,
        "epoch": 0.9972429004687069,
        "step": 7234
    },
    {
        "loss": 1.0891,
        "grad_norm": 2.5102736949920654,
        "learning_rate": 4.113120162007045e-08,
        "epoch": 0.9973807554452716,
        "step": 7235
    },
    {
        "loss": 2.0894,
        "grad_norm": 2.3274829387664795,
        "learning_rate": 3.747721156212025e-08,
        "epoch": 0.9975186104218362,
        "step": 7236
    },
    {
        "loss": 1.4463,
        "grad_norm": 2.3313281536102295,
        "learning_rate": 3.399313307378416e-08,
        "epoch": 0.9976564653984009,
        "step": 7237
    },
    {
        "loss": 2.074,
        "grad_norm": 1.9959145784378052,
        "learning_rate": 3.0678972077136105e-08,
        "epoch": 0.9977943203749655,
        "step": 7238
    },
    {
        "loss": 1.8011,
        "grad_norm": 1.9902061223983765,
        "learning_rate": 2.7534734205436617e-08,
        "epoch": 0.9979321753515302,
        "step": 7239
    },
    {
        "loss": 2.2325,
        "grad_norm": 1.12578284740448,
        "learning_rate": 2.45604248030995e-08,
        "epoch": 0.9980700303280948,
        "step": 7240
    },
    {
        "loss": 2.0973,
        "grad_norm": 2.0447254180908203,
        "learning_rate": 2.1756048925725136e-08,
        "epoch": 0.9982078853046595,
        "step": 7241
    },
    {
        "loss": 1.6309,
        "grad_norm": 1.75058913230896,
        "learning_rate": 1.9121611340056077e-08,
        "epoch": 0.9983457402812241,
        "step": 7242
    },
    {
        "loss": 2.6003,
        "grad_norm": 1.4457142353057861,
        "learning_rate": 1.6657116523977058e-08,
        "epoch": 0.9984835952577888,
        "step": 7243
    },
    {
        "loss": 1.4743,
        "grad_norm": 2.0023937225341797,
        "learning_rate": 1.4362568666526078e-08,
        "epoch": 0.9986214502343534,
        "step": 7244
    },
    {
        "loss": 1.2578,
        "grad_norm": 2.452869176864624,
        "learning_rate": 1.2237971667883319e-08,
        "epoch": 0.9987593052109182,
        "step": 7245
    },
    {
        "loss": 1.6373,
        "grad_norm": 4.5986247062683105,
        "learning_rate": 1.028332913930452e-08,
        "epoch": 0.9988971601874828,
        "step": 7246
    },
    {
        "loss": 1.5742,
        "grad_norm": 1.6745697259902954,
        "learning_rate": 8.498644403232003e-09,
        "epoch": 0.9990350151640475,
        "step": 7247
    },
    {
        "loss": 2.4088,
        "grad_norm": 2.332456588745117,
        "learning_rate": 6.883920493161444e-09,
        "epoch": 0.9991728701406121,
        "step": 7248
    },
    {
        "loss": 2.2211,
        "grad_norm": 1.6449952125549316,
        "learning_rate": 5.439160153741796e-09,
        "epoch": 0.9993107251171768,
        "step": 7249
    },
    {
        "loss": 1.2507,
        "grad_norm": 2.2934231758117676,
        "learning_rate": 4.164365840708672e-09,
        "epoch": 0.9994485800937414,
        "step": 7250
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.7923858165740967,
        "learning_rate": 3.0595397208843524e-09,
        "epoch": 0.9995864350703061,
        "step": 7251
    },
    {
        "loss": 1.4711,
        "grad_norm": 1.7622272968292236,
        "learning_rate": 2.124683672211081e-09,
        "epoch": 0.9997242900468707,
        "step": 7252
    },
    {
        "loss": 1.362,
        "grad_norm": 2.5091135501861572,
        "learning_rate": 1.3597992837066643e-09,
        "epoch": 0.9998621450234354,
        "step": 7253
    },
    {
        "loss": 2.1512,
        "grad_norm": 2.5064656734466553,
        "learning_rate": 7.648878554866735e-10,
        "epoch": 1.0,
        "step": 7254
    },
    {
        "train_runtime": 20112.5565,
        "train_samples_per_second": 0.721,
        "train_steps_per_second": 0.361,
        "total_flos": 2.991030356685619e+17,
        "train_loss": 1.9487287126150807,
        "epoch": 1.0,
        "step": 7254
    }
]