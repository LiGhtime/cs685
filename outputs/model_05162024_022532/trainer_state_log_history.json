[
    {
        "loss": 3.8603,
        "grad_norm": 3.162442207336426,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 0.00013768415255404102,
        "step": 1
    },
    {
        "loss": 4.1929,
        "grad_norm": 3.0509564876556396,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.00027536830510808204,
        "step": 2
    },
    {
        "loss": 3.5851,
        "grad_norm": 2.2458696365356445,
        "learning_rate": 2.4e-05,
        "epoch": 0.0004130524576621231,
        "step": 3
    },
    {
        "loss": 3.8557,
        "grad_norm": 2.4725704193115234,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.0005507366102161641,
        "step": 4
    },
    {
        "loss": 3.2798,
        "grad_norm": 1.7046217918395996,
        "learning_rate": 4e-05,
        "epoch": 0.0006884207627702051,
        "step": 5
    },
    {
        "loss": 3.8621,
        "grad_norm": 2.67625093460083,
        "learning_rate": 4.8e-05,
        "epoch": 0.0008261049153242462,
        "step": 6
    },
    {
        "loss": 3.1261,
        "grad_norm": 1.7372932434082031,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0009637890678782872,
        "step": 7
    },
    {
        "loss": 3.9109,
        "grad_norm": 3.0341238975524902,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0011014732204323282,
        "step": 8
    },
    {
        "loss": 3.7295,
        "grad_norm": 2.8016083240509033,
        "learning_rate": 7.2e-05,
        "epoch": 0.0012391573729863693,
        "step": 9
    },
    {
        "loss": 4.0779,
        "grad_norm": 3.9396512508392334,
        "learning_rate": 8e-05,
        "epoch": 0.0013768415255404102,
        "step": 10
    },
    {
        "loss": 3.4604,
        "grad_norm": 3.1938605308532715,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0015145256780944513,
        "step": 11
    },
    {
        "loss": 3.2635,
        "grad_norm": 1.9814484119415283,
        "learning_rate": 9.6e-05,
        "epoch": 0.0016522098306484924,
        "step": 12
    },
    {
        "loss": 3.2645,
        "grad_norm": 1.546012282371521,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.0017898939832025334,
        "step": 13
    },
    {
        "loss": 3.2681,
        "grad_norm": 2.0706963539123535,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0019275781357565745,
        "step": 14
    },
    {
        "loss": 3.2454,
        "grad_norm": 2.514730453491211,
        "learning_rate": 0.00012,
        "epoch": 0.0020652622883106154,
        "step": 15
    },
    {
        "loss": 3.1887,
        "grad_norm": 4.119220733642578,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0022029464408646563,
        "step": 16
    },
    {
        "loss": 3.0807,
        "grad_norm": 2.9901161193847656,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.0023406305934186977,
        "step": 17
    },
    {
        "loss": 3.4552,
        "grad_norm": 2.719656467437744,
        "learning_rate": 0.000144,
        "epoch": 0.0024783147459727386,
        "step": 18
    },
    {
        "loss": 3.0199,
        "grad_norm": 1.9336833953857422,
        "learning_rate": 0.000152,
        "epoch": 0.0026159988985267795,
        "step": 19
    },
    {
        "loss": 3.1829,
        "grad_norm": 1.6922461986541748,
        "learning_rate": 0.00016,
        "epoch": 0.0027536830510808204,
        "step": 20
    },
    {
        "loss": 2.8459,
        "grad_norm": 1.91109037399292,
        "learning_rate": 0.000168,
        "epoch": 0.0028913672036348617,
        "step": 21
    },
    {
        "loss": 3.2009,
        "grad_norm": 3.941023111343384,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.0030290513561889026,
        "step": 22
    },
    {
        "loss": 3.0953,
        "grad_norm": 1.3252516984939575,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.0031667355087429436,
        "step": 23
    },
    {
        "loss": 2.6788,
        "grad_norm": 3.089646100997925,
        "learning_rate": 0.000192,
        "epoch": 0.003304419661296985,
        "step": 24
    },
    {
        "loss": 3.1421,
        "grad_norm": 1.6437218189239502,
        "learning_rate": 0.0002,
        "epoch": 0.003442103813851026,
        "step": 25
    },
    {
        "loss": 2.7292,
        "grad_norm": 1.969221591949463,
        "learning_rate": 0.0001999999152235864,
        "epoch": 0.0035797879664050667,
        "step": 26
    },
    {
        "loss": 2.5634,
        "grad_norm": 2.021404504776001,
        "learning_rate": 0.00019999966089448925,
        "epoch": 0.0037174721189591076,
        "step": 27
    },
    {
        "loss": 2.6634,
        "grad_norm": 3.2036874294281006,
        "learning_rate": 0.00019999923701313982,
        "epoch": 0.003855156271513149,
        "step": 28
    },
    {
        "loss": 2.0277,
        "grad_norm": 1.3976808786392212,
        "learning_rate": 0.00019999864358025683,
        "epoch": 0.0039928404240671895,
        "step": 29
    },
    {
        "loss": 2.8141,
        "grad_norm": 1.6078715324401855,
        "learning_rate": 0.00019999788059684644,
        "epoch": 0.004130524576621231,
        "step": 30
    },
    {
        "loss": 2.734,
        "grad_norm": 0.991961658000946,
        "learning_rate": 0.00019999694806420229,
        "epoch": 0.004268208729175272,
        "step": 31
    },
    {
        "loss": 2.8478,
        "grad_norm": 1.0777117013931274,
        "learning_rate": 0.00019999584598390555,
        "epoch": 0.004405892881729313,
        "step": 32
    },
    {
        "loss": 2.6318,
        "grad_norm": 0.9824682474136353,
        "learning_rate": 0.0001999945743578248,
        "epoch": 0.004543577034283354,
        "step": 33
    },
    {
        "loss": 2.5814,
        "grad_norm": 1.307984471321106,
        "learning_rate": 0.00019999313318811614,
        "epoch": 0.004681261186837395,
        "step": 34
    },
    {
        "loss": 2.6624,
        "grad_norm": 2.017761468887329,
        "learning_rate": 0.0001999915224772231,
        "epoch": 0.004818945339391436,
        "step": 35
    },
    {
        "loss": 2.4137,
        "grad_norm": 2.1859304904937744,
        "learning_rate": 0.0001999897422278767,
        "epoch": 0.004956629491945477,
        "step": 36
    },
    {
        "loss": 2.6559,
        "grad_norm": 0.8638195395469666,
        "learning_rate": 0.00019998779244309538,
        "epoch": 0.0050943136444995185,
        "step": 37
    },
    {
        "loss": 2.2227,
        "grad_norm": 2.31337833404541,
        "learning_rate": 0.0001999856731261851,
        "epoch": 0.005231997797053559,
        "step": 38
    },
    {
        "loss": 2.4553,
        "grad_norm": 1.3895658254623413,
        "learning_rate": 0.00019998338428073916,
        "epoch": 0.0053696819496076,
        "step": 39
    },
    {
        "loss": 2.4576,
        "grad_norm": 1.5402790307998657,
        "learning_rate": 0.00019998092591063838,
        "epoch": 0.005507366102161641,
        "step": 40
    },
    {
        "loss": 2.6761,
        "grad_norm": 1.1246392726898193,
        "learning_rate": 0.00019997829802005104,
        "epoch": 0.005645050254715682,
        "step": 41
    },
    {
        "loss": 2.6647,
        "grad_norm": 1.4962519407272339,
        "learning_rate": 0.00019997550061343278,
        "epoch": 0.0057827344072697235,
        "step": 42
    },
    {
        "loss": 1.9818,
        "grad_norm": 2.494053363800049,
        "learning_rate": 0.00019997253369552666,
        "epoch": 0.005920418559823764,
        "step": 43
    },
    {
        "loss": 2.4689,
        "grad_norm": 1.327582597732544,
        "learning_rate": 0.00019996939727136317,
        "epoch": 0.006058102712377805,
        "step": 44
    },
    {
        "loss": 2.7965,
        "grad_norm": 1.1809556484222412,
        "learning_rate": 0.00019996609134626024,
        "epoch": 0.006195786864931847,
        "step": 45
    },
    {
        "loss": 3.0203,
        "grad_norm": 1.4800053834915161,
        "learning_rate": 0.00019996261592582313,
        "epoch": 0.006333471017485887,
        "step": 46
    },
    {
        "loss": 2.484,
        "grad_norm": 1.7190041542053223,
        "learning_rate": 0.00019995897101594454,
        "epoch": 0.0064711551700399285,
        "step": 47
    },
    {
        "loss": 2.6983,
        "grad_norm": 2.8570942878723145,
        "learning_rate": 0.0001999551566228045,
        "epoch": 0.00660883932259397,
        "step": 48
    },
    {
        "loss": 2.5286,
        "grad_norm": 1.6048215627670288,
        "learning_rate": 0.00019995117275287046,
        "epoch": 0.00674652347514801,
        "step": 49
    },
    {
        "loss": 2.437,
        "grad_norm": 1.176591396331787,
        "learning_rate": 0.0001999470194128971,
        "epoch": 0.006884207627702052,
        "step": 50
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.5931999683380127,
        "learning_rate": 0.00019994269660992658,
        "epoch": 0.007021891780256092,
        "step": 51
    },
    {
        "loss": 2.5039,
        "grad_norm": 1.7065547704696655,
        "learning_rate": 0.00019993820435128834,
        "epoch": 0.0071595759328101334,
        "step": 52
    },
    {
        "loss": 2.3745,
        "grad_norm": 1.9422218799591064,
        "learning_rate": 0.00019993354264459913,
        "epoch": 0.007297260085364175,
        "step": 53
    },
    {
        "loss": 2.0541,
        "grad_norm": 2.554060697555542,
        "learning_rate": 0.00019992871149776298,
        "epoch": 0.007434944237918215,
        "step": 54
    },
    {
        "loss": 2.699,
        "grad_norm": 1.5580230951309204,
        "learning_rate": 0.00019992371091897125,
        "epoch": 0.007572628390472257,
        "step": 55
    },
    {
        "loss": 2.5035,
        "grad_norm": 1.8566023111343384,
        "learning_rate": 0.0001999185409167026,
        "epoch": 0.007710312543026298,
        "step": 56
    },
    {
        "loss": 2.5473,
        "grad_norm": 1.173354148864746,
        "learning_rate": 0.00019991320149972283,
        "epoch": 0.00784799669558034,
        "step": 57
    },
    {
        "loss": 2.5872,
        "grad_norm": 1.693225383758545,
        "learning_rate": 0.00019990769267708516,
        "epoch": 0.007985680848134379,
        "step": 58
    },
    {
        "loss": 2.4708,
        "grad_norm": 1.8845382928848267,
        "learning_rate": 0.0001999020144581299,
        "epoch": 0.00812336500068842,
        "step": 59
    },
    {
        "loss": 2.8814,
        "grad_norm": 2.3268299102783203,
        "learning_rate": 0.00019989616685248468,
        "epoch": 0.008261049153242462,
        "step": 60
    },
    {
        "loss": 2.5227,
        "grad_norm": 1.6151870489120483,
        "learning_rate": 0.0001998901498700642,
        "epoch": 0.008398733305796503,
        "step": 61
    },
    {
        "loss": 2.6195,
        "grad_norm": 1.8368284702301025,
        "learning_rate": 0.00019988396352107048,
        "epoch": 0.008536417458350544,
        "step": 62
    },
    {
        "loss": 2.6279,
        "grad_norm": 1.695733904838562,
        "learning_rate": 0.00019987760781599268,
        "epoch": 0.008674101610904586,
        "step": 63
    },
    {
        "loss": 2.222,
        "grad_norm": 2.2848775386810303,
        "learning_rate": 0.000199871082765607,
        "epoch": 0.008811785763458625,
        "step": 64
    },
    {
        "loss": 2.4491,
        "grad_norm": 1.7604470252990723,
        "learning_rate": 0.00019986438838097688,
        "epoch": 0.008949469916012667,
        "step": 65
    },
    {
        "loss": 2.606,
        "grad_norm": 1.543257236480713,
        "learning_rate": 0.0001998575246734529,
        "epoch": 0.009087154068566708,
        "step": 66
    },
    {
        "loss": 2.2485,
        "grad_norm": 2.514122724533081,
        "learning_rate": 0.00019985049165467257,
        "epoch": 0.00922483822112075,
        "step": 67
    },
    {
        "loss": 1.8052,
        "grad_norm": 2.6891837120056152,
        "learning_rate": 0.00019984328933656066,
        "epoch": 0.00936252237367479,
        "step": 68
    },
    {
        "loss": 2.7529,
        "grad_norm": 1.7555617094039917,
        "learning_rate": 0.00019983591773132882,
        "epoch": 0.00950020652622883,
        "step": 69
    },
    {
        "loss": 2.5904,
        "grad_norm": 1.5216631889343262,
        "learning_rate": 0.00019982837685147587,
        "epoch": 0.009637890678782872,
        "step": 70
    },
    {
        "loss": 2.426,
        "grad_norm": 1.7784309387207031,
        "learning_rate": 0.00019982066670978758,
        "epoch": 0.009775574831336913,
        "step": 71
    },
    {
        "loss": 2.5161,
        "grad_norm": 1.9475675821304321,
        "learning_rate": 0.0001998127873193367,
        "epoch": 0.009913258983890954,
        "step": 72
    },
    {
        "loss": 2.6521,
        "grad_norm": 1.3386735916137695,
        "learning_rate": 0.00019980473869348297,
        "epoch": 0.010050943136444996,
        "step": 73
    },
    {
        "loss": 2.2727,
        "grad_norm": 1.9491266012191772,
        "learning_rate": 0.00019979652084587305,
        "epoch": 0.010188627288999037,
        "step": 74
    },
    {
        "loss": 2.2769,
        "grad_norm": 1.8423879146575928,
        "learning_rate": 0.00019978813379044054,
        "epoch": 0.010326311441553077,
        "step": 75
    },
    {
        "loss": 2.4992,
        "grad_norm": 1.7484771013259888,
        "learning_rate": 0.00019977957754140592,
        "epoch": 0.010463995594107118,
        "step": 76
    },
    {
        "loss": 2.1419,
        "grad_norm": 2.3177297115325928,
        "learning_rate": 0.00019977085211327658,
        "epoch": 0.01060167974666116,
        "step": 77
    },
    {
        "loss": 2.3529,
        "grad_norm": 1.7814295291900635,
        "learning_rate": 0.0001997619575208467,
        "epoch": 0.0107393638992152,
        "step": 78
    },
    {
        "loss": 2.0519,
        "grad_norm": 1.2386443614959717,
        "learning_rate": 0.00019975289377919734,
        "epoch": 0.010877048051769242,
        "step": 79
    },
    {
        "loss": 2.7869,
        "grad_norm": 1.4393216371536255,
        "learning_rate": 0.0001997436609036963,
        "epoch": 0.011014732204323282,
        "step": 80
    },
    {
        "loss": 2.2563,
        "grad_norm": 1.3043544292449951,
        "learning_rate": 0.0001997342589099982,
        "epoch": 0.011152416356877323,
        "step": 81
    },
    {
        "loss": 2.6701,
        "grad_norm": 1.190127968788147,
        "learning_rate": 0.00019972468781404438,
        "epoch": 0.011290100509431364,
        "step": 82
    },
    {
        "loss": 3.0874,
        "grad_norm": 1.60750412940979,
        "learning_rate": 0.00019971494763206291,
        "epoch": 0.011427784661985406,
        "step": 83
    },
    {
        "loss": 2.3141,
        "grad_norm": 1.9098851680755615,
        "learning_rate": 0.00019970503838056853,
        "epoch": 0.011565468814539447,
        "step": 84
    },
    {
        "loss": 2.7977,
        "grad_norm": 1.237981915473938,
        "learning_rate": 0.0001996949600763627,
        "epoch": 0.011703152967093488,
        "step": 85
    },
    {
        "loss": 1.9957,
        "grad_norm": 2.2598745822906494,
        "learning_rate": 0.00019968471273653342,
        "epoch": 0.011840837119647528,
        "step": 86
    },
    {
        "loss": 2.5463,
        "grad_norm": 1.6595194339752197,
        "learning_rate": 0.00019967429637845533,
        "epoch": 0.01197852127220157,
        "step": 87
    },
    {
        "loss": 2.9016,
        "grad_norm": 1.6779922246932983,
        "learning_rate": 0.00019966371101978975,
        "epoch": 0.01211620542475561,
        "step": 88
    },
    {
        "loss": 2.5074,
        "grad_norm": 1.030254602432251,
        "learning_rate": 0.00019965295667848435,
        "epoch": 0.012253889577309652,
        "step": 89
    },
    {
        "loss": 2.4619,
        "grad_norm": 1.474216103553772,
        "learning_rate": 0.00019964203337277344,
        "epoch": 0.012391573729863693,
        "step": 90
    },
    {
        "loss": 2.708,
        "grad_norm": 1.4104622602462769,
        "learning_rate": 0.00019963094112117785,
        "epoch": 0.012529257882417733,
        "step": 91
    },
    {
        "loss": 2.8327,
        "grad_norm": 1.2964694499969482,
        "learning_rate": 0.00019961967994250478,
        "epoch": 0.012666942034971774,
        "step": 92
    },
    {
        "loss": 2.8026,
        "grad_norm": 2.040785074234009,
        "learning_rate": 0.0001996082498558479,
        "epoch": 0.012804626187525816,
        "step": 93
    },
    {
        "loss": 2.7444,
        "grad_norm": 1.2549186944961548,
        "learning_rate": 0.00019959665088058718,
        "epoch": 0.012942310340079857,
        "step": 94
    },
    {
        "loss": 2.7735,
        "grad_norm": 1.8046832084655762,
        "learning_rate": 0.00019958488303638906,
        "epoch": 0.013079994492633898,
        "step": 95
    },
    {
        "loss": 2.487,
        "grad_norm": 1.4069844484329224,
        "learning_rate": 0.0001995729463432062,
        "epoch": 0.01321767864518794,
        "step": 96
    },
    {
        "loss": 2.5861,
        "grad_norm": 1.2253402471542358,
        "learning_rate": 0.0001995608408212777,
        "epoch": 0.01335536279774198,
        "step": 97
    },
    {
        "loss": 2.4445,
        "grad_norm": 1.4857285022735596,
        "learning_rate": 0.00019954856649112874,
        "epoch": 0.01349304695029602,
        "step": 98
    },
    {
        "loss": 2.6464,
        "grad_norm": 1.631287932395935,
        "learning_rate": 0.0001995361233735708,
        "epoch": 0.013630731102850062,
        "step": 99
    },
    {
        "loss": 2.5034,
        "grad_norm": 1.9464905261993408,
        "learning_rate": 0.0001995235114897015,
        "epoch": 0.013768415255404103,
        "step": 100
    },
    {
        "loss": 1.952,
        "grad_norm": 1.5692158937454224,
        "learning_rate": 0.00019951073086090477,
        "epoch": 0.013906099407958145,
        "step": 101
    },
    {
        "loss": 2.166,
        "grad_norm": 1.5680838823318481,
        "learning_rate": 0.00019949778150885042,
        "epoch": 0.014043783560512184,
        "step": 102
    },
    {
        "loss": 2.0433,
        "grad_norm": 2.5917418003082275,
        "learning_rate": 0.00019948466345549447,
        "epoch": 0.014181467713066226,
        "step": 103
    },
    {
        "loss": 2.3493,
        "grad_norm": 2.3263652324676514,
        "learning_rate": 0.00019947137672307895,
        "epoch": 0.014319151865620267,
        "step": 104
    },
    {
        "loss": 2.5411,
        "grad_norm": 1.897844910621643,
        "learning_rate": 0.00019945792133413187,
        "epoch": 0.014456836018174308,
        "step": 105
    },
    {
        "loss": 1.985,
        "grad_norm": 1.3018659353256226,
        "learning_rate": 0.00019944429731146729,
        "epoch": 0.01459452017072835,
        "step": 106
    },
    {
        "loss": 2.5115,
        "grad_norm": 1.1051446199417114,
        "learning_rate": 0.00019943050467818507,
        "epoch": 0.014732204323282391,
        "step": 107
    },
    {
        "loss": 2.5692,
        "grad_norm": 1.6647909879684448,
        "learning_rate": 0.000199416543457671,
        "epoch": 0.01486988847583643,
        "step": 108
    },
    {
        "loss": 1.7679,
        "grad_norm": 1.5673375129699707,
        "learning_rate": 0.00019940241367359676,
        "epoch": 0.015007572628390472,
        "step": 109
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.273436188697815,
        "learning_rate": 0.0001993881153499198,
        "epoch": 0.015145256780944513,
        "step": 110
    },
    {
        "loss": 2.6442,
        "grad_norm": 1.969989538192749,
        "learning_rate": 0.00019937364851088326,
        "epoch": 0.015282940933498555,
        "step": 111
    },
    {
        "loss": 2.6308,
        "grad_norm": 1.6680340766906738,
        "learning_rate": 0.00019935901318101617,
        "epoch": 0.015420625086052596,
        "step": 112
    },
    {
        "loss": 1.5771,
        "grad_norm": 2.5025572776794434,
        "learning_rate": 0.00019934420938513313,
        "epoch": 0.015558309238606636,
        "step": 113
    },
    {
        "loss": 2.2091,
        "grad_norm": 1.5087916851043701,
        "learning_rate": 0.00019932923714833432,
        "epoch": 0.01569599339116068,
        "step": 114
    },
    {
        "loss": 2.2113,
        "grad_norm": 1.778424859046936,
        "learning_rate": 0.00019931409649600565,
        "epoch": 0.01583367754371472,
        "step": 115
    },
    {
        "loss": 2.5736,
        "grad_norm": 1.3063510656356812,
        "learning_rate": 0.00019929878745381853,
        "epoch": 0.015971361696268758,
        "step": 116
    },
    {
        "loss": 2.4418,
        "grad_norm": 1.62191641330719,
        "learning_rate": 0.00019928331004772986,
        "epoch": 0.0161090458488228,
        "step": 117
    },
    {
        "loss": 2.5327,
        "grad_norm": 1.8259475231170654,
        "learning_rate": 0.000199267664303982,
        "epoch": 0.01624673000137684,
        "step": 118
    },
    {
        "loss": 2.4654,
        "grad_norm": 1.5398430824279785,
        "learning_rate": 0.00019925185024910277,
        "epoch": 0.016384414153930882,
        "step": 119
    },
    {
        "loss": 2.397,
        "grad_norm": 1.2752012014389038,
        "learning_rate": 0.00019923586790990537,
        "epoch": 0.016522098306484923,
        "step": 120
    },
    {
        "loss": 2.2512,
        "grad_norm": 1.9211915731430054,
        "learning_rate": 0.00019921971731348828,
        "epoch": 0.016659782459038965,
        "step": 121
    },
    {
        "loss": 1.7105,
        "grad_norm": 2.100780963897705,
        "learning_rate": 0.00019920339848723527,
        "epoch": 0.016797466611593006,
        "step": 122
    },
    {
        "loss": 2.2658,
        "grad_norm": 2.1533007621765137,
        "learning_rate": 0.00019918691145881542,
        "epoch": 0.016935150764147047,
        "step": 123
    },
    {
        "loss": 2.1533,
        "grad_norm": 2.207231283187866,
        "learning_rate": 0.00019917025625618292,
        "epoch": 0.01707283491670109,
        "step": 124
    },
    {
        "loss": 2.8948,
        "grad_norm": 1.0899063348770142,
        "learning_rate": 0.00019915343290757717,
        "epoch": 0.01721051906925513,
        "step": 125
    },
    {
        "loss": 2.1493,
        "grad_norm": 1.7201682329177856,
        "learning_rate": 0.0001991364414415226,
        "epoch": 0.01734820322180917,
        "step": 126
    },
    {
        "loss": 2.1825,
        "grad_norm": 2.07877779006958,
        "learning_rate": 0.00019911928188682873,
        "epoch": 0.01748588737436321,
        "step": 127
    },
    {
        "loss": 2.5868,
        "grad_norm": 1.8362369537353516,
        "learning_rate": 0.00019910195427259007,
        "epoch": 0.01762357152691725,
        "step": 128
    },
    {
        "loss": 2.3241,
        "grad_norm": 1.1565883159637451,
        "learning_rate": 0.00019908445862818608,
        "epoch": 0.017761255679471292,
        "step": 129
    },
    {
        "loss": 2.6979,
        "grad_norm": 1.3765000104904175,
        "learning_rate": 0.00019906679498328113,
        "epoch": 0.017898939832025333,
        "step": 130
    },
    {
        "loss": 2.563,
        "grad_norm": 1.642143726348877,
        "learning_rate": 0.00019904896336782438,
        "epoch": 0.018036623984579375,
        "step": 131
    },
    {
        "loss": 2.7396,
        "grad_norm": 0.9485841393470764,
        "learning_rate": 0.00019903096381204994,
        "epoch": 0.018174308137133416,
        "step": 132
    },
    {
        "loss": 2.2195,
        "grad_norm": 2.0060291290283203,
        "learning_rate": 0.00019901279634647646,
        "epoch": 0.018311992289687457,
        "step": 133
    },
    {
        "loss": 2.3456,
        "grad_norm": 1.36540949344635,
        "learning_rate": 0.0001989944610019075,
        "epoch": 0.0184496764422415,
        "step": 134
    },
    {
        "loss": 2.0699,
        "grad_norm": 2.2381906509399414,
        "learning_rate": 0.00019897595780943102,
        "epoch": 0.01858736059479554,
        "step": 135
    },
    {
        "loss": 2.3792,
        "grad_norm": 1.5875883102416992,
        "learning_rate": 0.00019895728680041982,
        "epoch": 0.01872504474734958,
        "step": 136
    },
    {
        "loss": 2.3116,
        "grad_norm": 2.021535634994507,
        "learning_rate": 0.00019893844800653108,
        "epoch": 0.018862728899903623,
        "step": 137
    },
    {
        "loss": 2.379,
        "grad_norm": 1.5253468751907349,
        "learning_rate": 0.0001989194414597065,
        "epoch": 0.01900041305245766,
        "step": 138
    },
    {
        "loss": 2.3591,
        "grad_norm": 0.9860053062438965,
        "learning_rate": 0.0001989002671921722,
        "epoch": 0.019138097205011702,
        "step": 139
    },
    {
        "loss": 2.5134,
        "grad_norm": 1.806342601776123,
        "learning_rate": 0.00019888092523643875,
        "epoch": 0.019275781357565743,
        "step": 140
    },
    {
        "loss": 2.4646,
        "grad_norm": 2.250441789627075,
        "learning_rate": 0.00019886141562530096,
        "epoch": 0.019413465510119784,
        "step": 141
    },
    {
        "loss": 2.3316,
        "grad_norm": 1.736994743347168,
        "learning_rate": 0.0001988417383918379,
        "epoch": 0.019551149662673826,
        "step": 142
    },
    {
        "loss": 2.39,
        "grad_norm": 1.0886244773864746,
        "learning_rate": 0.00019882189356941292,
        "epoch": 0.019688833815227867,
        "step": 143
    },
    {
        "loss": 2.6097,
        "grad_norm": 1.6791132688522339,
        "learning_rate": 0.00019880188119167342,
        "epoch": 0.01982651796778191,
        "step": 144
    },
    {
        "loss": 2.451,
        "grad_norm": 1.690953254699707,
        "learning_rate": 0.00019878170129255097,
        "epoch": 0.01996420212033595,
        "step": 145
    },
    {
        "loss": 2.4481,
        "grad_norm": 1.4491889476776123,
        "learning_rate": 0.00019876135390626122,
        "epoch": 0.02010188627288999,
        "step": 146
    },
    {
        "loss": 1.2919,
        "grad_norm": 2.9999983310699463,
        "learning_rate": 0.00019874083906730369,
        "epoch": 0.020239570425444033,
        "step": 147
    },
    {
        "loss": 2.2119,
        "grad_norm": 2.2024893760681152,
        "learning_rate": 0.00019872015681046185,
        "epoch": 0.020377254577998074,
        "step": 148
    },
    {
        "loss": 2.8195,
        "grad_norm": 1.2103554010391235,
        "learning_rate": 0.00019869930717080308,
        "epoch": 0.020514938730552112,
        "step": 149
    },
    {
        "loss": 2.3544,
        "grad_norm": 2.0406200885772705,
        "learning_rate": 0.00019867829018367853,
        "epoch": 0.020652622883106153,
        "step": 150
    },
    {
        "loss": 2.6514,
        "grad_norm": 1.8219882249832153,
        "learning_rate": 0.0001986571058847231,
        "epoch": 0.020790307035660194,
        "step": 151
    },
    {
        "loss": 1.9854,
        "grad_norm": 1.929822325706482,
        "learning_rate": 0.00019863575430985534,
        "epoch": 0.020927991188214236,
        "step": 152
    },
    {
        "loss": 2.4428,
        "grad_norm": 1.446022629737854,
        "learning_rate": 0.0001986142354952775,
        "epoch": 0.021065675340768277,
        "step": 153
    },
    {
        "loss": 2.2002,
        "grad_norm": 1.9984233379364014,
        "learning_rate": 0.00019859254947747528,
        "epoch": 0.02120335949332232,
        "step": 154
    },
    {
        "loss": 2.2152,
        "grad_norm": 1.9893560409545898,
        "learning_rate": 0.00019857069629321798,
        "epoch": 0.02134104364587636,
        "step": 155
    },
    {
        "loss": 1.9171,
        "grad_norm": 2.6625938415527344,
        "learning_rate": 0.0001985486759795583,
        "epoch": 0.0214787277984304,
        "step": 156
    },
    {
        "loss": 2.4589,
        "grad_norm": 1.7824430465698242,
        "learning_rate": 0.00019852648857383222,
        "epoch": 0.021616411950984443,
        "step": 157
    },
    {
        "loss": 2.4588,
        "grad_norm": 1.2828034162521362,
        "learning_rate": 0.00019850413411365923,
        "epoch": 0.021754096103538484,
        "step": 158
    },
    {
        "loss": 2.3405,
        "grad_norm": 1.0931155681610107,
        "learning_rate": 0.00019848161263694185,
        "epoch": 0.021891780256092525,
        "step": 159
    },
    {
        "loss": 2.0845,
        "grad_norm": 1.6670280694961548,
        "learning_rate": 0.00019845892418186595,
        "epoch": 0.022029464408646563,
        "step": 160
    },
    {
        "loss": 2.6417,
        "grad_norm": 1.5370104312896729,
        "learning_rate": 0.00019843606878690043,
        "epoch": 0.022167148561200604,
        "step": 161
    },
    {
        "loss": 2.3476,
        "grad_norm": 0.860775887966156,
        "learning_rate": 0.00019841304649079723,
        "epoch": 0.022304832713754646,
        "step": 162
    },
    {
        "loss": 2.6073,
        "grad_norm": 1.1391398906707764,
        "learning_rate": 0.00019838985733259133,
        "epoch": 0.022442516866308687,
        "step": 163
    },
    {
        "loss": 2.2062,
        "grad_norm": 1.5655126571655273,
        "learning_rate": 0.0001983665013516006,
        "epoch": 0.02258020101886273,
        "step": 164
    },
    {
        "loss": 2.9129,
        "grad_norm": 1.230796217918396,
        "learning_rate": 0.00019834297858742575,
        "epoch": 0.02271788517141677,
        "step": 165
    },
    {
        "loss": 2.447,
        "grad_norm": 1.3005244731903076,
        "learning_rate": 0.0001983192890799503,
        "epoch": 0.02285556932397081,
        "step": 166
    },
    {
        "loss": 2.4626,
        "grad_norm": 2.715742588043213,
        "learning_rate": 0.0001982954328693405,
        "epoch": 0.022993253476524853,
        "step": 167
    },
    {
        "loss": 2.3759,
        "grad_norm": 1.385947346687317,
        "learning_rate": 0.0001982714099960452,
        "epoch": 0.023130937629078894,
        "step": 168
    },
    {
        "loss": 2.3479,
        "grad_norm": 1.2386839389801025,
        "learning_rate": 0.00019824722050079593,
        "epoch": 0.023268621781632935,
        "step": 169
    },
    {
        "loss": 1.676,
        "grad_norm": 1.8633201122283936,
        "learning_rate": 0.00019822286442460653,
        "epoch": 0.023406305934186977,
        "step": 170
    },
    {
        "loss": 2.3602,
        "grad_norm": 1.9209682941436768,
        "learning_rate": 0.00019819834180877352,
        "epoch": 0.023543990086741014,
        "step": 171
    },
    {
        "loss": 2.5259,
        "grad_norm": 1.5189284086227417,
        "learning_rate": 0.0001981736526948757,
        "epoch": 0.023681674239295056,
        "step": 172
    },
    {
        "loss": 2.403,
        "grad_norm": 2.2339322566986084,
        "learning_rate": 0.00019814879712477408,
        "epoch": 0.023819358391849097,
        "step": 173
    },
    {
        "loss": 2.5804,
        "grad_norm": 1.9847911596298218,
        "learning_rate": 0.00019812377514061202,
        "epoch": 0.02395704254440314,
        "step": 174
    },
    {
        "loss": 2.7323,
        "grad_norm": 1.6258631944656372,
        "learning_rate": 0.00019809858678481503,
        "epoch": 0.02409472669695718,
        "step": 175
    },
    {
        "loss": 2.2092,
        "grad_norm": 1.8033385276794434,
        "learning_rate": 0.00019807323210009067,
        "epoch": 0.02423241084951122,
        "step": 176
    },
    {
        "loss": 1.8615,
        "grad_norm": 2.618596315383911,
        "learning_rate": 0.00019804771112942845,
        "epoch": 0.024370095002065262,
        "step": 177
    },
    {
        "loss": 2.0988,
        "grad_norm": 1.3032245635986328,
        "learning_rate": 0.0001980220239161,
        "epoch": 0.024507779154619304,
        "step": 178
    },
    {
        "loss": 2.4432,
        "grad_norm": 0.984459400177002,
        "learning_rate": 0.0001979961705036587,
        "epoch": 0.024645463307173345,
        "step": 179
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.7730964422225952,
        "learning_rate": 0.0001979701509359397,
        "epoch": 0.024783147459727387,
        "step": 180
    },
    {
        "loss": 2.4073,
        "grad_norm": 2.486722469329834,
        "learning_rate": 0.0001979439652570599,
        "epoch": 0.024920831612281428,
        "step": 181
    },
    {
        "loss": 1.8859,
        "grad_norm": 1.826684594154358,
        "learning_rate": 0.0001979176135114179,
        "epoch": 0.025058515764835466,
        "step": 182
    },
    {
        "loss": 2.0412,
        "grad_norm": 1.4491608142852783,
        "learning_rate": 0.00019789109574369382,
        "epoch": 0.025196199917389507,
        "step": 183
    },
    {
        "loss": 2.5108,
        "grad_norm": 2.3009378910064697,
        "learning_rate": 0.00019786441199884928,
        "epoch": 0.02533388406994355,
        "step": 184
    },
    {
        "loss": 2.5925,
        "grad_norm": 1.2678016424179077,
        "learning_rate": 0.0001978375623221273,
        "epoch": 0.02547156822249759,
        "step": 185
    },
    {
        "loss": 2.5949,
        "grad_norm": 1.7851403951644897,
        "learning_rate": 0.00019781054675905235,
        "epoch": 0.02560925237505163,
        "step": 186
    },
    {
        "loss": 2.2576,
        "grad_norm": 2.051987886428833,
        "learning_rate": 0.00019778336535542998,
        "epoch": 0.025746936527605672,
        "step": 187
    },
    {
        "loss": 2.5025,
        "grad_norm": 1.0810705423355103,
        "learning_rate": 0.0001977560181573471,
        "epoch": 0.025884620680159714,
        "step": 188
    },
    {
        "loss": 2.4719,
        "grad_norm": 1.4546207189559937,
        "learning_rate": 0.0001977285052111716,
        "epoch": 0.026022304832713755,
        "step": 189
    },
    {
        "loss": 2.7302,
        "grad_norm": 1.4297001361846924,
        "learning_rate": 0.0001977008265635525,
        "epoch": 0.026159988985267797,
        "step": 190
    },
    {
        "loss": 2.5823,
        "grad_norm": 1.4451417922973633,
        "learning_rate": 0.0001976729822614197,
        "epoch": 0.026297673137821838,
        "step": 191
    },
    {
        "loss": 2.3014,
        "grad_norm": 1.3276960849761963,
        "learning_rate": 0.00019764497235198405,
        "epoch": 0.02643535729037588,
        "step": 192
    },
    {
        "loss": 2.4498,
        "grad_norm": 1.0396136045455933,
        "learning_rate": 0.00019761679688273708,
        "epoch": 0.026573041442929917,
        "step": 193
    },
    {
        "loss": 2.609,
        "grad_norm": 1.6533242464065552,
        "learning_rate": 0.00019758845590145115,
        "epoch": 0.02671072559548396,
        "step": 194
    },
    {
        "loss": 2.6893,
        "grad_norm": 0.9022952914237976,
        "learning_rate": 0.00019755994945617916,
        "epoch": 0.026848409748038,
        "step": 195
    },
    {
        "loss": 2.442,
        "grad_norm": 1.286275029182434,
        "learning_rate": 0.0001975312775952546,
        "epoch": 0.02698609390059204,
        "step": 196
    },
    {
        "loss": 2.6211,
        "grad_norm": 1.8119922876358032,
        "learning_rate": 0.0001975024403672914,
        "epoch": 0.027123778053146082,
        "step": 197
    },
    {
        "loss": 2.4205,
        "grad_norm": 1.4136751890182495,
        "learning_rate": 0.000197473437821184,
        "epoch": 0.027261462205700124,
        "step": 198
    },
    {
        "loss": 2.6065,
        "grad_norm": 1.8533384799957275,
        "learning_rate": 0.0001974442700061069,
        "epoch": 0.027399146358254165,
        "step": 199
    },
    {
        "loss": 2.3437,
        "grad_norm": 1.4238048791885376,
        "learning_rate": 0.00019741493697151503,
        "epoch": 0.027536830510808206,
        "step": 200
    },
    {
        "loss": 1.9035,
        "grad_norm": 1.4445019960403442,
        "learning_rate": 0.00019738543876714334,
        "epoch": 0.027674514663362248,
        "step": 201
    },
    {
        "loss": 2.6218,
        "grad_norm": 1.251132607460022,
        "learning_rate": 0.00019735577544300692,
        "epoch": 0.02781219881591629,
        "step": 202
    },
    {
        "loss": 2.8655,
        "grad_norm": 1.059720754623413,
        "learning_rate": 0.00019732594704940072,
        "epoch": 0.02794988296847033,
        "step": 203
    },
    {
        "loss": 2.6995,
        "grad_norm": 1.4902963638305664,
        "learning_rate": 0.00019729595363689967,
        "epoch": 0.02808756712102437,
        "step": 204
    },
    {
        "loss": 1.9206,
        "grad_norm": 2.185920000076294,
        "learning_rate": 0.00019726579525635842,
        "epoch": 0.02822525127357841,
        "step": 205
    },
    {
        "loss": 1.9372,
        "grad_norm": 1.7043830156326294,
        "learning_rate": 0.00019723547195891138,
        "epoch": 0.02836293542613245,
        "step": 206
    },
    {
        "loss": 2.1429,
        "grad_norm": 1.3705095052719116,
        "learning_rate": 0.00019720498379597253,
        "epoch": 0.028500619578686492,
        "step": 207
    },
    {
        "loss": 2.7276,
        "grad_norm": 2.129831314086914,
        "learning_rate": 0.00019717433081923544,
        "epoch": 0.028638303731240534,
        "step": 208
    },
    {
        "loss": 2.4842,
        "grad_norm": 1.264406681060791,
        "learning_rate": 0.0001971435130806731,
        "epoch": 0.028775987883794575,
        "step": 209
    },
    {
        "loss": 2.4152,
        "grad_norm": 1.9801336526870728,
        "learning_rate": 0.0001971125306325378,
        "epoch": 0.028913672036348616,
        "step": 210
    },
    {
        "loss": 2.4381,
        "grad_norm": 1.0608620643615723,
        "learning_rate": 0.00019708138352736123,
        "epoch": 0.029051356188902658,
        "step": 211
    },
    {
        "loss": 2.8785,
        "grad_norm": 1.2690494060516357,
        "learning_rate": 0.00019705007181795416,
        "epoch": 0.0291890403414567,
        "step": 212
    },
    {
        "loss": 2.5985,
        "grad_norm": 2.222695827484131,
        "learning_rate": 0.00019701859555740648,
        "epoch": 0.02932672449401074,
        "step": 213
    },
    {
        "loss": 2.1174,
        "grad_norm": 1.3945826292037964,
        "learning_rate": 0.00019698695479908703,
        "epoch": 0.029464408646564782,
        "step": 214
    },
    {
        "loss": 2.5002,
        "grad_norm": 1.0792750120162964,
        "learning_rate": 0.0001969551495966437,
        "epoch": 0.02960209279911882,
        "step": 215
    },
    {
        "loss": 2.3829,
        "grad_norm": 1.8277268409729004,
        "learning_rate": 0.00019692318000400305,
        "epoch": 0.02973977695167286,
        "step": 216
    },
    {
        "loss": 2.629,
        "grad_norm": 1.1634290218353271,
        "learning_rate": 0.00019689104607537044,
        "epoch": 0.029877461104226902,
        "step": 217
    },
    {
        "loss": 2.4194,
        "grad_norm": 1.1859596967697144,
        "learning_rate": 0.0001968587478652299,
        "epoch": 0.030015145256780944,
        "step": 218
    },
    {
        "loss": 2.2675,
        "grad_norm": 1.3831125497817993,
        "learning_rate": 0.00019682628542834387,
        "epoch": 0.030152829409334985,
        "step": 219
    },
    {
        "loss": 2.1626,
        "grad_norm": 1.3169068098068237,
        "learning_rate": 0.00019679365881975337,
        "epoch": 0.030290513561889026,
        "step": 220
    },
    {
        "loss": 2.2529,
        "grad_norm": 1.9915958642959595,
        "learning_rate": 0.0001967608680947778,
        "epoch": 0.030428197714443068,
        "step": 221
    },
    {
        "loss": 2.5687,
        "grad_norm": 1.2311581373214722,
        "learning_rate": 0.00019672791330901465,
        "epoch": 0.03056588186699711,
        "step": 222
    },
    {
        "loss": 2.3078,
        "grad_norm": 1.308883547782898,
        "learning_rate": 0.00019669479451833976,
        "epoch": 0.03070356601955115,
        "step": 223
    },
    {
        "loss": 2.3192,
        "grad_norm": 2.3717615604400635,
        "learning_rate": 0.00019666151177890698,
        "epoch": 0.030841250172105192,
        "step": 224
    },
    {
        "loss": 2.7773,
        "grad_norm": 1.00662362575531,
        "learning_rate": 0.0001966280651471481,
        "epoch": 0.030978934324659233,
        "step": 225
    },
    {
        "loss": 2.3709,
        "grad_norm": 1.5919609069824219,
        "learning_rate": 0.00019659445467977283,
        "epoch": 0.03111661847721327,
        "step": 226
    },
    {
        "loss": 1.856,
        "grad_norm": 1.2938035726547241,
        "learning_rate": 0.00019656068043376874,
        "epoch": 0.03125430262976731,
        "step": 227
    },
    {
        "loss": 2.3374,
        "grad_norm": 2.3775742053985596,
        "learning_rate": 0.00019652674246640092,
        "epoch": 0.03139198678232136,
        "step": 228
    },
    {
        "loss": 2.4912,
        "grad_norm": 1.5077208280563354,
        "learning_rate": 0.00019649264083521222,
        "epoch": 0.031529670934875395,
        "step": 229
    },
    {
        "loss": 2.6199,
        "grad_norm": 1.4613829851150513,
        "learning_rate": 0.0001964583755980229,
        "epoch": 0.03166735508742944,
        "step": 230
    },
    {
        "loss": 1.2478,
        "grad_norm": 2.5220186710357666,
        "learning_rate": 0.00019642394681293062,
        "epoch": 0.03180503923998348,
        "step": 231
    },
    {
        "loss": 2.5961,
        "grad_norm": 1.3877885341644287,
        "learning_rate": 0.00019638935453831038,
        "epoch": 0.031942723392537516,
        "step": 232
    },
    {
        "loss": 2.9777,
        "grad_norm": 1.0638402700424194,
        "learning_rate": 0.00019635459883281432,
        "epoch": 0.03208040754509156,
        "step": 233
    },
    {
        "loss": 2.5892,
        "grad_norm": 1.274113416671753,
        "learning_rate": 0.0001963196797553718,
        "epoch": 0.0322180916976456,
        "step": 234
    },
    {
        "loss": 2.0388,
        "grad_norm": 1.780666708946228,
        "learning_rate": 0.00019628459736518905,
        "epoch": 0.03235577585019964,
        "step": 235
    },
    {
        "loss": 2.0134,
        "grad_norm": 2.3382396697998047,
        "learning_rate": 0.00019624935172174926,
        "epoch": 0.03249346000275368,
        "step": 236
    },
    {
        "loss": 2.3494,
        "grad_norm": 2.268433094024658,
        "learning_rate": 0.0001962139428848124,
        "epoch": 0.032631144155307726,
        "step": 237
    },
    {
        "loss": 2.357,
        "grad_norm": 1.8519617319107056,
        "learning_rate": 0.0001961783709144152,
        "epoch": 0.032768828307861764,
        "step": 238
    },
    {
        "loss": 2.4669,
        "grad_norm": 1.0495216846466064,
        "learning_rate": 0.00019614263587087088,
        "epoch": 0.03290651246041581,
        "step": 239
    },
    {
        "loss": 2.45,
        "grad_norm": 1.830711841583252,
        "learning_rate": 0.00019610673781476928,
        "epoch": 0.033044196612969846,
        "step": 240
    },
    {
        "loss": 2.5964,
        "grad_norm": 1.0295779705047607,
        "learning_rate": 0.00019607067680697652,
        "epoch": 0.03318188076552389,
        "step": 241
    },
    {
        "loss": 2.2806,
        "grad_norm": 1.2712382078170776,
        "learning_rate": 0.0001960344529086351,
        "epoch": 0.03331956491807793,
        "step": 242
    },
    {
        "loss": 2.1113,
        "grad_norm": 2.1176371574401855,
        "learning_rate": 0.00019599806618116362,
        "epoch": 0.03345724907063197,
        "step": 243
    },
    {
        "loss": 2.3517,
        "grad_norm": 1.4345934391021729,
        "learning_rate": 0.00019596151668625683,
        "epoch": 0.03359493322318601,
        "step": 244
    },
    {
        "loss": 2.6444,
        "grad_norm": 1.041208267211914,
        "learning_rate": 0.00019592480448588542,
        "epoch": 0.03373261737574005,
        "step": 245
    },
    {
        "loss": 2.4724,
        "grad_norm": 1.334726095199585,
        "learning_rate": 0.00019588792964229595,
        "epoch": 0.033870301528294094,
        "step": 246
    },
    {
        "loss": 2.6235,
        "grad_norm": 1.1288495063781738,
        "learning_rate": 0.00019585089221801086,
        "epoch": 0.03400798568084813,
        "step": 247
    },
    {
        "loss": 2.0925,
        "grad_norm": 1.7996400594711304,
        "learning_rate": 0.00019581369227582803,
        "epoch": 0.03414566983340218,
        "step": 248
    },
    {
        "loss": 2.3566,
        "grad_norm": 1.7169468402862549,
        "learning_rate": 0.00019577632987882103,
        "epoch": 0.034283353985956215,
        "step": 249
    },
    {
        "loss": 2.3902,
        "grad_norm": 1.1515812873840332,
        "learning_rate": 0.0001957388050903389,
        "epoch": 0.03442103813851026,
        "step": 250
    },
    {
        "loss": 2.1791,
        "grad_norm": 1.8294700384140015,
        "learning_rate": 0.00019570111797400597,
        "epoch": 0.0345587222910643,
        "step": 251
    },
    {
        "loss": 2.619,
        "grad_norm": 1.4526283740997314,
        "learning_rate": 0.00019566326859372178,
        "epoch": 0.03469640644361834,
        "step": 252
    },
    {
        "loss": 2.3045,
        "grad_norm": 1.5474579334259033,
        "learning_rate": 0.0001956252570136611,
        "epoch": 0.03483409059617238,
        "step": 253
    },
    {
        "loss": 2.4947,
        "grad_norm": 1.879584789276123,
        "learning_rate": 0.00019558708329827358,
        "epoch": 0.03497177474872642,
        "step": 254
    },
    {
        "loss": 2.532,
        "grad_norm": 1.284583330154419,
        "learning_rate": 0.00019554874751228382,
        "epoch": 0.03510945890128046,
        "step": 255
    },
    {
        "loss": 2.6664,
        "grad_norm": 1.344555139541626,
        "learning_rate": 0.00019551024972069126,
        "epoch": 0.0352471430538345,
        "step": 256
    },
    {
        "loss": 2.1964,
        "grad_norm": 1.8589560985565186,
        "learning_rate": 0.00019547158998876998,
        "epoch": 0.035384827206388546,
        "step": 257
    },
    {
        "loss": 2.0958,
        "grad_norm": 1.7414846420288086,
        "learning_rate": 0.00019543276838206866,
        "epoch": 0.035522511358942584,
        "step": 258
    },
    {
        "loss": 1.8671,
        "grad_norm": 2.415541172027588,
        "learning_rate": 0.00019539378496641043,
        "epoch": 0.03566019551149663,
        "step": 259
    },
    {
        "loss": 2.5763,
        "grad_norm": 1.232715368270874,
        "learning_rate": 0.00019535463980789277,
        "epoch": 0.035797879664050666,
        "step": 260
    },
    {
        "loss": 2.5201,
        "grad_norm": 1.0494388341903687,
        "learning_rate": 0.0001953153329728874,
        "epoch": 0.03593556381660471,
        "step": 261
    },
    {
        "loss": 1.7144,
        "grad_norm": 2.2901904582977295,
        "learning_rate": 0.00019527586452804018,
        "epoch": 0.03607324796915875,
        "step": 262
    },
    {
        "loss": 2.6689,
        "grad_norm": 1.9122525453567505,
        "learning_rate": 0.00019523623454027097,
        "epoch": 0.036210932121712794,
        "step": 263
    },
    {
        "loss": 2.09,
        "grad_norm": 1.3259974718093872,
        "learning_rate": 0.0001951964430767735,
        "epoch": 0.03634861627426683,
        "step": 264
    },
    {
        "loss": 2.1172,
        "grad_norm": 0.9736406803131104,
        "learning_rate": 0.0001951564902050154,
        "epoch": 0.03648630042682087,
        "step": 265
    },
    {
        "loss": 2.0419,
        "grad_norm": 1.4524613618850708,
        "learning_rate": 0.00019511637599273782,
        "epoch": 0.036623984579374914,
        "step": 266
    },
    {
        "loss": 1.7568,
        "grad_norm": 1.6378629207611084,
        "learning_rate": 0.00019507610050795558,
        "epoch": 0.03676166873192895,
        "step": 267
    },
    {
        "loss": 2.3558,
        "grad_norm": 1.134590744972229,
        "learning_rate": 0.0001950356638189569,
        "epoch": 0.036899352884483,
        "step": 268
    },
    {
        "loss": 2.298,
        "grad_norm": 2.2798383235931396,
        "learning_rate": 0.00019499506599430328,
        "epoch": 0.037037037037037035,
        "step": 269
    },
    {
        "loss": 2.8212,
        "grad_norm": 1.1201368570327759,
        "learning_rate": 0.00019495430710282955,
        "epoch": 0.03717472118959108,
        "step": 270
    },
    {
        "loss": 2.3153,
        "grad_norm": 1.4140630960464478,
        "learning_rate": 0.00019491338721364354,
        "epoch": 0.03731240534214512,
        "step": 271
    },
    {
        "loss": 2.2721,
        "grad_norm": 1.352555751800537,
        "learning_rate": 0.00019487230639612607,
        "epoch": 0.03745008949469916,
        "step": 272
    },
    {
        "loss": 2.3866,
        "grad_norm": 1.5517748594284058,
        "learning_rate": 0.0001948310647199308,
        "epoch": 0.0375877736472532,
        "step": 273
    },
    {
        "loss": 2.7137,
        "grad_norm": 1.4646992683410645,
        "learning_rate": 0.0001947896622549842,
        "epoch": 0.037725457799807245,
        "step": 274
    },
    {
        "loss": 2.012,
        "grad_norm": 2.0017430782318115,
        "learning_rate": 0.00019474809907148532,
        "epoch": 0.03786314195236128,
        "step": 275
    },
    {
        "loss": 2.6356,
        "grad_norm": 1.3755186796188354,
        "learning_rate": 0.0001947063752399057,
        "epoch": 0.03800082610491532,
        "step": 276
    },
    {
        "loss": 2.3434,
        "grad_norm": 1.9050966501235962,
        "learning_rate": 0.0001946644908309893,
        "epoch": 0.038138510257469366,
        "step": 277
    },
    {
        "loss": 2.5437,
        "grad_norm": 1.5939432382583618,
        "learning_rate": 0.00019462244591575222,
        "epoch": 0.038276194410023404,
        "step": 278
    },
    {
        "loss": 2.1477,
        "grad_norm": 1.475701093673706,
        "learning_rate": 0.00019458024056548295,
        "epoch": 0.03841387856257745,
        "step": 279
    },
    {
        "loss": 2.6261,
        "grad_norm": 1.6203299760818481,
        "learning_rate": 0.00019453787485174175,
        "epoch": 0.038551562715131486,
        "step": 280
    },
    {
        "loss": 2.6385,
        "grad_norm": 1.4876728057861328,
        "learning_rate": 0.0001944953488463609,
        "epoch": 0.03868924686768553,
        "step": 281
    },
    {
        "loss": 2.0977,
        "grad_norm": 1.251285195350647,
        "learning_rate": 0.0001944526626214445,
        "epoch": 0.03882693102023957,
        "step": 282
    },
    {
        "loss": 2.1649,
        "grad_norm": 1.569525122642517,
        "learning_rate": 0.00019440981624936821,
        "epoch": 0.038964615172793614,
        "step": 283
    },
    {
        "loss": 2.3588,
        "grad_norm": 1.2401392459869385,
        "learning_rate": 0.0001943668098027793,
        "epoch": 0.03910229932534765,
        "step": 284
    },
    {
        "loss": 2.4398,
        "grad_norm": 1.9901254177093506,
        "learning_rate": 0.00019432364335459628,
        "epoch": 0.039239983477901696,
        "step": 285
    },
    {
        "loss": 2.7282,
        "grad_norm": 1.1501625776290894,
        "learning_rate": 0.0001942803169780093,
        "epoch": 0.039377667630455734,
        "step": 286
    },
    {
        "loss": 2.627,
        "grad_norm": 0.8275762796401978,
        "learning_rate": 0.00019423683074647929,
        "epoch": 0.03951535178300977,
        "step": 287
    },
    {
        "loss": 2.4883,
        "grad_norm": 1.8876557350158691,
        "learning_rate": 0.00019419318473373845,
        "epoch": 0.03965303593556382,
        "step": 288
    },
    {
        "loss": 2.4675,
        "grad_norm": 1.6650311946868896,
        "learning_rate": 0.00019414937901378982,
        "epoch": 0.039790720088117855,
        "step": 289
    },
    {
        "loss": 1.4391,
        "grad_norm": 2.207648277282715,
        "learning_rate": 0.00019410541366090723,
        "epoch": 0.0399284042406719,
        "step": 290
    },
    {
        "loss": 2.1994,
        "grad_norm": 1.9933356046676636,
        "learning_rate": 0.0001940612887496352,
        "epoch": 0.04006608839322594,
        "step": 291
    },
    {
        "loss": 2.4133,
        "grad_norm": 1.2939215898513794,
        "learning_rate": 0.00019401700435478874,
        "epoch": 0.04020377254577998,
        "step": 292
    },
    {
        "loss": 2.2465,
        "grad_norm": 1.949471116065979,
        "learning_rate": 0.00019397256055145328,
        "epoch": 0.04034145669833402,
        "step": 293
    },
    {
        "loss": 1.7442,
        "grad_norm": 2.287867546081543,
        "learning_rate": 0.0001939279574149846,
        "epoch": 0.040479140850888065,
        "step": 294
    },
    {
        "loss": 2.3523,
        "grad_norm": 1.161495327949524,
        "learning_rate": 0.0001938831950210085,
        "epoch": 0.0406168250034421,
        "step": 295
    },
    {
        "loss": 2.2933,
        "grad_norm": 2.0023550987243652,
        "learning_rate": 0.00019383827344542098,
        "epoch": 0.04075450915599615,
        "step": 296
    },
    {
        "loss": 2.5225,
        "grad_norm": 1.083147406578064,
        "learning_rate": 0.00019379319276438778,
        "epoch": 0.040892193308550186,
        "step": 297
    },
    {
        "loss": 2.6187,
        "grad_norm": 1.3937512636184692,
        "learning_rate": 0.00019374795305434446,
        "epoch": 0.041029877461104224,
        "step": 298
    },
    {
        "loss": 1.7631,
        "grad_norm": 1.7744927406311035,
        "learning_rate": 0.00019370255439199628,
        "epoch": 0.04116756161365827,
        "step": 299
    },
    {
        "loss": 2.5371,
        "grad_norm": 1.0155011415481567,
        "learning_rate": 0.0001936569968543179,
        "epoch": 0.041305245766212306,
        "step": 300
    },
    {
        "loss": 2.1241,
        "grad_norm": 1.9822537899017334,
        "learning_rate": 0.00019361128051855343,
        "epoch": 0.04144292991876635,
        "step": 301
    },
    {
        "loss": 1.9768,
        "grad_norm": 1.8306629657745361,
        "learning_rate": 0.00019356540546221625,
        "epoch": 0.04158061407132039,
        "step": 302
    },
    {
        "loss": 2.825,
        "grad_norm": 1.440824031829834,
        "learning_rate": 0.00019351937176308875,
        "epoch": 0.041718298223874434,
        "step": 303
    },
    {
        "loss": 2.3847,
        "grad_norm": 1.3502528667449951,
        "learning_rate": 0.00019347317949922245,
        "epoch": 0.04185598237642847,
        "step": 304
    },
    {
        "loss": 2.4952,
        "grad_norm": 1.4030144214630127,
        "learning_rate": 0.00019342682874893755,
        "epoch": 0.041993666528982516,
        "step": 305
    },
    {
        "loss": 1.3637,
        "grad_norm": 2.5765979290008545,
        "learning_rate": 0.00019338031959082307,
        "epoch": 0.042131350681536554,
        "step": 306
    },
    {
        "loss": 1.7272,
        "grad_norm": 1.8495537042617798,
        "learning_rate": 0.0001933336521037367,
        "epoch": 0.0422690348340906,
        "step": 307
    },
    {
        "loss": 2.4055,
        "grad_norm": 1.755516767501831,
        "learning_rate": 0.00019328682636680436,
        "epoch": 0.04240671898664464,
        "step": 308
    },
    {
        "loss": 2.6428,
        "grad_norm": 1.4750924110412598,
        "learning_rate": 0.00019323984245942047,
        "epoch": 0.042544403139198675,
        "step": 309
    },
    {
        "loss": 2.2644,
        "grad_norm": 2.4449357986450195,
        "learning_rate": 0.0001931927004612476,
        "epoch": 0.04268208729175272,
        "step": 310
    },
    {
        "loss": 2.7019,
        "grad_norm": 1.6748621463775635,
        "learning_rate": 0.00019314540045221626,
        "epoch": 0.04281977144430676,
        "step": 311
    },
    {
        "loss": 2.7279,
        "grad_norm": 1.1120160818099976,
        "learning_rate": 0.00019309794251252504,
        "epoch": 0.0429574555968608,
        "step": 312
    },
    {
        "loss": 2.44,
        "grad_norm": 1.3201029300689697,
        "learning_rate": 0.00019305032672264018,
        "epoch": 0.04309513974941484,
        "step": 313
    },
    {
        "loss": 2.5683,
        "grad_norm": 1.3307275772094727,
        "learning_rate": 0.0001930025531632956,
        "epoch": 0.043232823901968885,
        "step": 314
    },
    {
        "loss": 2.5411,
        "grad_norm": 1.776685357093811,
        "learning_rate": 0.0001929546219154927,
        "epoch": 0.04337050805452292,
        "step": 315
    },
    {
        "loss": 2.144,
        "grad_norm": 2.4853944778442383,
        "learning_rate": 0.0001929065330605003,
        "epoch": 0.04350819220707697,
        "step": 316
    },
    {
        "loss": 2.5215,
        "grad_norm": 1.3782840967178345,
        "learning_rate": 0.00019285828667985438,
        "epoch": 0.043645876359631006,
        "step": 317
    },
    {
        "loss": 2.1661,
        "grad_norm": 1.6488672494888306,
        "learning_rate": 0.00019280988285535805,
        "epoch": 0.04378356051218505,
        "step": 318
    },
    {
        "loss": 2.3226,
        "grad_norm": 0.9678875803947449,
        "learning_rate": 0.0001927613216690814,
        "epoch": 0.04392124466473909,
        "step": 319
    },
    {
        "loss": 2.6365,
        "grad_norm": 1.5224403142929077,
        "learning_rate": 0.00019271260320336125,
        "epoch": 0.044058928817293126,
        "step": 320
    },
    {
        "loss": 2.4272,
        "grad_norm": 1.0158329010009766,
        "learning_rate": 0.00019266372754080119,
        "epoch": 0.04419661296984717,
        "step": 321
    },
    {
        "loss": 1.0948,
        "grad_norm": 2.9306905269622803,
        "learning_rate": 0.0001926146947642712,
        "epoch": 0.04433429712240121,
        "step": 322
    },
    {
        "loss": 2.3995,
        "grad_norm": 0.9734619855880737,
        "learning_rate": 0.00019256550495690781,
        "epoch": 0.044471981274955254,
        "step": 323
    },
    {
        "loss": 2.0557,
        "grad_norm": 1.9589381217956543,
        "learning_rate": 0.00019251615820211373,
        "epoch": 0.04460966542750929,
        "step": 324
    },
    {
        "loss": 2.168,
        "grad_norm": 1.9700279235839844,
        "learning_rate": 0.0001924666545835577,
        "epoch": 0.044747349580063336,
        "step": 325
    },
    {
        "loss": 2.6267,
        "grad_norm": 1.3039865493774414,
        "learning_rate": 0.0001924169941851746,
        "epoch": 0.044885033732617374,
        "step": 326
    },
    {
        "loss": 2.0852,
        "grad_norm": 2.0154240131378174,
        "learning_rate": 0.00019236717709116504,
        "epoch": 0.04502271788517142,
        "step": 327
    },
    {
        "loss": 2.334,
        "grad_norm": 1.654240369796753,
        "learning_rate": 0.0001923172033859952,
        "epoch": 0.04516040203772546,
        "step": 328
    },
    {
        "loss": 2.6399,
        "grad_norm": 1.1648657321929932,
        "learning_rate": 0.00019226707315439702,
        "epoch": 0.0452980861902795,
        "step": 329
    },
    {
        "loss": 2.5294,
        "grad_norm": 1.3867298364639282,
        "learning_rate": 0.00019221678648136768,
        "epoch": 0.04543577034283354,
        "step": 330
    },
    {
        "loss": 2.4953,
        "grad_norm": 1.050082802772522,
        "learning_rate": 0.00019216634345216966,
        "epoch": 0.04557345449538758,
        "step": 331
    },
    {
        "loss": 2.3859,
        "grad_norm": 1.140616536140442,
        "learning_rate": 0.00019211574415233055,
        "epoch": 0.04571113864794162,
        "step": 332
    },
    {
        "loss": 2.325,
        "grad_norm": 1.1412795782089233,
        "learning_rate": 0.00019206498866764288,
        "epoch": 0.04584882280049566,
        "step": 333
    },
    {
        "loss": 2.486,
        "grad_norm": 1.164216160774231,
        "learning_rate": 0.00019201407708416404,
        "epoch": 0.045986506953049705,
        "step": 334
    },
    {
        "loss": 2.5173,
        "grad_norm": 0.8393114805221558,
        "learning_rate": 0.000191963009488216,
        "epoch": 0.04612419110560374,
        "step": 335
    },
    {
        "loss": 2.4499,
        "grad_norm": 1.527052879333496,
        "learning_rate": 0.00019191178596638539,
        "epoch": 0.04626187525815779,
        "step": 336
    },
    {
        "loss": 1.9404,
        "grad_norm": 2.2771284580230713,
        "learning_rate": 0.0001918604066055231,
        "epoch": 0.046399559410711826,
        "step": 337
    },
    {
        "loss": 2.2911,
        "grad_norm": 1.9868991374969482,
        "learning_rate": 0.00019180887149274425,
        "epoch": 0.04653724356326587,
        "step": 338
    },
    {
        "loss": 1.955,
        "grad_norm": 1.2240605354309082,
        "learning_rate": 0.00019175718071542814,
        "epoch": 0.04667492771581991,
        "step": 339
    },
    {
        "loss": 2.5346,
        "grad_norm": 1.418822169303894,
        "learning_rate": 0.00019170533436121793,
        "epoch": 0.04681261186837395,
        "step": 340
    },
    {
        "loss": 2.6843,
        "grad_norm": 1.4460999965667725,
        "learning_rate": 0.00019165333251802057,
        "epoch": 0.04695029602092799,
        "step": 341
    },
    {
        "loss": 2.0271,
        "grad_norm": 1.1634135246276855,
        "learning_rate": 0.00019160117527400664,
        "epoch": 0.04708798017348203,
        "step": 342
    },
    {
        "loss": 1.9166,
        "grad_norm": 2.040602445602417,
        "learning_rate": 0.00019154886271761026,
        "epoch": 0.047225664326036074,
        "step": 343
    },
    {
        "loss": 2.6366,
        "grad_norm": 1.4718341827392578,
        "learning_rate": 0.0001914963949375288,
        "epoch": 0.04736334847859011,
        "step": 344
    },
    {
        "loss": 2.5989,
        "grad_norm": 1.0832797288894653,
        "learning_rate": 0.00019144377202272295,
        "epoch": 0.047501032631144156,
        "step": 345
    },
    {
        "loss": 2.5174,
        "grad_norm": 1.0327454805374146,
        "learning_rate": 0.00019139099406241625,
        "epoch": 0.047638716783698194,
        "step": 346
    },
    {
        "loss": 2.5061,
        "grad_norm": 0.9989936351776123,
        "learning_rate": 0.00019133806114609528,
        "epoch": 0.04777640093625224,
        "step": 347
    },
    {
        "loss": 2.5983,
        "grad_norm": 2.283348798751831,
        "learning_rate": 0.00019128497336350927,
        "epoch": 0.04791408508880628,
        "step": 348
    },
    {
        "loss": 2.0217,
        "grad_norm": 2.038918972015381,
        "learning_rate": 0.00019123173080467007,
        "epoch": 0.04805176924136032,
        "step": 349
    },
    {
        "loss": 2.48,
        "grad_norm": 1.9515130519866943,
        "learning_rate": 0.00019117833355985196,
        "epoch": 0.04818945339391436,
        "step": 350
    },
    {
        "loss": 2.758,
        "grad_norm": 1.4587796926498413,
        "learning_rate": 0.00019112478171959144,
        "epoch": 0.048327137546468404,
        "step": 351
    },
    {
        "loss": 2.5144,
        "grad_norm": 1.442078709602356,
        "learning_rate": 0.0001910710753746872,
        "epoch": 0.04846482169902244,
        "step": 352
    },
    {
        "loss": 2.6556,
        "grad_norm": 1.4661046266555786,
        "learning_rate": 0.00019101721461619984,
        "epoch": 0.04860250585157648,
        "step": 353
    },
    {
        "loss": 2.6171,
        "grad_norm": 0.8063352704048157,
        "learning_rate": 0.00019096319953545185,
        "epoch": 0.048740190004130525,
        "step": 354
    },
    {
        "loss": 2.5858,
        "grad_norm": 1.211125135421753,
        "learning_rate": 0.00019090903022402729,
        "epoch": 0.04887787415668456,
        "step": 355
    },
    {
        "loss": 2.5111,
        "grad_norm": 1.841469645500183,
        "learning_rate": 0.00019085470677377174,
        "epoch": 0.04901555830923861,
        "step": 356
    },
    {
        "loss": 2.5265,
        "grad_norm": 1.6465866565704346,
        "learning_rate": 0.00019080022927679216,
        "epoch": 0.049153242461792646,
        "step": 357
    },
    {
        "loss": 2.4619,
        "grad_norm": 1.0792042016983032,
        "learning_rate": 0.00019074559782545668,
        "epoch": 0.04929092661434669,
        "step": 358
    },
    {
        "loss": 0.8465,
        "grad_norm": 1.9978406429290771,
        "learning_rate": 0.00019069081251239453,
        "epoch": 0.04942861076690073,
        "step": 359
    },
    {
        "loss": 2.2719,
        "grad_norm": 1.6434597969055176,
        "learning_rate": 0.0001906358734304957,
        "epoch": 0.04956629491945477,
        "step": 360
    },
    {
        "loss": 2.6978,
        "grad_norm": 1.490835189819336,
        "learning_rate": 0.00019058078067291095,
        "epoch": 0.04970397907200881,
        "step": 361
    },
    {
        "loss": 2.433,
        "grad_norm": 1.2038121223449707,
        "learning_rate": 0.00019052553433305164,
        "epoch": 0.049841663224562856,
        "step": 362
    },
    {
        "loss": 2.5241,
        "grad_norm": 1.5987379550933838,
        "learning_rate": 0.00019047013450458947,
        "epoch": 0.049979347377116894,
        "step": 363
    },
    {
        "loss": 1.6278,
        "grad_norm": 2.0548255443573,
        "learning_rate": 0.00019041458128145645,
        "epoch": 0.05011703152967093,
        "step": 364
    },
    {
        "loss": 1.7343,
        "grad_norm": 2.580024242401123,
        "learning_rate": 0.0001903588747578446,
        "epoch": 0.050254715682224976,
        "step": 365
    },
    {
        "loss": 1.7229,
        "grad_norm": 1.815213680267334,
        "learning_rate": 0.00019030301502820596,
        "epoch": 0.050392399834779014,
        "step": 366
    },
    {
        "loss": 2.4302,
        "grad_norm": 1.811025619506836,
        "learning_rate": 0.00019024700218725222,
        "epoch": 0.05053008398733306,
        "step": 367
    },
    {
        "loss": 2.0832,
        "grad_norm": 2.2842013835906982,
        "learning_rate": 0.00019019083632995475,
        "epoch": 0.0506677681398871,
        "step": 368
    },
    {
        "loss": 2.4252,
        "grad_norm": 1.3386311531066895,
        "learning_rate": 0.0001901345175515444,
        "epoch": 0.05080545229244114,
        "step": 369
    },
    {
        "loss": 3.0132,
        "grad_norm": 1.9875712394714355,
        "learning_rate": 0.0001900780459475112,
        "epoch": 0.05094313644499518,
        "step": 370
    },
    {
        "loss": 2.4238,
        "grad_norm": 0.9117612838745117,
        "learning_rate": 0.0001900214216136044,
        "epoch": 0.051080820597549224,
        "step": 371
    },
    {
        "loss": 2.4583,
        "grad_norm": 1.4647148847579956,
        "learning_rate": 0.0001899646446458321,
        "epoch": 0.05121850475010326,
        "step": 372
    },
    {
        "loss": 1.9606,
        "grad_norm": 1.9788545370101929,
        "learning_rate": 0.00018990771514046124,
        "epoch": 0.05135618890265731,
        "step": 373
    },
    {
        "loss": 2.3883,
        "grad_norm": 1.9375923871994019,
        "learning_rate": 0.0001898506331940175,
        "epoch": 0.051493873055211345,
        "step": 374
    },
    {
        "loss": 2.3753,
        "grad_norm": 1.3143243789672852,
        "learning_rate": 0.00018979339890328484,
        "epoch": 0.05163155720776538,
        "step": 375
    },
    {
        "loss": 2.2485,
        "grad_norm": 1.2076306343078613,
        "learning_rate": 0.00018973601236530567,
        "epoch": 0.05176924136031943,
        "step": 376
    },
    {
        "loss": 2.0064,
        "grad_norm": 1.5675824880599976,
        "learning_rate": 0.00018967847367738048,
        "epoch": 0.051906925512873466,
        "step": 377
    },
    {
        "loss": 2.5842,
        "grad_norm": 1.1048028469085693,
        "learning_rate": 0.00018962078293706773,
        "epoch": 0.05204460966542751,
        "step": 378
    },
    {
        "loss": 2.3935,
        "grad_norm": 1.7514827251434326,
        "learning_rate": 0.0001895629402421837,
        "epoch": 0.05218229381798155,
        "step": 379
    },
    {
        "loss": 1.9641,
        "grad_norm": 1.5919278860092163,
        "learning_rate": 0.00018950494569080234,
        "epoch": 0.05231997797053559,
        "step": 380
    },
    {
        "loss": 2.5181,
        "grad_norm": 2.7153477668762207,
        "learning_rate": 0.00018944679938125503,
        "epoch": 0.05245766212308963,
        "step": 381
    },
    {
        "loss": 1.8716,
        "grad_norm": 2.1908769607543945,
        "learning_rate": 0.00018938850141213048,
        "epoch": 0.052595346275643676,
        "step": 382
    },
    {
        "loss": 1.6493,
        "grad_norm": 1.6838871240615845,
        "learning_rate": 0.00018933005188227458,
        "epoch": 0.052733030428197714,
        "step": 383
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.452191948890686,
        "learning_rate": 0.00018927145089079012,
        "epoch": 0.05287071458075176,
        "step": 384
    },
    {
        "loss": 2.7681,
        "grad_norm": 1.261196255683899,
        "learning_rate": 0.0001892126985370367,
        "epoch": 0.053008398733305796,
        "step": 385
    },
    {
        "loss": 1.5885,
        "grad_norm": 2.474281072616577,
        "learning_rate": 0.0001891537949206307,
        "epoch": 0.053146082885859834,
        "step": 386
    },
    {
        "loss": 1.9887,
        "grad_norm": 2.420954465866089,
        "learning_rate": 0.00018909474014144477,
        "epoch": 0.05328376703841388,
        "step": 387
    },
    {
        "loss": 2.5871,
        "grad_norm": 1.051530122756958,
        "learning_rate": 0.00018903553429960802,
        "epoch": 0.05342145119096792,
        "step": 388
    },
    {
        "loss": 2.4768,
        "grad_norm": 1.298760175704956,
        "learning_rate": 0.0001889761774955056,
        "epoch": 0.05355913534352196,
        "step": 389
    },
    {
        "loss": 2.4684,
        "grad_norm": 1.5340498685836792,
        "learning_rate": 0.0001889166698297787,
        "epoch": 0.053696819496076,
        "step": 390
    },
    {
        "loss": 1.8761,
        "grad_norm": 1.5850077867507935,
        "learning_rate": 0.00018885701140332418,
        "epoch": 0.053834503648630044,
        "step": 391
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.9393936395645142,
        "learning_rate": 0.00018879720231729463,
        "epoch": 0.05397218780118408,
        "step": 392
    },
    {
        "loss": 2.2381,
        "grad_norm": 2.027597188949585,
        "learning_rate": 0.000188737242673098,
        "epoch": 0.05410987195373813,
        "step": 393
    },
    {
        "loss": 2.1573,
        "grad_norm": 1.7518072128295898,
        "learning_rate": 0.00018867713257239764,
        "epoch": 0.054247556106292165,
        "step": 394
    },
    {
        "loss": 2.3152,
        "grad_norm": 1.9516524076461792,
        "learning_rate": 0.00018861687211711186,
        "epoch": 0.05438524025884621,
        "step": 395
    },
    {
        "loss": 1.8226,
        "grad_norm": 1.3801934719085693,
        "learning_rate": 0.00018855646140941397,
        "epoch": 0.05452292441140025,
        "step": 396
    },
    {
        "loss": 2.5026,
        "grad_norm": 1.0909643173217773,
        "learning_rate": 0.00018849590055173204,
        "epoch": 0.054660608563954285,
        "step": 397
    },
    {
        "loss": 2.2284,
        "grad_norm": 1.4283334016799927,
        "learning_rate": 0.00018843518964674875,
        "epoch": 0.05479829271650833,
        "step": 398
    },
    {
        "loss": 1.9585,
        "grad_norm": 1.9375096559524536,
        "learning_rate": 0.00018837432879740114,
        "epoch": 0.05493597686906237,
        "step": 399
    },
    {
        "loss": 2.0655,
        "grad_norm": 1.4726165533065796,
        "learning_rate": 0.00018831331810688047,
        "epoch": 0.05507366102161641,
        "step": 400
    },
    {
        "loss": 1.7835,
        "grad_norm": 2.6892271041870117,
        "learning_rate": 0.00018825215767863214,
        "epoch": 0.05521134517417045,
        "step": 401
    },
    {
        "loss": 1.9192,
        "grad_norm": 1.1115683317184448,
        "learning_rate": 0.00018819084761635533,
        "epoch": 0.055349029326724496,
        "step": 402
    },
    {
        "loss": 2.8302,
        "grad_norm": 1.4237200021743774,
        "learning_rate": 0.00018812938802400303,
        "epoch": 0.055486713479278534,
        "step": 403
    },
    {
        "loss": 2.5134,
        "grad_norm": 2.0301942825317383,
        "learning_rate": 0.0001880677790057817,
        "epoch": 0.05562439763183258,
        "step": 404
    },
    {
        "loss": 2.395,
        "grad_norm": 1.4034883975982666,
        "learning_rate": 0.00018800602066615114,
        "epoch": 0.055762081784386616,
        "step": 405
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.4497288465499878,
        "learning_rate": 0.00018794411310982443,
        "epoch": 0.05589976593694066,
        "step": 406
    },
    {
        "loss": 1.7817,
        "grad_norm": 1.9956737756729126,
        "learning_rate": 0.00018788205644176752,
        "epoch": 0.0560374500894947,
        "step": 407
    },
    {
        "loss": 2.5185,
        "grad_norm": 2.070126533508301,
        "learning_rate": 0.00018781985076719927,
        "epoch": 0.05617513424204874,
        "step": 408
    },
    {
        "loss": 2.2646,
        "grad_norm": 1.222240924835205,
        "learning_rate": 0.00018775749619159116,
        "epoch": 0.05631281839460278,
        "step": 409
    },
    {
        "loss": 2.6601,
        "grad_norm": 1.1551237106323242,
        "learning_rate": 0.00018769499282066717,
        "epoch": 0.05645050254715682,
        "step": 410
    },
    {
        "loss": 2.2628,
        "grad_norm": 1.4087461233139038,
        "learning_rate": 0.00018763234076040345,
        "epoch": 0.056588186699710864,
        "step": 411
    },
    {
        "loss": 2.3515,
        "grad_norm": 1.9567337036132812,
        "learning_rate": 0.00018756954011702842,
        "epoch": 0.0567258708522649,
        "step": 412
    },
    {
        "loss": 2.6593,
        "grad_norm": 1.054904818534851,
        "learning_rate": 0.0001875065909970223,
        "epoch": 0.05686355500481895,
        "step": 413
    },
    {
        "loss": 2.357,
        "grad_norm": 1.9404540061950684,
        "learning_rate": 0.00018744349350711712,
        "epoch": 0.057001239157372985,
        "step": 414
    },
    {
        "loss": 2.4917,
        "grad_norm": 1.8374890089035034,
        "learning_rate": 0.00018738024775429645,
        "epoch": 0.05713892330992703,
        "step": 415
    },
    {
        "loss": 2.1914,
        "grad_norm": 1.7069493532180786,
        "learning_rate": 0.00018731685384579524,
        "epoch": 0.05727660746248107,
        "step": 416
    },
    {
        "loss": 2.4602,
        "grad_norm": 1.2641286849975586,
        "learning_rate": 0.00018725331188909966,
        "epoch": 0.05741429161503511,
        "step": 417
    },
    {
        "loss": 2.1571,
        "grad_norm": 1.409929871559143,
        "learning_rate": 0.00018718962199194695,
        "epoch": 0.05755197576758915,
        "step": 418
    },
    {
        "loss": 1.803,
        "grad_norm": 2.447557210922241,
        "learning_rate": 0.00018712578426232507,
        "epoch": 0.05768965992014319,
        "step": 419
    },
    {
        "loss": 2.4179,
        "grad_norm": 1.5824388265609741,
        "learning_rate": 0.0001870617988084727,
        "epoch": 0.05782734407269723,
        "step": 420
    },
    {
        "loss": 2.0418,
        "grad_norm": 2.058117389678955,
        "learning_rate": 0.000186997665738879,
        "epoch": 0.05796502822525127,
        "step": 421
    },
    {
        "loss": 2.1697,
        "grad_norm": 1.5659786462783813,
        "learning_rate": 0.00018693338516228338,
        "epoch": 0.058102712377805316,
        "step": 422
    },
    {
        "loss": 2.4927,
        "grad_norm": 1.1112830638885498,
        "learning_rate": 0.00018686895718767542,
        "epoch": 0.058240396530359353,
        "step": 423
    },
    {
        "loss": 1.6412,
        "grad_norm": 1.5361555814743042,
        "learning_rate": 0.00018680438192429454,
        "epoch": 0.0583780806829134,
        "step": 424
    },
    {
        "loss": 2.5995,
        "grad_norm": 1.2667560577392578,
        "learning_rate": 0.00018673965948162993,
        "epoch": 0.058515764835467436,
        "step": 425
    },
    {
        "loss": 2.7673,
        "grad_norm": 1.073992371559143,
        "learning_rate": 0.00018667478996942035,
        "epoch": 0.05865344898802148,
        "step": 426
    },
    {
        "loss": 2.3352,
        "grad_norm": 2.031893253326416,
        "learning_rate": 0.00018660977349765383,
        "epoch": 0.05879113314057552,
        "step": 427
    },
    {
        "loss": 2.4724,
        "grad_norm": 1.579978585243225,
        "learning_rate": 0.0001865446101765677,
        "epoch": 0.058928817293129564,
        "step": 428
    },
    {
        "loss": 2.2455,
        "grad_norm": 1.2940764427185059,
        "learning_rate": 0.00018647930011664814,
        "epoch": 0.0590665014456836,
        "step": 429
    },
    {
        "loss": 1.8829,
        "grad_norm": 2.0266106128692627,
        "learning_rate": 0.00018641384342863026,
        "epoch": 0.05920418559823764,
        "step": 430
    },
    {
        "loss": 1.8063,
        "grad_norm": 2.328390121459961,
        "learning_rate": 0.00018634824022349772,
        "epoch": 0.059341869750791684,
        "step": 431
    },
    {
        "loss": 1.8151,
        "grad_norm": 2.5957741737365723,
        "learning_rate": 0.00018628249061248262,
        "epoch": 0.05947955390334572,
        "step": 432
    },
    {
        "loss": 2.286,
        "grad_norm": 1.8959792852401733,
        "learning_rate": 0.00018621659470706528,
        "epoch": 0.05961723805589977,
        "step": 433
    },
    {
        "loss": 2.0756,
        "grad_norm": 1.130771517753601,
        "learning_rate": 0.00018615055261897402,
        "epoch": 0.059754922208453805,
        "step": 434
    },
    {
        "loss": 2.6826,
        "grad_norm": 1.1243023872375488,
        "learning_rate": 0.0001860843644601851,
        "epoch": 0.05989260636100785,
        "step": 435
    },
    {
        "loss": 2.1523,
        "grad_norm": 1.4099713563919067,
        "learning_rate": 0.00018601803034292246,
        "epoch": 0.06003029051356189,
        "step": 436
    },
    {
        "loss": 2.3529,
        "grad_norm": 0.9862663745880127,
        "learning_rate": 0.00018595155037965739,
        "epoch": 0.06016797466611593,
        "step": 437
    },
    {
        "loss": 2.3634,
        "grad_norm": 1.408966064453125,
        "learning_rate": 0.0001858849246831086,
        "epoch": 0.06030565881866997,
        "step": 438
    },
    {
        "loss": 2.0263,
        "grad_norm": 1.8555794954299927,
        "learning_rate": 0.00018581815336624185,
        "epoch": 0.060443342971224015,
        "step": 439
    },
    {
        "loss": 1.9302,
        "grad_norm": 1.8644872903823853,
        "learning_rate": 0.00018575123654226974,
        "epoch": 0.06058102712377805,
        "step": 440
    },
    {
        "loss": 2.0984,
        "grad_norm": 1.510414719581604,
        "learning_rate": 0.0001856841743246517,
        "epoch": 0.06071871127633209,
        "step": 441
    },
    {
        "loss": 2.5352,
        "grad_norm": 1.3570646047592163,
        "learning_rate": 0.00018561696682709358,
        "epoch": 0.060856395428886136,
        "step": 442
    },
    {
        "loss": 2.5325,
        "grad_norm": 1.444096565246582,
        "learning_rate": 0.0001855496141635476,
        "epoch": 0.06099407958144017,
        "step": 443
    },
    {
        "loss": 2.0037,
        "grad_norm": 1.993070125579834,
        "learning_rate": 0.00018548211644821207,
        "epoch": 0.06113176373399422,
        "step": 444
    },
    {
        "loss": 2.4755,
        "grad_norm": 1.4694702625274658,
        "learning_rate": 0.00018541447379553136,
        "epoch": 0.061269447886548256,
        "step": 445
    },
    {
        "loss": 1.8025,
        "grad_norm": 2.2939910888671875,
        "learning_rate": 0.00018534668632019544,
        "epoch": 0.0614071320391023,
        "step": 446
    },
    {
        "loss": 2.549,
        "grad_norm": 2.0763120651245117,
        "learning_rate": 0.0001852787541371399,
        "epoch": 0.06154481619165634,
        "step": 447
    },
    {
        "loss": 1.8866,
        "grad_norm": 2.073197841644287,
        "learning_rate": 0.00018521067736154568,
        "epoch": 0.061682500344210384,
        "step": 448
    },
    {
        "loss": 2.068,
        "grad_norm": 2.006342649459839,
        "learning_rate": 0.00018514245610883886,
        "epoch": 0.06182018449676442,
        "step": 449
    },
    {
        "loss": 1.8509,
        "grad_norm": 1.5443916320800781,
        "learning_rate": 0.00018507409049469051,
        "epoch": 0.061957868649318466,
        "step": 450
    },
    {
        "loss": 2.5849,
        "grad_norm": 0.882473349571228,
        "learning_rate": 0.0001850055806350165,
        "epoch": 0.062095552801872504,
        "step": 451
    },
    {
        "loss": 2.6208,
        "grad_norm": 1.4673314094543457,
        "learning_rate": 0.0001849369266459772,
        "epoch": 0.06223323695442654,
        "step": 452
    },
    {
        "loss": 2.756,
        "grad_norm": 1.5369971990585327,
        "learning_rate": 0.00018486812864397737,
        "epoch": 0.06237092110698059,
        "step": 453
    },
    {
        "loss": 2.163,
        "grad_norm": 1.6676108837127686,
        "learning_rate": 0.000184799186745666,
        "epoch": 0.06250860525953462,
        "step": 454
    },
    {
        "loss": 2.1434,
        "grad_norm": 1.558098316192627,
        "learning_rate": 0.00018473010106793603,
        "epoch": 0.06264628941208866,
        "step": 455
    },
    {
        "loss": 2.3415,
        "grad_norm": 0.9134262800216675,
        "learning_rate": 0.00018466087172792418,
        "epoch": 0.06278397356464271,
        "step": 456
    },
    {
        "loss": 2.4224,
        "grad_norm": 1.1628340482711792,
        "learning_rate": 0.00018459149884301072,
        "epoch": 0.06292165771719675,
        "step": 457
    },
    {
        "loss": 2.2843,
        "grad_norm": 1.5234606266021729,
        "learning_rate": 0.00018452198253081936,
        "epoch": 0.06305934186975079,
        "step": 458
    },
    {
        "loss": 2.3054,
        "grad_norm": 1.640828251838684,
        "learning_rate": 0.00018445232290921699,
        "epoch": 0.06319702602230483,
        "step": 459
    },
    {
        "loss": 2.3352,
        "grad_norm": 1.0899207592010498,
        "learning_rate": 0.00018438252009631343,
        "epoch": 0.06333471017485888,
        "step": 460
    },
    {
        "loss": 1.8869,
        "grad_norm": 1.8093211650848389,
        "learning_rate": 0.00018431257421046136,
        "epoch": 0.06347239432741292,
        "step": 461
    },
    {
        "loss": 1.9812,
        "grad_norm": 2.2926571369171143,
        "learning_rate": 0.00018424248537025597,
        "epoch": 0.06361007847996696,
        "step": 462
    },
    {
        "loss": 2.2038,
        "grad_norm": 1.6954820156097412,
        "learning_rate": 0.00018417225369453491,
        "epoch": 0.063747762632521,
        "step": 463
    },
    {
        "loss": 2.6242,
        "grad_norm": 1.5334821939468384,
        "learning_rate": 0.00018410187930237792,
        "epoch": 0.06388544678507503,
        "step": 464
    },
    {
        "loss": 1.2786,
        "grad_norm": 2.594355583190918,
        "learning_rate": 0.00018403136231310684,
        "epoch": 0.06402313093762908,
        "step": 465
    },
    {
        "loss": 2.4795,
        "grad_norm": 1.4523720741271973,
        "learning_rate": 0.00018396070284628518,
        "epoch": 0.06416081509018312,
        "step": 466
    },
    {
        "loss": 2.2897,
        "grad_norm": 1.51255202293396,
        "learning_rate": 0.00018388990102171805,
        "epoch": 0.06429849924273716,
        "step": 467
    },
    {
        "loss": 2.2687,
        "grad_norm": 1.2467191219329834,
        "learning_rate": 0.00018381895695945198,
        "epoch": 0.0644361833952912,
        "step": 468
    },
    {
        "loss": 2.3711,
        "grad_norm": 1.880769968032837,
        "learning_rate": 0.0001837478707797746,
        "epoch": 0.06457386754784525,
        "step": 469
    },
    {
        "loss": 2.7192,
        "grad_norm": 1.2878376245498657,
        "learning_rate": 0.00018367664260321456,
        "epoch": 0.06471155170039929,
        "step": 470
    },
    {
        "loss": 2.5011,
        "grad_norm": 1.324715256690979,
        "learning_rate": 0.00018360527255054123,
        "epoch": 0.06484923585295332,
        "step": 471
    },
    {
        "loss": 2.401,
        "grad_norm": 1.1866471767425537,
        "learning_rate": 0.00018353376074276457,
        "epoch": 0.06498692000550736,
        "step": 472
    },
    {
        "loss": 2.2486,
        "grad_norm": 1.3184115886688232,
        "learning_rate": 0.00018346210730113484,
        "epoch": 0.06512460415806141,
        "step": 473
    },
    {
        "loss": 2.2463,
        "grad_norm": 1.4826643466949463,
        "learning_rate": 0.00018339031234714255,
        "epoch": 0.06526228831061545,
        "step": 474
    },
    {
        "loss": 2.432,
        "grad_norm": 1.620948314666748,
        "learning_rate": 0.000183318376002518,
        "epoch": 0.06539997246316949,
        "step": 475
    },
    {
        "loss": 2.7768,
        "grad_norm": 1.5080605745315552,
        "learning_rate": 0.00018324629838923132,
        "epoch": 0.06553765661572353,
        "step": 476
    },
    {
        "loss": 2.1785,
        "grad_norm": 1.4553030729293823,
        "learning_rate": 0.0001831740796294921,
        "epoch": 0.06567534076827757,
        "step": 477
    },
    {
        "loss": 2.1453,
        "grad_norm": 1.8206714391708374,
        "learning_rate": 0.00018310171984574938,
        "epoch": 0.06581302492083162,
        "step": 478
    },
    {
        "loss": 1.8997,
        "grad_norm": 2.2348124980926514,
        "learning_rate": 0.00018302921916069117,
        "epoch": 0.06595070907338565,
        "step": 479
    },
    {
        "loss": 1.9973,
        "grad_norm": 1.6622390747070312,
        "learning_rate": 0.0001829565776972444,
        "epoch": 0.06608839322593969,
        "step": 480
    },
    {
        "loss": 2.5355,
        "grad_norm": 1.1026939153671265,
        "learning_rate": 0.00018288379557857478,
        "epoch": 0.06622607737849373,
        "step": 481
    },
    {
        "loss": 2.1562,
        "grad_norm": 1.222834587097168,
        "learning_rate": 0.00018281087292808644,
        "epoch": 0.06636376153104778,
        "step": 482
    },
    {
        "loss": 2.5712,
        "grad_norm": 1.201985478401184,
        "learning_rate": 0.00018273780986942172,
        "epoch": 0.06650144568360182,
        "step": 483
    },
    {
        "loss": 1.0402,
        "grad_norm": 2.4489052295684814,
        "learning_rate": 0.00018266460652646121,
        "epoch": 0.06663912983615586,
        "step": 484
    },
    {
        "loss": 1.9836,
        "grad_norm": 1.6171159744262695,
        "learning_rate": 0.00018259126302332321,
        "epoch": 0.0667768139887099,
        "step": 485
    },
    {
        "loss": 2.4417,
        "grad_norm": 1.5683152675628662,
        "learning_rate": 0.00018251777948436368,
        "epoch": 0.06691449814126393,
        "step": 486
    },
    {
        "loss": 2.619,
        "grad_norm": 0.9652795195579529,
        "learning_rate": 0.00018244415603417603,
        "epoch": 0.06705218229381799,
        "step": 487
    },
    {
        "loss": 2.0516,
        "grad_norm": 1.4903923273086548,
        "learning_rate": 0.00018237039279759095,
        "epoch": 0.06718986644637202,
        "step": 488
    },
    {
        "loss": 1.9227,
        "grad_norm": 2.115403652191162,
        "learning_rate": 0.00018229648989967606,
        "epoch": 0.06732755059892606,
        "step": 489
    },
    {
        "loss": 2.6867,
        "grad_norm": 1.2194786071777344,
        "learning_rate": 0.00018222244746573583,
        "epoch": 0.0674652347514801,
        "step": 490
    },
    {
        "loss": 2.1291,
        "grad_norm": 1.041663408279419,
        "learning_rate": 0.00018214826562131127,
        "epoch": 0.06760291890403415,
        "step": 491
    },
    {
        "loss": 2.5934,
        "grad_norm": 1.0775161981582642,
        "learning_rate": 0.0001820739444921798,
        "epoch": 0.06774060305658819,
        "step": 492
    },
    {
        "loss": 1.589,
        "grad_norm": 2.679225444793701,
        "learning_rate": 0.00018199948420435504,
        "epoch": 0.06787828720914223,
        "step": 493
    },
    {
        "loss": 2.2159,
        "grad_norm": 1.5122196674346924,
        "learning_rate": 0.00018192488488408647,
        "epoch": 0.06801597136169626,
        "step": 494
    },
    {
        "loss": 2.2877,
        "grad_norm": 1.3444104194641113,
        "learning_rate": 0.00018185014665785936,
        "epoch": 0.06815365551425032,
        "step": 495
    },
    {
        "loss": 2.7345,
        "grad_norm": 1.1083608865737915,
        "learning_rate": 0.00018177526965239448,
        "epoch": 0.06829133966680435,
        "step": 496
    },
    {
        "loss": 2.0493,
        "grad_norm": 1.8184458017349243,
        "learning_rate": 0.00018170025399464793,
        "epoch": 0.06842902381935839,
        "step": 497
    },
    {
        "loss": 1.5811,
        "grad_norm": 1.9515008926391602,
        "learning_rate": 0.00018162509981181084,
        "epoch": 0.06856670797191243,
        "step": 498
    },
    {
        "loss": 2.4378,
        "grad_norm": 1.1947741508483887,
        "learning_rate": 0.00018154980723130928,
        "epoch": 0.06870439212446647,
        "step": 499
    },
    {
        "loss": 2.6518,
        "grad_norm": 1.3687148094177246,
        "learning_rate": 0.00018147437638080396,
        "epoch": 0.06884207627702052,
        "step": 500
    },
    {
        "loss": 1.9232,
        "grad_norm": 2.3272616863250732,
        "learning_rate": 0.00018139880738818998,
        "epoch": 0.06897976042957456,
        "step": 501
    },
    {
        "loss": 2.2733,
        "grad_norm": 1.5490297079086304,
        "learning_rate": 0.00018132310038159675,
        "epoch": 0.0691174445821286,
        "step": 502
    },
    {
        "loss": 2.7207,
        "grad_norm": 1.41384756565094,
        "learning_rate": 0.0001812472554893876,
        "epoch": 0.06925512873468263,
        "step": 503
    },
    {
        "loss": 2.6106,
        "grad_norm": 1.6112102270126343,
        "learning_rate": 0.00018117127284015972,
        "epoch": 0.06939281288723669,
        "step": 504
    },
    {
        "loss": 2.3728,
        "grad_norm": 1.261087417602539,
        "learning_rate": 0.0001810951525627438,
        "epoch": 0.06953049703979072,
        "step": 505
    },
    {
        "loss": 2.2145,
        "grad_norm": 1.535266399383545,
        "learning_rate": 0.00018101889478620397,
        "epoch": 0.06966818119234476,
        "step": 506
    },
    {
        "loss": 2.7282,
        "grad_norm": 2.1445670127868652,
        "learning_rate": 0.00018094249963983741,
        "epoch": 0.0698058653448988,
        "step": 507
    },
    {
        "loss": 2.2719,
        "grad_norm": 1.650779128074646,
        "learning_rate": 0.00018086596725317428,
        "epoch": 0.06994354949745284,
        "step": 508
    },
    {
        "loss": 2.4478,
        "grad_norm": 1.374786615371704,
        "learning_rate": 0.0001807892977559774,
        "epoch": 0.07008123365000689,
        "step": 509
    },
    {
        "loss": 2.6697,
        "grad_norm": 1.548545002937317,
        "learning_rate": 0.00018071249127824205,
        "epoch": 0.07021891780256093,
        "step": 510
    },
    {
        "loss": 2.5105,
        "grad_norm": 0.9977691769599915,
        "learning_rate": 0.00018063554795019578,
        "epoch": 0.07035660195511496,
        "step": 511
    },
    {
        "loss": 2.1627,
        "grad_norm": 2.0277316570281982,
        "learning_rate": 0.0001805584679022982,
        "epoch": 0.070494286107669,
        "step": 512
    },
    {
        "loss": 2.4552,
        "grad_norm": 2.0603816509246826,
        "learning_rate": 0.0001804812512652407,
        "epoch": 0.07063197026022305,
        "step": 513
    },
    {
        "loss": 2.1558,
        "grad_norm": 1.8651782274246216,
        "learning_rate": 0.00018040389816994628,
        "epoch": 0.07076965441277709,
        "step": 514
    },
    {
        "loss": 2.8638,
        "grad_norm": 1.77626633644104,
        "learning_rate": 0.00018032640874756931,
        "epoch": 0.07090733856533113,
        "step": 515
    },
    {
        "loss": 1.6364,
        "grad_norm": 2.5145797729492188,
        "learning_rate": 0.00018024878312949527,
        "epoch": 0.07104502271788517,
        "step": 516
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.5915998220443726,
        "learning_rate": 0.00018017102144734061,
        "epoch": 0.07118270687043922,
        "step": 517
    },
    {
        "loss": 2.4821,
        "grad_norm": 1.8918075561523438,
        "learning_rate": 0.00018009312383295245,
        "epoch": 0.07132039102299326,
        "step": 518
    },
    {
        "loss": 2.3443,
        "grad_norm": 1.8383244276046753,
        "learning_rate": 0.0001800150904184084,
        "epoch": 0.0714580751755473,
        "step": 519
    },
    {
        "loss": 2.6421,
        "grad_norm": 1.1039574146270752,
        "learning_rate": 0.0001799369213360163,
        "epoch": 0.07159575932810133,
        "step": 520
    },
    {
        "loss": 2.0646,
        "grad_norm": 2.1262733936309814,
        "learning_rate": 0.00017985861671831406,
        "epoch": 0.07173344348065537,
        "step": 521
    },
    {
        "loss": 2.3882,
        "grad_norm": 1.5054246187210083,
        "learning_rate": 0.0001797801766980694,
        "epoch": 0.07187112763320942,
        "step": 522
    },
    {
        "loss": 2.5709,
        "grad_norm": 1.0588114261627197,
        "learning_rate": 0.00017970160140827953,
        "epoch": 0.07200881178576346,
        "step": 523
    },
    {
        "loss": 2.2643,
        "grad_norm": 1.9008625745773315,
        "learning_rate": 0.00017962289098217115,
        "epoch": 0.0721464959383175,
        "step": 524
    },
    {
        "loss": 2.4281,
        "grad_norm": 1.0369223356246948,
        "learning_rate": 0.00017954404555319995,
        "epoch": 0.07228418009087154,
        "step": 525
    },
    {
        "loss": 2.4709,
        "grad_norm": 1.6680219173431396,
        "learning_rate": 0.0001794650652550506,
        "epoch": 0.07242186424342559,
        "step": 526
    },
    {
        "loss": 2.3645,
        "grad_norm": 2.439809799194336,
        "learning_rate": 0.00017938595022163643,
        "epoch": 0.07255954839597963,
        "step": 527
    },
    {
        "loss": 2.2564,
        "grad_norm": 1.4414482116699219,
        "learning_rate": 0.00017930670058709925,
        "epoch": 0.07269723254853366,
        "step": 528
    },
    {
        "loss": 2.44,
        "grad_norm": 1.462852954864502,
        "learning_rate": 0.000179227316485809,
        "epoch": 0.0728349167010877,
        "step": 529
    },
    {
        "loss": 2.7349,
        "grad_norm": 1.942459225654602,
        "learning_rate": 0.0001791477980523637,
        "epoch": 0.07297260085364174,
        "step": 530
    },
    {
        "loss": 2.7485,
        "grad_norm": 1.1720376014709473,
        "learning_rate": 0.0001790681454215891,
        "epoch": 0.07311028500619579,
        "step": 531
    },
    {
        "loss": 2.0582,
        "grad_norm": 1.3359217643737793,
        "learning_rate": 0.0001789883587285385,
        "epoch": 0.07324796915874983,
        "step": 532
    },
    {
        "loss": 1.9333,
        "grad_norm": 2.238654375076294,
        "learning_rate": 0.00017890843810849247,
        "epoch": 0.07338565331130387,
        "step": 533
    },
    {
        "loss": 2.6616,
        "grad_norm": 1.8598488569259644,
        "learning_rate": 0.0001788283836969587,
        "epoch": 0.0735233374638579,
        "step": 534
    },
    {
        "loss": 2.3521,
        "grad_norm": 1.6410778760910034,
        "learning_rate": 0.00017874819562967167,
        "epoch": 0.07366102161641196,
        "step": 535
    },
    {
        "loss": 2.3449,
        "grad_norm": 1.833436131477356,
        "learning_rate": 0.00017866787404259256,
        "epoch": 0.073798705768966,
        "step": 536
    },
    {
        "loss": 2.527,
        "grad_norm": 0.82371985912323,
        "learning_rate": 0.0001785874190719089,
        "epoch": 0.07393638992152003,
        "step": 537
    },
    {
        "loss": 1.7524,
        "grad_norm": 1.423355221748352,
        "learning_rate": 0.00017850683085403427,
        "epoch": 0.07407407407407407,
        "step": 538
    },
    {
        "loss": 2.22,
        "grad_norm": 1.996380090713501,
        "learning_rate": 0.0001784261095256084,
        "epoch": 0.07421175822662812,
        "step": 539
    },
    {
        "loss": 1.6346,
        "grad_norm": 2.4281229972839355,
        "learning_rate": 0.0001783452552234965,
        "epoch": 0.07434944237918216,
        "step": 540
    },
    {
        "loss": 2.1165,
        "grad_norm": 2.4151270389556885,
        "learning_rate": 0.00017826426808478937,
        "epoch": 0.0744871265317362,
        "step": 541
    },
    {
        "loss": 2.2693,
        "grad_norm": 2.0509121417999268,
        "learning_rate": 0.000178183148246803,
        "epoch": 0.07462481068429024,
        "step": 542
    },
    {
        "loss": 1.9706,
        "grad_norm": 1.813740849494934,
        "learning_rate": 0.00017810189584707833,
        "epoch": 0.07476249483684427,
        "step": 543
    },
    {
        "loss": 2.3633,
        "grad_norm": 1.424372673034668,
        "learning_rate": 0.0001780205110233811,
        "epoch": 0.07490017898939832,
        "step": 544
    },
    {
        "loss": 2.6692,
        "grad_norm": 1.2840512990951538,
        "learning_rate": 0.0001779389939137016,
        "epoch": 0.07503786314195236,
        "step": 545
    },
    {
        "loss": 2.0178,
        "grad_norm": 1.457922339439392,
        "learning_rate": 0.0001778573446562544,
        "epoch": 0.0751755472945064,
        "step": 546
    },
    {
        "loss": 2.6889,
        "grad_norm": 1.1510025262832642,
        "learning_rate": 0.0001777755633894781,
        "epoch": 0.07531323144706044,
        "step": 547
    },
    {
        "loss": 2.8224,
        "grad_norm": 1.341896891593933,
        "learning_rate": 0.00017769365025203517,
        "epoch": 0.07545091559961449,
        "step": 548
    },
    {
        "loss": 2.0927,
        "grad_norm": 1.4162864685058594,
        "learning_rate": 0.00017761160538281163,
        "epoch": 0.07558859975216853,
        "step": 549
    },
    {
        "loss": 2.4135,
        "grad_norm": 1.2383756637573242,
        "learning_rate": 0.00017752942892091693,
        "epoch": 0.07572628390472257,
        "step": 550
    },
    {
        "loss": 2.5696,
        "grad_norm": 1.6210920810699463,
        "learning_rate": 0.00017744712100568354,
        "epoch": 0.0758639680572766,
        "step": 551
    },
    {
        "loss": 1.5323,
        "grad_norm": 1.8287004232406616,
        "learning_rate": 0.0001773646817766668,
        "epoch": 0.07600165220983064,
        "step": 552
    },
    {
        "loss": 2.182,
        "grad_norm": 1.6802375316619873,
        "learning_rate": 0.00017728211137364489,
        "epoch": 0.0761393363623847,
        "step": 553
    },
    {
        "loss": 2.0021,
        "grad_norm": 1.2242064476013184,
        "learning_rate": 0.00017719940993661816,
        "epoch": 0.07627702051493873,
        "step": 554
    },
    {
        "loss": 1.5968,
        "grad_norm": 1.4887795448303223,
        "learning_rate": 0.00017711657760580927,
        "epoch": 0.07641470466749277,
        "step": 555
    },
    {
        "loss": 2.4769,
        "grad_norm": 1.2889442443847656,
        "learning_rate": 0.00017703361452166275,
        "epoch": 0.07655238882004681,
        "step": 556
    },
    {
        "loss": 2.1103,
        "grad_norm": 2.742982864379883,
        "learning_rate": 0.0001769505208248449,
        "epoch": 0.07669007297260086,
        "step": 557
    },
    {
        "loss": 2.3293,
        "grad_norm": 1.0640931129455566,
        "learning_rate": 0.00017686729665624342,
        "epoch": 0.0768277571251549,
        "step": 558
    },
    {
        "loss": 2.0846,
        "grad_norm": 2.3828976154327393,
        "learning_rate": 0.0001767839421569672,
        "epoch": 0.07696544127770893,
        "step": 559
    },
    {
        "loss": 1.7907,
        "grad_norm": 1.5216618776321411,
        "learning_rate": 0.00017670045746834618,
        "epoch": 0.07710312543026297,
        "step": 560
    },
    {
        "loss": 2.0239,
        "grad_norm": 1.3587086200714111,
        "learning_rate": 0.00017661684273193105,
        "epoch": 0.07724080958281702,
        "step": 561
    },
    {
        "loss": 2.382,
        "grad_norm": 1.2765605449676514,
        "learning_rate": 0.00017653309808949287,
        "epoch": 0.07737849373537106,
        "step": 562
    },
    {
        "loss": 2.4652,
        "grad_norm": 1.3684769868850708,
        "learning_rate": 0.00017644922368302313,
        "epoch": 0.0775161778879251,
        "step": 563
    },
    {
        "loss": 2.0795,
        "grad_norm": 2.3037941455841064,
        "learning_rate": 0.00017636521965473323,
        "epoch": 0.07765386204047914,
        "step": 564
    },
    {
        "loss": 2.4311,
        "grad_norm": 1.3645259141921997,
        "learning_rate": 0.00017628108614705434,
        "epoch": 0.07779154619303318,
        "step": 565
    },
    {
        "loss": 2.3967,
        "grad_norm": 1.0624786615371704,
        "learning_rate": 0.00017619682330263724,
        "epoch": 0.07792923034558723,
        "step": 566
    },
    {
        "loss": 2.3562,
        "grad_norm": 1.699142575263977,
        "learning_rate": 0.00017611243126435197,
        "epoch": 0.07806691449814127,
        "step": 567
    },
    {
        "loss": 1.3318,
        "grad_norm": 2.2813708782196045,
        "learning_rate": 0.0001760279101752876,
        "epoch": 0.0782045986506953,
        "step": 568
    },
    {
        "loss": 2.4597,
        "grad_norm": 2.1488661766052246,
        "learning_rate": 0.00017594326017875202,
        "epoch": 0.07834228280324934,
        "step": 569
    },
    {
        "loss": 2.1347,
        "grad_norm": 2.036353349685669,
        "learning_rate": 0.00017585848141827174,
        "epoch": 0.07847996695580339,
        "step": 570
    },
    {
        "loss": 1.8384,
        "grad_norm": 2.203246593475342,
        "learning_rate": 0.00017577357403759147,
        "epoch": 0.07861765110835743,
        "step": 571
    },
    {
        "loss": 1.9025,
        "grad_norm": 1.9659711122512817,
        "learning_rate": 0.00017568853818067411,
        "epoch": 0.07875533526091147,
        "step": 572
    },
    {
        "loss": 2.786,
        "grad_norm": 1.2404521703720093,
        "learning_rate": 0.00017560337399170038,
        "epoch": 0.0788930194134655,
        "step": 573
    },
    {
        "loss": 2.2501,
        "grad_norm": 2.0218145847320557,
        "learning_rate": 0.00017551808161506854,
        "epoch": 0.07903070356601954,
        "step": 574
    },
    {
        "loss": 2.3982,
        "grad_norm": 1.1182085275650024,
        "learning_rate": 0.00017543266119539422,
        "epoch": 0.0791683877185736,
        "step": 575
    },
    {
        "loss": 2.4945,
        "grad_norm": 1.9636777639389038,
        "learning_rate": 0.0001753471128775102,
        "epoch": 0.07930607187112763,
        "step": 576
    },
    {
        "loss": 2.5156,
        "grad_norm": 1.387608289718628,
        "learning_rate": 0.00017526143680646604,
        "epoch": 0.07944375602368167,
        "step": 577
    },
    {
        "loss": 2.1819,
        "grad_norm": 1.5598160028457642,
        "learning_rate": 0.00017517563312752796,
        "epoch": 0.07958144017623571,
        "step": 578
    },
    {
        "loss": 2.1768,
        "grad_norm": 2.3861653804779053,
        "learning_rate": 0.00017508970198617849,
        "epoch": 0.07971912432878976,
        "step": 579
    },
    {
        "loss": 2.5381,
        "grad_norm": 1.623689889907837,
        "learning_rate": 0.00017500364352811637,
        "epoch": 0.0798568084813438,
        "step": 580
    },
    {
        "loss": 2.5478,
        "grad_norm": 1.4579930305480957,
        "learning_rate": 0.00017491745789925608,
        "epoch": 0.07999449263389784,
        "step": 581
    },
    {
        "loss": 2.1826,
        "grad_norm": 1.095165491104126,
        "learning_rate": 0.0001748311452457278,
        "epoch": 0.08013217678645188,
        "step": 582
    },
    {
        "loss": 1.654,
        "grad_norm": 2.5168557167053223,
        "learning_rate": 0.0001747447057138771,
        "epoch": 0.08026986093900593,
        "step": 583
    },
    {
        "loss": 2.3392,
        "grad_norm": 2.9184043407440186,
        "learning_rate": 0.00017465813945026466,
        "epoch": 0.08040754509155996,
        "step": 584
    },
    {
        "loss": 2.6145,
        "grad_norm": 1.1807023286819458,
        "learning_rate": 0.000174571446601666,
        "epoch": 0.080545229244114,
        "step": 585
    },
    {
        "loss": 2.2734,
        "grad_norm": 1.0980051755905151,
        "learning_rate": 0.0001744846273150713,
        "epoch": 0.08068291339666804,
        "step": 586
    },
    {
        "loss": 2.274,
        "grad_norm": 1.664448618888855,
        "learning_rate": 0.00017439768173768515,
        "epoch": 0.08082059754922208,
        "step": 587
    },
    {
        "loss": 2.5663,
        "grad_norm": 1.4644893407821655,
        "learning_rate": 0.0001743106100169262,
        "epoch": 0.08095828170177613,
        "step": 588
    },
    {
        "loss": 2.5625,
        "grad_norm": 1.957682490348816,
        "learning_rate": 0.000174223412300427,
        "epoch": 0.08109596585433017,
        "step": 589
    },
    {
        "loss": 2.4687,
        "grad_norm": 1.1335498094558716,
        "learning_rate": 0.00017413608873603377,
        "epoch": 0.0812336500068842,
        "step": 590
    },
    {
        "loss": 2.3738,
        "grad_norm": 1.4427601099014282,
        "learning_rate": 0.0001740486394718061,
        "epoch": 0.08137133415943824,
        "step": 591
    },
    {
        "loss": 2.722,
        "grad_norm": 1.222598671913147,
        "learning_rate": 0.00017396106465601663,
        "epoch": 0.0815090183119923,
        "step": 592
    },
    {
        "loss": 1.8543,
        "grad_norm": 2.266955614089966,
        "learning_rate": 0.00017387336443715098,
        "epoch": 0.08164670246454633,
        "step": 593
    },
    {
        "loss": 1.9845,
        "grad_norm": 1.8138411045074463,
        "learning_rate": 0.00017378553896390736,
        "epoch": 0.08178438661710037,
        "step": 594
    },
    {
        "loss": 2.0302,
        "grad_norm": 1.7543359994888306,
        "learning_rate": 0.00017369758838519633,
        "epoch": 0.08192207076965441,
        "step": 595
    },
    {
        "loss": 1.9363,
        "grad_norm": 2.3025999069213867,
        "learning_rate": 0.00017360951285014055,
        "epoch": 0.08205975492220845,
        "step": 596
    },
    {
        "loss": 1.284,
        "grad_norm": 2.537179946899414,
        "learning_rate": 0.00017352131250807467,
        "epoch": 0.0821974390747625,
        "step": 597
    },
    {
        "loss": 2.262,
        "grad_norm": 1.5213594436645508,
        "learning_rate": 0.00017343298750854472,
        "epoch": 0.08233512322731654,
        "step": 598
    },
    {
        "loss": 1.9489,
        "grad_norm": 1.67516028881073,
        "learning_rate": 0.00017334453800130838,
        "epoch": 0.08247280737987057,
        "step": 599
    },
    {
        "loss": 2.6525,
        "grad_norm": 1.2111413478851318,
        "learning_rate": 0.00017325596413633417,
        "epoch": 0.08261049153242461,
        "step": 600
    },
    {
        "loss": 1.3929,
        "grad_norm": 1.9951505661010742,
        "learning_rate": 0.0001731672660638017,
        "epoch": 0.08274817568497866,
        "step": 601
    },
    {
        "loss": 2.2627,
        "grad_norm": 0.9811823964118958,
        "learning_rate": 0.00017307844393410097,
        "epoch": 0.0828858598375327,
        "step": 602
    },
    {
        "loss": 2.0869,
        "grad_norm": 1.94391667842865,
        "learning_rate": 0.00017298949789783244,
        "epoch": 0.08302354399008674,
        "step": 603
    },
    {
        "loss": 2.5343,
        "grad_norm": 1.9286754131317139,
        "learning_rate": 0.0001729004281058066,
        "epoch": 0.08316122814264078,
        "step": 604
    },
    {
        "loss": 2.1127,
        "grad_norm": 1.4687014818191528,
        "learning_rate": 0.0001728112347090439,
        "epoch": 0.08329891229519483,
        "step": 605
    },
    {
        "loss": 1.9813,
        "grad_norm": 1.4409399032592773,
        "learning_rate": 0.00017272191785877415,
        "epoch": 0.08343659644774887,
        "step": 606
    },
    {
        "loss": 2.2657,
        "grad_norm": 1.2861758470535278,
        "learning_rate": 0.00017263247770643662,
        "epoch": 0.0835742806003029,
        "step": 607
    },
    {
        "loss": 2.6933,
        "grad_norm": 1.5186545848846436,
        "learning_rate": 0.00017254291440367968,
        "epoch": 0.08371196475285694,
        "step": 608
    },
    {
        "loss": 2.4914,
        "grad_norm": 1.1656770706176758,
        "learning_rate": 0.0001724532281023604,
        "epoch": 0.08384964890541098,
        "step": 609
    },
    {
        "loss": 2.5637,
        "grad_norm": 1.855706810951233,
        "learning_rate": 0.00017236341895454443,
        "epoch": 0.08398733305796503,
        "step": 610
    },
    {
        "loss": 1.5464,
        "grad_norm": 2.1879498958587646,
        "learning_rate": 0.0001722734871125057,
        "epoch": 0.08412501721051907,
        "step": 611
    },
    {
        "loss": 2.5248,
        "grad_norm": 1.9429941177368164,
        "learning_rate": 0.00017218343272872627,
        "epoch": 0.08426270136307311,
        "step": 612
    },
    {
        "loss": 2.0975,
        "grad_norm": 1.8815723657608032,
        "learning_rate": 0.0001720932559558958,
        "epoch": 0.08440038551562715,
        "step": 613
    },
    {
        "loss": 1.7151,
        "grad_norm": 2.0836055278778076,
        "learning_rate": 0.00017200295694691162,
        "epoch": 0.0845380696681812,
        "step": 614
    },
    {
        "loss": 2.5052,
        "grad_norm": 1.0791267156600952,
        "learning_rate": 0.00017191253585487824,
        "epoch": 0.08467575382073524,
        "step": 615
    },
    {
        "loss": 2.8074,
        "grad_norm": 1.4635754823684692,
        "learning_rate": 0.00017182199283310716,
        "epoch": 0.08481343797328927,
        "step": 616
    },
    {
        "loss": 2.5185,
        "grad_norm": 2.07000994682312,
        "learning_rate": 0.00017173132803511665,
        "epoch": 0.08495112212584331,
        "step": 617
    },
    {
        "loss": 1.9309,
        "grad_norm": 1.4496424198150635,
        "learning_rate": 0.00017164054161463143,
        "epoch": 0.08508880627839735,
        "step": 618
    },
    {
        "loss": 2.1398,
        "grad_norm": 1.248645305633545,
        "learning_rate": 0.00017154963372558246,
        "epoch": 0.0852264904309514,
        "step": 619
    },
    {
        "loss": 2.4815,
        "grad_norm": 1.2575761079788208,
        "learning_rate": 0.0001714586045221066,
        "epoch": 0.08536417458350544,
        "step": 620
    },
    {
        "loss": 2.6961,
        "grad_norm": 1.1258416175842285,
        "learning_rate": 0.00017136745415854648,
        "epoch": 0.08550185873605948,
        "step": 621
    },
    {
        "loss": 2.2803,
        "grad_norm": 1.2679328918457031,
        "learning_rate": 0.00017127618278945007,
        "epoch": 0.08563954288861352,
        "step": 622
    },
    {
        "loss": 2.071,
        "grad_norm": 1.8533154726028442,
        "learning_rate": 0.00017118479056957062,
        "epoch": 0.08577722704116757,
        "step": 623
    },
    {
        "loss": 1.5937,
        "grad_norm": 2.6543307304382324,
        "learning_rate": 0.0001710932776538662,
        "epoch": 0.0859149111937216,
        "step": 624
    },
    {
        "loss": 2.4077,
        "grad_norm": 1.445115089416504,
        "learning_rate": 0.0001710016441974995,
        "epoch": 0.08605259534627564,
        "step": 625
    },
    {
        "loss": 2.2042,
        "grad_norm": 1.5250331163406372,
        "learning_rate": 0.00017090989035583765,
        "epoch": 0.08619027949882968,
        "step": 626
    },
    {
        "loss": 1.8139,
        "grad_norm": 1.3863704204559326,
        "learning_rate": 0.00017081801628445197,
        "epoch": 0.08632796365138373,
        "step": 627
    },
    {
        "loss": 1.9916,
        "grad_norm": 1.5881388187408447,
        "learning_rate": 0.00017072602213911742,
        "epoch": 0.08646564780393777,
        "step": 628
    },
    {
        "loss": 2.2858,
        "grad_norm": 1.4260865449905396,
        "learning_rate": 0.00017063390807581276,
        "epoch": 0.08660333195649181,
        "step": 629
    },
    {
        "loss": 2.3227,
        "grad_norm": 1.56675386428833,
        "learning_rate": 0.00017054167425071995,
        "epoch": 0.08674101610904585,
        "step": 630
    },
    {
        "loss": 1.2687,
        "grad_norm": 1.661673665046692,
        "learning_rate": 0.00017044932082022407,
        "epoch": 0.08687870026159988,
        "step": 631
    },
    {
        "loss": 2.4673,
        "grad_norm": 1.5206549167633057,
        "learning_rate": 0.00017035684794091297,
        "epoch": 0.08701638441415394,
        "step": 632
    },
    {
        "loss": 2.4557,
        "grad_norm": 1.674590826034546,
        "learning_rate": 0.00017026425576957698,
        "epoch": 0.08715406856670797,
        "step": 633
    },
    {
        "loss": 2.4391,
        "grad_norm": 1.9629205465316772,
        "learning_rate": 0.00017017154446320883,
        "epoch": 0.08729175271926201,
        "step": 634
    },
    {
        "loss": 1.434,
        "grad_norm": 2.183455228805542,
        "learning_rate": 0.0001700787141790031,
        "epoch": 0.08742943687181605,
        "step": 635
    },
    {
        "loss": 2.4273,
        "grad_norm": 1.2158693075180054,
        "learning_rate": 0.00016998576507435618,
        "epoch": 0.0875671210243701,
        "step": 636
    },
    {
        "loss": 1.9123,
        "grad_norm": 2.9490110874176025,
        "learning_rate": 0.00016989269730686591,
        "epoch": 0.08770480517692414,
        "step": 637
    },
    {
        "loss": 1.9138,
        "grad_norm": 1.6631999015808105,
        "learning_rate": 0.00016979951103433133,
        "epoch": 0.08784248932947818,
        "step": 638
    },
    {
        "loss": 2.4193,
        "grad_norm": 1.2658116817474365,
        "learning_rate": 0.00016970620641475236,
        "epoch": 0.08798017348203221,
        "step": 639
    },
    {
        "loss": 2.2996,
        "grad_norm": 1.7525206804275513,
        "learning_rate": 0.00016961278360632967,
        "epoch": 0.08811785763458625,
        "step": 640
    },
    {
        "loss": 1.9317,
        "grad_norm": 2.0134782791137695,
        "learning_rate": 0.00016951924276746425,
        "epoch": 0.0882555417871403,
        "step": 641
    },
    {
        "loss": 2.1258,
        "grad_norm": 1.904697060585022,
        "learning_rate": 0.0001694255840567572,
        "epoch": 0.08839322593969434,
        "step": 642
    },
    {
        "loss": 2.7169,
        "grad_norm": 1.0180002450942993,
        "learning_rate": 0.00016933180763300956,
        "epoch": 0.08853091009224838,
        "step": 643
    },
    {
        "loss": 1.992,
        "grad_norm": 2.218135356903076,
        "learning_rate": 0.00016923791365522193,
        "epoch": 0.08866859424480242,
        "step": 644
    },
    {
        "loss": 1.7083,
        "grad_norm": 1.8289625644683838,
        "learning_rate": 0.00016914390228259414,
        "epoch": 0.08880627839735647,
        "step": 645
    },
    {
        "loss": 2.3747,
        "grad_norm": 2.584989309310913,
        "learning_rate": 0.00016904977367452516,
        "epoch": 0.08894396254991051,
        "step": 646
    },
    {
        "loss": 1.8978,
        "grad_norm": 1.8760757446289062,
        "learning_rate": 0.00016895552799061272,
        "epoch": 0.08908164670246455,
        "step": 647
    },
    {
        "loss": 2.6898,
        "grad_norm": 1.5538345575332642,
        "learning_rate": 0.00016886116539065302,
        "epoch": 0.08921933085501858,
        "step": 648
    },
    {
        "loss": 1.9042,
        "grad_norm": 2.5451200008392334,
        "learning_rate": 0.00016876668603464054,
        "epoch": 0.08935701500757263,
        "step": 649
    },
    {
        "loss": 2.183,
        "grad_norm": 1.6908705234527588,
        "learning_rate": 0.00016867209008276766,
        "epoch": 0.08949469916012667,
        "step": 650
    },
    {
        "loss": 1.8774,
        "grad_norm": 1.2754254341125488,
        "learning_rate": 0.00016857737769542452,
        "epoch": 0.08963238331268071,
        "step": 651
    },
    {
        "loss": 2.3103,
        "grad_norm": 1.3834080696105957,
        "learning_rate": 0.00016848254903319867,
        "epoch": 0.08977006746523475,
        "step": 652
    },
    {
        "loss": 2.2615,
        "grad_norm": 1.2132948637008667,
        "learning_rate": 0.00016838760425687472,
        "epoch": 0.08990775161778879,
        "step": 653
    },
    {
        "loss": 2.557,
        "grad_norm": 1.5415489673614502,
        "learning_rate": 0.00016829254352743428,
        "epoch": 0.09004543577034284,
        "step": 654
    },
    {
        "loss": 2.353,
        "grad_norm": 1.4283192157745361,
        "learning_rate": 0.00016819736700605549,
        "epoch": 0.09018311992289688,
        "step": 655
    },
    {
        "loss": 1.9172,
        "grad_norm": 1.8860758543014526,
        "learning_rate": 0.00016810207485411284,
        "epoch": 0.09032080407545091,
        "step": 656
    },
    {
        "loss": 2.4519,
        "grad_norm": 1.4094102382659912,
        "learning_rate": 0.00016800666723317686,
        "epoch": 0.09045848822800495,
        "step": 657
    },
    {
        "loss": 2.0365,
        "grad_norm": 2.022249698638916,
        "learning_rate": 0.00016791114430501387,
        "epoch": 0.090596172380559,
        "step": 658
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.3062536716461182,
        "learning_rate": 0.00016781550623158568,
        "epoch": 0.09073385653311304,
        "step": 659
    },
    {
        "loss": 1.5445,
        "grad_norm": 2.3630614280700684,
        "learning_rate": 0.00016771975317504936,
        "epoch": 0.09087154068566708,
        "step": 660
    },
    {
        "loss": 0.8328,
        "grad_norm": 2.3607006072998047,
        "learning_rate": 0.00016762388529775694,
        "epoch": 0.09100922483822112,
        "step": 661
    },
    {
        "loss": 2.491,
        "grad_norm": 1.151425838470459,
        "learning_rate": 0.0001675279027622551,
        "epoch": 0.09114690899077516,
        "step": 662
    },
    {
        "loss": 2.8048,
        "grad_norm": 1.7278119325637817,
        "learning_rate": 0.00016743180573128495,
        "epoch": 0.0912845931433292,
        "step": 663
    },
    {
        "loss": 2.1732,
        "grad_norm": 2.284942626953125,
        "learning_rate": 0.0001673355943677817,
        "epoch": 0.09142227729588324,
        "step": 664
    },
    {
        "loss": 2.2337,
        "grad_norm": 1.4670610427856445,
        "learning_rate": 0.0001672392688348745,
        "epoch": 0.09155996144843728,
        "step": 665
    },
    {
        "loss": 1.8452,
        "grad_norm": 2.3184757232666016,
        "learning_rate": 0.00016714282929588593,
        "epoch": 0.09169764560099132,
        "step": 666
    },
    {
        "loss": 2.2012,
        "grad_norm": 2.043314218521118,
        "learning_rate": 0.00016704627591433198,
        "epoch": 0.09183532975354537,
        "step": 667
    },
    {
        "loss": 2.491,
        "grad_norm": 1.4550386667251587,
        "learning_rate": 0.00016694960885392168,
        "epoch": 0.09197301390609941,
        "step": 668
    },
    {
        "loss": 1.0873,
        "grad_norm": 1.0554132461547852,
        "learning_rate": 0.00016685282827855674,
        "epoch": 0.09211069805865345,
        "step": 669
    },
    {
        "loss": 2.1293,
        "grad_norm": 1.7246893644332886,
        "learning_rate": 0.00016675593435233133,
        "epoch": 0.09224838221120749,
        "step": 670
    },
    {
        "loss": 1.8674,
        "grad_norm": 2.04091215133667,
        "learning_rate": 0.0001666589272395319,
        "epoch": 0.09238606636376154,
        "step": 671
    },
    {
        "loss": 2.4799,
        "grad_norm": 1.4238033294677734,
        "learning_rate": 0.0001665618071046367,
        "epoch": 0.09252375051631558,
        "step": 672
    },
    {
        "loss": 1.1713,
        "grad_norm": 3.544923782348633,
        "learning_rate": 0.0001664645741123157,
        "epoch": 0.09266143466886961,
        "step": 673
    },
    {
        "loss": 1.6936,
        "grad_norm": 1.8925410509109497,
        "learning_rate": 0.00016636722842743016,
        "epoch": 0.09279911882142365,
        "step": 674
    },
    {
        "loss": 2.6259,
        "grad_norm": 1.666567087173462,
        "learning_rate": 0.00016626977021503244,
        "epoch": 0.09293680297397769,
        "step": 675
    },
    {
        "loss": 1.6786,
        "grad_norm": 2.2810781002044678,
        "learning_rate": 0.00016617219964036576,
        "epoch": 0.09307448712653174,
        "step": 676
    },
    {
        "loss": 2.5229,
        "grad_norm": 0.9525535702705383,
        "learning_rate": 0.00016607451686886368,
        "epoch": 0.09321217127908578,
        "step": 677
    },
    {
        "loss": 2.1299,
        "grad_norm": 1.3384921550750732,
        "learning_rate": 0.00016597672206615017,
        "epoch": 0.09334985543163982,
        "step": 678
    },
    {
        "loss": 2.1495,
        "grad_norm": 3.721353054046631,
        "learning_rate": 0.0001658788153980391,
        "epoch": 0.09348753958419385,
        "step": 679
    },
    {
        "loss": 2.1694,
        "grad_norm": 1.3584856986999512,
        "learning_rate": 0.00016578079703053397,
        "epoch": 0.0936252237367479,
        "step": 680
    },
    {
        "loss": 2.1822,
        "grad_norm": 2.019291639328003,
        "learning_rate": 0.00016568266712982765,
        "epoch": 0.09376290788930194,
        "step": 681
    },
    {
        "loss": 1.2863,
        "grad_norm": 2.301849126815796,
        "learning_rate": 0.00016558442586230222,
        "epoch": 0.09390059204185598,
        "step": 682
    },
    {
        "loss": 2.1452,
        "grad_norm": 2.3030405044555664,
        "learning_rate": 0.00016548607339452853,
        "epoch": 0.09403827619441002,
        "step": 683
    },
    {
        "loss": 2.404,
        "grad_norm": 1.4665122032165527,
        "learning_rate": 0.0001653876098932659,
        "epoch": 0.09417596034696406,
        "step": 684
    },
    {
        "loss": 2.0612,
        "grad_norm": 1.9074455499649048,
        "learning_rate": 0.00016528903552546207,
        "epoch": 0.09431364449951811,
        "step": 685
    },
    {
        "loss": 1.6174,
        "grad_norm": 1.5565210580825806,
        "learning_rate": 0.0001651903504582526,
        "epoch": 0.09445132865207215,
        "step": 686
    },
    {
        "loss": 2.4297,
        "grad_norm": 1.0719337463378906,
        "learning_rate": 0.00016509155485896083,
        "epoch": 0.09458901280462619,
        "step": 687
    },
    {
        "loss": 2.5252,
        "grad_norm": 1.850355625152588,
        "learning_rate": 0.0001649926488950975,
        "epoch": 0.09472669695718022,
        "step": 688
    },
    {
        "loss": 2.24,
        "grad_norm": 1.9209898710250854,
        "learning_rate": 0.00016489363273436048,
        "epoch": 0.09486438110973427,
        "step": 689
    },
    {
        "loss": 2.1187,
        "grad_norm": 1.0938305854797363,
        "learning_rate": 0.00016479450654463443,
        "epoch": 0.09500206526228831,
        "step": 690
    },
    {
        "loss": 2.5867,
        "grad_norm": 1.3120940923690796,
        "learning_rate": 0.00016469527049399062,
        "epoch": 0.09513974941484235,
        "step": 691
    },
    {
        "loss": 2.1845,
        "grad_norm": 1.2460867166519165,
        "learning_rate": 0.0001645959247506866,
        "epoch": 0.09527743356739639,
        "step": 692
    },
    {
        "loss": 2.2961,
        "grad_norm": 1.5286668539047241,
        "learning_rate": 0.00016449646948316592,
        "epoch": 0.09541511771995044,
        "step": 693
    },
    {
        "loss": 1.604,
        "grad_norm": 2.9392378330230713,
        "learning_rate": 0.00016439690486005773,
        "epoch": 0.09555280187250448,
        "step": 694
    },
    {
        "loss": 2.8318,
        "grad_norm": 4.146946907043457,
        "learning_rate": 0.0001642972310501767,
        "epoch": 0.09569048602505852,
        "step": 695
    },
    {
        "loss": 2.6656,
        "grad_norm": 1.4090603590011597,
        "learning_rate": 0.00016419744822252253,
        "epoch": 0.09582817017761255,
        "step": 696
    },
    {
        "loss": 2.5318,
        "grad_norm": 1.4284292459487915,
        "learning_rate": 0.00016409755654627994,
        "epoch": 0.09596585433016659,
        "step": 697
    },
    {
        "loss": 1.8832,
        "grad_norm": 1.9213966131210327,
        "learning_rate": 0.000163997556190818,
        "epoch": 0.09610353848272064,
        "step": 698
    },
    {
        "loss": 2.5702,
        "grad_norm": 1.6306092739105225,
        "learning_rate": 0.0001638974473256902,
        "epoch": 0.09624122263527468,
        "step": 699
    },
    {
        "loss": 2.0847,
        "grad_norm": 1.6162363290786743,
        "learning_rate": 0.00016379723012063387,
        "epoch": 0.09637890678782872,
        "step": 700
    },
    {
        "loss": 2.3634,
        "grad_norm": 1.018174409866333,
        "learning_rate": 0.00016369690474557017,
        "epoch": 0.09651659094038276,
        "step": 701
    },
    {
        "loss": 1.4064,
        "grad_norm": 1.678061842918396,
        "learning_rate": 0.00016359647137060364,
        "epoch": 0.09665427509293681,
        "step": 702
    },
    {
        "loss": 2.3183,
        "grad_norm": 1.3201287984848022,
        "learning_rate": 0.00016349593016602186,
        "epoch": 0.09679195924549085,
        "step": 703
    },
    {
        "loss": 2.5217,
        "grad_norm": 1.7445015907287598,
        "learning_rate": 0.00016339528130229528,
        "epoch": 0.09692964339804488,
        "step": 704
    },
    {
        "loss": 2.4807,
        "grad_norm": 1.6112228631973267,
        "learning_rate": 0.00016329452495007692,
        "epoch": 0.09706732755059892,
        "step": 705
    },
    {
        "loss": 2.1412,
        "grad_norm": 1.2322266101837158,
        "learning_rate": 0.000163193661280202,
        "epoch": 0.09720501170315296,
        "step": 706
    },
    {
        "loss": 2.4445,
        "grad_norm": 1.1825454235076904,
        "learning_rate": 0.00016309269046368776,
        "epoch": 0.09734269585570701,
        "step": 707
    },
    {
        "loss": 2.1138,
        "grad_norm": 1.0092904567718506,
        "learning_rate": 0.000162991612671733,
        "epoch": 0.09748038000826105,
        "step": 708
    },
    {
        "loss": 1.6962,
        "grad_norm": 2.168632745742798,
        "learning_rate": 0.0001628904280757181,
        "epoch": 0.09761806416081509,
        "step": 709
    },
    {
        "loss": 2.4115,
        "grad_norm": 1.2555525302886963,
        "learning_rate": 0.0001627891368472043,
        "epoch": 0.09775574831336913,
        "step": 710
    },
    {
        "loss": 2.4277,
        "grad_norm": 1.3955626487731934,
        "learning_rate": 0.00016268773915793376,
        "epoch": 0.09789343246592318,
        "step": 711
    },
    {
        "loss": 2.4846,
        "grad_norm": 1.398406744003296,
        "learning_rate": 0.00016258623517982916,
        "epoch": 0.09803111661847722,
        "step": 712
    },
    {
        "loss": 2.2884,
        "grad_norm": 1.3701353073120117,
        "learning_rate": 0.00016248462508499333,
        "epoch": 0.09816880077103125,
        "step": 713
    },
    {
        "loss": 2.3229,
        "grad_norm": 1.8189982175827026,
        "learning_rate": 0.00016238290904570905,
        "epoch": 0.09830648492358529,
        "step": 714
    },
    {
        "loss": 2.4302,
        "grad_norm": 1.5040478706359863,
        "learning_rate": 0.0001622810872344388,
        "epoch": 0.09844416907613934,
        "step": 715
    },
    {
        "loss": 2.135,
        "grad_norm": 2.6041836738586426,
        "learning_rate": 0.00016217915982382427,
        "epoch": 0.09858185322869338,
        "step": 716
    },
    {
        "loss": 2.4848,
        "grad_norm": 1.4984588623046875,
        "learning_rate": 0.00016207712698668636,
        "epoch": 0.09871953738124742,
        "step": 717
    },
    {
        "loss": 2.1077,
        "grad_norm": 1.9816151857376099,
        "learning_rate": 0.00016197498889602448,
        "epoch": 0.09885722153380146,
        "step": 718
    },
    {
        "loss": 2.373,
        "grad_norm": 1.0433545112609863,
        "learning_rate": 0.00016187274572501678,
        "epoch": 0.0989949056863555,
        "step": 719
    },
    {
        "loss": 2.5595,
        "grad_norm": 2.306628465652466,
        "learning_rate": 0.00016177039764701944,
        "epoch": 0.09913258983890955,
        "step": 720
    },
    {
        "loss": 2.6869,
        "grad_norm": 1.1700862646102905,
        "learning_rate": 0.00016166794483556647,
        "epoch": 0.09927027399146358,
        "step": 721
    },
    {
        "loss": 1.877,
        "grad_norm": 1.105040192604065,
        "learning_rate": 0.0001615653874643695,
        "epoch": 0.09940795814401762,
        "step": 722
    },
    {
        "loss": 2.5728,
        "grad_norm": 2.028716564178467,
        "learning_rate": 0.0001614627257073175,
        "epoch": 0.09954564229657166,
        "step": 723
    },
    {
        "loss": 1.9865,
        "grad_norm": 1.6667418479919434,
        "learning_rate": 0.00016135995973847633,
        "epoch": 0.09968332644912571,
        "step": 724
    },
    {
        "loss": 2.8081,
        "grad_norm": 1.7731270790100098,
        "learning_rate": 0.0001612570897320887,
        "epoch": 0.09982101060167975,
        "step": 725
    },
    {
        "loss": 2.0954,
        "grad_norm": 1.9960346221923828,
        "learning_rate": 0.00016115411586257346,
        "epoch": 0.09995869475423379,
        "step": 726
    },
    {
        "loss": 2.475,
        "grad_norm": 1.3822133541107178,
        "learning_rate": 0.0001610510383045258,
        "epoch": 0.10009637890678783,
        "step": 727
    },
    {
        "loss": 2.5256,
        "grad_norm": 1.0847173929214478,
        "learning_rate": 0.00016094785723271668,
        "epoch": 0.10023406305934186,
        "step": 728
    },
    {
        "loss": 2.4237,
        "grad_norm": 1.8978632688522339,
        "learning_rate": 0.00016084457282209243,
        "epoch": 0.10037174721189591,
        "step": 729
    },
    {
        "loss": 2.6554,
        "grad_norm": 1.3399326801300049,
        "learning_rate": 0.00016074118524777475,
        "epoch": 0.10050943136444995,
        "step": 730
    },
    {
        "loss": 2.2301,
        "grad_norm": 1.4239557981491089,
        "learning_rate": 0.0001606376946850602,
        "epoch": 0.10064711551700399,
        "step": 731
    },
    {
        "loss": 2.3177,
        "grad_norm": 1.385964274406433,
        "learning_rate": 0.0001605341013094199,
        "epoch": 0.10078479966955803,
        "step": 732
    },
    {
        "loss": 2.0312,
        "grad_norm": 2.1853749752044678,
        "learning_rate": 0.00016043040529649939,
        "epoch": 0.10092248382211208,
        "step": 733
    },
    {
        "loss": 2.0566,
        "grad_norm": 1.3955541849136353,
        "learning_rate": 0.00016032660682211819,
        "epoch": 0.10106016797466612,
        "step": 734
    },
    {
        "loss": 2.475,
        "grad_norm": 1.0302644968032837,
        "learning_rate": 0.00016022270606226954,
        "epoch": 0.10119785212722016,
        "step": 735
    },
    {
        "loss": 2.5211,
        "grad_norm": 1.2186845541000366,
        "learning_rate": 0.00016011870319312013,
        "epoch": 0.1013355362797742,
        "step": 736
    },
    {
        "loss": 2.1267,
        "grad_norm": 1.87954843044281,
        "learning_rate": 0.0001600145983910097,
        "epoch": 0.10147322043232825,
        "step": 737
    },
    {
        "loss": 2.2444,
        "grad_norm": 0.9785807728767395,
        "learning_rate": 0.00015991039183245099,
        "epoch": 0.10161090458488228,
        "step": 738
    },
    {
        "loss": 2.5522,
        "grad_norm": 1.1447118520736694,
        "learning_rate": 0.0001598060836941291,
        "epoch": 0.10174858873743632,
        "step": 739
    },
    {
        "loss": 2.5089,
        "grad_norm": 1.174916386604309,
        "learning_rate": 0.0001597016741529014,
        "epoch": 0.10188627288999036,
        "step": 740
    },
    {
        "loss": 2.4537,
        "grad_norm": 0.8992122411727905,
        "learning_rate": 0.00015959716338579728,
        "epoch": 0.1020239570425444,
        "step": 741
    },
    {
        "loss": 2.5492,
        "grad_norm": 2.186886787414551,
        "learning_rate": 0.0001594925515700177,
        "epoch": 0.10216164119509845,
        "step": 742
    },
    {
        "loss": 2.6063,
        "grad_norm": 1.6923635005950928,
        "learning_rate": 0.00015938783888293492,
        "epoch": 0.10229932534765249,
        "step": 743
    },
    {
        "loss": 2.3895,
        "grad_norm": 1.0177104473114014,
        "learning_rate": 0.0001592830255020923,
        "epoch": 0.10243700950020652,
        "step": 744
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.3180543184280396,
        "learning_rate": 0.00015917811160520378,
        "epoch": 0.10257469365276056,
        "step": 745
    },
    {
        "loss": 0.8118,
        "grad_norm": 1.800173044204712,
        "learning_rate": 0.00015907309737015399,
        "epoch": 0.10271237780531461,
        "step": 746
    },
    {
        "loss": 1.8196,
        "grad_norm": 2.2827606201171875,
        "learning_rate": 0.00015896798297499747,
        "epoch": 0.10285006195786865,
        "step": 747
    },
    {
        "loss": 1.915,
        "grad_norm": 1.8995122909545898,
        "learning_rate": 0.00015886276859795862,
        "epoch": 0.10298774611042269,
        "step": 748
    },
    {
        "loss": 2.3534,
        "grad_norm": 1.5577318668365479,
        "learning_rate": 0.00015875745441743143,
        "epoch": 0.10312543026297673,
        "step": 749
    },
    {
        "loss": 2.0276,
        "grad_norm": 1.2932367324829102,
        "learning_rate": 0.00015865204061197904,
        "epoch": 0.10326311441553077,
        "step": 750
    },
    {
        "loss": 1.8122,
        "grad_norm": 0.9253769516944885,
        "learning_rate": 0.00015854652736033354,
        "epoch": 0.10340079856808482,
        "step": 751
    },
    {
        "loss": 2.4594,
        "grad_norm": 0.9717426896095276,
        "learning_rate": 0.00015844091484139568,
        "epoch": 0.10353848272063886,
        "step": 752
    },
    {
        "loss": 1.9749,
        "grad_norm": 1.4195101261138916,
        "learning_rate": 0.0001583352032342344,
        "epoch": 0.1036761668731929,
        "step": 753
    },
    {
        "loss": 2.498,
        "grad_norm": 1.1630116701126099,
        "learning_rate": 0.00015822939271808678,
        "epoch": 0.10381385102574693,
        "step": 754
    },
    {
        "loss": 2.3269,
        "grad_norm": 1.5735890865325928,
        "learning_rate": 0.00015812348347235753,
        "epoch": 0.10395153517830098,
        "step": 755
    },
    {
        "loss": 2.5104,
        "grad_norm": 0.9916473627090454,
        "learning_rate": 0.00015801747567661874,
        "epoch": 0.10408921933085502,
        "step": 756
    },
    {
        "loss": 2.2268,
        "grad_norm": 1.4750230312347412,
        "learning_rate": 0.00015791136951060964,
        "epoch": 0.10422690348340906,
        "step": 757
    },
    {
        "loss": 2.4137,
        "grad_norm": 0.8925480246543884,
        "learning_rate": 0.00015780516515423625,
        "epoch": 0.1043645876359631,
        "step": 758
    },
    {
        "loss": 1.21,
        "grad_norm": 1.8671824932098389,
        "learning_rate": 0.00015769886278757102,
        "epoch": 0.10450227178851715,
        "step": 759
    },
    {
        "loss": 1.8788,
        "grad_norm": 1.1204595565795898,
        "learning_rate": 0.00015759246259085266,
        "epoch": 0.10463995594107119,
        "step": 760
    },
    {
        "loss": 2.3791,
        "grad_norm": 1.8343067169189453,
        "learning_rate": 0.0001574859647444857,
        "epoch": 0.10477764009362522,
        "step": 761
    },
    {
        "loss": 2.305,
        "grad_norm": 1.435733675956726,
        "learning_rate": 0.00015737936942904023,
        "epoch": 0.10491532424617926,
        "step": 762
    },
    {
        "loss": 2.0357,
        "grad_norm": 1.9919579029083252,
        "learning_rate": 0.00015727267682525163,
        "epoch": 0.1050530083987333,
        "step": 763
    },
    {
        "loss": 1.8004,
        "grad_norm": 1.2832409143447876,
        "learning_rate": 0.00015716588711402025,
        "epoch": 0.10519069255128735,
        "step": 764
    },
    {
        "loss": 2.5849,
        "grad_norm": 1.919418454170227,
        "learning_rate": 0.00015705900047641107,
        "epoch": 0.10532837670384139,
        "step": 765
    },
    {
        "loss": 2.2328,
        "grad_norm": 1.6615532636642456,
        "learning_rate": 0.0001569520170936533,
        "epoch": 0.10546606085639543,
        "step": 766
    },
    {
        "loss": 2.5771,
        "grad_norm": 2.102313995361328,
        "learning_rate": 0.00015684493714714047,
        "epoch": 0.10560374500894947,
        "step": 767
    },
    {
        "loss": 2.6496,
        "grad_norm": 1.7948237657546997,
        "learning_rate": 0.0001567377608184295,
        "epoch": 0.10574142916150352,
        "step": 768
    },
    {
        "loss": 2.0907,
        "grad_norm": 1.9255890846252441,
        "learning_rate": 0.00015663048828924096,
        "epoch": 0.10587911331405755,
        "step": 769
    },
    {
        "loss": 2.5481,
        "grad_norm": 1.3995081186294556,
        "learning_rate": 0.00015652311974145847,
        "epoch": 0.10601679746661159,
        "step": 770
    },
    {
        "loss": 1.8319,
        "grad_norm": 1.564276099205017,
        "learning_rate": 0.00015641565535712837,
        "epoch": 0.10615448161916563,
        "step": 771
    },
    {
        "loss": 2.5322,
        "grad_norm": 2.2399895191192627,
        "learning_rate": 0.0001563080953184596,
        "epoch": 0.10629216577171967,
        "step": 772
    },
    {
        "loss": 1.5816,
        "grad_norm": 2.1789000034332275,
        "learning_rate": 0.00015620043980782327,
        "epoch": 0.10642984992427372,
        "step": 773
    },
    {
        "loss": 2.586,
        "grad_norm": 2.1274261474609375,
        "learning_rate": 0.0001560926890077523,
        "epoch": 0.10656753407682776,
        "step": 774
    },
    {
        "loss": 2.0469,
        "grad_norm": 1.363300085067749,
        "learning_rate": 0.00015598484310094124,
        "epoch": 0.1067052182293818,
        "step": 775
    },
    {
        "loss": 1.7425,
        "grad_norm": 1.6562844514846802,
        "learning_rate": 0.0001558769022702459,
        "epoch": 0.10684290238193583,
        "step": 776
    },
    {
        "loss": 2.5486,
        "grad_norm": 1.6109330654144287,
        "learning_rate": 0.00015576886669868296,
        "epoch": 0.10698058653448989,
        "step": 777
    },
    {
        "loss": 1.7007,
        "grad_norm": 1.2763473987579346,
        "learning_rate": 0.0001556607365694298,
        "epoch": 0.10711827068704392,
        "step": 778
    },
    {
        "loss": 1.5169,
        "grad_norm": 1.6881932020187378,
        "learning_rate": 0.00015555251206582414,
        "epoch": 0.10725595483959796,
        "step": 779
    },
    {
        "loss": 2.62,
        "grad_norm": 2.1452486515045166,
        "learning_rate": 0.00015544419337136365,
        "epoch": 0.107393638992152,
        "step": 780
    },
    {
        "loss": 2.3245,
        "grad_norm": 1.2992243766784668,
        "learning_rate": 0.00015533578066970574,
        "epoch": 0.10753132314470605,
        "step": 781
    },
    {
        "loss": 2.4036,
        "grad_norm": 1.6504935026168823,
        "learning_rate": 0.00015522727414466725,
        "epoch": 0.10766900729726009,
        "step": 782
    },
    {
        "loss": 2.3114,
        "grad_norm": 1.9322054386138916,
        "learning_rate": 0.00015511867398022405,
        "epoch": 0.10780669144981413,
        "step": 783
    },
    {
        "loss": 2.6145,
        "grad_norm": 1.5675787925720215,
        "learning_rate": 0.00015500998036051074,
        "epoch": 0.10794437560236816,
        "step": 784
    },
    {
        "loss": 1.6414,
        "grad_norm": 2.64385986328125,
        "learning_rate": 0.00015490119346982047,
        "epoch": 0.1080820597549222,
        "step": 785
    },
    {
        "loss": 2.3897,
        "grad_norm": 1.4538246393203735,
        "learning_rate": 0.00015479231349260447,
        "epoch": 0.10821974390747625,
        "step": 786
    },
    {
        "loss": 1.9979,
        "grad_norm": 1.739253044128418,
        "learning_rate": 0.00015468334061347183,
        "epoch": 0.10835742806003029,
        "step": 787
    },
    {
        "loss": 1.872,
        "grad_norm": 1.6308670043945312,
        "learning_rate": 0.00015457427501718916,
        "epoch": 0.10849511221258433,
        "step": 788
    },
    {
        "loss": 2.3818,
        "grad_norm": 1.3076516389846802,
        "learning_rate": 0.00015446511688868025,
        "epoch": 0.10863279636513837,
        "step": 789
    },
    {
        "loss": 1.7268,
        "grad_norm": 2.313716173171997,
        "learning_rate": 0.0001543558664130258,
        "epoch": 0.10877048051769242,
        "step": 790
    },
    {
        "loss": 2.2025,
        "grad_norm": 3.2461793422698975,
        "learning_rate": 0.00015424652377546302,
        "epoch": 0.10890816467024646,
        "step": 791
    },
    {
        "loss": 1.4937,
        "grad_norm": 2.761949062347412,
        "learning_rate": 0.00015413708916138554,
        "epoch": 0.1090458488228005,
        "step": 792
    },
    {
        "loss": 2.556,
        "grad_norm": 1.343590259552002,
        "learning_rate": 0.00015402756275634277,
        "epoch": 0.10918353297535453,
        "step": 793
    },
    {
        "loss": 1.6244,
        "grad_norm": 3.790038824081421,
        "learning_rate": 0.0001539179447460399,
        "epoch": 0.10932121712790857,
        "step": 794
    },
    {
        "loss": 2.0237,
        "grad_norm": 1.9127657413482666,
        "learning_rate": 0.00015380823531633729,
        "epoch": 0.10945890128046262,
        "step": 795
    },
    {
        "loss": 2.4429,
        "grad_norm": 0.8477599620819092,
        "learning_rate": 0.0001536984346532504,
        "epoch": 0.10959658543301666,
        "step": 796
    },
    {
        "loss": 2.4337,
        "grad_norm": 1.0099338293075562,
        "learning_rate": 0.00015358854294294938,
        "epoch": 0.1097342695855707,
        "step": 797
    },
    {
        "loss": 2.5605,
        "grad_norm": 1.597885251045227,
        "learning_rate": 0.0001534785603717587,
        "epoch": 0.10987195373812474,
        "step": 798
    },
    {
        "loss": 2.4222,
        "grad_norm": 1.5607562065124512,
        "learning_rate": 0.00015336848712615696,
        "epoch": 0.11000963789067879,
        "step": 799
    },
    {
        "loss": 2.5813,
        "grad_norm": 2.079977035522461,
        "learning_rate": 0.00015325832339277643,
        "epoch": 0.11014732204323283,
        "step": 800
    },
    {
        "loss": 2.0283,
        "grad_norm": 2.0008888244628906,
        "learning_rate": 0.00015314806935840281,
        "epoch": 0.11028500619578686,
        "step": 801
    },
    {
        "loss": 2.3687,
        "grad_norm": 1.8008713722229004,
        "learning_rate": 0.00015303772520997504,
        "epoch": 0.1104226903483409,
        "step": 802
    },
    {
        "loss": 2.4194,
        "grad_norm": 2.058938503265381,
        "learning_rate": 0.0001529272911345846,
        "epoch": 0.11056037450089495,
        "step": 803
    },
    {
        "loss": 2.2819,
        "grad_norm": 1.4230378866195679,
        "learning_rate": 0.00015281676731947568,
        "epoch": 0.11069805865344899,
        "step": 804
    },
    {
        "loss": 1.3641,
        "grad_norm": 1.7184611558914185,
        "learning_rate": 0.0001527061539520445,
        "epoch": 0.11083574280600303,
        "step": 805
    },
    {
        "loss": 2.4797,
        "grad_norm": 1.324542760848999,
        "learning_rate": 0.0001525954512198392,
        "epoch": 0.11097342695855707,
        "step": 806
    },
    {
        "loss": 2.5129,
        "grad_norm": 1.2589805126190186,
        "learning_rate": 0.00015248465931055929,
        "epoch": 0.1111111111111111,
        "step": 807
    },
    {
        "loss": 2.0206,
        "grad_norm": 1.4257057905197144,
        "learning_rate": 0.0001523737784120557,
        "epoch": 0.11124879526366516,
        "step": 808
    },
    {
        "loss": 2.7549,
        "grad_norm": 1.1046054363250732,
        "learning_rate": 0.00015226280871233003,
        "epoch": 0.1113864794162192,
        "step": 809
    },
    {
        "loss": 1.3523,
        "grad_norm": 1.8505911827087402,
        "learning_rate": 0.0001521517503995346,
        "epoch": 0.11152416356877323,
        "step": 810
    },
    {
        "loss": 2.543,
        "grad_norm": 2.536269426345825,
        "learning_rate": 0.00015204060366197196,
        "epoch": 0.11166184772132727,
        "step": 811
    },
    {
        "loss": 2.3929,
        "grad_norm": 1.8534703254699707,
        "learning_rate": 0.00015192936868809443,
        "epoch": 0.11179953187388132,
        "step": 812
    },
    {
        "loss": 2.2082,
        "grad_norm": 1.5484867095947266,
        "learning_rate": 0.00015181804566650415,
        "epoch": 0.11193721602643536,
        "step": 813
    },
    {
        "loss": 2.4369,
        "grad_norm": 1.2560063600540161,
        "learning_rate": 0.00015170663478595241,
        "epoch": 0.1120749001789894,
        "step": 814
    },
    {
        "loss": 2.2765,
        "grad_norm": 2.1858913898468018,
        "learning_rate": 0.0001515951362353395,
        "epoch": 0.11221258433154344,
        "step": 815
    },
    {
        "loss": 2.1833,
        "grad_norm": 2.597752809524536,
        "learning_rate": 0.0001514835502037144,
        "epoch": 0.11235026848409747,
        "step": 816
    },
    {
        "loss": 2.5552,
        "grad_norm": 1.8524647951126099,
        "learning_rate": 0.00015137187688027436,
        "epoch": 0.11248795263665153,
        "step": 817
    },
    {
        "loss": 2.1989,
        "grad_norm": 2.0055720806121826,
        "learning_rate": 0.00015126011645436464,
        "epoch": 0.11262563678920556,
        "step": 818
    },
    {
        "loss": 2.7537,
        "grad_norm": 2.7436294555664062,
        "learning_rate": 0.00015114826911547826,
        "epoch": 0.1127633209417596,
        "step": 819
    },
    {
        "loss": 1.6233,
        "grad_norm": 1.9488524198532104,
        "learning_rate": 0.0001510363350532555,
        "epoch": 0.11290100509431364,
        "step": 820
    },
    {
        "loss": 2.1875,
        "grad_norm": 1.3855299949645996,
        "learning_rate": 0.0001509243144574837,
        "epoch": 0.11303868924686769,
        "step": 821
    },
    {
        "loss": 2.5657,
        "grad_norm": 1.6224864721298218,
        "learning_rate": 0.00015081220751809698,
        "epoch": 0.11317637339942173,
        "step": 822
    },
    {
        "loss": 2.5444,
        "grad_norm": 2.0034737586975098,
        "learning_rate": 0.00015070001442517582,
        "epoch": 0.11331405755197577,
        "step": 823
    },
    {
        "loss": 2.2669,
        "grad_norm": 1.191215991973877,
        "learning_rate": 0.00015058773536894685,
        "epoch": 0.1134517417045298,
        "step": 824
    },
    {
        "loss": 2.3863,
        "grad_norm": 0.9654034376144409,
        "learning_rate": 0.00015047537053978227,
        "epoch": 0.11358942585708386,
        "step": 825
    },
    {
        "loss": 1.8036,
        "grad_norm": 1.7503066062927246,
        "learning_rate": 0.0001503629201281999,
        "epoch": 0.1137271100096379,
        "step": 826
    },
    {
        "loss": 2.3278,
        "grad_norm": 1.1979211568832397,
        "learning_rate": 0.0001502503843248625,
        "epoch": 0.11386479416219193,
        "step": 827
    },
    {
        "loss": 1.8886,
        "grad_norm": 1.2652254104614258,
        "learning_rate": 0.00015013776332057786,
        "epoch": 0.11400247831474597,
        "step": 828
    },
    {
        "loss": 1.8994,
        "grad_norm": 1.2652403116226196,
        "learning_rate": 0.00015002505730629797,
        "epoch": 0.11414016246730001,
        "step": 829
    },
    {
        "loss": 2.0859,
        "grad_norm": 1.6411954164505005,
        "learning_rate": 0.00014991226647311905,
        "epoch": 0.11427784661985406,
        "step": 830
    },
    {
        "loss": 2.2296,
        "grad_norm": 1.9576340913772583,
        "learning_rate": 0.0001497993910122812,
        "epoch": 0.1144155307724081,
        "step": 831
    },
    {
        "loss": 1.1591,
        "grad_norm": 2.6328790187835693,
        "learning_rate": 0.00014968643111516795,
        "epoch": 0.11455321492496214,
        "step": 832
    },
    {
        "loss": 2.5014,
        "grad_norm": 1.469887614250183,
        "learning_rate": 0.00014957338697330595,
        "epoch": 0.11469089907751617,
        "step": 833
    },
    {
        "loss": 2.2914,
        "grad_norm": 1.3586384057998657,
        "learning_rate": 0.0001494602587783648,
        "epoch": 0.11482858323007022,
        "step": 834
    },
    {
        "loss": 2.4835,
        "grad_norm": 1.6544849872589111,
        "learning_rate": 0.00014934704672215653,
        "epoch": 0.11496626738262426,
        "step": 835
    },
    {
        "loss": 2.2172,
        "grad_norm": 1.1984837055206299,
        "learning_rate": 0.0001492337509966354,
        "epoch": 0.1151039515351783,
        "step": 836
    },
    {
        "loss": 2.0916,
        "grad_norm": 2.2735507488250732,
        "learning_rate": 0.00014912037179389746,
        "epoch": 0.11524163568773234,
        "step": 837
    },
    {
        "loss": 2.3551,
        "grad_norm": 0.9834814071655273,
        "learning_rate": 0.0001490069093061804,
        "epoch": 0.11537931984028638,
        "step": 838
    },
    {
        "loss": 1.633,
        "grad_norm": 2.3294730186462402,
        "learning_rate": 0.00014889336372586305,
        "epoch": 0.11551700399284043,
        "step": 839
    },
    {
        "loss": 2.3479,
        "grad_norm": 1.215009093284607,
        "learning_rate": 0.00014877973524546518,
        "epoch": 0.11565468814539447,
        "step": 840
    },
    {
        "loss": 1.9947,
        "grad_norm": 1.966888427734375,
        "learning_rate": 0.00014866602405764706,
        "epoch": 0.1157923722979485,
        "step": 841
    },
    {
        "loss": 2.0694,
        "grad_norm": 1.3161910772323608,
        "learning_rate": 0.00014855223035520926,
        "epoch": 0.11593005645050254,
        "step": 842
    },
    {
        "loss": 2.2107,
        "grad_norm": 1.5185624361038208,
        "learning_rate": 0.00014843835433109222,
        "epoch": 0.1160677406030566,
        "step": 843
    },
    {
        "loss": 2.4327,
        "grad_norm": 2.033076286315918,
        "learning_rate": 0.0001483243961783759,
        "epoch": 0.11620542475561063,
        "step": 844
    },
    {
        "loss": 2.2169,
        "grad_norm": 1.4469783306121826,
        "learning_rate": 0.0001482103560902796,
        "epoch": 0.11634310890816467,
        "step": 845
    },
    {
        "loss": 2.4561,
        "grad_norm": 0.9080806374549866,
        "learning_rate": 0.00014809623426016158,
        "epoch": 0.11648079306071871,
        "step": 846
    },
    {
        "loss": 2.3887,
        "grad_norm": 1.1703953742980957,
        "learning_rate": 0.00014798203088151854,
        "epoch": 0.11661847721327276,
        "step": 847
    },
    {
        "loss": 2.4702,
        "grad_norm": 1.121064305305481,
        "learning_rate": 0.00014786774614798556,
        "epoch": 0.1167561613658268,
        "step": 848
    },
    {
        "loss": 2.4073,
        "grad_norm": 1.2468597888946533,
        "learning_rate": 0.00014775338025333566,
        "epoch": 0.11689384551838083,
        "step": 849
    },
    {
        "loss": 2.1961,
        "grad_norm": 0.9722253084182739,
        "learning_rate": 0.0001476389333914794,
        "epoch": 0.11703152967093487,
        "step": 850
    },
    {
        "loss": 2.1797,
        "grad_norm": 1.364350438117981,
        "learning_rate": 0.00014752440575646473,
        "epoch": 0.11716921382348891,
        "step": 851
    },
    {
        "loss": 2.4268,
        "grad_norm": 1.4890553951263428,
        "learning_rate": 0.00014740979754247646,
        "epoch": 0.11730689797604296,
        "step": 852
    },
    {
        "loss": 2.2977,
        "grad_norm": 1.3402149677276611,
        "learning_rate": 0.00014729510894383608,
        "epoch": 0.117444582128597,
        "step": 853
    },
    {
        "loss": 2.136,
        "grad_norm": 1.966652750968933,
        "learning_rate": 0.00014718034015500132,
        "epoch": 0.11758226628115104,
        "step": 854
    },
    {
        "loss": 2.0441,
        "grad_norm": 1.549736499786377,
        "learning_rate": 0.00014706549137056594,
        "epoch": 0.11771995043370508,
        "step": 855
    },
    {
        "loss": 2.2798,
        "grad_norm": 2.272311210632324,
        "learning_rate": 0.00014695056278525928,
        "epoch": 0.11785763458625913,
        "step": 856
    },
    {
        "loss": 2.2136,
        "grad_norm": 1.3595541715621948,
        "learning_rate": 0.000146835554593946,
        "epoch": 0.11799531873881317,
        "step": 857
    },
    {
        "loss": 1.9201,
        "grad_norm": 2.177426815032959,
        "learning_rate": 0.00014672046699162574,
        "epoch": 0.1181330028913672,
        "step": 858
    },
    {
        "loss": 1.9506,
        "grad_norm": 1.9832241535186768,
        "learning_rate": 0.00014660530017343278,
        "epoch": 0.11827068704392124,
        "step": 859
    },
    {
        "loss": 2.0776,
        "grad_norm": 2.126737356185913,
        "learning_rate": 0.00014649005433463576,
        "epoch": 0.11840837119647528,
        "step": 860
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.8588656187057495,
        "learning_rate": 0.0001463747296706372,
        "epoch": 0.11854605534902933,
        "step": 861
    },
    {
        "loss": 2.037,
        "grad_norm": 1.5720763206481934,
        "learning_rate": 0.00014625932637697336,
        "epoch": 0.11868373950158337,
        "step": 862
    },
    {
        "loss": 2.31,
        "grad_norm": 1.4257069826126099,
        "learning_rate": 0.00014614384464931382,
        "epoch": 0.1188214236541374,
        "step": 863
    },
    {
        "loss": 2.3308,
        "grad_norm": 1.7771100997924805,
        "learning_rate": 0.00014602828468346105,
        "epoch": 0.11895910780669144,
        "step": 864
    },
    {
        "loss": 2.429,
        "grad_norm": 1.5334663391113281,
        "learning_rate": 0.00014591264667535025,
        "epoch": 0.1190967919592455,
        "step": 865
    },
    {
        "loss": 2.0997,
        "grad_norm": 1.337988257408142,
        "learning_rate": 0.00014579693082104894,
        "epoch": 0.11923447611179953,
        "step": 866
    },
    {
        "loss": 2.4811,
        "grad_norm": 1.5359681844711304,
        "learning_rate": 0.00014568113731675667,
        "epoch": 0.11937216026435357,
        "step": 867
    },
    {
        "loss": 2.8271,
        "grad_norm": 1.1700360774993896,
        "learning_rate": 0.0001455652663588045,
        "epoch": 0.11950984441690761,
        "step": 868
    },
    {
        "loss": 1.9815,
        "grad_norm": 1.7258827686309814,
        "learning_rate": 0.00014544931814365505,
        "epoch": 0.11964752856946166,
        "step": 869
    },
    {
        "loss": 2.5364,
        "grad_norm": 1.3856886625289917,
        "learning_rate": 0.00014533329286790168,
        "epoch": 0.1197852127220157,
        "step": 870
    },
    {
        "loss": 2.1048,
        "grad_norm": 1.6160225868225098,
        "learning_rate": 0.00014521719072826858,
        "epoch": 0.11992289687456974,
        "step": 871
    },
    {
        "loss": 2.5754,
        "grad_norm": 1.2062495946884155,
        "learning_rate": 0.00014510101192161018,
        "epoch": 0.12006058102712377,
        "step": 872
    },
    {
        "loss": 2.1465,
        "grad_norm": 1.53996741771698,
        "learning_rate": 0.00014498475664491094,
        "epoch": 0.12019826517967781,
        "step": 873
    },
    {
        "loss": 2.4068,
        "grad_norm": 1.5413199663162231,
        "learning_rate": 0.00014486842509528502,
        "epoch": 0.12033594933223186,
        "step": 874
    },
    {
        "loss": 2.1976,
        "grad_norm": 1.0295069217681885,
        "learning_rate": 0.00014475201746997582,
        "epoch": 0.1204736334847859,
        "step": 875
    },
    {
        "loss": 2.0545,
        "grad_norm": 2.165087938308716,
        "learning_rate": 0.0001446355339663557,
        "epoch": 0.12061131763733994,
        "step": 876
    },
    {
        "loss": 1.7282,
        "grad_norm": 2.820330858230591,
        "learning_rate": 0.0001445189747819258,
        "epoch": 0.12074900178989398,
        "step": 877
    },
    {
        "loss": 2.2532,
        "grad_norm": 1.1724209785461426,
        "learning_rate": 0.00014440234011431547,
        "epoch": 0.12088668594244803,
        "step": 878
    },
    {
        "loss": 1.7927,
        "grad_norm": 2.4921114444732666,
        "learning_rate": 0.00014428563016128214,
        "epoch": 0.12102437009500207,
        "step": 879
    },
    {
        "loss": 2.1776,
        "grad_norm": 0.9470633268356323,
        "learning_rate": 0.0001441688451207108,
        "epoch": 0.1211620542475561,
        "step": 880
    },
    {
        "loss": 2.0518,
        "grad_norm": 1.7872490882873535,
        "learning_rate": 0.00014405198519061382,
        "epoch": 0.12129973840011014,
        "step": 881
    },
    {
        "loss": 2.2419,
        "grad_norm": 1.5266306400299072,
        "learning_rate": 0.00014393505056913046,
        "epoch": 0.12143742255266418,
        "step": 882
    },
    {
        "loss": 2.2013,
        "grad_norm": 1.1939177513122559,
        "learning_rate": 0.0001438180414545267,
        "epoch": 0.12157510670521823,
        "step": 883
    },
    {
        "loss": 2.1905,
        "grad_norm": 2.6690995693206787,
        "learning_rate": 0.00014370095804519485,
        "epoch": 0.12171279085777227,
        "step": 884
    },
    {
        "loss": 2.2745,
        "grad_norm": 2.0825893878936768,
        "learning_rate": 0.00014358380053965307,
        "epoch": 0.12185047501032631,
        "step": 885
    },
    {
        "loss": 2.6318,
        "grad_norm": 1.0409384965896606,
        "learning_rate": 0.00014346656913654526,
        "epoch": 0.12198815916288035,
        "step": 886
    },
    {
        "loss": 2.2127,
        "grad_norm": 1.0837105512619019,
        "learning_rate": 0.00014334926403464056,
        "epoch": 0.1221258433154344,
        "step": 887
    },
    {
        "loss": 2.1414,
        "grad_norm": 2.208961009979248,
        "learning_rate": 0.0001432318854328331,
        "epoch": 0.12226352746798844,
        "step": 888
    },
    {
        "loss": 0.772,
        "grad_norm": 3.0553393363952637,
        "learning_rate": 0.00014311443353014162,
        "epoch": 0.12240121162054247,
        "step": 889
    },
    {
        "loss": 1.9425,
        "grad_norm": 1.7730107307434082,
        "learning_rate": 0.00014299690852570914,
        "epoch": 0.12253889577309651,
        "step": 890
    },
    {
        "loss": 2.0203,
        "grad_norm": 1.7786192893981934,
        "learning_rate": 0.00014287931061880262,
        "epoch": 0.12267657992565056,
        "step": 891
    },
    {
        "loss": 1.8894,
        "grad_norm": 1.0740439891815186,
        "learning_rate": 0.00014276164000881264,
        "epoch": 0.1228142640782046,
        "step": 892
    },
    {
        "loss": 2.3967,
        "grad_norm": 1.3221362829208374,
        "learning_rate": 0.00014264389689525308,
        "epoch": 0.12295194823075864,
        "step": 893
    },
    {
        "loss": 2.3963,
        "grad_norm": 1.4060866832733154,
        "learning_rate": 0.00014252608147776062,
        "epoch": 0.12308963238331268,
        "step": 894
    },
    {
        "loss": 1.9352,
        "grad_norm": 1.559919834136963,
        "learning_rate": 0.00014240819395609477,
        "epoch": 0.12322731653586672,
        "step": 895
    },
    {
        "loss": 2.2878,
        "grad_norm": 1.794320821762085,
        "learning_rate": 0.00014229023453013703,
        "epoch": 0.12336500068842077,
        "step": 896
    },
    {
        "loss": 2.2808,
        "grad_norm": 1.6249860525131226,
        "learning_rate": 0.00014217220339989099,
        "epoch": 0.1235026848409748,
        "step": 897
    },
    {
        "loss": 2.6578,
        "grad_norm": 1.4403955936431885,
        "learning_rate": 0.00014205410076548183,
        "epoch": 0.12364036899352884,
        "step": 898
    },
    {
        "loss": 1.7755,
        "grad_norm": 2.172341823577881,
        "learning_rate": 0.00014193592682715578,
        "epoch": 0.12377805314608288,
        "step": 899
    },
    {
        "loss": 1.9248,
        "grad_norm": 2.4680392742156982,
        "learning_rate": 0.0001418176817852802,
        "epoch": 0.12391573729863693,
        "step": 900
    },
    {
        "loss": 1.9681,
        "grad_norm": 1.9907020330429077,
        "learning_rate": 0.00014169936584034287,
        "epoch": 0.12405342145119097,
        "step": 901
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.085310459136963,
        "learning_rate": 0.0001415809791929518,
        "epoch": 0.12419110560374501,
        "step": 902
    },
    {
        "loss": 2.3723,
        "grad_norm": 1.3108950853347778,
        "learning_rate": 0.0001414625220438349,
        "epoch": 0.12432878975629905,
        "step": 903
    },
    {
        "loss": 2.2247,
        "grad_norm": 1.4987022876739502,
        "learning_rate": 0.00014134399459383965,
        "epoch": 0.12446647390885308,
        "step": 904
    },
    {
        "loss": 1.6888,
        "grad_norm": 1.6488527059555054,
        "learning_rate": 0.00014122539704393265,
        "epoch": 0.12460415806140714,
        "step": 905
    },
    {
        "loss": 2.463,
        "grad_norm": 1.4542263746261597,
        "learning_rate": 0.00014110672959519941,
        "epoch": 0.12474184221396117,
        "step": 906
    },
    {
        "loss": 2.7135,
        "grad_norm": 1.9251865148544312,
        "learning_rate": 0.00014098799244884397,
        "epoch": 0.12487952636651521,
        "step": 907
    },
    {
        "loss": 2.5435,
        "grad_norm": 1.4451340436935425,
        "learning_rate": 0.00014086918580618848,
        "epoch": 0.12501721051906925,
        "step": 908
    },
    {
        "loss": 2.4382,
        "grad_norm": 1.139878749847412,
        "learning_rate": 0.00014075030986867302,
        "epoch": 0.1251548946716233,
        "step": 909
    },
    {
        "loss": 2.4203,
        "grad_norm": 1.9075316190719604,
        "learning_rate": 0.00014063136483785505,
        "epoch": 0.12529257882417733,
        "step": 910
    },
    {
        "loss": 2.518,
        "grad_norm": 1.8156977891921997,
        "learning_rate": 0.00014051235091540922,
        "epoch": 0.12543026297673138,
        "step": 911
    },
    {
        "loss": 1.7837,
        "grad_norm": 2.931565046310425,
        "learning_rate": 0.00014039326830312705,
        "epoch": 0.12556794712928543,
        "step": 912
    },
    {
        "loss": 1.8584,
        "grad_norm": 1.4028534889221191,
        "learning_rate": 0.00014027411720291645,
        "epoch": 0.12570563128183945,
        "step": 913
    },
    {
        "loss": 1.4384,
        "grad_norm": 2.031909704208374,
        "learning_rate": 0.0001401548978168015,
        "epoch": 0.1258433154343935,
        "step": 914
    },
    {
        "loss": 2.5309,
        "grad_norm": 1.5008641481399536,
        "learning_rate": 0.00014003561034692198,
        "epoch": 0.12598099958694753,
        "step": 915
    },
    {
        "loss": 2.4197,
        "grad_norm": 1.2882241010665894,
        "learning_rate": 0.00013991625499553325,
        "epoch": 0.12611868373950158,
        "step": 916
    },
    {
        "loss": 1.7618,
        "grad_norm": 2.0267908573150635,
        "learning_rate": 0.00013979683196500565,
        "epoch": 0.12625636789205563,
        "step": 917
    },
    {
        "loss": 2.3858,
        "grad_norm": 1.7422555685043335,
        "learning_rate": 0.00013967734145782425,
        "epoch": 0.12639405204460966,
        "step": 918
    },
    {
        "loss": 2.5376,
        "grad_norm": 1.275882363319397,
        "learning_rate": 0.00013955778367658866,
        "epoch": 0.1265317361971637,
        "step": 919
    },
    {
        "loss": 2.3643,
        "grad_norm": 1.6176092624664307,
        "learning_rate": 0.00013943815882401248,
        "epoch": 0.12666942034971776,
        "step": 920
    },
    {
        "loss": 2.0601,
        "grad_norm": 1.6005094051361084,
        "learning_rate": 0.00013931846710292295,
        "epoch": 0.12680710450227178,
        "step": 921
    },
    {
        "loss": 1.8403,
        "grad_norm": 2.599482774734497,
        "learning_rate": 0.00013919870871626083,
        "epoch": 0.12694478865482584,
        "step": 922
    },
    {
        "loss": 2.018,
        "grad_norm": 1.4496207237243652,
        "learning_rate": 0.00013907888386707987,
        "epoch": 0.12708247280737986,
        "step": 923
    },
    {
        "loss": 1.7474,
        "grad_norm": 1.5950385332107544,
        "learning_rate": 0.0001389589927585464,
        "epoch": 0.1272201569599339,
        "step": 924
    },
    {
        "loss": 2.1413,
        "grad_norm": 2.184514284133911,
        "learning_rate": 0.00013883903559393928,
        "epoch": 0.12735784111248796,
        "step": 925
    },
    {
        "loss": 2.6625,
        "grad_norm": 1.223634958267212,
        "learning_rate": 0.00013871901257664923,
        "epoch": 0.127495525265042,
        "step": 926
    },
    {
        "loss": 1.7081,
        "grad_norm": 1.9607460498809814,
        "learning_rate": 0.00013859892391017865,
        "epoch": 0.12763320941759604,
        "step": 927
    },
    {
        "loss": 2.195,
        "grad_norm": 1.3550158739089966,
        "learning_rate": 0.00013847876979814134,
        "epoch": 0.12777089357015006,
        "step": 928
    },
    {
        "loss": 2.3888,
        "grad_norm": 1.193344235420227,
        "learning_rate": 0.00013835855044426189,
        "epoch": 0.12790857772270411,
        "step": 929
    },
    {
        "loss": 2.1354,
        "grad_norm": 1.107646107673645,
        "learning_rate": 0.00013823826605237571,
        "epoch": 0.12804626187525817,
        "step": 930
    },
    {
        "loss": 1.8868,
        "grad_norm": 1.7666341066360474,
        "learning_rate": 0.0001381179168264283,
        "epoch": 0.1281839460278122,
        "step": 931
    },
    {
        "loss": 2.3038,
        "grad_norm": 1.578887939453125,
        "learning_rate": 0.0001379975029704753,
        "epoch": 0.12832163018036624,
        "step": 932
    },
    {
        "loss": 2.339,
        "grad_norm": 1.734542727470398,
        "learning_rate": 0.00013787702468868166,
        "epoch": 0.1284593143329203,
        "step": 933
    },
    {
        "loss": 2.6398,
        "grad_norm": 1.7643150091171265,
        "learning_rate": 0.0001377564821853218,
        "epoch": 0.12859699848547432,
        "step": 934
    },
    {
        "loss": 2.2008,
        "grad_norm": 1.7973484992980957,
        "learning_rate": 0.00013763587566477896,
        "epoch": 0.12873468263802837,
        "step": 935
    },
    {
        "loss": 2.7687,
        "grad_norm": 1.179276466369629,
        "learning_rate": 0.00013751520533154484,
        "epoch": 0.1288723667905824,
        "step": 936
    },
    {
        "loss": 2.5533,
        "grad_norm": 2.6822426319122314,
        "learning_rate": 0.0001373944713902194,
        "epoch": 0.12901005094313645,
        "step": 937
    },
    {
        "loss": 2.3918,
        "grad_norm": 2.0512421131134033,
        "learning_rate": 0.00013727367404551055,
        "epoch": 0.1291477350956905,
        "step": 938
    },
    {
        "loss": 2.172,
        "grad_norm": 2.443463087081909,
        "learning_rate": 0.00013715281350223352,
        "epoch": 0.12928541924824452,
        "step": 939
    },
    {
        "loss": 2.5259,
        "grad_norm": 1.303389549255371,
        "learning_rate": 0.00013703188996531075,
        "epoch": 0.12942310340079857,
        "step": 940
    },
    {
        "loss": 2.1366,
        "grad_norm": 1.2243432998657227,
        "learning_rate": 0.00013691090363977161,
        "epoch": 0.1295607875533526,
        "step": 941
    },
    {
        "loss": 2.5002,
        "grad_norm": 1.1717890501022339,
        "learning_rate": 0.00013678985473075176,
        "epoch": 0.12969847170590665,
        "step": 942
    },
    {
        "loss": 1.6179,
        "grad_norm": 2.288255453109741,
        "learning_rate": 0.0001366687434434931,
        "epoch": 0.1298361558584607,
        "step": 943
    },
    {
        "loss": 2.3383,
        "grad_norm": 1.5813721418380737,
        "learning_rate": 0.00013654756998334318,
        "epoch": 0.12997384001101472,
        "step": 944
    },
    {
        "loss": 2.5203,
        "grad_norm": 1.9787168502807617,
        "learning_rate": 0.0001364263345557551,
        "epoch": 0.13011152416356878,
        "step": 945
    },
    {
        "loss": 2.3165,
        "grad_norm": 1.8592580556869507,
        "learning_rate": 0.00013630503736628688,
        "epoch": 0.13024920831612283,
        "step": 946
    },
    {
        "loss": 2.379,
        "grad_norm": 1.3226428031921387,
        "learning_rate": 0.0001361836786206014,
        "epoch": 0.13038689246867685,
        "step": 947
    },
    {
        "loss": 2.5839,
        "grad_norm": 1.4719746112823486,
        "learning_rate": 0.0001360622585244658,
        "epoch": 0.1305245766212309,
        "step": 948
    },
    {
        "loss": 1.7279,
        "grad_norm": 1.9026142358779907,
        "learning_rate": 0.00013594077728375128,
        "epoch": 0.13066226077378493,
        "step": 949
    },
    {
        "loss": 2.2994,
        "grad_norm": 1.7772480249404907,
        "learning_rate": 0.0001358192351044328,
        "epoch": 0.13079994492633898,
        "step": 950
    },
    {
        "loss": 2.1919,
        "grad_norm": 1.039559006690979,
        "learning_rate": 0.00013569763219258845,
        "epoch": 0.13093762907889303,
        "step": 951
    },
    {
        "loss": 1.8959,
        "grad_norm": 1.4355418682098389,
        "learning_rate": 0.00013557596875439946,
        "epoch": 0.13107531323144705,
        "step": 952
    },
    {
        "loss": 2.4337,
        "grad_norm": 1.158817172050476,
        "learning_rate": 0.00013545424499614964,
        "epoch": 0.1312129973840011,
        "step": 953
    },
    {
        "loss": 2.352,
        "grad_norm": 1.2867356538772583,
        "learning_rate": 0.00013533246112422506,
        "epoch": 0.13135068153655513,
        "step": 954
    },
    {
        "loss": 2.4391,
        "grad_norm": 1.2243525981903076,
        "learning_rate": 0.0001352106173451137,
        "epoch": 0.13148836568910918,
        "step": 955
    },
    {
        "loss": 2.1547,
        "grad_norm": 1.0338404178619385,
        "learning_rate": 0.00013508871386540516,
        "epoch": 0.13162604984166323,
        "step": 956
    },
    {
        "loss": 2.3803,
        "grad_norm": 1.4720773696899414,
        "learning_rate": 0.0001349667508917902,
        "epoch": 0.13176373399421726,
        "step": 957
    },
    {
        "loss": 2.6141,
        "grad_norm": 1.1507892608642578,
        "learning_rate": 0.00013484472863106047,
        "epoch": 0.1319014181467713,
        "step": 958
    },
    {
        "loss": 2.2883,
        "grad_norm": 1.1498699188232422,
        "learning_rate": 0.00013472264729010822,
        "epoch": 0.13203910229932533,
        "step": 959
    },
    {
        "loss": 2.5339,
        "grad_norm": 1.782859206199646,
        "learning_rate": 0.0001346005070759258,
        "epoch": 0.13217678645187939,
        "step": 960
    },
    {
        "loss": 1.901,
        "grad_norm": 1.7905739545822144,
        "learning_rate": 0.0001344783081956054,
        "epoch": 0.13231447060443344,
        "step": 961
    },
    {
        "loss": 2.1128,
        "grad_norm": 2.1538407802581787,
        "learning_rate": 0.00013435605085633867,
        "epoch": 0.13245215475698746,
        "step": 962
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.834517002105713,
        "learning_rate": 0.00013423373526541636,
        "epoch": 0.1325898389095415,
        "step": 963
    },
    {
        "loss": 1.835,
        "grad_norm": 2.493793249130249,
        "learning_rate": 0.00013411136163022803,
        "epoch": 0.13272752306209556,
        "step": 964
    },
    {
        "loss": 1.6802,
        "grad_norm": 1.4096448421478271,
        "learning_rate": 0.0001339889301582617,
        "epoch": 0.1328652072146496,
        "step": 965
    },
    {
        "loss": 2.395,
        "grad_norm": 1.8752477169036865,
        "learning_rate": 0.00013386644105710326,
        "epoch": 0.13300289136720364,
        "step": 966
    },
    {
        "loss": 2.1977,
        "grad_norm": 1.6405887603759766,
        "learning_rate": 0.00013374389453443657,
        "epoch": 0.13314057551975766,
        "step": 967
    },
    {
        "loss": 2.1341,
        "grad_norm": 1.7230479717254639,
        "learning_rate": 0.00013362129079804268,
        "epoch": 0.13327825967231172,
        "step": 968
    },
    {
        "loss": 2.7886,
        "grad_norm": 1.2287631034851074,
        "learning_rate": 0.00013349863005579962,
        "epoch": 0.13341594382486577,
        "step": 969
    },
    {
        "loss": 1.6312,
        "grad_norm": 2.779366970062256,
        "learning_rate": 0.00013337591251568227,
        "epoch": 0.1335536279774198,
        "step": 970
    },
    {
        "loss": 1.6774,
        "grad_norm": 1.7462506294250488,
        "learning_rate": 0.00013325313838576162,
        "epoch": 0.13369131212997384,
        "step": 971
    },
    {
        "loss": 2.3932,
        "grad_norm": 2.134885311126709,
        "learning_rate": 0.00013313030787420467,
        "epoch": 0.13382899628252787,
        "step": 972
    },
    {
        "loss": 2.7151,
        "grad_norm": 1.2936276197433472,
        "learning_rate": 0.00013300742118927406,
        "epoch": 0.13396668043508192,
        "step": 973
    },
    {
        "loss": 2.7599,
        "grad_norm": 1.5614761114120483,
        "learning_rate": 0.00013288447853932763,
        "epoch": 0.13410436458763597,
        "step": 974
    },
    {
        "loss": 1.8324,
        "grad_norm": 2.6449332237243652,
        "learning_rate": 0.0001327614801328181,
        "epoch": 0.13424204874019,
        "step": 975
    },
    {
        "loss": 2.2466,
        "grad_norm": 1.3253816366195679,
        "learning_rate": 0.00013263842617829272,
        "epoch": 0.13437973289274405,
        "step": 976
    },
    {
        "loss": 1.7215,
        "grad_norm": 2.180985927581787,
        "learning_rate": 0.00013251531688439303,
        "epoch": 0.1345174170452981,
        "step": 977
    },
    {
        "loss": 2.0858,
        "grad_norm": 1.8063660860061646,
        "learning_rate": 0.00013239215245985426,
        "epoch": 0.13465510119785212,
        "step": 978
    },
    {
        "loss": 2.4618,
        "grad_norm": 1.161617636680603,
        "learning_rate": 0.0001322689331135052,
        "epoch": 0.13479278535040617,
        "step": 979
    },
    {
        "loss": 2.251,
        "grad_norm": 2.22932767868042,
        "learning_rate": 0.0001321456590542677,
        "epoch": 0.1349304695029602,
        "step": 980
    },
    {
        "loss": 2.3746,
        "grad_norm": 2.05658221244812,
        "learning_rate": 0.00013202233049115645,
        "epoch": 0.13506815365551425,
        "step": 981
    },
    {
        "loss": 2.0704,
        "grad_norm": 1.0659010410308838,
        "learning_rate": 0.0001318989476332785,
        "epoch": 0.1352058378080683,
        "step": 982
    },
    {
        "loss": 2.6857,
        "grad_norm": 0.993681788444519,
        "learning_rate": 0.00013177551068983297,
        "epoch": 0.13534352196062233,
        "step": 983
    },
    {
        "loss": 2.5187,
        "grad_norm": 1.6372342109680176,
        "learning_rate": 0.00013165201987011072,
        "epoch": 0.13548120611317638,
        "step": 984
    },
    {
        "loss": 1.7143,
        "grad_norm": 1.5419436693191528,
        "learning_rate": 0.00013152847538349383,
        "epoch": 0.1356188902657304,
        "step": 985
    },
    {
        "loss": 1.9106,
        "grad_norm": 1.946934461593628,
        "learning_rate": 0.00013140487743945562,
        "epoch": 0.13575657441828445,
        "step": 986
    },
    {
        "loss": 1.8157,
        "grad_norm": 2.6536624431610107,
        "learning_rate": 0.0001312812262475598,
        "epoch": 0.1358942585708385,
        "step": 987
    },
    {
        "loss": 1.7385,
        "grad_norm": 2.273977518081665,
        "learning_rate": 0.00013115752201746042,
        "epoch": 0.13603194272339253,
        "step": 988
    },
    {
        "loss": 2.6287,
        "grad_norm": 1.3544896841049194,
        "learning_rate": 0.0001310337649589016,
        "epoch": 0.13616962687594658,
        "step": 989
    },
    {
        "loss": 2.4443,
        "grad_norm": 1.9884593486785889,
        "learning_rate": 0.00013090995528171692,
        "epoch": 0.13630731102850063,
        "step": 990
    },
    {
        "loss": 1.6775,
        "grad_norm": 2.8129897117614746,
        "learning_rate": 0.0001307860931958291,
        "epoch": 0.13644499518105466,
        "step": 991
    },
    {
        "loss": 2.352,
        "grad_norm": 2.123243808746338,
        "learning_rate": 0.00013066217891124991,
        "epoch": 0.1365826793336087,
        "step": 992
    },
    {
        "loss": 2.1397,
        "grad_norm": 1.4457032680511475,
        "learning_rate": 0.00013053821263807948,
        "epoch": 0.13672036348616273,
        "step": 993
    },
    {
        "loss": 2.5231,
        "grad_norm": 1.4994393587112427,
        "learning_rate": 0.0001304141945865061,
        "epoch": 0.13685804763871678,
        "step": 994
    },
    {
        "loss": 2.3589,
        "grad_norm": 1.664137601852417,
        "learning_rate": 0.00013029012496680592,
        "epoch": 0.13699573179127084,
        "step": 995
    },
    {
        "loss": 2.0651,
        "grad_norm": 1.2815544605255127,
        "learning_rate": 0.00013016600398934252,
        "epoch": 0.13713341594382486,
        "step": 996
    },
    {
        "loss": 2.0234,
        "grad_norm": 2.185494899749756,
        "learning_rate": 0.00013004183186456645,
        "epoch": 0.1372711000963789,
        "step": 997
    },
    {
        "loss": 2.393,
        "grad_norm": 2.668165445327759,
        "learning_rate": 0.00012991760880301512,
        "epoch": 0.13740878424893294,
        "step": 998
    },
    {
        "loss": 2.4072,
        "grad_norm": 0.8652409315109253,
        "learning_rate": 0.0001297933350153122,
        "epoch": 0.137546468401487,
        "step": 999
    },
    {
        "loss": 2.1805,
        "grad_norm": 1.6826932430267334,
        "learning_rate": 0.0001296690107121674,
        "epoch": 0.13768415255404104,
        "step": 1000
    },
    {
        "loss": 2.0232,
        "grad_norm": 2.322169780731201,
        "learning_rate": 0.00012954463610437617,
        "epoch": 0.13782183670659506,
        "step": 1001
    },
    {
        "loss": 1.7855,
        "grad_norm": 2.29632306098938,
        "learning_rate": 0.00012942021140281915,
        "epoch": 0.13795952085914912,
        "step": 1002
    },
    {
        "loss": 1.7318,
        "grad_norm": 2.5369412899017334,
        "learning_rate": 0.00012929573681846182,
        "epoch": 0.13809720501170314,
        "step": 1003
    },
    {
        "loss": 1.7938,
        "grad_norm": 2.3776278495788574,
        "learning_rate": 0.00012917121256235455,
        "epoch": 0.1382348891642572,
        "step": 1004
    },
    {
        "loss": 1.917,
        "grad_norm": 1.4606001377105713,
        "learning_rate": 0.0001290466388456316,
        "epoch": 0.13837257331681124,
        "step": 1005
    },
    {
        "loss": 2.0144,
        "grad_norm": 2.4186511039733887,
        "learning_rate": 0.00012892201587951126,
        "epoch": 0.13851025746936527,
        "step": 1006
    },
    {
        "loss": 2.4629,
        "grad_norm": 1.39419424533844,
        "learning_rate": 0.00012879734387529528,
        "epoch": 0.13864794162191932,
        "step": 1007
    },
    {
        "loss": 2.3678,
        "grad_norm": 1.626405954360962,
        "learning_rate": 0.00012867262304436862,
        "epoch": 0.13878562577447337,
        "step": 1008
    },
    {
        "loss": 2.2052,
        "grad_norm": 2.430100440979004,
        "learning_rate": 0.00012854785359819895,
        "epoch": 0.1389233099270274,
        "step": 1009
    },
    {
        "loss": 1.8287,
        "grad_norm": 2.210171699523926,
        "learning_rate": 0.00012842303574833635,
        "epoch": 0.13906099407958145,
        "step": 1010
    },
    {
        "loss": 2.6473,
        "grad_norm": 1.6934053897857666,
        "learning_rate": 0.0001282981697064131,
        "epoch": 0.13919867823213547,
        "step": 1011
    },
    {
        "loss": 2.226,
        "grad_norm": 1.1973059177398682,
        "learning_rate": 0.00012817325568414297,
        "epoch": 0.13933636238468952,
        "step": 1012
    },
    {
        "loss": 2.0406,
        "grad_norm": 1.5128726959228516,
        "learning_rate": 0.00012804829389332135,
        "epoch": 0.13947404653724357,
        "step": 1013
    },
    {
        "loss": 1.8227,
        "grad_norm": 1.216190218925476,
        "learning_rate": 0.00012792328454582449,
        "epoch": 0.1396117306897976,
        "step": 1014
    },
    {
        "loss": 2.3572,
        "grad_norm": 1.881131649017334,
        "learning_rate": 0.00012779822785360912,
        "epoch": 0.13974941484235165,
        "step": 1015
    },
    {
        "loss": 1.4053,
        "grad_norm": 2.33614444732666,
        "learning_rate": 0.00012767312402871253,
        "epoch": 0.13988709899490567,
        "step": 1016
    },
    {
        "loss": 2.6853,
        "grad_norm": 1.938091516494751,
        "learning_rate": 0.0001275479732832518,
        "epoch": 0.14002478314745972,
        "step": 1017
    },
    {
        "loss": 2.0181,
        "grad_norm": 2.6740705966949463,
        "learning_rate": 0.0001274227758294235,
        "epoch": 0.14016246730001378,
        "step": 1018
    },
    {
        "loss": 2.0154,
        "grad_norm": 1.2084743976593018,
        "learning_rate": 0.00012729753187950345,
        "epoch": 0.1403001514525678,
        "step": 1019
    },
    {
        "loss": 2.2185,
        "grad_norm": 1.5054497718811035,
        "learning_rate": 0.00012717224164584637,
        "epoch": 0.14043783560512185,
        "step": 1020
    },
    {
        "loss": 2.7063,
        "grad_norm": 1.0757629871368408,
        "learning_rate": 0.0001270469053408853,
        "epoch": 0.1405755197576759,
        "step": 1021
    },
    {
        "loss": 2.2446,
        "grad_norm": 1.6072555780410767,
        "learning_rate": 0.00012692152317713158,
        "epoch": 0.14071320391022993,
        "step": 1022
    },
    {
        "loss": 2.1626,
        "grad_norm": 2.0364019870758057,
        "learning_rate": 0.00012679609536717418,
        "epoch": 0.14085088806278398,
        "step": 1023
    },
    {
        "loss": 2.1484,
        "grad_norm": 1.9404724836349487,
        "learning_rate": 0.00012667062212367948,
        "epoch": 0.140988572215338,
        "step": 1024
    },
    {
        "loss": 1.9817,
        "grad_norm": 1.5950212478637695,
        "learning_rate": 0.00012654510365939093,
        "epoch": 0.14112625636789206,
        "step": 1025
    },
    {
        "loss": 1.5751,
        "grad_norm": 2.2002274990081787,
        "learning_rate": 0.00012641954018712863,
        "epoch": 0.1412639405204461,
        "step": 1026
    },
    {
        "loss": 2.1914,
        "grad_norm": 1.2640448808670044,
        "learning_rate": 0.00012629393191978898,
        "epoch": 0.14140162467300013,
        "step": 1027
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.6058125495910645,
        "learning_rate": 0.00012616827907034438,
        "epoch": 0.14153930882555418,
        "step": 1028
    },
    {
        "loss": 2.5669,
        "grad_norm": 1.9760758876800537,
        "learning_rate": 0.00012604258185184273,
        "epoch": 0.1416769929781082,
        "step": 1029
    },
    {
        "loss": 2.2637,
        "grad_norm": 1.1579350233078003,
        "learning_rate": 0.00012591684047740732,
        "epoch": 0.14181467713066226,
        "step": 1030
    },
    {
        "loss": 2.3238,
        "grad_norm": 1.654439091682434,
        "learning_rate": 0.00012579105516023615,
        "epoch": 0.1419523612832163,
        "step": 1031
    },
    {
        "loss": 1.7611,
        "grad_norm": 1.8390382528305054,
        "learning_rate": 0.00012566522611360173,
        "epoch": 0.14209004543577033,
        "step": 1032
    },
    {
        "loss": 1.7398,
        "grad_norm": 1.182532787322998,
        "learning_rate": 0.00012553935355085085,
        "epoch": 0.1422277295883244,
        "step": 1033
    },
    {
        "loss": 2.4037,
        "grad_norm": 1.7062944173812866,
        "learning_rate": 0.00012541343768540395,
        "epoch": 0.14236541374087844,
        "step": 1034
    },
    {
        "loss": 2.4925,
        "grad_norm": 1.4914077520370483,
        "learning_rate": 0.00012528747873075496,
        "epoch": 0.14250309789343246,
        "step": 1035
    },
    {
        "loss": 1.7371,
        "grad_norm": 2.140752077102661,
        "learning_rate": 0.00012516147690047085,
        "epoch": 0.14264078204598651,
        "step": 1036
    },
    {
        "loss": 2.333,
        "grad_norm": 1.7241395711898804,
        "learning_rate": 0.00012503543240819127,
        "epoch": 0.14277846619854054,
        "step": 1037
    },
    {
        "loss": 2.5014,
        "grad_norm": 2.3008835315704346,
        "learning_rate": 0.00012490934546762822,
        "epoch": 0.1429161503510946,
        "step": 1038
    },
    {
        "loss": 1.9169,
        "grad_norm": 1.9247926473617554,
        "learning_rate": 0.00012478321629256567,
        "epoch": 0.14305383450364864,
        "step": 1039
    },
    {
        "loss": 2.3264,
        "grad_norm": 1.277718186378479,
        "learning_rate": 0.00012465704509685924,
        "epoch": 0.14319151865620267,
        "step": 1040
    },
    {
        "loss": 2.2597,
        "grad_norm": 1.4084256887435913,
        "learning_rate": 0.0001245308320944357,
        "epoch": 0.14332920280875672,
        "step": 1041
    },
    {
        "loss": 2.4267,
        "grad_norm": 1.2487523555755615,
        "learning_rate": 0.00012440457749929285,
        "epoch": 0.14346688696131074,
        "step": 1042
    },
    {
        "loss": 1.914,
        "grad_norm": 2.162720203399658,
        "learning_rate": 0.00012427828152549884,
        "epoch": 0.1436045711138648,
        "step": 1043
    },
    {
        "loss": 2.3475,
        "grad_norm": 0.8742092251777649,
        "learning_rate": 0.0001241519443871921,
        "epoch": 0.14374225526641884,
        "step": 1044
    },
    {
        "loss": 1.683,
        "grad_norm": 2.704766035079956,
        "learning_rate": 0.0001240255662985808,
        "epoch": 0.14387993941897287,
        "step": 1045
    },
    {
        "loss": 2.2953,
        "grad_norm": 1.2559305429458618,
        "learning_rate": 0.00012389914747394258,
        "epoch": 0.14401762357152692,
        "step": 1046
    },
    {
        "loss": 2.4961,
        "grad_norm": 1.2768632173538208,
        "learning_rate": 0.00012377268812762416,
        "epoch": 0.14415530772408097,
        "step": 1047
    },
    {
        "loss": 2.4606,
        "grad_norm": 1.1693192720413208,
        "learning_rate": 0.0001236461884740409,
        "epoch": 0.144292991876635,
        "step": 1048
    },
    {
        "loss": 2.1073,
        "grad_norm": 2.676837682723999,
        "learning_rate": 0.0001235196487276765,
        "epoch": 0.14443067602918905,
        "step": 1049
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.277370572090149,
        "learning_rate": 0.00012339306910308278,
        "epoch": 0.14456836018174307,
        "step": 1050
    },
    {
        "loss": 2.4033,
        "grad_norm": 2.4072248935699463,
        "learning_rate": 0.00012326644981487897,
        "epoch": 0.14470604433429712,
        "step": 1051
    },
    {
        "loss": 2.0674,
        "grad_norm": 1.905360221862793,
        "learning_rate": 0.0001231397910777517,
        "epoch": 0.14484372848685118,
        "step": 1052
    },
    {
        "loss": 1.6697,
        "grad_norm": 2.135883331298828,
        "learning_rate": 0.00012301309310645448,
        "epoch": 0.1449814126394052,
        "step": 1053
    },
    {
        "loss": 2.2689,
        "grad_norm": 1.6968013048171997,
        "learning_rate": 0.00012288635611580723,
        "epoch": 0.14511909679195925,
        "step": 1054
    },
    {
        "loss": 2.2681,
        "grad_norm": 1.3645763397216797,
        "learning_rate": 0.00012275958032069616,
        "epoch": 0.14525678094451328,
        "step": 1055
    },
    {
        "loss": 1.8571,
        "grad_norm": 1.3136892318725586,
        "learning_rate": 0.00012263276593607315,
        "epoch": 0.14539446509706733,
        "step": 1056
    },
    {
        "loss": 2.098,
        "grad_norm": 1.1246891021728516,
        "learning_rate": 0.00012250591317695562,
        "epoch": 0.14553214924962138,
        "step": 1057
    },
    {
        "loss": 2.618,
        "grad_norm": 1.3901450634002686,
        "learning_rate": 0.000122379022258426,
        "epoch": 0.1456698334021754,
        "step": 1058
    },
    {
        "loss": 1.2654,
        "grad_norm": 0.9903416633605957,
        "learning_rate": 0.00012225209339563145,
        "epoch": 0.14580751755472945,
        "step": 1059
    },
    {
        "loss": 2.2172,
        "grad_norm": 1.2231886386871338,
        "learning_rate": 0.0001221251268037834,
        "epoch": 0.14594520170728348,
        "step": 1060
    },
    {
        "loss": 2.5715,
        "grad_norm": 1.410102128982544,
        "learning_rate": 0.00012199812269815733,
        "epoch": 0.14608288585983753,
        "step": 1061
    },
    {
        "loss": 1.1157,
        "grad_norm": 2.0076732635498047,
        "learning_rate": 0.00012187108129409232,
        "epoch": 0.14622057001239158,
        "step": 1062
    },
    {
        "loss": 2.3874,
        "grad_norm": 1.6614803075790405,
        "learning_rate": 0.00012174400280699057,
        "epoch": 0.1463582541649456,
        "step": 1063
    },
    {
        "loss": 2.4655,
        "grad_norm": 1.3349671363830566,
        "learning_rate": 0.00012161688745231732,
        "epoch": 0.14649593831749966,
        "step": 1064
    },
    {
        "loss": 1.9202,
        "grad_norm": 2.4358792304992676,
        "learning_rate": 0.00012148973544560025,
        "epoch": 0.1466336224700537,
        "step": 1065
    },
    {
        "loss": 2.5125,
        "grad_norm": 1.3067268133163452,
        "learning_rate": 0.00012136254700242915,
        "epoch": 0.14677130662260773,
        "step": 1066
    },
    {
        "loss": 2.2301,
        "grad_norm": 1.592786192893982,
        "learning_rate": 0.00012123532233845565,
        "epoch": 0.14690899077516179,
        "step": 1067
    },
    {
        "loss": 1.9891,
        "grad_norm": 2.148878812789917,
        "learning_rate": 0.00012110806166939274,
        "epoch": 0.1470466749277158,
        "step": 1068
    },
    {
        "loss": 2.3568,
        "grad_norm": 2.4533963203430176,
        "learning_rate": 0.00012098076521101452,
        "epoch": 0.14718435908026986,
        "step": 1069
    },
    {
        "loss": 1.9247,
        "grad_norm": 1.93686842918396,
        "learning_rate": 0.00012085343317915565,
        "epoch": 0.1473220432328239,
        "step": 1070
    },
    {
        "loss": 2.3845,
        "grad_norm": 1.4030344486236572,
        "learning_rate": 0.00012072606578971127,
        "epoch": 0.14745972738537794,
        "step": 1071
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.3803787231445312,
        "learning_rate": 0.00012059866325863638,
        "epoch": 0.147597411537932,
        "step": 1072
    },
    {
        "loss": 2.1078,
        "grad_norm": 2.3882434368133545,
        "learning_rate": 0.00012047122580194552,
        "epoch": 0.147735095690486,
        "step": 1073
    },
    {
        "loss": 2.1741,
        "grad_norm": 2.317697048187256,
        "learning_rate": 0.0001203437536357126,
        "epoch": 0.14787277984304006,
        "step": 1074
    },
    {
        "loss": 2.7678,
        "grad_norm": 1.3189772367477417,
        "learning_rate": 0.00012021624697607021,
        "epoch": 0.14801046399559412,
        "step": 1075
    },
    {
        "loss": 2.2698,
        "grad_norm": 1.6493350267410278,
        "learning_rate": 0.00012008870603920946,
        "epoch": 0.14814814814814814,
        "step": 1076
    },
    {
        "loss": 1.8037,
        "grad_norm": 2.6258459091186523,
        "learning_rate": 0.00011996113104137972,
        "epoch": 0.1482858323007022,
        "step": 1077
    },
    {
        "loss": 2.1536,
        "grad_norm": 1.2583556175231934,
        "learning_rate": 0.00011983352219888792,
        "epoch": 0.14842351645325624,
        "step": 1078
    },
    {
        "loss": 2.2366,
        "grad_norm": 1.858127474784851,
        "learning_rate": 0.00011970587972809845,
        "epoch": 0.14856120060581027,
        "step": 1079
    },
    {
        "loss": 2.3153,
        "grad_norm": 1.4117658138275146,
        "learning_rate": 0.00011957820384543283,
        "epoch": 0.14869888475836432,
        "step": 1080
    },
    {
        "loss": 2.4562,
        "grad_norm": 1.1183991432189941,
        "learning_rate": 0.00011945049476736905,
        "epoch": 0.14883656891091834,
        "step": 1081
    },
    {
        "loss": 1.8482,
        "grad_norm": 1.6652300357818604,
        "learning_rate": 0.00011932275271044147,
        "epoch": 0.1489742530634724,
        "step": 1082
    },
    {
        "loss": 2.2564,
        "grad_norm": 1.9969834089279175,
        "learning_rate": 0.00011919497789124037,
        "epoch": 0.14911193721602645,
        "step": 1083
    },
    {
        "loss": 1.1979,
        "grad_norm": 2.3709311485290527,
        "learning_rate": 0.00011906717052641159,
        "epoch": 0.14924962136858047,
        "step": 1084
    },
    {
        "loss": 2.2301,
        "grad_norm": 1.2863752841949463,
        "learning_rate": 0.00011893933083265602,
        "epoch": 0.14938730552113452,
        "step": 1085
    },
    {
        "loss": 2.1332,
        "grad_norm": 1.6498852968215942,
        "learning_rate": 0.00011881145902672966,
        "epoch": 0.14952498967368855,
        "step": 1086
    },
    {
        "loss": 1.3031,
        "grad_norm": 2.2015576362609863,
        "learning_rate": 0.00011868355532544264,
        "epoch": 0.1496626738262426,
        "step": 1087
    },
    {
        "loss": 1.7724,
        "grad_norm": 3.8008155822753906,
        "learning_rate": 0.00011855561994565931,
        "epoch": 0.14980035797879665,
        "step": 1088
    },
    {
        "loss": 2.0989,
        "grad_norm": 1.5856956243515015,
        "learning_rate": 0.00011842765310429777,
        "epoch": 0.14993804213135067,
        "step": 1089
    },
    {
        "loss": 1.4467,
        "grad_norm": 2.199897050857544,
        "learning_rate": 0.00011829965501832943,
        "epoch": 0.15007572628390473,
        "step": 1090
    },
    {
        "loss": 2.1447,
        "grad_norm": 1.074818730354309,
        "learning_rate": 0.00011817162590477857,
        "epoch": 0.15021341043645878,
        "step": 1091
    },
    {
        "loss": 2.3514,
        "grad_norm": 1.3270312547683716,
        "learning_rate": 0.00011804356598072223,
        "epoch": 0.1503510945890128,
        "step": 1092
    },
    {
        "loss": 2.3712,
        "grad_norm": 1.2388869524002075,
        "learning_rate": 0.0001179154754632897,
        "epoch": 0.15048877874156685,
        "step": 1093
    },
    {
        "loss": 2.3088,
        "grad_norm": 2.177180051803589,
        "learning_rate": 0.00011778735456966195,
        "epoch": 0.15062646289412088,
        "step": 1094
    },
    {
        "loss": 1.881,
        "grad_norm": 2.192410707473755,
        "learning_rate": 0.00011765920351707165,
        "epoch": 0.15076414704667493,
        "step": 1095
    },
    {
        "loss": 2.2654,
        "grad_norm": 1.0777374505996704,
        "learning_rate": 0.00011753102252280252,
        "epoch": 0.15090183119922898,
        "step": 1096
    },
    {
        "loss": 2.1896,
        "grad_norm": 1.7700471878051758,
        "learning_rate": 0.00011740281180418906,
        "epoch": 0.151039515351783,
        "step": 1097
    },
    {
        "loss": 1.8218,
        "grad_norm": 2.6132102012634277,
        "learning_rate": 0.00011727457157861615,
        "epoch": 0.15117719950433706,
        "step": 1098
    },
    {
        "loss": 1.3575,
        "grad_norm": 1.965761661529541,
        "learning_rate": 0.00011714630206351875,
        "epoch": 0.15131488365689108,
        "step": 1099
    },
    {
        "loss": 2.2565,
        "grad_norm": 1.8164546489715576,
        "learning_rate": 0.00011701800347638141,
        "epoch": 0.15145256780944513,
        "step": 1100
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.2265574932098389,
        "learning_rate": 0.00011688967603473806,
        "epoch": 0.15159025196199918,
        "step": 1101
    },
    {
        "loss": 2.6741,
        "grad_norm": 0.9228039383888245,
        "learning_rate": 0.00011676131995617148,
        "epoch": 0.1517279361145532,
        "step": 1102
    },
    {
        "loss": 2.5389,
        "grad_norm": 1.0133509635925293,
        "learning_rate": 0.00011663293545831302,
        "epoch": 0.15186562026710726,
        "step": 1103
    },
    {
        "loss": 2.3631,
        "grad_norm": 1.2283252477645874,
        "learning_rate": 0.0001165045227588422,
        "epoch": 0.15200330441966128,
        "step": 1104
    },
    {
        "loss": 2.194,
        "grad_norm": 1.1146103143692017,
        "learning_rate": 0.00011637608207548649,
        "epoch": 0.15214098857221534,
        "step": 1105
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.237044334411621,
        "learning_rate": 0.00011624761362602061,
        "epoch": 0.1522786727247694,
        "step": 1106
    },
    {
        "loss": 2.1484,
        "grad_norm": 1.5982297658920288,
        "learning_rate": 0.00011611911762826641,
        "epoch": 0.1524163568773234,
        "step": 1107
    },
    {
        "loss": 1.9471,
        "grad_norm": 2.420471668243408,
        "learning_rate": 0.0001159905943000926,
        "epoch": 0.15255404102987746,
        "step": 1108
    },
    {
        "loss": 1.3179,
        "grad_norm": 2.283910036087036,
        "learning_rate": 0.00011586204385941406,
        "epoch": 0.15269172518243151,
        "step": 1109
    },
    {
        "loss": 2.034,
        "grad_norm": 1.9361556768417358,
        "learning_rate": 0.00011573346652419169,
        "epoch": 0.15282940933498554,
        "step": 1110
    },
    {
        "loss": 2.4243,
        "grad_norm": 3.0984294414520264,
        "learning_rate": 0.00011560486251243197,
        "epoch": 0.1529670934875396,
        "step": 1111
    },
    {
        "loss": 2.1964,
        "grad_norm": 1.535518765449524,
        "learning_rate": 0.00011547623204218671,
        "epoch": 0.15310477764009361,
        "step": 1112
    },
    {
        "loss": 2.0515,
        "grad_norm": 1.6385853290557861,
        "learning_rate": 0.00011534757533155246,
        "epoch": 0.15324246179264767,
        "step": 1113
    },
    {
        "loss": 2.355,
        "grad_norm": 1.405860185623169,
        "learning_rate": 0.00011521889259867032,
        "epoch": 0.15338014594520172,
        "step": 1114
    },
    {
        "loss": 2.201,
        "grad_norm": 1.7668766975402832,
        "learning_rate": 0.0001150901840617255,
        "epoch": 0.15351783009775574,
        "step": 1115
    },
    {
        "loss": 1.8343,
        "grad_norm": 1.6166231632232666,
        "learning_rate": 0.00011496144993894695,
        "epoch": 0.1536555142503098,
        "step": 1116
    },
    {
        "loss": 2.0621,
        "grad_norm": 2.0630123615264893,
        "learning_rate": 0.00011483269044860705,
        "epoch": 0.15379319840286382,
        "step": 1117
    },
    {
        "loss": 1.7525,
        "grad_norm": 1.820915937423706,
        "learning_rate": 0.00011470390580902118,
        "epoch": 0.15393088255541787,
        "step": 1118
    },
    {
        "loss": 1.7496,
        "grad_norm": 2.089460611343384,
        "learning_rate": 0.00011457509623854724,
        "epoch": 0.15406856670797192,
        "step": 1119
    },
    {
        "loss": 1.8429,
        "grad_norm": 1.187258005142212,
        "learning_rate": 0.00011444626195558554,
        "epoch": 0.15420625086052595,
        "step": 1120
    },
    {
        "loss": 1.8514,
        "grad_norm": 2.283364772796631,
        "learning_rate": 0.0001143174031785783,
        "epoch": 0.15434393501308,
        "step": 1121
    },
    {
        "loss": 2.1107,
        "grad_norm": 2.479288339614868,
        "learning_rate": 0.00011418852012600916,
        "epoch": 0.15448161916563405,
        "step": 1122
    },
    {
        "loss": 1.8856,
        "grad_norm": 1.8642289638519287,
        "learning_rate": 0.00011405961301640305,
        "epoch": 0.15461930331818807,
        "step": 1123
    },
    {
        "loss": 2.3562,
        "grad_norm": 1.8739434480667114,
        "learning_rate": 0.00011393068206832551,
        "epoch": 0.15475698747074212,
        "step": 1124
    },
    {
        "loss": 2.0214,
        "grad_norm": 2.2918734550476074,
        "learning_rate": 0.00011380172750038269,
        "epoch": 0.15489467162329615,
        "step": 1125
    },
    {
        "loss": 2.2228,
        "grad_norm": 1.6150039434432983,
        "learning_rate": 0.00011367274953122069,
        "epoch": 0.1550323557758502,
        "step": 1126
    },
    {
        "loss": 2.422,
        "grad_norm": 1.6821757555007935,
        "learning_rate": 0.00011354374837952528,
        "epoch": 0.15517003992840425,
        "step": 1127
    },
    {
        "loss": 2.0662,
        "grad_norm": 2.60117244720459,
        "learning_rate": 0.00011341472426402158,
        "epoch": 0.15530772408095828,
        "step": 1128
    },
    {
        "loss": 2.1365,
        "grad_norm": 1.98235285282135,
        "learning_rate": 0.00011328567740347367,
        "epoch": 0.15544540823351233,
        "step": 1129
    },
    {
        "loss": 2.0912,
        "grad_norm": 1.050768494606018,
        "learning_rate": 0.00011315660801668404,
        "epoch": 0.15558309238606635,
        "step": 1130
    },
    {
        "loss": 2.1649,
        "grad_norm": 1.205888032913208,
        "learning_rate": 0.00011302751632249358,
        "epoch": 0.1557207765386204,
        "step": 1131
    },
    {
        "loss": 2.0167,
        "grad_norm": 1.9418768882751465,
        "learning_rate": 0.00011289840253978088,
        "epoch": 0.15585846069117446,
        "step": 1132
    },
    {
        "loss": 1.691,
        "grad_norm": 2.1508164405822754,
        "learning_rate": 0.00011276926688746196,
        "epoch": 0.15599614484372848,
        "step": 1133
    },
    {
        "loss": 2.3198,
        "grad_norm": 1.6753507852554321,
        "learning_rate": 0.00011264010958449007,
        "epoch": 0.15613382899628253,
        "step": 1134
    },
    {
        "loss": 2.0153,
        "grad_norm": 1.4327616691589355,
        "learning_rate": 0.00011251093084985498,
        "epoch": 0.15627151314883658,
        "step": 1135
    },
    {
        "loss": 2.0348,
        "grad_norm": 2.532088041305542,
        "learning_rate": 0.00011238173090258293,
        "epoch": 0.1564091973013906,
        "step": 1136
    },
    {
        "loss": 1.7654,
        "grad_norm": 3.088167428970337,
        "learning_rate": 0.00011225250996173607,
        "epoch": 0.15654688145394466,
        "step": 1137
    },
    {
        "loss": 2.3218,
        "grad_norm": 1.1601274013519287,
        "learning_rate": 0.00011212326824641217,
        "epoch": 0.15668456560649868,
        "step": 1138
    },
    {
        "loss": 2.5733,
        "grad_norm": 1.160882830619812,
        "learning_rate": 0.00011199400597574416,
        "epoch": 0.15682224975905273,
        "step": 1139
    },
    {
        "loss": 2.2531,
        "grad_norm": 1.2063226699829102,
        "learning_rate": 0.00011186472336889997,
        "epoch": 0.15695993391160679,
        "step": 1140
    },
    {
        "loss": 1.8359,
        "grad_norm": 2.1049795150756836,
        "learning_rate": 0.00011173542064508186,
        "epoch": 0.1570976180641608,
        "step": 1141
    },
    {
        "loss": 2.3974,
        "grad_norm": 1.1507474184036255,
        "learning_rate": 0.00011160609802352621,
        "epoch": 0.15723530221671486,
        "step": 1142
    },
    {
        "loss": 2.1132,
        "grad_norm": 2.631547212600708,
        "learning_rate": 0.00011147675572350327,
        "epoch": 0.15737298636926889,
        "step": 1143
    },
    {
        "loss": 2.8394,
        "grad_norm": 2.110948085784912,
        "learning_rate": 0.00011134739396431652,
        "epoch": 0.15751067052182294,
        "step": 1144
    },
    {
        "loss": 2.1209,
        "grad_norm": 1.2740774154663086,
        "learning_rate": 0.00011121801296530245,
        "epoch": 0.157648354674377,
        "step": 1145
    },
    {
        "loss": 2.4319,
        "grad_norm": 1.706911563873291,
        "learning_rate": 0.00011108861294583029,
        "epoch": 0.157786038826931,
        "step": 1146
    },
    {
        "loss": 2.0134,
        "grad_norm": 1.4992910623550415,
        "learning_rate": 0.00011095919412530136,
        "epoch": 0.15792372297948506,
        "step": 1147
    },
    {
        "loss": 1.989,
        "grad_norm": 1.7452799081802368,
        "learning_rate": 0.00011082975672314892,
        "epoch": 0.1580614071320391,
        "step": 1148
    },
    {
        "loss": 2.1841,
        "grad_norm": 1.0384188890457153,
        "learning_rate": 0.00011070030095883779,
        "epoch": 0.15819909128459314,
        "step": 1149
    },
    {
        "loss": 1.9285,
        "grad_norm": 1.9428573846817017,
        "learning_rate": 0.00011057082705186385,
        "epoch": 0.1583367754371472,
        "step": 1150
    },
    {
        "loss": 2.0778,
        "grad_norm": 1.7945297956466675,
        "learning_rate": 0.00011044133522175382,
        "epoch": 0.15847445958970122,
        "step": 1151
    },
    {
        "loss": 2.9133,
        "grad_norm": 1.3902878761291504,
        "learning_rate": 0.00011031182568806467,
        "epoch": 0.15861214374225527,
        "step": 1152
    },
    {
        "loss": 1.3245,
        "grad_norm": 3.0864996910095215,
        "learning_rate": 0.00011018229867038356,
        "epoch": 0.15874982789480932,
        "step": 1153
    },
    {
        "loss": 2.3082,
        "grad_norm": 1.4970765113830566,
        "learning_rate": 0.00011005275438832718,
        "epoch": 0.15888751204736334,
        "step": 1154
    },
    {
        "loss": 2.1784,
        "grad_norm": 0.9312658905982971,
        "learning_rate": 0.00010992319306154144,
        "epoch": 0.1590251961999174,
        "step": 1155
    },
    {
        "loss": 2.4488,
        "grad_norm": 1.1129016876220703,
        "learning_rate": 0.00010979361490970139,
        "epoch": 0.15916288035247142,
        "step": 1156
    },
    {
        "loss": 2.1196,
        "grad_norm": 1.763015627861023,
        "learning_rate": 0.00010966402015251036,
        "epoch": 0.15930056450502547,
        "step": 1157
    },
    {
        "loss": 2.3439,
        "grad_norm": 1.2710473537445068,
        "learning_rate": 0.00010953440900969991,
        "epoch": 0.15943824865757952,
        "step": 1158
    },
    {
        "loss": 2.221,
        "grad_norm": 2.0415289402008057,
        "learning_rate": 0.00010940478170102945,
        "epoch": 0.15957593281013355,
        "step": 1159
    },
    {
        "loss": 2.3222,
        "grad_norm": 1.7243150472640991,
        "learning_rate": 0.00010927513844628571,
        "epoch": 0.1597136169626876,
        "step": 1160
    },
    {
        "loss": 1.3468,
        "grad_norm": 2.1571896076202393,
        "learning_rate": 0.00010914547946528246,
        "epoch": 0.15985130111524162,
        "step": 1161
    },
    {
        "loss": 2.0533,
        "grad_norm": 1.513131856918335,
        "learning_rate": 0.00010901580497786027,
        "epoch": 0.15998898526779567,
        "step": 1162
    },
    {
        "loss": 2.6307,
        "grad_norm": 1.5065420866012573,
        "learning_rate": 0.00010888611520388585,
        "epoch": 0.16012666942034973,
        "step": 1163
    },
    {
        "loss": 2.6743,
        "grad_norm": 0.9638360142707825,
        "learning_rate": 0.00010875641036325182,
        "epoch": 0.16026435357290375,
        "step": 1164
    },
    {
        "loss": 1.8775,
        "grad_norm": 1.9208827018737793,
        "learning_rate": 0.00010862669067587648,
        "epoch": 0.1604020377254578,
        "step": 1165
    },
    {
        "loss": 2.4159,
        "grad_norm": 1.3903751373291016,
        "learning_rate": 0.00010849695636170322,
        "epoch": 0.16053972187801185,
        "step": 1166
    },
    {
        "loss": 2.0066,
        "grad_norm": 2.49007248878479,
        "learning_rate": 0.00010836720764070014,
        "epoch": 0.16067740603056588,
        "step": 1167
    },
    {
        "loss": 1.624,
        "grad_norm": 2.058255672454834,
        "learning_rate": 0.00010823744473286001,
        "epoch": 0.16081509018311993,
        "step": 1168
    },
    {
        "loss": 1.7123,
        "grad_norm": 1.843923807144165,
        "learning_rate": 0.00010810766785819946,
        "epoch": 0.16095277433567395,
        "step": 1169
    },
    {
        "loss": 1.8173,
        "grad_norm": 2.011284589767456,
        "learning_rate": 0.00010797787723675878,
        "epoch": 0.161090458488228,
        "step": 1170
    },
    {
        "loss": 1.7946,
        "grad_norm": 1.9628657102584839,
        "learning_rate": 0.00010784807308860171,
        "epoch": 0.16122814264078206,
        "step": 1171
    },
    {
        "loss": 2.1081,
        "grad_norm": 1.4169912338256836,
        "learning_rate": 0.00010771825563381488,
        "epoch": 0.16136582679333608,
        "step": 1172
    },
    {
        "loss": 2.4448,
        "grad_norm": 1.215925931930542,
        "learning_rate": 0.00010758842509250735,
        "epoch": 0.16150351094589013,
        "step": 1173
    },
    {
        "loss": 1.4328,
        "grad_norm": 2.4707999229431152,
        "learning_rate": 0.00010745858168481053,
        "epoch": 0.16164119509844416,
        "step": 1174
    },
    {
        "loss": 2.3084,
        "grad_norm": 2.106658458709717,
        "learning_rate": 0.00010732872563087761,
        "epoch": 0.1617788792509982,
        "step": 1175
    },
    {
        "loss": 2.602,
        "grad_norm": 1.3694950342178345,
        "learning_rate": 0.0001071988571508832,
        "epoch": 0.16191656340355226,
        "step": 1176
    },
    {
        "loss": 1.9818,
        "grad_norm": 1.1669906377792358,
        "learning_rate": 0.00010706897646502292,
        "epoch": 0.16205424755610628,
        "step": 1177
    },
    {
        "loss": 1.7718,
        "grad_norm": 2.2348296642303467,
        "learning_rate": 0.00010693908379351323,
        "epoch": 0.16219193170866034,
        "step": 1178
    },
    {
        "loss": 2.2424,
        "grad_norm": 1.0608487129211426,
        "learning_rate": 0.00010680917935659076,
        "epoch": 0.1623296158612144,
        "step": 1179
    },
    {
        "loss": 2.2143,
        "grad_norm": 2.0037503242492676,
        "learning_rate": 0.00010667926337451217,
        "epoch": 0.1624673000137684,
        "step": 1180
    },
    {
        "loss": 1.6232,
        "grad_norm": 2.364577054977417,
        "learning_rate": 0.00010654933606755368,
        "epoch": 0.16260498416632246,
        "step": 1181
    },
    {
        "loss": 1.0465,
        "grad_norm": 2.355229377746582,
        "learning_rate": 0.00010641939765601077,
        "epoch": 0.1627426683188765,
        "step": 1182
    },
    {
        "loss": 2.469,
        "grad_norm": 1.397936224937439,
        "learning_rate": 0.00010628944836019759,
        "epoch": 0.16288035247143054,
        "step": 1183
    },
    {
        "loss": 2.1886,
        "grad_norm": 1.430930495262146,
        "learning_rate": 0.00010615948840044694,
        "epoch": 0.1630180366239846,
        "step": 1184
    },
    {
        "loss": 2.3434,
        "grad_norm": 1.580600380897522,
        "learning_rate": 0.00010602951799710952,
        "epoch": 0.16315572077653862,
        "step": 1185
    },
    {
        "loss": 1.7688,
        "grad_norm": 1.9992345571517944,
        "learning_rate": 0.0001058995373705539,
        "epoch": 0.16329340492909267,
        "step": 1186
    },
    {
        "loss": 2.2401,
        "grad_norm": 1.6949025392532349,
        "learning_rate": 0.00010576954674116586,
        "epoch": 0.1634310890816467,
        "step": 1187
    },
    {
        "loss": 1.9132,
        "grad_norm": 1.5118367671966553,
        "learning_rate": 0.0001056395463293482,
        "epoch": 0.16356877323420074,
        "step": 1188
    },
    {
        "loss": 1.7474,
        "grad_norm": 1.9820834398269653,
        "learning_rate": 0.00010550953635552028,
        "epoch": 0.1637064573867548,
        "step": 1189
    },
    {
        "loss": 1.6963,
        "grad_norm": 2.1719799041748047,
        "learning_rate": 0.00010537951704011772,
        "epoch": 0.16384414153930882,
        "step": 1190
    },
    {
        "loss": 1.4289,
        "grad_norm": 0.9982337355613708,
        "learning_rate": 0.00010524948860359193,
        "epoch": 0.16398182569186287,
        "step": 1191
    },
    {
        "loss": 1.758,
        "grad_norm": 1.1930230855941772,
        "learning_rate": 0.00010511945126640979,
        "epoch": 0.1641195098444169,
        "step": 1192
    },
    {
        "loss": 1.0826,
        "grad_norm": 2.1704208850860596,
        "learning_rate": 0.0001049894052490533,
        "epoch": 0.16425719399697095,
        "step": 1193
    },
    {
        "loss": 2.6046,
        "grad_norm": 1.8449066877365112,
        "learning_rate": 0.00010485935077201914,
        "epoch": 0.164394878149525,
        "step": 1194
    },
    {
        "loss": 1.9652,
        "grad_norm": 1.6729788780212402,
        "learning_rate": 0.00010472928805581834,
        "epoch": 0.16453256230207902,
        "step": 1195
    },
    {
        "loss": 1.5451,
        "grad_norm": 2.4959239959716797,
        "learning_rate": 0.00010459921732097595,
        "epoch": 0.16467024645463307,
        "step": 1196
    },
    {
        "loss": 2.3267,
        "grad_norm": 1.7594738006591797,
        "learning_rate": 0.00010446913878803055,
        "epoch": 0.16480793060718713,
        "step": 1197
    },
    {
        "loss": 2.2332,
        "grad_norm": 1.1620609760284424,
        "learning_rate": 0.00010433905267753401,
        "epoch": 0.16494561475974115,
        "step": 1198
    },
    {
        "loss": 2.0018,
        "grad_norm": 1.9036062955856323,
        "learning_rate": 0.00010420895921005096,
        "epoch": 0.1650832989122952,
        "step": 1199
    },
    {
        "loss": 2.4966,
        "grad_norm": 1.13765549659729,
        "learning_rate": 0.00010407885860615859,
        "epoch": 0.16522098306484923,
        "step": 1200
    },
    {
        "loss": 2.5936,
        "grad_norm": 2.8483643531799316,
        "learning_rate": 0.00010394875108644608,
        "epoch": 0.16535866721740328,
        "step": 1201
    },
    {
        "loss": 1.5244,
        "grad_norm": 1.9298540353775024,
        "learning_rate": 0.0001038186368715145,
        "epoch": 0.16549635136995733,
        "step": 1202
    },
    {
        "loss": 1.7772,
        "grad_norm": 1.7959964275360107,
        "learning_rate": 0.00010368851618197615,
        "epoch": 0.16563403552251135,
        "step": 1203
    },
    {
        "loss": 2.2952,
        "grad_norm": 1.4401371479034424,
        "learning_rate": 0.00010355838923845433,
        "epoch": 0.1657717196750654,
        "step": 1204
    },
    {
        "loss": 2.1997,
        "grad_norm": 1.1226602792739868,
        "learning_rate": 0.00010342825626158294,
        "epoch": 0.16590940382761943,
        "step": 1205
    },
    {
        "loss": 2.5774,
        "grad_norm": 1.2678645849227905,
        "learning_rate": 0.00010329811747200613,
        "epoch": 0.16604708798017348,
        "step": 1206
    },
    {
        "loss": 2.5639,
        "grad_norm": 1.2568048238754272,
        "learning_rate": 0.00010316797309037789,
        "epoch": 0.16618477213272753,
        "step": 1207
    },
    {
        "loss": 2.1415,
        "grad_norm": 1.2722958326339722,
        "learning_rate": 0.00010303782333736172,
        "epoch": 0.16632245628528156,
        "step": 1208
    },
    {
        "loss": 2.175,
        "grad_norm": 1.935787558555603,
        "learning_rate": 0.0001029076684336302,
        "epoch": 0.1664601404378356,
        "step": 1209
    },
    {
        "loss": 2.1541,
        "grad_norm": 1.5837619304656982,
        "learning_rate": 0.00010277750859986465,
        "epoch": 0.16659782459038966,
        "step": 1210
    },
    {
        "loss": 2.086,
        "grad_norm": 1.1687730550765991,
        "learning_rate": 0.00010264734405675473,
        "epoch": 0.16673550874294368,
        "step": 1211
    },
    {
        "loss": 2.2985,
        "grad_norm": 1.152665615081787,
        "learning_rate": 0.00010251717502499811,
        "epoch": 0.16687319289549774,
        "step": 1212
    },
    {
        "loss": 2.4029,
        "grad_norm": 1.6844011545181274,
        "learning_rate": 0.00010238700172530009,
        "epoch": 0.16701087704805176,
        "step": 1213
    },
    {
        "loss": 2.3381,
        "grad_norm": 1.801077127456665,
        "learning_rate": 0.00010225682437837316,
        "epoch": 0.1671485612006058,
        "step": 1214
    },
    {
        "loss": 1.7376,
        "grad_norm": 1.3986085653305054,
        "learning_rate": 0.00010212664320493668,
        "epoch": 0.16728624535315986,
        "step": 1215
    },
    {
        "loss": 2.3144,
        "grad_norm": 1.2277464866638184,
        "learning_rate": 0.0001019964584257165,
        "epoch": 0.1674239295057139,
        "step": 1216
    },
    {
        "loss": 2.427,
        "grad_norm": 1.7858508825302124,
        "learning_rate": 0.00010186627026144466,
        "epoch": 0.16756161365826794,
        "step": 1217
    },
    {
        "loss": 2.3261,
        "grad_norm": 1.7910070419311523,
        "learning_rate": 0.00010173607893285877,
        "epoch": 0.16769929781082196,
        "step": 1218
    },
    {
        "loss": 2.163,
        "grad_norm": 1.8080376386642456,
        "learning_rate": 0.00010160588466070198,
        "epoch": 0.16783698196337601,
        "step": 1219
    },
    {
        "loss": 2.3508,
        "grad_norm": 1.7082713842391968,
        "learning_rate": 0.00010147568766572238,
        "epoch": 0.16797466611593007,
        "step": 1220
    },
    {
        "loss": 2.3891,
        "grad_norm": 1.6826050281524658,
        "learning_rate": 0.0001013454881686726,
        "epoch": 0.1681123502684841,
        "step": 1221
    },
    {
        "loss": 2.2457,
        "grad_norm": 1.5414185523986816,
        "learning_rate": 0.00010121528639030959,
        "epoch": 0.16825003442103814,
        "step": 1222
    },
    {
        "loss": 2.2256,
        "grad_norm": 2.6025264263153076,
        "learning_rate": 0.00010108508255139415,
        "epoch": 0.1683877185735922,
        "step": 1223
    },
    {
        "loss": 2.1736,
        "grad_norm": 1.7695398330688477,
        "learning_rate": 0.00010095487687269054,
        "epoch": 0.16852540272614622,
        "step": 1224
    },
    {
        "loss": 1.3828,
        "grad_norm": 2.2307283878326416,
        "learning_rate": 0.00010082466957496621,
        "epoch": 0.16866308687870027,
        "step": 1225
    },
    {
        "loss": 2.3511,
        "grad_norm": 1.0544698238372803,
        "learning_rate": 0.00010069446087899132,
        "epoch": 0.1688007710312543,
        "step": 1226
    },
    {
        "loss": 2.2825,
        "grad_norm": 1.705548882484436,
        "learning_rate": 0.00010056425100553837,
        "epoch": 0.16893845518380834,
        "step": 1227
    },
    {
        "loss": 2.0692,
        "grad_norm": 1.7768803834915161,
        "learning_rate": 0.0001004340401753819,
        "epoch": 0.1690761393363624,
        "step": 1228
    },
    {
        "loss": 2.4354,
        "grad_norm": 1.8459957838058472,
        "learning_rate": 0.00010030382860929803,
        "epoch": 0.16921382348891642,
        "step": 1229
    },
    {
        "loss": 2.0,
        "grad_norm": 2.179250478744507,
        "learning_rate": 0.00010017361652806412,
        "epoch": 0.16935150764147047,
        "step": 1230
    },
    {
        "loss": 1.7585,
        "grad_norm": 1.4223411083221436,
        "learning_rate": 0.00010004340415245852,
        "epoch": 0.1694891917940245,
        "step": 1231
    },
    {
        "loss": 2.7308,
        "grad_norm": 1.4958686828613281,
        "learning_rate": 9.991319170325997e-05,
        "epoch": 0.16962687594657855,
        "step": 1232
    },
    {
        "loss": 2.0791,
        "grad_norm": 1.492419719696045,
        "learning_rate": 9.97829794012473e-05,
        "epoch": 0.1697645600991326,
        "step": 1233
    },
    {
        "loss": 2.3547,
        "grad_norm": 1.027858018875122,
        "learning_rate": 9.965276746719921e-05,
        "epoch": 0.16990224425168662,
        "step": 1234
    },
    {
        "loss": 1.6169,
        "grad_norm": 1.603248953819275,
        "learning_rate": 9.952255612189368e-05,
        "epoch": 0.17003992840424068,
        "step": 1235
    },
    {
        "loss": 2.2947,
        "grad_norm": 1.3907593488693237,
        "learning_rate": 9.93923455861078e-05,
        "epoch": 0.1701776125567947,
        "step": 1236
    },
    {
        "loss": 2.3602,
        "grad_norm": 1.2981486320495605,
        "learning_rate": 9.926213608061707e-05,
        "epoch": 0.17031529670934875,
        "step": 1237
    },
    {
        "loss": 1.9217,
        "grad_norm": 1.8327302932739258,
        "learning_rate": 9.913192782619556e-05,
        "epoch": 0.1704529808619028,
        "step": 1238
    },
    {
        "loss": 2.5777,
        "grad_norm": 1.2817649841308594,
        "learning_rate": 9.900172104361493e-05,
        "epoch": 0.17059066501445683,
        "step": 1239
    },
    {
        "loss": 1.9956,
        "grad_norm": 1.342190146446228,
        "learning_rate": 9.887151595364447e-05,
        "epoch": 0.17072834916701088,
        "step": 1240
    },
    {
        "loss": 1.6464,
        "grad_norm": 2.7851152420043945,
        "learning_rate": 9.874131277705063e-05,
        "epoch": 0.17086603331956493,
        "step": 1241
    },
    {
        "loss": 2.5575,
        "grad_norm": 1.443621277809143,
        "learning_rate": 9.86111117345966e-05,
        "epoch": 0.17100371747211895,
        "step": 1242
    },
    {
        "loss": 2.6188,
        "grad_norm": 1.1802113056182861,
        "learning_rate": 9.848091304704184e-05,
        "epoch": 0.171141401624673,
        "step": 1243
    },
    {
        "loss": 2.1038,
        "grad_norm": 1.178904414176941,
        "learning_rate": 9.835071693514201e-05,
        "epoch": 0.17127908577722703,
        "step": 1244
    },
    {
        "loss": 1.8474,
        "grad_norm": 1.9438700675964355,
        "learning_rate": 9.82205236196482e-05,
        "epoch": 0.17141676992978108,
        "step": 1245
    },
    {
        "loss": 2.5644,
        "grad_norm": 1.5013654232025146,
        "learning_rate": 9.80903333213069e-05,
        "epoch": 0.17155445408233513,
        "step": 1246
    },
    {
        "loss": 2.1759,
        "grad_norm": 1.739945650100708,
        "learning_rate": 9.796014626085945e-05,
        "epoch": 0.17169213823488916,
        "step": 1247
    },
    {
        "loss": 2.7779,
        "grad_norm": 1.5847071409225464,
        "learning_rate": 9.782996265904174e-05,
        "epoch": 0.1718298223874432,
        "step": 1248
    },
    {
        "loss": 2.5883,
        "grad_norm": 1.3652420043945312,
        "learning_rate": 9.769978273658363e-05,
        "epoch": 0.17196750653999723,
        "step": 1249
    },
    {
        "loss": 2.3265,
        "grad_norm": 1.154919981956482,
        "learning_rate": 9.756960671420896e-05,
        "epoch": 0.17210519069255129,
        "step": 1250
    },
    {
        "loss": 1.9174,
        "grad_norm": 1.73055899143219,
        "learning_rate": 9.743943481263482e-05,
        "epoch": 0.17224287484510534,
        "step": 1251
    },
    {
        "loss": 2.4031,
        "grad_norm": 1.683608889579773,
        "learning_rate": 9.73092672525713e-05,
        "epoch": 0.17238055899765936,
        "step": 1252
    },
    {
        "loss": 2.0665,
        "grad_norm": 1.5589516162872314,
        "learning_rate": 9.717910425472124e-05,
        "epoch": 0.1725182431502134,
        "step": 1253
    },
    {
        "loss": 2.692,
        "grad_norm": 1.5323158502578735,
        "learning_rate": 9.704894603977973e-05,
        "epoch": 0.17265592730276746,
        "step": 1254
    },
    {
        "loss": 2.1838,
        "grad_norm": 1.3151073455810547,
        "learning_rate": 9.691879282843358e-05,
        "epoch": 0.1727936114553215,
        "step": 1255
    },
    {
        "loss": 2.3901,
        "grad_norm": 1.7405146360397339,
        "learning_rate": 9.678864484136135e-05,
        "epoch": 0.17293129560787554,
        "step": 1256
    },
    {
        "loss": 2.4284,
        "grad_norm": 1.132993459701538,
        "learning_rate": 9.665850229923258e-05,
        "epoch": 0.17306897976042956,
        "step": 1257
    },
    {
        "loss": 1.9497,
        "grad_norm": 2.0528852939605713,
        "learning_rate": 9.65283654227076e-05,
        "epoch": 0.17320666391298362,
        "step": 1258
    },
    {
        "loss": 1.1648,
        "grad_norm": 2.8609721660614014,
        "learning_rate": 9.639823443243718e-05,
        "epoch": 0.17334434806553767,
        "step": 1259
    },
    {
        "loss": 2.2064,
        "grad_norm": 1.4918581247329712,
        "learning_rate": 9.626810954906217e-05,
        "epoch": 0.1734820322180917,
        "step": 1260
    },
    {
        "loss": 1.5668,
        "grad_norm": 3.0056405067443848,
        "learning_rate": 9.61379909932129e-05,
        "epoch": 0.17361971637064574,
        "step": 1261
    },
    {
        "loss": 2.4535,
        "grad_norm": 1.5410313606262207,
        "learning_rate": 9.600787898550911e-05,
        "epoch": 0.17375740052319977,
        "step": 1262
    },
    {
        "loss": 2.2753,
        "grad_norm": 0.8633594512939453,
        "learning_rate": 9.587777374655934e-05,
        "epoch": 0.17389508467575382,
        "step": 1263
    },
    {
        "loss": 2.7498,
        "grad_norm": 1.137001395225525,
        "learning_rate": 9.574767549696074e-05,
        "epoch": 0.17403276882830787,
        "step": 1264
    },
    {
        "loss": 1.8267,
        "grad_norm": 1.9697085618972778,
        "learning_rate": 9.56175844572985e-05,
        "epoch": 0.1741704529808619,
        "step": 1265
    },
    {
        "loss": 2.3933,
        "grad_norm": 2.0332863330841064,
        "learning_rate": 9.548750084814582e-05,
        "epoch": 0.17430813713341595,
        "step": 1266
    },
    {
        "loss": 1.9035,
        "grad_norm": 1.709260106086731,
        "learning_rate": 9.535742489006297e-05,
        "epoch": 0.17444582128597,
        "step": 1267
    },
    {
        "loss": 2.3,
        "grad_norm": 1.2195980548858643,
        "learning_rate": 9.52273568035975e-05,
        "epoch": 0.17458350543852402,
        "step": 1268
    },
    {
        "loss": 1.6015,
        "grad_norm": 1.2846996784210205,
        "learning_rate": 9.509729680928349e-05,
        "epoch": 0.17472118959107807,
        "step": 1269
    },
    {
        "loss": 2.6638,
        "grad_norm": 1.425379753112793,
        "learning_rate": 9.496724512764136e-05,
        "epoch": 0.1748588737436321,
        "step": 1270
    },
    {
        "loss": 2.3718,
        "grad_norm": 1.933229684829712,
        "learning_rate": 9.483720197917737e-05,
        "epoch": 0.17499655789618615,
        "step": 1271
    },
    {
        "loss": 2.3205,
        "grad_norm": 1.5621918439865112,
        "learning_rate": 9.470716758438344e-05,
        "epoch": 0.1751342420487402,
        "step": 1272
    },
    {
        "loss": 2.6871,
        "grad_norm": 1.0353878736495972,
        "learning_rate": 9.457714216373648e-05,
        "epoch": 0.17527192620129423,
        "step": 1273
    },
    {
        "loss": 1.9859,
        "grad_norm": 1.399610161781311,
        "learning_rate": 9.444712593769834e-05,
        "epoch": 0.17540961035384828,
        "step": 1274
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.812883973121643,
        "learning_rate": 9.431711912671512e-05,
        "epoch": 0.1755472945064023,
        "step": 1275
    },
    {
        "loss": 2.1141,
        "grad_norm": 1.7957254648208618,
        "learning_rate": 9.418712195121711e-05,
        "epoch": 0.17568497865895635,
        "step": 1276
    },
    {
        "loss": 1.9195,
        "grad_norm": 2.6817684173583984,
        "learning_rate": 9.405713463161816e-05,
        "epoch": 0.1758226628115104,
        "step": 1277
    },
    {
        "loss": 2.4208,
        "grad_norm": 1.9512077569961548,
        "learning_rate": 9.392715738831547e-05,
        "epoch": 0.17596034696406443,
        "step": 1278
    },
    {
        "loss": 2.1484,
        "grad_norm": 2.2615966796875,
        "learning_rate": 9.379719044168915e-05,
        "epoch": 0.17609803111661848,
        "step": 1279
    },
    {
        "loss": 1.8471,
        "grad_norm": 1.516802191734314,
        "learning_rate": 9.366723401210177e-05,
        "epoch": 0.1762357152691725,
        "step": 1280
    },
    {
        "loss": 1.9089,
        "grad_norm": 2.044996738433838,
        "learning_rate": 9.353728831989816e-05,
        "epoch": 0.17637339942172656,
        "step": 1281
    },
    {
        "loss": 1.6793,
        "grad_norm": 2.603919267654419,
        "learning_rate": 9.340735358540494e-05,
        "epoch": 0.1765110835742806,
        "step": 1282
    },
    {
        "loss": 2.1889,
        "grad_norm": 2.1956849098205566,
        "learning_rate": 9.32774300289301e-05,
        "epoch": 0.17664876772683463,
        "step": 1283
    },
    {
        "loss": 2.078,
        "grad_norm": 1.8542677164077759,
        "learning_rate": 9.31475178707627e-05,
        "epoch": 0.17678645187938868,
        "step": 1284
    },
    {
        "loss": 2.1842,
        "grad_norm": 1.7683148384094238,
        "learning_rate": 9.301761733117253e-05,
        "epoch": 0.17692413603194274,
        "step": 1285
    },
    {
        "loss": 1.7008,
        "grad_norm": 1.835843563079834,
        "learning_rate": 9.288772863040954e-05,
        "epoch": 0.17706182018449676,
        "step": 1286
    },
    {
        "loss": 1.802,
        "grad_norm": 1.8788796663284302,
        "learning_rate": 9.275785198870374e-05,
        "epoch": 0.1771995043370508,
        "step": 1287
    },
    {
        "loss": 2.3156,
        "grad_norm": 1.3416274785995483,
        "learning_rate": 9.262798762626467e-05,
        "epoch": 0.17733718848960484,
        "step": 1288
    },
    {
        "loss": 2.1662,
        "grad_norm": 2.2407116889953613,
        "learning_rate": 9.2498135763281e-05,
        "epoch": 0.1774748726421589,
        "step": 1289
    },
    {
        "loss": 1.2927,
        "grad_norm": 2.2228806018829346,
        "learning_rate": 9.236829661992023e-05,
        "epoch": 0.17761255679471294,
        "step": 1290
    },
    {
        "loss": 2.3606,
        "grad_norm": 1.6178146600723267,
        "learning_rate": 9.223847041632832e-05,
        "epoch": 0.17775024094726696,
        "step": 1291
    },
    {
        "loss": 1.813,
        "grad_norm": 2.344618797302246,
        "learning_rate": 9.210865737262924e-05,
        "epoch": 0.17788792509982101,
        "step": 1292
    },
    {
        "loss": 1.8993,
        "grad_norm": 1.5222628116607666,
        "learning_rate": 9.19788577089247e-05,
        "epoch": 0.17802560925237504,
        "step": 1293
    },
    {
        "loss": 2.287,
        "grad_norm": 1.121826171875,
        "learning_rate": 9.184907164529368e-05,
        "epoch": 0.1781632934049291,
        "step": 1294
    },
    {
        "loss": 1.7305,
        "grad_norm": 2.068222761154175,
        "learning_rate": 9.171929940179214e-05,
        "epoch": 0.17830097755748314,
        "step": 1295
    },
    {
        "loss": 1.7341,
        "grad_norm": 2.0747714042663574,
        "learning_rate": 9.158954119845261e-05,
        "epoch": 0.17843866171003717,
        "step": 1296
    },
    {
        "loss": 1.6996,
        "grad_norm": 2.3435258865356445,
        "learning_rate": 9.145979725528373e-05,
        "epoch": 0.17857634586259122,
        "step": 1297
    },
    {
        "loss": 1.8456,
        "grad_norm": 1.7385231256484985,
        "learning_rate": 9.133006779227007e-05,
        "epoch": 0.17871403001514527,
        "step": 1298
    },
    {
        "loss": 2.4595,
        "grad_norm": 1.332651972770691,
        "learning_rate": 9.120035302937163e-05,
        "epoch": 0.1788517141676993,
        "step": 1299
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.5787198543548584,
        "learning_rate": 9.107065318652331e-05,
        "epoch": 0.17898939832025335,
        "step": 1300
    },
    {
        "loss": 1.9714,
        "grad_norm": 1.0675753355026245,
        "learning_rate": 9.094096848363502e-05,
        "epoch": 0.17912708247280737,
        "step": 1301
    },
    {
        "loss": 1.6385,
        "grad_norm": 2.006218671798706,
        "learning_rate": 9.081129914059082e-05,
        "epoch": 0.17926476662536142,
        "step": 1302
    },
    {
        "loss": 1.6481,
        "grad_norm": 2.4118916988372803,
        "learning_rate": 9.068164537724868e-05,
        "epoch": 0.17940245077791547,
        "step": 1303
    },
    {
        "loss": 1.9476,
        "grad_norm": 2.3838417530059814,
        "learning_rate": 9.055200741344026e-05,
        "epoch": 0.1795401349304695,
        "step": 1304
    },
    {
        "loss": 1.571,
        "grad_norm": 1.9674097299575806,
        "learning_rate": 9.042238546897043e-05,
        "epoch": 0.17967781908302355,
        "step": 1305
    },
    {
        "loss": 1.8029,
        "grad_norm": 1.7198470830917358,
        "learning_rate": 9.029277976361674e-05,
        "epoch": 0.17981550323557757,
        "step": 1306
    },
    {
        "loss": 1.7019,
        "grad_norm": 2.2578413486480713,
        "learning_rate": 9.016319051712947e-05,
        "epoch": 0.17995318738813162,
        "step": 1307
    },
    {
        "loss": 1.9722,
        "grad_norm": 2.2842283248901367,
        "learning_rate": 9.003361794923078e-05,
        "epoch": 0.18009087154068568,
        "step": 1308
    },
    {
        "loss": 2.501,
        "grad_norm": 1.6043145656585693,
        "learning_rate": 8.990406227961461e-05,
        "epoch": 0.1802285556932397,
        "step": 1309
    },
    {
        "loss": 2.4082,
        "grad_norm": 1.353048324584961,
        "learning_rate": 8.977452372794629e-05,
        "epoch": 0.18036623984579375,
        "step": 1310
    },
    {
        "loss": 2.2084,
        "grad_norm": 1.8121323585510254,
        "learning_rate": 8.964500251386212e-05,
        "epoch": 0.1805039239983478,
        "step": 1311
    },
    {
        "loss": 1.9753,
        "grad_norm": 2.958906650543213,
        "learning_rate": 8.951549885696889e-05,
        "epoch": 0.18064160815090183,
        "step": 1312
    },
    {
        "loss": 2.3346,
        "grad_norm": 1.0238929986953735,
        "learning_rate": 8.938601297684383e-05,
        "epoch": 0.18077929230345588,
        "step": 1313
    },
    {
        "loss": 1.9331,
        "grad_norm": 2.366440773010254,
        "learning_rate": 8.925654509303382e-05,
        "epoch": 0.1809169764560099,
        "step": 1314
    },
    {
        "loss": 1.6172,
        "grad_norm": 2.2427785396575928,
        "learning_rate": 8.912709542505534e-05,
        "epoch": 0.18105466060856396,
        "step": 1315
    },
    {
        "loss": 2.0315,
        "grad_norm": 1.7187126874923706,
        "learning_rate": 8.899766419239398e-05,
        "epoch": 0.181192344761118,
        "step": 1316
    },
    {
        "loss": 2.2165,
        "grad_norm": 1.3562835454940796,
        "learning_rate": 8.88682516145041e-05,
        "epoch": 0.18133002891367203,
        "step": 1317
    },
    {
        "loss": 2.3483,
        "grad_norm": 1.7495800256729126,
        "learning_rate": 8.873885791080825e-05,
        "epoch": 0.18146771306622608,
        "step": 1318
    },
    {
        "loss": 1.4554,
        "grad_norm": 2.4730844497680664,
        "learning_rate": 8.860948330069721e-05,
        "epoch": 0.1816053972187801,
        "step": 1319
    },
    {
        "loss": 1.9314,
        "grad_norm": 1.6012022495269775,
        "learning_rate": 8.848012800352929e-05,
        "epoch": 0.18174308137133416,
        "step": 1320
    },
    {
        "loss": 2.2652,
        "grad_norm": 1.049569010734558,
        "learning_rate": 8.835079223863006e-05,
        "epoch": 0.1818807655238882,
        "step": 1321
    },
    {
        "loss": 2.2253,
        "grad_norm": 1.607561707496643,
        "learning_rate": 8.822147622529188e-05,
        "epoch": 0.18201844967644223,
        "step": 1322
    },
    {
        "loss": 2.5574,
        "grad_norm": 1.3913346529006958,
        "learning_rate": 8.809218018277384e-05,
        "epoch": 0.1821561338289963,
        "step": 1323
    },
    {
        "loss": 1.8561,
        "grad_norm": 2.066004753112793,
        "learning_rate": 8.79629043303009e-05,
        "epoch": 0.1822938179815503,
        "step": 1324
    },
    {
        "loss": 2.663,
        "grad_norm": 2.1097347736358643,
        "learning_rate": 8.7833648887064e-05,
        "epoch": 0.18243150213410436,
        "step": 1325
    },
    {
        "loss": 1.7696,
        "grad_norm": 1.7627222537994385,
        "learning_rate": 8.770441407221938e-05,
        "epoch": 0.1825691862866584,
        "step": 1326
    },
    {
        "loss": 2.1852,
        "grad_norm": 1.262462854385376,
        "learning_rate": 8.757520010488839e-05,
        "epoch": 0.18270687043921244,
        "step": 1327
    },
    {
        "loss": 2.2491,
        "grad_norm": 1.7245593070983887,
        "learning_rate": 8.744600720415682e-05,
        "epoch": 0.1828445545917665,
        "step": 1328
    },
    {
        "loss": 2.3984,
        "grad_norm": 1.595870852470398,
        "learning_rate": 8.731683558907503e-05,
        "epoch": 0.18298223874432054,
        "step": 1329
    },
    {
        "loss": 1.4656,
        "grad_norm": 1.8455164432525635,
        "learning_rate": 8.718768547865705e-05,
        "epoch": 0.18311992289687457,
        "step": 1330
    },
    {
        "loss": 1.9591,
        "grad_norm": 2.4900119304656982,
        "learning_rate": 8.705855709188055e-05,
        "epoch": 0.18325760704942862,
        "step": 1331
    },
    {
        "loss": 1.9046,
        "grad_norm": 1.7938034534454346,
        "learning_rate": 8.692945064768642e-05,
        "epoch": 0.18339529120198264,
        "step": 1332
    },
    {
        "loss": 1.6381,
        "grad_norm": 1.930599570274353,
        "learning_rate": 8.680036636497831e-05,
        "epoch": 0.1835329753545367,
        "step": 1333
    },
    {
        "loss": 2.0587,
        "grad_norm": 1.9176334142684937,
        "learning_rate": 8.66713044626221e-05,
        "epoch": 0.18367065950709074,
        "step": 1334
    },
    {
        "loss": 2.5979,
        "grad_norm": 1.0325757265090942,
        "learning_rate": 8.65422651594461e-05,
        "epoch": 0.18380834365964477,
        "step": 1335
    },
    {
        "loss": 2.1068,
        "grad_norm": 2.78437876701355,
        "learning_rate": 8.641324867423997e-05,
        "epoch": 0.18394602781219882,
        "step": 1336
    },
    {
        "loss": 1.352,
        "grad_norm": 3.384782075881958,
        "learning_rate": 8.628425522575483e-05,
        "epoch": 0.18408371196475284,
        "step": 1337
    },
    {
        "loss": 1.6828,
        "grad_norm": 2.333134412765503,
        "learning_rate": 8.615528503270274e-05,
        "epoch": 0.1842213961173069,
        "step": 1338
    },
    {
        "loss": 2.5704,
        "grad_norm": 1.0950604677200317,
        "learning_rate": 8.602633831375636e-05,
        "epoch": 0.18435908026986095,
        "step": 1339
    },
    {
        "loss": 2.0077,
        "grad_norm": 1.8973662853240967,
        "learning_rate": 8.589741528754834e-05,
        "epoch": 0.18449676442241497,
        "step": 1340
    },
    {
        "loss": 2.2546,
        "grad_norm": 1.3767114877700806,
        "learning_rate": 8.57685161726715e-05,
        "epoch": 0.18463444857496902,
        "step": 1341
    },
    {
        "loss": 2.0885,
        "grad_norm": 1.3593422174453735,
        "learning_rate": 8.563964118767782e-05,
        "epoch": 0.18477213272752308,
        "step": 1342
    },
    {
        "loss": 1.0151,
        "grad_norm": 3.2416646480560303,
        "learning_rate": 8.551079055107851e-05,
        "epoch": 0.1849098168800771,
        "step": 1343
    },
    {
        "loss": 2.1705,
        "grad_norm": 2.182933807373047,
        "learning_rate": 8.538196448134342e-05,
        "epoch": 0.18504750103263115,
        "step": 1344
    },
    {
        "loss": 2.2064,
        "grad_norm": 2.1048614978790283,
        "learning_rate": 8.525316319690092e-05,
        "epoch": 0.18518518518518517,
        "step": 1345
    },
    {
        "loss": 2.303,
        "grad_norm": 1.3892035484313965,
        "learning_rate": 8.512438691613712e-05,
        "epoch": 0.18532286933773923,
        "step": 1346
    },
    {
        "loss": 1.9509,
        "grad_norm": 2.126918077468872,
        "learning_rate": 8.499563585739585e-05,
        "epoch": 0.18546055349029328,
        "step": 1347
    },
    {
        "loss": 2.4277,
        "grad_norm": 2.102790355682373,
        "learning_rate": 8.486691023897821e-05,
        "epoch": 0.1855982376428473,
        "step": 1348
    },
    {
        "loss": 2.0617,
        "grad_norm": 1.7003705501556396,
        "learning_rate": 8.473821027914212e-05,
        "epoch": 0.18573592179540135,
        "step": 1349
    },
    {
        "loss": 2.203,
        "grad_norm": 0.9952118396759033,
        "learning_rate": 8.460953619610193e-05,
        "epoch": 0.18587360594795538,
        "step": 1350
    },
    {
        "loss": 2.9097,
        "grad_norm": 0.974125862121582,
        "learning_rate": 8.448088820802833e-05,
        "epoch": 0.18601129010050943,
        "step": 1351
    },
    {
        "loss": 1.4825,
        "grad_norm": 1.6689555644989014,
        "learning_rate": 8.435226653304751e-05,
        "epoch": 0.18614897425306348,
        "step": 1352
    },
    {
        "loss": 2.4552,
        "grad_norm": 1.8090403079986572,
        "learning_rate": 8.422367138924117e-05,
        "epoch": 0.1862866584056175,
        "step": 1353
    },
    {
        "loss": 0.9103,
        "grad_norm": 2.094865560531616,
        "learning_rate": 8.409510299464604e-05,
        "epoch": 0.18642434255817156,
        "step": 1354
    },
    {
        "loss": 1.5418,
        "grad_norm": 1.8248673677444458,
        "learning_rate": 8.396656156725345e-05,
        "epoch": 0.1865620267107256,
        "step": 1355
    },
    {
        "loss": 1.993,
        "grad_norm": 1.2280761003494263,
        "learning_rate": 8.383804732500898e-05,
        "epoch": 0.18669971086327963,
        "step": 1356
    },
    {
        "loss": 2.2403,
        "grad_norm": 1.964360237121582,
        "learning_rate": 8.370956048581228e-05,
        "epoch": 0.18683739501583368,
        "step": 1357
    },
    {
        "loss": 2.3022,
        "grad_norm": 1.8002232313156128,
        "learning_rate": 8.358110126751633e-05,
        "epoch": 0.1869750791683877,
        "step": 1358
    },
    {
        "loss": 2.0501,
        "grad_norm": 1.473510503768921,
        "learning_rate": 8.345266988792736e-05,
        "epoch": 0.18711276332094176,
        "step": 1359
    },
    {
        "loss": 1.7477,
        "grad_norm": 1.8785916566848755,
        "learning_rate": 8.332426656480445e-05,
        "epoch": 0.1872504474734958,
        "step": 1360
    },
    {
        "loss": 1.7134,
        "grad_norm": 3.4866135120391846,
        "learning_rate": 8.319589151585905e-05,
        "epoch": 0.18738813162604984,
        "step": 1361
    },
    {
        "loss": 2.2624,
        "grad_norm": 1.9236881732940674,
        "learning_rate": 8.306754495875465e-05,
        "epoch": 0.1875258157786039,
        "step": 1362
    },
    {
        "loss": 2.2932,
        "grad_norm": 1.7118558883666992,
        "learning_rate": 8.29392271111065e-05,
        "epoch": 0.1876634999311579,
        "step": 1363
    },
    {
        "loss": 2.249,
        "grad_norm": 1.8450053930282593,
        "learning_rate": 8.281093819048114e-05,
        "epoch": 0.18780118408371196,
        "step": 1364
    },
    {
        "loss": 1.8206,
        "grad_norm": 2.003126382827759,
        "learning_rate": 8.268267841439606e-05,
        "epoch": 0.18793886823626602,
        "step": 1365
    },
    {
        "loss": 2.2414,
        "grad_norm": 1.9881670475006104,
        "learning_rate": 8.255444800031935e-05,
        "epoch": 0.18807655238882004,
        "step": 1366
    },
    {
        "loss": 2.3883,
        "grad_norm": 1.1400073766708374,
        "learning_rate": 8.242624716566927e-05,
        "epoch": 0.1882142365413741,
        "step": 1367
    },
    {
        "loss": 1.85,
        "grad_norm": 2.3028621673583984,
        "learning_rate": 8.229807612781396e-05,
        "epoch": 0.18835192069392812,
        "step": 1368
    },
    {
        "loss": 2.1671,
        "grad_norm": 2.3282647132873535,
        "learning_rate": 8.216993510407106e-05,
        "epoch": 0.18848960484648217,
        "step": 1369
    },
    {
        "loss": 2.1299,
        "grad_norm": 2.039152145385742,
        "learning_rate": 8.20418243117073e-05,
        "epoch": 0.18862728899903622,
        "step": 1370
    },
    {
        "loss": 2.4959,
        "grad_norm": 2.5169146060943604,
        "learning_rate": 8.191374396793814e-05,
        "epoch": 0.18876497315159024,
        "step": 1371
    },
    {
        "loss": 2.2178,
        "grad_norm": 1.2733393907546997,
        "learning_rate": 8.178569428992742e-05,
        "epoch": 0.1889026573041443,
        "step": 1372
    },
    {
        "loss": 2.2099,
        "grad_norm": 2.000811815261841,
        "learning_rate": 8.1657675494787e-05,
        "epoch": 0.18904034145669835,
        "step": 1373
    },
    {
        "loss": 2.1483,
        "grad_norm": 1.2532519102096558,
        "learning_rate": 8.152968779957635e-05,
        "epoch": 0.18917802560925237,
        "step": 1374
    },
    {
        "loss": 2.3528,
        "grad_norm": 2.5906472206115723,
        "learning_rate": 8.140173142130224e-05,
        "epoch": 0.18931570976180642,
        "step": 1375
    },
    {
        "loss": 2.092,
        "grad_norm": 1.172746181488037,
        "learning_rate": 8.127380657691833e-05,
        "epoch": 0.18945339391436045,
        "step": 1376
    },
    {
        "loss": 2.1959,
        "grad_norm": 1.1196891069412231,
        "learning_rate": 8.114591348332483e-05,
        "epoch": 0.1895910780669145,
        "step": 1377
    },
    {
        "loss": 2.427,
        "grad_norm": 1.471369743347168,
        "learning_rate": 8.101805235736804e-05,
        "epoch": 0.18972876221946855,
        "step": 1378
    },
    {
        "loss": 1.7406,
        "grad_norm": 2.140873432159424,
        "learning_rate": 8.089022341584014e-05,
        "epoch": 0.18986644637202257,
        "step": 1379
    },
    {
        "loss": 2.5021,
        "grad_norm": 1.7083029747009277,
        "learning_rate": 8.076242687547873e-05,
        "epoch": 0.19000413052457663,
        "step": 1380
    },
    {
        "loss": 2.0347,
        "grad_norm": 1.4634289741516113,
        "learning_rate": 8.063466295296645e-05,
        "epoch": 0.19014181467713065,
        "step": 1381
    },
    {
        "loss": 2.1947,
        "grad_norm": 2.0395848751068115,
        "learning_rate": 8.050693186493064e-05,
        "epoch": 0.1902794988296847,
        "step": 1382
    },
    {
        "loss": 1.9551,
        "grad_norm": 1.166164517402649,
        "learning_rate": 8.037923382794297e-05,
        "epoch": 0.19041718298223875,
        "step": 1383
    },
    {
        "loss": 2.6017,
        "grad_norm": 1.8764344453811646,
        "learning_rate": 8.025156905851908e-05,
        "epoch": 0.19055486713479278,
        "step": 1384
    },
    {
        "loss": 1.9338,
        "grad_norm": 1.312501072883606,
        "learning_rate": 8.012393777311816e-05,
        "epoch": 0.19069255128734683,
        "step": 1385
    },
    {
        "loss": 1.2697,
        "grad_norm": 2.953545331954956,
        "learning_rate": 7.999634018814271e-05,
        "epoch": 0.19083023543990088,
        "step": 1386
    },
    {
        "loss": 2.4473,
        "grad_norm": 1.2087427377700806,
        "learning_rate": 7.986877651993802e-05,
        "epoch": 0.1909679195924549,
        "step": 1387
    },
    {
        "loss": 2.156,
        "grad_norm": 1.437476396560669,
        "learning_rate": 7.974124698479192e-05,
        "epoch": 0.19110560374500896,
        "step": 1388
    },
    {
        "loss": 1.9737,
        "grad_norm": 1.9480667114257812,
        "learning_rate": 7.96137517989343e-05,
        "epoch": 0.19124328789756298,
        "step": 1389
    },
    {
        "loss": 1.9175,
        "grad_norm": 1.4276338815689087,
        "learning_rate": 7.948629117853692e-05,
        "epoch": 0.19138097205011703,
        "step": 1390
    },
    {
        "loss": 2.4546,
        "grad_norm": 1.3246026039123535,
        "learning_rate": 7.935886533971274e-05,
        "epoch": 0.19151865620267108,
        "step": 1391
    },
    {
        "loss": 1.9489,
        "grad_norm": 2.4935765266418457,
        "learning_rate": 7.923147449851597e-05,
        "epoch": 0.1916563403552251,
        "step": 1392
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.7407200336456299,
        "learning_rate": 7.91041188709414e-05,
        "epoch": 0.19179402450777916,
        "step": 1393
    },
    {
        "loss": 2.3759,
        "grad_norm": 2.1551735401153564,
        "learning_rate": 7.897679867292407e-05,
        "epoch": 0.19193170866033318,
        "step": 1394
    },
    {
        "loss": 2.1648,
        "grad_norm": 1.7478617429733276,
        "learning_rate": 7.884951412033894e-05,
        "epoch": 0.19206939281288724,
        "step": 1395
    },
    {
        "loss": 2.3233,
        "grad_norm": 1.6227765083312988,
        "learning_rate": 7.872226542900065e-05,
        "epoch": 0.1922070769654413,
        "step": 1396
    },
    {
        "loss": 2.9658,
        "grad_norm": 1.5540271997451782,
        "learning_rate": 7.859505281466282e-05,
        "epoch": 0.1923447611179953,
        "step": 1397
    },
    {
        "loss": 2.1227,
        "grad_norm": 1.619715690612793,
        "learning_rate": 7.846787649301813e-05,
        "epoch": 0.19248244527054936,
        "step": 1398
    },
    {
        "loss": 2.2236,
        "grad_norm": 2.043215274810791,
        "learning_rate": 7.834073667969766e-05,
        "epoch": 0.19262012942310341,
        "step": 1399
    },
    {
        "loss": 2.0686,
        "grad_norm": 1.40790855884552,
        "learning_rate": 7.821363359027048e-05,
        "epoch": 0.19275781357565744,
        "step": 1400
    },
    {
        "loss": 2.2909,
        "grad_norm": 1.449198603630066,
        "learning_rate": 7.80865674402435e-05,
        "epoch": 0.1928954977282115,
        "step": 1401
    },
    {
        "loss": 2.1453,
        "grad_norm": 2.0791356563568115,
        "learning_rate": 7.795953844506102e-05,
        "epoch": 0.19303318188076551,
        "step": 1402
    },
    {
        "loss": 2.9247,
        "grad_norm": 2.0437660217285156,
        "learning_rate": 7.783254682010414e-05,
        "epoch": 0.19317086603331957,
        "step": 1403
    },
    {
        "loss": 2.4875,
        "grad_norm": 1.6172316074371338,
        "learning_rate": 7.770559278069092e-05,
        "epoch": 0.19330855018587362,
        "step": 1404
    },
    {
        "loss": 2.2274,
        "grad_norm": 1.8376171588897705,
        "learning_rate": 7.757867654207548e-05,
        "epoch": 0.19344623433842764,
        "step": 1405
    },
    {
        "loss": 2.2549,
        "grad_norm": 1.0676653385162354,
        "learning_rate": 7.745179831944786e-05,
        "epoch": 0.1935839184909817,
        "step": 1406
    },
    {
        "loss": 2.3072,
        "grad_norm": 1.1365025043487549,
        "learning_rate": 7.732495832793364e-05,
        "epoch": 0.19372160264353572,
        "step": 1407
    },
    {
        "loss": 2.3442,
        "grad_norm": 2.3947222232818604,
        "learning_rate": 7.719815678259373e-05,
        "epoch": 0.19385928679608977,
        "step": 1408
    },
    {
        "loss": 1.2585,
        "grad_norm": 1.8525416851043701,
        "learning_rate": 7.707139389842361e-05,
        "epoch": 0.19399697094864382,
        "step": 1409
    },
    {
        "loss": 1.9549,
        "grad_norm": 2.053149700164795,
        "learning_rate": 7.694466989035335e-05,
        "epoch": 0.19413465510119784,
        "step": 1410
    },
    {
        "loss": 2.426,
        "grad_norm": 1.6668936014175415,
        "learning_rate": 7.681798497324716e-05,
        "epoch": 0.1942723392537519,
        "step": 1411
    },
    {
        "loss": 1.9489,
        "grad_norm": 2.1001765727996826,
        "learning_rate": 7.669133936190289e-05,
        "epoch": 0.19441002340630592,
        "step": 1412
    },
    {
        "loss": 2.2649,
        "grad_norm": 2.5082757472991943,
        "learning_rate": 7.656473327105167e-05,
        "epoch": 0.19454770755885997,
        "step": 1413
    },
    {
        "loss": 2.2541,
        "grad_norm": 0.9550118446350098,
        "learning_rate": 7.643816691535784e-05,
        "epoch": 0.19468539171141402,
        "step": 1414
    },
    {
        "loss": 2.5613,
        "grad_norm": 1.7363277673721313,
        "learning_rate": 7.631164050941811e-05,
        "epoch": 0.19482307586396805,
        "step": 1415
    },
    {
        "loss": 1.886,
        "grad_norm": 1.5179533958435059,
        "learning_rate": 7.618515426776159e-05,
        "epoch": 0.1949607600165221,
        "step": 1416
    },
    {
        "loss": 1.7984,
        "grad_norm": 3.0589771270751953,
        "learning_rate": 7.605870840484933e-05,
        "epoch": 0.19509844416907615,
        "step": 1417
    },
    {
        "loss": 2.2231,
        "grad_norm": 1.5585933923721313,
        "learning_rate": 7.593230313507392e-05,
        "epoch": 0.19523612832163018,
        "step": 1418
    },
    {
        "loss": 1.5412,
        "grad_norm": 2.0459818840026855,
        "learning_rate": 7.580593867275888e-05,
        "epoch": 0.19537381247418423,
        "step": 1419
    },
    {
        "loss": 2.4877,
        "grad_norm": 1.091433048248291,
        "learning_rate": 7.567961523215895e-05,
        "epoch": 0.19551149662673825,
        "step": 1420
    },
    {
        "loss": 2.3852,
        "grad_norm": 1.0265535116195679,
        "learning_rate": 7.555333302745895e-05,
        "epoch": 0.1956491807792923,
        "step": 1421
    },
    {
        "loss": 2.7616,
        "grad_norm": 1.5348693132400513,
        "learning_rate": 7.542709227277393e-05,
        "epoch": 0.19578686493184635,
        "step": 1422
    },
    {
        "loss": 1.5166,
        "grad_norm": 2.2228710651397705,
        "learning_rate": 7.530089318214874e-05,
        "epoch": 0.19592454908440038,
        "step": 1423
    },
    {
        "loss": 1.696,
        "grad_norm": 2.117889642715454,
        "learning_rate": 7.517473596955751e-05,
        "epoch": 0.19606223323695443,
        "step": 1424
    },
    {
        "loss": 2.3639,
        "grad_norm": 1.145782709121704,
        "learning_rate": 7.504862084890327e-05,
        "epoch": 0.19619991738950845,
        "step": 1425
    },
    {
        "loss": 2.555,
        "grad_norm": 1.0064723491668701,
        "learning_rate": 7.492254803401781e-05,
        "epoch": 0.1963376015420625,
        "step": 1426
    },
    {
        "loss": 2.2081,
        "grad_norm": 1.5322058200836182,
        "learning_rate": 7.479651773866117e-05,
        "epoch": 0.19647528569461656,
        "step": 1427
    },
    {
        "loss": 1.2533,
        "grad_norm": 2.5848684310913086,
        "learning_rate": 7.467053017652126e-05,
        "epoch": 0.19661296984717058,
        "step": 1428
    },
    {
        "loss": 2.6491,
        "grad_norm": 1.865909218788147,
        "learning_rate": 7.454458556121354e-05,
        "epoch": 0.19675065399972463,
        "step": 1429
    },
    {
        "loss": 1.9834,
        "grad_norm": 0.9339730143547058,
        "learning_rate": 7.441868410628077e-05,
        "epoch": 0.19688833815227869,
        "step": 1430
    },
    {
        "loss": 1.6457,
        "grad_norm": 1.9499098062515259,
        "learning_rate": 7.429282602519227e-05,
        "epoch": 0.1970260223048327,
        "step": 1431
    },
    {
        "loss": 2.4698,
        "grad_norm": 2.2067692279815674,
        "learning_rate": 7.416701153134409e-05,
        "epoch": 0.19716370645738676,
        "step": 1432
    },
    {
        "loss": 2.5075,
        "grad_norm": 2.0855751037597656,
        "learning_rate": 7.404124083805819e-05,
        "epoch": 0.19730139060994079,
        "step": 1433
    },
    {
        "loss": 2.1745,
        "grad_norm": 1.4073489904403687,
        "learning_rate": 7.391551415858238e-05,
        "epoch": 0.19743907476249484,
        "step": 1434
    },
    {
        "loss": 2.3812,
        "grad_norm": 1.6232107877731323,
        "learning_rate": 7.378983170608978e-05,
        "epoch": 0.1975767589150489,
        "step": 1435
    },
    {
        "loss": 2.2784,
        "grad_norm": 2.3385391235351562,
        "learning_rate": 7.366419369367859e-05,
        "epoch": 0.1977144430676029,
        "step": 1436
    },
    {
        "loss": 2.4531,
        "grad_norm": 1.3022654056549072,
        "learning_rate": 7.353860033437153e-05,
        "epoch": 0.19785212722015696,
        "step": 1437
    },
    {
        "loss": 2.1546,
        "grad_norm": 2.095672130584717,
        "learning_rate": 7.341305184111575e-05,
        "epoch": 0.197989811372711,
        "step": 1438
    },
    {
        "loss": 2.1823,
        "grad_norm": 1.6495493650436401,
        "learning_rate": 7.328754842678222e-05,
        "epoch": 0.19812749552526504,
        "step": 1439
    },
    {
        "loss": 2.3595,
        "grad_norm": 1.649314284324646,
        "learning_rate": 7.316209030416558e-05,
        "epoch": 0.1982651796778191,
        "step": 1440
    },
    {
        "loss": 2.6065,
        "grad_norm": 1.4854289293289185,
        "learning_rate": 7.30366776859836e-05,
        "epoch": 0.19840286383037312,
        "step": 1441
    },
    {
        "loss": 2.2194,
        "grad_norm": 1.3371156454086304,
        "learning_rate": 7.291131078487697e-05,
        "epoch": 0.19854054798292717,
        "step": 1442
    },
    {
        "loss": 2.3231,
        "grad_norm": 1.6552133560180664,
        "learning_rate": 7.278598981340873e-05,
        "epoch": 0.19867823213548122,
        "step": 1443
    },
    {
        "loss": 2.2238,
        "grad_norm": 0.8620926141738892,
        "learning_rate": 7.266071498406417e-05,
        "epoch": 0.19881591628803524,
        "step": 1444
    },
    {
        "loss": 2.4592,
        "grad_norm": 1.4197769165039062,
        "learning_rate": 7.25354865092503e-05,
        "epoch": 0.1989536004405893,
        "step": 1445
    },
    {
        "loss": 2.2904,
        "grad_norm": 1.5673824548721313,
        "learning_rate": 7.241030460129557e-05,
        "epoch": 0.19909128459314332,
        "step": 1446
    },
    {
        "loss": 0.9345,
        "grad_norm": 1.3194173574447632,
        "learning_rate": 7.228516947244938e-05,
        "epoch": 0.19922896874569737,
        "step": 1447
    },
    {
        "loss": 2.1559,
        "grad_norm": 1.2466049194335938,
        "learning_rate": 7.216008133488192e-05,
        "epoch": 0.19936665289825142,
        "step": 1448
    },
    {
        "loss": 2.2916,
        "grad_norm": 2.1556191444396973,
        "learning_rate": 7.203504040068368e-05,
        "epoch": 0.19950433705080545,
        "step": 1449
    },
    {
        "loss": 1.4109,
        "grad_norm": 2.4005179405212402,
        "learning_rate": 7.191004688186507e-05,
        "epoch": 0.1996420212033595,
        "step": 1450
    },
    {
        "loss": 2.2199,
        "grad_norm": 1.4080981016159058,
        "learning_rate": 7.178510099035614e-05,
        "epoch": 0.19977970535591352,
        "step": 1451
    },
    {
        "loss": 1.6856,
        "grad_norm": 1.454757571220398,
        "learning_rate": 7.166020293800622e-05,
        "epoch": 0.19991738950846757,
        "step": 1452
    },
    {
        "loss": 2.2035,
        "grad_norm": 1.2761836051940918,
        "learning_rate": 7.153535293658343e-05,
        "epoch": 0.20005507366102163,
        "step": 1453
    },
    {
        "loss": 2.4891,
        "grad_norm": 1.8411144018173218,
        "learning_rate": 7.141055119777448e-05,
        "epoch": 0.20019275781357565,
        "step": 1454
    },
    {
        "loss": 2.1789,
        "grad_norm": 2.40805721282959,
        "learning_rate": 7.128579793318428e-05,
        "epoch": 0.2003304419661297,
        "step": 1455
    },
    {
        "loss": 2.4375,
        "grad_norm": 2.1545567512512207,
        "learning_rate": 7.116109335433551e-05,
        "epoch": 0.20046812611868373,
        "step": 1456
    },
    {
        "loss": 1.9358,
        "grad_norm": 2.3752357959747314,
        "learning_rate": 7.103643767266832e-05,
        "epoch": 0.20060581027123778,
        "step": 1457
    },
    {
        "loss": 1.6829,
        "grad_norm": 2.23740816116333,
        "learning_rate": 7.091183109953994e-05,
        "epoch": 0.20074349442379183,
        "step": 1458
    },
    {
        "loss": 1.4737,
        "grad_norm": 2.476945400238037,
        "learning_rate": 7.07872738462243e-05,
        "epoch": 0.20088117857634585,
        "step": 1459
    },
    {
        "loss": 2.4485,
        "grad_norm": 1.4985905885696411,
        "learning_rate": 7.066276612391176e-05,
        "epoch": 0.2010188627288999,
        "step": 1460
    },
    {
        "loss": 1.6096,
        "grad_norm": 2.564621925354004,
        "learning_rate": 7.05383081437087e-05,
        "epoch": 0.20115654688145396,
        "step": 1461
    },
    {
        "loss": 1.4144,
        "grad_norm": 2.9101665019989014,
        "learning_rate": 7.041390011663716e-05,
        "epoch": 0.20129423103400798,
        "step": 1462
    },
    {
        "loss": 2.519,
        "grad_norm": 1.2223155498504639,
        "learning_rate": 7.028954225363444e-05,
        "epoch": 0.20143191518656203,
        "step": 1463
    },
    {
        "loss": 2.3026,
        "grad_norm": 1.087032675743103,
        "learning_rate": 7.01652347655528e-05,
        "epoch": 0.20156959933911606,
        "step": 1464
    },
    {
        "loss": 2.4714,
        "grad_norm": 1.2236067056655884,
        "learning_rate": 7.004097786315911e-05,
        "epoch": 0.2017072834916701,
        "step": 1465
    },
    {
        "loss": 1.7472,
        "grad_norm": 2.4222896099090576,
        "learning_rate": 6.991677175713449e-05,
        "epoch": 0.20184496764422416,
        "step": 1466
    },
    {
        "loss": 2.1413,
        "grad_norm": 1.9635807275772095,
        "learning_rate": 6.979261665807389e-05,
        "epoch": 0.20198265179677818,
        "step": 1467
    },
    {
        "loss": 2.3662,
        "grad_norm": 1.026558518409729,
        "learning_rate": 6.966851277648577e-05,
        "epoch": 0.20212033594933224,
        "step": 1468
    },
    {
        "loss": 2.003,
        "grad_norm": 1.122751235961914,
        "learning_rate": 6.95444603227918e-05,
        "epoch": 0.20225802010188626,
        "step": 1469
    },
    {
        "loss": 1.8844,
        "grad_norm": 2.1789839267730713,
        "learning_rate": 6.942045950732638e-05,
        "epoch": 0.2023957042544403,
        "step": 1470
    },
    {
        "loss": 2.4006,
        "grad_norm": 1.7342087030410767,
        "learning_rate": 6.929651054033643e-05,
        "epoch": 0.20253338840699436,
        "step": 1471
    },
    {
        "loss": 2.2014,
        "grad_norm": 2.491499423980713,
        "learning_rate": 6.917261363198093e-05,
        "epoch": 0.2026710725595484,
        "step": 1472
    },
    {
        "loss": 1.6963,
        "grad_norm": 2.808819055557251,
        "learning_rate": 6.904876899233057e-05,
        "epoch": 0.20280875671210244,
        "step": 1473
    },
    {
        "loss": 2.3804,
        "grad_norm": 1.4662885665893555,
        "learning_rate": 6.89249768313675e-05,
        "epoch": 0.2029464408646565,
        "step": 1474
    },
    {
        "loss": 2.0463,
        "grad_norm": 1.4572322368621826,
        "learning_rate": 6.880123735898476e-05,
        "epoch": 0.20308412501721052,
        "step": 1475
    },
    {
        "loss": 2.1493,
        "grad_norm": 2.9709291458129883,
        "learning_rate": 6.86775507849861e-05,
        "epoch": 0.20322180916976457,
        "step": 1476
    },
    {
        "loss": 1.9419,
        "grad_norm": 1.187311053276062,
        "learning_rate": 6.855391731908567e-05,
        "epoch": 0.2033594933223186,
        "step": 1477
    },
    {
        "loss": 1.8811,
        "grad_norm": 2.9217116832733154,
        "learning_rate": 6.84303371709075e-05,
        "epoch": 0.20349717747487264,
        "step": 1478
    },
    {
        "loss": 1.9502,
        "grad_norm": 1.5985734462738037,
        "learning_rate": 6.830681054998518e-05,
        "epoch": 0.2036348616274267,
        "step": 1479
    },
    {
        "loss": 2.0221,
        "grad_norm": 1.4901238679885864,
        "learning_rate": 6.818333766576167e-05,
        "epoch": 0.20377254577998072,
        "step": 1480
    },
    {
        "loss": 2.5578,
        "grad_norm": 1.4525758028030396,
        "learning_rate": 6.805991872758867e-05,
        "epoch": 0.20391022993253477,
        "step": 1481
    },
    {
        "loss": 1.7666,
        "grad_norm": 1.771148443222046,
        "learning_rate": 6.793655394472644e-05,
        "epoch": 0.2040479140850888,
        "step": 1482
    },
    {
        "loss": 1.5283,
        "grad_norm": 2.756350040435791,
        "learning_rate": 6.781324352634355e-05,
        "epoch": 0.20418559823764285,
        "step": 1483
    },
    {
        "loss": 2.2307,
        "grad_norm": 1.096900224685669,
        "learning_rate": 6.768998768151627e-05,
        "epoch": 0.2043232823901969,
        "step": 1484
    },
    {
        "loss": 1.0024,
        "grad_norm": 2.6025469303131104,
        "learning_rate": 6.756678661922836e-05,
        "epoch": 0.20446096654275092,
        "step": 1485
    },
    {
        "loss": 2.1146,
        "grad_norm": 1.7501553297042847,
        "learning_rate": 6.744364054837068e-05,
        "epoch": 0.20459865069530497,
        "step": 1486
    },
    {
        "loss": 2.4539,
        "grad_norm": 1.824942946434021,
        "learning_rate": 6.732054967774093e-05,
        "epoch": 0.20473633484785903,
        "step": 1487
    },
    {
        "loss": 2.3009,
        "grad_norm": 1.4739036560058594,
        "learning_rate": 6.719751421604312e-05,
        "epoch": 0.20487401900041305,
        "step": 1488
    },
    {
        "loss": 2.4272,
        "grad_norm": 2.709238052368164,
        "learning_rate": 6.707453437188732e-05,
        "epoch": 0.2050117031529671,
        "step": 1489
    },
    {
        "loss": 1.4584,
        "grad_norm": 2.1310458183288574,
        "learning_rate": 6.69516103537894e-05,
        "epoch": 0.20514938730552112,
        "step": 1490
    },
    {
        "loss": 1.7127,
        "grad_norm": 3.0923895835876465,
        "learning_rate": 6.682874237017055e-05,
        "epoch": 0.20528707145807518,
        "step": 1491
    },
    {
        "loss": 2.6841,
        "grad_norm": 1.1180301904678345,
        "learning_rate": 6.670593062935677e-05,
        "epoch": 0.20542475561062923,
        "step": 1492
    },
    {
        "loss": 2.1229,
        "grad_norm": 2.892059803009033,
        "learning_rate": 6.658317533957898e-05,
        "epoch": 0.20556243976318325,
        "step": 1493
    },
    {
        "loss": 2.2768,
        "grad_norm": 1.1741737127304077,
        "learning_rate": 6.646047670897213e-05,
        "epoch": 0.2057001239157373,
        "step": 1494
    },
    {
        "loss": 2.1695,
        "grad_norm": 1.1917564868927002,
        "learning_rate": 6.633783494557522e-05,
        "epoch": 0.20583780806829133,
        "step": 1495
    },
    {
        "loss": 2.2391,
        "grad_norm": 1.1953691244125366,
        "learning_rate": 6.621525025733093e-05,
        "epoch": 0.20597549222084538,
        "step": 1496
    },
    {
        "loss": 2.654,
        "grad_norm": 1.5528208017349243,
        "learning_rate": 6.609272285208504e-05,
        "epoch": 0.20611317637339943,
        "step": 1497
    },
    {
        "loss": 1.6854,
        "grad_norm": 2.1122469902038574,
        "learning_rate": 6.597025293758613e-05,
        "epoch": 0.20625086052595346,
        "step": 1498
    },
    {
        "loss": 1.8173,
        "grad_norm": 1.190330982208252,
        "learning_rate": 6.584784072148555e-05,
        "epoch": 0.2063885446785075,
        "step": 1499
    },
    {
        "loss": 2.4771,
        "grad_norm": 1.9346848726272583,
        "learning_rate": 6.572548641133654e-05,
        "epoch": 0.20652622883106153,
        "step": 1500
    },
    {
        "loss": 2.1017,
        "grad_norm": 1.9206734895706177,
        "learning_rate": 6.56031902145943e-05,
        "epoch": 0.20666391298361558,
        "step": 1501
    },
    {
        "loss": 1.3918,
        "grad_norm": 2.644057035446167,
        "learning_rate": 6.54809523386156e-05,
        "epoch": 0.20680159713616963,
        "step": 1502
    },
    {
        "loss": 1.6461,
        "grad_norm": 1.6642168760299683,
        "learning_rate": 6.535877299065813e-05,
        "epoch": 0.20693928128872366,
        "step": 1503
    },
    {
        "loss": 1.9816,
        "grad_norm": 1.533657193183899,
        "learning_rate": 6.523665237788043e-05,
        "epoch": 0.2070769654412777,
        "step": 1504
    },
    {
        "loss": 2.4831,
        "grad_norm": 1.457322359085083,
        "learning_rate": 6.511459070734153e-05,
        "epoch": 0.20721464959383176,
        "step": 1505
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.5231008529663086,
        "learning_rate": 6.499258818600033e-05,
        "epoch": 0.2073523337463858,
        "step": 1506
    },
    {
        "loss": 2.3484,
        "grad_norm": 1.218937635421753,
        "learning_rate": 6.487064502071558e-05,
        "epoch": 0.20749001789893984,
        "step": 1507
    },
    {
        "loss": 2.3565,
        "grad_norm": 1.091773509979248,
        "learning_rate": 6.474876141824547e-05,
        "epoch": 0.20762770205149386,
        "step": 1508
    },
    {
        "loss": 1.8815,
        "grad_norm": 1.795405387878418,
        "learning_rate": 6.462693758524698e-05,
        "epoch": 0.2077653862040479,
        "step": 1509
    },
    {
        "loss": 1.7341,
        "grad_norm": 2.257465124130249,
        "learning_rate": 6.450517372827591e-05,
        "epoch": 0.20790307035660197,
        "step": 1510
    },
    {
        "loss": 1.4621,
        "grad_norm": 2.735502243041992,
        "learning_rate": 6.438347005378628e-05,
        "epoch": 0.208040754509156,
        "step": 1511
    },
    {
        "loss": 2.2927,
        "grad_norm": 1.2609742879867554,
        "learning_rate": 6.426182676813017e-05,
        "epoch": 0.20817843866171004,
        "step": 1512
    },
    {
        "loss": 2.0854,
        "grad_norm": 1.762078046798706,
        "learning_rate": 6.414024407755716e-05,
        "epoch": 0.20831612281426407,
        "step": 1513
    },
    {
        "loss": 2.1332,
        "grad_norm": 1.8616759777069092,
        "learning_rate": 6.401872218821415e-05,
        "epoch": 0.20845380696681812,
        "step": 1514
    },
    {
        "loss": 1.9848,
        "grad_norm": 2.219782829284668,
        "learning_rate": 6.389726130614499e-05,
        "epoch": 0.20859149111937217,
        "step": 1515
    },
    {
        "loss": 1.5273,
        "grad_norm": 2.495055913925171,
        "learning_rate": 6.377586163728999e-05,
        "epoch": 0.2087291752719262,
        "step": 1516
    },
    {
        "loss": 2.1174,
        "grad_norm": 1.8948001861572266,
        "learning_rate": 6.365452338748572e-05,
        "epoch": 0.20886685942448024,
        "step": 1517
    },
    {
        "loss": 2.0473,
        "grad_norm": 1.9243602752685547,
        "learning_rate": 6.353324676246462e-05,
        "epoch": 0.2090045435770343,
        "step": 1518
    },
    {
        "loss": 2.4599,
        "grad_norm": 1.2075961828231812,
        "learning_rate": 6.341203196785463e-05,
        "epoch": 0.20914222772958832,
        "step": 1519
    },
    {
        "loss": 2.0782,
        "grad_norm": 2.1162638664245605,
        "learning_rate": 6.329087920917886e-05,
        "epoch": 0.20927991188214237,
        "step": 1520
    },
    {
        "loss": 2.3148,
        "grad_norm": 1.2682877779006958,
        "learning_rate": 6.316978869185532e-05,
        "epoch": 0.2094175960346964,
        "step": 1521
    },
    {
        "loss": 2.5173,
        "grad_norm": 1.5983142852783203,
        "learning_rate": 6.304876062119629e-05,
        "epoch": 0.20955528018725045,
        "step": 1522
    },
    {
        "loss": 2.238,
        "grad_norm": 1.8705660104751587,
        "learning_rate": 6.292779520240832e-05,
        "epoch": 0.2096929643398045,
        "step": 1523
    },
    {
        "loss": 2.4277,
        "grad_norm": 1.5999219417572021,
        "learning_rate": 6.280689264059171e-05,
        "epoch": 0.20983064849235852,
        "step": 1524
    },
    {
        "loss": 1.935,
        "grad_norm": 2.3132946491241455,
        "learning_rate": 6.268605314074017e-05,
        "epoch": 0.20996833264491258,
        "step": 1525
    },
    {
        "loss": 2.2585,
        "grad_norm": 1.8030158281326294,
        "learning_rate": 6.256527690774048e-05,
        "epoch": 0.2101060167974666,
        "step": 1526
    },
    {
        "loss": 2.3545,
        "grad_norm": 1.7929279804229736,
        "learning_rate": 6.244456414637221e-05,
        "epoch": 0.21024370095002065,
        "step": 1527
    },
    {
        "loss": 1.8486,
        "grad_norm": 1.6052441596984863,
        "learning_rate": 6.232391506130718e-05,
        "epoch": 0.2103813851025747,
        "step": 1528
    },
    {
        "loss": 2.1213,
        "grad_norm": 2.421212673187256,
        "learning_rate": 6.220332985710936e-05,
        "epoch": 0.21051906925512873,
        "step": 1529
    },
    {
        "loss": 2.1506,
        "grad_norm": 1.677616834640503,
        "learning_rate": 6.208280873823437e-05,
        "epoch": 0.21065675340768278,
        "step": 1530
    },
    {
        "loss": 2.1052,
        "grad_norm": 1.6052124500274658,
        "learning_rate": 6.19623519090292e-05,
        "epoch": 0.21079443756023683,
        "step": 1531
    },
    {
        "loss": 1.9076,
        "grad_norm": 2.2515792846679688,
        "learning_rate": 6.184195957373176e-05,
        "epoch": 0.21093212171279085,
        "step": 1532
    },
    {
        "loss": 2.5021,
        "grad_norm": 1.3007715940475464,
        "learning_rate": 6.172163193647071e-05,
        "epoch": 0.2110698058653449,
        "step": 1533
    },
    {
        "loss": 2.4958,
        "grad_norm": 1.7333004474639893,
        "learning_rate": 6.160136920126492e-05,
        "epoch": 0.21120749001789893,
        "step": 1534
    },
    {
        "loss": 1.9236,
        "grad_norm": 2.0778396129608154,
        "learning_rate": 6.148117157202326e-05,
        "epoch": 0.21134517417045298,
        "step": 1535
    },
    {
        "loss": 2.5656,
        "grad_norm": 1.8695611953735352,
        "learning_rate": 6.136103925254425e-05,
        "epoch": 0.21148285832300703,
        "step": 1536
    },
    {
        "loss": 2.6902,
        "grad_norm": 1.3262641429901123,
        "learning_rate": 6.124097244651558e-05,
        "epoch": 0.21162054247556106,
        "step": 1537
    },
    {
        "loss": 1.4735,
        "grad_norm": 2.6830759048461914,
        "learning_rate": 6.112097135751398e-05,
        "epoch": 0.2117582266281151,
        "step": 1538
    },
    {
        "loss": 2.0347,
        "grad_norm": 1.1634693145751953,
        "learning_rate": 6.1001036189004634e-05,
        "epoch": 0.21189591078066913,
        "step": 1539
    },
    {
        "loss": 2.1493,
        "grad_norm": 1.2634754180908203,
        "learning_rate": 6.088116714434101e-05,
        "epoch": 0.21203359493322319,
        "step": 1540
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.5665030479431152,
        "learning_rate": 6.076136442676449e-05,
        "epoch": 0.21217127908577724,
        "step": 1541
    },
    {
        "loss": 2.3673,
        "grad_norm": 1.5047537088394165,
        "learning_rate": 6.064162823940396e-05,
        "epoch": 0.21230896323833126,
        "step": 1542
    },
    {
        "loss": 1.6888,
        "grad_norm": 2.4044361114501953,
        "learning_rate": 6.05219587852755e-05,
        "epoch": 0.2124466473908853,
        "step": 1543
    },
    {
        "loss": 2.0778,
        "grad_norm": 1.1244920492172241,
        "learning_rate": 6.0402356267282076e-05,
        "epoch": 0.21258433154343934,
        "step": 1544
    },
    {
        "loss": 2.3235,
        "grad_norm": 2.1303865909576416,
        "learning_rate": 6.0282820888213134e-05,
        "epoch": 0.2127220156959934,
        "step": 1545
    },
    {
        "loss": 2.2929,
        "grad_norm": 1.8863404989242554,
        "learning_rate": 6.016335285074426e-05,
        "epoch": 0.21285969984854744,
        "step": 1546
    },
    {
        "loss": 1.7161,
        "grad_norm": 2.2890498638153076,
        "learning_rate": 6.00439523574369e-05,
        "epoch": 0.21299738400110146,
        "step": 1547
    },
    {
        "loss": 1.5419,
        "grad_norm": 2.791041374206543,
        "learning_rate": 5.9924619610737986e-05,
        "epoch": 0.21313506815365552,
        "step": 1548
    },
    {
        "loss": 2.4182,
        "grad_norm": 1.6083213090896606,
        "learning_rate": 5.9805354812979576e-05,
        "epoch": 0.21327275230620957,
        "step": 1549
    },
    {
        "loss": 2.0352,
        "grad_norm": 2.5585761070251465,
        "learning_rate": 5.968615816637845e-05,
        "epoch": 0.2134104364587636,
        "step": 1550
    },
    {
        "loss": 2.147,
        "grad_norm": 1.757284164428711,
        "learning_rate": 5.956702987303593e-05,
        "epoch": 0.21354812061131764,
        "step": 1551
    },
    {
        "loss": 2.3492,
        "grad_norm": 1.4041306972503662,
        "learning_rate": 5.9447970134937406e-05,
        "epoch": 0.21368580476387167,
        "step": 1552
    },
    {
        "loss": 2.1034,
        "grad_norm": 2.077172040939331,
        "learning_rate": 5.932897915395202e-05,
        "epoch": 0.21382348891642572,
        "step": 1553
    },
    {
        "loss": 2.3546,
        "grad_norm": 1.794504165649414,
        "learning_rate": 5.9210057131832384e-05,
        "epoch": 0.21396117306897977,
        "step": 1554
    },
    {
        "loss": 2.228,
        "grad_norm": 1.3231618404388428,
        "learning_rate": 5.909120427021407e-05,
        "epoch": 0.2140988572215338,
        "step": 1555
    },
    {
        "loss": 2.3057,
        "grad_norm": 1.7341699600219727,
        "learning_rate": 5.8972420770615525e-05,
        "epoch": 0.21423654137408785,
        "step": 1556
    },
    {
        "loss": 1.8747,
        "grad_norm": 1.4100689888000488,
        "learning_rate": 5.8853706834437505e-05,
        "epoch": 0.21437422552664187,
        "step": 1557
    },
    {
        "loss": 1.6108,
        "grad_norm": 1.8593311309814453,
        "learning_rate": 5.873506266296287e-05,
        "epoch": 0.21451190967919592,
        "step": 1558
    },
    {
        "loss": 2.5756,
        "grad_norm": 1.4734574556350708,
        "learning_rate": 5.861648845735614e-05,
        "epoch": 0.21464959383174997,
        "step": 1559
    },
    {
        "loss": 1.9066,
        "grad_norm": 1.8237626552581787,
        "learning_rate": 5.849798441866329e-05,
        "epoch": 0.214787277984304,
        "step": 1560
    },
    {
        "loss": 1.9751,
        "grad_norm": 1.325835943222046,
        "learning_rate": 5.837955074781114e-05,
        "epoch": 0.21492496213685805,
        "step": 1561
    },
    {
        "loss": 2.6004,
        "grad_norm": 1.1749347448349,
        "learning_rate": 5.826118764560745e-05,
        "epoch": 0.2150626462894121,
        "step": 1562
    },
    {
        "loss": 2.8921,
        "grad_norm": 1.5567667484283447,
        "learning_rate": 5.8142895312740205e-05,
        "epoch": 0.21520033044196613,
        "step": 1563
    },
    {
        "loss": 2.0568,
        "grad_norm": 1.9610235691070557,
        "learning_rate": 5.802467394977733e-05,
        "epoch": 0.21533801459452018,
        "step": 1564
    },
    {
        "loss": 1.6408,
        "grad_norm": 2.5793559551239014,
        "learning_rate": 5.790652375716652e-05,
        "epoch": 0.2154756987470742,
        "step": 1565
    },
    {
        "loss": 2.2184,
        "grad_norm": 2.2437527179718018,
        "learning_rate": 5.77884449352348e-05,
        "epoch": 0.21561338289962825,
        "step": 1566
    },
    {
        "loss": 2.2304,
        "grad_norm": 1.141921043395996,
        "learning_rate": 5.767043768418804e-05,
        "epoch": 0.2157510670521823,
        "step": 1567
    },
    {
        "loss": 2.0914,
        "grad_norm": 1.6875836849212646,
        "learning_rate": 5.7552502204110993e-05,
        "epoch": 0.21588875120473633,
        "step": 1568
    },
    {
        "loss": 2.0876,
        "grad_norm": 1.8628417253494263,
        "learning_rate": 5.743463869496655e-05,
        "epoch": 0.21602643535729038,
        "step": 1569
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.5277119874954224,
        "learning_rate": 5.731684735659563e-05,
        "epoch": 0.2161641195098444,
        "step": 1570
    },
    {
        "loss": 2.6355,
        "grad_norm": 1.285644292831421,
        "learning_rate": 5.719912838871677e-05,
        "epoch": 0.21630180366239846,
        "step": 1571
    },
    {
        "loss": 2.4711,
        "grad_norm": 1.734896183013916,
        "learning_rate": 5.708148199092587e-05,
        "epoch": 0.2164394878149525,
        "step": 1572
    },
    {
        "loss": 2.2462,
        "grad_norm": 2.4049184322357178,
        "learning_rate": 5.6963908362695605e-05,
        "epoch": 0.21657717196750653,
        "step": 1573
    },
    {
        "loss": 2.4697,
        "grad_norm": 1.2354305982589722,
        "learning_rate": 5.684640770337542e-05,
        "epoch": 0.21671485612006058,
        "step": 1574
    },
    {
        "loss": 2.1471,
        "grad_norm": 1.856273889541626,
        "learning_rate": 5.672898021219111e-05,
        "epoch": 0.21685254027261464,
        "step": 1575
    },
    {
        "loss": 2.3469,
        "grad_norm": 1.0063985586166382,
        "learning_rate": 5.6611626088244194e-05,
        "epoch": 0.21699022442516866,
        "step": 1576
    },
    {
        "loss": 1.7392,
        "grad_norm": 1.5948947668075562,
        "learning_rate": 5.6494345530511936e-05,
        "epoch": 0.2171279085777227,
        "step": 1577
    },
    {
        "loss": 2.0257,
        "grad_norm": 1.7514163255691528,
        "learning_rate": 5.6377138737846894e-05,
        "epoch": 0.21726559273027674,
        "step": 1578
    },
    {
        "loss": 1.8133,
        "grad_norm": 1.6232596635818481,
        "learning_rate": 5.626000590897641e-05,
        "epoch": 0.2174032768828308,
        "step": 1579
    },
    {
        "loss": 1.9129,
        "grad_norm": 1.2853682041168213,
        "learning_rate": 5.61429472425025e-05,
        "epoch": 0.21754096103538484,
        "step": 1580
    },
    {
        "loss": 1.8162,
        "grad_norm": 1.7441140413284302,
        "learning_rate": 5.602596293690152e-05,
        "epoch": 0.21767864518793886,
        "step": 1581
    },
    {
        "loss": 1.6633,
        "grad_norm": 1.2555711269378662,
        "learning_rate": 5.5909053190523684e-05,
        "epoch": 0.21781632934049291,
        "step": 1582
    },
    {
        "loss": 2.4931,
        "grad_norm": 2.0836970806121826,
        "learning_rate": 5.579221820159265e-05,
        "epoch": 0.21795401349304694,
        "step": 1583
    },
    {
        "loss": 2.1594,
        "grad_norm": 1.8161537647247314,
        "learning_rate": 5.567545816820558e-05,
        "epoch": 0.218091697645601,
        "step": 1584
    },
    {
        "loss": 2.2255,
        "grad_norm": 1.6896311044692993,
        "learning_rate": 5.555877328833233e-05,
        "epoch": 0.21822938179815504,
        "step": 1585
    },
    {
        "loss": 1.9699,
        "grad_norm": 1.5719130039215088,
        "learning_rate": 5.5442163759815436e-05,
        "epoch": 0.21836706595070907,
        "step": 1586
    },
    {
        "loss": 2.0539,
        "grad_norm": 1.868292212486267,
        "learning_rate": 5.5325629780369635e-05,
        "epoch": 0.21850475010326312,
        "step": 1587
    },
    {
        "loss": 2.2527,
        "grad_norm": 1.8636959791183472,
        "learning_rate": 5.520917154758164e-05,
        "epoch": 0.21864243425581714,
        "step": 1588
    },
    {
        "loss": 2.1236,
        "grad_norm": 1.6386175155639648,
        "learning_rate": 5.509278925890955e-05,
        "epoch": 0.2187801184083712,
        "step": 1589
    },
    {
        "loss": 2.2104,
        "grad_norm": 1.790224313735962,
        "learning_rate": 5.4976483111683e-05,
        "epoch": 0.21891780256092525,
        "step": 1590
    },
    {
        "loss": 2.1356,
        "grad_norm": 1.8219801187515259,
        "learning_rate": 5.4860253303102214e-05,
        "epoch": 0.21905548671347927,
        "step": 1591
    },
    {
        "loss": 2.5395,
        "grad_norm": 1.3619396686553955,
        "learning_rate": 5.474410003023813e-05,
        "epoch": 0.21919317086603332,
        "step": 1592
    },
    {
        "loss": 2.404,
        "grad_norm": 0.9648406505584717,
        "learning_rate": 5.4628023490031974e-05,
        "epoch": 0.21933085501858737,
        "step": 1593
    },
    {
        "loss": 1.2518,
        "grad_norm": 1.1135618686676025,
        "learning_rate": 5.451202387929482e-05,
        "epoch": 0.2194685391711414,
        "step": 1594
    },
    {
        "loss": 2.2449,
        "grad_norm": 1.7227199077606201,
        "learning_rate": 5.439610139470719e-05,
        "epoch": 0.21960622332369545,
        "step": 1595
    },
    {
        "loss": 2.3154,
        "grad_norm": 0.9504837989807129,
        "learning_rate": 5.428025623281898e-05,
        "epoch": 0.21974390747624947,
        "step": 1596
    },
    {
        "loss": 1.8983,
        "grad_norm": 2.042367935180664,
        "learning_rate": 5.4164488590048945e-05,
        "epoch": 0.21988159162880352,
        "step": 1597
    },
    {
        "loss": 2.5156,
        "grad_norm": 1.7300466299057007,
        "learning_rate": 5.4048798662684376e-05,
        "epoch": 0.22001927578135758,
        "step": 1598
    },
    {
        "loss": 1.9525,
        "grad_norm": 1.4788198471069336,
        "learning_rate": 5.393318664688084e-05,
        "epoch": 0.2201569599339116,
        "step": 1599
    },
    {
        "loss": 1.5473,
        "grad_norm": 2.2899158000946045,
        "learning_rate": 5.38176527386618e-05,
        "epoch": 0.22029464408646565,
        "step": 1600
    },
    {
        "loss": 2.9027,
        "grad_norm": 1.187624216079712,
        "learning_rate": 5.37021971339182e-05,
        "epoch": 0.22043232823901968,
        "step": 1601
    },
    {
        "loss": 2.11,
        "grad_norm": 1.3380093574523926,
        "learning_rate": 5.3586820028408314e-05,
        "epoch": 0.22057001239157373,
        "step": 1602
    },
    {
        "loss": 2.0477,
        "grad_norm": 1.924088716506958,
        "learning_rate": 5.347152161775728e-05,
        "epoch": 0.22070769654412778,
        "step": 1603
    },
    {
        "loss": 1.9558,
        "grad_norm": 2.312225818634033,
        "learning_rate": 5.3356302097456814e-05,
        "epoch": 0.2208453806966818,
        "step": 1604
    },
    {
        "loss": 2.4352,
        "grad_norm": 1.6690922975540161,
        "learning_rate": 5.324116166286484e-05,
        "epoch": 0.22098306484923586,
        "step": 1605
    },
    {
        "loss": 0.5934,
        "grad_norm": 2.9226486682891846,
        "learning_rate": 5.3126100509205335e-05,
        "epoch": 0.2211207490017899,
        "step": 1606
    },
    {
        "loss": 2.3987,
        "grad_norm": 2.021279811859131,
        "learning_rate": 5.301111883156762e-05,
        "epoch": 0.22125843315434393,
        "step": 1607
    },
    {
        "loss": 2.3804,
        "grad_norm": 1.5007421970367432,
        "learning_rate": 5.289621682490642e-05,
        "epoch": 0.22139611730689798,
        "step": 1608
    },
    {
        "loss": 2.2828,
        "grad_norm": 1.6147301197052002,
        "learning_rate": 5.278139468404133e-05,
        "epoch": 0.221533801459452,
        "step": 1609
    },
    {
        "loss": 2.2636,
        "grad_norm": 1.585409164428711,
        "learning_rate": 5.266665260365654e-05,
        "epoch": 0.22167148561200606,
        "step": 1610
    },
    {
        "loss": 2.1548,
        "grad_norm": 1.363618016242981,
        "learning_rate": 5.25519907783005e-05,
        "epoch": 0.2218091697645601,
        "step": 1611
    },
    {
        "loss": 2.449,
        "grad_norm": 1.2370394468307495,
        "learning_rate": 5.2437409402385595e-05,
        "epoch": 0.22194685391711413,
        "step": 1612
    },
    {
        "loss": 2.3725,
        "grad_norm": 1.1353341341018677,
        "learning_rate": 5.232290867018773e-05,
        "epoch": 0.22208453806966819,
        "step": 1613
    },
    {
        "loss": 1.8593,
        "grad_norm": 2.269076347351074,
        "learning_rate": 5.2208488775846164e-05,
        "epoch": 0.2222222222222222,
        "step": 1614
    },
    {
        "loss": 2.0662,
        "grad_norm": 1.301485300064087,
        "learning_rate": 5.209414991336307e-05,
        "epoch": 0.22235990637477626,
        "step": 1615
    },
    {
        "loss": 2.1275,
        "grad_norm": 2.369485855102539,
        "learning_rate": 5.19798922766032e-05,
        "epoch": 0.2224975905273303,
        "step": 1616
    },
    {
        "loss": 1.6746,
        "grad_norm": 2.0014543533325195,
        "learning_rate": 5.186571605929363e-05,
        "epoch": 0.22263527467988434,
        "step": 1617
    },
    {
        "loss": 1.8312,
        "grad_norm": 1.497251033782959,
        "learning_rate": 5.175162145502337e-05,
        "epoch": 0.2227729588324384,
        "step": 1618
    },
    {
        "loss": 1.9426,
        "grad_norm": 2.3857414722442627,
        "learning_rate": 5.163760865724301e-05,
        "epoch": 0.22291064298499244,
        "step": 1619
    },
    {
        "loss": 1.9768,
        "grad_norm": 1.3804441690444946,
        "learning_rate": 5.1523677859264516e-05,
        "epoch": 0.22304832713754646,
        "step": 1620
    },
    {
        "loss": 1.6018,
        "grad_norm": 1.7608977556228638,
        "learning_rate": 5.140982925426075e-05,
        "epoch": 0.22318601129010052,
        "step": 1621
    },
    {
        "loss": 2.1818,
        "grad_norm": 2.1315841674804688,
        "learning_rate": 5.1296063035265286e-05,
        "epoch": 0.22332369544265454,
        "step": 1622
    },
    {
        "loss": 2.2372,
        "grad_norm": 1.2362709045410156,
        "learning_rate": 5.11823793951719e-05,
        "epoch": 0.2234613795952086,
        "step": 1623
    },
    {
        "loss": 2.644,
        "grad_norm": 2.152437925338745,
        "learning_rate": 5.1068778526734426e-05,
        "epoch": 0.22359906374776264,
        "step": 1624
    },
    {
        "loss": 2.2119,
        "grad_norm": 2.4169838428497314,
        "learning_rate": 5.0955260622566384e-05,
        "epoch": 0.22373674790031667,
        "step": 1625
    },
    {
        "loss": 2.0461,
        "grad_norm": 1.951356053352356,
        "learning_rate": 5.084182587514057e-05,
        "epoch": 0.22387443205287072,
        "step": 1626
    },
    {
        "loss": 2.2194,
        "grad_norm": 1.7978750467300415,
        "learning_rate": 5.072847447678879e-05,
        "epoch": 0.22401211620542474,
        "step": 1627
    },
    {
        "loss": 1.8632,
        "grad_norm": 2.282731294631958,
        "learning_rate": 5.061520661970158e-05,
        "epoch": 0.2241498003579788,
        "step": 1628
    },
    {
        "loss": 1.3111,
        "grad_norm": 2.411262035369873,
        "learning_rate": 5.050202249592777e-05,
        "epoch": 0.22428748451053285,
        "step": 1629
    },
    {
        "loss": 1.8907,
        "grad_norm": 3.328594207763672,
        "learning_rate": 5.038892229737426e-05,
        "epoch": 0.22442516866308687,
        "step": 1630
    },
    {
        "loss": 2.4656,
        "grad_norm": 1.6592819690704346,
        "learning_rate": 5.0275906215805625e-05,
        "epoch": 0.22456285281564092,
        "step": 1631
    },
    {
        "loss": 2.2223,
        "grad_norm": 1.6133654117584229,
        "learning_rate": 5.016297444284381e-05,
        "epoch": 0.22470053696819495,
        "step": 1632
    },
    {
        "loss": 2.0924,
        "grad_norm": 2.083928108215332,
        "learning_rate": 5.0050127169967885e-05,
        "epoch": 0.224838221120749,
        "step": 1633
    },
    {
        "loss": 2.2531,
        "grad_norm": 1.844774603843689,
        "learning_rate": 4.993736458851353e-05,
        "epoch": 0.22497590527330305,
        "step": 1634
    },
    {
        "loss": 2.3513,
        "grad_norm": 1.514703631401062,
        "learning_rate": 4.982468688967289e-05,
        "epoch": 0.22511358942585707,
        "step": 1635
    },
    {
        "loss": 2.0006,
        "grad_norm": 2.312974452972412,
        "learning_rate": 4.9712094264494215e-05,
        "epoch": 0.22525127357841113,
        "step": 1636
    },
    {
        "loss": 2.1675,
        "grad_norm": 2.2566168308258057,
        "learning_rate": 4.9599586903881476e-05,
        "epoch": 0.22538895773096518,
        "step": 1637
    },
    {
        "loss": 1.4683,
        "grad_norm": 2.1713860034942627,
        "learning_rate": 4.948716499859408e-05,
        "epoch": 0.2255266418835192,
        "step": 1638
    },
    {
        "loss": 2.0107,
        "grad_norm": 1.3642910718917847,
        "learning_rate": 4.9374828739246606e-05,
        "epoch": 0.22566432603607325,
        "step": 1639
    },
    {
        "loss": 2.6675,
        "grad_norm": 1.442111849784851,
        "learning_rate": 4.9262578316308216e-05,
        "epoch": 0.22580201018862728,
        "step": 1640
    },
    {
        "loss": 1.8113,
        "grad_norm": 1.4665066003799438,
        "learning_rate": 4.915041392010281e-05,
        "epoch": 0.22593969434118133,
        "step": 1641
    },
    {
        "loss": 2.1369,
        "grad_norm": 1.3988648653030396,
        "learning_rate": 4.903833574080825e-05,
        "epoch": 0.22607737849373538,
        "step": 1642
    },
    {
        "loss": 2.2611,
        "grad_norm": 2.32310152053833,
        "learning_rate": 4.892634396845624e-05,
        "epoch": 0.2262150626462894,
        "step": 1643
    },
    {
        "loss": 2.3568,
        "grad_norm": 1.4817579984664917,
        "learning_rate": 4.881443879293206e-05,
        "epoch": 0.22635274679884346,
        "step": 1644
    },
    {
        "loss": 2.3908,
        "grad_norm": 1.0660169124603271,
        "learning_rate": 4.8702620403974006e-05,
        "epoch": 0.22649043095139748,
        "step": 1645
    },
    {
        "loss": 1.9115,
        "grad_norm": 2.8959882259368896,
        "learning_rate": 4.859088899117337e-05,
        "epoch": 0.22662811510395153,
        "step": 1646
    },
    {
        "loss": 1.9718,
        "grad_norm": 3.210724115371704,
        "learning_rate": 4.847924474397391e-05,
        "epoch": 0.22676579925650558,
        "step": 1647
    },
    {
        "loss": 1.9977,
        "grad_norm": 1.7190781831741333,
        "learning_rate": 4.83676878516716e-05,
        "epoch": 0.2269034834090596,
        "step": 1648
    },
    {
        "loss": 2.311,
        "grad_norm": 1.1473249197006226,
        "learning_rate": 4.825621850341431e-05,
        "epoch": 0.22704116756161366,
        "step": 1649
    },
    {
        "loss": 2.1265,
        "grad_norm": 1.2596791982650757,
        "learning_rate": 4.8144836888201485e-05,
        "epoch": 0.2271788517141677,
        "step": 1650
    },
    {
        "loss": 2.2127,
        "grad_norm": 1.5857170820236206,
        "learning_rate": 4.8033543194883814e-05,
        "epoch": 0.22731653586672174,
        "step": 1651
    },
    {
        "loss": 1.9682,
        "grad_norm": 1.5160130262374878,
        "learning_rate": 4.79223376121628e-05,
        "epoch": 0.2274542200192758,
        "step": 1652
    },
    {
        "loss": 2.4033,
        "grad_norm": 1.9429690837860107,
        "learning_rate": 4.781122032859079e-05,
        "epoch": 0.2275919041718298,
        "step": 1653
    },
    {
        "loss": 2.5406,
        "grad_norm": 1.2730578184127808,
        "learning_rate": 4.7700191532570193e-05,
        "epoch": 0.22772958832438386,
        "step": 1654
    },
    {
        "loss": 1.4841,
        "grad_norm": 1.4546394348144531,
        "learning_rate": 4.758925141235355e-05,
        "epoch": 0.22786727247693792,
        "step": 1655
    },
    {
        "loss": 2.1137,
        "grad_norm": 0.9298983216285706,
        "learning_rate": 4.747840015604288e-05,
        "epoch": 0.22800495662949194,
        "step": 1656
    },
    {
        "loss": 2.3023,
        "grad_norm": 1.9358407258987427,
        "learning_rate": 4.736763795158966e-05,
        "epoch": 0.228142640782046,
        "step": 1657
    },
    {
        "loss": 2.2467,
        "grad_norm": 1.7449772357940674,
        "learning_rate": 4.7256964986794386e-05,
        "epoch": 0.22828032493460002,
        "step": 1658
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.473152995109558,
        "learning_rate": 4.7146381449306067e-05,
        "epoch": 0.22841800908715407,
        "step": 1659
    },
    {
        "loss": 2.5322,
        "grad_norm": 1.3318432569503784,
        "learning_rate": 4.7035887526622344e-05,
        "epoch": 0.22855569323970812,
        "step": 1660
    },
    {
        "loss": 2.5324,
        "grad_norm": 1.5621333122253418,
        "learning_rate": 4.692548340608879e-05,
        "epoch": 0.22869337739226214,
        "step": 1661
    },
    {
        "loss": 1.8335,
        "grad_norm": 1.3934195041656494,
        "learning_rate": 4.681516927489859e-05,
        "epoch": 0.2288310615448162,
        "step": 1662
    },
    {
        "loss": 2.3949,
        "grad_norm": 1.0386972427368164,
        "learning_rate": 4.6704945320092616e-05,
        "epoch": 0.22896874569737025,
        "step": 1663
    },
    {
        "loss": 2.5099,
        "grad_norm": 1.527540683746338,
        "learning_rate": 4.659481172855862e-05,
        "epoch": 0.22910642984992427,
        "step": 1664
    },
    {
        "loss": 2.0462,
        "grad_norm": 2.4459424018859863,
        "learning_rate": 4.6484768687031187e-05,
        "epoch": 0.22924411400247832,
        "step": 1665
    },
    {
        "loss": 2.0411,
        "grad_norm": 0.9687381386756897,
        "learning_rate": 4.637481638209149e-05,
        "epoch": 0.22938179815503235,
        "step": 1666
    },
    {
        "loss": 2.1608,
        "grad_norm": 1.5467529296875,
        "learning_rate": 4.626495500016678e-05,
        "epoch": 0.2295194823075864,
        "step": 1667
    },
    {
        "loss": 2.5666,
        "grad_norm": 1.456882357597351,
        "learning_rate": 4.615518472753004e-05,
        "epoch": 0.22965716646014045,
        "step": 1668
    },
    {
        "loss": 2.2837,
        "grad_norm": 1.3996387720108032,
        "learning_rate": 4.604550575029997e-05,
        "epoch": 0.22979485061269447,
        "step": 1669
    },
    {
        "loss": 2.2441,
        "grad_norm": 2.1074631214141846,
        "learning_rate": 4.593591825444028e-05,
        "epoch": 0.22993253476524853,
        "step": 1670
    },
    {
        "loss": 2.32,
        "grad_norm": 1.0779837369918823,
        "learning_rate": 4.582642242575969e-05,
        "epoch": 0.23007021891780255,
        "step": 1671
    },
    {
        "loss": 1.8659,
        "grad_norm": 2.113865613937378,
        "learning_rate": 4.5717018449911517e-05,
        "epoch": 0.2302079030703566,
        "step": 1672
    },
    {
        "loss": 2.4925,
        "grad_norm": 2.2278077602386475,
        "learning_rate": 4.5607706512393334e-05,
        "epoch": 0.23034558722291065,
        "step": 1673
    },
    {
        "loss": 2.5109,
        "grad_norm": 1.5310828685760498,
        "learning_rate": 4.549848679854648e-05,
        "epoch": 0.23048327137546468,
        "step": 1674
    },
    {
        "loss": 1.9008,
        "grad_norm": 1.9720603227615356,
        "learning_rate": 4.538935949355623e-05,
        "epoch": 0.23062095552801873,
        "step": 1675
    },
    {
        "loss": 2.3068,
        "grad_norm": 1.636718511581421,
        "learning_rate": 4.52803247824509e-05,
        "epoch": 0.23075863968057275,
        "step": 1676
    },
    {
        "loss": 1.7712,
        "grad_norm": 2.4698293209075928,
        "learning_rate": 4.517138285010196e-05,
        "epoch": 0.2308963238331268,
        "step": 1677
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.86900794506073,
        "learning_rate": 4.506253388122355e-05,
        "epoch": 0.23103400798568086,
        "step": 1678
    },
    {
        "loss": 2.6026,
        "grad_norm": 1.1365464925765991,
        "learning_rate": 4.495377806037214e-05,
        "epoch": 0.23117169213823488,
        "step": 1679
    },
    {
        "loss": 2.6633,
        "grad_norm": 1.3558101654052734,
        "learning_rate": 4.484511557194638e-05,
        "epoch": 0.23130937629078893,
        "step": 1680
    },
    {
        "loss": 1.8615,
        "grad_norm": 2.3961257934570312,
        "learning_rate": 4.473654660018649e-05,
        "epoch": 0.23144706044334298,
        "step": 1681
    },
    {
        "loss": 2.1755,
        "grad_norm": 1.5222938060760498,
        "learning_rate": 4.4628071329174284e-05,
        "epoch": 0.231584744595897,
        "step": 1682
    },
    {
        "loss": 2.6632,
        "grad_norm": 1.039677619934082,
        "learning_rate": 4.451968994283262e-05,
        "epoch": 0.23172242874845106,
        "step": 1683
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.7109756469726562,
        "learning_rate": 4.44114026249252e-05,
        "epoch": 0.23186011290100508,
        "step": 1684
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.3053756952285767,
        "learning_rate": 4.430320955905634e-05,
        "epoch": 0.23199779705355913,
        "step": 1685
    },
    {
        "loss": 2.3939,
        "grad_norm": 2.284447193145752,
        "learning_rate": 4.41951109286703e-05,
        "epoch": 0.2321354812061132,
        "step": 1686
    },
    {
        "loss": 2.1626,
        "grad_norm": 2.0004894733428955,
        "learning_rate": 4.4087106917051445e-05,
        "epoch": 0.2322731653586672,
        "step": 1687
    },
    {
        "loss": 1.9331,
        "grad_norm": 1.1070095300674438,
        "learning_rate": 4.3979197707323596e-05,
        "epoch": 0.23241084951122126,
        "step": 1688
    },
    {
        "loss": 1.1397,
        "grad_norm": 2.414064884185791,
        "learning_rate": 4.387138348244988e-05,
        "epoch": 0.2325485336637753,
        "step": 1689
    },
    {
        "loss": 2.2336,
        "grad_norm": 1.6598987579345703,
        "learning_rate": 4.376366442523238e-05,
        "epoch": 0.23268621781632934,
        "step": 1690
    },
    {
        "loss": 1.9263,
        "grad_norm": 1.070992112159729,
        "learning_rate": 4.365604071831181e-05,
        "epoch": 0.2328239019688834,
        "step": 1691
    },
    {
        "loss": 1.9172,
        "grad_norm": 1.724157691001892,
        "learning_rate": 4.354851254416715e-05,
        "epoch": 0.23296158612143741,
        "step": 1692
    },
    {
        "loss": 2.6242,
        "grad_norm": 1.996080756187439,
        "learning_rate": 4.344108008511551e-05,
        "epoch": 0.23309927027399147,
        "step": 1693
    },
    {
        "loss": 2.0319,
        "grad_norm": 1.175482153892517,
        "learning_rate": 4.333374352331163e-05,
        "epoch": 0.23323695442654552,
        "step": 1694
    },
    {
        "loss": 2.1148,
        "grad_norm": 1.7378498315811157,
        "learning_rate": 4.3226503040747704e-05,
        "epoch": 0.23337463857909954,
        "step": 1695
    },
    {
        "loss": 1.6257,
        "grad_norm": 1.7354352474212646,
        "learning_rate": 4.3119358819252975e-05,
        "epoch": 0.2335123227316536,
        "step": 1696
    },
    {
        "loss": 2.241,
        "grad_norm": 1.756606936454773,
        "learning_rate": 4.3012311040493594e-05,
        "epoch": 0.23365000688420762,
        "step": 1697
    },
    {
        "loss": 1.6151,
        "grad_norm": 1.2992804050445557,
        "learning_rate": 4.2905359885971986e-05,
        "epoch": 0.23378769103676167,
        "step": 1698
    },
    {
        "loss": 1.5574,
        "grad_norm": 1.534956932067871,
        "learning_rate": 4.279850553702689e-05,
        "epoch": 0.23392537518931572,
        "step": 1699
    },
    {
        "loss": 1.3495,
        "grad_norm": 2.346888542175293,
        "learning_rate": 4.26917481748329e-05,
        "epoch": 0.23406305934186974,
        "step": 1700
    },
    {
        "loss": 2.5652,
        "grad_norm": 1.50620698928833,
        "learning_rate": 4.258508798040012e-05,
        "epoch": 0.2342007434944238,
        "step": 1701
    },
    {
        "loss": 1.2566,
        "grad_norm": 3.5872180461883545,
        "learning_rate": 4.247852513457393e-05,
        "epoch": 0.23433842764697782,
        "step": 1702
    },
    {
        "loss": 2.4027,
        "grad_norm": 1.5756597518920898,
        "learning_rate": 4.2372059818034695e-05,
        "epoch": 0.23447611179953187,
        "step": 1703
    },
    {
        "loss": 2.6513,
        "grad_norm": 1.3862903118133545,
        "learning_rate": 4.2265692211297284e-05,
        "epoch": 0.23461379595208592,
        "step": 1704
    },
    {
        "loss": 2.2232,
        "grad_norm": 2.829237222671509,
        "learning_rate": 4.215942249471101e-05,
        "epoch": 0.23475148010463995,
        "step": 1705
    },
    {
        "loss": 1.4723,
        "grad_norm": 2.9468390941619873,
        "learning_rate": 4.2053250848459204e-05,
        "epoch": 0.234889164257194,
        "step": 1706
    },
    {
        "loss": 2.158,
        "grad_norm": 1.8502458333969116,
        "learning_rate": 4.194717745255887e-05,
        "epoch": 0.23502684840974805,
        "step": 1707
    },
    {
        "loss": 2.08,
        "grad_norm": 1.443876028060913,
        "learning_rate": 4.184120248686048e-05,
        "epoch": 0.23516453256230208,
        "step": 1708
    },
    {
        "loss": 2.1793,
        "grad_norm": 1.452805519104004,
        "learning_rate": 4.173532613104756e-05,
        "epoch": 0.23530221671485613,
        "step": 1709
    },
    {
        "loss": 1.8186,
        "grad_norm": 1.1973590850830078,
        "learning_rate": 4.162954856463647e-05,
        "epoch": 0.23543990086741015,
        "step": 1710
    },
    {
        "loss": 1.6872,
        "grad_norm": 2.502243995666504,
        "learning_rate": 4.152386996697607e-05,
        "epoch": 0.2355775850199642,
        "step": 1711
    },
    {
        "loss": 2.4154,
        "grad_norm": 1.443293809890747,
        "learning_rate": 4.141829051724741e-05,
        "epoch": 0.23571526917251825,
        "step": 1712
    },
    {
        "loss": 2.4565,
        "grad_norm": 1.3773666620254517,
        "learning_rate": 4.131281039446342e-05,
        "epoch": 0.23585295332507228,
        "step": 1713
    },
    {
        "loss": 1.4418,
        "grad_norm": 3.1631624698638916,
        "learning_rate": 4.120742977746867e-05,
        "epoch": 0.23599063747762633,
        "step": 1714
    },
    {
        "loss": 2.0713,
        "grad_norm": 2.2383174896240234,
        "learning_rate": 4.1102148844938925e-05,
        "epoch": 0.23612832163018035,
        "step": 1715
    },
    {
        "loss": 1.839,
        "grad_norm": 2.236212968826294,
        "learning_rate": 4.099696777538099e-05,
        "epoch": 0.2362660057827344,
        "step": 1716
    },
    {
        "loss": 1.3431,
        "grad_norm": 2.142310857772827,
        "learning_rate": 4.089188674713236e-05,
        "epoch": 0.23640368993528846,
        "step": 1717
    },
    {
        "loss": 1.6334,
        "grad_norm": 1.5255475044250488,
        "learning_rate": 4.0786905938360874e-05,
        "epoch": 0.23654137408784248,
        "step": 1718
    },
    {
        "loss": 2.2071,
        "grad_norm": 2.1068458557128906,
        "learning_rate": 4.0682025527064486e-05,
        "epoch": 0.23667905824039653,
        "step": 1719
    },
    {
        "loss": 1.6828,
        "grad_norm": 1.4913170337677002,
        "learning_rate": 4.0577245691070876e-05,
        "epoch": 0.23681674239295056,
        "step": 1720
    },
    {
        "loss": 2.3896,
        "grad_norm": 2.252627372741699,
        "learning_rate": 4.0472566608037224e-05,
        "epoch": 0.2369544265455046,
        "step": 1721
    },
    {
        "loss": 1.842,
        "grad_norm": 2.253203868865967,
        "learning_rate": 4.036798845544988e-05,
        "epoch": 0.23709211069805866,
        "step": 1722
    },
    {
        "loss": 2.0794,
        "grad_norm": 1.8139630556106567,
        "learning_rate": 4.0263511410624044e-05,
        "epoch": 0.23722979485061269,
        "step": 1723
    },
    {
        "loss": 2.2563,
        "grad_norm": 1.1121528148651123,
        "learning_rate": 4.015913565070356e-05,
        "epoch": 0.23736747900316674,
        "step": 1724
    },
    {
        "loss": 2.3394,
        "grad_norm": 1.7809361219406128,
        "learning_rate": 4.005486135266039e-05,
        "epoch": 0.2375051631557208,
        "step": 1725
    },
    {
        "loss": 2.0585,
        "grad_norm": 2.04382061958313,
        "learning_rate": 3.9950688693294594e-05,
        "epoch": 0.2376428473082748,
        "step": 1726
    },
    {
        "loss": 2.3843,
        "grad_norm": 1.7759015560150146,
        "learning_rate": 3.9846617849233856e-05,
        "epoch": 0.23778053146082886,
        "step": 1727
    },
    {
        "loss": 2.519,
        "grad_norm": 1.5655001401901245,
        "learning_rate": 3.9742648996933255e-05,
        "epoch": 0.2379182156133829,
        "step": 1728
    },
    {
        "loss": 2.2136,
        "grad_norm": 1.0892260074615479,
        "learning_rate": 3.9638782312674896e-05,
        "epoch": 0.23805589976593694,
        "step": 1729
    },
    {
        "loss": 2.0167,
        "grad_norm": 1.9776040315628052,
        "learning_rate": 3.953501797256771e-05,
        "epoch": 0.238193583918491,
        "step": 1730
    },
    {
        "loss": 2.0625,
        "grad_norm": 1.6422221660614014,
        "learning_rate": 3.943135615254699e-05,
        "epoch": 0.23833126807104502,
        "step": 1731
    },
    {
        "loss": 2.1682,
        "grad_norm": 1.5779677629470825,
        "learning_rate": 3.932779702837438e-05,
        "epoch": 0.23846895222359907,
        "step": 1732
    },
    {
        "loss": 1.9425,
        "grad_norm": 2.5010552406311035,
        "learning_rate": 3.922434077563726e-05,
        "epoch": 0.2386066363761531,
        "step": 1733
    },
    {
        "loss": 1.1271,
        "grad_norm": 2.819031238555908,
        "learning_rate": 3.9120987569748626e-05,
        "epoch": 0.23874432052870714,
        "step": 1734
    },
    {
        "loss": 2.1795,
        "grad_norm": 1.9294767379760742,
        "learning_rate": 3.901773758594677e-05,
        "epoch": 0.2388820046812612,
        "step": 1735
    },
    {
        "loss": 2.4093,
        "grad_norm": 1.2687177658081055,
        "learning_rate": 3.8914590999294995e-05,
        "epoch": 0.23901968883381522,
        "step": 1736
    },
    {
        "loss": 1.8144,
        "grad_norm": 2.4377336502075195,
        "learning_rate": 3.881154798468118e-05,
        "epoch": 0.23915737298636927,
        "step": 1737
    },
    {
        "loss": 1.987,
        "grad_norm": 1.6814002990722656,
        "learning_rate": 3.8708608716817704e-05,
        "epoch": 0.23929505713892332,
        "step": 1738
    },
    {
        "loss": 2.0869,
        "grad_norm": 1.801356315612793,
        "learning_rate": 3.860577337024102e-05,
        "epoch": 0.23943274129147735,
        "step": 1739
    },
    {
        "loss": 2.2289,
        "grad_norm": 1.4063773155212402,
        "learning_rate": 3.850304211931137e-05,
        "epoch": 0.2395704254440314,
        "step": 1740
    },
    {
        "loss": 1.847,
        "grad_norm": 2.170391321182251,
        "learning_rate": 3.840041513821243e-05,
        "epoch": 0.23970810959658542,
        "step": 1741
    },
    {
        "loss": 1.1207,
        "grad_norm": 2.677558422088623,
        "learning_rate": 3.829789260095126e-05,
        "epoch": 0.23984579374913947,
        "step": 1742
    },
    {
        "loss": 2.421,
        "grad_norm": 1.4184545278549194,
        "learning_rate": 3.819547468135761e-05,
        "epoch": 0.23998347790169353,
        "step": 1743
    },
    {
        "loss": 2.4017,
        "grad_norm": 1.46902596950531,
        "learning_rate": 3.8093161553083976e-05,
        "epoch": 0.24012116205424755,
        "step": 1744
    },
    {
        "loss": 1.7008,
        "grad_norm": 2.005197286605835,
        "learning_rate": 3.799095338960523e-05,
        "epoch": 0.2402588462068016,
        "step": 1745
    },
    {
        "loss": 1.6165,
        "grad_norm": 1.8190568685531616,
        "learning_rate": 3.7888850364218196e-05,
        "epoch": 0.24039653035935563,
        "step": 1746
    },
    {
        "loss": 1.7662,
        "grad_norm": 1.3213118314743042,
        "learning_rate": 3.778685265004137e-05,
        "epoch": 0.24053421451190968,
        "step": 1747
    },
    {
        "loss": 1.9927,
        "grad_norm": 2.055858612060547,
        "learning_rate": 3.768496042001479e-05,
        "epoch": 0.24067189866446373,
        "step": 1748
    },
    {
        "loss": 2.6903,
        "grad_norm": 1.751643180847168,
        "learning_rate": 3.758317384689962e-05,
        "epoch": 0.24080958281701775,
        "step": 1749
    },
    {
        "loss": 2.0301,
        "grad_norm": 1.757781744003296,
        "learning_rate": 3.748149310327788e-05,
        "epoch": 0.2409472669695718,
        "step": 1750
    },
    {
        "loss": 2.1612,
        "grad_norm": 1.7013130187988281,
        "learning_rate": 3.737991836155214e-05,
        "epoch": 0.24108495112212586,
        "step": 1751
    },
    {
        "loss": 1.4117,
        "grad_norm": 2.3994972705841064,
        "learning_rate": 3.727844979394526e-05,
        "epoch": 0.24122263527467988,
        "step": 1752
    },
    {
        "loss": 2.4719,
        "grad_norm": 1.2150607109069824,
        "learning_rate": 3.717708757249999e-05,
        "epoch": 0.24136031942723393,
        "step": 1753
    },
    {
        "loss": 1.8842,
        "grad_norm": 2.840062379837036,
        "learning_rate": 3.707583186907897e-05,
        "epoch": 0.24149800357978796,
        "step": 1754
    },
    {
        "loss": 2.1625,
        "grad_norm": 2.4149200916290283,
        "learning_rate": 3.6974682855364005e-05,
        "epoch": 0.241635687732342,
        "step": 1755
    },
    {
        "loss": 2.4828,
        "grad_norm": 1.0284048318862915,
        "learning_rate": 3.68736407028561e-05,
        "epoch": 0.24177337188489606,
        "step": 1756
    },
    {
        "loss": 1.9955,
        "grad_norm": 2.513091564178467,
        "learning_rate": 3.677270558287517e-05,
        "epoch": 0.24191105603745008,
        "step": 1757
    },
    {
        "loss": 1.4173,
        "grad_norm": 1.3885843753814697,
        "learning_rate": 3.667187766655954e-05,
        "epoch": 0.24204874019000414,
        "step": 1758
    },
    {
        "loss": 1.6977,
        "grad_norm": 2.3732759952545166,
        "learning_rate": 3.657115712486573e-05,
        "epoch": 0.24218642434255816,
        "step": 1759
    },
    {
        "loss": 2.2099,
        "grad_norm": 1.793397307395935,
        "learning_rate": 3.6470544128568315e-05,
        "epoch": 0.2423241084951122,
        "step": 1760
    },
    {
        "loss": 2.2182,
        "grad_norm": 1.723219871520996,
        "learning_rate": 3.63700388482595e-05,
        "epoch": 0.24246179264766626,
        "step": 1761
    },
    {
        "loss": 1.9184,
        "grad_norm": 1.9441800117492676,
        "learning_rate": 3.626964145434876e-05,
        "epoch": 0.2425994768002203,
        "step": 1762
    },
    {
        "loss": 1.6956,
        "grad_norm": 2.2794322967529297,
        "learning_rate": 3.616935211706271e-05,
        "epoch": 0.24273716095277434,
        "step": 1763
    },
    {
        "loss": 2.1442,
        "grad_norm": 1.1358160972595215,
        "learning_rate": 3.606917100644488e-05,
        "epoch": 0.24287484510532836,
        "step": 1764
    },
    {
        "loss": 1.5218,
        "grad_norm": 1.8219610452651978,
        "learning_rate": 3.5969098292355043e-05,
        "epoch": 0.24301252925788241,
        "step": 1765
    },
    {
        "loss": 2.2353,
        "grad_norm": 1.497206687927246,
        "learning_rate": 3.5869134144469375e-05,
        "epoch": 0.24315021341043647,
        "step": 1766
    },
    {
        "loss": 2.0214,
        "grad_norm": 2.277061939239502,
        "learning_rate": 3.576927873227989e-05,
        "epoch": 0.2432878975629905,
        "step": 1767
    },
    {
        "loss": 2.034,
        "grad_norm": 1.71784508228302,
        "learning_rate": 3.5669532225094285e-05,
        "epoch": 0.24342558171554454,
        "step": 1768
    },
    {
        "loss": 1.7358,
        "grad_norm": 1.1229304075241089,
        "learning_rate": 3.556989479203554e-05,
        "epoch": 0.2435632658680986,
        "step": 1769
    },
    {
        "loss": 2.0682,
        "grad_norm": 1.6598645448684692,
        "learning_rate": 3.547036660204184e-05,
        "epoch": 0.24370095002065262,
        "step": 1770
    },
    {
        "loss": 2.4993,
        "grad_norm": 1.4640629291534424,
        "learning_rate": 3.537094782386595e-05,
        "epoch": 0.24383863417320667,
        "step": 1771
    },
    {
        "loss": 1.9504,
        "grad_norm": 1.8560384511947632,
        "learning_rate": 3.5271638626075276e-05,
        "epoch": 0.2439763183257607,
        "step": 1772
    },
    {
        "loss": 1.8054,
        "grad_norm": 2.637239933013916,
        "learning_rate": 3.517243917705132e-05,
        "epoch": 0.24411400247831475,
        "step": 1773
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.1319290399551392,
        "learning_rate": 3.5073349644989564e-05,
        "epoch": 0.2442516866308688,
        "step": 1774
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.0707225799560547,
        "learning_rate": 3.49743701978991e-05,
        "epoch": 0.24438937078342282,
        "step": 1775
    },
    {
        "loss": 2.1022,
        "grad_norm": 1.9751720428466797,
        "learning_rate": 3.487550100360246e-05,
        "epoch": 0.24452705493597687,
        "step": 1776
    },
    {
        "loss": 1.575,
        "grad_norm": 2.808004140853882,
        "learning_rate": 3.477674222973507e-05,
        "epoch": 0.2446647390885309,
        "step": 1777
    },
    {
        "loss": 1.3426,
        "grad_norm": 2.4956247806549072,
        "learning_rate": 3.4678094043745246e-05,
        "epoch": 0.24480242324108495,
        "step": 1778
    },
    {
        "loss": 1.2938,
        "grad_norm": 2.6135497093200684,
        "learning_rate": 3.457955661289379e-05,
        "epoch": 0.244940107393639,
        "step": 1779
    },
    {
        "loss": 2.6635,
        "grad_norm": 1.8061033487319946,
        "learning_rate": 3.44811301042537e-05,
        "epoch": 0.24507779154619302,
        "step": 1780
    },
    {
        "loss": 1.3998,
        "grad_norm": 2.0954556465148926,
        "learning_rate": 3.4382814684709895e-05,
        "epoch": 0.24521547569874708,
        "step": 1781
    },
    {
        "loss": 1.9646,
        "grad_norm": 1.2668803930282593,
        "learning_rate": 3.428461052095895e-05,
        "epoch": 0.24535315985130113,
        "step": 1782
    },
    {
        "loss": 2.5756,
        "grad_norm": 2.28287410736084,
        "learning_rate": 3.4186517779508845e-05,
        "epoch": 0.24549084400385515,
        "step": 1783
    },
    {
        "loss": 2.3507,
        "grad_norm": 1.1547678709030151,
        "learning_rate": 3.4088536626678514e-05,
        "epoch": 0.2456285281564092,
        "step": 1784
    },
    {
        "loss": 1.8159,
        "grad_norm": 2.611956834793091,
        "learning_rate": 3.399066722859782e-05,
        "epoch": 0.24576621230896323,
        "step": 1785
    },
    {
        "loss": 2.6241,
        "grad_norm": 1.9608392715454102,
        "learning_rate": 3.389290975120708e-05,
        "epoch": 0.24590389646151728,
        "step": 1786
    },
    {
        "loss": 2.1144,
        "grad_norm": 1.4184015989303589,
        "learning_rate": 3.3795264360256875e-05,
        "epoch": 0.24604158061407133,
        "step": 1787
    },
    {
        "loss": 2.0527,
        "grad_norm": 1.811269998550415,
        "learning_rate": 3.369773122130771e-05,
        "epoch": 0.24617926476662536,
        "step": 1788
    },
    {
        "loss": 1.5008,
        "grad_norm": 1.5403525829315186,
        "learning_rate": 3.360031049972979e-05,
        "epoch": 0.2463169489191794,
        "step": 1789
    },
    {
        "loss": 2.4381,
        "grad_norm": 1.3020964860916138,
        "learning_rate": 3.350300236070272e-05,
        "epoch": 0.24645463307173343,
        "step": 1790
    },
    {
        "loss": 2.567,
        "grad_norm": 0.9067345857620239,
        "learning_rate": 3.340580696921517e-05,
        "epoch": 0.24659231722428748,
        "step": 1791
    },
    {
        "loss": 2.1089,
        "grad_norm": 1.87765371799469,
        "learning_rate": 3.330872449006468e-05,
        "epoch": 0.24673000137684153,
        "step": 1792
    },
    {
        "loss": 1.9564,
        "grad_norm": 1.4298694133758545,
        "learning_rate": 3.321175508785736e-05,
        "epoch": 0.24686768552939556,
        "step": 1793
    },
    {
        "loss": 1.6791,
        "grad_norm": 2.1695871353149414,
        "learning_rate": 3.311489892700755e-05,
        "epoch": 0.2470053696819496,
        "step": 1794
    },
    {
        "loss": 1.9741,
        "grad_norm": 2.2438762187957764,
        "learning_rate": 3.3018156171737646e-05,
        "epoch": 0.24714305383450366,
        "step": 1795
    },
    {
        "loss": 2.4131,
        "grad_norm": 1.295356035232544,
        "learning_rate": 3.292152698607768e-05,
        "epoch": 0.24728073798705769,
        "step": 1796
    },
    {
        "loss": 2.572,
        "grad_norm": 1.2761061191558838,
        "learning_rate": 3.282501153386517e-05,
        "epoch": 0.24741842213961174,
        "step": 1797
    },
    {
        "loss": 2.2408,
        "grad_norm": 1.6369677782058716,
        "learning_rate": 3.2728609978744804e-05,
        "epoch": 0.24755610629216576,
        "step": 1798
    },
    {
        "loss": 2.0214,
        "grad_norm": 0.8788749575614929,
        "learning_rate": 3.2632322484168155e-05,
        "epoch": 0.2476937904447198,
        "step": 1799
    },
    {
        "loss": 1.8953,
        "grad_norm": 2.0134196281433105,
        "learning_rate": 3.2536149213393386e-05,
        "epoch": 0.24783147459727387,
        "step": 1800
    },
    {
        "loss": 1.8561,
        "grad_norm": 1.0915476083755493,
        "learning_rate": 3.2440090329484995e-05,
        "epoch": 0.2479691587498279,
        "step": 1801
    },
    {
        "loss": 2.3176,
        "grad_norm": 1.5549421310424805,
        "learning_rate": 3.234414599531354e-05,
        "epoch": 0.24810684290238194,
        "step": 1802
    },
    {
        "loss": 2.4993,
        "grad_norm": 1.186579942703247,
        "learning_rate": 3.2248316373555385e-05,
        "epoch": 0.24824452705493597,
        "step": 1803
    },
    {
        "loss": 1.4678,
        "grad_norm": 2.41438364982605,
        "learning_rate": 3.215260162669226e-05,
        "epoch": 0.24838221120749002,
        "step": 1804
    },
    {
        "loss": 2.5585,
        "grad_norm": 1.1590455770492554,
        "learning_rate": 3.205700191701132e-05,
        "epoch": 0.24851989536004407,
        "step": 1805
    },
    {
        "loss": 2.1383,
        "grad_norm": 1.0273393392562866,
        "learning_rate": 3.196151740660458e-05,
        "epoch": 0.2486575795125981,
        "step": 1806
    },
    {
        "loss": 1.6824,
        "grad_norm": 2.448021650314331,
        "learning_rate": 3.1866148257368665e-05,
        "epoch": 0.24879526366515214,
        "step": 1807
    },
    {
        "loss": 1.8017,
        "grad_norm": 1.4394621849060059,
        "learning_rate": 3.177089463100469e-05,
        "epoch": 0.24893294781770617,
        "step": 1808
    },
    {
        "loss": 1.6202,
        "grad_norm": 2.1848344802856445,
        "learning_rate": 3.167575668901791e-05,
        "epoch": 0.24907063197026022,
        "step": 1809
    },
    {
        "loss": 2.2913,
        "grad_norm": 1.2377625703811646,
        "learning_rate": 3.158073459271729e-05,
        "epoch": 0.24920831612281427,
        "step": 1810
    },
    {
        "loss": 2.405,
        "grad_norm": 1.7665419578552246,
        "learning_rate": 3.1485828503215585e-05,
        "epoch": 0.2493460002753683,
        "step": 1811
    },
    {
        "loss": 2.188,
        "grad_norm": 1.1205865144729614,
        "learning_rate": 3.139103858142872e-05,
        "epoch": 0.24948368442792235,
        "step": 1812
    },
    {
        "loss": 1.7358,
        "grad_norm": 1.6040512323379517,
        "learning_rate": 3.1296364988075685e-05,
        "epoch": 0.2496213685804764,
        "step": 1813
    },
    {
        "loss": 2.285,
        "grad_norm": 1.1519831418991089,
        "learning_rate": 3.120180788367824e-05,
        "epoch": 0.24975905273303042,
        "step": 1814
    },
    {
        "loss": 1.9221,
        "grad_norm": 1.9428731203079224,
        "learning_rate": 3.110736742856064e-05,
        "epoch": 0.24989673688558448,
        "step": 1815
    },
    {
        "loss": 2.1147,
        "grad_norm": 1.793276309967041,
        "learning_rate": 3.1013043782849284e-05,
        "epoch": 0.2500344210381385,
        "step": 1816
    },
    {
        "loss": 2.3147,
        "grad_norm": 1.9561293125152588,
        "learning_rate": 3.0918837106472686e-05,
        "epoch": 0.25017210519069255,
        "step": 1817
    },
    {
        "loss": 1.8103,
        "grad_norm": 1.5258960723876953,
        "learning_rate": 3.0824747559160836e-05,
        "epoch": 0.2503097893432466,
        "step": 1818
    },
    {
        "loss": 2.1612,
        "grad_norm": 1.5330804586410522,
        "learning_rate": 3.0730775300445255e-05,
        "epoch": 0.25044747349580065,
        "step": 1819
    },
    {
        "loss": 2.053,
        "grad_norm": 1.2550796270370483,
        "learning_rate": 3.063692048965857e-05,
        "epoch": 0.25058515764835465,
        "step": 1820
    },
    {
        "loss": 1.8793,
        "grad_norm": 1.6958738565444946,
        "learning_rate": 3.05431832859343e-05,
        "epoch": 0.2507228418009087,
        "step": 1821
    },
    {
        "loss": 1.9353,
        "grad_norm": 2.0891244411468506,
        "learning_rate": 3.044956384820641e-05,
        "epoch": 0.25086052595346275,
        "step": 1822
    },
    {
        "loss": 1.6107,
        "grad_norm": 1.6510276794433594,
        "learning_rate": 3.035606233520941e-05,
        "epoch": 0.2509982101060168,
        "step": 1823
    },
    {
        "loss": 1.9578,
        "grad_norm": 2.678722620010376,
        "learning_rate": 3.0262678905477727e-05,
        "epoch": 0.25113589425857086,
        "step": 1824
    },
    {
        "loss": 1.645,
        "grad_norm": 2.686163902282715,
        "learning_rate": 3.0169413717345642e-05,
        "epoch": 0.25127357841112485,
        "step": 1825
    },
    {
        "loss": 2.031,
        "grad_norm": 1.013445496559143,
        "learning_rate": 3.007626692894683e-05,
        "epoch": 0.2514112625636789,
        "step": 1826
    },
    {
        "loss": 2.5366,
        "grad_norm": 1.519574761390686,
        "learning_rate": 2.998323869821441e-05,
        "epoch": 0.25154894671623296,
        "step": 1827
    },
    {
        "loss": 2.2435,
        "grad_norm": 1.7600822448730469,
        "learning_rate": 2.9890329182880295e-05,
        "epoch": 0.251686630868787,
        "step": 1828
    },
    {
        "loss": 2.4835,
        "grad_norm": 1.5318669080734253,
        "learning_rate": 2.979753854047522e-05,
        "epoch": 0.25182431502134106,
        "step": 1829
    },
    {
        "loss": 2.547,
        "grad_norm": 1.2993948459625244,
        "learning_rate": 2.9704866928328355e-05,
        "epoch": 0.25196199917389506,
        "step": 1830
    },
    {
        "loss": 2.5711,
        "grad_norm": 1.2145253419876099,
        "learning_rate": 2.9612314503567063e-05,
        "epoch": 0.2520996833264491,
        "step": 1831
    },
    {
        "loss": 2.5498,
        "grad_norm": 1.4891544580459595,
        "learning_rate": 2.95198814231165e-05,
        "epoch": 0.25223736747900316,
        "step": 1832
    },
    {
        "loss": 2.0577,
        "grad_norm": 2.2608683109283447,
        "learning_rate": 2.94275678436997e-05,
        "epoch": 0.2523750516315572,
        "step": 1833
    },
    {
        "loss": 2.455,
        "grad_norm": 1.2385953664779663,
        "learning_rate": 2.933537392183684e-05,
        "epoch": 0.25251273578411126,
        "step": 1834
    },
    {
        "loss": 2.1171,
        "grad_norm": 0.9800699949264526,
        "learning_rate": 2.924329981384536e-05,
        "epoch": 0.2526504199366653,
        "step": 1835
    },
    {
        "loss": 2.0434,
        "grad_norm": 1.5789991617202759,
        "learning_rate": 2.915134567583948e-05,
        "epoch": 0.2527881040892193,
        "step": 1836
    },
    {
        "loss": 2.1101,
        "grad_norm": 1.4285016059875488,
        "learning_rate": 2.905951166373011e-05,
        "epoch": 0.25292578824177336,
        "step": 1837
    },
    {
        "loss": 2.2277,
        "grad_norm": 2.403762102127075,
        "learning_rate": 2.8967797933224417e-05,
        "epoch": 0.2530634723943274,
        "step": 1838
    },
    {
        "loss": 2.4221,
        "grad_norm": 2.1516740322113037,
        "learning_rate": 2.8876204639825545e-05,
        "epoch": 0.25320115654688147,
        "step": 1839
    },
    {
        "loss": 1.5949,
        "grad_norm": 1.6017733812332153,
        "learning_rate": 2.8784731938832556e-05,
        "epoch": 0.2533388406994355,
        "step": 1840
    },
    {
        "loss": 1.9151,
        "grad_norm": 2.0632951259613037,
        "learning_rate": 2.8693379985339995e-05,
        "epoch": 0.2534765248519895,
        "step": 1841
    },
    {
        "loss": 1.7069,
        "grad_norm": 2.3639919757843018,
        "learning_rate": 2.8602148934237683e-05,
        "epoch": 0.25361420900454357,
        "step": 1842
    },
    {
        "loss": 1.9973,
        "grad_norm": 1.654976725578308,
        "learning_rate": 2.8511038940210453e-05,
        "epoch": 0.2537518931570976,
        "step": 1843
    },
    {
        "loss": 2.52,
        "grad_norm": 1.4647269248962402,
        "learning_rate": 2.84200501577379e-05,
        "epoch": 0.25388957730965167,
        "step": 1844
    },
    {
        "loss": 1.9161,
        "grad_norm": 3.0744616985321045,
        "learning_rate": 2.832918274109401e-05,
        "epoch": 0.2540272614622057,
        "step": 1845
    },
    {
        "loss": 2.4056,
        "grad_norm": 1.8924460411071777,
        "learning_rate": 2.8238436844347073e-05,
        "epoch": 0.2541649456147597,
        "step": 1846
    },
    {
        "loss": 2.3135,
        "grad_norm": 1.3032976388931274,
        "learning_rate": 2.8147812621359394e-05,
        "epoch": 0.25430262976731377,
        "step": 1847
    },
    {
        "loss": 2.4973,
        "grad_norm": 1.4860562086105347,
        "learning_rate": 2.8057310225786804e-05,
        "epoch": 0.2544403139198678,
        "step": 1848
    },
    {
        "loss": 1.9933,
        "grad_norm": 1.7455716133117676,
        "learning_rate": 2.7966929811078747e-05,
        "epoch": 0.2545779980724219,
        "step": 1849
    },
    {
        "loss": 2.4757,
        "grad_norm": 1.3158314228057861,
        "learning_rate": 2.7876671530477793e-05,
        "epoch": 0.2547156822249759,
        "step": 1850
    },
    {
        "loss": 2.4032,
        "grad_norm": 1.0960596799850464,
        "learning_rate": 2.778653553701932e-05,
        "epoch": 0.2548533663775299,
        "step": 1851
    },
    {
        "loss": 2.0701,
        "grad_norm": 1.327527403831482,
        "learning_rate": 2.769652198353151e-05,
        "epoch": 0.254991050530084,
        "step": 1852
    },
    {
        "loss": 2.284,
        "grad_norm": 1.362575888633728,
        "learning_rate": 2.7606631022634898e-05,
        "epoch": 0.255128734682638,
        "step": 1853
    },
    {
        "loss": 1.7447,
        "grad_norm": 2.381495714187622,
        "learning_rate": 2.7516862806742074e-05,
        "epoch": 0.2552664188351921,
        "step": 1854
    },
    {
        "loss": 2.2229,
        "grad_norm": 1.4221644401550293,
        "learning_rate": 2.7427217488057667e-05,
        "epoch": 0.25540410298774613,
        "step": 1855
    },
    {
        "loss": 2.0232,
        "grad_norm": 1.1264901161193848,
        "learning_rate": 2.7337695218577842e-05,
        "epoch": 0.2555417871403001,
        "step": 1856
    },
    {
        "loss": 2.3397,
        "grad_norm": 1.9147400856018066,
        "learning_rate": 2.7248296150090102e-05,
        "epoch": 0.2556794712928542,
        "step": 1857
    },
    {
        "loss": 2.4953,
        "grad_norm": 1.5529941320419312,
        "learning_rate": 2.7159020434173134e-05,
        "epoch": 0.25581715544540823,
        "step": 1858
    },
    {
        "loss": 1.9914,
        "grad_norm": 1.458299160003662,
        "learning_rate": 2.7069868222196358e-05,
        "epoch": 0.2559548395979623,
        "step": 1859
    },
    {
        "loss": 2.3416,
        "grad_norm": 2.3110647201538086,
        "learning_rate": 2.6980839665319922e-05,
        "epoch": 0.25609252375051633,
        "step": 1860
    },
    {
        "loss": 2.2566,
        "grad_norm": 1.6334768533706665,
        "learning_rate": 2.6891934914494333e-05,
        "epoch": 0.2562302079030704,
        "step": 1861
    },
    {
        "loss": 1.9689,
        "grad_norm": 2.7337381839752197,
        "learning_rate": 2.6803154120460007e-05,
        "epoch": 0.2563678920556244,
        "step": 1862
    },
    {
        "loss": 1.7508,
        "grad_norm": 1.8895384073257446,
        "learning_rate": 2.6714497433747332e-05,
        "epoch": 0.25650557620817843,
        "step": 1863
    },
    {
        "loss": 1.3661,
        "grad_norm": 3.1197657585144043,
        "learning_rate": 2.6625965004676255e-05,
        "epoch": 0.2566432603607325,
        "step": 1864
    },
    {
        "loss": 1.9546,
        "grad_norm": 2.505706548690796,
        "learning_rate": 2.653755698335594e-05,
        "epoch": 0.25678094451328654,
        "step": 1865
    },
    {
        "loss": 2.2475,
        "grad_norm": 1.2942185401916504,
        "learning_rate": 2.6449273519684702e-05,
        "epoch": 0.2569186286658406,
        "step": 1866
    },
    {
        "loss": 2.6308,
        "grad_norm": 1.9542800188064575,
        "learning_rate": 2.6361114763349727e-05,
        "epoch": 0.2570563128183946,
        "step": 1867
    },
    {
        "loss": 2.3304,
        "grad_norm": 2.047874927520752,
        "learning_rate": 2.6273080863826595e-05,
        "epoch": 0.25719399697094864,
        "step": 1868
    },
    {
        "loss": 1.3773,
        "grad_norm": 2.1842076778411865,
        "learning_rate": 2.6185171970379307e-05,
        "epoch": 0.2573316811235027,
        "step": 1869
    },
    {
        "loss": 2.606,
        "grad_norm": 1.078381896018982,
        "learning_rate": 2.6097388232059905e-05,
        "epoch": 0.25746936527605674,
        "step": 1870
    },
    {
        "loss": 2.2601,
        "grad_norm": 1.1781232357025146,
        "learning_rate": 2.6009729797708093e-05,
        "epoch": 0.2576070494286108,
        "step": 1871
    },
    {
        "loss": 2.1337,
        "grad_norm": 1.5381351709365845,
        "learning_rate": 2.5922196815951307e-05,
        "epoch": 0.2577447335811648,
        "step": 1872
    },
    {
        "loss": 2.2415,
        "grad_norm": 2.460752248764038,
        "learning_rate": 2.5834789435204243e-05,
        "epoch": 0.25788241773371884,
        "step": 1873
    },
    {
        "loss": 2.6781,
        "grad_norm": 2.111443281173706,
        "learning_rate": 2.5747507803668492e-05,
        "epoch": 0.2580201018862729,
        "step": 1874
    },
    {
        "loss": 2.7699,
        "grad_norm": 2.109987497329712,
        "learning_rate": 2.5660352069332606e-05,
        "epoch": 0.25815778603882694,
        "step": 1875
    },
    {
        "loss": 1.9672,
        "grad_norm": 2.776745080947876,
        "learning_rate": 2.557332237997151e-05,
        "epoch": 0.258295470191381,
        "step": 1876
    },
    {
        "loss": 2.4821,
        "grad_norm": 2.1005101203918457,
        "learning_rate": 2.5486418883146546e-05,
        "epoch": 0.258433154343935,
        "step": 1877
    },
    {
        "loss": 1.665,
        "grad_norm": 2.3022100925445557,
        "learning_rate": 2.5399641726205024e-05,
        "epoch": 0.25857083849648904,
        "step": 1878
    },
    {
        "loss": 1.8663,
        "grad_norm": 2.5720136165618896,
        "learning_rate": 2.5312991056280144e-05,
        "epoch": 0.2587085226490431,
        "step": 1879
    },
    {
        "loss": 2.4286,
        "grad_norm": 1.941414475440979,
        "learning_rate": 2.52264670202905e-05,
        "epoch": 0.25884620680159715,
        "step": 1880
    },
    {
        "loss": 2.616,
        "grad_norm": 1.604335069656372,
        "learning_rate": 2.5140069764940044e-05,
        "epoch": 0.2589838909541512,
        "step": 1881
    },
    {
        "loss": 1.8777,
        "grad_norm": 1.1771234273910522,
        "learning_rate": 2.505379943671782e-05,
        "epoch": 0.2591215751067052,
        "step": 1882
    },
    {
        "loss": 1.9315,
        "grad_norm": 1.7353976964950562,
        "learning_rate": 2.496765618189749e-05,
        "epoch": 0.25925925925925924,
        "step": 1883
    },
    {
        "loss": 2.1585,
        "grad_norm": 2.1001017093658447,
        "learning_rate": 2.4881640146537498e-05,
        "epoch": 0.2593969434118133,
        "step": 1884
    },
    {
        "loss": 2.1395,
        "grad_norm": 1.8845463991165161,
        "learning_rate": 2.479575147648041e-05,
        "epoch": 0.25953462756436735,
        "step": 1885
    },
    {
        "loss": 2.1663,
        "grad_norm": 1.3432152271270752,
        "learning_rate": 2.4709990317352916e-05,
        "epoch": 0.2596723117169214,
        "step": 1886
    },
    {
        "loss": 2.1377,
        "grad_norm": 1.6077971458435059,
        "learning_rate": 2.4624356814565497e-05,
        "epoch": 0.2598099958694754,
        "step": 1887
    },
    {
        "loss": 2.1485,
        "grad_norm": 1.8831907510757446,
        "learning_rate": 2.4538851113312133e-05,
        "epoch": 0.25994768002202945,
        "step": 1888
    },
    {
        "loss": 1.5128,
        "grad_norm": 1.768557071685791,
        "learning_rate": 2.4453473358570177e-05,
        "epoch": 0.2600853641745835,
        "step": 1889
    },
    {
        "loss": 2.2946,
        "grad_norm": 2.26324200630188,
        "learning_rate": 2.436822369509999e-05,
        "epoch": 0.26022304832713755,
        "step": 1890
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.678478717803955,
        "learning_rate": 2.42831022674449e-05,
        "epoch": 0.2603607324796916,
        "step": 1891
    },
    {
        "loss": 2.4998,
        "grad_norm": 1.8318867683410645,
        "learning_rate": 2.4198109219930586e-05,
        "epoch": 0.26049841663224566,
        "step": 1892
    },
    {
        "loss": 2.2681,
        "grad_norm": 2.2220170497894287,
        "learning_rate": 2.4113244696665194e-05,
        "epoch": 0.26063610078479965,
        "step": 1893
    },
    {
        "loss": 1.9074,
        "grad_norm": 1.5824676752090454,
        "learning_rate": 2.4028508841538953e-05,
        "epoch": 0.2607737849373537,
        "step": 1894
    },
    {
        "loss": 1.7451,
        "grad_norm": 1.6197489500045776,
        "learning_rate": 2.394390179822382e-05,
        "epoch": 0.26091146908990775,
        "step": 1895
    },
    {
        "loss": 1.9365,
        "grad_norm": 1.7901322841644287,
        "learning_rate": 2.385942371017351e-05,
        "epoch": 0.2610491532424618,
        "step": 1896
    },
    {
        "loss": 2.4681,
        "grad_norm": 2.070159673690796,
        "learning_rate": 2.377507472062299e-05,
        "epoch": 0.26118683739501586,
        "step": 1897
    },
    {
        "loss": 2.3238,
        "grad_norm": 1.0071066617965698,
        "learning_rate": 2.369085497258837e-05,
        "epoch": 0.26132452154756985,
        "step": 1898
    },
    {
        "loss": 0.7258,
        "grad_norm": 2.463107109069824,
        "learning_rate": 2.360676460886657e-05,
        "epoch": 0.2614622057001239,
        "step": 1899
    },
    {
        "loss": 2.3974,
        "grad_norm": 1.6332919597625732,
        "learning_rate": 2.352280377203523e-05,
        "epoch": 0.26159988985267796,
        "step": 1900
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.39145028591156,
        "learning_rate": 2.3438972604452236e-05,
        "epoch": 0.261737574005232,
        "step": 1901
    },
    {
        "loss": 2.0424,
        "grad_norm": 1.1204147338867188,
        "learning_rate": 2.3355271248255816e-05,
        "epoch": 0.26187525815778606,
        "step": 1902
    },
    {
        "loss": 1.349,
        "grad_norm": 1.3834387063980103,
        "learning_rate": 2.3271699845363882e-05,
        "epoch": 0.26201294231034006,
        "step": 1903
    },
    {
        "loss": 2.3776,
        "grad_norm": 1.8179200887680054,
        "learning_rate": 2.3188258537474184e-05,
        "epoch": 0.2621506264628941,
        "step": 1904
    },
    {
        "loss": 2.2501,
        "grad_norm": 1.5266155004501343,
        "learning_rate": 2.310494746606382e-05,
        "epoch": 0.26228831061544816,
        "step": 1905
    },
    {
        "loss": 2.126,
        "grad_norm": 1.7928752899169922,
        "learning_rate": 2.302176677238902e-05,
        "epoch": 0.2624259947680022,
        "step": 1906
    },
    {
        "loss": 2.2868,
        "grad_norm": 1.1365092992782593,
        "learning_rate": 2.2938716597484988e-05,
        "epoch": 0.26256367892055626,
        "step": 1907
    },
    {
        "loss": 2.3094,
        "grad_norm": 1.2115048170089722,
        "learning_rate": 2.285579708216574e-05,
        "epoch": 0.26270136307311026,
        "step": 1908
    },
    {
        "loss": 2.5215,
        "grad_norm": 1.1460018157958984,
        "learning_rate": 2.2773008367023563e-05,
        "epoch": 0.2628390472256643,
        "step": 1909
    },
    {
        "loss": 2.3919,
        "grad_norm": 1.0733550786972046,
        "learning_rate": 2.269035059242911e-05,
        "epoch": 0.26297673137821836,
        "step": 1910
    },
    {
        "loss": 1.6434,
        "grad_norm": 2.9718050956726074,
        "learning_rate": 2.260782389853098e-05,
        "epoch": 0.2631144155307724,
        "step": 1911
    },
    {
        "loss": 2.4244,
        "grad_norm": 1.8671342134475708,
        "learning_rate": 2.252542842525548e-05,
        "epoch": 0.26325209968332647,
        "step": 1912
    },
    {
        "loss": 2.4638,
        "grad_norm": 1.2223827838897705,
        "learning_rate": 2.2443164312306442e-05,
        "epoch": 0.26338978383588046,
        "step": 1913
    },
    {
        "loss": 1.7033,
        "grad_norm": 2.4293622970581055,
        "learning_rate": 2.23610316991651e-05,
        "epoch": 0.2635274679884345,
        "step": 1914
    },
    {
        "loss": 2.0473,
        "grad_norm": 2.061128616333008,
        "learning_rate": 2.227903072508951e-05,
        "epoch": 0.26366515214098857,
        "step": 1915
    },
    {
        "loss": 2.3242,
        "grad_norm": 1.3554391860961914,
        "learning_rate": 2.2197161529114717e-05,
        "epoch": 0.2638028362935426,
        "step": 1916
    },
    {
        "loss": 1.8657,
        "grad_norm": 2.557056188583374,
        "learning_rate": 2.211542425005225e-05,
        "epoch": 0.26394052044609667,
        "step": 1917
    },
    {
        "loss": 2.2564,
        "grad_norm": 1.5050928592681885,
        "learning_rate": 2.2033819026489943e-05,
        "epoch": 0.26407820459865067,
        "step": 1918
    },
    {
        "loss": 1.8089,
        "grad_norm": 1.7420706748962402,
        "learning_rate": 2.1952345996791733e-05,
        "epoch": 0.2642158887512047,
        "step": 1919
    },
    {
        "loss": 2.249,
        "grad_norm": 1.570888638496399,
        "learning_rate": 2.1871005299097547e-05,
        "epoch": 0.26435357290375877,
        "step": 1920
    },
    {
        "loss": 2.342,
        "grad_norm": 1.1656306982040405,
        "learning_rate": 2.178979707132276e-05,
        "epoch": 0.2644912570563128,
        "step": 1921
    },
    {
        "loss": 2.21,
        "grad_norm": 1.18729829788208,
        "learning_rate": 2.170872145115822e-05,
        "epoch": 0.2646289412088669,
        "step": 1922
    },
    {
        "loss": 2.3838,
        "grad_norm": 1.4064127206802368,
        "learning_rate": 2.162777857606998e-05,
        "epoch": 0.2647666253614209,
        "step": 1923
    },
    {
        "loss": 2.3484,
        "grad_norm": 2.1764955520629883,
        "learning_rate": 2.154696858329892e-05,
        "epoch": 0.2649043095139749,
        "step": 1924
    },
    {
        "loss": 2.0302,
        "grad_norm": 1.5471973419189453,
        "learning_rate": 2.1466291609860656e-05,
        "epoch": 0.265041993666529,
        "step": 1925
    },
    {
        "loss": 1.2716,
        "grad_norm": 1.6511411666870117,
        "learning_rate": 2.1385747792545364e-05,
        "epoch": 0.265179677819083,
        "step": 1926
    },
    {
        "loss": 2.0917,
        "grad_norm": 2.103271722793579,
        "learning_rate": 2.130533726791726e-05,
        "epoch": 0.2653173619716371,
        "step": 1927
    },
    {
        "loss": 2.0493,
        "grad_norm": 1.3384572267532349,
        "learning_rate": 2.1225060172314737e-05,
        "epoch": 0.26545504612419113,
        "step": 1928
    },
    {
        "loss": 1.4289,
        "grad_norm": 2.1375892162323,
        "learning_rate": 2.1144916641849887e-05,
        "epoch": 0.2655927302767451,
        "step": 1929
    },
    {
        "loss": 2.3678,
        "grad_norm": 1.715923547744751,
        "learning_rate": 2.106490681240827e-05,
        "epoch": 0.2657304144292992,
        "step": 1930
    },
    {
        "loss": 1.8458,
        "grad_norm": 2.7759902477264404,
        "learning_rate": 2.098503081964884e-05,
        "epoch": 0.26586809858185323,
        "step": 1931
    },
    {
        "loss": 2.5941,
        "grad_norm": 1.327739953994751,
        "learning_rate": 2.090528879900361e-05,
        "epoch": 0.2660057827344073,
        "step": 1932
    },
    {
        "loss": 2.277,
        "grad_norm": 1.995051622390747,
        "learning_rate": 2.0825680885677424e-05,
        "epoch": 0.26614346688696133,
        "step": 1933
    },
    {
        "loss": 2.4665,
        "grad_norm": 1.0878641605377197,
        "learning_rate": 2.074620721464775e-05,
        "epoch": 0.26628115103951533,
        "step": 1934
    },
    {
        "loss": 2.0397,
        "grad_norm": 2.0120081901550293,
        "learning_rate": 2.0666867920664468e-05,
        "epoch": 0.2664188351920694,
        "step": 1935
    },
    {
        "loss": 2.0382,
        "grad_norm": 2.4699504375457764,
        "learning_rate": 2.0587663138249548e-05,
        "epoch": 0.26655651934462343,
        "step": 1936
    },
    {
        "loss": 1.4377,
        "grad_norm": 1.6487035751342773,
        "learning_rate": 2.0508593001696942e-05,
        "epoch": 0.2666942034971775,
        "step": 1937
    },
    {
        "loss": 2.348,
        "grad_norm": 1.121395230293274,
        "learning_rate": 2.0429657645072365e-05,
        "epoch": 0.26683188764973154,
        "step": 1938
    },
    {
        "loss": 1.1782,
        "grad_norm": 2.4434800148010254,
        "learning_rate": 2.0350857202212847e-05,
        "epoch": 0.26696957180228553,
        "step": 1939
    },
    {
        "loss": 2.3035,
        "grad_norm": 1.7144246101379395,
        "learning_rate": 2.0272191806726882e-05,
        "epoch": 0.2671072559548396,
        "step": 1940
    },
    {
        "loss": 2.1016,
        "grad_norm": 2.104192018508911,
        "learning_rate": 2.0193661591993786e-05,
        "epoch": 0.26724494010739364,
        "step": 1941
    },
    {
        "loss": 2.1701,
        "grad_norm": 1.3012607097625732,
        "learning_rate": 2.011526669116378e-05,
        "epoch": 0.2673826242599477,
        "step": 1942
    },
    {
        "loss": 2.6631,
        "grad_norm": 1.9712353944778442,
        "learning_rate": 2.003700723715767e-05,
        "epoch": 0.26752030841250174,
        "step": 1943
    },
    {
        "loss": 2.3888,
        "grad_norm": 2.2704179286956787,
        "learning_rate": 1.995888336266648e-05,
        "epoch": 0.26765799256505574,
        "step": 1944
    },
    {
        "loss": 2.6298,
        "grad_norm": 1.4027241468429565,
        "learning_rate": 1.988089520015154e-05,
        "epoch": 0.2677956767176098,
        "step": 1945
    },
    {
        "loss": 1.6553,
        "grad_norm": 2.272780656814575,
        "learning_rate": 1.980304288184398e-05,
        "epoch": 0.26793336087016384,
        "step": 1946
    },
    {
        "loss": 1.5755,
        "grad_norm": 1.8809088468551636,
        "learning_rate": 1.972532653974456e-05,
        "epoch": 0.2680710450227179,
        "step": 1947
    },
    {
        "loss": 2.3094,
        "grad_norm": 1.9252588748931885,
        "learning_rate": 1.964774630562356e-05,
        "epoch": 0.26820872917527194,
        "step": 1948
    },
    {
        "loss": 2.2772,
        "grad_norm": 1.9980391263961792,
        "learning_rate": 1.9570302311020473e-05,
        "epoch": 0.268346413327826,
        "step": 1949
    },
    {
        "loss": 2.3639,
        "grad_norm": 1.658989429473877,
        "learning_rate": 1.9492994687243714e-05,
        "epoch": 0.26848409748038,
        "step": 1950
    },
    {
        "loss": 2.1872,
        "grad_norm": 1.8513270616531372,
        "learning_rate": 1.9415823565370607e-05,
        "epoch": 0.26862178163293404,
        "step": 1951
    },
    {
        "loss": 1.6092,
        "grad_norm": 1.6799191236495972,
        "learning_rate": 1.9338789076247e-05,
        "epoch": 0.2687594657854881,
        "step": 1952
    },
    {
        "loss": 2.584,
        "grad_norm": 1.1754515171051025,
        "learning_rate": 1.926189135048698e-05,
        "epoch": 0.26889714993804215,
        "step": 1953
    },
    {
        "loss": 2.2087,
        "grad_norm": 2.3661580085754395,
        "learning_rate": 1.918513051847285e-05,
        "epoch": 0.2690348340905962,
        "step": 1954
    },
    {
        "loss": 2.2316,
        "grad_norm": 1.4319452047348022,
        "learning_rate": 1.910850671035479e-05,
        "epoch": 0.2691725182431502,
        "step": 1955
    },
    {
        "loss": 2.3694,
        "grad_norm": 1.6036291122436523,
        "learning_rate": 1.903202005605057e-05,
        "epoch": 0.26931020239570425,
        "step": 1956
    },
    {
        "loss": 2.2807,
        "grad_norm": 1.7708631753921509,
        "learning_rate": 1.8955670685245497e-05,
        "epoch": 0.2694478865482583,
        "step": 1957
    },
    {
        "loss": 2.527,
        "grad_norm": 1.0779147148132324,
        "learning_rate": 1.887945872739214e-05,
        "epoch": 0.26958557070081235,
        "step": 1958
    },
    {
        "loss": 2.4072,
        "grad_norm": 1.427551031112671,
        "learning_rate": 1.880338431170997e-05,
        "epoch": 0.2697232548533664,
        "step": 1959
    },
    {
        "loss": 2.0055,
        "grad_norm": 1.5647938251495361,
        "learning_rate": 1.8727447567185342e-05,
        "epoch": 0.2698609390059204,
        "step": 1960
    },
    {
        "loss": 0.6676,
        "grad_norm": 2.3759233951568604,
        "learning_rate": 1.8651648622571148e-05,
        "epoch": 0.26999862315847445,
        "step": 1961
    },
    {
        "loss": 1.8036,
        "grad_norm": 1.5277196168899536,
        "learning_rate": 1.857598760638659e-05,
        "epoch": 0.2701363073110285,
        "step": 1962
    },
    {
        "loss": 2.3099,
        "grad_norm": 1.3152265548706055,
        "learning_rate": 1.8500464646917116e-05,
        "epoch": 0.27027399146358255,
        "step": 1963
    },
    {
        "loss": 2.694,
        "grad_norm": 3.177633285522461,
        "learning_rate": 1.8425079872214058e-05,
        "epoch": 0.2704116756161366,
        "step": 1964
    },
    {
        "loss": 1.7993,
        "grad_norm": 2.1141722202301025,
        "learning_rate": 1.83498334100944e-05,
        "epoch": 0.2705493597686906,
        "step": 1965
    },
    {
        "loss": 1.4447,
        "grad_norm": 2.219212293624878,
        "learning_rate": 1.8274725388140622e-05,
        "epoch": 0.27068704392124465,
        "step": 1966
    },
    {
        "loss": 1.5917,
        "grad_norm": 1.9591405391693115,
        "learning_rate": 1.8199755933700567e-05,
        "epoch": 0.2708247280737987,
        "step": 1967
    },
    {
        "loss": 2.154,
        "grad_norm": 2.343327283859253,
        "learning_rate": 1.8124925173887e-05,
        "epoch": 0.27096241222635276,
        "step": 1968
    },
    {
        "loss": 1.7319,
        "grad_norm": 2.548558473587036,
        "learning_rate": 1.8050233235577575e-05,
        "epoch": 0.2711000963789068,
        "step": 1969
    },
    {
        "loss": 1.9316,
        "grad_norm": 1.5397753715515137,
        "learning_rate": 1.7975680245414684e-05,
        "epoch": 0.2712377805314608,
        "step": 1970
    },
    {
        "loss": 2.5753,
        "grad_norm": 1.1848087310791016,
        "learning_rate": 1.7901266329804934e-05,
        "epoch": 0.27137546468401486,
        "step": 1971
    },
    {
        "loss": 2.449,
        "grad_norm": 1.0617202520370483,
        "learning_rate": 1.7826991614919265e-05,
        "epoch": 0.2715131488365689,
        "step": 1972
    },
    {
        "loss": 2.4299,
        "grad_norm": 1.1105250120162964,
        "learning_rate": 1.775285622669257e-05,
        "epoch": 0.27165083298912296,
        "step": 1973
    },
    {
        "loss": 1.7133,
        "grad_norm": 2.147792100906372,
        "learning_rate": 1.7678860290823428e-05,
        "epoch": 0.271788517141677,
        "step": 1974
    },
    {
        "loss": 2.438,
        "grad_norm": 1.6704554557800293,
        "learning_rate": 1.7605003932774077e-05,
        "epoch": 0.271926201294231,
        "step": 1975
    },
    {
        "loss": 2.4895,
        "grad_norm": 1.7173926830291748,
        "learning_rate": 1.7531287277770124e-05,
        "epoch": 0.27206388544678506,
        "step": 1976
    },
    {
        "loss": 2.2084,
        "grad_norm": 2.143472909927368,
        "learning_rate": 1.745771045080016e-05,
        "epoch": 0.2722015695993391,
        "step": 1977
    },
    {
        "loss": 2.0434,
        "grad_norm": 1.9368934631347656,
        "learning_rate": 1.7384273576615807e-05,
        "epoch": 0.27233925375189316,
        "step": 1978
    },
    {
        "loss": 2.0394,
        "grad_norm": 1.0530916452407837,
        "learning_rate": 1.731097677973138e-05,
        "epoch": 0.2724769379044472,
        "step": 1979
    },
    {
        "loss": 2.2966,
        "grad_norm": 0.9467560052871704,
        "learning_rate": 1.7237820184423624e-05,
        "epoch": 0.27261462205700127,
        "step": 1980
    },
    {
        "loss": 2.6188,
        "grad_norm": 1.3527358770370483,
        "learning_rate": 1.716480391473161e-05,
        "epoch": 0.27275230620955526,
        "step": 1981
    },
    {
        "loss": 2.0595,
        "grad_norm": 2.750136375427246,
        "learning_rate": 1.7091928094456565e-05,
        "epoch": 0.2728899903621093,
        "step": 1982
    },
    {
        "loss": 1.922,
        "grad_norm": 2.4217467308044434,
        "learning_rate": 1.7019192847161425e-05,
        "epoch": 0.27302767451466337,
        "step": 1983
    },
    {
        "loss": 1.8128,
        "grad_norm": 2.467888116836548,
        "learning_rate": 1.6946598296170914e-05,
        "epoch": 0.2731653586672174,
        "step": 1984
    },
    {
        "loss": 1.6169,
        "grad_norm": 2.218167543411255,
        "learning_rate": 1.6874144564571094e-05,
        "epoch": 0.27330304281977147,
        "step": 1985
    },
    {
        "loss": 2.4054,
        "grad_norm": 2.2277004718780518,
        "learning_rate": 1.680183177520932e-05,
        "epoch": 0.27344072697232547,
        "step": 1986
    },
    {
        "loss": 2.2439,
        "grad_norm": 2.087472915649414,
        "learning_rate": 1.6729660050694028e-05,
        "epoch": 0.2735784111248795,
        "step": 1987
    },
    {
        "loss": 2.055,
        "grad_norm": 1.68740713596344,
        "learning_rate": 1.6657629513394334e-05,
        "epoch": 0.27371609527743357,
        "step": 1988
    },
    {
        "loss": 2.119,
        "grad_norm": 1.2295454740524292,
        "learning_rate": 1.658574028544012e-05,
        "epoch": 0.2738537794299876,
        "step": 1989
    },
    {
        "loss": 1.8487,
        "grad_norm": 2.441422462463379,
        "learning_rate": 1.6513992488721608e-05,
        "epoch": 0.27399146358254167,
        "step": 1990
    },
    {
        "loss": 2.8099,
        "grad_norm": 1.7492740154266357,
        "learning_rate": 1.6442386244889163e-05,
        "epoch": 0.27412914773509567,
        "step": 1991
    },
    {
        "loss": 1.8524,
        "grad_norm": 1.7196656465530396,
        "learning_rate": 1.6370921675353223e-05,
        "epoch": 0.2742668318876497,
        "step": 1992
    },
    {
        "loss": 2.4224,
        "grad_norm": 1.0042022466659546,
        "learning_rate": 1.629959890128401e-05,
        "epoch": 0.27440451604020377,
        "step": 1993
    },
    {
        "loss": 2.2721,
        "grad_norm": 2.5283265113830566,
        "learning_rate": 1.6228418043611227e-05,
        "epoch": 0.2745422001927578,
        "step": 1994
    },
    {
        "loss": 2.1532,
        "grad_norm": 1.8904086351394653,
        "learning_rate": 1.6157379223024105e-05,
        "epoch": 0.2746798843453119,
        "step": 1995
    },
    {
        "loss": 2.3962,
        "grad_norm": 1.4752477407455444,
        "learning_rate": 1.608648255997097e-05,
        "epoch": 0.27481756849786587,
        "step": 1996
    },
    {
        "loss": 1.9989,
        "grad_norm": 1.0073440074920654,
        "learning_rate": 1.6015728174659085e-05,
        "epoch": 0.2749552526504199,
        "step": 1997
    },
    {
        "loss": 2.2767,
        "grad_norm": 1.5273518562316895,
        "learning_rate": 1.59451161870545e-05,
        "epoch": 0.275092936802974,
        "step": 1998
    },
    {
        "loss": 2.1402,
        "grad_norm": 1.1007578372955322,
        "learning_rate": 1.587464671688189e-05,
        "epoch": 0.275230620955528,
        "step": 1999
    },
    {
        "loss": 2.163,
        "grad_norm": 1.4412113428115845,
        "learning_rate": 1.5804319883624173e-05,
        "epoch": 0.2753683051080821,
        "step": 2000
    },
    {
        "loss": 1.9609,
        "grad_norm": 1.7426717281341553,
        "learning_rate": 1.5734135806522554e-05,
        "epoch": 0.2755059892606361,
        "step": 2001
    },
    {
        "loss": 1.6508,
        "grad_norm": 2.180232048034668,
        "learning_rate": 1.5664094604576063e-05,
        "epoch": 0.2756436734131901,
        "step": 2002
    },
    {
        "loss": 1.9849,
        "grad_norm": 2.262420892715454,
        "learning_rate": 1.5594196396541584e-05,
        "epoch": 0.2757813575657442,
        "step": 2003
    },
    {
        "loss": 2.3014,
        "grad_norm": 1.5272297859191895,
        "learning_rate": 1.5524441300933414e-05,
        "epoch": 0.27591904171829823,
        "step": 2004
    },
    {
        "loss": 1.8757,
        "grad_norm": 1.354231834411621,
        "learning_rate": 1.545482943602341e-05,
        "epoch": 0.2760567258708523,
        "step": 2005
    },
    {
        "loss": 2.2671,
        "grad_norm": 1.6671191453933716,
        "learning_rate": 1.5385360919840353e-05,
        "epoch": 0.2761944100234063,
        "step": 2006
    },
    {
        "loss": 1.7586,
        "grad_norm": 2.028707981109619,
        "learning_rate": 1.5316035870170143e-05,
        "epoch": 0.27633209417596033,
        "step": 2007
    },
    {
        "loss": 1.5607,
        "grad_norm": 1.4194743633270264,
        "learning_rate": 1.5246854404555355e-05,
        "epoch": 0.2764697783285144,
        "step": 2008
    },
    {
        "loss": 1.6166,
        "grad_norm": 2.101940393447876,
        "learning_rate": 1.5177816640295094e-05,
        "epoch": 0.27660746248106843,
        "step": 2009
    },
    {
        "loss": 2.4011,
        "grad_norm": 1.5133260488510132,
        "learning_rate": 1.5108922694444828e-05,
        "epoch": 0.2767451466336225,
        "step": 2010
    },
    {
        "loss": 2.3411,
        "grad_norm": 1.243935227394104,
        "learning_rate": 1.5040172683816267e-05,
        "epoch": 0.27688283078617654,
        "step": 2011
    },
    {
        "loss": 1.8928,
        "grad_norm": 1.7949424982070923,
        "learning_rate": 1.4971566724976915e-05,
        "epoch": 0.27702051493873053,
        "step": 2012
    },
    {
        "loss": 2.3984,
        "grad_norm": 1.5648467540740967,
        "learning_rate": 1.4903104934250145e-05,
        "epoch": 0.2771581990912846,
        "step": 2013
    },
    {
        "loss": 2.6377,
        "grad_norm": 1.7696304321289062,
        "learning_rate": 1.4834787427714869e-05,
        "epoch": 0.27729588324383864,
        "step": 2014
    },
    {
        "loss": 2.4362,
        "grad_norm": 1.4785557985305786,
        "learning_rate": 1.4766614321205351e-05,
        "epoch": 0.2774335673963927,
        "step": 2015
    },
    {
        "loss": 1.4075,
        "grad_norm": 1.5976754426956177,
        "learning_rate": 1.4698585730310976e-05,
        "epoch": 0.27757125154894674,
        "step": 2016
    },
    {
        "loss": 2.3561,
        "grad_norm": 1.8309879302978516,
        "learning_rate": 1.4630701770376221e-05,
        "epoch": 0.27770893570150074,
        "step": 2017
    },
    {
        "loss": 2.0228,
        "grad_norm": 1.7844009399414062,
        "learning_rate": 1.456296255650017e-05,
        "epoch": 0.2778466198540548,
        "step": 2018
    },
    {
        "loss": 2.2675,
        "grad_norm": 1.5528827905654907,
        "learning_rate": 1.4495368203536641e-05,
        "epoch": 0.27798430400660884,
        "step": 2019
    },
    {
        "loss": 1.8162,
        "grad_norm": 1.3728824853897095,
        "learning_rate": 1.4427918826093788e-05,
        "epoch": 0.2781219881591629,
        "step": 2020
    },
    {
        "loss": 2.4051,
        "grad_norm": 1.7027987241744995,
        "learning_rate": 1.4360614538533867e-05,
        "epoch": 0.27825967231171694,
        "step": 2021
    },
    {
        "loss": 2.3823,
        "grad_norm": 1.3542428016662598,
        "learning_rate": 1.4293455454973215e-05,
        "epoch": 0.27839735646427094,
        "step": 2022
    },
    {
        "loss": 2.3058,
        "grad_norm": 1.5231561660766602,
        "learning_rate": 1.4226441689282011e-05,
        "epoch": 0.278535040616825,
        "step": 2023
    },
    {
        "loss": 1.9121,
        "grad_norm": 2.2322816848754883,
        "learning_rate": 1.4159573355083943e-05,
        "epoch": 0.27867272476937904,
        "step": 2024
    },
    {
        "loss": 1.5395,
        "grad_norm": 3.8985860347747803,
        "learning_rate": 1.4092850565756165e-05,
        "epoch": 0.2788104089219331,
        "step": 2025
    },
    {
        "loss": 1.9493,
        "grad_norm": 1.1984844207763672,
        "learning_rate": 1.4026273434429093e-05,
        "epoch": 0.27894809307448715,
        "step": 2026
    },
    {
        "loss": 2.2593,
        "grad_norm": 1.9670675992965698,
        "learning_rate": 1.3959842073986085e-05,
        "epoch": 0.27908577722704114,
        "step": 2027
    },
    {
        "loss": 1.9381,
        "grad_norm": 1.8812270164489746,
        "learning_rate": 1.3893556597063418e-05,
        "epoch": 0.2792234613795952,
        "step": 2028
    },
    {
        "loss": 2.0798,
        "grad_norm": 1.5240888595581055,
        "learning_rate": 1.3827417116049945e-05,
        "epoch": 0.27936114553214925,
        "step": 2029
    },
    {
        "loss": 1.8178,
        "grad_norm": 1.106074571609497,
        "learning_rate": 1.3761423743087066e-05,
        "epoch": 0.2794988296847033,
        "step": 2030
    },
    {
        "loss": 2.5557,
        "grad_norm": 2.0029103755950928,
        "learning_rate": 1.369557659006846e-05,
        "epoch": 0.27963651383725735,
        "step": 2031
    },
    {
        "loss": 1.6632,
        "grad_norm": 2.7925260066986084,
        "learning_rate": 1.3629875768639755e-05,
        "epoch": 0.27977419798981135,
        "step": 2032
    },
    {
        "loss": 2.3438,
        "grad_norm": 1.7825905084609985,
        "learning_rate": 1.3564321390198586e-05,
        "epoch": 0.2799118821423654,
        "step": 2033
    },
    {
        "loss": 2.4752,
        "grad_norm": 1.4748663902282715,
        "learning_rate": 1.3498913565894266e-05,
        "epoch": 0.28004956629491945,
        "step": 2034
    },
    {
        "loss": 2.6235,
        "grad_norm": 1.6239155530929565,
        "learning_rate": 1.3433652406627562e-05,
        "epoch": 0.2801872504474735,
        "step": 2035
    },
    {
        "loss": 2.3007,
        "grad_norm": 1.2133727073669434,
        "learning_rate": 1.336853802305067e-05,
        "epoch": 0.28032493460002755,
        "step": 2036
    },
    {
        "loss": 2.4298,
        "grad_norm": 2.171020746231079,
        "learning_rate": 1.3303570525566866e-05,
        "epoch": 0.2804626187525816,
        "step": 2037
    },
    {
        "loss": 2.0426,
        "grad_norm": 1.5191082954406738,
        "learning_rate": 1.3238750024330338e-05,
        "epoch": 0.2806003029051356,
        "step": 2038
    },
    {
        "loss": 2.207,
        "grad_norm": 1.639859676361084,
        "learning_rate": 1.317407662924609e-05,
        "epoch": 0.28073798705768965,
        "step": 2039
    },
    {
        "loss": 2.0078,
        "grad_norm": 1.9123741388320923,
        "learning_rate": 1.3109550449969732e-05,
        "epoch": 0.2808756712102437,
        "step": 2040
    },
    {
        "loss": 1.5639,
        "grad_norm": 2.1795456409454346,
        "learning_rate": 1.3045171595907157e-05,
        "epoch": 0.28101335536279776,
        "step": 2041
    },
    {
        "loss": 2.3757,
        "grad_norm": 1.8227866888046265,
        "learning_rate": 1.2980940176214562e-05,
        "epoch": 0.2811510395153518,
        "step": 2042
    },
    {
        "loss": 1.3129,
        "grad_norm": 2.404395818710327,
        "learning_rate": 1.2916856299798185e-05,
        "epoch": 0.2812887236679058,
        "step": 2043
    },
    {
        "loss": 1.0478,
        "grad_norm": 2.0949113368988037,
        "learning_rate": 1.2852920075314e-05,
        "epoch": 0.28142640782045986,
        "step": 2044
    },
    {
        "loss": 2.219,
        "grad_norm": 1.7878309488296509,
        "learning_rate": 1.2789131611167683e-05,
        "epoch": 0.2815640919730139,
        "step": 2045
    },
    {
        "loss": 2.2789,
        "grad_norm": 2.0174262523651123,
        "learning_rate": 1.2725491015514402e-05,
        "epoch": 0.28170177612556796,
        "step": 2046
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.418117642402649,
        "learning_rate": 1.2661998396258535e-05,
        "epoch": 0.281839460278122,
        "step": 2047
    },
    {
        "loss": 2.2982,
        "grad_norm": 1.3446354866027832,
        "learning_rate": 1.2598653861053677e-05,
        "epoch": 0.281977144430676,
        "step": 2048
    },
    {
        "loss": 1.7325,
        "grad_norm": 1.6867510080337524,
        "learning_rate": 1.2535457517302263e-05,
        "epoch": 0.28211482858323006,
        "step": 2049
    },
    {
        "loss": 1.6619,
        "grad_norm": 1.8530436754226685,
        "learning_rate": 1.2472409472155456e-05,
        "epoch": 0.2822525127357841,
        "step": 2050
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.1766499280929565,
        "learning_rate": 1.2409509832513e-05,
        "epoch": 0.28239019688833816,
        "step": 2051
    },
    {
        "loss": 2.5595,
        "grad_norm": 1.8949919939041138,
        "learning_rate": 1.2346758705023054e-05,
        "epoch": 0.2825278810408922,
        "step": 2052
    },
    {
        "loss": 1.6441,
        "grad_norm": 2.4176058769226074,
        "learning_rate": 1.2284156196081852e-05,
        "epoch": 0.2826655651934462,
        "step": 2053
    },
    {
        "loss": 2.1506,
        "grad_norm": 2.200857639312744,
        "learning_rate": 1.2221702411833769e-05,
        "epoch": 0.28280324934600026,
        "step": 2054
    },
    {
        "loss": 2.5801,
        "grad_norm": 1.6093500852584839,
        "learning_rate": 1.2159397458170996e-05,
        "epoch": 0.2829409334985543,
        "step": 2055
    },
    {
        "loss": 2.6619,
        "grad_norm": 1.085663914680481,
        "learning_rate": 1.2097241440733286e-05,
        "epoch": 0.28307861765110837,
        "step": 2056
    },
    {
        "loss": 1.4865,
        "grad_norm": 1.6586155891418457,
        "learning_rate": 1.203523446490794e-05,
        "epoch": 0.2832163018036624,
        "step": 2057
    },
    {
        "loss": 2.4485,
        "grad_norm": 1.1808788776397705,
        "learning_rate": 1.1973376635829558e-05,
        "epoch": 0.2833539859562164,
        "step": 2058
    },
    {
        "loss": 1.5664,
        "grad_norm": 2.647581100463867,
        "learning_rate": 1.191166805837981e-05,
        "epoch": 0.28349167010877047,
        "step": 2059
    },
    {
        "loss": 2.0781,
        "grad_norm": 1.9573577642440796,
        "learning_rate": 1.1850108837187313e-05,
        "epoch": 0.2836293542613245,
        "step": 2060
    },
    {
        "loss": 2.503,
        "grad_norm": 1.803886890411377,
        "learning_rate": 1.1788699076627552e-05,
        "epoch": 0.28376703841387857,
        "step": 2061
    },
    {
        "loss": 2.3328,
        "grad_norm": 2.029529094696045,
        "learning_rate": 1.1727438880822428e-05,
        "epoch": 0.2839047225664326,
        "step": 2062
    },
    {
        "loss": 2.1116,
        "grad_norm": 1.0793250799179077,
        "learning_rate": 1.1666328353640377e-05,
        "epoch": 0.2840424067189866,
        "step": 2063
    },
    {
        "loss": 2.0886,
        "grad_norm": 1.6283791065216064,
        "learning_rate": 1.1605367598696027e-05,
        "epoch": 0.28418009087154067,
        "step": 2064
    },
    {
        "loss": 2.2447,
        "grad_norm": 1.4412097930908203,
        "learning_rate": 1.154455671935002e-05,
        "epoch": 0.2843177750240947,
        "step": 2065
    },
    {
        "loss": 1.4958,
        "grad_norm": 2.5598690509796143,
        "learning_rate": 1.1483895818708979e-05,
        "epoch": 0.2844554591766488,
        "step": 2066
    },
    {
        "loss": 1.7953,
        "grad_norm": 1.782548427581787,
        "learning_rate": 1.1423384999625109e-05,
        "epoch": 0.2845931433292028,
        "step": 2067
    },
    {
        "loss": 2.4038,
        "grad_norm": 1.7396069765090942,
        "learning_rate": 1.1363024364696273e-05,
        "epoch": 0.2847308274817569,
        "step": 2068
    },
    {
        "loss": 1.8703,
        "grad_norm": 1.2495580911636353,
        "learning_rate": 1.130281401626564e-05,
        "epoch": 0.2848685116343109,
        "step": 2069
    },
    {
        "loss": 2.3328,
        "grad_norm": 1.3009963035583496,
        "learning_rate": 1.1242754056421511e-05,
        "epoch": 0.2850061957868649,
        "step": 2070
    },
    {
        "loss": 1.919,
        "grad_norm": 2.679460048675537,
        "learning_rate": 1.1182844586997266e-05,
        "epoch": 0.285143879939419,
        "step": 2071
    },
    {
        "loss": 2.3647,
        "grad_norm": 1.7400208711624146,
        "learning_rate": 1.1123085709571102e-05,
        "epoch": 0.28528156409197303,
        "step": 2072
    },
    {
        "loss": 2.6624,
        "grad_norm": 1.2134814262390137,
        "learning_rate": 1.1063477525465893e-05,
        "epoch": 0.2854192482445271,
        "step": 2073
    },
    {
        "loss": 2.1709,
        "grad_norm": 1.6459544897079468,
        "learning_rate": 1.1004020135748993e-05,
        "epoch": 0.2855569323970811,
        "step": 2074
    },
    {
        "loss": 2.8447,
        "grad_norm": 1.3108901977539062,
        "learning_rate": 1.0944713641232118e-05,
        "epoch": 0.2856946165496351,
        "step": 2075
    },
    {
        "loss": 1.8658,
        "grad_norm": 1.6626136302947998,
        "learning_rate": 1.0885558142471053e-05,
        "epoch": 0.2858323007021892,
        "step": 2076
    },
    {
        "loss": 1.2467,
        "grad_norm": 2.777836561203003,
        "learning_rate": 1.0826553739765622e-05,
        "epoch": 0.28596998485474323,
        "step": 2077
    },
    {
        "loss": 2.057,
        "grad_norm": 1.4458423852920532,
        "learning_rate": 1.0767700533159519e-05,
        "epoch": 0.2861076690072973,
        "step": 2078
    },
    {
        "loss": 2.281,
        "grad_norm": 1.2511590719223022,
        "learning_rate": 1.0708998622439947e-05,
        "epoch": 0.2862453531598513,
        "step": 2079
    },
    {
        "loss": 2.092,
        "grad_norm": 1.541231393814087,
        "learning_rate": 1.0650448107137701e-05,
        "epoch": 0.28638303731240533,
        "step": 2080
    },
    {
        "loss": 2.5058,
        "grad_norm": 1.4075959920883179,
        "learning_rate": 1.0592049086526845e-05,
        "epoch": 0.2865207214649594,
        "step": 2081
    },
    {
        "loss": 1.6835,
        "grad_norm": 1.9386847019195557,
        "learning_rate": 1.0533801659624531e-05,
        "epoch": 0.28665840561751343,
        "step": 2082
    },
    {
        "loss": 1.49,
        "grad_norm": 2.979825258255005,
        "learning_rate": 1.0475705925190938e-05,
        "epoch": 0.2867960897700675,
        "step": 2083
    },
    {
        "loss": 2.169,
        "grad_norm": 1.308117389678955,
        "learning_rate": 1.0417761981729035e-05,
        "epoch": 0.2869337739226215,
        "step": 2084
    },
    {
        "loss": 2.3407,
        "grad_norm": 1.0350509881973267,
        "learning_rate": 1.035996992748437e-05,
        "epoch": 0.28707145807517553,
        "step": 2085
    },
    {
        "loss": 1.6563,
        "grad_norm": 3.271042823791504,
        "learning_rate": 1.0302329860445059e-05,
        "epoch": 0.2872091422277296,
        "step": 2086
    },
    {
        "loss": 2.2266,
        "grad_norm": 1.8490304946899414,
        "learning_rate": 1.0244841878341471e-05,
        "epoch": 0.28734682638028364,
        "step": 2087
    },
    {
        "loss": 1.6566,
        "grad_norm": 2.471144676208496,
        "learning_rate": 1.0187506078646047e-05,
        "epoch": 0.2874845105328377,
        "step": 2088
    },
    {
        "loss": 1.952,
        "grad_norm": 2.5261380672454834,
        "learning_rate": 1.013032255857328e-05,
        "epoch": 0.2876221946853917,
        "step": 2089
    },
    {
        "loss": 2.3185,
        "grad_norm": 1.1505409479141235,
        "learning_rate": 1.007329141507949e-05,
        "epoch": 0.28775987883794574,
        "step": 2090
    },
    {
        "loss": 2.532,
        "grad_norm": 1.4989112615585327,
        "learning_rate": 1.0016412744862524e-05,
        "epoch": 0.2878975629904998,
        "step": 2091
    },
    {
        "loss": 2.1573,
        "grad_norm": 2.470916509628296,
        "learning_rate": 9.959686644361821e-06,
        "epoch": 0.28803524714305384,
        "step": 2092
    },
    {
        "loss": 2.1816,
        "grad_norm": 1.910353183746338,
        "learning_rate": 9.903113209758096e-06,
        "epoch": 0.2881729312956079,
        "step": 2093
    },
    {
        "loss": 2.0794,
        "grad_norm": 1.6770873069763184,
        "learning_rate": 9.846692536973167e-06,
        "epoch": 0.28831061544816194,
        "step": 2094
    },
    {
        "loss": 2.1557,
        "grad_norm": 1.5479432344436646,
        "learning_rate": 9.7904247216699e-06,
        "epoch": 0.28844829960071594,
        "step": 2095
    },
    {
        "loss": 2.2343,
        "grad_norm": 1.7887109518051147,
        "learning_rate": 9.73430985925199e-06,
        "epoch": 0.28858598375327,
        "step": 2096
    },
    {
        "loss": 2.0356,
        "grad_norm": 1.7012590169906616,
        "learning_rate": 9.678348044863705e-06,
        "epoch": 0.28872366790582404,
        "step": 2097
    },
    {
        "loss": 2.3004,
        "grad_norm": 1.1773407459259033,
        "learning_rate": 9.622539373389938e-06,
        "epoch": 0.2888613520583781,
        "step": 2098
    },
    {
        "loss": 2.8388,
        "grad_norm": 1.4820642471313477,
        "learning_rate": 9.566883939455895e-06,
        "epoch": 0.28899903621093215,
        "step": 2099
    },
    {
        "loss": 1.8099,
        "grad_norm": 1.6697442531585693,
        "learning_rate": 9.51138183742687e-06,
        "epoch": 0.28913672036348614,
        "step": 2100
    },
    {
        "loss": 1.7072,
        "grad_norm": 1.783107876777649,
        "learning_rate": 9.456033161408273e-06,
        "epoch": 0.2892744045160402,
        "step": 2101
    },
    {
        "loss": 2.375,
        "grad_norm": 1.2736870050430298,
        "learning_rate": 9.400838005245382e-06,
        "epoch": 0.28941208866859425,
        "step": 2102
    },
    {
        "loss": 2.1411,
        "grad_norm": 3.0902116298675537,
        "learning_rate": 9.345796462523071e-06,
        "epoch": 0.2895497728211483,
        "step": 2103
    },
    {
        "loss": 2.2979,
        "grad_norm": 1.0333678722381592,
        "learning_rate": 9.29090862656593e-06,
        "epoch": 0.28968745697370235,
        "step": 2104
    },
    {
        "loss": 2.0622,
        "grad_norm": 1.4061164855957031,
        "learning_rate": 9.236174590437762e-06,
        "epoch": 0.28982514112625635,
        "step": 2105
    },
    {
        "loss": 2.2452,
        "grad_norm": 0.9888097047805786,
        "learning_rate": 9.181594446941689e-06,
        "epoch": 0.2899628252788104,
        "step": 2106
    },
    {
        "loss": 2.238,
        "grad_norm": 1.6280871629714966,
        "learning_rate": 9.127168288619892e-06,
        "epoch": 0.29010050943136445,
        "step": 2107
    },
    {
        "loss": 2.4273,
        "grad_norm": 1.341152310371399,
        "learning_rate": 9.07289620775349e-06,
        "epoch": 0.2902381935839185,
        "step": 2108
    },
    {
        "loss": 2.2752,
        "grad_norm": 2.0541088581085205,
        "learning_rate": 9.018778296362263e-06,
        "epoch": 0.29037587773647255,
        "step": 2109
    },
    {
        "loss": 2.1146,
        "grad_norm": 1.5946464538574219,
        "learning_rate": 8.964814646204733e-06,
        "epoch": 0.29051356188902655,
        "step": 2110
    },
    {
        "loss": 1.2827,
        "grad_norm": 1.839767575263977,
        "learning_rate": 8.911005348777746e-06,
        "epoch": 0.2906512460415806,
        "step": 2111
    },
    {
        "loss": 1.8264,
        "grad_norm": 2.4082772731781006,
        "learning_rate": 8.857350495316496e-06,
        "epoch": 0.29078893019413465,
        "step": 2112
    },
    {
        "loss": 2.1248,
        "grad_norm": 1.6660969257354736,
        "learning_rate": 8.803850176794337e-06,
        "epoch": 0.2909266143466887,
        "step": 2113
    },
    {
        "loss": 1.8147,
        "grad_norm": 2.30098819732666,
        "learning_rate": 8.750504483922506e-06,
        "epoch": 0.29106429849924276,
        "step": 2114
    },
    {
        "loss": 2.3037,
        "grad_norm": 1.498998761177063,
        "learning_rate": 8.697313507150184e-06,
        "epoch": 0.29120198265179675,
        "step": 2115
    },
    {
        "loss": 1.93,
        "grad_norm": 2.212712287902832,
        "learning_rate": 8.644277336664175e-06,
        "epoch": 0.2913396668043508,
        "step": 2116
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.534161925315857,
        "learning_rate": 8.59139606238879e-06,
        "epoch": 0.29147735095690486,
        "step": 2117
    },
    {
        "loss": 1.7503,
        "grad_norm": 2.5980913639068604,
        "learning_rate": 8.538669773985753e-06,
        "epoch": 0.2916150351094589,
        "step": 2118
    },
    {
        "loss": 1.9104,
        "grad_norm": 2.722665786743164,
        "learning_rate": 8.486098560853928e-06,
        "epoch": 0.29175271926201296,
        "step": 2119
    },
    {
        "loss": 2.05,
        "grad_norm": 1.5406997203826904,
        "learning_rate": 8.433682512129314e-06,
        "epoch": 0.29189040341456696,
        "step": 2120
    },
    {
        "loss": 1.5156,
        "grad_norm": 2.4161014556884766,
        "learning_rate": 8.3814217166848e-06,
        "epoch": 0.292028087567121,
        "step": 2121
    },
    {
        "loss": 1.5997,
        "grad_norm": 1.9232488870620728,
        "learning_rate": 8.329316263130082e-06,
        "epoch": 0.29216577171967506,
        "step": 2122
    },
    {
        "loss": 2.359,
        "grad_norm": 1.5098443031311035,
        "learning_rate": 8.27736623981139e-06,
        "epoch": 0.2923034558722291,
        "step": 2123
    },
    {
        "loss": 2.2894,
        "grad_norm": 1.0242961645126343,
        "learning_rate": 8.225571734811455e-06,
        "epoch": 0.29244114002478316,
        "step": 2124
    },
    {
        "loss": 1.7891,
        "grad_norm": 3.088919162750244,
        "learning_rate": 8.173932835949349e-06,
        "epoch": 0.2925788241773372,
        "step": 2125
    },
    {
        "loss": 2.3266,
        "grad_norm": 1.1912062168121338,
        "learning_rate": 8.122449630780238e-06,
        "epoch": 0.2927165083298912,
        "step": 2126
    },
    {
        "loss": 1.7272,
        "grad_norm": 1.1644845008850098,
        "learning_rate": 8.071122206595405e-06,
        "epoch": 0.29285419248244526,
        "step": 2127
    },
    {
        "loss": 1.7602,
        "grad_norm": 2.263605833053589,
        "learning_rate": 8.019950650421937e-06,
        "epoch": 0.2929918766349993,
        "step": 2128
    },
    {
        "loss": 2.3527,
        "grad_norm": 1.267932415008545,
        "learning_rate": 7.968935049022652e-06,
        "epoch": 0.29312956078755337,
        "step": 2129
    },
    {
        "loss": 2.2003,
        "grad_norm": 1.6948614120483398,
        "learning_rate": 7.918075488895916e-06,
        "epoch": 0.2932672449401074,
        "step": 2130
    },
    {
        "loss": 2.1401,
        "grad_norm": 1.1133322715759277,
        "learning_rate": 7.867372056275602e-06,
        "epoch": 0.2934049290926614,
        "step": 2131
    },
    {
        "loss": 2.2435,
        "grad_norm": 1.8446568250656128,
        "learning_rate": 7.816824837130743e-06,
        "epoch": 0.29354261324521547,
        "step": 2132
    },
    {
        "loss": 2.1261,
        "grad_norm": 2.3669931888580322,
        "learning_rate": 7.766433917165628e-06,
        "epoch": 0.2936802973977695,
        "step": 2133
    },
    {
        "loss": 1.9009,
        "grad_norm": 1.7835097312927246,
        "learning_rate": 7.716199381819511e-06,
        "epoch": 0.29381798155032357,
        "step": 2134
    },
    {
        "loss": 2.242,
        "grad_norm": 1.6623982191085815,
        "learning_rate": 7.666121316266417e-06,
        "epoch": 0.2939556657028776,
        "step": 2135
    },
    {
        "loss": 2.0175,
        "grad_norm": 2.566044569015503,
        "learning_rate": 7.616199805415136e-06,
        "epoch": 0.2940933498554316,
        "step": 2136
    },
    {
        "loss": 2.102,
        "grad_norm": 1.5637716054916382,
        "learning_rate": 7.566434933909017e-06,
        "epoch": 0.29423103400798567,
        "step": 2137
    },
    {
        "loss": 2.2446,
        "grad_norm": 1.525439977645874,
        "learning_rate": 7.516826786125763e-06,
        "epoch": 0.2943687181605397,
        "step": 2138
    },
    {
        "loss": 2.0164,
        "grad_norm": 1.5440047979354858,
        "learning_rate": 7.467375446177449e-06,
        "epoch": 0.2945064023130938,
        "step": 2139
    },
    {
        "loss": 2.4426,
        "grad_norm": 1.4405109882354736,
        "learning_rate": 7.4180809979102036e-06,
        "epoch": 0.2946440864656478,
        "step": 2140
    },
    {
        "loss": 1.9593,
        "grad_norm": 2.353463888168335,
        "learning_rate": 7.368943524904126e-06,
        "epoch": 0.2947817706182018,
        "step": 2141
    },
    {
        "loss": 2.1697,
        "grad_norm": 2.8098907470703125,
        "learning_rate": 7.319963110473216e-06,
        "epoch": 0.2949194547707559,
        "step": 2142
    },
    {
        "loss": 2.1308,
        "grad_norm": 2.0102765560150146,
        "learning_rate": 7.271139837665164e-06,
        "epoch": 0.2950571389233099,
        "step": 2143
    },
    {
        "loss": 2.3109,
        "grad_norm": 2.0902650356292725,
        "learning_rate": 7.2224737892611635e-06,
        "epoch": 0.295194823075864,
        "step": 2144
    },
    {
        "loss": 2.4558,
        "grad_norm": 1.4482723474502563,
        "learning_rate": 7.173965047775899e-06,
        "epoch": 0.29533250722841803,
        "step": 2145
    },
    {
        "loss": 2.2848,
        "grad_norm": 2.3057966232299805,
        "learning_rate": 7.125613695457356e-06,
        "epoch": 0.295470191380972,
        "step": 2146
    },
    {
        "loss": 2.4075,
        "grad_norm": 1.562067985534668,
        "learning_rate": 7.077419814286557e-06,
        "epoch": 0.2956078755335261,
        "step": 2147
    },
    {
        "loss": 2.3953,
        "grad_norm": 1.8045493364334106,
        "learning_rate": 7.029383485977625e-06,
        "epoch": 0.29574555968608013,
        "step": 2148
    },
    {
        "loss": 2.1988,
        "grad_norm": 3.8237507343292236,
        "learning_rate": 6.981504791977511e-06,
        "epoch": 0.2958832438386342,
        "step": 2149
    },
    {
        "loss": 2.1137,
        "grad_norm": 2.4051671028137207,
        "learning_rate": 6.9337838134658665e-06,
        "epoch": 0.29602092799118823,
        "step": 2150
    },
    {
        "loss": 2.2832,
        "grad_norm": 1.12457275390625,
        "learning_rate": 6.886220631355e-06,
        "epoch": 0.29615861214374223,
        "step": 2151
    },
    {
        "loss": 1.8482,
        "grad_norm": 2.623610019683838,
        "learning_rate": 6.838815326289594e-06,
        "epoch": 0.2962962962962963,
        "step": 2152
    },
    {
        "loss": 1.9725,
        "grad_norm": 2.048459529876709,
        "learning_rate": 6.791567978646707e-06,
        "epoch": 0.29643398044885033,
        "step": 2153
    },
    {
        "loss": 2.287,
        "grad_norm": 1.9550942182540894,
        "learning_rate": 6.744478668535559e-06,
        "epoch": 0.2965716646014044,
        "step": 2154
    },
    {
        "loss": 2.2606,
        "grad_norm": 2.214752674102783,
        "learning_rate": 6.6975474757973835e-06,
        "epoch": 0.29670934875395844,
        "step": 2155
    },
    {
        "loss": 1.9777,
        "grad_norm": 2.489652395248413,
        "learning_rate": 6.650774480005328e-06,
        "epoch": 0.2968470329065125,
        "step": 2156
    },
    {
        "loss": 1.6093,
        "grad_norm": 2.192636251449585,
        "learning_rate": 6.60415976046439e-06,
        "epoch": 0.2969847170590665,
        "step": 2157
    },
    {
        "loss": 2.1468,
        "grad_norm": 1.56693696975708,
        "learning_rate": 6.55770339621109e-06,
        "epoch": 0.29712240121162053,
        "step": 2158
    },
    {
        "loss": 2.2318,
        "grad_norm": 1.7312076091766357,
        "learning_rate": 6.5114054660135315e-06,
        "epoch": 0.2972600853641746,
        "step": 2159
    },
    {
        "loss": 1.3512,
        "grad_norm": 2.694910764694214,
        "learning_rate": 6.465266048371188e-06,
        "epoch": 0.29739776951672864,
        "step": 2160
    },
    {
        "loss": 2.0866,
        "grad_norm": 1.9504108428955078,
        "learning_rate": 6.419285221514693e-06,
        "epoch": 0.2975354536692827,
        "step": 2161
    },
    {
        "loss": 2.1986,
        "grad_norm": 1.2383700609207153,
        "learning_rate": 6.373463063405849e-06,
        "epoch": 0.2976731378218367,
        "step": 2162
    },
    {
        "loss": 2.0467,
        "grad_norm": 1.3335437774658203,
        "learning_rate": 6.327799651737476e-06,
        "epoch": 0.29781082197439074,
        "step": 2163
    },
    {
        "loss": 1.9843,
        "grad_norm": 1.4807711839675903,
        "learning_rate": 6.282295063933119e-06,
        "epoch": 0.2979485061269448,
        "step": 2164
    },
    {
        "loss": 2.0734,
        "grad_norm": 2.000469923019409,
        "learning_rate": 6.236949377147105e-06,
        "epoch": 0.29808619027949884,
        "step": 2165
    },
    {
        "loss": 2.1096,
        "grad_norm": 1.9631446599960327,
        "learning_rate": 6.191762668264367e-06,
        "epoch": 0.2982238744320529,
        "step": 2166
    },
    {
        "loss": 2.0002,
        "grad_norm": 1.9029697179794312,
        "learning_rate": 6.146735013900184e-06,
        "epoch": 0.2983615585846069,
        "step": 2167
    },
    {
        "loss": 2.2495,
        "grad_norm": 2.2479677200317383,
        "learning_rate": 6.101866490400255e-06,
        "epoch": 0.29849924273716094,
        "step": 2168
    },
    {
        "loss": 2.1759,
        "grad_norm": 1.700932502746582,
        "learning_rate": 6.0571571738404445e-06,
        "epoch": 0.298636926889715,
        "step": 2169
    },
    {
        "loss": 2.2177,
        "grad_norm": 2.0746824741363525,
        "learning_rate": 6.012607140026605e-06,
        "epoch": 0.29877461104226904,
        "step": 2170
    },
    {
        "loss": 1.865,
        "grad_norm": 2.321093797683716,
        "learning_rate": 5.968216464494647e-06,
        "epoch": 0.2989122951948231,
        "step": 2171
    },
    {
        "loss": 1.942,
        "grad_norm": 2.3604161739349365,
        "learning_rate": 5.923985222510187e-06,
        "epoch": 0.2990499793473771,
        "step": 2172
    },
    {
        "loss": 2.0654,
        "grad_norm": 1.4559338092803955,
        "learning_rate": 5.879913489068545e-06,
        "epoch": 0.29918766349993114,
        "step": 2173
    },
    {
        "loss": 2.2697,
        "grad_norm": 1.7662369012832642,
        "learning_rate": 5.836001338894581e-06,
        "epoch": 0.2993253476524852,
        "step": 2174
    },
    {
        "loss": 2.2167,
        "grad_norm": 2.0106985569000244,
        "learning_rate": 5.792248846442594e-06,
        "epoch": 0.29946303180503925,
        "step": 2175
    },
    {
        "loss": 2.1148,
        "grad_norm": 2.1660141944885254,
        "learning_rate": 5.748656085896187e-06,
        "epoch": 0.2996007159575933,
        "step": 2176
    },
    {
        "loss": 2.129,
        "grad_norm": 1.3140336275100708,
        "learning_rate": 5.705223131168091e-06,
        "epoch": 0.2997384001101473,
        "step": 2177
    },
    {
        "loss": 2.7559,
        "grad_norm": 2.3147218227386475,
        "learning_rate": 5.661950055900156e-06,
        "epoch": 0.29987608426270135,
        "step": 2178
    },
    {
        "loss": 2.0155,
        "grad_norm": 2.6636013984680176,
        "learning_rate": 5.618836933463045e-06,
        "epoch": 0.3000137684152554,
        "step": 2179
    },
    {
        "loss": 2.5918,
        "grad_norm": 1.1787683963775635,
        "learning_rate": 5.575883836956297e-06,
        "epoch": 0.30015145256780945,
        "step": 2180
    },
    {
        "loss": 1.635,
        "grad_norm": 2.3916237354278564,
        "learning_rate": 5.533090839208133e-06,
        "epoch": 0.3002891367203635,
        "step": 2181
    },
    {
        "loss": 2.0178,
        "grad_norm": 2.6222329139709473,
        "learning_rate": 5.490458012775235e-06,
        "epoch": 0.30042682087291755,
        "step": 2182
    },
    {
        "loss": 1.2738,
        "grad_norm": 1.6832523345947266,
        "learning_rate": 5.447985429942803e-06,
        "epoch": 0.30056450502547155,
        "step": 2183
    },
    {
        "loss": 2.0362,
        "grad_norm": 1.3922481536865234,
        "learning_rate": 5.405673162724323e-06,
        "epoch": 0.3007021891780256,
        "step": 2184
    },
    {
        "loss": 2.4986,
        "grad_norm": 1.4808558225631714,
        "learning_rate": 5.363521282861395e-06,
        "epoch": 0.30083987333057965,
        "step": 2185
    },
    {
        "loss": 2.6051,
        "grad_norm": 2.3342220783233643,
        "learning_rate": 5.3215298618237374e-06,
        "epoch": 0.3009775574831337,
        "step": 2186
    },
    {
        "loss": 2.108,
        "grad_norm": 2.6064510345458984,
        "learning_rate": 5.27969897080901e-06,
        "epoch": 0.30111524163568776,
        "step": 2187
    },
    {
        "loss": 2.6018,
        "grad_norm": 0.9379730224609375,
        "learning_rate": 5.238028680742624e-06,
        "epoch": 0.30125292578824175,
        "step": 2188
    },
    {
        "loss": 1.5661,
        "grad_norm": 3.0888936519622803,
        "learning_rate": 5.196519062277783e-06,
        "epoch": 0.3013906099407958,
        "step": 2189
    },
    {
        "loss": 1.7345,
        "grad_norm": 2.3790838718414307,
        "learning_rate": 5.155170185795222e-06,
        "epoch": 0.30152829409334986,
        "step": 2190
    },
    {
        "loss": 2.3164,
        "grad_norm": 1.333528995513916,
        "learning_rate": 5.113982121403082e-06,
        "epoch": 0.3016659782459039,
        "step": 2191
    },
    {
        "loss": 2.5295,
        "grad_norm": 1.3938806056976318,
        "learning_rate": 5.0729549389369135e-06,
        "epoch": 0.30180366239845796,
        "step": 2192
    },
    {
        "loss": 1.7355,
        "grad_norm": 1.5781327486038208,
        "learning_rate": 5.032088707959504e-06,
        "epoch": 0.30194134655101196,
        "step": 2193
    },
    {
        "loss": 2.2424,
        "grad_norm": 1.3628596067428589,
        "learning_rate": 4.991383497760627e-06,
        "epoch": 0.302079030703566,
        "step": 2194
    },
    {
        "loss": 2.5206,
        "grad_norm": 1.0997207164764404,
        "learning_rate": 4.950839377357197e-06,
        "epoch": 0.30221671485612006,
        "step": 2195
    },
    {
        "loss": 2.1182,
        "grad_norm": 1.5181944370269775,
        "learning_rate": 4.910456415492859e-06,
        "epoch": 0.3023543990086741,
        "step": 2196
    },
    {
        "loss": 2.2873,
        "grad_norm": 2.050809860229492,
        "learning_rate": 4.870234680638086e-06,
        "epoch": 0.30249208316122816,
        "step": 2197
    },
    {
        "loss": 1.6079,
        "grad_norm": 1.9682327508926392,
        "learning_rate": 4.830174240989993e-06,
        "epoch": 0.30262976731378216,
        "step": 2198
    },
    {
        "loss": 2.4956,
        "grad_norm": 1.3096610307693481,
        "learning_rate": 4.790275164472135e-06,
        "epoch": 0.3027674514663362,
        "step": 2199
    },
    {
        "loss": 2.4233,
        "grad_norm": 1.3465896844863892,
        "learning_rate": 4.750537518734543e-06,
        "epoch": 0.30290513561889026,
        "step": 2200
    },
    {
        "loss": 1.9503,
        "grad_norm": 2.1000661849975586,
        "learning_rate": 4.710961371153555e-06,
        "epoch": 0.3030428197714443,
        "step": 2201
    },
    {
        "loss": 2.2589,
        "grad_norm": 1.0793077945709229,
        "learning_rate": 4.671546788831604e-06,
        "epoch": 0.30318050392399837,
        "step": 2202
    },
    {
        "loss": 2.1291,
        "grad_norm": 2.8003172874450684,
        "learning_rate": 4.632293838597246e-06,
        "epoch": 0.30331818807655236,
        "step": 2203
    },
    {
        "loss": 1.9789,
        "grad_norm": 2.848644971847534,
        "learning_rate": 4.593202587004974e-06,
        "epoch": 0.3034558722291064,
        "step": 2204
    },
    {
        "loss": 2.3881,
        "grad_norm": 2.6766371726989746,
        "learning_rate": 4.554273100335083e-06,
        "epoch": 0.30359355638166047,
        "step": 2205
    },
    {
        "loss": 2.3571,
        "grad_norm": 1.176745057106018,
        "learning_rate": 4.515505444593637e-06,
        "epoch": 0.3037312405342145,
        "step": 2206
    },
    {
        "loss": 2.2673,
        "grad_norm": 3.1000616550445557,
        "learning_rate": 4.476899685512315e-06,
        "epoch": 0.30386892468676857,
        "step": 2207
    },
    {
        "loss": 2.5366,
        "grad_norm": 1.1704005002975464,
        "learning_rate": 4.438455888548243e-06,
        "epoch": 0.30400660883932257,
        "step": 2208
    },
    {
        "loss": 2.6204,
        "grad_norm": 1.3945237398147583,
        "learning_rate": 4.4001741188839815e-06,
        "epoch": 0.3041442929918766,
        "step": 2209
    },
    {
        "loss": 2.3647,
        "grad_norm": 1.2068309783935547,
        "learning_rate": 4.362054441427355e-06,
        "epoch": 0.30428197714443067,
        "step": 2210
    },
    {
        "loss": 2.593,
        "grad_norm": 2.0395724773406982,
        "learning_rate": 4.324096920811327e-06,
        "epoch": 0.3044196612969847,
        "step": 2211
    },
    {
        "loss": 2.1049,
        "grad_norm": 2.563549518585205,
        "learning_rate": 4.286301621393951e-06,
        "epoch": 0.3045573454495388,
        "step": 2212
    },
    {
        "loss": 2.1413,
        "grad_norm": 3.1181223392486572,
        "learning_rate": 4.248668607258266e-06,
        "epoch": 0.3046950296020928,
        "step": 2213
    },
    {
        "loss": 1.9351,
        "grad_norm": 2.972257375717163,
        "learning_rate": 4.211197942212086e-06,
        "epoch": 0.3048327137546468,
        "step": 2214
    },
    {
        "loss": 2.4692,
        "grad_norm": 1.3604494333267212,
        "learning_rate": 4.173889689787969e-06,
        "epoch": 0.3049703979072009,
        "step": 2215
    },
    {
        "loss": 2.1222,
        "grad_norm": 2.2693090438842773,
        "learning_rate": 4.136743913243146e-06,
        "epoch": 0.3051080820597549,
        "step": 2216
    },
    {
        "loss": 1.6046,
        "grad_norm": 1.1966681480407715,
        "learning_rate": 4.099760675559272e-06,
        "epoch": 0.305245766212309,
        "step": 2217
    },
    {
        "loss": 2.4648,
        "grad_norm": 1.2024320363998413,
        "learning_rate": 4.062940039442542e-06,
        "epoch": 0.30538345036486303,
        "step": 2218
    },
    {
        "loss": 2.2717,
        "grad_norm": 1.9466217756271362,
        "learning_rate": 4.026282067323339e-06,
        "epoch": 0.305521134517417,
        "step": 2219
    },
    {
        "loss": 1.9524,
        "grad_norm": 2.241427421569824,
        "learning_rate": 3.989786821356312e-06,
        "epoch": 0.3056588186699711,
        "step": 2220
    },
    {
        "loss": 2.0154,
        "grad_norm": 1.977745771408081,
        "learning_rate": 3.9534543634201945e-06,
        "epoch": 0.30579650282252513,
        "step": 2221
    },
    {
        "loss": 2.4874,
        "grad_norm": 1.117240071296692,
        "learning_rate": 3.917284755117656e-06,
        "epoch": 0.3059341869750792,
        "step": 2222
    },
    {
        "loss": 2.3082,
        "grad_norm": 2.0784428119659424,
        "learning_rate": 3.881278057775317e-06,
        "epoch": 0.30607187112763323,
        "step": 2223
    },
    {
        "loss": 2.3738,
        "grad_norm": 1.8325698375701904,
        "learning_rate": 3.845434332443532e-06,
        "epoch": 0.30620955528018723,
        "step": 2224
    },
    {
        "loss": 1.62,
        "grad_norm": 2.2024941444396973,
        "learning_rate": 3.8097536398963963e-06,
        "epoch": 0.3063472394327413,
        "step": 2225
    },
    {
        "loss": 2.1424,
        "grad_norm": 1.6452051401138306,
        "learning_rate": 3.7742360406314847e-06,
        "epoch": 0.30648492358529533,
        "step": 2226
    },
    {
        "loss": 1.966,
        "grad_norm": 1.7697899341583252,
        "learning_rate": 3.738881594869903e-06,
        "epoch": 0.3066226077378494,
        "step": 2227
    },
    {
        "loss": 2.0444,
        "grad_norm": 1.0689926147460938,
        "learning_rate": 3.7036903625561337e-06,
        "epoch": 0.30676029189040344,
        "step": 2228
    },
    {
        "loss": 2.676,
        "grad_norm": 1.2061822414398193,
        "learning_rate": 3.66866240335787e-06,
        "epoch": 0.30689797604295743,
        "step": 2229
    },
    {
        "loss": 2.0291,
        "grad_norm": 1.6045418977737427,
        "learning_rate": 3.633797776666037e-06,
        "epoch": 0.3070356601955115,
        "step": 2230
    },
    {
        "loss": 2.1527,
        "grad_norm": 1.3351243734359741,
        "learning_rate": 3.599096541594582e-06,
        "epoch": 0.30717334434806554,
        "step": 2231
    },
    {
        "loss": 2.0334,
        "grad_norm": 1.473037838935852,
        "learning_rate": 3.5645587569804517e-06,
        "epoch": 0.3073110285006196,
        "step": 2232
    },
    {
        "loss": 2.4366,
        "grad_norm": 2.0174808502197266,
        "learning_rate": 3.5301844813834027e-06,
        "epoch": 0.30744871265317364,
        "step": 2233
    },
    {
        "loss": 2.3331,
        "grad_norm": 1.3341479301452637,
        "learning_rate": 3.495973773086025e-06,
        "epoch": 0.30758639680572764,
        "step": 2234
    },
    {
        "loss": 2.0336,
        "grad_norm": 2.997875928878784,
        "learning_rate": 3.4619266900934975e-06,
        "epoch": 0.3077240809582817,
        "step": 2235
    },
    {
        "loss": 2.007,
        "grad_norm": 1.9283925294876099,
        "learning_rate": 3.4280432901336535e-06,
        "epoch": 0.30786176511083574,
        "step": 2236
    },
    {
        "loss": 2.2279,
        "grad_norm": 2.0818498134613037,
        "learning_rate": 3.394323630656726e-06,
        "epoch": 0.3079994492633898,
        "step": 2237
    },
    {
        "loss": 1.546,
        "grad_norm": 1.8839609622955322,
        "learning_rate": 3.360767768835371e-06,
        "epoch": 0.30813713341594384,
        "step": 2238
    },
    {
        "loss": 2.1222,
        "grad_norm": 1.8774148225784302,
        "learning_rate": 3.3273757615645108e-06,
        "epoch": 0.30827481756849784,
        "step": 2239
    },
    {
        "loss": 2.2511,
        "grad_norm": 1.8386231660842896,
        "learning_rate": 3.2941476654612115e-06,
        "epoch": 0.3084125017210519,
        "step": 2240
    },
    {
        "loss": 2.1655,
        "grad_norm": 2.0151844024658203,
        "learning_rate": 3.261083536864651e-06,
        "epoch": 0.30855018587360594,
        "step": 2241
    },
    {
        "loss": 2.0497,
        "grad_norm": 1.3883558511734009,
        "learning_rate": 3.228183431836018e-06,
        "epoch": 0.30868787002616,
        "step": 2242
    },
    {
        "loss": 2.5185,
        "grad_norm": 1.7434110641479492,
        "learning_rate": 3.1954474061583475e-06,
        "epoch": 0.30882555417871405,
        "step": 2243
    },
    {
        "loss": 2.388,
        "grad_norm": 1.539175033569336,
        "learning_rate": 3.1628755153365055e-06,
        "epoch": 0.3089632383312681,
        "step": 2244
    },
    {
        "loss": 2.297,
        "grad_norm": 1.4617094993591309,
        "learning_rate": 3.1304678145970602e-06,
        "epoch": 0.3091009224838221,
        "step": 2245
    },
    {
        "loss": 1.8569,
        "grad_norm": 2.0016684532165527,
        "learning_rate": 3.09822435888818e-06,
        "epoch": 0.30923860663637615,
        "step": 2246
    },
    {
        "loss": 1.7598,
        "grad_norm": 2.1086032390594482,
        "learning_rate": 3.0661452028795336e-06,
        "epoch": 0.3093762907889302,
        "step": 2247
    },
    {
        "loss": 2.0139,
        "grad_norm": 1.4777071475982666,
        "learning_rate": 3.0342304009622792e-06,
        "epoch": 0.30951397494148425,
        "step": 2248
    },
    {
        "loss": 2.4501,
        "grad_norm": 1.1478902101516724,
        "learning_rate": 3.0024800072488202e-06,
        "epoch": 0.3096516590940383,
        "step": 2249
    },
    {
        "loss": 2.4404,
        "grad_norm": 1.0661431550979614,
        "learning_rate": 2.970894075572883e-06,
        "epoch": 0.3097893432465923,
        "step": 2250
    },
    {
        "loss": 1.9194,
        "grad_norm": 1.1764429807662964,
        "learning_rate": 2.9394726594893064e-06,
        "epoch": 0.30992702739914635,
        "step": 2251
    },
    {
        "loss": 2.1432,
        "grad_norm": 1.219530463218689,
        "learning_rate": 2.9082158122739624e-06,
        "epoch": 0.3100647115517004,
        "step": 2252
    },
    {
        "loss": 2.3407,
        "grad_norm": 2.3309972286224365,
        "learning_rate": 2.8771235869237358e-06,
        "epoch": 0.31020239570425445,
        "step": 2253
    },
    {
        "loss": 2.559,
        "grad_norm": 1.2398693561553955,
        "learning_rate": 2.8461960361563788e-06,
        "epoch": 0.3103400798568085,
        "step": 2254
    },
    {
        "loss": 1.9917,
        "grad_norm": 2.7450857162475586,
        "learning_rate": 2.8154332124104234e-06,
        "epoch": 0.3104777640093625,
        "step": 2255
    },
    {
        "loss": 2.059,
        "grad_norm": 1.4401119947433472,
        "learning_rate": 2.784835167845101e-06,
        "epoch": 0.31061544816191655,
        "step": 2256
    },
    {
        "loss": 1.867,
        "grad_norm": 3.067091226577759,
        "learning_rate": 2.7544019543402687e-06,
        "epoch": 0.3107531323144706,
        "step": 2257
    },
    {
        "loss": 1.6439,
        "grad_norm": 2.163254499435425,
        "learning_rate": 2.7241336234962944e-06,
        "epoch": 0.31089081646702466,
        "step": 2258
    },
    {
        "loss": 2.5441,
        "grad_norm": 1.6765629053115845,
        "learning_rate": 2.694030226633981e-06,
        "epoch": 0.3110285006195787,
        "step": 2259
    },
    {
        "loss": 2.5796,
        "grad_norm": 1.4017430543899536,
        "learning_rate": 2.6640918147945226e-06,
        "epoch": 0.3111661847721327,
        "step": 2260
    },
    {
        "loss": 2.4142,
        "grad_norm": 1.0700321197509766,
        "learning_rate": 2.6343184387392917e-06,
        "epoch": 0.31130386892468676,
        "step": 2261
    },
    {
        "loss": 2.2123,
        "grad_norm": 1.9540812969207764,
        "learning_rate": 2.6047101489499405e-06,
        "epoch": 0.3114415530772408,
        "step": 2262
    },
    {
        "loss": 2.2579,
        "grad_norm": 1.7344030141830444,
        "learning_rate": 2.5752669956281338e-06,
        "epoch": 0.31157923722979486,
        "step": 2263
    },
    {
        "loss": 2.3987,
        "grad_norm": 1.108992338180542,
        "learning_rate": 2.5459890286955723e-06,
        "epoch": 0.3117169213823489,
        "step": 2264
    },
    {
        "loss": 2.081,
        "grad_norm": 1.4308840036392212,
        "learning_rate": 2.51687629779388e-06,
        "epoch": 0.3118546055349029,
        "step": 2265
    },
    {
        "loss": 2.2853,
        "grad_norm": 1.3838579654693604,
        "learning_rate": 2.487928852284527e-06,
        "epoch": 0.31199228968745696,
        "step": 2266
    },
    {
        "loss": 1.9816,
        "grad_norm": 2.0254409313201904,
        "learning_rate": 2.459146741248697e-06,
        "epoch": 0.312129973840011,
        "step": 2267
    },
    {
        "loss": 2.2636,
        "grad_norm": 1.8237500190734863,
        "learning_rate": 2.430530013487298e-06,
        "epoch": 0.31226765799256506,
        "step": 2268
    },
    {
        "loss": 2.2756,
        "grad_norm": 1.8249839544296265,
        "learning_rate": 2.402078717520795e-06,
        "epoch": 0.3124053421451191,
        "step": 2269
    },
    {
        "loss": 2.2522,
        "grad_norm": 2.019780158996582,
        "learning_rate": 2.373792901589156e-06,
        "epoch": 0.31254302629767317,
        "step": 2270
    },
    {
        "loss": 2.4226,
        "grad_norm": 1.3626747131347656,
        "learning_rate": 2.3456726136517837e-06,
        "epoch": 0.31268071045022716,
        "step": 2271
    },
    {
        "loss": 2.0157,
        "grad_norm": 1.5030981302261353,
        "learning_rate": 2.3177179013874395e-06,
        "epoch": 0.3128183946027812,
        "step": 2272
    },
    {
        "loss": 2.3386,
        "grad_norm": 1.4004075527191162,
        "learning_rate": 2.2899288121940864e-06,
        "epoch": 0.31295607875533527,
        "step": 2273
    },
    {
        "loss": 1.6755,
        "grad_norm": 2.4650161266326904,
        "learning_rate": 2.2623053931889683e-06,
        "epoch": 0.3130937629078893,
        "step": 2274
    },
    {
        "loss": 2.5221,
        "grad_norm": 1.2642326354980469,
        "learning_rate": 2.2348476912083414e-06,
        "epoch": 0.31323144706044337,
        "step": 2275
    },
    {
        "loss": 1.8494,
        "grad_norm": 1.1544957160949707,
        "learning_rate": 2.2075557528074996e-06,
        "epoch": 0.31336913121299736,
        "step": 2276
    },
    {
        "loss": 2.4024,
        "grad_norm": 1.4076579809188843,
        "learning_rate": 2.180429624260738e-06,
        "epoch": 0.3135068153655514,
        "step": 2277
    },
    {
        "loss": 2.08,
        "grad_norm": 1.2552629709243774,
        "learning_rate": 2.1534693515611328e-06,
        "epoch": 0.31364449951810547,
        "step": 2278
    },
    {
        "loss": 2.0833,
        "grad_norm": 1.656928539276123,
        "learning_rate": 2.1266749804206067e-06,
        "epoch": 0.3137821836706595,
        "step": 2279
    },
    {
        "loss": 2.2555,
        "grad_norm": 1.7073792219161987,
        "learning_rate": 2.1000465562697856e-06,
        "epoch": 0.31391986782321357,
        "step": 2280
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.375013828277588,
        "learning_rate": 2.073584124257899e-06,
        "epoch": 0.31405755197576757,
        "step": 2281
    },
    {
        "loss": 2.4511,
        "grad_norm": 1.8589863777160645,
        "learning_rate": 2.0472877292527446e-06,
        "epoch": 0.3141952361283216,
        "step": 2282
    },
    {
        "loss": 2.0851,
        "grad_norm": 0.9336152076721191,
        "learning_rate": 2.0211574158406243e-06,
        "epoch": 0.31433292028087567,
        "step": 2283
    },
    {
        "loss": 1.1886,
        "grad_norm": 1.6804118156433105,
        "learning_rate": 1.9951932283261974e-06,
        "epoch": 0.3144706044334297,
        "step": 2284
    },
    {
        "loss": 2.0361,
        "grad_norm": 1.9309853315353394,
        "learning_rate": 1.969395210732483e-06,
        "epoch": 0.3146082885859838,
        "step": 2285
    },
    {
        "loss": 2.3899,
        "grad_norm": 0.9598128795623779,
        "learning_rate": 1.9437634068007802e-06,
        "epoch": 0.31474597273853777,
        "step": 2286
    },
    {
        "loss": 2.0554,
        "grad_norm": 0.9217087030410767,
        "learning_rate": 1.9182978599905033e-06,
        "epoch": 0.3148836568910918,
        "step": 2287
    },
    {
        "loss": 2.1161,
        "grad_norm": 2.2913806438446045,
        "learning_rate": 1.8929986134792244e-06,
        "epoch": 0.3150213410436459,
        "step": 2288
    },
    {
        "loss": 2.3256,
        "grad_norm": 1.6729679107666016,
        "learning_rate": 1.8678657101625307e-06,
        "epoch": 0.3151590251961999,
        "step": 2289
    },
    {
        "loss": 2.2262,
        "grad_norm": 1.7045423984527588,
        "learning_rate": 1.8428991926539573e-06,
        "epoch": 0.315296709348754,
        "step": 2290
    },
    {
        "loss": 1.685,
        "grad_norm": 2.070695161819458,
        "learning_rate": 1.8180991032849315e-06,
        "epoch": 0.315434393501308,
        "step": 2291
    },
    {
        "loss": 1.3636,
        "grad_norm": 2.286940097808838,
        "learning_rate": 1.7934654841047505e-06,
        "epoch": 0.315572077653862,
        "step": 2292
    },
    {
        "loss": 1.8531,
        "grad_norm": 1.272894024848938,
        "learning_rate": 1.7689983768803709e-06,
        "epoch": 0.3157097618064161,
        "step": 2293
    },
    {
        "loss": 2.0596,
        "grad_norm": 1.7837324142456055,
        "learning_rate": 1.744697823096475e-06,
        "epoch": 0.31584744595897013,
        "step": 2294
    },
    {
        "loss": 2.2665,
        "grad_norm": 1.0501594543457031,
        "learning_rate": 1.720563863955349e-06,
        "epoch": 0.3159851301115242,
        "step": 2295
    },
    {
        "loss": 2.4993,
        "grad_norm": 1.258307695388794,
        "learning_rate": 1.6965965403767824e-06,
        "epoch": 0.3161228142640782,
        "step": 2296
    },
    {
        "loss": 2.0498,
        "grad_norm": 1.3887288570404053,
        "learning_rate": 1.6727958929980691e-06,
        "epoch": 0.31626049841663223,
        "step": 2297
    },
    {
        "loss": 2.3444,
        "grad_norm": 1.6096950769424438,
        "learning_rate": 1.6491619621738841e-06,
        "epoch": 0.3163981825691863,
        "step": 2298
    },
    {
        "loss": 2.2188,
        "grad_norm": 1.2429965734481812,
        "learning_rate": 1.6256947879762063e-06,
        "epoch": 0.31653586672174033,
        "step": 2299
    },
    {
        "loss": 1.3522,
        "grad_norm": 2.4988856315612793,
        "learning_rate": 1.6023944101942966e-06,
        "epoch": 0.3166735508742944,
        "step": 2300
    },
    {
        "loss": 2.2252,
        "grad_norm": 2.383561849594116,
        "learning_rate": 1.57926086833462e-06,
        "epoch": 0.31681123502684844,
        "step": 2301
    },
    {
        "loss": 1.6105,
        "grad_norm": 1.9145013093948364,
        "learning_rate": 1.5562942016207338e-06,
        "epoch": 0.31694891917940243,
        "step": 2302
    },
    {
        "loss": 2.1062,
        "grad_norm": 1.953351378440857,
        "learning_rate": 1.5334944489932778e-06,
        "epoch": 0.3170866033319565,
        "step": 2303
    },
    {
        "loss": 2.0546,
        "grad_norm": 2.3611743450164795,
        "learning_rate": 1.5108616491098736e-06,
        "epoch": 0.31722428748451054,
        "step": 2304
    },
    {
        "loss": 2.566,
        "grad_norm": 1.2371927499771118,
        "learning_rate": 1.4883958403450803e-06,
        "epoch": 0.3173619716370646,
        "step": 2305
    },
    {
        "loss": 2.0156,
        "grad_norm": 1.777692198753357,
        "learning_rate": 1.4660970607903058e-06,
        "epoch": 0.31749965578961864,
        "step": 2306
    },
    {
        "loss": 1.6092,
        "grad_norm": 2.2475335597991943,
        "learning_rate": 1.443965348253773e-06,
        "epoch": 0.31763733994217264,
        "step": 2307
    },
    {
        "loss": 1.8249,
        "grad_norm": 2.1591455936431885,
        "learning_rate": 1.4220007402603985e-06,
        "epoch": 0.3177750240947267,
        "step": 2308
    },
    {
        "loss": 1.4304,
        "grad_norm": 2.5820841789245605,
        "learning_rate": 1.4002032740518145e-06,
        "epoch": 0.31791270824728074,
        "step": 2309
    },
    {
        "loss": 2.4151,
        "grad_norm": 2.009779930114746,
        "learning_rate": 1.3785729865862574e-06,
        "epoch": 0.3180503923998348,
        "step": 2310
    },
    {
        "loss": 1.8678,
        "grad_norm": 1.3202142715454102,
        "learning_rate": 1.357109914538468e-06,
        "epoch": 0.31818807655238884,
        "step": 2311
    },
    {
        "loss": 2.4115,
        "grad_norm": 1.0788640975952148,
        "learning_rate": 1.3358140942996922e-06,
        "epoch": 0.31832576070494284,
        "step": 2312
    },
    {
        "loss": 1.958,
        "grad_norm": 1.853374719619751,
        "learning_rate": 1.3146855619776134e-06,
        "epoch": 0.3184634448574969,
        "step": 2313
    },
    {
        "loss": 2.34,
        "grad_norm": 1.1751039028167725,
        "learning_rate": 1.2937243533962307e-06,
        "epoch": 0.31860112901005094,
        "step": 2314
    },
    {
        "loss": 1.805,
        "grad_norm": 3.0224807262420654,
        "learning_rate": 1.2729305040958706e-06,
        "epoch": 0.318738813162605,
        "step": 2315
    },
    {
        "loss": 1.8869,
        "grad_norm": 1.1660988330841064,
        "learning_rate": 1.2523040493331195e-06,
        "epoch": 0.31887649731515905,
        "step": 2316
    },
    {
        "loss": 2.1758,
        "grad_norm": 1.374880075454712,
        "learning_rate": 1.2318450240806912e-06,
        "epoch": 0.31901418146771304,
        "step": 2317
    },
    {
        "loss": 2.4616,
        "grad_norm": 1.4276870489120483,
        "learning_rate": 1.2115534630274371e-06,
        "epoch": 0.3191518656202671,
        "step": 2318
    },
    {
        "loss": 2.1081,
        "grad_norm": 2.4120969772338867,
        "learning_rate": 1.1914294005783033e-06,
        "epoch": 0.31928954977282115,
        "step": 2319
    },
    {
        "loss": 2.4361,
        "grad_norm": 1.035398006439209,
        "learning_rate": 1.1714728708541512e-06,
        "epoch": 0.3194272339253752,
        "step": 2320
    },
    {
        "loss": 2.2002,
        "grad_norm": 2.466310977935791,
        "learning_rate": 1.1516839076919028e-06,
        "epoch": 0.31956491807792925,
        "step": 2321
    },
    {
        "loss": 2.2147,
        "grad_norm": 2.5157828330993652,
        "learning_rate": 1.1320625446442634e-06,
        "epoch": 0.31970260223048325,
        "step": 2322
    },
    {
        "loss": 2.5374,
        "grad_norm": 1.2289730310440063,
        "learning_rate": 1.1126088149798208e-06,
        "epoch": 0.3198402863830373,
        "step": 2323
    },
    {
        "loss": 2.2225,
        "grad_norm": 2.6816179752349854,
        "learning_rate": 1.0933227516829347e-06,
        "epoch": 0.31997797053559135,
        "step": 2324
    },
    {
        "loss": 2.2204,
        "grad_norm": 1.96968674659729,
        "learning_rate": 1.0742043874536477e-06,
        "epoch": 0.3201156546881454,
        "step": 2325
    },
    {
        "loss": 2.0498,
        "grad_norm": 2.0824129581451416,
        "learning_rate": 1.055253754707708e-06,
        "epoch": 0.32025333884069945,
        "step": 2326
    },
    {
        "loss": 1.8878,
        "grad_norm": 1.7340087890625,
        "learning_rate": 1.036470885576446e-06,
        "epoch": 0.32039102299325345,
        "step": 2327
    },
    {
        "loss": 1.5939,
        "grad_norm": 2.1607911586761475,
        "learning_rate": 1.0178558119067315e-06,
        "epoch": 0.3205287071458075,
        "step": 2328
    },
    {
        "loss": 1.9221,
        "grad_norm": 2.2696516513824463,
        "learning_rate": 9.99408565260962e-07,
        "epoch": 0.32066639129836155,
        "step": 2329
    },
    {
        "loss": 2.3213,
        "grad_norm": 1.3245060443878174,
        "learning_rate": 9.811291769169616e-07,
        "epoch": 0.3208040754509156,
        "step": 2330
    },
    {
        "loss": 2.4966,
        "grad_norm": 1.140523076057434,
        "learning_rate": 9.630176778679612e-07,
        "epoch": 0.32094175960346966,
        "step": 2331
    },
    {
        "loss": 2.3769,
        "grad_norm": 1.144445538520813,
        "learning_rate": 9.45074098822496e-07,
        "epoch": 0.3210794437560237,
        "step": 2332
    },
    {
        "loss": 2.461,
        "grad_norm": 2.3358092308044434,
        "learning_rate": 9.272984702044297e-07,
        "epoch": 0.3212171279085777,
        "step": 2333
    },
    {
        "loss": 1.4519,
        "grad_norm": 2.252570390701294,
        "learning_rate": 9.096908221528311e-07,
        "epoch": 0.32135481206113176,
        "step": 2334
    },
    {
        "loss": 2.0146,
        "grad_norm": 2.9602763652801514,
        "learning_rate": 8.922511845219971e-07,
        "epoch": 0.3214924962136858,
        "step": 2335
    },
    {
        "loss": 1.9127,
        "grad_norm": 2.3940348625183105,
        "learning_rate": 8.749795868812861e-07,
        "epoch": 0.32163018036623986,
        "step": 2336
    },
    {
        "loss": 2.363,
        "grad_norm": 1.3199623823165894,
        "learning_rate": 8.578760585151946e-07,
        "epoch": 0.3217678645187939,
        "step": 2337
    },
    {
        "loss": 2.3967,
        "grad_norm": 1.4670010805130005,
        "learning_rate": 8.409406284232368e-07,
        "epoch": 0.3219055486713479,
        "step": 2338
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.0698845386505127,
        "learning_rate": 8.241733253199324e-07,
        "epoch": 0.32204323282390196,
        "step": 2339
    },
    {
        "loss": 1.8376,
        "grad_norm": 1.6599698066711426,
        "learning_rate": 8.075741776346846e-07,
        "epoch": 0.322180916976456,
        "step": 2340
    },
    {
        "loss": 1.4774,
        "grad_norm": 2.320390224456787,
        "learning_rate": 7.911432135118357e-07,
        "epoch": 0.32231860112901006,
        "step": 2341
    },
    {
        "loss": 2.2473,
        "grad_norm": 1.612318754196167,
        "learning_rate": 7.748804608105675e-07,
        "epoch": 0.3224562852815641,
        "step": 2342
    },
    {
        "loss": 1.6869,
        "grad_norm": 2.450376033782959,
        "learning_rate": 7.587859471048008e-07,
        "epoch": 0.3225939694341181,
        "step": 2343
    },
    {
        "loss": 2.173,
        "grad_norm": 1.4152156114578247,
        "learning_rate": 7.428596996832627e-07,
        "epoch": 0.32273165358667216,
        "step": 2344
    },
    {
        "loss": 2.2996,
        "grad_norm": 1.0893701314926147,
        "learning_rate": 7.271017455493523e-07,
        "epoch": 0.3228693377392262,
        "step": 2345
    },
    {
        "loss": 2.2035,
        "grad_norm": 3.196882486343384,
        "learning_rate": 7.115121114211199e-07,
        "epoch": 0.32300702189178027,
        "step": 2346
    },
    {
        "loss": 2.3552,
        "grad_norm": 2.1825199127197266,
        "learning_rate": 6.960908237312325e-07,
        "epoch": 0.3231447060443343,
        "step": 2347
    },
    {
        "loss": 2.0191,
        "grad_norm": 2.4806971549987793,
        "learning_rate": 6.8083790862693e-07,
        "epoch": 0.3232823901968883,
        "step": 2348
    },
    {
        "loss": 2.3728,
        "grad_norm": 1.0971646308898926,
        "learning_rate": 6.657533919699477e-07,
        "epoch": 0.32342007434944237,
        "step": 2349
    },
    {
        "loss": 2.0272,
        "grad_norm": 2.1820473670959473,
        "learning_rate": 6.508372993365153e-07,
        "epoch": 0.3235577585019964,
        "step": 2350
    },
    {
        "loss": 2.2147,
        "grad_norm": 0.9947776198387146,
        "learning_rate": 6.360896560172913e-07,
        "epoch": 0.32369544265455047,
        "step": 2351
    },
    {
        "loss": 1.7181,
        "grad_norm": 2.9859814643859863,
        "learning_rate": 6.215104870173183e-07,
        "epoch": 0.3238331268071045,
        "step": 2352
    },
    {
        "loss": 1.5083,
        "grad_norm": 1.357235312461853,
        "learning_rate": 6.070998170559894e-07,
        "epoch": 0.3239708109596585,
        "step": 2353
    },
    {
        "loss": 2.1571,
        "grad_norm": 2.414954423904419,
        "learning_rate": 5.928576705670152e-07,
        "epoch": 0.32410849511221257,
        "step": 2354
    },
    {
        "loss": 2.0867,
        "grad_norm": 2.100036144256592,
        "learning_rate": 5.78784071698335e-07,
        "epoch": 0.3242461792647666,
        "step": 2355
    },
    {
        "loss": 2.0491,
        "grad_norm": 1.6123976707458496,
        "learning_rate": 5.648790443121498e-07,
        "epoch": 0.3243838634173207,
        "step": 2356
    },
    {
        "loss": 2.3028,
        "grad_norm": 1.8556030988693237,
        "learning_rate": 5.51142611984834e-07,
        "epoch": 0.3245215475698747,
        "step": 2357
    },
    {
        "loss": 1.5297,
        "grad_norm": 2.4538538455963135,
        "learning_rate": 5.37574798006868e-07,
        "epoch": 0.3246592317224288,
        "step": 2358
    },
    {
        "loss": 2.2707,
        "grad_norm": 1.818278431892395,
        "learning_rate": 5.241756253828945e-07,
        "epoch": 0.3247969158749828,
        "step": 2359
    },
    {
        "loss": 2.5177,
        "grad_norm": 2.156832456588745,
        "learning_rate": 5.109451168315738e-07,
        "epoch": 0.3249346000275368,
        "step": 2360
    },
    {
        "loss": 2.0306,
        "grad_norm": 1.6635960340499878,
        "learning_rate": 4.978832947856171e-07,
        "epoch": 0.3250722841800909,
        "step": 2361
    },
    {
        "loss": 2.3815,
        "grad_norm": 1.4333350658416748,
        "learning_rate": 4.84990181391698e-07,
        "epoch": 0.3252099683326449,
        "step": 2362
    },
    {
        "loss": 2.5216,
        "grad_norm": 1.8277019262313843,
        "learning_rate": 4.722657985104628e-07,
        "epoch": 0.325347652485199,
        "step": 2363
    },
    {
        "loss": 2.2128,
        "grad_norm": 1.097947359085083,
        "learning_rate": 4.5971016771645393e-07,
        "epoch": 0.325485336637753,
        "step": 2364
    },
    {
        "loss": 1.6774,
        "grad_norm": 2.381920099258423,
        "learning_rate": 4.4732331029812e-07,
        "epoch": 0.325623020790307,
        "step": 2365
    },
    {
        "loss": 1.7544,
        "grad_norm": 1.2955745458602905,
        "learning_rate": 4.351052472577166e-07,
        "epoch": 0.3257607049428611,
        "step": 2366
    },
    {
        "loss": 2.3319,
        "grad_norm": 1.1191256046295166,
        "learning_rate": 4.230559993113059e-07,
        "epoch": 0.32589838909541513,
        "step": 2367
    },
    {
        "loss": 2.2464,
        "grad_norm": 1.0870075225830078,
        "learning_rate": 4.111755868887346e-07,
        "epoch": 0.3260360732479692,
        "step": 2368
    },
    {
        "loss": 1.7744,
        "grad_norm": 1.4306204319000244,
        "learning_rate": 3.9946403013358944e-07,
        "epoch": 0.3261737574005232,
        "step": 2369
    },
    {
        "loss": 1.7191,
        "grad_norm": 1.354038953781128,
        "learning_rate": 3.879213489031197e-07,
        "epoch": 0.32631144155307723,
        "step": 2370
    },
    {
        "loss": 2.2759,
        "grad_norm": 2.1558821201324463,
        "learning_rate": 3.765475627683035e-07,
        "epoch": 0.3264491257056313,
        "step": 2371
    },
    {
        "loss": 1.84,
        "grad_norm": 3.0811550617218018,
        "learning_rate": 3.6534269101368147e-07,
        "epoch": 0.32658680985818533,
        "step": 2372
    },
    {
        "loss": 1.8021,
        "grad_norm": 2.348719596862793,
        "learning_rate": 3.543067526374566e-07,
        "epoch": 0.3267244940107394,
        "step": 2373
    },
    {
        "loss": 1.6849,
        "grad_norm": 2.2663981914520264,
        "learning_rate": 3.434397663513611e-07,
        "epoch": 0.3268621781632934,
        "step": 2374
    },
    {
        "loss": 2.2464,
        "grad_norm": 1.9808250665664673,
        "learning_rate": 3.3274175058067846e-07,
        "epoch": 0.32699986231584743,
        "step": 2375
    },
    {
        "loss": 2.4415,
        "grad_norm": 1.2838027477264404,
        "learning_rate": 3.222127234641881e-07,
        "epoch": 0.3271375464684015,
        "step": 2376
    },
    {
        "loss": 2.322,
        "grad_norm": 0.9877794981002808,
        "learning_rate": 3.1185270285418734e-07,
        "epoch": 0.32727523062095554,
        "step": 2377
    },
    {
        "loss": 1.2984,
        "grad_norm": 2.504619598388672,
        "learning_rate": 3.016617063163474e-07,
        "epoch": 0.3274129147735096,
        "step": 2378
    },
    {
        "loss": 1.8013,
        "grad_norm": 1.9573757648468018,
        "learning_rate": 2.916397511298019e-07,
        "epoch": 0.3275505989260636,
        "step": 2379
    },
    {
        "loss": 2.1004,
        "grad_norm": 0.9555368423461914,
        "learning_rate": 2.817868542870694e-07,
        "epoch": 0.32768828307861764,
        "step": 2380
    },
    {
        "loss": 2.3964,
        "grad_norm": 1.4821120500564575,
        "learning_rate": 2.721030324939977e-07,
        "epoch": 0.3278259672311717,
        "step": 2381
    },
    {
        "loss": 2.1196,
        "grad_norm": 1.8217512369155884,
        "learning_rate": 2.6258830216979724e-07,
        "epoch": 0.32796365138372574,
        "step": 2382
    },
    {
        "loss": 2.1689,
        "grad_norm": 1.261738896369934,
        "learning_rate": 2.532426794469522e-07,
        "epoch": 0.3281013355362798,
        "step": 2383
    },
    {
        "loss": 2.5049,
        "grad_norm": 1.6997617483139038,
        "learning_rate": 2.4406618017123184e-07,
        "epoch": 0.3282390196888338,
        "step": 2384
    },
    {
        "loss": 2.5167,
        "grad_norm": 1.4105442762374878,
        "learning_rate": 2.350588199016568e-07,
        "epoch": 0.32837670384138784,
        "step": 2385
    },
    {
        "loss": 2.1978,
        "grad_norm": 1.7294234037399292,
        "learning_rate": 2.2622061391044392e-07,
        "epoch": 0.3285143879939419,
        "step": 2386
    },
    {
        "loss": 2.2278,
        "grad_norm": 2.3944835662841797,
        "learning_rate": 2.1755157718303941e-07,
        "epoch": 0.32865207214649594,
        "step": 2387
    },
    {
        "loss": 1.7747,
        "grad_norm": 2.754329204559326,
        "learning_rate": 2.0905172441803012e-07,
        "epoch": 0.32878975629905,
        "step": 2388
    },
    {
        "loss": 2.5447,
        "grad_norm": 1.9719730615615845,
        "learning_rate": 2.0072107002716557e-07,
        "epoch": 0.32892744045160405,
        "step": 2389
    },
    {
        "loss": 2.0683,
        "grad_norm": 1.654869794845581,
        "learning_rate": 1.925596281353026e-07,
        "epoch": 0.32906512460415804,
        "step": 2390
    },
    {
        "loss": 1.5027,
        "grad_norm": 2.389500617980957,
        "learning_rate": 1.8456741258039423e-07,
        "epoch": 0.3292028087567121,
        "step": 2391
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.5384036302566528,
        "learning_rate": 1.767444369134674e-07,
        "epoch": 0.32934049290926615,
        "step": 2392
    },
    {
        "loss": 2.4734,
        "grad_norm": 0.8678306937217712,
        "learning_rate": 1.690907143985898e-07,
        "epoch": 0.3294781770618202,
        "step": 2393
    },
    {
        "loss": 1.6015,
        "grad_norm": 2.1017630100250244,
        "learning_rate": 1.6160625801288076e-07,
        "epoch": 0.32961586121437425,
        "step": 2394
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.0722445249557495,
        "learning_rate": 1.542910804464448e-07,
        "epoch": 0.32975354536692825,
        "step": 2395
    },
    {
        "loss": 1.7224,
        "grad_norm": 1.8594748973846436,
        "learning_rate": 1.471451941023605e-07,
        "epoch": 0.3298912295194823,
        "step": 2396
    },
    {
        "loss": 2.0547,
        "grad_norm": 2.4017655849456787,
        "learning_rate": 1.4016861109669155e-07,
        "epoch": 0.33002891367203635,
        "step": 2397
    },
    {
        "loss": 2.393,
        "grad_norm": 1.3702143430709839,
        "learning_rate": 1.333613432584313e-07,
        "epoch": 0.3301665978245904,
        "step": 2398
    },
    {
        "loss": 2.0871,
        "grad_norm": 1.735231876373291,
        "learning_rate": 1.2672340212948052e-07,
        "epoch": 0.33030428197714445,
        "step": 2399
    },
    {
        "loss": 2.3347,
        "grad_norm": 1.562269926071167,
        "learning_rate": 1.2025479896468074e-07,
        "epoch": 0.33044196612969845,
        "step": 2400
    },
    {
        "loss": 2.3167,
        "grad_norm": 1.8027725219726562,
        "learning_rate": 1.1395554473171422e-07,
        "epoch": 0.3305796502822525,
        "step": 2401
    },
    {
        "loss": 2.0078,
        "grad_norm": 1.3881416320800781,
        "learning_rate": 1.0782565011114854e-07,
        "epoch": 0.33071733443480655,
        "step": 2402
    },
    {
        "loss": 2.6304,
        "grad_norm": 1.931684970855713,
        "learning_rate": 1.0186512549639205e-07,
        "epoch": 0.3308550185873606,
        "step": 2403
    },
    {
        "loss": 2.2619,
        "grad_norm": 2.1279351711273193,
        "learning_rate": 9.607398099368282e-08,
        "epoch": 0.33099270273991466,
        "step": 2404
    },
    {
        "loss": 1.5608,
        "grad_norm": 1.899956464767456,
        "learning_rate": 9.045222642206641e-08,
        "epoch": 0.33113038689246865,
        "step": 2405
    },
    {
        "loss": 2.4054,
        "grad_norm": 1.6224607229232788,
        "learning_rate": 8.499987131339593e-08,
        "epoch": 0.3312680710450227,
        "step": 2406
    },
    {
        "loss": 1.6627,
        "grad_norm": 3.1962015628814697,
        "learning_rate": 7.971692491228755e-08,
        "epoch": 0.33140575519757676,
        "step": 2407
    },
    {
        "loss": 1.9426,
        "grad_norm": 1.0227192640304565,
        "learning_rate": 7.460339617613165e-08,
        "epoch": 0.3315434393501308,
        "step": 2408
    },
    {
        "loss": 1.8179,
        "grad_norm": 1.1988248825073242,
        "learning_rate": 6.965929377504843e-08,
        "epoch": 0.33168112350268486,
        "step": 2409
    },
    {
        "loss": 1.8377,
        "grad_norm": 2.0442821979522705,
        "learning_rate": 6.488462609192114e-08,
        "epoch": 0.33181880765523886,
        "step": 2410
    },
    {
        "loss": 1.8904,
        "grad_norm": 2.0320794582366943,
        "learning_rate": 6.027940122231845e-08,
        "epoch": 0.3319564918077929,
        "step": 2411
    },
    {
        "loss": 2.3386,
        "grad_norm": 1.4581921100616455,
        "learning_rate": 5.584362697453882e-08,
        "epoch": 0.33209417596034696,
        "step": 2412
    },
    {
        "loss": 1.9108,
        "grad_norm": 1.641991138458252,
        "learning_rate": 5.1577310869554976e-08,
        "epoch": 0.332231860112901,
        "step": 2413
    },
    {
        "loss": 1.8964,
        "grad_norm": 2.711519479751587,
        "learning_rate": 4.7480460141036134e-08,
        "epoch": 0.33236954426545506,
        "step": 2414
    },
    {
        "loss": 2.3675,
        "grad_norm": 1.6739416122436523,
        "learning_rate": 4.355308173530359e-08,
        "epoch": 0.33250722841800906,
        "step": 2415
    },
    {
        "loss": 2.1448,
        "grad_norm": 1.156875729560852,
        "learning_rate": 3.9795182311330724e-08,
        "epoch": 0.3326449125705631,
        "step": 2416
    },
    {
        "loss": 2.0571,
        "grad_norm": 2.9164462089538574,
        "learning_rate": 3.620676824075409e-08,
        "epoch": 0.33278259672311716,
        "step": 2417
    },
    {
        "loss": 1.6111,
        "grad_norm": 1.929187536239624,
        "learning_rate": 3.278784560782899e-08,
        "epoch": 0.3329202808756712,
        "step": 2418
    },
    {
        "loss": 1.8665,
        "grad_norm": 2.576862335205078,
        "learning_rate": 2.953842020942954e-08,
        "epoch": 0.33305796502822527,
        "step": 2419
    },
    {
        "loss": 2.4944,
        "grad_norm": 1.6810799837112427,
        "learning_rate": 2.6458497555059692e-08,
        "epoch": 0.3331956491807793,
        "step": 2420
    },
    {
        "loss": 2.3621,
        "grad_norm": 2.148801565170288,
        "learning_rate": 2.3548082866797773e-08,
        "epoch": 0.3333333333333333,
        "step": 2421
    },
    {
        "loss": 1.7195,
        "grad_norm": 1.4307317733764648,
        "learning_rate": 2.080718107935198e-08,
        "epoch": 0.33347101748588737,
        "step": 2422
    },
    {
        "loss": 1.7372,
        "grad_norm": 1.9220619201660156,
        "learning_rate": 1.8235796839982665e-08,
        "epoch": 0.3336087016384414,
        "step": 2423
    },
    {
        "loss": 1.933,
        "grad_norm": 1.8713828325271606,
        "learning_rate": 1.5833934508557857e-08,
        "epoch": 0.33374638579099547,
        "step": 2424
    },
    {
        "loss": 2.2954,
        "grad_norm": 1.4363160133361816,
        "learning_rate": 1.360159815749773e-08,
        "epoch": 0.3338840699435495,
        "step": 2425
    },
    {
        "loss": 2.1544,
        "grad_norm": 1.0649604797363281,
        "learning_rate": 1.1538791571796825e-08,
        "epoch": 0.3340217540961035,
        "step": 2426
    },
    {
        "loss": 2.3381,
        "grad_norm": 1.5574835538864136,
        "learning_rate": 9.645518248990737e-09,
        "epoch": 0.33415943824865757,
        "step": 2427
    },
    {
        "loss": 2.1685,
        "grad_norm": 1.7695645093917847,
        "learning_rate": 7.921781399178318e-09,
        "epoch": 0.3342971224012116,
        "step": 2428
    },
    {
        "loss": 2.199,
        "grad_norm": 1.0358840227127075,
        "learning_rate": 6.367583945021682e-09,
        "epoch": 0.3344348065537657,
        "step": 2429
    },
    {
        "loss": 1.645,
        "grad_norm": 2.4592175483703613,
        "learning_rate": 4.9829285216906884e-09,
        "epoch": 0.3345724907063197,
        "step": 2430
    },
    {
        "loss": 1.8197,
        "grad_norm": 1.7997015714645386,
        "learning_rate": 3.767817476907354e-09,
        "epoch": 0.3347101748588737,
        "step": 2431
    },
    {
        "loss": 2.191,
        "grad_norm": 1.1259214878082275,
        "learning_rate": 2.7222528709347496e-09,
        "epoch": 0.3348478590114278,
        "step": 2432
    },
    {
        "loss": 2.1543,
        "grad_norm": 1.7473559379577637,
        "learning_rate": 1.8462364765547967e-09,
        "epoch": 0.3349855431639818,
        "step": 2433
    },
    {
        "loss": 1.8948,
        "grad_norm": 2.1688506603240967,
        "learning_rate": 1.1397697790793694e-09,
        "epoch": 0.3351232273165359,
        "step": 2434
    },
    {
        "loss": 2.4495,
        "grad_norm": 1.2575856447219849,
        "learning_rate": 6.028539763391905e-10,
        "epoch": 0.33526091146908993,
        "step": 2435
    },
    {
        "loss": 1.7768,
        "grad_norm": 3.187065362930298,
        "learning_rate": 2.3548997869493605e-10,
        "epoch": 0.3353985956216439,
        "step": 2436
    },
    {
        "loss": 2.3046,
        "grad_norm": 1.7616316080093384,
        "learning_rate": 3.76784090150295e-11,
        "epoch": 0.335536279774198,
        "step": 2437
    },
    {
        "loss": 0.801,
        "grad_norm": 2.279482126235962,
        "learning_rate": 0.00019999999058039732,
        "epoch": 0.33567396392675203,
        "step": 2438
    },
    {
        "loss": 2.4931,
        "grad_norm": 0.9604811072349548,
        "learning_rate": 0.00019999984928639235,
        "epoch": 0.3358116480793061,
        "step": 2439
    },
    {
        "loss": 2.6561,
        "grad_norm": 1.0530242919921875,
        "learning_rate": 0.00019999953843981568,
        "epoch": 0.33594933223186013,
        "step": 2440
    },
    {
        "loss": 2.1112,
        "grad_norm": 1.0997036695480347,
        "learning_rate": 0.00019999905804119435,
        "epoch": 0.33608701638441413,
        "step": 2441
    },
    {
        "loss": 1.4373,
        "grad_norm": 2.45375394821167,
        "learning_rate": 0.0001999984080913429,
        "epoch": 0.3362247005369682,
        "step": 2442
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.0899028778076172,
        "learning_rate": 0.0001999975885913633,
        "epoch": 0.33636238468952223,
        "step": 2443
    },
    {
        "loss": 2.3503,
        "grad_norm": 2.0431599617004395,
        "learning_rate": 0.00019999659954264508,
        "epoch": 0.3365000688420763,
        "step": 2444
    },
    {
        "loss": 2.3811,
        "grad_norm": 2.306255340576172,
        "learning_rate": 0.0001999954409468652,
        "epoch": 0.33663775299463033,
        "step": 2445
    },
    {
        "loss": 2.152,
        "grad_norm": 1.2209337949752808,
        "learning_rate": 0.00019999411280598804,
        "epoch": 0.3367754371471844,
        "step": 2446
    },
    {
        "loss": 2.1987,
        "grad_norm": 1.6145776510238647,
        "learning_rate": 0.00019999261512226556,
        "epoch": 0.3369131212997384,
        "step": 2447
    },
    {
        "loss": 2.0133,
        "grad_norm": 1.6319012641906738,
        "learning_rate": 0.0001999909478982371,
        "epoch": 0.33705080545229243,
        "step": 2448
    },
    {
        "loss": 1.45,
        "grad_norm": 1.9407944679260254,
        "learning_rate": 0.00019998911113672946,
        "epoch": 0.3371884896048465,
        "step": 2449
    },
    {
        "loss": 1.5947,
        "grad_norm": 1.4553176164627075,
        "learning_rate": 0.000199987104840857,
        "epoch": 0.33732617375740054,
        "step": 2450
    },
    {
        "loss": 2.49,
        "grad_norm": 1.9417661428451538,
        "learning_rate": 0.00019998492901402134,
        "epoch": 0.3374638579099546,
        "step": 2451
    },
    {
        "loss": 2.5806,
        "grad_norm": 1.172615647315979,
        "learning_rate": 0.00019998258365991174,
        "epoch": 0.3376015420625086,
        "step": 2452
    },
    {
        "loss": 1.6141,
        "grad_norm": 3.2390084266662598,
        "learning_rate": 0.00019998006878250478,
        "epoch": 0.33773922621506264,
        "step": 2453
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.3909845352172852,
        "learning_rate": 0.00019997738438606454,
        "epoch": 0.3378769103676167,
        "step": 2454
    },
    {
        "loss": 0.642,
        "grad_norm": 1.9001460075378418,
        "learning_rate": 0.00019997453047514242,
        "epoch": 0.33801459452017074,
        "step": 2455
    },
    {
        "loss": 2.0231,
        "grad_norm": 1.9423640966415405,
        "learning_rate": 0.0001999715070545774,
        "epoch": 0.3381522786727248,
        "step": 2456
    },
    {
        "loss": 1.8695,
        "grad_norm": 3.1362297534942627,
        "learning_rate": 0.00019996831412949568,
        "epoch": 0.3382899628252788,
        "step": 2457
    },
    {
        "loss": 2.0893,
        "grad_norm": 1.6434718370437622,
        "learning_rate": 0.00019996495170531103,
        "epoch": 0.33842764697783284,
        "step": 2458
    },
    {
        "loss": 2.25,
        "grad_norm": 1.2299875020980835,
        "learning_rate": 0.0001999614197877245,
        "epoch": 0.3385653311303869,
        "step": 2459
    },
    {
        "loss": 1.9098,
        "grad_norm": 2.8806443214416504,
        "learning_rate": 0.00019995771838272457,
        "epoch": 0.33870301528294094,
        "step": 2460
    },
    {
        "loss": 2.18,
        "grad_norm": 2.4685399532318115,
        "learning_rate": 0.00019995384749658704,
        "epoch": 0.338840699435495,
        "step": 2461
    },
    {
        "loss": 2.2302,
        "grad_norm": 2.7097368240356445,
        "learning_rate": 0.00019994980713587516,
        "epoch": 0.338978383588049,
        "step": 2462
    },
    {
        "loss": 2.1189,
        "grad_norm": 2.401524543762207,
        "learning_rate": 0.00019994559730743947,
        "epoch": 0.33911606774060304,
        "step": 2463
    },
    {
        "loss": 0.5698,
        "grad_norm": 2.4396519660949707,
        "learning_rate": 0.0001999412180184178,
        "epoch": 0.3392537518931571,
        "step": 2464
    },
    {
        "loss": 2.4132,
        "grad_norm": 2.9139580726623535,
        "learning_rate": 0.0001999366692762354,
        "epoch": 0.33939143604571115,
        "step": 2465
    },
    {
        "loss": 2.1938,
        "grad_norm": 1.4590001106262207,
        "learning_rate": 0.0001999319510886048,
        "epoch": 0.3395291201982652,
        "step": 2466
    },
    {
        "loss": 2.0702,
        "grad_norm": 2.8757007122039795,
        "learning_rate": 0.00019992706346352577,
        "epoch": 0.3396668043508192,
        "step": 2467
    },
    {
        "loss": 2.1538,
        "grad_norm": 2.0891096591949463,
        "learning_rate": 0.00019992200640928548,
        "epoch": 0.33980448850337325,
        "step": 2468
    },
    {
        "loss": 2.1035,
        "grad_norm": 1.9006000757217407,
        "learning_rate": 0.0001999167799344583,
        "epoch": 0.3399421726559273,
        "step": 2469
    },
    {
        "loss": 1.5114,
        "grad_norm": 2.350044012069702,
        "learning_rate": 0.00019991138404790583,
        "epoch": 0.34007985680848135,
        "step": 2470
    },
    {
        "loss": 1.9123,
        "grad_norm": 1.764731526374817,
        "learning_rate": 0.00019990581875877695,
        "epoch": 0.3402175409610354,
        "step": 2471
    },
    {
        "loss": 2.2519,
        "grad_norm": 1.4285846948623657,
        "learning_rate": 0.00019990008407650778,
        "epoch": 0.3403552251135894,
        "step": 2472
    },
    {
        "loss": 2.6699,
        "grad_norm": 1.0043212175369263,
        "learning_rate": 0.00019989418001082162,
        "epoch": 0.34049290926614345,
        "step": 2473
    },
    {
        "loss": 2.1755,
        "grad_norm": 3.33284854888916,
        "learning_rate": 0.00019988810657172902,
        "epoch": 0.3406305934186975,
        "step": 2474
    },
    {
        "loss": 2.3163,
        "grad_norm": 1.3905742168426514,
        "learning_rate": 0.00019988186376952764,
        "epoch": 0.34076827757125155,
        "step": 2475
    },
    {
        "loss": 1.4941,
        "grad_norm": 2.2345330715179443,
        "learning_rate": 0.00019987545161480235,
        "epoch": 0.3409059617238056,
        "step": 2476
    },
    {
        "loss": 2.1135,
        "grad_norm": 1.5648307800292969,
        "learning_rate": 0.00019986887011842508,
        "epoch": 0.34104364587635966,
        "step": 2477
    },
    {
        "loss": 2.2705,
        "grad_norm": 1.736635446548462,
        "learning_rate": 0.000199862119291555,
        "epoch": 0.34118133002891365,
        "step": 2478
    },
    {
        "loss": 1.4877,
        "grad_norm": 3.1240954399108887,
        "learning_rate": 0.0001998551991456383,
        "epoch": 0.3413190141814677,
        "step": 2479
    },
    {
        "loss": 2.5192,
        "grad_norm": 1.6598856449127197,
        "learning_rate": 0.00019984810969240832,
        "epoch": 0.34145669833402176,
        "step": 2480
    },
    {
        "loss": 1.8466,
        "grad_norm": 2.371938943862915,
        "learning_rate": 0.00019984085094388538,
        "epoch": 0.3415943824865758,
        "step": 2481
    },
    {
        "loss": 2.1236,
        "grad_norm": 2.9854278564453125,
        "learning_rate": 0.00019983342291237692,
        "epoch": 0.34173206663912986,
        "step": 2482
    },
    {
        "loss": 2.0134,
        "grad_norm": 1.4572817087173462,
        "learning_rate": 0.00019982582561047738,
        "epoch": 0.34186975079168386,
        "step": 2483
    },
    {
        "loss": 2.3212,
        "grad_norm": 1.4694803953170776,
        "learning_rate": 0.0001998180590510682,
        "epoch": 0.3420074349442379,
        "step": 2484
    },
    {
        "loss": 2.1412,
        "grad_norm": 1.472129225730896,
        "learning_rate": 0.00019981012324731776,
        "epoch": 0.34214511909679196,
        "step": 2485
    },
    {
        "loss": 1.788,
        "grad_norm": 2.5818934440612793,
        "learning_rate": 0.0001998020182126815,
        "epoch": 0.342282803249346,
        "step": 2486
    },
    {
        "loss": 1.9926,
        "grad_norm": 1.590041995048523,
        "learning_rate": 0.0001997937439609017,
        "epoch": 0.34242048740190006,
        "step": 2487
    },
    {
        "loss": 1.8387,
        "grad_norm": 2.407198905944824,
        "learning_rate": 0.00019978530050600762,
        "epoch": 0.34255817155445406,
        "step": 2488
    },
    {
        "loss": 1.7546,
        "grad_norm": 1.8922762870788574,
        "learning_rate": 0.00019977668786231534,
        "epoch": 0.3426958557070081,
        "step": 2489
    },
    {
        "loss": 2.1922,
        "grad_norm": 2.0117878913879395,
        "learning_rate": 0.00019976790604442783,
        "epoch": 0.34283353985956216,
        "step": 2490
    },
    {
        "loss": 2.2438,
        "grad_norm": 2.39005184173584,
        "learning_rate": 0.00019975895506723495,
        "epoch": 0.3429712240121162,
        "step": 2491
    },
    {
        "loss": 2.0449,
        "grad_norm": 2.4244470596313477,
        "learning_rate": 0.00019974983494591334,
        "epoch": 0.34310890816467027,
        "step": 2492
    },
    {
        "loss": 2.3844,
        "grad_norm": 1.994944453239441,
        "learning_rate": 0.0001997405456959264,
        "epoch": 0.34324659231722426,
        "step": 2493
    },
    {
        "loss": 2.2944,
        "grad_norm": 1.4485968351364136,
        "learning_rate": 0.0001997310873330243,
        "epoch": 0.3433842764697783,
        "step": 2494
    },
    {
        "loss": 1.7313,
        "grad_norm": 2.9818477630615234,
        "learning_rate": 0.00019972145987324399,
        "epoch": 0.34352196062233237,
        "step": 2495
    },
    {
        "loss": 2.1338,
        "grad_norm": 1.89664888381958,
        "learning_rate": 0.0001997116633329091,
        "epoch": 0.3436596447748864,
        "step": 2496
    },
    {
        "loss": 2.1877,
        "grad_norm": 2.1052350997924805,
        "learning_rate": 0.0001997016977286299,
        "epoch": 0.34379732892744047,
        "step": 2497
    },
    {
        "loss": 1.8339,
        "grad_norm": 2.39689302444458,
        "learning_rate": 0.00019969156307730342,
        "epoch": 0.34393501307999447,
        "step": 2498
    },
    {
        "loss": 1.7686,
        "grad_norm": 1.5870952606201172,
        "learning_rate": 0.0001996812593961132,
        "epoch": 0.3440726972325485,
        "step": 2499
    },
    {
        "loss": 2.5691,
        "grad_norm": 1.0270065069198608,
        "learning_rate": 0.00019967078670252945,
        "epoch": 0.34421038138510257,
        "step": 2500
    },
    {
        "loss": 1.9487,
        "grad_norm": 1.958210825920105,
        "learning_rate": 0.0001996601450143089,
        "epoch": 0.3443480655376566,
        "step": 2501
    },
    {
        "loss": 2.4729,
        "grad_norm": 0.9921632409095764,
        "learning_rate": 0.00019964933434949483,
        "epoch": 0.3444857496902107,
        "step": 2502
    },
    {
        "loss": 2.7845,
        "grad_norm": 0.9534778594970703,
        "learning_rate": 0.00019963835472641702,
        "epoch": 0.34462343384276467,
        "step": 2503
    },
    {
        "loss": 2.2635,
        "grad_norm": 1.7230793237686157,
        "learning_rate": 0.0001996272061636918,
        "epoch": 0.3447611179953187,
        "step": 2504
    },
    {
        "loss": 2.2252,
        "grad_norm": 1.8194717168807983,
        "learning_rate": 0.0001996158886802218,
        "epoch": 0.3448988021478728,
        "step": 2505
    },
    {
        "loss": 2.5895,
        "grad_norm": 1.5361870527267456,
        "learning_rate": 0.00019960440229519613,
        "epoch": 0.3450364863004268,
        "step": 2506
    },
    {
        "loss": 2.2669,
        "grad_norm": 1.1980806589126587,
        "learning_rate": 0.00019959274702809033,
        "epoch": 0.3451741704529809,
        "step": 2507
    },
    {
        "loss": 2.4696,
        "grad_norm": 2.232473134994507,
        "learning_rate": 0.00019958092289866616,
        "epoch": 0.34531185460553493,
        "step": 2508
    },
    {
        "loss": 2.0906,
        "grad_norm": 1.7589651346206665,
        "learning_rate": 0.00019956892992697186,
        "epoch": 0.3454495387580889,
        "step": 2509
    },
    {
        "loss": 2.2754,
        "grad_norm": 1.341077208518982,
        "learning_rate": 0.0001995567681333418,
        "epoch": 0.345587222910643,
        "step": 2510
    },
    {
        "loss": 1.8072,
        "grad_norm": 2.3658013343811035,
        "learning_rate": 0.00019954443753839667,
        "epoch": 0.34572490706319703,
        "step": 2511
    },
    {
        "loss": 2.4639,
        "grad_norm": 1.984682321548462,
        "learning_rate": 0.0001995319381630433,
        "epoch": 0.3458625912157511,
        "step": 2512
    },
    {
        "loss": 2.1475,
        "grad_norm": 1.260668158531189,
        "learning_rate": 0.00019951927002847476,
        "epoch": 0.34600027536830513,
        "step": 2513
    },
    {
        "loss": 2.0664,
        "grad_norm": 2.1882615089416504,
        "learning_rate": 0.00019950643315617023,
        "epoch": 0.34613795952085913,
        "step": 2514
    },
    {
        "loss": 2.2626,
        "grad_norm": 1.1535817384719849,
        "learning_rate": 0.000199493427567895,
        "epoch": 0.3462756436734132,
        "step": 2515
    },
    {
        "loss": 2.2226,
        "grad_norm": 1.713037371635437,
        "learning_rate": 0.00019948025328570042,
        "epoch": 0.34641332782596723,
        "step": 2516
    },
    {
        "loss": 0.7597,
        "grad_norm": 2.127601146697998,
        "learning_rate": 0.0001994669103319238,
        "epoch": 0.3465510119785213,
        "step": 2517
    },
    {
        "loss": 1.5324,
        "grad_norm": 3.255791664123535,
        "learning_rate": 0.00019945339872918857,
        "epoch": 0.34668869613107534,
        "step": 2518
    },
    {
        "loss": 1.9997,
        "grad_norm": 1.602251648902893,
        "learning_rate": 0.000199439718500404,
        "epoch": 0.34682638028362933,
        "step": 2519
    },
    {
        "loss": 2.0505,
        "grad_norm": 1.7220268249511719,
        "learning_rate": 0.00019942586966876526,
        "epoch": 0.3469640644361834,
        "step": 2520
    },
    {
        "loss": 1.1918,
        "grad_norm": 2.1401805877685547,
        "learning_rate": 0.00019941185225775352,
        "epoch": 0.34710174858873744,
        "step": 2521
    },
    {
        "loss": 1.9528,
        "grad_norm": 2.0444235801696777,
        "learning_rate": 0.00019939766629113566,
        "epoch": 0.3472394327412915,
        "step": 2522
    },
    {
        "loss": 2.4906,
        "grad_norm": 1.2446796894073486,
        "learning_rate": 0.00019938331179296436,
        "epoch": 0.34737711689384554,
        "step": 2523
    },
    {
        "loss": 1.6897,
        "grad_norm": 1.9216042757034302,
        "learning_rate": 0.0001993687887875781,
        "epoch": 0.34751480104639954,
        "step": 2524
    },
    {
        "loss": 2.0029,
        "grad_norm": 1.4535983800888062,
        "learning_rate": 0.00019935409729960105,
        "epoch": 0.3476524851989536,
        "step": 2525
    },
    {
        "loss": 2.1373,
        "grad_norm": 2.1459338665008545,
        "learning_rate": 0.00019933923735394307,
        "epoch": 0.34779016935150764,
        "step": 2526
    },
    {
        "loss": 2.4445,
        "grad_norm": 1.4729743003845215,
        "learning_rate": 0.00019932420897579957,
        "epoch": 0.3479278535040617,
        "step": 2527
    },
    {
        "loss": 2.1442,
        "grad_norm": 1.9327061176300049,
        "learning_rate": 0.00019930901219065162,
        "epoch": 0.34806553765661574,
        "step": 2528
    },
    {
        "loss": 2.2275,
        "grad_norm": 1.2815852165222168,
        "learning_rate": 0.00019929364702426575,
        "epoch": 0.34820322180916974,
        "step": 2529
    },
    {
        "loss": 2.3181,
        "grad_norm": 0.9835036396980286,
        "learning_rate": 0.0001992781135026941,
        "epoch": 0.3483409059617238,
        "step": 2530
    },
    {
        "loss": 1.7149,
        "grad_norm": 1.575998067855835,
        "learning_rate": 0.0001992624116522742,
        "epoch": 0.34847859011427784,
        "step": 2531
    },
    {
        "loss": 1.8577,
        "grad_norm": 2.1724367141723633,
        "learning_rate": 0.00019924654149962892,
        "epoch": 0.3486162742668319,
        "step": 2532
    },
    {
        "loss": 2.1204,
        "grad_norm": 2.0320522785186768,
        "learning_rate": 0.00019923050307166655,
        "epoch": 0.34875395841938595,
        "step": 2533
    },
    {
        "loss": 2.2005,
        "grad_norm": 2.5558555126190186,
        "learning_rate": 0.00019921429639558076,
        "epoch": 0.34889164257194,
        "step": 2534
    },
    {
        "loss": 2.6573,
        "grad_norm": 1.1210688352584839,
        "learning_rate": 0.0001991979214988504,
        "epoch": 0.349029326724494,
        "step": 2535
    },
    {
        "loss": 2.5172,
        "grad_norm": 1.289757490158081,
        "learning_rate": 0.00019918137840923956,
        "epoch": 0.34916701087704805,
        "step": 2536
    },
    {
        "loss": 1.8908,
        "grad_norm": 2.675632953643799,
        "learning_rate": 0.00019916466715479752,
        "epoch": 0.3493046950296021,
        "step": 2537
    },
    {
        "loss": 2.3561,
        "grad_norm": 2.155646800994873,
        "learning_rate": 0.0001991477877638587,
        "epoch": 0.34944237918215615,
        "step": 2538
    },
    {
        "loss": 1.8313,
        "grad_norm": 1.576501727104187,
        "learning_rate": 0.00019913074026504254,
        "epoch": 0.3495800633347102,
        "step": 2539
    },
    {
        "loss": 2.173,
        "grad_norm": 1.1067594289779663,
        "learning_rate": 0.0001991135246872536,
        "epoch": 0.3497177474872642,
        "step": 2540
    },
    {
        "loss": 2.2461,
        "grad_norm": 1.5610228776931763,
        "learning_rate": 0.00019909614105968138,
        "epoch": 0.34985543163981825,
        "step": 2541
    },
    {
        "loss": 2.0969,
        "grad_norm": 1.462910532951355,
        "learning_rate": 0.0001990785894118003,
        "epoch": 0.3499931157923723,
        "step": 2542
    },
    {
        "loss": 2.0856,
        "grad_norm": 2.2339928150177,
        "learning_rate": 0.00019906086977336968,
        "epoch": 0.35013079994492635,
        "step": 2543
    },
    {
        "loss": 2.0736,
        "grad_norm": 2.614147424697876,
        "learning_rate": 0.00019904298217443366,
        "epoch": 0.3502684840974804,
        "step": 2544
    },
    {
        "loss": 2.6751,
        "grad_norm": 1.8574270009994507,
        "learning_rate": 0.00019902492664532114,
        "epoch": 0.3504061682500344,
        "step": 2545
    },
    {
        "loss": 2.1257,
        "grad_norm": 2.1436338424682617,
        "learning_rate": 0.00019900670321664586,
        "epoch": 0.35054385240258845,
        "step": 2546
    },
    {
        "loss": 2.1271,
        "grad_norm": 2.0633468627929688,
        "learning_rate": 0.00019898831191930607,
        "epoch": 0.3506815365551425,
        "step": 2547
    },
    {
        "loss": 1.5483,
        "grad_norm": 1.732161045074463,
        "learning_rate": 0.0001989697527844848,
        "epoch": 0.35081922070769656,
        "step": 2548
    },
    {
        "loss": 2.2645,
        "grad_norm": 2.0055172443389893,
        "learning_rate": 0.00019895102584364954,
        "epoch": 0.3509569048602506,
        "step": 2549
    },
    {
        "loss": 2.2818,
        "grad_norm": 1.8835742473602295,
        "learning_rate": 0.0001989321311285524,
        "epoch": 0.3510945890128046,
        "step": 2550
    },
    {
        "loss": 2.0843,
        "grad_norm": 1.3763452768325806,
        "learning_rate": 0.0001989130686712299,
        "epoch": 0.35123227316535865,
        "step": 2551
    },
    {
        "loss": 1.8476,
        "grad_norm": 2.3560752868652344,
        "learning_rate": 0.00019889383850400287,
        "epoch": 0.3513699573179127,
        "step": 2552
    },
    {
        "loss": 1.7264,
        "grad_norm": 3.030271291732788,
        "learning_rate": 0.00019887444065947674,
        "epoch": 0.35150764147046676,
        "step": 2553
    },
    {
        "loss": 1.8698,
        "grad_norm": 1.4908878803253174,
        "learning_rate": 0.00019885487517054104,
        "epoch": 0.3516453256230208,
        "step": 2554
    },
    {
        "loss": 2.5203,
        "grad_norm": 1.5394490957260132,
        "learning_rate": 0.00019883514207036956,
        "epoch": 0.3517830097755748,
        "step": 2555
    },
    {
        "loss": 2.1409,
        "grad_norm": 1.1071124076843262,
        "learning_rate": 0.00019881524139242048,
        "epoch": 0.35192069392812886,
        "step": 2556
    },
    {
        "loss": 2.2918,
        "grad_norm": 1.7991485595703125,
        "learning_rate": 0.0001987951731704358,
        "epoch": 0.3520583780806829,
        "step": 2557
    },
    {
        "loss": 2.0025,
        "grad_norm": 2.4153060913085938,
        "learning_rate": 0.00019877493743844182,
        "epoch": 0.35219606223323696,
        "step": 2558
    },
    {
        "loss": 2.5381,
        "grad_norm": 1.6729726791381836,
        "learning_rate": 0.00019875453423074882,
        "epoch": 0.352333746385791,
        "step": 2559
    },
    {
        "loss": 1.6298,
        "grad_norm": 1.566907525062561,
        "learning_rate": 0.000198733963581951,
        "epoch": 0.352471430538345,
        "step": 2560
    },
    {
        "loss": 1.8151,
        "grad_norm": 1.5763475894927979,
        "learning_rate": 0.00019871322552692647,
        "epoch": 0.35260911469089906,
        "step": 2561
    },
    {
        "loss": 2.3055,
        "grad_norm": 1.9996496438980103,
        "learning_rate": 0.00019869232010083718,
        "epoch": 0.3527467988434531,
        "step": 2562
    },
    {
        "loss": 2.4015,
        "grad_norm": 1.2086412906646729,
        "learning_rate": 0.0001986712473391289,
        "epoch": 0.35288448299600716,
        "step": 2563
    },
    {
        "loss": 2.2758,
        "grad_norm": 2.247251033782959,
        "learning_rate": 0.00019865000727753107,
        "epoch": 0.3530221671485612,
        "step": 2564
    },
    {
        "loss": 2.1452,
        "grad_norm": 1.8847256898880005,
        "learning_rate": 0.00019862859995205682,
        "epoch": 0.35315985130111527,
        "step": 2565
    },
    {
        "loss": 1.8668,
        "grad_norm": 1.5881197452545166,
        "learning_rate": 0.00019860702539900284,
        "epoch": 0.35329753545366926,
        "step": 2566
    },
    {
        "loss": 2.0934,
        "grad_norm": 1.563843846321106,
        "learning_rate": 0.00019858528365494947,
        "epoch": 0.3534352196062233,
        "step": 2567
    },
    {
        "loss": 2.5876,
        "grad_norm": 1.4552632570266724,
        "learning_rate": 0.00019856337475676044,
        "epoch": 0.35357290375877737,
        "step": 2568
    },
    {
        "loss": 2.1178,
        "grad_norm": 2.3019139766693115,
        "learning_rate": 0.0001985412987415828,
        "epoch": 0.3537105879113314,
        "step": 2569
    },
    {
        "loss": 1.4321,
        "grad_norm": 2.119384765625,
        "learning_rate": 0.0001985190556468472,
        "epoch": 0.35384827206388547,
        "step": 2570
    },
    {
        "loss": 2.3386,
        "grad_norm": 1.683532953262329,
        "learning_rate": 0.00019849664551026735,
        "epoch": 0.35398595621643947,
        "step": 2571
    },
    {
        "loss": 1.9357,
        "grad_norm": 2.7483348846435547,
        "learning_rate": 0.00019847406836984028,
        "epoch": 0.3541236403689935,
        "step": 2572
    },
    {
        "loss": 1.9802,
        "grad_norm": 1.3834632635116577,
        "learning_rate": 0.0001984513242638462,
        "epoch": 0.35426132452154757,
        "step": 2573
    },
    {
        "loss": 2.2855,
        "grad_norm": 1.6308376789093018,
        "learning_rate": 0.0001984284132308484,
        "epoch": 0.3543990086741016,
        "step": 2574
    },
    {
        "loss": 1.3703,
        "grad_norm": 3.676731824874878,
        "learning_rate": 0.00019840533530969307,
        "epoch": 0.3545366928266557,
        "step": 2575
    },
    {
        "loss": 2.012,
        "grad_norm": 2.2959485054016113,
        "learning_rate": 0.00019838209053950963,
        "epoch": 0.35467437697920967,
        "step": 2576
    },
    {
        "loss": 2.1535,
        "grad_norm": 1.5377614498138428,
        "learning_rate": 0.00019835867895971014,
        "epoch": 0.3548120611317637,
        "step": 2577
    },
    {
        "loss": 2.1736,
        "grad_norm": 1.5160883665084839,
        "learning_rate": 0.0001983351006099896,
        "epoch": 0.3549497452843178,
        "step": 2578
    },
    {
        "loss": 2.4105,
        "grad_norm": 2.2573204040527344,
        "learning_rate": 0.00019831135553032586,
        "epoch": 0.3550874294368718,
        "step": 2579
    },
    {
        "loss": 2.3468,
        "grad_norm": 1.6563819646835327,
        "learning_rate": 0.0001982874437609793,
        "epoch": 0.3552251135894259,
        "step": 2580
    },
    {
        "loss": 1.7368,
        "grad_norm": 3.0927772521972656,
        "learning_rate": 0.000198263365342493,
        "epoch": 0.3553627977419799,
        "step": 2581
    },
    {
        "loss": 1.4322,
        "grad_norm": 1.5347979068756104,
        "learning_rate": 0.00019823912031569261,
        "epoch": 0.3555004818945339,
        "step": 2582
    },
    {
        "loss": 2.224,
        "grad_norm": 1.832087516784668,
        "learning_rate": 0.00019821470872168626,
        "epoch": 0.355638166047088,
        "step": 2583
    },
    {
        "loss": 1.2957,
        "grad_norm": 2.0274288654327393,
        "learning_rate": 0.0001981901306018645,
        "epoch": 0.35577585019964203,
        "step": 2584
    },
    {
        "loss": 0.7336,
        "grad_norm": 2.9293277263641357,
        "learning_rate": 0.00019816538599790024,
        "epoch": 0.3559135343521961,
        "step": 2585
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.6881492137908936,
        "learning_rate": 0.0001981404749517486,
        "epoch": 0.3560512185047501,
        "step": 2586
    },
    {
        "loss": 2.3402,
        "grad_norm": 2.7609493732452393,
        "learning_rate": 0.00019811539750564703,
        "epoch": 0.35618890265730413,
        "step": 2587
    },
    {
        "loss": 1.7326,
        "grad_norm": 2.19109845161438,
        "learning_rate": 0.00019809015370211502,
        "epoch": 0.3563265868098582,
        "step": 2588
    },
    {
        "loss": 2.411,
        "grad_norm": 1.658713698387146,
        "learning_rate": 0.00019806474358395417,
        "epoch": 0.35646427096241223,
        "step": 2589
    },
    {
        "loss": 2.2232,
        "grad_norm": 2.2766284942626953,
        "learning_rate": 0.000198039167194248,
        "epoch": 0.3566019551149663,
        "step": 2590
    },
    {
        "loss": 2.199,
        "grad_norm": 1.3449002504348755,
        "learning_rate": 0.0001980134245763621,
        "epoch": 0.3567396392675203,
        "step": 2591
    },
    {
        "loss": 2.4803,
        "grad_norm": 1.7587487697601318,
        "learning_rate": 0.00019798751577394374,
        "epoch": 0.35687732342007433,
        "step": 2592
    },
    {
        "loss": 1.7026,
        "grad_norm": 2.1334586143493652,
        "learning_rate": 0.000197961440830922,
        "epoch": 0.3570150075726284,
        "step": 2593
    },
    {
        "loss": 2.1916,
        "grad_norm": 1.8231219053268433,
        "learning_rate": 0.00019793519979150771,
        "epoch": 0.35715269172518244,
        "step": 2594
    },
    {
        "loss": 2.1833,
        "grad_norm": 1.3591723442077637,
        "learning_rate": 0.00019790879270019333,
        "epoch": 0.3572903758777365,
        "step": 2595
    },
    {
        "loss": 2.3508,
        "grad_norm": 2.274122953414917,
        "learning_rate": 0.00019788221960175283,
        "epoch": 0.35742806003029054,
        "step": 2596
    },
    {
        "loss": 1.2861,
        "grad_norm": 2.057778835296631,
        "learning_rate": 0.0001978554805412416,
        "epoch": 0.35756574418284454,
        "step": 2597
    },
    {
        "loss": 2.1293,
        "grad_norm": 1.513660192489624,
        "learning_rate": 0.00019782857556399646,
        "epoch": 0.3577034283353986,
        "step": 2598
    },
    {
        "loss": 1.9171,
        "grad_norm": 1.786400556564331,
        "learning_rate": 0.00019780150471563558,
        "epoch": 0.35784111248795264,
        "step": 2599
    },
    {
        "loss": 2.68,
        "grad_norm": 1.0575942993164062,
        "learning_rate": 0.00019777426804205842,
        "epoch": 0.3579787966405067,
        "step": 2600
    },
    {
        "loss": 2.3682,
        "grad_norm": 1.2925220727920532,
        "learning_rate": 0.00019774686558944544,
        "epoch": 0.35811648079306074,
        "step": 2601
    },
    {
        "loss": 2.0832,
        "grad_norm": 1.7251650094985962,
        "learning_rate": 0.00019771929740425832,
        "epoch": 0.35825416494561474,
        "step": 2602
    },
    {
        "loss": 2.5425,
        "grad_norm": 1.625410795211792,
        "learning_rate": 0.0001976915635332397,
        "epoch": 0.3583918490981688,
        "step": 2603
    },
    {
        "loss": 2.1788,
        "grad_norm": 1.3763476610183716,
        "learning_rate": 0.0001976636640234131,
        "epoch": 0.35852953325072284,
        "step": 2604
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.4913486242294312,
        "learning_rate": 0.00019763559892208298,
        "epoch": 0.3586672174032769,
        "step": 2605
    },
    {
        "loss": 1.9213,
        "grad_norm": 2.295042037963867,
        "learning_rate": 0.00019760736827683446,
        "epoch": 0.35880490155583095,
        "step": 2606
    },
    {
        "loss": 1.5567,
        "grad_norm": 1.964154839515686,
        "learning_rate": 0.0001975789721355334,
        "epoch": 0.35894258570838494,
        "step": 2607
    },
    {
        "loss": 2.2538,
        "grad_norm": 1.9921917915344238,
        "learning_rate": 0.00019755041054632635,
        "epoch": 0.359080269860939,
        "step": 2608
    },
    {
        "loss": 2.8103,
        "grad_norm": 1.956549048423767,
        "learning_rate": 0.00019752168355764021,
        "epoch": 0.35921795401349305,
        "step": 2609
    },
    {
        "loss": 2.1527,
        "grad_norm": 2.514585256576538,
        "learning_rate": 0.00019749279121818235,
        "epoch": 0.3593556381660471,
        "step": 2610
    },
    {
        "loss": 2.5903,
        "grad_norm": 1.316176176071167,
        "learning_rate": 0.0001974637335769407,
        "epoch": 0.35949332231860115,
        "step": 2611
    },
    {
        "loss": 2.6639,
        "grad_norm": 1.2378565073013306,
        "learning_rate": 0.00019743451068318318,
        "epoch": 0.35963100647115515,
        "step": 2612
    },
    {
        "loss": 1.5977,
        "grad_norm": 3.6495611667633057,
        "learning_rate": 0.0001974051225864581,
        "epoch": 0.3597686906237092,
        "step": 2613
    },
    {
        "loss": 2.0594,
        "grad_norm": 1.1651173830032349,
        "learning_rate": 0.00019737556933659378,
        "epoch": 0.35990637477626325,
        "step": 2614
    },
    {
        "loss": 1.3453,
        "grad_norm": 2.3559346199035645,
        "learning_rate": 0.0001973458509836986,
        "epoch": 0.3600440589288173,
        "step": 2615
    },
    {
        "loss": 2.3682,
        "grad_norm": 1.9492558240890503,
        "learning_rate": 0.0001973159675781609,
        "epoch": 0.36018174308137135,
        "step": 2616
    },
    {
        "loss": 1.7807,
        "grad_norm": 1.5901052951812744,
        "learning_rate": 0.00019728591917064878,
        "epoch": 0.36031942723392535,
        "step": 2617
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.4893054962158203,
        "learning_rate": 0.0001972557058121102,
        "epoch": 0.3604571113864794,
        "step": 2618
    },
    {
        "loss": 1.924,
        "grad_norm": 1.2740542888641357,
        "learning_rate": 0.00019722532755377272,
        "epoch": 0.36059479553903345,
        "step": 2619
    },
    {
        "loss": 1.9375,
        "grad_norm": 2.0250821113586426,
        "learning_rate": 0.00019719478444714362,
        "epoch": 0.3607324796915875,
        "step": 2620
    },
    {
        "loss": 2.4103,
        "grad_norm": 1.6298869848251343,
        "learning_rate": 0.00019716407654400952,
        "epoch": 0.36087016384414156,
        "step": 2621
    },
    {
        "loss": 2.3341,
        "grad_norm": 2.809530735015869,
        "learning_rate": 0.00019713320389643657,
        "epoch": 0.3610078479966956,
        "step": 2622
    },
    {
        "loss": 2.2897,
        "grad_norm": 1.7317297458648682,
        "learning_rate": 0.00019710216655677022,
        "epoch": 0.3611455321492496,
        "step": 2623
    },
    {
        "loss": 2.3168,
        "grad_norm": 1.0790141820907593,
        "learning_rate": 0.00019707096457763517,
        "epoch": 0.36128321630180366,
        "step": 2624
    },
    {
        "loss": 1.4226,
        "grad_norm": 3.803740978240967,
        "learning_rate": 0.00019703959801193523,
        "epoch": 0.3614209004543577,
        "step": 2625
    },
    {
        "loss": 2.1081,
        "grad_norm": 1.685451626777649,
        "learning_rate": 0.0001970080669128533,
        "epoch": 0.36155858460691176,
        "step": 2626
    },
    {
        "loss": 2.532,
        "grad_norm": 1.7194734811782837,
        "learning_rate": 0.00019697637133385127,
        "epoch": 0.3616962687594658,
        "step": 2627
    },
    {
        "loss": 1.9974,
        "grad_norm": 1.8759580850601196,
        "learning_rate": 0.0001969445113286699,
        "epoch": 0.3618339529120198,
        "step": 2628
    },
    {
        "loss": 2.3131,
        "grad_norm": 1.5973024368286133,
        "learning_rate": 0.0001969124869513287,
        "epoch": 0.36197163706457386,
        "step": 2629
    },
    {
        "loss": 2.406,
        "grad_norm": 1.719362735748291,
        "learning_rate": 0.00019688029825612588,
        "epoch": 0.3621093212171279,
        "step": 2630
    },
    {
        "loss": 1.6591,
        "grad_norm": 3.6676013469696045,
        "learning_rate": 0.00019684794529763834,
        "epoch": 0.36224700536968196,
        "step": 2631
    },
    {
        "loss": 2.2435,
        "grad_norm": 1.7929199934005737,
        "learning_rate": 0.00019681542813072145,
        "epoch": 0.362384689522236,
        "step": 2632
    },
    {
        "loss": 2.2183,
        "grad_norm": 2.505446195602417,
        "learning_rate": 0.00019678274681050892,
        "epoch": 0.36252237367479,
        "step": 2633
    },
    {
        "loss": 2.2317,
        "grad_norm": 1.7857234477996826,
        "learning_rate": 0.00019674990139241288,
        "epoch": 0.36266005782734406,
        "step": 2634
    },
    {
        "loss": 2.44,
        "grad_norm": 1.7409344911575317,
        "learning_rate": 0.0001967168919321237,
        "epoch": 0.3627977419798981,
        "step": 2635
    },
    {
        "loss": 0.9644,
        "grad_norm": 2.878755569458008,
        "learning_rate": 0.0001966837184856098,
        "epoch": 0.36293542613245217,
        "step": 2636
    },
    {
        "loss": 2.4961,
        "grad_norm": 1.633954405784607,
        "learning_rate": 0.0001966503811091177,
        "epoch": 0.3630731102850062,
        "step": 2637
    },
    {
        "loss": 2.409,
        "grad_norm": 1.90314519405365,
        "learning_rate": 0.00019661687985917192,
        "epoch": 0.3632107944375602,
        "step": 2638
    },
    {
        "loss": 2.1466,
        "grad_norm": 1.1333043575286865,
        "learning_rate": 0.00019658321479257472,
        "epoch": 0.36334847859011427,
        "step": 2639
    },
    {
        "loss": 2.1586,
        "grad_norm": 1.68108069896698,
        "learning_rate": 0.00019654938596640618,
        "epoch": 0.3634861627426683,
        "step": 2640
    },
    {
        "loss": 2.4278,
        "grad_norm": 1.4875239133834839,
        "learning_rate": 0.00019651539343802407,
        "epoch": 0.36362384689522237,
        "step": 2641
    },
    {
        "loss": 2.0049,
        "grad_norm": 1.4763693809509277,
        "learning_rate": 0.00019648123726506364,
        "epoch": 0.3637615310477764,
        "step": 2642
    },
    {
        "loss": 1.8986,
        "grad_norm": 1.8695199489593506,
        "learning_rate": 0.00019644691750543767,
        "epoch": 0.3638992152003304,
        "step": 2643
    },
    {
        "loss": 2.0657,
        "grad_norm": 2.0089633464813232,
        "learning_rate": 0.00019641243421733627,
        "epoch": 0.36403689935288447,
        "step": 2644
    },
    {
        "loss": 2.3886,
        "grad_norm": 1.8139114379882812,
        "learning_rate": 0.00019637778745922683,
        "epoch": 0.3641745835054385,
        "step": 2645
    },
    {
        "loss": 1.0435,
        "grad_norm": 2.3804166316986084,
        "learning_rate": 0.00019634297728985393,
        "epoch": 0.3643122676579926,
        "step": 2646
    },
    {
        "loss": 2.1631,
        "grad_norm": 2.12278413772583,
        "learning_rate": 0.00019630800376823916,
        "epoch": 0.3644499518105466,
        "step": 2647
    },
    {
        "loss": 2.2819,
        "grad_norm": 1.5200895071029663,
        "learning_rate": 0.00019627286695368115,
        "epoch": 0.3645876359631006,
        "step": 2648
    },
    {
        "loss": 2.3374,
        "grad_norm": 2.1726181507110596,
        "learning_rate": 0.00019623756690575532,
        "epoch": 0.36472532011565467,
        "step": 2649
    },
    {
        "loss": 2.1826,
        "grad_norm": 2.3582210540771484,
        "learning_rate": 0.00019620210368431399,
        "epoch": 0.3648630042682087,
        "step": 2650
    },
    {
        "loss": 2.4981,
        "grad_norm": 2.014045000076294,
        "learning_rate": 0.00019616647734948593,
        "epoch": 0.3650006884207628,
        "step": 2651
    },
    {
        "loss": 1.8627,
        "grad_norm": 1.3772823810577393,
        "learning_rate": 0.00019613068796167667,
        "epoch": 0.3651383725733168,
        "step": 2652
    },
    {
        "loss": 1.4534,
        "grad_norm": 2.4856345653533936,
        "learning_rate": 0.00019609473558156813,
        "epoch": 0.3652760567258709,
        "step": 2653
    },
    {
        "loss": 2.5833,
        "grad_norm": 1.2294832468032837,
        "learning_rate": 0.00019605862027011854,
        "epoch": 0.3654137408784249,
        "step": 2654
    },
    {
        "loss": 2.5548,
        "grad_norm": 1.9909944534301758,
        "learning_rate": 0.00019602234208856252,
        "epoch": 0.3655514250309789,
        "step": 2655
    },
    {
        "loss": 2.62,
        "grad_norm": 1.5010008811950684,
        "learning_rate": 0.00019598590109841065,
        "epoch": 0.365689109183533,
        "step": 2656
    },
    {
        "loss": 1.958,
        "grad_norm": 2.1796398162841797,
        "learning_rate": 0.00019594929736144976,
        "epoch": 0.36582679333608703,
        "step": 2657
    },
    {
        "loss": 2.5574,
        "grad_norm": 1.3071871995925903,
        "learning_rate": 0.00019591253093974244,
        "epoch": 0.3659644774886411,
        "step": 2658
    },
    {
        "loss": 2.178,
        "grad_norm": 2.1907999515533447,
        "learning_rate": 0.0001958756018956272,
        "epoch": 0.3661021616411951,
        "step": 2659
    },
    {
        "loss": 2.3501,
        "grad_norm": 1.4026353359222412,
        "learning_rate": 0.00019583851029171832,
        "epoch": 0.36623984579374913,
        "step": 2660
    },
    {
        "loss": 2.6568,
        "grad_norm": 1.6586873531341553,
        "learning_rate": 0.00019580125619090568,
        "epoch": 0.3663775299463032,
        "step": 2661
    },
    {
        "loss": 2.1345,
        "grad_norm": 2.3102035522460938,
        "learning_rate": 0.0001957638396563546,
        "epoch": 0.36651521409885723,
        "step": 2662
    },
    {
        "loss": 2.2177,
        "grad_norm": 2.36568546295166,
        "learning_rate": 0.00019572626075150588,
        "epoch": 0.3666528982514113,
        "step": 2663
    },
    {
        "loss": 2.1314,
        "grad_norm": 1.3662368059158325,
        "learning_rate": 0.0001956885195400757,
        "epoch": 0.3667905824039653,
        "step": 2664
    },
    {
        "loss": 2.5804,
        "grad_norm": 1.8510425090789795,
        "learning_rate": 0.00019565061608605526,
        "epoch": 0.36692826655651933,
        "step": 2665
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.2440197467803955,
        "learning_rate": 0.00019561255045371094,
        "epoch": 0.3670659507090734,
        "step": 2666
    },
    {
        "loss": 2.7434,
        "grad_norm": 1.3617827892303467,
        "learning_rate": 0.00019557432270758413,
        "epoch": 0.36720363486162744,
        "step": 2667
    },
    {
        "loss": 2.4798,
        "grad_norm": 1.605863332748413,
        "learning_rate": 0.00019553593291249106,
        "epoch": 0.3673413190141815,
        "step": 2668
    },
    {
        "loss": 2.0461,
        "grad_norm": 1.3781788349151611,
        "learning_rate": 0.0001954973811335227,
        "epoch": 0.3674790031667355,
        "step": 2669
    },
    {
        "loss": 1.9815,
        "grad_norm": 2.037348508834839,
        "learning_rate": 0.0001954586674360447,
        "epoch": 0.36761668731928954,
        "step": 2670
    },
    {
        "loss": 2.5134,
        "grad_norm": 1.15384840965271,
        "learning_rate": 0.00019541979188569716,
        "epoch": 0.3677543714718436,
        "step": 2671
    },
    {
        "loss": 2.5486,
        "grad_norm": 2.278257369995117,
        "learning_rate": 0.00019538075454839473,
        "epoch": 0.36789205562439764,
        "step": 2672
    },
    {
        "loss": 1.6085,
        "grad_norm": 2.8123269081115723,
        "learning_rate": 0.00019534155549032634,
        "epoch": 0.3680297397769517,
        "step": 2673
    },
    {
        "loss": 1.8548,
        "grad_norm": 1.1493034362792969,
        "learning_rate": 0.00019530219477795503,
        "epoch": 0.3681674239295057,
        "step": 2674
    },
    {
        "loss": 1.8527,
        "grad_norm": 2.5453360080718994,
        "learning_rate": 0.00019526267247801807,
        "epoch": 0.36830510808205974,
        "step": 2675
    },
    {
        "loss": 2.406,
        "grad_norm": 2.4002909660339355,
        "learning_rate": 0.0001952229886575266,
        "epoch": 0.3684427922346138,
        "step": 2676
    },
    {
        "loss": 2.1135,
        "grad_norm": 1.5227569341659546,
        "learning_rate": 0.00019518314338376572,
        "epoch": 0.36858047638716784,
        "step": 2677
    },
    {
        "loss": 1.8446,
        "grad_norm": 2.4188363552093506,
        "learning_rate": 0.00019514313672429413,
        "epoch": 0.3687181605397219,
        "step": 2678
    },
    {
        "loss": 1.6497,
        "grad_norm": 2.2623555660247803,
        "learning_rate": 0.0001951029687469443,
        "epoch": 0.3688558446922759,
        "step": 2679
    },
    {
        "loss": 2.3929,
        "grad_norm": 2.7392868995666504,
        "learning_rate": 0.00019506263951982216,
        "epoch": 0.36899352884482994,
        "step": 2680
    },
    {
        "loss": 2.4446,
        "grad_norm": 1.4210116863250732,
        "learning_rate": 0.00019502214911130706,
        "epoch": 0.369131212997384,
        "step": 2681
    },
    {
        "loss": 2.2006,
        "grad_norm": 1.6811943054199219,
        "learning_rate": 0.00019498149759005165,
        "epoch": 0.36926889714993805,
        "step": 2682
    },
    {
        "loss": 2.4152,
        "grad_norm": 1.2293105125427246,
        "learning_rate": 0.00019494068502498167,
        "epoch": 0.3694065813024921,
        "step": 2683
    },
    {
        "loss": 2.0069,
        "grad_norm": 1.859302043914795,
        "learning_rate": 0.000194899711485296,
        "epoch": 0.36954426545504615,
        "step": 2684
    },
    {
        "loss": 2.2832,
        "grad_norm": 1.0536715984344482,
        "learning_rate": 0.0001948585770404665,
        "epoch": 0.36968194960760015,
        "step": 2685
    },
    {
        "loss": 2.3743,
        "grad_norm": 1.4041377305984497,
        "learning_rate": 0.00019481728176023774,
        "epoch": 0.3698196337601542,
        "step": 2686
    },
    {
        "loss": 2.2285,
        "grad_norm": 2.4635047912597656,
        "learning_rate": 0.000194775825714627,
        "epoch": 0.36995731791270825,
        "step": 2687
    },
    {
        "loss": 2.5205,
        "grad_norm": 1.7483454942703247,
        "learning_rate": 0.00019473420897392427,
        "epoch": 0.3700950020652623,
        "step": 2688
    },
    {
        "loss": 2.1276,
        "grad_norm": 1.0177280902862549,
        "learning_rate": 0.00019469243160869185,
        "epoch": 0.37023268621781635,
        "step": 2689
    },
    {
        "loss": 2.3365,
        "grad_norm": 2.5670902729034424,
        "learning_rate": 0.0001946504936897644,
        "epoch": 0.37037037037037035,
        "step": 2690
    },
    {
        "loss": 2.0325,
        "grad_norm": 1.6005247831344604,
        "learning_rate": 0.00019460839528824891,
        "epoch": 0.3705080545229244,
        "step": 2691
    },
    {
        "loss": 1.8614,
        "grad_norm": 2.3667726516723633,
        "learning_rate": 0.0001945661364755244,
        "epoch": 0.37064573867547845,
        "step": 2692
    },
    {
        "loss": 1.3305,
        "grad_norm": 2.812013626098633,
        "learning_rate": 0.00019452371732324187,
        "epoch": 0.3707834228280325,
        "step": 2693
    },
    {
        "loss": 2.2502,
        "grad_norm": 1.4742196798324585,
        "learning_rate": 0.00019448113790332424,
        "epoch": 0.37092110698058656,
        "step": 2694
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.6093745231628418,
        "learning_rate": 0.00019443839828796601,
        "epoch": 0.37105879113314055,
        "step": 2695
    },
    {
        "loss": 2.3707,
        "grad_norm": 1.6748909950256348,
        "learning_rate": 0.0001943954985496335,
        "epoch": 0.3711964752856946,
        "step": 2696
    },
    {
        "loss": 2.4666,
        "grad_norm": 1.4069056510925293,
        "learning_rate": 0.00019435243876106444,
        "epoch": 0.37133415943824866,
        "step": 2697
    },
    {
        "loss": 1.7309,
        "grad_norm": 2.4361929893493652,
        "learning_rate": 0.00019430921899526784,
        "epoch": 0.3714718435908027,
        "step": 2698
    },
    {
        "loss": 1.9212,
        "grad_norm": 2.580183506011963,
        "learning_rate": 0.00019426583932552412,
        "epoch": 0.37160952774335676,
        "step": 2699
    },
    {
        "loss": 2.1024,
        "grad_norm": 1.5014253854751587,
        "learning_rate": 0.00019422229982538465,
        "epoch": 0.37174721189591076,
        "step": 2700
    },
    {
        "loss": 2.1649,
        "grad_norm": 2.195220708847046,
        "learning_rate": 0.00019417860056867195,
        "epoch": 0.3718848960484648,
        "step": 2701
    },
    {
        "loss": 2.0823,
        "grad_norm": 2.63143253326416,
        "learning_rate": 0.00019413474162947934,
        "epoch": 0.37202258020101886,
        "step": 2702
    },
    {
        "loss": 2.215,
        "grad_norm": 1.711522102355957,
        "learning_rate": 0.00019409072308217083,
        "epoch": 0.3721602643535729,
        "step": 2703
    },
    {
        "loss": 2.2835,
        "grad_norm": 2.0740346908569336,
        "learning_rate": 0.00019404654500138115,
        "epoch": 0.37229794850612696,
        "step": 2704
    },
    {
        "loss": 2.1844,
        "grad_norm": 1.7730698585510254,
        "learning_rate": 0.00019400220746201553,
        "epoch": 0.37243563265868096,
        "step": 2705
    },
    {
        "loss": 2.0033,
        "grad_norm": 2.2261300086975098,
        "learning_rate": 0.00019395771053924948,
        "epoch": 0.372573316811235,
        "step": 2706
    },
    {
        "loss": 2.4086,
        "grad_norm": 0.8878855109214783,
        "learning_rate": 0.00019391305430852877,
        "epoch": 0.37271100096378906,
        "step": 2707
    },
    {
        "loss": 2.5014,
        "grad_norm": 1.7158483266830444,
        "learning_rate": 0.00019386823884556936,
        "epoch": 0.3728486851163431,
        "step": 2708
    },
    {
        "loss": 2.0282,
        "grad_norm": 1.7688086032867432,
        "learning_rate": 0.00019382326422635705,
        "epoch": 0.37298636926889717,
        "step": 2709
    },
    {
        "loss": 2.124,
        "grad_norm": 1.7473325729370117,
        "learning_rate": 0.00019377813052714763,
        "epoch": 0.3731240534214512,
        "step": 2710
    },
    {
        "loss": 2.3886,
        "grad_norm": 1.976523756980896,
        "learning_rate": 0.0001937328378244666,
        "epoch": 0.3732617375740052,
        "step": 2711
    },
    {
        "loss": 2.0698,
        "grad_norm": 1.560839295387268,
        "learning_rate": 0.00019368738619510896,
        "epoch": 0.37339942172655927,
        "step": 2712
    },
    {
        "loss": 2.7669,
        "grad_norm": 1.3060847520828247,
        "learning_rate": 0.00019364177571613926,
        "epoch": 0.3735371058791133,
        "step": 2713
    },
    {
        "loss": 2.2846,
        "grad_norm": 1.2656044960021973,
        "learning_rate": 0.00019359600646489135,
        "epoch": 0.37367479003166737,
        "step": 2714
    },
    {
        "loss": 2.5668,
        "grad_norm": 1.3650310039520264,
        "learning_rate": 0.00019355007851896825,
        "epoch": 0.3738124741842214,
        "step": 2715
    },
    {
        "loss": 2.174,
        "grad_norm": 1.6421889066696167,
        "learning_rate": 0.00019350399195624217,
        "epoch": 0.3739501583367754,
        "step": 2716
    },
    {
        "loss": 2.3826,
        "grad_norm": 1.5548559427261353,
        "learning_rate": 0.00019345774685485413,
        "epoch": 0.37408784248932947,
        "step": 2717
    },
    {
        "loss": 2.1576,
        "grad_norm": 2.1225597858428955,
        "learning_rate": 0.000193411343293214,
        "epoch": 0.3742255266418835,
        "step": 2718
    },
    {
        "loss": 1.5589,
        "grad_norm": 2.2436208724975586,
        "learning_rate": 0.00019336478135000037,
        "epoch": 0.3743632107944376,
        "step": 2719
    },
    {
        "loss": 2.3789,
        "grad_norm": 1.4903366565704346,
        "learning_rate": 0.00019331806110416027,
        "epoch": 0.3745008949469916,
        "step": 2720
    },
    {
        "loss": 1.8902,
        "grad_norm": 1.3746204376220703,
        "learning_rate": 0.00019327118263490927,
        "epoch": 0.3746385790995456,
        "step": 2721
    },
    {
        "loss": 2.4901,
        "grad_norm": 2.210271120071411,
        "learning_rate": 0.00019322414602173103,
        "epoch": 0.3747762632520997,
        "step": 2722
    },
    {
        "loss": 2.4865,
        "grad_norm": 1.6780683994293213,
        "learning_rate": 0.00019317695134437763,
        "epoch": 0.3749139474046537,
        "step": 2723
    },
    {
        "loss": 2.6612,
        "grad_norm": 1.346566081047058,
        "learning_rate": 0.00019312959868286882,
        "epoch": 0.3750516315572078,
        "step": 2724
    },
    {
        "loss": 2.4244,
        "grad_norm": 1.8724361658096313,
        "learning_rate": 0.0001930820881174925,
        "epoch": 0.37518931570976183,
        "step": 2725
    },
    {
        "loss": 2.3396,
        "grad_norm": 1.649849772453308,
        "learning_rate": 0.00019303441972880406,
        "epoch": 0.3753269998623158,
        "step": 2726
    },
    {
        "loss": 1.9018,
        "grad_norm": 2.6841163635253906,
        "learning_rate": 0.00019298659359762667,
        "epoch": 0.3754646840148699,
        "step": 2727
    },
    {
        "loss": 2.4717,
        "grad_norm": 1.5582853555679321,
        "learning_rate": 0.00019293860980505086,
        "epoch": 0.37560236816742393,
        "step": 2728
    },
    {
        "loss": 1.971,
        "grad_norm": 2.4962191581726074,
        "learning_rate": 0.00019289046843243456,
        "epoch": 0.375740052319978,
        "step": 2729
    },
    {
        "loss": 2.1417,
        "grad_norm": 2.4127087593078613,
        "learning_rate": 0.00019284216956140274,
        "epoch": 0.37587773647253203,
        "step": 2730
    },
    {
        "loss": 2.3913,
        "grad_norm": 1.8549610376358032,
        "learning_rate": 0.0001927937132738476,
        "epoch": 0.376015420625086,
        "step": 2731
    },
    {
        "loss": 1.0541,
        "grad_norm": Infinity,
        "learning_rate": 0.0001927937132738476,
        "epoch": 0.3761531047776401,
        "step": 2732
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.9271571636199951,
        "learning_rate": 0.00019274509965192804,
        "epoch": 0.37629078893019413,
        "step": 2733
    },
    {
        "loss": 2.1626,
        "grad_norm": 1.339035153388977,
        "learning_rate": 0.0001926963287780699,
        "epoch": 0.3764284730827482,
        "step": 2734
    },
    {
        "loss": 2.1728,
        "grad_norm": 1.3808563947677612,
        "learning_rate": 0.00019264740073496558,
        "epoch": 0.37656615723530223,
        "step": 2735
    },
    {
        "loss": 1.7828,
        "grad_norm": 2.2863235473632812,
        "learning_rate": 0.00019259831560557394,
        "epoch": 0.37670384138785623,
        "step": 2736
    },
    {
        "loss": 1.753,
        "grad_norm": 2.9174530506134033,
        "learning_rate": 0.0001925490734731202,
        "epoch": 0.3768415255404103,
        "step": 2737
    },
    {
        "loss": 2.2837,
        "grad_norm": 1.0344109535217285,
        "learning_rate": 0.0001924996744210958,
        "epoch": 0.37697920969296433,
        "step": 2738
    },
    {
        "loss": 1.9184,
        "grad_norm": 2.927644729614258,
        "learning_rate": 0.00019245011853325823,
        "epoch": 0.3771168938455184,
        "step": 2739
    },
    {
        "loss": 2.1249,
        "grad_norm": 1.9200241565704346,
        "learning_rate": 0.00019240040589363087,
        "epoch": 0.37725457799807244,
        "step": 2740
    },
    {
        "loss": 1.8696,
        "grad_norm": 1.762632131576538,
        "learning_rate": 0.00019235053658650295,
        "epoch": 0.3773922621506265,
        "step": 2741
    },
    {
        "loss": 1.9729,
        "grad_norm": 2.266967535018921,
        "learning_rate": 0.00019230051069642925,
        "epoch": 0.3775299463031805,
        "step": 2742
    },
    {
        "loss": 2.5046,
        "grad_norm": 1.55841863155365,
        "learning_rate": 0.00019225032830823011,
        "epoch": 0.37766763045573454,
        "step": 2743
    },
    {
        "loss": 2.2138,
        "grad_norm": 1.2428878545761108,
        "learning_rate": 0.0001921999895069912,
        "epoch": 0.3778053146082886,
        "step": 2744
    },
    {
        "loss": 2.5063,
        "grad_norm": 1.7315428256988525,
        "learning_rate": 0.0001921494943780633,
        "epoch": 0.37794299876084264,
        "step": 2745
    },
    {
        "loss": 1.9914,
        "grad_norm": 2.15175724029541,
        "learning_rate": 0.00019209884300706243,
        "epoch": 0.3780806829133967,
        "step": 2746
    },
    {
        "loss": 1.9012,
        "grad_norm": 1.6521656513214111,
        "learning_rate": 0.00019204803547986937,
        "epoch": 0.3782183670659507,
        "step": 2747
    },
    {
        "loss": 1.838,
        "grad_norm": 2.90850830078125,
        "learning_rate": 0.0001919970718826297,
        "epoch": 0.37835605121850474,
        "step": 2748
    },
    {
        "loss": 1.7534,
        "grad_norm": 2.0457065105438232,
        "learning_rate": 0.00019194595230175368,
        "epoch": 0.3784937353710588,
        "step": 2749
    },
    {
        "loss": 2.2997,
        "grad_norm": 1.6378955841064453,
        "learning_rate": 0.00019189467682391605,
        "epoch": 0.37863141952361284,
        "step": 2750
    },
    {
        "loss": 2.301,
        "grad_norm": 1.6019867658615112,
        "learning_rate": 0.0001918432455360557,
        "epoch": 0.3787691036761669,
        "step": 2751
    },
    {
        "loss": 2.285,
        "grad_norm": 2.0093994140625,
        "learning_rate": 0.00019179165852537593,
        "epoch": 0.3789067878287209,
        "step": 2752
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.4950284957885742,
        "learning_rate": 0.00019173991587934398,
        "epoch": 0.37904447198127494,
        "step": 2753
    },
    {
        "loss": 1.4451,
        "grad_norm": 2.25217342376709,
        "learning_rate": 0.0001916880176856909,
        "epoch": 0.379182156133829,
        "step": 2754
    },
    {
        "loss": 1.9452,
        "grad_norm": 1.9821405410766602,
        "learning_rate": 0.00019163596403241163,
        "epoch": 0.37931984028638305,
        "step": 2755
    },
    {
        "loss": 2.5653,
        "grad_norm": 2.523972511291504,
        "learning_rate": 0.00019158375500776456,
        "epoch": 0.3794575244389371,
        "step": 2756
    },
    {
        "loss": 2.2406,
        "grad_norm": 3.7898151874542236,
        "learning_rate": 0.00019153139070027153,
        "epoch": 0.3795952085914911,
        "step": 2757
    },
    {
        "loss": 1.3465,
        "grad_norm": 2.9544970989227295,
        "learning_rate": 0.00019147887119871774,
        "epoch": 0.37973289274404515,
        "step": 2758
    },
    {
        "loss": 2.4266,
        "grad_norm": 1.5106618404388428,
        "learning_rate": 0.00019142619659215154,
        "epoch": 0.3798705768965992,
        "step": 2759
    },
    {
        "loss": 2.3087,
        "grad_norm": 2.168745756149292,
        "learning_rate": 0.00019137336696988417,
        "epoch": 0.38000826104915325,
        "step": 2760
    },
    {
        "loss": 2.6591,
        "grad_norm": 1.279038667678833,
        "learning_rate": 0.00019132038242148974,
        "epoch": 0.3801459452017073,
        "step": 2761
    },
    {
        "loss": 2.0342,
        "grad_norm": 1.599103331565857,
        "learning_rate": 0.00019126724303680507,
        "epoch": 0.3802836293542613,
        "step": 2762
    },
    {
        "loss": 1.7016,
        "grad_norm": 2.1170690059661865,
        "learning_rate": 0.00019121394890592947,
        "epoch": 0.38042131350681535,
        "step": 2763
    },
    {
        "loss": 1.7169,
        "grad_norm": 2.5514755249023438,
        "learning_rate": 0.00019116050011922466,
        "epoch": 0.3805589976593694,
        "step": 2764
    },
    {
        "loss": 2.5179,
        "grad_norm": 1.790382981300354,
        "learning_rate": 0.00019110689676731456,
        "epoch": 0.38069668181192345,
        "step": 2765
    },
    {
        "loss": 1.8144,
        "grad_norm": 2.785046100616455,
        "learning_rate": 0.00019105313894108517,
        "epoch": 0.3808343659644775,
        "step": 2766
    },
    {
        "loss": 2.5609,
        "grad_norm": 1.501214861869812,
        "learning_rate": 0.00019099922673168442,
        "epoch": 0.3809720501170315,
        "step": 2767
    },
    {
        "loss": 1.3172,
        "grad_norm": 3.2309374809265137,
        "learning_rate": 0.00019094516023052197,
        "epoch": 0.38110973426958555,
        "step": 2768
    },
    {
        "loss": 2.1887,
        "grad_norm": 1.6455799341201782,
        "learning_rate": 0.0001908909395292691,
        "epoch": 0.3812474184221396,
        "step": 2769
    },
    {
        "loss": 1.8482,
        "grad_norm": 1.35589599609375,
        "learning_rate": 0.00019083656471985852,
        "epoch": 0.38138510257469366,
        "step": 2770
    },
    {
        "loss": 1.1043,
        "grad_norm": 2.8468737602233887,
        "learning_rate": 0.00019078203589448432,
        "epoch": 0.3815227867272477,
        "step": 2771
    },
    {
        "loss": 2.1925,
        "grad_norm": 1.7297568321228027,
        "learning_rate": 0.00019072735314560157,
        "epoch": 0.38166047087980176,
        "step": 2772
    },
    {
        "loss": 1.5605,
        "grad_norm": 1.7896013259887695,
        "learning_rate": 0.00019067251656592653,
        "epoch": 0.38179815503235576,
        "step": 2773
    },
    {
        "loss": 1.7615,
        "grad_norm": 2.2523176670074463,
        "learning_rate": 0.00019061752624843613,
        "epoch": 0.3819358391849098,
        "step": 2774
    },
    {
        "loss": 2.5222,
        "grad_norm": 1.3590558767318726,
        "learning_rate": 0.00019056238228636792,
        "epoch": 0.38207352333746386,
        "step": 2775
    },
    {
        "loss": 2.1249,
        "grad_norm": 2.1286966800689697,
        "learning_rate": 0.00019050708477322018,
        "epoch": 0.3822112074900179,
        "step": 2776
    },
    {
        "loss": 2.4221,
        "grad_norm": 2.0283515453338623,
        "learning_rate": 0.00019045163380275134,
        "epoch": 0.38234889164257196,
        "step": 2777
    },
    {
        "loss": 2.0937,
        "grad_norm": 2.3303134441375732,
        "learning_rate": 0.00019039602946898008,
        "epoch": 0.38248657579512596,
        "step": 2778
    },
    {
        "loss": 2.1546,
        "grad_norm": 2.0201632976531982,
        "learning_rate": 0.00019034027186618516,
        "epoch": 0.38262425994768,
        "step": 2779
    },
    {
        "loss": 2.2027,
        "grad_norm": 1.4705712795257568,
        "learning_rate": 0.00019028436108890512,
        "epoch": 0.38276194410023406,
        "step": 2780
    },
    {
        "loss": 1.6653,
        "grad_norm": 2.2889015674591064,
        "learning_rate": 0.0001902282972319383,
        "epoch": 0.3828996282527881,
        "step": 2781
    },
    {
        "loss": 1.9477,
        "grad_norm": 2.3918352127075195,
        "learning_rate": 0.0001901720803903425,
        "epoch": 0.38303731240534217,
        "step": 2782
    },
    {
        "loss": 2.0054,
        "grad_norm": 2.2571959495544434,
        "learning_rate": 0.00019011571065943509,
        "epoch": 0.38317499655789616,
        "step": 2783
    },
    {
        "loss": 2.1025,
        "grad_norm": 2.0811469554901123,
        "learning_rate": 0.0001900591881347924,
        "epoch": 0.3833126807104502,
        "step": 2784
    },
    {
        "loss": 1.9659,
        "grad_norm": 3.43607759475708,
        "learning_rate": 0.00019000251291225007,
        "epoch": 0.38345036486300427,
        "step": 2785
    },
    {
        "loss": 2.0846,
        "grad_norm": 1.7807749509811401,
        "learning_rate": 0.00018994568508790246,
        "epoch": 0.3835880490155583,
        "step": 2786
    },
    {
        "loss": 2.0471,
        "grad_norm": 2.7529659271240234,
        "learning_rate": 0.00018988870475810282,
        "epoch": 0.38372573316811237,
        "step": 2787
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.170318841934204,
        "learning_rate": 0.00018983157201946292,
        "epoch": 0.38386341732066637,
        "step": 2788
    },
    {
        "loss": 2.2275,
        "grad_norm": 1.9453939199447632,
        "learning_rate": 0.00018977428696885284,
        "epoch": 0.3840011014732204,
        "step": 2789
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.916001319885254,
        "learning_rate": 0.0001897168497034011,
        "epoch": 0.38413878562577447,
        "step": 2790
    },
    {
        "loss": 2.3918,
        "grad_norm": 2.7313072681427,
        "learning_rate": 0.00018965926032049417,
        "epoch": 0.3842764697783285,
        "step": 2791
    },
    {
        "loss": 2.398,
        "grad_norm": 1.3678674697875977,
        "learning_rate": 0.0001896015189177765,
        "epoch": 0.3844141539308826,
        "step": 2792
    },
    {
        "loss": 2.0203,
        "grad_norm": 2.252011299133301,
        "learning_rate": 0.00018954362559315014,
        "epoch": 0.38455183808343657,
        "step": 2793
    },
    {
        "loss": 2.3167,
        "grad_norm": 1.8172231912612915,
        "learning_rate": 0.0001894855804447751,
        "epoch": 0.3846895222359906,
        "step": 2794
    },
    {
        "loss": 2.4414,
        "grad_norm": 1.3823617696762085,
        "learning_rate": 0.00018942738357106834,
        "epoch": 0.3848272063885447,
        "step": 2795
    },
    {
        "loss": 2.0677,
        "grad_norm": 2.550706148147583,
        "learning_rate": 0.00018936903507070442,
        "epoch": 0.3849648905410987,
        "step": 2796
    },
    {
        "loss": 2.8115,
        "grad_norm": 1.7648929357528687,
        "learning_rate": 0.00018931053504261488,
        "epoch": 0.3851025746936528,
        "step": 2797
    },
    {
        "loss": 2.1899,
        "grad_norm": 2.0229909420013428,
        "learning_rate": 0.00018925188358598813,
        "epoch": 0.38524025884620683,
        "step": 2798
    },
    {
        "loss": 2.2427,
        "grad_norm": 1.1000796556472778,
        "learning_rate": 0.0001891930808002694,
        "epoch": 0.3853779429987608,
        "step": 2799
    },
    {
        "loss": 1.9594,
        "grad_norm": 1.7529120445251465,
        "learning_rate": 0.00018913412678516048,
        "epoch": 0.3855156271513149,
        "step": 2800
    },
    {
        "loss": 1.7547,
        "grad_norm": 1.8144406080245972,
        "learning_rate": 0.0001890750216406195,
        "epoch": 0.38565331130386893,
        "step": 2801
    },
    {
        "loss": 1.602,
        "grad_norm": 2.2763965129852295,
        "learning_rate": 0.00018901576546686096,
        "epoch": 0.385790995456423,
        "step": 2802
    },
    {
        "loss": 2.047,
        "grad_norm": 1.6622036695480347,
        "learning_rate": 0.00018895635836435541,
        "epoch": 0.38592867960897703,
        "step": 2803
    },
    {
        "loss": 2.4864,
        "grad_norm": 1.0995092391967773,
        "learning_rate": 0.00018889680043382922,
        "epoch": 0.38606636376153103,
        "step": 2804
    },
    {
        "loss": 1.8583,
        "grad_norm": 2.4729437828063965,
        "learning_rate": 0.00018883709177626456,
        "epoch": 0.3862040479140851,
        "step": 2805
    },
    {
        "loss": 1.8428,
        "grad_norm": 2.1005654335021973,
        "learning_rate": 0.00018877723249289917,
        "epoch": 0.38634173206663913,
        "step": 2806
    },
    {
        "loss": 2.2623,
        "grad_norm": 2.425879716873169,
        "learning_rate": 0.00018871722268522608,
        "epoch": 0.3864794162191932,
        "step": 2807
    },
    {
        "loss": 2.4801,
        "grad_norm": 1.645456075668335,
        "learning_rate": 0.0001886570624549937,
        "epoch": 0.38661710037174724,
        "step": 2808
    },
    {
        "loss": 0.9821,
        "grad_norm": 1.945093035697937,
        "learning_rate": 0.00018859675190420537,
        "epoch": 0.38675478452430123,
        "step": 2809
    },
    {
        "loss": 2.4091,
        "grad_norm": 2.051032066345215,
        "learning_rate": 0.00018853629113511935,
        "epoch": 0.3868924686768553,
        "step": 2810
    },
    {
        "loss": 1.6684,
        "grad_norm": 2.385422706604004,
        "learning_rate": 0.0001884756802502486,
        "epoch": 0.38703015282940934,
        "step": 2811
    },
    {
        "loss": 2.4267,
        "grad_norm": 2.0182437896728516,
        "learning_rate": 0.00018841491935236052,
        "epoch": 0.3871678369819634,
        "step": 2812
    },
    {
        "loss": 1.7689,
        "grad_norm": 2.725224494934082,
        "learning_rate": 0.00018835400854447695,
        "epoch": 0.38730552113451744,
        "step": 2813
    },
    {
        "loss": 1.8165,
        "grad_norm": 1.539879322052002,
        "learning_rate": 0.00018829294792987392,
        "epoch": 0.38744320528707143,
        "step": 2814
    },
    {
        "loss": 2.4129,
        "grad_norm": 1.2567583322525024,
        "learning_rate": 0.00018823173761208143,
        "epoch": 0.3875808894396255,
        "step": 2815
    },
    {
        "loss": 2.4308,
        "grad_norm": 2.045278787612915,
        "learning_rate": 0.00018817037769488327,
        "epoch": 0.38771857359217954,
        "step": 2816
    },
    {
        "loss": 2.2365,
        "grad_norm": 2.152359962463379,
        "learning_rate": 0.00018810886828231696,
        "epoch": 0.3878562577447336,
        "step": 2817
    },
    {
        "loss": 1.9201,
        "grad_norm": 1.7671462297439575,
        "learning_rate": 0.00018804720947867342,
        "epoch": 0.38799394189728764,
        "step": 2818
    },
    {
        "loss": 2.4445,
        "grad_norm": 2.1610498428344727,
        "learning_rate": 0.00018798540138849685,
        "epoch": 0.38813162604984164,
        "step": 2819
    },
    {
        "loss": 2.2431,
        "grad_norm": 1.9282305240631104,
        "learning_rate": 0.00018792344411658468,
        "epoch": 0.3882693102023957,
        "step": 2820
    },
    {
        "loss": 2.1488,
        "grad_norm": 2.2905027866363525,
        "learning_rate": 0.0001878613377679872,
        "epoch": 0.38840699435494974,
        "step": 2821
    },
    {
        "loss": 2.3155,
        "grad_norm": 1.5414198637008667,
        "learning_rate": 0.00018779908244800746,
        "epoch": 0.3885446785075038,
        "step": 2822
    },
    {
        "loss": 2.6929,
        "grad_norm": 1.4595253467559814,
        "learning_rate": 0.00018773667826220118,
        "epoch": 0.38868236266005785,
        "step": 2823
    },
    {
        "loss": 1.866,
        "grad_norm": 2.8799514770507812,
        "learning_rate": 0.00018767412531637637,
        "epoch": 0.38882004681261184,
        "step": 2824
    },
    {
        "loss": 2.174,
        "grad_norm": 1.6741753816604614,
        "learning_rate": 0.00018761142371659327,
        "epoch": 0.3889577309651659,
        "step": 2825
    },
    {
        "loss": 1.7547,
        "grad_norm": 2.514674425125122,
        "learning_rate": 0.0001875485735691643,
        "epoch": 0.38909541511771994,
        "step": 2826
    },
    {
        "loss": 2.4172,
        "grad_norm": 1.3352863788604736,
        "learning_rate": 0.0001874855749806536,
        "epoch": 0.389233099270274,
        "step": 2827
    },
    {
        "loss": 2.37,
        "grad_norm": 2.55827260017395,
        "learning_rate": 0.00018742242805787712,
        "epoch": 0.38937078342282805,
        "step": 2828
    },
    {
        "loss": 2.4328,
        "grad_norm": 1.546876072883606,
        "learning_rate": 0.00018735913290790222,
        "epoch": 0.3895084675753821,
        "step": 2829
    },
    {
        "loss": 2.2331,
        "grad_norm": 2.267512798309326,
        "learning_rate": 0.00018729568963804757,
        "epoch": 0.3896461517279361,
        "step": 2830
    },
    {
        "loss": 2.6755,
        "grad_norm": 1.4990992546081543,
        "learning_rate": 0.0001872320983558831,
        "epoch": 0.38978383588049015,
        "step": 2831
    },
    {
        "loss": 1.4054,
        "grad_norm": 2.5043604373931885,
        "learning_rate": 0.0001871683591692296,
        "epoch": 0.3899215200330442,
        "step": 2832
    },
    {
        "loss": 1.6946,
        "grad_norm": 2.2812278270721436,
        "learning_rate": 0.00018710447218615867,
        "epoch": 0.39005920418559825,
        "step": 2833
    },
    {
        "loss": 2.1391,
        "grad_norm": 1.218851923942566,
        "learning_rate": 0.00018704043751499244,
        "epoch": 0.3901968883381523,
        "step": 2834
    },
    {
        "loss": 2.0809,
        "grad_norm": 1.916356086730957,
        "learning_rate": 0.0001869762552643036,
        "epoch": 0.3903345724907063,
        "step": 2835
    },
    {
        "loss": 1.9516,
        "grad_norm": 1.5982511043548584,
        "learning_rate": 0.00018691192554291493,
        "epoch": 0.39047225664326035,
        "step": 2836
    },
    {
        "loss": 2.0072,
        "grad_norm": 2.2299587726593018,
        "learning_rate": 0.00018684744845989926,
        "epoch": 0.3906099407958144,
        "step": 2837
    },
    {
        "loss": 1.7654,
        "grad_norm": 2.5536999702453613,
        "learning_rate": 0.00018678282412457937,
        "epoch": 0.39074762494836845,
        "step": 2838
    },
    {
        "loss": 1.7785,
        "grad_norm": 1.9362486600875854,
        "learning_rate": 0.00018671805264652757,
        "epoch": 0.3908853091009225,
        "step": 2839
    },
    {
        "loss": 1.9673,
        "grad_norm": 2.2450568675994873,
        "learning_rate": 0.00018665313413556583,
        "epoch": 0.3910229932534765,
        "step": 2840
    },
    {
        "loss": 0.9698,
        "grad_norm": 2.813981056213379,
        "learning_rate": 0.0001865880687017652,
        "epoch": 0.39116067740603055,
        "step": 2841
    },
    {
        "loss": 2.2464,
        "grad_norm": 2.0017812252044678,
        "learning_rate": 0.00018652285645544606,
        "epoch": 0.3912983615585846,
        "step": 2842
    },
    {
        "loss": 2.5322,
        "grad_norm": 1.4519810676574707,
        "learning_rate": 0.00018645749750717755,
        "epoch": 0.39143604571113866,
        "step": 2843
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.5510886907577515,
        "learning_rate": 0.0001863919919677777,
        "epoch": 0.3915737298636927,
        "step": 2844
    },
    {
        "loss": 2.3477,
        "grad_norm": 2.1788218021392822,
        "learning_rate": 0.00018632633994831289,
        "epoch": 0.3917114140162467,
        "step": 2845
    },
    {
        "loss": 2.1878,
        "grad_norm": 1.950628399848938,
        "learning_rate": 0.00018626054156009806,
        "epoch": 0.39184909816880076,
        "step": 2846
    },
    {
        "loss": 2.4884,
        "grad_norm": 2.4879937171936035,
        "learning_rate": 0.00018619459691469623,
        "epoch": 0.3919867823213548,
        "step": 2847
    },
    {
        "loss": 1.6008,
        "grad_norm": 1.7638092041015625,
        "learning_rate": 0.00018612850612391841,
        "epoch": 0.39212446647390886,
        "step": 2848
    },
    {
        "loss": 2.1214,
        "grad_norm": 2.4496517181396484,
        "learning_rate": 0.00018606226929982335,
        "epoch": 0.3922621506264629,
        "step": 2849
    },
    {
        "loss": 2.0284,
        "grad_norm": 1.2243977785110474,
        "learning_rate": 0.00018599588655471755,
        "epoch": 0.3923998347790169,
        "step": 2850
    },
    {
        "loss": 1.9088,
        "grad_norm": 1.2583421468734741,
        "learning_rate": 0.00018592935800115475,
        "epoch": 0.39253751893157096,
        "step": 2851
    },
    {
        "loss": 2.1709,
        "grad_norm": 2.257782459259033,
        "learning_rate": 0.00018586268375193603,
        "epoch": 0.392675203084125,
        "step": 2852
    },
    {
        "loss": 1.802,
        "grad_norm": 1.6777667999267578,
        "learning_rate": 0.00018579586392010946,
        "epoch": 0.39281288723667906,
        "step": 2853
    },
    {
        "loss": 1.3343,
        "grad_norm": 3.228395938873291,
        "learning_rate": 0.00018572889861896992,
        "epoch": 0.3929505713892331,
        "step": 2854
    },
    {
        "loss": 2.1771,
        "grad_norm": 1.5241315364837646,
        "learning_rate": 0.000185661787962059,
        "epoch": 0.3930882555417871,
        "step": 2855
    },
    {
        "loss": 2.3295,
        "grad_norm": 1.4791218042373657,
        "learning_rate": 0.00018559453206316477,
        "epoch": 0.39322593969434116,
        "step": 2856
    },
    {
        "loss": 1.299,
        "grad_norm": 3.137730598449707,
        "learning_rate": 0.0001855271310363214,
        "epoch": 0.3933636238468952,
        "step": 2857
    },
    {
        "loss": 2.5153,
        "grad_norm": 1.9716137647628784,
        "learning_rate": 0.00018545958499580932,
        "epoch": 0.39350130799944927,
        "step": 2858
    },
    {
        "loss": 0.9835,
        "grad_norm": 3.5335915088653564,
        "learning_rate": 0.0001853918940561547,
        "epoch": 0.3936389921520033,
        "step": 2859
    },
    {
        "loss": 2.5196,
        "grad_norm": 2.042426109313965,
        "learning_rate": 0.00018532405833212955,
        "epoch": 0.39377667630455737,
        "step": 2860
    },
    {
        "loss": 2.139,
        "grad_norm": 2.020113468170166,
        "learning_rate": 0.0001852560779387511,
        "epoch": 0.39391436045711137,
        "step": 2861
    },
    {
        "loss": 2.3573,
        "grad_norm": 1.4831225872039795,
        "learning_rate": 0.00018518795299128212,
        "epoch": 0.3940520446096654,
        "step": 2862
    },
    {
        "loss": 2.0957,
        "grad_norm": 2.0293080806732178,
        "learning_rate": 0.00018511968360523037,
        "epoch": 0.39418972876221947,
        "step": 2863
    },
    {
        "loss": 2.3022,
        "grad_norm": 1.1839677095413208,
        "learning_rate": 0.00018505126989634853,
        "epoch": 0.3943274129147735,
        "step": 2864
    },
    {
        "loss": 1.9144,
        "grad_norm": 1.5738856792449951,
        "learning_rate": 0.00018498271198063397,
        "epoch": 0.3944650970673276,
        "step": 2865
    },
    {
        "loss": 2.3826,
        "grad_norm": 1.6332508325576782,
        "learning_rate": 0.00018491400997432854,
        "epoch": 0.39460278121988157,
        "step": 2866
    },
    {
        "loss": 1.3642,
        "grad_norm": 3.3017756938934326,
        "learning_rate": 0.0001848451639939185,
        "epoch": 0.3947404653724356,
        "step": 2867
    },
    {
        "loss": 1.1105,
        "grad_norm": 2.159834623336792,
        "learning_rate": 0.00018477617415613414,
        "epoch": 0.3948781495249897,
        "step": 2868
    },
    {
        "loss": 2.0637,
        "grad_norm": 2.0282347202301025,
        "learning_rate": 0.0001847070405779496,
        "epoch": 0.3950158336775437,
        "step": 2869
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.2656084299087524,
        "learning_rate": 0.00018463776337658295,
        "epoch": 0.3951535178300978,
        "step": 2870
    },
    {
        "loss": 1.2793,
        "grad_norm": 2.542349100112915,
        "learning_rate": 0.00018456834266949553,
        "epoch": 0.3952912019826518,
        "step": 2871
    },
    {
        "loss": 2.6213,
        "grad_norm": 1.3319615125656128,
        "learning_rate": 0.00018449877857439222,
        "epoch": 0.3954288861352058,
        "step": 2872
    },
    {
        "loss": 2.2495,
        "grad_norm": 1.0261399745941162,
        "learning_rate": 0.0001844290712092208,
        "epoch": 0.3955665702877599,
        "step": 2873
    },
    {
        "loss": 1.4471,
        "grad_norm": 2.72344970703125,
        "learning_rate": 0.00018435922069217213,
        "epoch": 0.39570425444031393,
        "step": 2874
    },
    {
        "loss": 1.9757,
        "grad_norm": 1.54304039478302,
        "learning_rate": 0.0001842892271416797,
        "epoch": 0.395841938592868,
        "step": 2875
    },
    {
        "loss": 2.0378,
        "grad_norm": 2.36974835395813,
        "learning_rate": 0.0001842190906764196,
        "epoch": 0.395979622745422,
        "step": 2876
    },
    {
        "loss": 1.704,
        "grad_norm": 2.5560059547424316,
        "learning_rate": 0.00018414881141531024,
        "epoch": 0.39611730689797603,
        "step": 2877
    },
    {
        "loss": 2.3166,
        "grad_norm": 2.3357341289520264,
        "learning_rate": 0.00018407838947751196,
        "epoch": 0.3962549910505301,
        "step": 2878
    },
    {
        "loss": 2.5155,
        "grad_norm": 1.9450418949127197,
        "learning_rate": 0.00018400782498242726,
        "epoch": 0.39639267520308413,
        "step": 2879
    },
    {
        "loss": 2.0594,
        "grad_norm": 1.7504862546920776,
        "learning_rate": 0.00018393711804970015,
        "epoch": 0.3965303593556382,
        "step": 2880
    },
    {
        "loss": 2.3567,
        "grad_norm": 1.715173602104187,
        "learning_rate": 0.00018386626879921626,
        "epoch": 0.3966680435081922,
        "step": 2881
    },
    {
        "loss": 1.8856,
        "grad_norm": 2.489328145980835,
        "learning_rate": 0.00018379527735110259,
        "epoch": 0.39680572766074623,
        "step": 2882
    },
    {
        "loss": 2.1297,
        "grad_norm": 1.7220661640167236,
        "learning_rate": 0.000183724143825727,
        "epoch": 0.3969434118133003,
        "step": 2883
    },
    {
        "loss": 2.1411,
        "grad_norm": 1.2518500089645386,
        "learning_rate": 0.00018365286834369854,
        "epoch": 0.39708109596585434,
        "step": 2884
    },
    {
        "loss": 2.2942,
        "grad_norm": 1.9145063161849976,
        "learning_rate": 0.00018358145102586665,
        "epoch": 0.3972187801184084,
        "step": 2885
    },
    {
        "loss": 2.0425,
        "grad_norm": 2.079200267791748,
        "learning_rate": 0.00018350989199332154,
        "epoch": 0.39735646427096244,
        "step": 2886
    },
    {
        "loss": 2.165,
        "grad_norm": 2.0137100219726562,
        "learning_rate": 0.0001834381913673935,
        "epoch": 0.39749414842351644,
        "step": 2887
    },
    {
        "loss": 1.3649,
        "grad_norm": 1.7649178504943848,
        "learning_rate": 0.000183366349269653,
        "epoch": 0.3976318325760705,
        "step": 2888
    },
    {
        "loss": 2.018,
        "grad_norm": 2.051575183868408,
        "learning_rate": 0.00018329436582191036,
        "epoch": 0.39776951672862454,
        "step": 2889
    },
    {
        "loss": 1.9777,
        "grad_norm": 1.6276428699493408,
        "learning_rate": 0.00018322224114621547,
        "epoch": 0.3979072008811786,
        "step": 2890
    },
    {
        "loss": 2.0645,
        "grad_norm": 1.525657296180725,
        "learning_rate": 0.00018314997536485788,
        "epoch": 0.39804488503373264,
        "step": 2891
    },
    {
        "loss": 1.8761,
        "grad_norm": 2.414297342300415,
        "learning_rate": 0.00018307756860036617,
        "epoch": 0.39818256918628664,
        "step": 2892
    },
    {
        "loss": 1.6862,
        "grad_norm": 1.7776650190353394,
        "learning_rate": 0.00018300502097550803,
        "epoch": 0.3983202533388407,
        "step": 2893
    },
    {
        "loss": 1.6821,
        "grad_norm": 1.5073039531707764,
        "learning_rate": 0.00018293233261329017,
        "epoch": 0.39845793749139474,
        "step": 2894
    },
    {
        "loss": 1.6327,
        "grad_norm": 2.8739943504333496,
        "learning_rate": 0.0001828595036369576,
        "epoch": 0.3985956216439488,
        "step": 2895
    },
    {
        "loss": 2.299,
        "grad_norm": 1.2096920013427734,
        "learning_rate": 0.000182786534169994,
        "epoch": 0.39873330579650285,
        "step": 2896
    },
    {
        "loss": 2.2276,
        "grad_norm": 2.2518739700317383,
        "learning_rate": 0.00018271342433612118,
        "epoch": 0.39887098994905684,
        "step": 2897
    },
    {
        "loss": 2.3781,
        "grad_norm": 1.946921944618225,
        "learning_rate": 0.00018264017425929878,
        "epoch": 0.3990086741016109,
        "step": 2898
    },
    {
        "loss": 2.325,
        "grad_norm": 1.6457090377807617,
        "learning_rate": 0.0001825667840637245,
        "epoch": 0.39914635825416495,
        "step": 2899
    },
    {
        "loss": 2.418,
        "grad_norm": 1.2564553022384644,
        "learning_rate": 0.00018249325387383354,
        "epoch": 0.399284042406719,
        "step": 2900
    },
    {
        "loss": 2.7039,
        "grad_norm": 1.4307994842529297,
        "learning_rate": 0.00018241958381429833,
        "epoch": 0.39942172655927305,
        "step": 2901
    },
    {
        "loss": 1.7703,
        "grad_norm": 2.098257541656494,
        "learning_rate": 0.0001823457740100285,
        "epoch": 0.39955941071182705,
        "step": 2902
    },
    {
        "loss": 2.6492,
        "grad_norm": 1.8546366691589355,
        "learning_rate": 0.0001822718245861708,
        "epoch": 0.3996970948643811,
        "step": 2903
    },
    {
        "loss": 2.869,
        "grad_norm": 1.1858996152877808,
        "learning_rate": 0.0001821977356681084,
        "epoch": 0.39983477901693515,
        "step": 2904
    },
    {
        "loss": 2.0018,
        "grad_norm": 1.7181766033172607,
        "learning_rate": 0.00018212350738146127,
        "epoch": 0.3999724631694892,
        "step": 2905
    },
    {
        "loss": 2.5261,
        "grad_norm": 2.0296988487243652,
        "learning_rate": 0.00018204913985208555,
        "epoch": 0.40011014732204325,
        "step": 2906
    },
    {
        "loss": 1.5135,
        "grad_norm": 1.391628384590149,
        "learning_rate": 0.00018197463320607346,
        "epoch": 0.40024783147459725,
        "step": 2907
    },
    {
        "loss": 1.8377,
        "grad_norm": 2.6602625846862793,
        "learning_rate": 0.00018189998756975318,
        "epoch": 0.4003855156271513,
        "step": 2908
    },
    {
        "loss": 2.0611,
        "grad_norm": 1.3760021924972534,
        "learning_rate": 0.00018182520306968844,
        "epoch": 0.40052319977970535,
        "step": 2909
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.9924018383026123,
        "learning_rate": 0.00018175027983267849,
        "epoch": 0.4006608839322594,
        "step": 2910
    },
    {
        "loss": 2.2338,
        "grad_norm": 1.8587274551391602,
        "learning_rate": 0.00018167521798575782,
        "epoch": 0.40079856808481346,
        "step": 2911
    },
    {
        "loss": 1.9602,
        "grad_norm": 2.9351046085357666,
        "learning_rate": 0.00018160001765619586,
        "epoch": 0.40093625223736745,
        "step": 2912
    },
    {
        "loss": 2.1782,
        "grad_norm": 1.0734437704086304,
        "learning_rate": 0.00018152467897149696,
        "epoch": 0.4010739363899215,
        "step": 2913
    },
    {
        "loss": 2.2484,
        "grad_norm": 1.871821403503418,
        "learning_rate": 0.00018144920205939995,
        "epoch": 0.40121162054247556,
        "step": 2914
    },
    {
        "loss": 2.4158,
        "grad_norm": 2.364504814147949,
        "learning_rate": 0.00018137358704787805,
        "epoch": 0.4013493046950296,
        "step": 2915
    },
    {
        "loss": 2.3957,
        "grad_norm": 1.5091804265975952,
        "learning_rate": 0.00018129783406513867,
        "epoch": 0.40148698884758366,
        "step": 2916
    },
    {
        "loss": 1.8451,
        "grad_norm": 1.9308524131774902,
        "learning_rate": 0.0001812219432396232,
        "epoch": 0.4016246730001377,
        "step": 2917
    },
    {
        "loss": 1.9121,
        "grad_norm": 1.3694374561309814,
        "learning_rate": 0.00018114591470000656,
        "epoch": 0.4017623571526917,
        "step": 2918
    },
    {
        "loss": 2.4593,
        "grad_norm": 1.5591014623641968,
        "learning_rate": 0.00018106974857519736,
        "epoch": 0.40190004130524576,
        "step": 2919
    },
    {
        "loss": 2.18,
        "grad_norm": 1.3350780010223389,
        "learning_rate": 0.00018099344499433745,
        "epoch": 0.4020377254577998,
        "step": 2920
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.7272627353668213,
        "learning_rate": 0.00018091700408680165,
        "epoch": 0.40217540961035386,
        "step": 2921
    },
    {
        "loss": 2.1079,
        "grad_norm": 2.7413201332092285,
        "learning_rate": 0.00018084042598219766,
        "epoch": 0.4023130937629079,
        "step": 2922
    },
    {
        "loss": 2.2252,
        "grad_norm": 1.5744844675064087,
        "learning_rate": 0.0001807637108103659,
        "epoch": 0.4024507779154619,
        "step": 2923
    },
    {
        "loss": 2.4254,
        "grad_norm": 1.5993443727493286,
        "learning_rate": 0.00018068685870137902,
        "epoch": 0.40258846206801596,
        "step": 2924
    },
    {
        "loss": 2.1475,
        "grad_norm": 2.1685163974761963,
        "learning_rate": 0.00018060986978554207,
        "epoch": 0.40272614622057,
        "step": 2925
    },
    {
        "loss": 2.1775,
        "grad_norm": 1.8032747507095337,
        "learning_rate": 0.00018053274419339185,
        "epoch": 0.40286383037312407,
        "step": 2926
    },
    {
        "loss": 1.7432,
        "grad_norm": 2.6537036895751953,
        "learning_rate": 0.00018045548205569692,
        "epoch": 0.4030015145256781,
        "step": 2927
    },
    {
        "loss": 1.9854,
        "grad_norm": 2.1296308040618896,
        "learning_rate": 0.0001803780835034575,
        "epoch": 0.4031391986782321,
        "step": 2928
    },
    {
        "loss": 2.0832,
        "grad_norm": 1.2948660850524902,
        "learning_rate": 0.00018030054866790505,
        "epoch": 0.40327688283078617,
        "step": 2929
    },
    {
        "loss": 2.69,
        "grad_norm": 2.134103536605835,
        "learning_rate": 0.000180222877680502,
        "epoch": 0.4034145669833402,
        "step": 2930
    },
    {
        "loss": 0.6234,
        "grad_norm": 2.1328821182250977,
        "learning_rate": 0.00018014507067294175,
        "epoch": 0.40355225113589427,
        "step": 2931
    },
    {
        "loss": 2.1573,
        "grad_norm": 1.6432186365127563,
        "learning_rate": 0.00018006712777714833,
        "epoch": 0.4036899352884483,
        "step": 2932
    },
    {
        "loss": 1.5963,
        "grad_norm": 2.4093916416168213,
        "learning_rate": 0.00017998904912527603,
        "epoch": 0.4038276194410023,
        "step": 2933
    },
    {
        "loss": 2.7058,
        "grad_norm": 1.4328668117523193,
        "learning_rate": 0.00017991083484970941,
        "epoch": 0.40396530359355637,
        "step": 2934
    },
    {
        "loss": 2.1058,
        "grad_norm": 0.9777196645736694,
        "learning_rate": 0.00017983248508306307,
        "epoch": 0.4041029877461104,
        "step": 2935
    },
    {
        "loss": 2.1537,
        "grad_norm": 1.4716182947158813,
        "learning_rate": 0.0001797539999581812,
        "epoch": 0.40424067189866447,
        "step": 2936
    },
    {
        "loss": 2.04,
        "grad_norm": 2.064565896987915,
        "learning_rate": 0.00017967537960813756,
        "epoch": 0.4043783560512185,
        "step": 2937
    },
    {
        "loss": 2.276,
        "grad_norm": 1.8738689422607422,
        "learning_rate": 0.00017959662416623518,
        "epoch": 0.4045160402037725,
        "step": 2938
    },
    {
        "loss": 2.2094,
        "grad_norm": 2.4353373050689697,
        "learning_rate": 0.00017951773376600612,
        "epoch": 0.40465372435632657,
        "step": 2939
    },
    {
        "loss": 2.3663,
        "grad_norm": 1.07987642288208,
        "learning_rate": 0.00017943870854121124,
        "epoch": 0.4047914085088806,
        "step": 2940
    },
    {
        "loss": 2.1798,
        "grad_norm": 1.9056262969970703,
        "learning_rate": 0.00017935954862584018,
        "epoch": 0.4049290926614347,
        "step": 2941
    },
    {
        "loss": 2.4724,
        "grad_norm": 1.6037395000457764,
        "learning_rate": 0.0001792802541541107,
        "epoch": 0.4050667768139887,
        "step": 2942
    },
    {
        "loss": 2.3232,
        "grad_norm": 1.0338822603225708,
        "learning_rate": 0.00017920082526046884,
        "epoch": 0.4052044609665427,
        "step": 2943
    },
    {
        "loss": 2.191,
        "grad_norm": 1.7714486122131348,
        "learning_rate": 0.00017912126207958858,
        "epoch": 0.4053421451190968,
        "step": 2944
    },
    {
        "loss": 2.6446,
        "grad_norm": 1.416874885559082,
        "learning_rate": 0.0001790415647463715,
        "epoch": 0.4054798292716508,
        "step": 2945
    },
    {
        "loss": 1.51,
        "grad_norm": 2.5207436084747314,
        "learning_rate": 0.0001789617333959467,
        "epoch": 0.4056175134242049,
        "step": 2946
    },
    {
        "loss": 2.41,
        "grad_norm": 1.2764770984649658,
        "learning_rate": 0.0001788817681636705,
        "epoch": 0.40575519757675893,
        "step": 2947
    },
    {
        "loss": 1.7976,
        "grad_norm": 2.0880584716796875,
        "learning_rate": 0.00017880166918512612,
        "epoch": 0.405892881729313,
        "step": 2948
    },
    {
        "loss": 2.0014,
        "grad_norm": 2.3696022033691406,
        "learning_rate": 0.00017872143659612377,
        "epoch": 0.406030565881867,
        "step": 2949
    },
    {
        "loss": 2.229,
        "grad_norm": 1.7455227375030518,
        "learning_rate": 0.00017864107053270004,
        "epoch": 0.40616825003442103,
        "step": 2950
    },
    {
        "loss": 1.5934,
        "grad_norm": 2.9867117404937744,
        "learning_rate": 0.0001785605711311178,
        "epoch": 0.4063059341869751,
        "step": 2951
    },
    {
        "loss": 2.6495,
        "grad_norm": 1.8129433393478394,
        "learning_rate": 0.00017847993852786612,
        "epoch": 0.40644361833952913,
        "step": 2952
    },
    {
        "loss": 2.1299,
        "grad_norm": 1.94374680519104,
        "learning_rate": 0.00017839917285965984,
        "epoch": 0.4065813024920832,
        "step": 2953
    },
    {
        "loss": 1.8838,
        "grad_norm": 1.3481335639953613,
        "learning_rate": 0.00017831827426343942,
        "epoch": 0.4067189866446372,
        "step": 2954
    },
    {
        "loss": 2.4281,
        "grad_norm": 1.7686322927474976,
        "learning_rate": 0.00017823724287637077,
        "epoch": 0.40685667079719123,
        "step": 2955
    },
    {
        "loss": 2.2787,
        "grad_norm": 1.8967547416687012,
        "learning_rate": 0.00017815607883584481,
        "epoch": 0.4069943549497453,
        "step": 2956
    },
    {
        "loss": 1.8909,
        "grad_norm": 1.7539210319519043,
        "learning_rate": 0.00017807478227947757,
        "epoch": 0.40713203910229934,
        "step": 2957
    },
    {
        "loss": 0.8731,
        "grad_norm": 3.3874094486236572,
        "learning_rate": 0.00017799335334510962,
        "epoch": 0.4072697232548534,
        "step": 2958
    },
    {
        "loss": 2.5018,
        "grad_norm": 1.529345154762268,
        "learning_rate": 0.00017791179217080595,
        "epoch": 0.4074074074074074,
        "step": 2959
    },
    {
        "loss": 1.3845,
        "grad_norm": 1.9919613599777222,
        "learning_rate": 0.00017783009889485592,
        "epoch": 0.40754509155996144,
        "step": 2960
    },
    {
        "loss": 1.8529,
        "grad_norm": 1.7122313976287842,
        "learning_rate": 0.00017774827365577277,
        "epoch": 0.4076827757125155,
        "step": 2961
    },
    {
        "loss": 2.7212,
        "grad_norm": 1.4793727397918701,
        "learning_rate": 0.00017766631659229352,
        "epoch": 0.40782045986506954,
        "step": 2962
    },
    {
        "loss": 2.2594,
        "grad_norm": 1.969168782234192,
        "learning_rate": 0.00017758422784337863,
        "epoch": 0.4079581440176236,
        "step": 2963
    },
    {
        "loss": 2.2485,
        "grad_norm": 2.1222686767578125,
        "learning_rate": 0.00017750200754821198,
        "epoch": 0.4080958281701776,
        "step": 2964
    },
    {
        "loss": 1.8537,
        "grad_norm": 2.3075578212738037,
        "learning_rate": 0.0001774196558462003,
        "epoch": 0.40823351232273164,
        "step": 2965
    },
    {
        "loss": 2.2857,
        "grad_norm": 2.282402753829956,
        "learning_rate": 0.00017733717287697328,
        "epoch": 0.4083711964752857,
        "step": 2966
    },
    {
        "loss": 2.3393,
        "grad_norm": 1.9943046569824219,
        "learning_rate": 0.0001772545587803832,
        "epoch": 0.40850888062783974,
        "step": 2967
    },
    {
        "loss": 1.8543,
        "grad_norm": 2.3988921642303467,
        "learning_rate": 0.00017717181369650446,
        "epoch": 0.4086465647803938,
        "step": 2968
    },
    {
        "loss": 1.8292,
        "grad_norm": 2.5856010913848877,
        "learning_rate": 0.0001770889377656338,
        "epoch": 0.4087842489329478,
        "step": 2969
    },
    {
        "loss": 2.3573,
        "grad_norm": 1.3392767906188965,
        "learning_rate": 0.00017700593112828967,
        "epoch": 0.40892193308550184,
        "step": 2970
    },
    {
        "loss": 2.174,
        "grad_norm": 1.4404114484786987,
        "learning_rate": 0.00017692279392521215,
        "epoch": 0.4090596172380559,
        "step": 2971
    },
    {
        "loss": 2.2631,
        "grad_norm": 1.531716227531433,
        "learning_rate": 0.00017683952629736268,
        "epoch": 0.40919730139060995,
        "step": 2972
    },
    {
        "loss": 1.5425,
        "grad_norm": 2.698620319366455,
        "learning_rate": 0.000176756128385924,
        "epoch": 0.409334985543164,
        "step": 2973
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.8996721506118774,
        "learning_rate": 0.00017667260033229956,
        "epoch": 0.40947266969571805,
        "step": 2974
    },
    {
        "loss": 2.2748,
        "grad_norm": 1.6898536682128906,
        "learning_rate": 0.00017658894227811346,
        "epoch": 0.40961035384827205,
        "step": 2975
    },
    {
        "loss": 2.0071,
        "grad_norm": 1.8062437772750854,
        "learning_rate": 0.00017650515436521045,
        "epoch": 0.4097480380008261,
        "step": 2976
    },
    {
        "loss": 2.2107,
        "grad_norm": 1.7629263401031494,
        "learning_rate": 0.00017642123673565515,
        "epoch": 0.40988572215338015,
        "step": 2977
    },
    {
        "loss": 2.0601,
        "grad_norm": 1.573429822921753,
        "learning_rate": 0.00017633718953173237,
        "epoch": 0.4100234063059342,
        "step": 2978
    },
    {
        "loss": 0.8518,
        "grad_norm": 2.293172836303711,
        "learning_rate": 0.00017625301289594654,
        "epoch": 0.41016109045848825,
        "step": 2979
    },
    {
        "loss": 2.2458,
        "grad_norm": 2.3536534309387207,
        "learning_rate": 0.00017616870697102142,
        "epoch": 0.41029877461104225,
        "step": 2980
    },
    {
        "loss": 1.9611,
        "grad_norm": 2.3069562911987305,
        "learning_rate": 0.0001760842718999002,
        "epoch": 0.4104364587635963,
        "step": 2981
    },
    {
        "loss": 2.5573,
        "grad_norm": 1.2626738548278809,
        "learning_rate": 0.00017599970782574488,
        "epoch": 0.41057414291615035,
        "step": 2982
    },
    {
        "loss": 1.0412,
        "grad_norm": 2.4619193077087402,
        "learning_rate": 0.0001759150148919362,
        "epoch": 0.4107118270687044,
        "step": 2983
    },
    {
        "loss": 2.251,
        "grad_norm": 1.6608436107635498,
        "learning_rate": 0.0001758301932420735,
        "epoch": 0.41084951122125846,
        "step": 2984
    },
    {
        "loss": 2.0331,
        "grad_norm": 2.641597032546997,
        "learning_rate": 0.00017574524301997423,
        "epoch": 0.41098719537381245,
        "step": 2985
    },
    {
        "loss": 1.5539,
        "grad_norm": 1.515481948852539,
        "learning_rate": 0.00017566016436967394,
        "epoch": 0.4111248795263665,
        "step": 2986
    },
    {
        "loss": 2.3609,
        "grad_norm": 1.1775716543197632,
        "learning_rate": 0.00017557495743542582,
        "epoch": 0.41126256367892056,
        "step": 2987
    },
    {
        "loss": 2.6893,
        "grad_norm": 1.6621264219284058,
        "learning_rate": 0.00017548962236170072,
        "epoch": 0.4114002478314746,
        "step": 2988
    },
    {
        "loss": 2.093,
        "grad_norm": 2.17985200881958,
        "learning_rate": 0.0001754041592931866,
        "epoch": 0.41153793198402866,
        "step": 2989
    },
    {
        "loss": 2.506,
        "grad_norm": 1.2329906225204468,
        "learning_rate": 0.00017531856837478852,
        "epoch": 0.41167561613658266,
        "step": 2990
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.6635286808013916,
        "learning_rate": 0.00017523284975162837,
        "epoch": 0.4118133002891367,
        "step": 2991
    },
    {
        "loss": 2.2343,
        "grad_norm": 2.3122289180755615,
        "learning_rate": 0.00017514700356904437,
        "epoch": 0.41195098444169076,
        "step": 2992
    },
    {
        "loss": 1.8487,
        "grad_norm": 2.7370831966400146,
        "learning_rate": 0.0001750610299725913,
        "epoch": 0.4120886685942448,
        "step": 2993
    },
    {
        "loss": 2.207,
        "grad_norm": 1.590816617012024,
        "learning_rate": 0.00017497492910803977,
        "epoch": 0.41222635274679886,
        "step": 2994
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.2124040126800537,
        "learning_rate": 0.00017488870112137612,
        "epoch": 0.41236403689935286,
        "step": 2995
    },
    {
        "loss": 1.6948,
        "grad_norm": 2.2037410736083984,
        "learning_rate": 0.0001748023461588025,
        "epoch": 0.4125017210519069,
        "step": 2996
    },
    {
        "loss": 2.2728,
        "grad_norm": 2.1788365840911865,
        "learning_rate": 0.00017471586436673605,
        "epoch": 0.41263940520446096,
        "step": 2997
    },
    {
        "loss": 1.5954,
        "grad_norm": 2.9109957218170166,
        "learning_rate": 0.00017462925589180922,
        "epoch": 0.412777089357015,
        "step": 2998
    },
    {
        "loss": 2.2006,
        "grad_norm": 2.0794684886932373,
        "learning_rate": 0.0001745425208808691,
        "epoch": 0.41291477350956907,
        "step": 2999
    },
    {
        "loss": 1.7672,
        "grad_norm": 2.5020344257354736,
        "learning_rate": 0.00017445565948097724,
        "epoch": 0.41305245766212306,
        "step": 3000
    },
    {
        "loss": 1.8844,
        "grad_norm": 2.355660915374756,
        "learning_rate": 0.0001743686718394097,
        "epoch": 0.4131901418146771,
        "step": 3001
    },
    {
        "loss": 1.1393,
        "grad_norm": 1.771019458770752,
        "learning_rate": 0.0001742815581036565,
        "epoch": 0.41332782596723117,
        "step": 3002
    },
    {
        "loss": 2.2569,
        "grad_norm": 1.7527979612350464,
        "learning_rate": 0.0001741943184214214,
        "epoch": 0.4134655101197852,
        "step": 3003
    },
    {
        "loss": 2.3137,
        "grad_norm": 1.6910781860351562,
        "learning_rate": 0.00017410695294062175,
        "epoch": 0.41360319427233927,
        "step": 3004
    },
    {
        "loss": 2.2321,
        "grad_norm": 1.5531138181686401,
        "learning_rate": 0.00017401946180938823,
        "epoch": 0.4137408784248933,
        "step": 3005
    },
    {
        "loss": 2.2322,
        "grad_norm": 1.8313872814178467,
        "learning_rate": 0.0001739318451760645,
        "epoch": 0.4138785625774473,
        "step": 3006
    },
    {
        "loss": 2.4195,
        "grad_norm": 1.259644865989685,
        "learning_rate": 0.00017384410318920697,
        "epoch": 0.41401624673000137,
        "step": 3007
    },
    {
        "loss": 2.1867,
        "grad_norm": 2.327315330505371,
        "learning_rate": 0.0001737562359975848,
        "epoch": 0.4141539308825554,
        "step": 3008
    },
    {
        "loss": 2.241,
        "grad_norm": 1.4205927848815918,
        "learning_rate": 0.00017366824375017917,
        "epoch": 0.4142916150351095,
        "step": 3009
    },
    {
        "loss": 2.6553,
        "grad_norm": 1.4326478242874146,
        "learning_rate": 0.00017358012659618352,
        "epoch": 0.4144292991876635,
        "step": 3010
    },
    {
        "loss": 2.0186,
        "grad_norm": 2.035008668899536,
        "learning_rate": 0.00017349188468500297,
        "epoch": 0.4145669833402175,
        "step": 3011
    },
    {
        "loss": 2.3855,
        "grad_norm": 2.034724235534668,
        "learning_rate": 0.00017340351816625405,
        "epoch": 0.4147046674927716,
        "step": 3012
    },
    {
        "loss": 2.1279,
        "grad_norm": 2.2121779918670654,
        "learning_rate": 0.00017331502718976477,
        "epoch": 0.4148423516453256,
        "step": 3013
    },
    {
        "loss": 2.0776,
        "grad_norm": 2.5128793716430664,
        "learning_rate": 0.0001732264119055742,
        "epoch": 0.4149800357978797,
        "step": 3014
    },
    {
        "loss": 1.7613,
        "grad_norm": 2.846660614013672,
        "learning_rate": 0.00017313767246393184,
        "epoch": 0.41511771995043373,
        "step": 3015
    },
    {
        "loss": 1.3486,
        "grad_norm": 2.0892763137817383,
        "learning_rate": 0.00017304880901529806,
        "epoch": 0.4152554041029877,
        "step": 3016
    },
    {
        "loss": 2.3984,
        "grad_norm": 1.2519183158874512,
        "learning_rate": 0.00017295982171034339,
        "epoch": 0.4153930882555418,
        "step": 3017
    },
    {
        "loss": 2.2505,
        "grad_norm": 1.8618565797805786,
        "learning_rate": 0.00017287071069994824,
        "epoch": 0.4155307724080958,
        "step": 3018
    },
    {
        "loss": 1.9948,
        "grad_norm": 1.9979782104492188,
        "learning_rate": 0.00017278147613520277,
        "epoch": 0.4156684565606499,
        "step": 3019
    },
    {
        "loss": 1.0619,
        "grad_norm": 2.415159225463867,
        "learning_rate": 0.0001726921181674069,
        "epoch": 0.41580614071320393,
        "step": 3020
    },
    {
        "loss": 2.8384,
        "grad_norm": 1.2079075574874878,
        "learning_rate": 0.00017260263694806943,
        "epoch": 0.4159438248657579,
        "step": 3021
    },
    {
        "loss": 2.0345,
        "grad_norm": 1.7274632453918457,
        "learning_rate": 0.0001725130326289084,
        "epoch": 0.416081509018312,
        "step": 3022
    },
    {
        "loss": 1.611,
        "grad_norm": 2.5723350048065186,
        "learning_rate": 0.00017242330536185044,
        "epoch": 0.41621919317086603,
        "step": 3023
    },
    {
        "loss": 1.9707,
        "grad_norm": 1.147895336151123,
        "learning_rate": 0.00017233345529903057,
        "epoch": 0.4163568773234201,
        "step": 3024
    },
    {
        "loss": 2.4284,
        "grad_norm": 2.5478265285491943,
        "learning_rate": 0.0001722434825927922,
        "epoch": 0.41649456147597413,
        "step": 3025
    },
    {
        "loss": 2.5244,
        "grad_norm": 2.5627033710479736,
        "learning_rate": 0.00017215338739568666,
        "epoch": 0.41663224562852813,
        "step": 3026
    },
    {
        "loss": 1.365,
        "grad_norm": 2.6977102756500244,
        "learning_rate": 0.0001720631698604728,
        "epoch": 0.4167699297810822,
        "step": 3027
    },
    {
        "loss": 2.08,
        "grad_norm": 1.3394780158996582,
        "learning_rate": 0.00017197283014011702,
        "epoch": 0.41690761393363623,
        "step": 3028
    },
    {
        "loss": 1.4785,
        "grad_norm": 2.850858688354492,
        "learning_rate": 0.00017188236838779292,
        "epoch": 0.4170452980861903,
        "step": 3029
    },
    {
        "loss": 2.6796,
        "grad_norm": 2.2853643894195557,
        "learning_rate": 0.00017179178475688095,
        "epoch": 0.41718298223874434,
        "step": 3030
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.5928362607955933,
        "learning_rate": 0.00017170107940096813,
        "epoch": 0.41732066639129833,
        "step": 3031
    },
    {
        "loss": 1.7492,
        "grad_norm": 1.554898738861084,
        "learning_rate": 0.00017161025247384805,
        "epoch": 0.4174583505438524,
        "step": 3032
    },
    {
        "loss": 2.2055,
        "grad_norm": 2.5865769386291504,
        "learning_rate": 0.00017151930412952028,
        "epoch": 0.41759603469640644,
        "step": 3033
    },
    {
        "loss": 1.9068,
        "grad_norm": 2.269033908843994,
        "learning_rate": 0.00017142823452219038,
        "epoch": 0.4177337188489605,
        "step": 3034
    },
    {
        "loss": 1.9591,
        "grad_norm": 1.3311773538589478,
        "learning_rate": 0.00017133704380626938,
        "epoch": 0.41787140300151454,
        "step": 3035
    },
    {
        "loss": 1.5509,
        "grad_norm": 2.8359031677246094,
        "learning_rate": 0.00017124573213637368,
        "epoch": 0.4180090871540686,
        "step": 3036
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.5215617418289185,
        "learning_rate": 0.0001711542996673249,
        "epoch": 0.4181467713066226,
        "step": 3037
    },
    {
        "loss": 1.9838,
        "grad_norm": 1.5176199674606323,
        "learning_rate": 0.0001710627465541493,
        "epoch": 0.41828445545917664,
        "step": 3038
    },
    {
        "loss": 2.2544,
        "grad_norm": 1.6835159063339233,
        "learning_rate": 0.00017097107295207774,
        "epoch": 0.4184221396117307,
        "step": 3039
    },
    {
        "loss": 2.57,
        "grad_norm": 1.6503456830978394,
        "learning_rate": 0.00017087927901654557,
        "epoch": 0.41855982376428474,
        "step": 3040
    },
    {
        "loss": 2.117,
        "grad_norm": 1.092542290687561,
        "learning_rate": 0.00017078736490319183,
        "epoch": 0.4186975079168388,
        "step": 3041
    },
    {
        "loss": 2.2094,
        "grad_norm": 1.4041242599487305,
        "learning_rate": 0.00017069533076785959,
        "epoch": 0.4188351920693928,
        "step": 3042
    },
    {
        "loss": 1.0501,
        "grad_norm": 3.5590763092041016,
        "learning_rate": 0.00017060317676659533,
        "epoch": 0.41897287622194684,
        "step": 3043
    },
    {
        "loss": 1.6665,
        "grad_norm": 2.068807363510132,
        "learning_rate": 0.00017051090305564864,
        "epoch": 0.4191105603745009,
        "step": 3044
    },
    {
        "loss": 1.9377,
        "grad_norm": 2.204280138015747,
        "learning_rate": 0.00017041850979147236,
        "epoch": 0.41924824452705495,
        "step": 3045
    },
    {
        "loss": 2.3708,
        "grad_norm": 1.758115530014038,
        "learning_rate": 0.00017032599713072183,
        "epoch": 0.419385928679609,
        "step": 3046
    },
    {
        "loss": 1.7661,
        "grad_norm": 2.468794345855713,
        "learning_rate": 0.0001702333652302549,
        "epoch": 0.419523612832163,
        "step": 3047
    },
    {
        "loss": 2.1782,
        "grad_norm": 2.0071232318878174,
        "learning_rate": 0.00017014061424713148,
        "epoch": 0.41966129698471705,
        "step": 3048
    },
    {
        "loss": 0.6716,
        "grad_norm": 2.529085397720337,
        "learning_rate": 0.0001700477443386136,
        "epoch": 0.4197989811372711,
        "step": 3049
    },
    {
        "loss": 2.0847,
        "grad_norm": 1.2137081623077393,
        "learning_rate": 0.00016995475566216475,
        "epoch": 0.41993666528982515,
        "step": 3050
    },
    {
        "loss": 2.2771,
        "grad_norm": 2.555311679840088,
        "learning_rate": 0.00016986164837544984,
        "epoch": 0.4200743494423792,
        "step": 3051
    },
    {
        "loss": 1.5286,
        "grad_norm": 2.8109593391418457,
        "learning_rate": 0.00016976842263633502,
        "epoch": 0.4202120335949332,
        "step": 3052
    },
    {
        "loss": 2.0255,
        "grad_norm": 1.5465809106826782,
        "learning_rate": 0.00016967507860288702,
        "epoch": 0.42034971774748725,
        "step": 3053
    },
    {
        "loss": 1.7352,
        "grad_norm": 2.3498284816741943,
        "learning_rate": 0.00016958161643337345,
        "epoch": 0.4204874019000413,
        "step": 3054
    },
    {
        "loss": 1.7757,
        "grad_norm": 2.8282363414764404,
        "learning_rate": 0.00016948803628626195,
        "epoch": 0.42062508605259535,
        "step": 3055
    },
    {
        "loss": 2.0724,
        "grad_norm": 1.8945859670639038,
        "learning_rate": 0.0001693943383202203,
        "epoch": 0.4207627702051494,
        "step": 3056
    },
    {
        "loss": 2.8,
        "grad_norm": 1.5858111381530762,
        "learning_rate": 0.00016930052269411604,
        "epoch": 0.4209004543577034,
        "step": 3057
    },
    {
        "loss": 2.4455,
        "grad_norm": 1.5232791900634766,
        "learning_rate": 0.00016920658956701633,
        "epoch": 0.42103813851025745,
        "step": 3058
    },
    {
        "loss": 2.173,
        "grad_norm": 1.7381300926208496,
        "learning_rate": 0.00016911253909818736,
        "epoch": 0.4211758226628115,
        "step": 3059
    },
    {
        "loss": 2.0013,
        "grad_norm": 2.149648666381836,
        "learning_rate": 0.0001690183714470943,
        "epoch": 0.42131350681536556,
        "step": 3060
    },
    {
        "loss": 2.2927,
        "grad_norm": 1.3429633378982544,
        "learning_rate": 0.00016892408677340122,
        "epoch": 0.4214511909679196,
        "step": 3061
    },
    {
        "loss": 1.9283,
        "grad_norm": 1.4938660860061646,
        "learning_rate": 0.00016882968523697028,
        "epoch": 0.42158887512047366,
        "step": 3062
    },
    {
        "loss": 2.515,
        "grad_norm": 1.676534652709961,
        "learning_rate": 0.00016873516699786206,
        "epoch": 0.42172655927302766,
        "step": 3063
    },
    {
        "loss": 2.3836,
        "grad_norm": 2.6113662719726562,
        "learning_rate": 0.0001686405322163349,
        "epoch": 0.4218642434255817,
        "step": 3064
    },
    {
        "loss": 1.5532,
        "grad_norm": 1.512444019317627,
        "learning_rate": 0.0001685457810528447,
        "epoch": 0.42200192757813576,
        "step": 3065
    },
    {
        "loss": 1.4336,
        "grad_norm": 2.4015936851501465,
        "learning_rate": 0.0001684509136680448,
        "epoch": 0.4221396117306898,
        "step": 3066
    },
    {
        "loss": 1.9502,
        "grad_norm": 2.681098699569702,
        "learning_rate": 0.00016835593022278552,
        "epoch": 0.42227729588324386,
        "step": 3067
    },
    {
        "loss": 2.4485,
        "grad_norm": 1.0753200054168701,
        "learning_rate": 0.00016826083087811386,
        "epoch": 0.42241498003579786,
        "step": 3068
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.4649202823638916,
        "learning_rate": 0.00016816561579527356,
        "epoch": 0.4225526641883519,
        "step": 3069
    },
    {
        "loss": 1.3321,
        "grad_norm": 2.5339808464050293,
        "learning_rate": 0.00016807028513570455,
        "epoch": 0.42269034834090596,
        "step": 3070
    },
    {
        "loss": 2.6777,
        "grad_norm": 1.3044836521148682,
        "learning_rate": 0.00016797483906104256,
        "epoch": 0.42282803249346,
        "step": 3071
    },
    {
        "loss": 2.1062,
        "grad_norm": 1.410934329032898,
        "learning_rate": 0.00016787927773311906,
        "epoch": 0.42296571664601407,
        "step": 3072
    },
    {
        "loss": 2.572,
        "grad_norm": 2.3232462406158447,
        "learning_rate": 0.00016778360131396112,
        "epoch": 0.42310340079856806,
        "step": 3073
    },
    {
        "loss": 2.0448,
        "grad_norm": 1.6004563570022583,
        "learning_rate": 0.00016768780996579067,
        "epoch": 0.4232410849511221,
        "step": 3074
    },
    {
        "loss": 2.2148,
        "grad_norm": 2.333928346633911,
        "learning_rate": 0.00016759190385102473,
        "epoch": 0.42337876910367617,
        "step": 3075
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.262845754623413,
        "learning_rate": 0.0001674958831322749,
        "epoch": 0.4235164532562302,
        "step": 3076
    },
    {
        "loss": 2.2951,
        "grad_norm": 2.186401605606079,
        "learning_rate": 0.00016739974797234686,
        "epoch": 0.42365413740878427,
        "step": 3077
    },
    {
        "loss": 2.5154,
        "grad_norm": 1.9014257192611694,
        "learning_rate": 0.0001673034985342406,
        "epoch": 0.42379182156133827,
        "step": 3078
    },
    {
        "loss": 1.4455,
        "grad_norm": 2.169726848602295,
        "learning_rate": 0.0001672071349811498,
        "epoch": 0.4239295057138923,
        "step": 3079
    },
    {
        "loss": 2.0944,
        "grad_norm": 1.5880354642868042,
        "learning_rate": 0.00016711065747646142,
        "epoch": 0.42406718986644637,
        "step": 3080
    },
    {
        "loss": 2.3034,
        "grad_norm": 1.8416811227798462,
        "learning_rate": 0.00016701406618375598,
        "epoch": 0.4242048740190004,
        "step": 3081
    },
    {
        "loss": 1.7829,
        "grad_norm": 2.163816213607788,
        "learning_rate": 0.0001669173612668066,
        "epoch": 0.4243425581715545,
        "step": 3082
    },
    {
        "loss": 2.3175,
        "grad_norm": 1.2380480766296387,
        "learning_rate": 0.00016682054288957935,
        "epoch": 0.42448024232410847,
        "step": 3083
    },
    {
        "loss": 2.1697,
        "grad_norm": 1.761550784111023,
        "learning_rate": 0.00016672361121623244,
        "epoch": 0.4246179264766625,
        "step": 3084
    },
    {
        "loss": 1.919,
        "grad_norm": 1.0543407201766968,
        "learning_rate": 0.00016662656641111624,
        "epoch": 0.4247556106292166,
        "step": 3085
    },
    {
        "loss": 2.2186,
        "grad_norm": 2.18931245803833,
        "learning_rate": 0.00016652940863877294,
        "epoch": 0.4248932947817706,
        "step": 3086
    },
    {
        "loss": 1.7947,
        "grad_norm": 3.451369285583496,
        "learning_rate": 0.00016643213806393642,
        "epoch": 0.4250309789343247,
        "step": 3087
    },
    {
        "loss": 2.0259,
        "grad_norm": 2.573038339614868,
        "learning_rate": 0.0001663347548515316,
        "epoch": 0.4251686630868787,
        "step": 3088
    },
    {
        "loss": 2.4583,
        "grad_norm": 2.199829578399658,
        "learning_rate": 0.0001662372591666744,
        "epoch": 0.4253063472394327,
        "step": 3089
    },
    {
        "loss": 2.4275,
        "grad_norm": 1.5522260665893555,
        "learning_rate": 0.00016613965117467168,
        "epoch": 0.4254440313919868,
        "step": 3090
    },
    {
        "loss": 2.2311,
        "grad_norm": 2.028444528579712,
        "learning_rate": 0.00016604193104102048,
        "epoch": 0.42558171554454083,
        "step": 3091
    },
    {
        "loss": 2.2644,
        "grad_norm": 1.871445894241333,
        "learning_rate": 0.00016594409893140797,
        "epoch": 0.4257193996970949,
        "step": 3092
    },
    {
        "loss": 2.2234,
        "grad_norm": 1.6551330089569092,
        "learning_rate": 0.00016584615501171138,
        "epoch": 0.42585708384964893,
        "step": 3093
    },
    {
        "loss": 2.1244,
        "grad_norm": 1.7856128215789795,
        "learning_rate": 0.00016574809944799724,
        "epoch": 0.42599476800220293,
        "step": 3094
    },
    {
        "loss": 1.7881,
        "grad_norm": 1.6300419569015503,
        "learning_rate": 0.0001656499324065217,
        "epoch": 0.426132452154757,
        "step": 3095
    },
    {
        "loss": 2.5074,
        "grad_norm": 1.202343225479126,
        "learning_rate": 0.0001655516540537297,
        "epoch": 0.42627013630731103,
        "step": 3096
    },
    {
        "loss": 1.2301,
        "grad_norm": 1.6403377056121826,
        "learning_rate": 0.00016545326455625486,
        "epoch": 0.4264078204598651,
        "step": 3097
    },
    {
        "loss": 2.4695,
        "grad_norm": 1.770127296447754,
        "learning_rate": 0.00016535476408091946,
        "epoch": 0.42654550461241914,
        "step": 3098
    },
    {
        "loss": 2.6521,
        "grad_norm": 2.1674487590789795,
        "learning_rate": 0.00016525615279473383,
        "epoch": 0.42668318876497313,
        "step": 3099
    },
    {
        "loss": 2.0916,
        "grad_norm": 1.8163936138153076,
        "learning_rate": 0.00016515743086489617,
        "epoch": 0.4268208729175272,
        "step": 3100
    },
    {
        "loss": 1.7186,
        "grad_norm": 2.6000471115112305,
        "learning_rate": 0.00016505859845879233,
        "epoch": 0.42695855707008123,
        "step": 3101
    },
    {
        "loss": 1.6964,
        "grad_norm": 1.4228465557098389,
        "learning_rate": 0.00016495965574399546,
        "epoch": 0.4270962412226353,
        "step": 3102
    },
    {
        "loss": 1.4217,
        "grad_norm": 2.5031700134277344,
        "learning_rate": 0.0001648606028882657,
        "epoch": 0.42723392537518934,
        "step": 3103
    },
    {
        "loss": 1.5949,
        "grad_norm": 2.3579587936401367,
        "learning_rate": 0.00016476144005954995,
        "epoch": 0.42737160952774333,
        "step": 3104
    },
    {
        "loss": 1.1176,
        "grad_norm": 1.8343877792358398,
        "learning_rate": 0.00016466216742598162,
        "epoch": 0.4275092936802974,
        "step": 3105
    },
    {
        "loss": 2.0543,
        "grad_norm": 2.3461856842041016,
        "learning_rate": 0.00016456278515588024,
        "epoch": 0.42764697783285144,
        "step": 3106
    },
    {
        "loss": 2.5521,
        "grad_norm": 1.772158145904541,
        "learning_rate": 0.00016446329341775133,
        "epoch": 0.4277846619854055,
        "step": 3107
    },
    {
        "loss": 1.9636,
        "grad_norm": 2.170732259750366,
        "learning_rate": 0.00016436369238028586,
        "epoch": 0.42792234613795954,
        "step": 3108
    },
    {
        "loss": 2.1325,
        "grad_norm": 1.3123749494552612,
        "learning_rate": 0.00016426398221236022,
        "epoch": 0.42806003029051354,
        "step": 3109
    },
    {
        "loss": 2.2952,
        "grad_norm": 1.4353896379470825,
        "learning_rate": 0.00016416416308303583,
        "epoch": 0.4281977144430676,
        "step": 3110
    },
    {
        "loss": 1.8983,
        "grad_norm": 1.5903509855270386,
        "learning_rate": 0.00016406423516155888,
        "epoch": 0.42833539859562164,
        "step": 3111
    },
    {
        "loss": 1.1584,
        "grad_norm": 2.5038766860961914,
        "learning_rate": 0.00016396419861735995,
        "epoch": 0.4284730827481757,
        "step": 3112
    },
    {
        "loss": 1.6308,
        "grad_norm": 2.4287471771240234,
        "learning_rate": 0.0001638640536200538,
        "epoch": 0.42861076690072974,
        "step": 3113
    },
    {
        "loss": 1.6827,
        "grad_norm": 3.0230700969696045,
        "learning_rate": 0.0001637638003394392,
        "epoch": 0.42874845105328374,
        "step": 3114
    },
    {
        "loss": 2.2552,
        "grad_norm": 1.0636128187179565,
        "learning_rate": 0.0001636634389454984,
        "epoch": 0.4288861352058378,
        "step": 3115
    },
    {
        "loss": 2.3996,
        "grad_norm": 2.2816286087036133,
        "learning_rate": 0.00016356296960839684,
        "epoch": 0.42902381935839184,
        "step": 3116
    },
    {
        "loss": 1.9298,
        "grad_norm": 1.7757964134216309,
        "learning_rate": 0.00016346239249848333,
        "epoch": 0.4291615035109459,
        "step": 3117
    },
    {
        "loss": 1.4343,
        "grad_norm": 1.1971752643585205,
        "learning_rate": 0.00016336170778628902,
        "epoch": 0.42929918766349995,
        "step": 3118
    },
    {
        "loss": 1.7077,
        "grad_norm": 2.441387414932251,
        "learning_rate": 0.0001632609156425278,
        "epoch": 0.429436871816054,
        "step": 3119
    },
    {
        "loss": 1.8138,
        "grad_norm": 5.244905948638916,
        "learning_rate": 0.0001631600162380956,
        "epoch": 0.429574555968608,
        "step": 3120
    },
    {
        "loss": 2.3934,
        "grad_norm": 1.6123183965682983,
        "learning_rate": 0.00016305900974407006,
        "epoch": 0.42971224012116205,
        "step": 3121
    },
    {
        "loss": 1.8467,
        "grad_norm": 1.3031058311462402,
        "learning_rate": 0.0001629578963317108,
        "epoch": 0.4298499242737161,
        "step": 3122
    },
    {
        "loss": 2.4338,
        "grad_norm": 2.1013190746307373,
        "learning_rate": 0.00016285667617245824,
        "epoch": 0.42998760842627015,
        "step": 3123
    },
    {
        "loss": 1.895,
        "grad_norm": 1.5929404497146606,
        "learning_rate": 0.0001627553494379341,
        "epoch": 0.4301252925788242,
        "step": 3124
    },
    {
        "loss": 2.3463,
        "grad_norm": 1.8114582300186157,
        "learning_rate": 0.0001626539162999408,
        "epoch": 0.4302629767313782,
        "step": 3125
    },
    {
        "loss": 2.3976,
        "grad_norm": 1.7017686367034912,
        "learning_rate": 0.00016255237693046102,
        "epoch": 0.43040066088393225,
        "step": 3126
    },
    {
        "loss": 2.4518,
        "grad_norm": 1.6534844636917114,
        "learning_rate": 0.00016245073150165763,
        "epoch": 0.4305383450364863,
        "step": 3127
    },
    {
        "loss": 2.0437,
        "grad_norm": 2.4143905639648438,
        "learning_rate": 0.0001623489801858734,
        "epoch": 0.43067602918904035,
        "step": 3128
    },
    {
        "loss": 1.2572,
        "grad_norm": 2.4859468936920166,
        "learning_rate": 0.0001622471231556304,
        "epoch": 0.4308137133415944,
        "step": 3129
    },
    {
        "loss": 2.327,
        "grad_norm": 1.6345901489257812,
        "learning_rate": 0.00016214516058363026,
        "epoch": 0.4309513974941484,
        "step": 3130
    },
    {
        "loss": 1.891,
        "grad_norm": 2.852713108062744,
        "learning_rate": 0.00016204309264275337,
        "epoch": 0.43108908164670245,
        "step": 3131
    },
    {
        "loss": 2.0338,
        "grad_norm": 1.2841346263885498,
        "learning_rate": 0.00016194091950605882,
        "epoch": 0.4312267657992565,
        "step": 3132
    },
    {
        "loss": 2.0306,
        "grad_norm": 1.2486943006515503,
        "learning_rate": 0.00016183864134678395,
        "epoch": 0.43136444995181056,
        "step": 3133
    },
    {
        "loss": 1.989,
        "grad_norm": 2.2743380069732666,
        "learning_rate": 0.0001617362583383444,
        "epoch": 0.4315021341043646,
        "step": 3134
    },
    {
        "loss": 1.7882,
        "grad_norm": 2.3733034133911133,
        "learning_rate": 0.00016163377065433335,
        "epoch": 0.4316398182569186,
        "step": 3135
    },
    {
        "loss": 2.2126,
        "grad_norm": 2.3693647384643555,
        "learning_rate": 0.0001615311784685216,
        "epoch": 0.43177750240947266,
        "step": 3136
    },
    {
        "loss": 2.3988,
        "grad_norm": 1.3088377714157104,
        "learning_rate": 0.0001614284819548572,
        "epoch": 0.4319151865620267,
        "step": 3137
    },
    {
        "loss": 2.6827,
        "grad_norm": 1.8985213041305542,
        "learning_rate": 0.00016132568128746484,
        "epoch": 0.43205287071458076,
        "step": 3138
    },
    {
        "loss": 2.2082,
        "grad_norm": 1.6657078266143799,
        "learning_rate": 0.0001612227766406461,
        "epoch": 0.4321905548671348,
        "step": 3139
    },
    {
        "loss": 1.9857,
        "grad_norm": 1.808914065361023,
        "learning_rate": 0.00016111976818887862,
        "epoch": 0.4323282390196888,
        "step": 3140
    },
    {
        "loss": 2.2638,
        "grad_norm": 2.051927089691162,
        "learning_rate": 0.0001610166561068161,
        "epoch": 0.43246592317224286,
        "step": 3141
    },
    {
        "loss": 2.2138,
        "grad_norm": 1.4924663305282593,
        "learning_rate": 0.00016091344056928808,
        "epoch": 0.4326036073247969,
        "step": 3142
    },
    {
        "loss": 2.4527,
        "grad_norm": 1.0653598308563232,
        "learning_rate": 0.00016081012175129945,
        "epoch": 0.43274129147735096,
        "step": 3143
    },
    {
        "loss": 2.1382,
        "grad_norm": 1.1945157051086426,
        "learning_rate": 0.00016070669982803015,
        "epoch": 0.432878975629905,
        "step": 3144
    },
    {
        "loss": 2.6042,
        "grad_norm": 1.4611175060272217,
        "learning_rate": 0.0001606031749748349,
        "epoch": 0.433016659782459,
        "step": 3145
    },
    {
        "loss": 2.2914,
        "grad_norm": 1.671739935874939,
        "learning_rate": 0.00016049954736724312,
        "epoch": 0.43315434393501306,
        "step": 3146
    },
    {
        "loss": 2.2515,
        "grad_norm": 2.3424222469329834,
        "learning_rate": 0.00016039581718095826,
        "epoch": 0.4332920280875671,
        "step": 3147
    },
    {
        "loss": 2.2471,
        "grad_norm": 1.8285586833953857,
        "learning_rate": 0.00016029198459185785,
        "epoch": 0.43342971224012117,
        "step": 3148
    },
    {
        "loss": 1.9174,
        "grad_norm": 2.35485577583313,
        "learning_rate": 0.00016018804977599302,
        "epoch": 0.4335673963926752,
        "step": 3149
    },
    {
        "loss": 1.9781,
        "grad_norm": 2.208806276321411,
        "learning_rate": 0.00016008401290958804,
        "epoch": 0.43370508054522927,
        "step": 3150
    },
    {
        "loss": 2.1352,
        "grad_norm": 1.1070570945739746,
        "learning_rate": 0.00015997987416904052,
        "epoch": 0.43384276469778327,
        "step": 3151
    },
    {
        "loss": 2.2843,
        "grad_norm": 2.4862351417541504,
        "learning_rate": 0.00015987563373092055,
        "epoch": 0.4339804488503373,
        "step": 3152
    },
    {
        "loss": 2.1264,
        "grad_norm": 1.2482706308364868,
        "learning_rate": 0.00015977129177197072,
        "epoch": 0.43411813300289137,
        "step": 3153
    },
    {
        "loss": 1.8941,
        "grad_norm": 1.8799824714660645,
        "learning_rate": 0.0001596668484691058,
        "epoch": 0.4342558171554454,
        "step": 3154
    },
    {
        "loss": 2.441,
        "grad_norm": 2.0135257244110107,
        "learning_rate": 0.00015956230399941242,
        "epoch": 0.4343935013079995,
        "step": 3155
    },
    {
        "loss": 2.604,
        "grad_norm": 1.775800347328186,
        "learning_rate": 0.00015945765854014866,
        "epoch": 0.43453118546055347,
        "step": 3156
    },
    {
        "loss": 2.5659,
        "grad_norm": 1.483106017112732,
        "learning_rate": 0.00015935291226874373,
        "epoch": 0.4346688696131075,
        "step": 3157
    },
    {
        "loss": 1.8022,
        "grad_norm": 2.255061626434326,
        "learning_rate": 0.0001592480653627981,
        "epoch": 0.4348065537656616,
        "step": 3158
    },
    {
        "loss": 1.6609,
        "grad_norm": 1.9905197620391846,
        "learning_rate": 0.00015914311800008246,
        "epoch": 0.4349442379182156,
        "step": 3159
    },
    {
        "loss": 2.1941,
        "grad_norm": 1.8015750646591187,
        "learning_rate": 0.00015903807035853816,
        "epoch": 0.4350819220707697,
        "step": 3160
    },
    {
        "loss": 2.0091,
        "grad_norm": 1.0606054067611694,
        "learning_rate": 0.00015893292261627643,
        "epoch": 0.4352196062233237,
        "step": 3161
    },
    {
        "loss": 2.436,
        "grad_norm": 1.6076191663742065,
        "learning_rate": 0.0001588276749515782,
        "epoch": 0.4353572903758777,
        "step": 3162
    },
    {
        "loss": 1.8301,
        "grad_norm": 1.5418708324432373,
        "learning_rate": 0.00015872232754289396,
        "epoch": 0.4354949745284318,
        "step": 3163
    },
    {
        "loss": 1.9828,
        "grad_norm": 2.908967971801758,
        "learning_rate": 0.0001586168805688431,
        "epoch": 0.43563265868098583,
        "step": 3164
    },
    {
        "loss": 1.7149,
        "grad_norm": 2.4324896335601807,
        "learning_rate": 0.00015851133420821396,
        "epoch": 0.4357703428335399,
        "step": 3165
    },
    {
        "loss": 1.6281,
        "grad_norm": 3.3287792205810547,
        "learning_rate": 0.00015840568863996347,
        "epoch": 0.4359080269860939,
        "step": 3166
    },
    {
        "loss": 1.7856,
        "grad_norm": 2.478238344192505,
        "learning_rate": 0.00015829994404321652,
        "epoch": 0.43604571113864793,
        "step": 3167
    },
    {
        "loss": 2.2015,
        "grad_norm": 1.5742478370666504,
        "learning_rate": 0.00015819410059726626,
        "epoch": 0.436183395291202,
        "step": 3168
    },
    {
        "loss": 2.5037,
        "grad_norm": 1.7863293886184692,
        "learning_rate": 0.00015808815848157313,
        "epoch": 0.43632107944375603,
        "step": 3169
    },
    {
        "loss": 2.2928,
        "grad_norm": 1.3526535034179688,
        "learning_rate": 0.00015798211787576495,
        "epoch": 0.4364587635963101,
        "step": 3170
    },
    {
        "loss": 2.0982,
        "grad_norm": 1.7097514867782593,
        "learning_rate": 0.00015787597895963658,
        "epoch": 0.4365964477488641,
        "step": 3171
    },
    {
        "loss": 0.9646,
        "grad_norm": 4.9852070808410645,
        "learning_rate": 0.0001577697419131497,
        "epoch": 0.43673413190141813,
        "step": 3172
    },
    {
        "loss": 2.2882,
        "grad_norm": 1.983538031578064,
        "learning_rate": 0.000157663406916432,
        "epoch": 0.4368718160539722,
        "step": 3173
    },
    {
        "loss": 2.55,
        "grad_norm": 1.1922926902770996,
        "learning_rate": 0.0001575569741497776,
        "epoch": 0.43700950020652624,
        "step": 3174
    },
    {
        "loss": 2.212,
        "grad_norm": 1.1177536249160767,
        "learning_rate": 0.00015745044379364634,
        "epoch": 0.4371471843590803,
        "step": 3175
    },
    {
        "loss": 2.0434,
        "grad_norm": 1.7133055925369263,
        "learning_rate": 0.00015734381602866332,
        "epoch": 0.4372848685116343,
        "step": 3176
    },
    {
        "loss": 2.2966,
        "grad_norm": 2.184295415878296,
        "learning_rate": 0.00015723709103561894,
        "epoch": 0.43742255266418834,
        "step": 3177
    },
    {
        "loss": 1.8588,
        "grad_norm": 1.363425374031067,
        "learning_rate": 0.0001571302689954685,
        "epoch": 0.4375602368167424,
        "step": 3178
    },
    {
        "loss": 2.1469,
        "grad_norm": 2.2962472438812256,
        "learning_rate": 0.00015702335008933177,
        "epoch": 0.43769792096929644,
        "step": 3179
    },
    {
        "loss": 2.1839,
        "grad_norm": 1.2302011251449585,
        "learning_rate": 0.00015691633449849276,
        "epoch": 0.4378356051218505,
        "step": 3180
    },
    {
        "loss": 2.3207,
        "grad_norm": 2.460211992263794,
        "learning_rate": 0.00015680922240439947,
        "epoch": 0.43797328927440454,
        "step": 3181
    },
    {
        "loss": 1.9283,
        "grad_norm": 1.4631242752075195,
        "learning_rate": 0.0001567020139886634,
        "epoch": 0.43811097342695854,
        "step": 3182
    },
    {
        "loss": 1.9152,
        "grad_norm": 1.8083823919296265,
        "learning_rate": 0.00015659470943305952,
        "epoch": 0.4382486575795126,
        "step": 3183
    },
    {
        "loss": 2.214,
        "grad_norm": 2.6607327461242676,
        "learning_rate": 0.00015648730891952575,
        "epoch": 0.43838634173206664,
        "step": 3184
    },
    {
        "loss": 2.2979,
        "grad_norm": 2.0752670764923096,
        "learning_rate": 0.00015637981263016263,
        "epoch": 0.4385240258846207,
        "step": 3185
    },
    {
        "loss": 1.9279,
        "grad_norm": 1.4150464534759521,
        "learning_rate": 0.0001562722207472332,
        "epoch": 0.43866171003717475,
        "step": 3186
    },
    {
        "loss": 1.8074,
        "grad_norm": 1.7757182121276855,
        "learning_rate": 0.0001561645334531626,
        "epoch": 0.43879939418972874,
        "step": 3187
    },
    {
        "loss": 1.861,
        "grad_norm": 2.294783592224121,
        "learning_rate": 0.0001560567509305376,
        "epoch": 0.4389370783422828,
        "step": 3188
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.5646617412567139,
        "learning_rate": 0.00015594887336210648,
        "epoch": 0.43907476249483685,
        "step": 3189
    },
    {
        "loss": 1.9281,
        "grad_norm": 2.4926624298095703,
        "learning_rate": 0.00015584090093077884,
        "epoch": 0.4392124466473909,
        "step": 3190
    },
    {
        "loss": 1.2827,
        "grad_norm": 2.7488794326782227,
        "learning_rate": 0.00015573283381962482,
        "epoch": 0.43935013079994495,
        "step": 3191
    },
    {
        "loss": 2.2146,
        "grad_norm": 1.2271592617034912,
        "learning_rate": 0.00015562467221187545,
        "epoch": 0.43948781495249895,
        "step": 3192
    },
    {
        "loss": 2.4989,
        "grad_norm": 3.033273935317993,
        "learning_rate": 0.00015551641629092165,
        "epoch": 0.439625499105053,
        "step": 3193
    },
    {
        "loss": 2.6686,
        "grad_norm": 1.8104369640350342,
        "learning_rate": 0.00015540806624031442,
        "epoch": 0.43976318325760705,
        "step": 3194
    },
    {
        "loss": 1.5627,
        "grad_norm": 3.3606605529785156,
        "learning_rate": 0.00015529962224376432,
        "epoch": 0.4399008674101611,
        "step": 3195
    },
    {
        "loss": 2.373,
        "grad_norm": 2.900181770324707,
        "learning_rate": 0.00015519108448514135,
        "epoch": 0.44003855156271515,
        "step": 3196
    },
    {
        "loss": 1.9296,
        "grad_norm": 1.9097782373428345,
        "learning_rate": 0.00015508245314847417,
        "epoch": 0.44017623571526915,
        "step": 3197
    },
    {
        "loss": 1.6595,
        "grad_norm": 2.264648675918579,
        "learning_rate": 0.00015497372841795034,
        "epoch": 0.4403139198678232,
        "step": 3198
    },
    {
        "loss": 2.2351,
        "grad_norm": 1.2569023370742798,
        "learning_rate": 0.00015486491047791583,
        "epoch": 0.44045160402037725,
        "step": 3199
    },
    {
        "loss": 1.4653,
        "grad_norm": 2.5942816734313965,
        "learning_rate": 0.0001547559995128744,
        "epoch": 0.4405892881729313,
        "step": 3200
    },
    {
        "loss": 2.3071,
        "grad_norm": 1.5728366374969482,
        "learning_rate": 0.0001546469957074877,
        "epoch": 0.44072697232548536,
        "step": 3201
    },
    {
        "loss": 2.2277,
        "grad_norm": 2.577772378921509,
        "learning_rate": 0.00015453789924657477,
        "epoch": 0.44086465647803935,
        "step": 3202
    },
    {
        "loss": 2.4659,
        "grad_norm": 2.3703091144561768,
        "learning_rate": 0.00015442871031511173,
        "epoch": 0.4410023406305934,
        "step": 3203
    },
    {
        "loss": 1.9936,
        "grad_norm": 1.9862065315246582,
        "learning_rate": 0.00015431942909823154,
        "epoch": 0.44114002478314746,
        "step": 3204
    },
    {
        "loss": 2.3562,
        "grad_norm": 2.070939540863037,
        "learning_rate": 0.0001542100557812236,
        "epoch": 0.4412777089357015,
        "step": 3205
    },
    {
        "loss": 2.1908,
        "grad_norm": 1.2348289489746094,
        "learning_rate": 0.00015410059054953335,
        "epoch": 0.44141539308825556,
        "step": 3206
    },
    {
        "loss": 2.6204,
        "grad_norm": 1.7872346639633179,
        "learning_rate": 0.00015399103358876233,
        "epoch": 0.4415530772408096,
        "step": 3207
    },
    {
        "loss": 2.0782,
        "grad_norm": 2.80332088470459,
        "learning_rate": 0.00015388138508466733,
        "epoch": 0.4416907613933636,
        "step": 3208
    },
    {
        "loss": 2.0067,
        "grad_norm": 1.706368088722229,
        "learning_rate": 0.00015377164522316057,
        "epoch": 0.44182844554591766,
        "step": 3209
    },
    {
        "loss": 1.8476,
        "grad_norm": 1.5965882539749146,
        "learning_rate": 0.00015366181419030914,
        "epoch": 0.4419661296984717,
        "step": 3210
    },
    {
        "loss": 2.4133,
        "grad_norm": 2.3441319465637207,
        "learning_rate": 0.00015355189217233455,
        "epoch": 0.44210381385102576,
        "step": 3211
    },
    {
        "loss": 1.6567,
        "grad_norm": 2.2471039295196533,
        "learning_rate": 0.00015344187935561275,
        "epoch": 0.4422414980035798,
        "step": 3212
    },
    {
        "loss": 1.8904,
        "grad_norm": 1.4226747751235962,
        "learning_rate": 0.00015333177592667362,
        "epoch": 0.4423791821561338,
        "step": 3213
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.517784595489502,
        "learning_rate": 0.00015322158207220049,
        "epoch": 0.44251686630868786,
        "step": 3214
    },
    {
        "loss": 1.5135,
        "grad_norm": 1.6590089797973633,
        "learning_rate": 0.00015311129797903028,
        "epoch": 0.4426545504612419,
        "step": 3215
    },
    {
        "loss": 2.1161,
        "grad_norm": 1.4627161026000977,
        "learning_rate": 0.00015300092383415282,
        "epoch": 0.44279223461379597,
        "step": 3216
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.4938682317733765,
        "learning_rate": 0.0001528904598247105,
        "epoch": 0.44292991876635,
        "step": 3217
    },
    {
        "loss": 2.3831,
        "grad_norm": 1.5819302797317505,
        "learning_rate": 0.00015277990613799815,
        "epoch": 0.443067602918904,
        "step": 3218
    },
    {
        "loss": 2.4366,
        "grad_norm": 2.1536760330200195,
        "learning_rate": 0.00015266926296146277,
        "epoch": 0.44320528707145807,
        "step": 3219
    },
    {
        "loss": 2.0761,
        "grad_norm": 1.51835036277771,
        "learning_rate": 0.0001525585304827029,
        "epoch": 0.4433429712240121,
        "step": 3220
    },
    {
        "loss": 2.2054,
        "grad_norm": 1.7688090801239014,
        "learning_rate": 0.00015244770888946866,
        "epoch": 0.44348065537656617,
        "step": 3221
    },
    {
        "loss": 1.7867,
        "grad_norm": 2.0097951889038086,
        "learning_rate": 0.00015233679836966122,
        "epoch": 0.4436183395291202,
        "step": 3222
    },
    {
        "loss": 1.8589,
        "grad_norm": 1.6873431205749512,
        "learning_rate": 0.00015222579911133236,
        "epoch": 0.4437560236816742,
        "step": 3223
    },
    {
        "loss": 2.369,
        "grad_norm": 2.2365431785583496,
        "learning_rate": 0.00015211471130268465,
        "epoch": 0.44389370783422827,
        "step": 3224
    },
    {
        "loss": 2.4824,
        "grad_norm": 1.4952675104141235,
        "learning_rate": 0.00015200353513207052,
        "epoch": 0.4440313919867823,
        "step": 3225
    },
    {
        "loss": 2.2582,
        "grad_norm": 2.5676496028900146,
        "learning_rate": 0.00015189227078799222,
        "epoch": 0.44416907613933637,
        "step": 3226
    },
    {
        "loss": 1.6926,
        "grad_norm": 1.778268814086914,
        "learning_rate": 0.0001517809184591017,
        "epoch": 0.4443067602918904,
        "step": 3227
    },
    {
        "loss": 2.2905,
        "grad_norm": 1.873349666595459,
        "learning_rate": 0.0001516694783342,
        "epoch": 0.4444444444444444,
        "step": 3228
    },
    {
        "loss": 1.7015,
        "grad_norm": 2.938795804977417,
        "learning_rate": 0.00015155795060223695,
        "epoch": 0.44458212859699847,
        "step": 3229
    },
    {
        "loss": 0.8985,
        "grad_norm": 3.4000842571258545,
        "learning_rate": 0.00015144633545231093,
        "epoch": 0.4447198127495525,
        "step": 3230
    },
    {
        "loss": 2.0559,
        "grad_norm": 2.699035406112671,
        "learning_rate": 0.0001513346330736687,
        "epoch": 0.4448574969021066,
        "step": 3231
    },
    {
        "loss": 2.2893,
        "grad_norm": 1.7888139486312866,
        "learning_rate": 0.00015122284365570466,
        "epoch": 0.4449951810546606,
        "step": 3232
    },
    {
        "loss": 1.9803,
        "grad_norm": 1.8112972974777222,
        "learning_rate": 0.00015111096738796103,
        "epoch": 0.4451328652072146,
        "step": 3233
    },
    {
        "loss": 1.5812,
        "grad_norm": 2.3625869750976562,
        "learning_rate": 0.00015099900446012722,
        "epoch": 0.4452705493597687,
        "step": 3234
    },
    {
        "loss": 1.7366,
        "grad_norm": 1.9472671747207642,
        "learning_rate": 0.00015088695506203941,
        "epoch": 0.4454082335123227,
        "step": 3235
    },
    {
        "loss": 1.8546,
        "grad_norm": 1.6113473176956177,
        "learning_rate": 0.00015077481938368062,
        "epoch": 0.4455459176648768,
        "step": 3236
    },
    {
        "loss": 1.5981,
        "grad_norm": 2.2240521907806396,
        "learning_rate": 0.00015066259761518008,
        "epoch": 0.44568360181743083,
        "step": 3237
    },
    {
        "loss": 1.85,
        "grad_norm": 3.054928779602051,
        "learning_rate": 0.00015055028994681284,
        "epoch": 0.4458212859699849,
        "step": 3238
    },
    {
        "loss": 1.5272,
        "grad_norm": 2.697545289993286,
        "learning_rate": 0.00015043789656899982,
        "epoch": 0.4459589701225389,
        "step": 3239
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.7342090606689453,
        "learning_rate": 0.00015032541767230723,
        "epoch": 0.44609665427509293,
        "step": 3240
    },
    {
        "loss": 2.2345,
        "grad_norm": 1.1596784591674805,
        "learning_rate": 0.00015021285344744617,
        "epoch": 0.446234338427647,
        "step": 3241
    },
    {
        "loss": 1.1023,
        "grad_norm": 2.7206835746765137,
        "learning_rate": 0.00015010020408527238,
        "epoch": 0.44637202258020103,
        "step": 3242
    },
    {
        "loss": 1.8662,
        "grad_norm": 1.5655769109725952,
        "learning_rate": 0.00014998746977678613,
        "epoch": 0.4465097067327551,
        "step": 3243
    },
    {
        "loss": 1.8897,
        "grad_norm": 1.879548192024231,
        "learning_rate": 0.00014987465071313158,
        "epoch": 0.4466473908853091,
        "step": 3244
    },
    {
        "loss": 2.0276,
        "grad_norm": 1.8379309177398682,
        "learning_rate": 0.00014976174708559664,
        "epoch": 0.44678507503786313,
        "step": 3245
    },
    {
        "loss": 1.8816,
        "grad_norm": 2.100717782974243,
        "learning_rate": 0.0001496487590856127,
        "epoch": 0.4469227591904172,
        "step": 3246
    },
    {
        "loss": 1.5161,
        "grad_norm": 2.7045419216156006,
        "learning_rate": 0.00014953568690475395,
        "epoch": 0.44706044334297124,
        "step": 3247
    },
    {
        "loss": 2.395,
        "grad_norm": 1.861332893371582,
        "learning_rate": 0.00014942253073473765,
        "epoch": 0.4471981274955253,
        "step": 3248
    },
    {
        "loss": 2.4202,
        "grad_norm": 1.7390753030776978,
        "learning_rate": 0.0001493092907674232,
        "epoch": 0.4473358116480793,
        "step": 3249
    },
    {
        "loss": 2.755,
        "grad_norm": 2.4471569061279297,
        "learning_rate": 0.00014919596719481208,
        "epoch": 0.44747349580063334,
        "step": 3250
    },
    {
        "loss": 2.3158,
        "grad_norm": 1.6662386655807495,
        "learning_rate": 0.00014908256020904772,
        "epoch": 0.4476111799531874,
        "step": 3251
    },
    {
        "loss": 2.3983,
        "grad_norm": 2.3075268268585205,
        "learning_rate": 0.00014896907000241482,
        "epoch": 0.44774886410574144,
        "step": 3252
    },
    {
        "loss": 1.9436,
        "grad_norm": 1.5499138832092285,
        "learning_rate": 0.00014885549676733932,
        "epoch": 0.4478865482582955,
        "step": 3253
    },
    {
        "loss": 1.8183,
        "grad_norm": 2.5966713428497314,
        "learning_rate": 0.00014874184069638775,
        "epoch": 0.4480242324108495,
        "step": 3254
    },
    {
        "loss": 2.2108,
        "grad_norm": 1.8045053482055664,
        "learning_rate": 0.0001486281019822672,
        "epoch": 0.44816191656340354,
        "step": 3255
    },
    {
        "loss": 2.2112,
        "grad_norm": 2.617608070373535,
        "learning_rate": 0.00014851428081782487,
        "epoch": 0.4482996007159576,
        "step": 3256
    },
    {
        "loss": 2.6314,
        "grad_norm": 1.1214373111724854,
        "learning_rate": 0.00014840037739604787,
        "epoch": 0.44843728486851164,
        "step": 3257
    },
    {
        "loss": 1.9718,
        "grad_norm": 2.1188418865203857,
        "learning_rate": 0.00014828639191006253,
        "epoch": 0.4485749690210657,
        "step": 3258
    },
    {
        "loss": 2.1966,
        "grad_norm": 1.245605230331421,
        "learning_rate": 0.00014817232455313455,
        "epoch": 0.4487126531736197,
        "step": 3259
    },
    {
        "loss": 1.2957,
        "grad_norm": 3.037112236022949,
        "learning_rate": 0.00014805817551866835,
        "epoch": 0.44885033732617374,
        "step": 3260
    },
    {
        "loss": 1.89,
        "grad_norm": 1.6807193756103516,
        "learning_rate": 0.0001479439450002069,
        "epoch": 0.4489880214787278,
        "step": 3261
    },
    {
        "loss": 1.528,
        "grad_norm": 1.786743402481079,
        "learning_rate": 0.0001478296331914311,
        "epoch": 0.44912570563128185,
        "step": 3262
    },
    {
        "loss": 2.4316,
        "grad_norm": 2.3872323036193848,
        "learning_rate": 0.00014771524028616003,
        "epoch": 0.4492633897838359,
        "step": 3263
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.571638584136963,
        "learning_rate": 0.00014760076647834998,
        "epoch": 0.4494010739363899,
        "step": 3264
    },
    {
        "loss": 2.0871,
        "grad_norm": 1.289574146270752,
        "learning_rate": 0.00014748621196209464,
        "epoch": 0.44953875808894395,
        "step": 3265
    },
    {
        "loss": 2.2992,
        "grad_norm": 2.572387933731079,
        "learning_rate": 0.00014737157693162436,
        "epoch": 0.449676442241498,
        "step": 3266
    },
    {
        "loss": 1.9061,
        "grad_norm": 1.5007846355438232,
        "learning_rate": 0.00014725686158130602,
        "epoch": 0.44981412639405205,
        "step": 3267
    },
    {
        "loss": 2.1923,
        "grad_norm": 1.9526457786560059,
        "learning_rate": 0.00014714206610564278,
        "epoch": 0.4499518105466061,
        "step": 3268
    },
    {
        "loss": 1.8843,
        "grad_norm": 1.137133240699768,
        "learning_rate": 0.00014702719069927371,
        "epoch": 0.45008949469916015,
        "step": 3269
    },
    {
        "loss": 2.4,
        "grad_norm": 1.8112220764160156,
        "learning_rate": 0.00014691223555697314,
        "epoch": 0.45022717885171415,
        "step": 3270
    },
    {
        "loss": 1.8426,
        "grad_norm": 1.696041464805603,
        "learning_rate": 0.0001467972008736509,
        "epoch": 0.4503648630042682,
        "step": 3271
    },
    {
        "loss": 2.4696,
        "grad_norm": 1.45905339717865,
        "learning_rate": 0.0001466820868443516,
        "epoch": 0.45050254715682225,
        "step": 3272
    },
    {
        "loss": 2.1573,
        "grad_norm": 1.3821173906326294,
        "learning_rate": 0.00014656689366425416,
        "epoch": 0.4506402313093763,
        "step": 3273
    },
    {
        "loss": 2.4408,
        "grad_norm": 2.40746808052063,
        "learning_rate": 0.00014645162152867192,
        "epoch": 0.45077791546193036,
        "step": 3274
    },
    {
        "loss": 1.9009,
        "grad_norm": 1.1133487224578857,
        "learning_rate": 0.00014633627063305216,
        "epoch": 0.45091559961448435,
        "step": 3275
    },
    {
        "loss": 2.2173,
        "grad_norm": 1.765315294265747,
        "learning_rate": 0.0001462208411729754,
        "epoch": 0.4510532837670384,
        "step": 3276
    },
    {
        "loss": 2.3229,
        "grad_norm": 1.7095197439193726,
        "learning_rate": 0.00014610533334415572,
        "epoch": 0.45119096791959246,
        "step": 3277
    },
    {
        "loss": 1.4575,
        "grad_norm": 3.2448678016662598,
        "learning_rate": 0.00014598974734243987,
        "epoch": 0.4513286520721465,
        "step": 3278
    },
    {
        "loss": 2.3307,
        "grad_norm": 1.104351282119751,
        "learning_rate": 0.00014587408336380706,
        "epoch": 0.45146633622470056,
        "step": 3279
    },
    {
        "loss": 1.7914,
        "grad_norm": 2.2580909729003906,
        "learning_rate": 0.00014575834160436896,
        "epoch": 0.45160402037725456,
        "step": 3280
    },
    {
        "loss": 1.8098,
        "grad_norm": 1.8899286985397339,
        "learning_rate": 0.00014564252226036904,
        "epoch": 0.4517417045298086,
        "step": 3281
    },
    {
        "loss": 1.4675,
        "grad_norm": 2.552875280380249,
        "learning_rate": 0.0001455266255281821,
        "epoch": 0.45187938868236266,
        "step": 3282
    },
    {
        "loss": 2.51,
        "grad_norm": 1.8566608428955078,
        "learning_rate": 0.00014541065160431443,
        "epoch": 0.4520170728349167,
        "step": 3283
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.979743242263794,
        "learning_rate": 0.0001452946006854032,
        "epoch": 0.45215475698747076,
        "step": 3284
    },
    {
        "loss": 2.6561,
        "grad_norm": 1.2518820762634277,
        "learning_rate": 0.00014517847296821587,
        "epoch": 0.45229244114002476,
        "step": 3285
    },
    {
        "loss": 1.9099,
        "grad_norm": 1.3590648174285889,
        "learning_rate": 0.00014506226864965027,
        "epoch": 0.4524301252925788,
        "step": 3286
    },
    {
        "loss": 2.6088,
        "grad_norm": 1.1723086833953857,
        "learning_rate": 0.0001449459879267342,
        "epoch": 0.45256780944513286,
        "step": 3287
    },
    {
        "loss": 1.4878,
        "grad_norm": 3.219491720199585,
        "learning_rate": 0.00014482963099662484,
        "epoch": 0.4527054935976869,
        "step": 3288
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.664934515953064,
        "learning_rate": 0.0001447131980566087,
        "epoch": 0.45284317775024097,
        "step": 3289
    },
    {
        "loss": 2.531,
        "grad_norm": 1.3389955759048462,
        "learning_rate": 0.00014459668930410114,
        "epoch": 0.45298086190279496,
        "step": 3290
    },
    {
        "loss": 2.27,
        "grad_norm": 1.486629605293274,
        "learning_rate": 0.0001444801049366459,
        "epoch": 0.453118546055349,
        "step": 3291
    },
    {
        "loss": 1.9811,
        "grad_norm": 2.3241302967071533,
        "learning_rate": 0.00014436344515191528,
        "epoch": 0.45325623020790307,
        "step": 3292
    },
    {
        "loss": 2.1405,
        "grad_norm": 2.2740063667297363,
        "learning_rate": 0.00014424671014770906,
        "epoch": 0.4533939143604571,
        "step": 3293
    },
    {
        "loss": 2.5154,
        "grad_norm": 1.082295298576355,
        "learning_rate": 0.0001441299001219548,
        "epoch": 0.45353159851301117,
        "step": 3294
    },
    {
        "loss": 1.9728,
        "grad_norm": 1.6165517568588257,
        "learning_rate": 0.00014401301527270733,
        "epoch": 0.4536692826655652,
        "step": 3295
    },
    {
        "loss": 2.1097,
        "grad_norm": 2.1402342319488525,
        "learning_rate": 0.00014389605579814802,
        "epoch": 0.4538069668181192,
        "step": 3296
    },
    {
        "loss": 2.2364,
        "grad_norm": 1.332167625427246,
        "learning_rate": 0.00014377902189658512,
        "epoch": 0.45394465097067327,
        "step": 3297
    },
    {
        "loss": 1.5459,
        "grad_norm": 1.5996977090835571,
        "learning_rate": 0.00014366191376645284,
        "epoch": 0.4540823351232273,
        "step": 3298
    },
    {
        "loss": 2.3066,
        "grad_norm": 1.383095622062683,
        "learning_rate": 0.0001435447316063113,
        "epoch": 0.4542200192757814,
        "step": 3299
    },
    {
        "loss": 1.7789,
        "grad_norm": 1.7778671979904175,
        "learning_rate": 0.0001434274756148462,
        "epoch": 0.4543577034283354,
        "step": 3300
    },
    {
        "loss": 1.4902,
        "grad_norm": 2.0672290325164795,
        "learning_rate": 0.00014331014599086844,
        "epoch": 0.4544953875808894,
        "step": 3301
    },
    {
        "loss": 2.5716,
        "grad_norm": 1.6046202182769775,
        "learning_rate": 0.00014319274293331366,
        "epoch": 0.4546330717334435,
        "step": 3302
    },
    {
        "loss": 1.7343,
        "grad_norm": 2.031158924102783,
        "learning_rate": 0.000143075266641242,
        "epoch": 0.4547707558859975,
        "step": 3303
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.8388346433639526,
        "learning_rate": 0.00014295771731383797,
        "epoch": 0.4549084400385516,
        "step": 3304
    },
    {
        "loss": 2.0702,
        "grad_norm": 1.6865265369415283,
        "learning_rate": 0.00014284009515040967,
        "epoch": 0.4550461241911056,
        "step": 3305
    },
    {
        "loss": 2.5075,
        "grad_norm": 1.6921117305755615,
        "learning_rate": 0.00014272240035038887,
        "epoch": 0.4551838083436596,
        "step": 3306
    },
    {
        "loss": 1.9942,
        "grad_norm": 1.4481027126312256,
        "learning_rate": 0.00014260463311333043,
        "epoch": 0.4553214924962137,
        "step": 3307
    },
    {
        "loss": 2.4706,
        "grad_norm": 1.5019422769546509,
        "learning_rate": 0.00014248679363891196,
        "epoch": 0.4554591766487677,
        "step": 3308
    },
    {
        "loss": 2.222,
        "grad_norm": 1.551483392715454,
        "learning_rate": 0.00014236888212693373,
        "epoch": 0.4555968608013218,
        "step": 3309
    },
    {
        "loss": 2.4364,
        "grad_norm": 1.5177277326583862,
        "learning_rate": 0.00014225089877731797,
        "epoch": 0.45573454495387583,
        "step": 3310
    },
    {
        "loss": 2.0939,
        "grad_norm": 1.5618066787719727,
        "learning_rate": 0.00014213284379010875,
        "epoch": 0.4558722291064298,
        "step": 3311
    },
    {
        "loss": 1.6894,
        "grad_norm": 1.6805163621902466,
        "learning_rate": 0.0001420147173654717,
        "epoch": 0.4560099132589839,
        "step": 3312
    },
    {
        "loss": 2.5935,
        "grad_norm": 1.9059879779815674,
        "learning_rate": 0.0001418965197036935,
        "epoch": 0.45614759741153793,
        "step": 3313
    },
    {
        "loss": 2.1481,
        "grad_norm": 1.4571822881698608,
        "learning_rate": 0.00014177825100518168,
        "epoch": 0.456285281564092,
        "step": 3314
    },
    {
        "loss": 2.1433,
        "grad_norm": 1.5115547180175781,
        "learning_rate": 0.00014165991147046403,
        "epoch": 0.45642296571664603,
        "step": 3315
    },
    {
        "loss": 1.7939,
        "grad_norm": 2.467132806777954,
        "learning_rate": 0.00014154150130018872,
        "epoch": 0.45656064986920003,
        "step": 3316
    },
    {
        "loss": 1.678,
        "grad_norm": 2.6146183013916016,
        "learning_rate": 0.00014142302069512335,
        "epoch": 0.4566983340217541,
        "step": 3317
    },
    {
        "loss": 1.5866,
        "grad_norm": 2.9422831535339355,
        "learning_rate": 0.00014130446985615534,
        "epoch": 0.45683601817430813,
        "step": 3318
    },
    {
        "loss": 1.3033,
        "grad_norm": 3.080273151397705,
        "learning_rate": 0.0001411858489842909,
        "epoch": 0.4569737023268622,
        "step": 3319
    },
    {
        "loss": 1.6206,
        "grad_norm": 1.682668924331665,
        "learning_rate": 0.00014106715828065506,
        "epoch": 0.45711138647941624,
        "step": 3320
    },
    {
        "loss": 1.9502,
        "grad_norm": 2.3588180541992188,
        "learning_rate": 0.00014094839794649136,
        "epoch": 0.45724907063197023,
        "step": 3321
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.4971574544906616,
        "learning_rate": 0.00014082956818316122,
        "epoch": 0.4573867547845243,
        "step": 3322
    },
    {
        "loss": 2.6869,
        "grad_norm": 3.4850542545318604,
        "learning_rate": 0.00014071066919214378,
        "epoch": 0.45752443893707834,
        "step": 3323
    },
    {
        "loss": 2.1134,
        "grad_norm": 2.002042531967163,
        "learning_rate": 0.00014059170117503578,
        "epoch": 0.4576621230896324,
        "step": 3324
    },
    {
        "loss": 1.7401,
        "grad_norm": 2.420348882675171,
        "learning_rate": 0.00014047266433355084,
        "epoch": 0.45779980724218644,
        "step": 3325
    },
    {
        "loss": 1.737,
        "grad_norm": 3.6519851684570312,
        "learning_rate": 0.00014035355886951923,
        "epoch": 0.4579374913947405,
        "step": 3326
    },
    {
        "loss": 1.2796,
        "grad_norm": 2.1407065391540527,
        "learning_rate": 0.00014023438498488765,
        "epoch": 0.4580751755472945,
        "step": 3327
    },
    {
        "loss": 2.3401,
        "grad_norm": 2.370474338531494,
        "learning_rate": 0.00014011514288171886,
        "epoch": 0.45821285969984854,
        "step": 3328
    },
    {
        "loss": 2.3673,
        "grad_norm": 1.6427574157714844,
        "learning_rate": 0.0001399958327621911,
        "epoch": 0.4583505438524026,
        "step": 3329
    },
    {
        "loss": 1.8882,
        "grad_norm": 1.5942896604537964,
        "learning_rate": 0.00013987645482859814,
        "epoch": 0.45848822800495664,
        "step": 3330
    },
    {
        "loss": 1.3874,
        "grad_norm": 3.700853109359741,
        "learning_rate": 0.0001397570092833486,
        "epoch": 0.4586259121575107,
        "step": 3331
    },
    {
        "loss": 1.8001,
        "grad_norm": 2.6981003284454346,
        "learning_rate": 0.0001396374963289657,
        "epoch": 0.4587635963100647,
        "step": 3332
    },
    {
        "loss": 1.9898,
        "grad_norm": 2.1224467754364014,
        "learning_rate": 0.0001395179161680873,
        "epoch": 0.45890128046261874,
        "step": 3333
    },
    {
        "loss": 2.0697,
        "grad_norm": 1.3694584369659424,
        "learning_rate": 0.0001393982690034647,
        "epoch": 0.4590389646151728,
        "step": 3334
    },
    {
        "loss": 2.3556,
        "grad_norm": 2.313136100769043,
        "learning_rate": 0.00013927855503796305,
        "epoch": 0.45917664876772685,
        "step": 3335
    },
    {
        "loss": 1.7686,
        "grad_norm": 2.640009880065918,
        "learning_rate": 0.0001391587744745609,
        "epoch": 0.4593143329202809,
        "step": 3336
    },
    {
        "loss": 1.7425,
        "grad_norm": 1.4162888526916504,
        "learning_rate": 0.00013903892751634947,
        "epoch": 0.4594520170728349,
        "step": 3337
    },
    {
        "loss": 1.8683,
        "grad_norm": 1.1334651708602905,
        "learning_rate": 0.00013891901436653277,
        "epoch": 0.45958970122538895,
        "step": 3338
    },
    {
        "loss": 2.4426,
        "grad_norm": 1.5325387716293335,
        "learning_rate": 0.0001387990352284269,
        "epoch": 0.459727385377943,
        "step": 3339
    },
    {
        "loss": 1.8807,
        "grad_norm": 2.4021308422088623,
        "learning_rate": 0.00013867899030545972,
        "epoch": 0.45986506953049705,
        "step": 3340
    },
    {
        "loss": 2.5179,
        "grad_norm": 1.8944146633148193,
        "learning_rate": 0.00013855887980117096,
        "epoch": 0.4600027536830511,
        "step": 3341
    },
    {
        "loss": 1.4631,
        "grad_norm": 2.5457706451416016,
        "learning_rate": 0.00013843870391921145,
        "epoch": 0.4601404378356051,
        "step": 3342
    },
    {
        "loss": 1.6842,
        "grad_norm": 2.4914731979370117,
        "learning_rate": 0.00013831846286334256,
        "epoch": 0.46027812198815915,
        "step": 3343
    },
    {
        "loss": 2.4445,
        "grad_norm": 1.4034154415130615,
        "learning_rate": 0.00013819815683743653,
        "epoch": 0.4604158061407132,
        "step": 3344
    },
    {
        "loss": 1.9384,
        "grad_norm": 2.1959667205810547,
        "learning_rate": 0.0001380777860454757,
        "epoch": 0.46055349029326725,
        "step": 3345
    },
    {
        "loss": 1.6602,
        "grad_norm": 2.496983289718628,
        "learning_rate": 0.0001379573506915521,
        "epoch": 0.4606911744458213,
        "step": 3346
    },
    {
        "loss": 1.8,
        "grad_norm": 1.2950170040130615,
        "learning_rate": 0.00013783685097986718,
        "epoch": 0.4608288585983753,
        "step": 3347
    },
    {
        "loss": 2.3487,
        "grad_norm": 1.624336838722229,
        "learning_rate": 0.00013771628711473175,
        "epoch": 0.46096654275092935,
        "step": 3348
    },
    {
        "loss": 1.572,
        "grad_norm": 2.051802396774292,
        "learning_rate": 0.0001375956593005651,
        "epoch": 0.4611042269034834,
        "step": 3349
    },
    {
        "loss": 1.9734,
        "grad_norm": 2.648655414581299,
        "learning_rate": 0.00013747496774189528,
        "epoch": 0.46124191105603746,
        "step": 3350
    },
    {
        "loss": 2.3093,
        "grad_norm": 2.0988006591796875,
        "learning_rate": 0.0001373542126433581,
        "epoch": 0.4613795952085915,
        "step": 3351
    },
    {
        "loss": 1.4547,
        "grad_norm": 2.4053235054016113,
        "learning_rate": 0.00013723339420969725,
        "epoch": 0.4615172793611455,
        "step": 3352
    },
    {
        "loss": 1.6419,
        "grad_norm": 1.125645637512207,
        "learning_rate": 0.00013711251264576384,
        "epoch": 0.46165496351369956,
        "step": 3353
    },
    {
        "loss": 1.4471,
        "grad_norm": 2.8194868564605713,
        "learning_rate": 0.00013699156815651602,
        "epoch": 0.4617926476662536,
        "step": 3354
    },
    {
        "loss": 2.2032,
        "grad_norm": 1.6202118396759033,
        "learning_rate": 0.0001368705609470185,
        "epoch": 0.46193033181880766,
        "step": 3355
    },
    {
        "loss": 1.4416,
        "grad_norm": 1.9618124961853027,
        "learning_rate": 0.00013674949122244247,
        "epoch": 0.4620680159713617,
        "step": 3356
    },
    {
        "loss": 1.5164,
        "grad_norm": 1.847550868988037,
        "learning_rate": 0.00013662835918806514,
        "epoch": 0.46220570012391576,
        "step": 3357
    },
    {
        "loss": 1.7467,
        "grad_norm": 1.8717124462127686,
        "learning_rate": 0.00013650716504926921,
        "epoch": 0.46234338427646976,
        "step": 3358
    },
    {
        "loss": 2.5411,
        "grad_norm": 2.2563369274139404,
        "learning_rate": 0.00013638590901154276,
        "epoch": 0.4624810684290238,
        "step": 3359
    },
    {
        "loss": 2.3072,
        "grad_norm": 2.1491119861602783,
        "learning_rate": 0.00013626459128047888,
        "epoch": 0.46261875258157786,
        "step": 3360
    },
    {
        "loss": 1.5704,
        "grad_norm": 2.6799402236938477,
        "learning_rate": 0.00013614321206177515,
        "epoch": 0.4627564367341319,
        "step": 3361
    },
    {
        "loss": 2.0557,
        "grad_norm": 1.568202257156372,
        "learning_rate": 0.0001360217715612336,
        "epoch": 0.46289412088668597,
        "step": 3362
    },
    {
        "loss": 1.4068,
        "grad_norm": 2.7722136974334717,
        "learning_rate": 0.00013590026998475991,
        "epoch": 0.46303180503923996,
        "step": 3363
    },
    {
        "loss": 1.3966,
        "grad_norm": 1.710114598274231,
        "learning_rate": 0.00013577870753836343,
        "epoch": 0.463169489191794,
        "step": 3364
    },
    {
        "loss": 1.6429,
        "grad_norm": 3.3054628372192383,
        "learning_rate": 0.00013565708442815676,
        "epoch": 0.46330717334434807,
        "step": 3365
    },
    {
        "loss": 2.1839,
        "grad_norm": 2.310131311416626,
        "learning_rate": 0.00013553540086035538,
        "epoch": 0.4634448574969021,
        "step": 3366
    },
    {
        "loss": 2.1794,
        "grad_norm": 2.7198333740234375,
        "learning_rate": 0.00013541365704127712,
        "epoch": 0.46358254164945617,
        "step": 3367
    },
    {
        "loss": 2.4788,
        "grad_norm": 1.7366225719451904,
        "learning_rate": 0.00013529185317734212,
        "epoch": 0.46372022580201017,
        "step": 3368
    },
    {
        "loss": 2.1795,
        "grad_norm": 1.6230695247650146,
        "learning_rate": 0.00013516998947507228,
        "epoch": 0.4638579099545642,
        "step": 3369
    },
    {
        "loss": 2.0397,
        "grad_norm": 2.657895803451538,
        "learning_rate": 0.000135048066141091,
        "epoch": 0.46399559410711827,
        "step": 3370
    },
    {
        "loss": 2.2136,
        "grad_norm": 1.7938237190246582,
        "learning_rate": 0.0001349260833821226,
        "epoch": 0.4641332782596723,
        "step": 3371
    },
    {
        "loss": 2.0017,
        "grad_norm": 1.1709283590316772,
        "learning_rate": 0.00013480404140499245,
        "epoch": 0.4642709624122264,
        "step": 3372
    },
    {
        "loss": 2.0413,
        "grad_norm": 1.9364113807678223,
        "learning_rate": 0.00013468194041662602,
        "epoch": 0.46440864656478037,
        "step": 3373
    },
    {
        "loss": 2.4389,
        "grad_norm": 2.0497937202453613,
        "learning_rate": 0.00013455978062404915,
        "epoch": 0.4645463307173344,
        "step": 3374
    },
    {
        "loss": 1.2534,
        "grad_norm": 1.996881365776062,
        "learning_rate": 0.00013443756223438714,
        "epoch": 0.4646840148698885,
        "step": 3375
    },
    {
        "loss": 1.8998,
        "grad_norm": 2.3518402576446533,
        "learning_rate": 0.00013431528545486464,
        "epoch": 0.4648216990224425,
        "step": 3376
    },
    {
        "loss": 1.8934,
        "grad_norm": 2.1910858154296875,
        "learning_rate": 0.00013419295049280555,
        "epoch": 0.4649593831749966,
        "step": 3377
    },
    {
        "loss": 1.1411,
        "grad_norm": 2.8348588943481445,
        "learning_rate": 0.00013407055755563214,
        "epoch": 0.4650970673275506,
        "step": 3378
    },
    {
        "loss": 2.2578,
        "grad_norm": 1.5975360870361328,
        "learning_rate": 0.0001339481068508651,
        "epoch": 0.4652347514801046,
        "step": 3379
    },
    {
        "loss": 1.345,
        "grad_norm": 2.6338882446289062,
        "learning_rate": 0.00013382559858612317,
        "epoch": 0.4653724356326587,
        "step": 3380
    },
    {
        "loss": 1.834,
        "grad_norm": 1.3278828859329224,
        "learning_rate": 0.00013370303296912246,
        "epoch": 0.46551011978521273,
        "step": 3381
    },
    {
        "loss": 2.7054,
        "grad_norm": 1.6328221559524536,
        "learning_rate": 0.0001335804102076765,
        "epoch": 0.4656478039377668,
        "step": 3382
    },
    {
        "loss": 2.2542,
        "grad_norm": 3.028822183609009,
        "learning_rate": 0.00013345773050969567,
        "epoch": 0.46578548809032083,
        "step": 3383
    },
    {
        "loss": 2.0021,
        "grad_norm": 1.846795678138733,
        "learning_rate": 0.00013333499408318676,
        "epoch": 0.46592317224287483,
        "step": 3384
    },
    {
        "loss": 2.4587,
        "grad_norm": 0.9855576157569885,
        "learning_rate": 0.00013321220113625293,
        "epoch": 0.4660608563954289,
        "step": 3385
    },
    {
        "loss": 2.3663,
        "grad_norm": 1.3430336713790894,
        "learning_rate": 0.00013308935187709314,
        "epoch": 0.46619854054798293,
        "step": 3386
    },
    {
        "loss": 1.5654,
        "grad_norm": 2.9885506629943848,
        "learning_rate": 0.0001329664465140017,
        "epoch": 0.466336224700537,
        "step": 3387
    },
    {
        "loss": 2.4747,
        "grad_norm": 1.3425748348236084,
        "learning_rate": 0.00013284348525536806,
        "epoch": 0.46647390885309103,
        "step": 3388
    },
    {
        "loss": 2.0276,
        "grad_norm": 2.450371265411377,
        "learning_rate": 0.00013272046830967673,
        "epoch": 0.46661159300564503,
        "step": 3389
    },
    {
        "loss": 1.4697,
        "grad_norm": 1.789646029472351,
        "learning_rate": 0.0001325973958855062,
        "epoch": 0.4667492771581991,
        "step": 3390
    },
    {
        "loss": 1.9175,
        "grad_norm": 2.6507480144500732,
        "learning_rate": 0.00013247426819152936,
        "epoch": 0.46688696131075313,
        "step": 3391
    },
    {
        "loss": 1.8679,
        "grad_norm": 1.8804476261138916,
        "learning_rate": 0.00013235108543651272,
        "epoch": 0.4670246454633072,
        "step": 3392
    },
    {
        "loss": 2.3279,
        "grad_norm": 2.1158783435821533,
        "learning_rate": 0.000132227847829316,
        "epoch": 0.46716232961586124,
        "step": 3393
    },
    {
        "loss": 2.005,
        "grad_norm": 2.1336216926574707,
        "learning_rate": 0.0001321045555788922,
        "epoch": 0.46730001376841523,
        "step": 3394
    },
    {
        "loss": 2.2295,
        "grad_norm": 1.504749059677124,
        "learning_rate": 0.00013198120889428675,
        "epoch": 0.4674376979209693,
        "step": 3395
    },
    {
        "loss": 1.519,
        "grad_norm": 2.897700071334839,
        "learning_rate": 0.00013185780798463735,
        "epoch": 0.46757538207352334,
        "step": 3396
    },
    {
        "loss": 2.1669,
        "grad_norm": 1.1964586973190308,
        "learning_rate": 0.00013173435305917383,
        "epoch": 0.4677130662260774,
        "step": 3397
    },
    {
        "loss": 2.5396,
        "grad_norm": 1.4389647245407104,
        "learning_rate": 0.00013161084432721757,
        "epoch": 0.46785075037863144,
        "step": 3398
    },
    {
        "loss": 2.1637,
        "grad_norm": 2.1983394622802734,
        "learning_rate": 0.000131487281998181,
        "epoch": 0.46798843453118544,
        "step": 3399
    },
    {
        "loss": 2.066,
        "grad_norm": 1.9413800239562988,
        "learning_rate": 0.00013136366628156753,
        "epoch": 0.4681261186837395,
        "step": 3400
    },
    {
        "loss": 2.0038,
        "grad_norm": 2.829035997390747,
        "learning_rate": 0.0001312399973869712,
        "epoch": 0.46826380283629354,
        "step": 3401
    },
    {
        "loss": 2.578,
        "grad_norm": 1.7349356412887573,
        "learning_rate": 0.00013111627552407602,
        "epoch": 0.4684014869888476,
        "step": 3402
    },
    {
        "loss": 2.2544,
        "grad_norm": 2.409747362136841,
        "learning_rate": 0.00013099250090265594,
        "epoch": 0.46853917114140164,
        "step": 3403
    },
    {
        "loss": 2.7906,
        "grad_norm": 1.0257971286773682,
        "learning_rate": 0.00013086867373257442,
        "epoch": 0.46867685529395564,
        "step": 3404
    },
    {
        "loss": 1.5031,
        "grad_norm": 2.1449356079101562,
        "learning_rate": 0.0001307447942237838,
        "epoch": 0.4688145394465097,
        "step": 3405
    },
    {
        "loss": 1.7299,
        "grad_norm": 2.5900626182556152,
        "learning_rate": 0.00013062086258632539,
        "epoch": 0.46895222359906374,
        "step": 3406
    },
    {
        "loss": 1.9535,
        "grad_norm": 2.217921733856201,
        "learning_rate": 0.0001304968790303287,
        "epoch": 0.4690899077516178,
        "step": 3407
    },
    {
        "loss": 1.2532,
        "grad_norm": 3.0445408821105957,
        "learning_rate": 0.00013037284376601137,
        "epoch": 0.46922759190417185,
        "step": 3408
    },
    {
        "loss": 2.4185,
        "grad_norm": 1.073465347290039,
        "learning_rate": 0.00013024875700367873,
        "epoch": 0.46936527605672584,
        "step": 3409
    },
    {
        "loss": 2.2866,
        "grad_norm": 1.81563401222229,
        "learning_rate": 0.00013012461895372344,
        "epoch": 0.4695029602092799,
        "step": 3410
    },
    {
        "loss": 1.9927,
        "grad_norm": 2.4738609790802,
        "learning_rate": 0.000130000429826625,
        "epoch": 0.46964064436183395,
        "step": 3411
    },
    {
        "loss": 1.127,
        "grad_norm": 2.638418436050415,
        "learning_rate": 0.00012987618983294956,
        "epoch": 0.469778328514388,
        "step": 3412
    },
    {
        "loss": 2.2152,
        "grad_norm": 1.3939439058303833,
        "learning_rate": 0.0001297518991833496,
        "epoch": 0.46991601266694205,
        "step": 3413
    },
    {
        "loss": 1.3191,
        "grad_norm": 2.608276605606079,
        "learning_rate": 0.0001296275580885634,
        "epoch": 0.4700536968194961,
        "step": 3414
    },
    {
        "loss": 2.225,
        "grad_norm": 2.16089129447937,
        "learning_rate": 0.00012950316675941487,
        "epoch": 0.4701913809720501,
        "step": 3415
    },
    {
        "loss": 1.1492,
        "grad_norm": 2.664581537246704,
        "learning_rate": 0.00012937872540681289,
        "epoch": 0.47032906512460415,
        "step": 3416
    },
    {
        "loss": 2.3442,
        "grad_norm": 1.2492374181747437,
        "learning_rate": 0.0001292542342417514,
        "epoch": 0.4704667492771582,
        "step": 3417
    },
    {
        "loss": 2.2143,
        "grad_norm": 2.5400314331054688,
        "learning_rate": 0.0001291296934753087,
        "epoch": 0.47060443342971225,
        "step": 3418
    },
    {
        "loss": 1.7596,
        "grad_norm": 2.475440740585327,
        "learning_rate": 0.00012900510331864714,
        "epoch": 0.4707421175822663,
        "step": 3419
    },
    {
        "loss": 2.2588,
        "grad_norm": 1.49210524559021,
        "learning_rate": 0.00012888046398301284,
        "epoch": 0.4708798017348203,
        "step": 3420
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.2100356817245483,
        "learning_rate": 0.00012875577567973535,
        "epoch": 0.47101748588737435,
        "step": 3421
    },
    {
        "loss": 2.1758,
        "grad_norm": 2.008613109588623,
        "learning_rate": 0.00012863103862022714,
        "epoch": 0.4711551700399284,
        "step": 3422
    },
    {
        "loss": 1.8144,
        "grad_norm": 2.952500104904175,
        "learning_rate": 0.00012850625301598356,
        "epoch": 0.47129285419248246,
        "step": 3423
    },
    {
        "loss": 2.0108,
        "grad_norm": 1.8598048686981201,
        "learning_rate": 0.00012838141907858206,
        "epoch": 0.4714305383450365,
        "step": 3424
    },
    {
        "loss": 1.9773,
        "grad_norm": 1.9367314577102661,
        "learning_rate": 0.00012825653701968205,
        "epoch": 0.4715682224975905,
        "step": 3425
    },
    {
        "loss": 2.2132,
        "grad_norm": 1.4944413900375366,
        "learning_rate": 0.00012813160705102457,
        "epoch": 0.47170590665014456,
        "step": 3426
    },
    {
        "loss": 2.4118,
        "grad_norm": 1.5138154029846191,
        "learning_rate": 0.00012800662938443208,
        "epoch": 0.4718435908026986,
        "step": 3427
    },
    {
        "loss": 2.3918,
        "grad_norm": 1.4747021198272705,
        "learning_rate": 0.00012788160423180762,
        "epoch": 0.47198127495525266,
        "step": 3428
    },
    {
        "loss": 2.1301,
        "grad_norm": 1.6785720586776733,
        "learning_rate": 0.00012775653180513484,
        "epoch": 0.4721189591078067,
        "step": 3429
    },
    {
        "loss": 2.3236,
        "grad_norm": 2.1745352745056152,
        "learning_rate": 0.00012763141231647777,
        "epoch": 0.4722566432603607,
        "step": 3430
    },
    {
        "loss": 1.6595,
        "grad_norm": 1.4441249370574951,
        "learning_rate": 0.0001275062459779799,
        "epoch": 0.47239432741291476,
        "step": 3431
    },
    {
        "loss": 1.7341,
        "grad_norm": 2.2067511081695557,
        "learning_rate": 0.0001273810330018642,
        "epoch": 0.4725320115654688,
        "step": 3432
    },
    {
        "loss": 1.773,
        "grad_norm": 2.643303632736206,
        "learning_rate": 0.00012725577360043298,
        "epoch": 0.47266969571802286,
        "step": 3433
    },
    {
        "loss": 2.4451,
        "grad_norm": 1.5294462442398071,
        "learning_rate": 0.000127130467986067,
        "epoch": 0.4728073798705769,
        "step": 3434
    },
    {
        "loss": 2.0555,
        "grad_norm": 2.8654792308807373,
        "learning_rate": 0.00012700511637122553,
        "epoch": 0.4729450640231309,
        "step": 3435
    },
    {
        "loss": 1.9839,
        "grad_norm": 1.9612348079681396,
        "learning_rate": 0.0001268797189684458,
        "epoch": 0.47308274817568496,
        "step": 3436
    },
    {
        "loss": 2.1366,
        "grad_norm": 2.135403633117676,
        "learning_rate": 0.0001267542759903425,
        "epoch": 0.473220432328239,
        "step": 3437
    },
    {
        "loss": 2.1396,
        "grad_norm": 2.1585350036621094,
        "learning_rate": 0.00012662878764960786,
        "epoch": 0.47335811648079307,
        "step": 3438
    },
    {
        "loss": 2.1983,
        "grad_norm": 1.9147682189941406,
        "learning_rate": 0.00012650325415901095,
        "epoch": 0.4734958006333471,
        "step": 3439
    },
    {
        "loss": 1.8736,
        "grad_norm": 1.5123872756958008,
        "learning_rate": 0.00012637767573139727,
        "epoch": 0.4736334847859011,
        "step": 3440
    },
    {
        "loss": 2.5893,
        "grad_norm": 1.3616197109222412,
        "learning_rate": 0.0001262520525796886,
        "epoch": 0.47377116893845517,
        "step": 3441
    },
    {
        "loss": 1.7732,
        "grad_norm": 2.770789384841919,
        "learning_rate": 0.00012612638491688263,
        "epoch": 0.4739088530910092,
        "step": 3442
    },
    {
        "loss": 2.0062,
        "grad_norm": 1.6521806716918945,
        "learning_rate": 0.0001260006729560524,
        "epoch": 0.47404653724356327,
        "step": 3443
    },
    {
        "loss": 0.9865,
        "grad_norm": 2.5436208248138428,
        "learning_rate": 0.00012587491691034598,
        "epoch": 0.4741842213961173,
        "step": 3444
    },
    {
        "loss": 1.8029,
        "grad_norm": 1.864719033241272,
        "learning_rate": 0.00012574911699298648,
        "epoch": 0.4743219055486714,
        "step": 3445
    },
    {
        "loss": 2.9286,
        "grad_norm": 1.360909104347229,
        "learning_rate": 0.00012562327341727106,
        "epoch": 0.47445958970122537,
        "step": 3446
    },
    {
        "loss": 1.2998,
        "grad_norm": 2.5609333515167236,
        "learning_rate": 0.00012549738639657115,
        "epoch": 0.4745972738537794,
        "step": 3447
    },
    {
        "loss": 1.7387,
        "grad_norm": 2.804410219192505,
        "learning_rate": 0.00012537145614433177,
        "epoch": 0.4747349580063335,
        "step": 3448
    },
    {
        "loss": 1.9846,
        "grad_norm": 2.674100875854492,
        "learning_rate": 0.00012524548287407112,
        "epoch": 0.4748726421588875,
        "step": 3449
    },
    {
        "loss": 2.2671,
        "grad_norm": 1.9230895042419434,
        "learning_rate": 0.00012511946679938046,
        "epoch": 0.4750103263114416,
        "step": 3450
    },
    {
        "loss": 2.2327,
        "grad_norm": 1.0015027523040771,
        "learning_rate": 0.00012499340813392377,
        "epoch": 0.4751480104639956,
        "step": 3451
    },
    {
        "loss": 1.8134,
        "grad_norm": 2.4979281425476074,
        "learning_rate": 0.00012486730709143688,
        "epoch": 0.4752856946165496,
        "step": 3452
    },
    {
        "loss": 2.3161,
        "grad_norm": 1.3812285661697388,
        "learning_rate": 0.00012474116388572773,
        "epoch": 0.4754233787691037,
        "step": 3453
    },
    {
        "loss": 1.4214,
        "grad_norm": 2.922912836074829,
        "learning_rate": 0.00012461497873067582,
        "epoch": 0.47556106292165773,
        "step": 3454
    },
    {
        "loss": 2.0821,
        "grad_norm": 2.0897443294525146,
        "learning_rate": 0.00012448875184023156,
        "epoch": 0.4756987470742118,
        "step": 3455
    },
    {
        "loss": 2.4029,
        "grad_norm": 1.6871699094772339,
        "learning_rate": 0.00012436248342841615,
        "epoch": 0.4758364312267658,
        "step": 3456
    },
    {
        "loss": 1.6953,
        "grad_norm": 1.7902343273162842,
        "learning_rate": 0.00012423617370932127,
        "epoch": 0.47597411537931983,
        "step": 3457
    },
    {
        "loss": 2.3702,
        "grad_norm": 1.4765363931655884,
        "learning_rate": 0.00012410982289710862,
        "epoch": 0.4761117995318739,
        "step": 3458
    },
    {
        "loss": 1.4413,
        "grad_norm": 2.998655319213867,
        "learning_rate": 0.00012398343120600968,
        "epoch": 0.47624948368442793,
        "step": 3459
    },
    {
        "loss": 2.2092,
        "grad_norm": 1.9056057929992676,
        "learning_rate": 0.00012385699885032506,
        "epoch": 0.476387167836982,
        "step": 3460
    },
    {
        "loss": 1.9967,
        "grad_norm": 2.1648125648498535,
        "learning_rate": 0.0001237305260444243,
        "epoch": 0.476524851989536,
        "step": 3461
    },
    {
        "loss": 2.2542,
        "grad_norm": 1.666857123374939,
        "learning_rate": 0.00012360401300274583,
        "epoch": 0.47666253614209003,
        "step": 3462
    },
    {
        "loss": 1.5765,
        "grad_norm": 3.0431244373321533,
        "learning_rate": 0.00012347745993979587,
        "epoch": 0.4768002202946441,
        "step": 3463
    },
    {
        "loss": 2.3696,
        "grad_norm": 2.0682153701782227,
        "learning_rate": 0.00012335086707014882,
        "epoch": 0.47693790444719814,
        "step": 3464
    },
    {
        "loss": 1.9301,
        "grad_norm": 1.80072021484375,
        "learning_rate": 0.00012322423460844653,
        "epoch": 0.4770755885997522,
        "step": 3465
    },
    {
        "loss": 2.2585,
        "grad_norm": 1.911603569984436,
        "learning_rate": 0.00012309756276939778,
        "epoch": 0.4772132727523062,
        "step": 3466
    },
    {
        "loss": 1.8265,
        "grad_norm": 1.369112491607666,
        "learning_rate": 0.00012297085176777843,
        "epoch": 0.47735095690486024,
        "step": 3467
    },
    {
        "loss": 1.3717,
        "grad_norm": 1.7527881860733032,
        "learning_rate": 0.00012284410181843044,
        "epoch": 0.4774886410574143,
        "step": 3468
    },
    {
        "loss": 1.5162,
        "grad_norm": 1.657813549041748,
        "learning_rate": 0.0001227173131362619,
        "epoch": 0.47762632520996834,
        "step": 3469
    },
    {
        "loss": 2.4813,
        "grad_norm": 1.2204692363739014,
        "learning_rate": 0.00012259048593624665,
        "epoch": 0.4777640093625224,
        "step": 3470
    },
    {
        "loss": 2.2484,
        "grad_norm": 2.9109251499176025,
        "learning_rate": 0.00012246362043342388,
        "epoch": 0.47790169351507644,
        "step": 3471
    },
    {
        "loss": 2.264,
        "grad_norm": 2.771116256713867,
        "learning_rate": 0.00012233671684289758,
        "epoch": 0.47803937766763044,
        "step": 3472
    },
    {
        "loss": 2.3845,
        "grad_norm": 1.6515270471572876,
        "learning_rate": 0.00012220977537983623,
        "epoch": 0.4781770618201845,
        "step": 3473
    },
    {
        "loss": 1.4144,
        "grad_norm": 1.9520879983901978,
        "learning_rate": 0.00012208279625947294,
        "epoch": 0.47831474597273854,
        "step": 3474
    },
    {
        "loss": 2.1132,
        "grad_norm": 2.6004183292388916,
        "learning_rate": 0.00012195577969710414,
        "epoch": 0.4784524301252926,
        "step": 3475
    },
    {
        "loss": 0.9632,
        "grad_norm": 2.09425687789917,
        "learning_rate": 0.00012182872590809013,
        "epoch": 0.47859011427784665,
        "step": 3476
    },
    {
        "loss": 2.1508,
        "grad_norm": 1.58662748336792,
        "learning_rate": 0.00012170163510785428,
        "epoch": 0.47872779843040064,
        "step": 3477
    },
    {
        "loss": 1.0986,
        "grad_norm": 2.157895088195801,
        "learning_rate": 0.00012157450751188247,
        "epoch": 0.4788654825829547,
        "step": 3478
    },
    {
        "loss": 2.236,
        "grad_norm": 1.248536467552185,
        "learning_rate": 0.00012144734333572325,
        "epoch": 0.47900316673550875,
        "step": 3479
    },
    {
        "loss": 1.7143,
        "grad_norm": 2.171560525894165,
        "learning_rate": 0.00012132014279498707,
        "epoch": 0.4791408508880628,
        "step": 3480
    },
    {
        "loss": 2.1495,
        "grad_norm": 1.608170986175537,
        "learning_rate": 0.00012119290610534592,
        "epoch": 0.47927853504061685,
        "step": 3481
    },
    {
        "loss": 1.7935,
        "grad_norm": 2.5273373126983643,
        "learning_rate": 0.00012106563348253328,
        "epoch": 0.47941621919317084,
        "step": 3482
    },
    {
        "loss": 1.8719,
        "grad_norm": 1.2507675886154175,
        "learning_rate": 0.00012093832514234358,
        "epoch": 0.4795539033457249,
        "step": 3483
    },
    {
        "loss": 2.0766,
        "grad_norm": 2.7478418350219727,
        "learning_rate": 0.00012081098130063164,
        "epoch": 0.47969158749827895,
        "step": 3484
    },
    {
        "loss": 2.2857,
        "grad_norm": 2.190887689590454,
        "learning_rate": 0.00012068360217331243,
        "epoch": 0.479829271650833,
        "step": 3485
    },
    {
        "loss": 2.4995,
        "grad_norm": 0.9256443381309509,
        "learning_rate": 0.00012055618797636103,
        "epoch": 0.47996695580338705,
        "step": 3486
    },
    {
        "loss": 2.3087,
        "grad_norm": 1.7821577787399292,
        "learning_rate": 0.00012042873892581169,
        "epoch": 0.48010463995594105,
        "step": 3487
    },
    {
        "loss": 1.8201,
        "grad_norm": 1.9044830799102783,
        "learning_rate": 0.0001203012552377579,
        "epoch": 0.4802423241084951,
        "step": 3488
    },
    {
        "loss": 2.2259,
        "grad_norm": 1.9577316045761108,
        "learning_rate": 0.00012017373712835196,
        "epoch": 0.48038000826104915,
        "step": 3489
    },
    {
        "loss": 2.5248,
        "grad_norm": 1.2364877462387085,
        "learning_rate": 0.00012004618481380432,
        "epoch": 0.4805176924136032,
        "step": 3490
    },
    {
        "loss": 1.9592,
        "grad_norm": 2.0952067375183105,
        "learning_rate": 0.0001199185985103836,
        "epoch": 0.48065537656615726,
        "step": 3491
    },
    {
        "loss": 2.2999,
        "grad_norm": 1.0568441152572632,
        "learning_rate": 0.000119790978434416,
        "epoch": 0.48079306071871125,
        "step": 3492
    },
    {
        "loss": 1.8133,
        "grad_norm": 1.8149096965789795,
        "learning_rate": 0.00011966332480228484,
        "epoch": 0.4809307448712653,
        "step": 3493
    },
    {
        "loss": 1.5855,
        "grad_norm": 2.571786642074585,
        "learning_rate": 0.00011953563783043053,
        "epoch": 0.48106842902381936,
        "step": 3494
    },
    {
        "loss": 1.4759,
        "grad_norm": 3.2647697925567627,
        "learning_rate": 0.00011940791773535003,
        "epoch": 0.4812061131763734,
        "step": 3495
    },
    {
        "loss": 2.1376,
        "grad_norm": 2.8452067375183105,
        "learning_rate": 0.00011928016473359633,
        "epoch": 0.48134379732892746,
        "step": 3496
    },
    {
        "loss": 1.964,
        "grad_norm": 2.077465295791626,
        "learning_rate": 0.00011915237904177812,
        "epoch": 0.48148148148148145,
        "step": 3497
    },
    {
        "loss": 1.9496,
        "grad_norm": 1.6748398542404175,
        "learning_rate": 0.00011902456087655984,
        "epoch": 0.4816191656340355,
        "step": 3498
    },
    {
        "loss": 1.314,
        "grad_norm": 2.478606939315796,
        "learning_rate": 0.00011889671045466066,
        "epoch": 0.48175684978658956,
        "step": 3499
    },
    {
        "loss": 2.5039,
        "grad_norm": 1.2353500127792358,
        "learning_rate": 0.00011876882799285474,
        "epoch": 0.4818945339391436,
        "step": 3500
    },
    {
        "loss": 1.885,
        "grad_norm": 1.5698556900024414,
        "learning_rate": 0.0001186409137079702,
        "epoch": 0.48203221809169766,
        "step": 3501
    },
    {
        "loss": 1.5068,
        "grad_norm": 2.430586576461792,
        "learning_rate": 0.00011851296781688948,
        "epoch": 0.4821699022442517,
        "step": 3502
    },
    {
        "loss": 1.7246,
        "grad_norm": 1.890399694442749,
        "learning_rate": 0.00011838499053654844,
        "epoch": 0.4823075863968057,
        "step": 3503
    },
    {
        "loss": 1.9861,
        "grad_norm": 1.984678864479065,
        "learning_rate": 0.00011825698208393619,
        "epoch": 0.48244527054935976,
        "step": 3504
    },
    {
        "loss": 2.1657,
        "grad_norm": 1.6283537149429321,
        "learning_rate": 0.00011812894267609458,
        "epoch": 0.4825829547019138,
        "step": 3505
    },
    {
        "loss": 1.8928,
        "grad_norm": 1.8387532234191895,
        "learning_rate": 0.0001180008725301182,
        "epoch": 0.48272063885446787,
        "step": 3506
    },
    {
        "loss": 1.9606,
        "grad_norm": 2.4448745250701904,
        "learning_rate": 0.0001178727718631534,
        "epoch": 0.4828583230070219,
        "step": 3507
    },
    {
        "loss": 2.1362,
        "grad_norm": 2.0917842388153076,
        "learning_rate": 0.0001177446408923987,
        "epoch": 0.4829960071595759,
        "step": 3508
    },
    {
        "loss": 1.5945,
        "grad_norm": 3.9246878623962402,
        "learning_rate": 0.00011761647983510369,
        "epoch": 0.48313369131212996,
        "step": 3509
    },
    {
        "loss": 1.2004,
        "grad_norm": 2.2946643829345703,
        "learning_rate": 0.00011748828890856893,
        "epoch": 0.483271375464684,
        "step": 3510
    },
    {
        "loss": 1.6975,
        "grad_norm": 3.3430142402648926,
        "learning_rate": 0.00011736006833014595,
        "epoch": 0.48340905961723807,
        "step": 3511
    },
    {
        "loss": 2.1983,
        "grad_norm": 1.2138653993606567,
        "learning_rate": 0.00011723181831723635,
        "epoch": 0.4835467437697921,
        "step": 3512
    },
    {
        "loss": 2.3692,
        "grad_norm": 1.4965569972991943,
        "learning_rate": 0.00011710353908729152,
        "epoch": 0.4836844279223461,
        "step": 3513
    },
    {
        "loss": 2.1951,
        "grad_norm": 1.851891040802002,
        "learning_rate": 0.00011697523085781265,
        "epoch": 0.48382211207490017,
        "step": 3514
    },
    {
        "loss": 1.9269,
        "grad_norm": 2.5928425788879395,
        "learning_rate": 0.00011684689384634999,
        "epoch": 0.4839597962274542,
        "step": 3515
    },
    {
        "loss": 2.4916,
        "grad_norm": 2.5123746395111084,
        "learning_rate": 0.00011671852827050247,
        "epoch": 0.48409748038000827,
        "step": 3516
    },
    {
        "loss": 2.2674,
        "grad_norm": 2.1110963821411133,
        "learning_rate": 0.00011659013434791756,
        "epoch": 0.4842351645325623,
        "step": 3517
    },
    {
        "loss": 1.8744,
        "grad_norm": 2.097108840942383,
        "learning_rate": 0.00011646171229629089,
        "epoch": 0.4843728486851163,
        "step": 3518
    },
    {
        "loss": 1.8679,
        "grad_norm": 1.4932843446731567,
        "learning_rate": 0.0001163332623333655,
        "epoch": 0.48451053283767037,
        "step": 3519
    },
    {
        "loss": 1.4251,
        "grad_norm": 2.0952415466308594,
        "learning_rate": 0.00011620478467693214,
        "epoch": 0.4846482169902244,
        "step": 3520
    },
    {
        "loss": 2.1028,
        "grad_norm": 1.4144200086593628,
        "learning_rate": 0.00011607627954482822,
        "epoch": 0.4847859011427785,
        "step": 3521
    },
    {
        "loss": 1.8312,
        "grad_norm": 1.4391170740127563,
        "learning_rate": 0.00011594774715493772,
        "epoch": 0.4849235852953325,
        "step": 3522
    },
    {
        "loss": 1.6506,
        "grad_norm": 1.7279943227767944,
        "learning_rate": 0.00011581918772519107,
        "epoch": 0.4850612694478865,
        "step": 3523
    },
    {
        "loss": 2.7436,
        "grad_norm": 1.1608495712280273,
        "learning_rate": 0.00011569060147356443,
        "epoch": 0.4851989536004406,
        "step": 3524
    },
    {
        "loss": 2.0508,
        "grad_norm": 3.3199243545532227,
        "learning_rate": 0.0001155619886180793,
        "epoch": 0.4853366377529946,
        "step": 3525
    },
    {
        "loss": 2.0165,
        "grad_norm": 1.7376114130020142,
        "learning_rate": 0.00011543334937680253,
        "epoch": 0.4854743219055487,
        "step": 3526
    },
    {
        "loss": 1.0645,
        "grad_norm": 3.427976131439209,
        "learning_rate": 0.0001153046839678456,
        "epoch": 0.48561200605810273,
        "step": 3527
    },
    {
        "loss": 0.9589,
        "grad_norm": 3.6790964603424072,
        "learning_rate": 0.00011517599260936429,
        "epoch": 0.4857496902106567,
        "step": 3528
    },
    {
        "loss": 1.9735,
        "grad_norm": 2.0953245162963867,
        "learning_rate": 0.00011504727551955838,
        "epoch": 0.4858873743632108,
        "step": 3529
    },
    {
        "loss": 2.3716,
        "grad_norm": 2.3632164001464844,
        "learning_rate": 0.0001149185329166715,
        "epoch": 0.48602505851576483,
        "step": 3530
    },
    {
        "loss": 2.1497,
        "grad_norm": 1.9643687009811401,
        "learning_rate": 0.00011478976501899017,
        "epoch": 0.4861627426683189,
        "step": 3531
    },
    {
        "loss": 2.314,
        "grad_norm": 3.0144505500793457,
        "learning_rate": 0.00011466097204484418,
        "epoch": 0.48630042682087293,
        "step": 3532
    },
    {
        "loss": 1.782,
        "grad_norm": 1.712949275970459,
        "learning_rate": 0.00011453215421260561,
        "epoch": 0.486438110973427,
        "step": 3533
    },
    {
        "loss": 2.1385,
        "grad_norm": 1.9154495000839233,
        "learning_rate": 0.00011440331174068864,
        "epoch": 0.486575795125981,
        "step": 3534
    },
    {
        "loss": 2.1448,
        "grad_norm": 1.6017853021621704,
        "learning_rate": 0.00011427444484754936,
        "epoch": 0.48671347927853503,
        "step": 3535
    },
    {
        "loss": 1.4612,
        "grad_norm": 3.0440499782562256,
        "learning_rate": 0.00011414555375168539,
        "epoch": 0.4868511634310891,
        "step": 3536
    },
    {
        "loss": 2.4758,
        "grad_norm": 1.4118895530700684,
        "learning_rate": 0.00011401663867163502,
        "epoch": 0.48698884758364314,
        "step": 3537
    },
    {
        "loss": 1.7423,
        "grad_norm": 3.329664468765259,
        "learning_rate": 0.00011388769982597747,
        "epoch": 0.4871265317361972,
        "step": 3538
    },
    {
        "loss": 2.5339,
        "grad_norm": 1.6033480167388916,
        "learning_rate": 0.00011375873743333233,
        "epoch": 0.4872642158887512,
        "step": 3539
    },
    {
        "loss": 2.0716,
        "grad_norm": 1.7546864748001099,
        "learning_rate": 0.00011362975171235883,
        "epoch": 0.48740190004130524,
        "step": 3540
    },
    {
        "loss": 1.9465,
        "grad_norm": 1.539190411567688,
        "learning_rate": 0.00011350074288175598,
        "epoch": 0.4875395841938593,
        "step": 3541
    },
    {
        "loss": 2.2879,
        "grad_norm": 2.4113028049468994,
        "learning_rate": 0.00011337171116026182,
        "epoch": 0.48767726834641334,
        "step": 3542
    },
    {
        "loss": 2.335,
        "grad_norm": 1.6176656484603882,
        "learning_rate": 0.00011324265676665333,
        "epoch": 0.4878149524989674,
        "step": 3543
    },
    {
        "loss": 1.4748,
        "grad_norm": 2.3386757373809814,
        "learning_rate": 0.00011311357991974593,
        "epoch": 0.4879526366515214,
        "step": 3544
    },
    {
        "loss": 1.9125,
        "grad_norm": 2.3771812915802,
        "learning_rate": 0.00011298448083839306,
        "epoch": 0.48809032080407544,
        "step": 3545
    },
    {
        "loss": 1.1891,
        "grad_norm": 2.467942237854004,
        "learning_rate": 0.00011285535974148576,
        "epoch": 0.4882280049566295,
        "step": 3546
    },
    {
        "loss": 1.399,
        "grad_norm": 2.797888994216919,
        "learning_rate": 0.00011272621684795259,
        "epoch": 0.48836568910918354,
        "step": 3547
    },
    {
        "loss": 2.164,
        "grad_norm": 1.6596201658248901,
        "learning_rate": 0.0001125970523767589,
        "epoch": 0.4885033732617376,
        "step": 3548
    },
    {
        "loss": 1.9746,
        "grad_norm": 1.60235595703125,
        "learning_rate": 0.0001124678665469068,
        "epoch": 0.4886410574142916,
        "step": 3549
    },
    {
        "loss": 2.0277,
        "grad_norm": 1.7004170417785645,
        "learning_rate": 0.00011233865957743448,
        "epoch": 0.48877874156684564,
        "step": 3550
    },
    {
        "loss": 2.133,
        "grad_norm": 1.7362474203109741,
        "learning_rate": 0.00011220943168741595,
        "epoch": 0.4889164257193997,
        "step": 3551
    },
    {
        "loss": 2.1986,
        "grad_norm": 1.6585251092910767,
        "learning_rate": 0.00011208018309596092,
        "epoch": 0.48905410987195375,
        "step": 3552
    },
    {
        "loss": 2.2605,
        "grad_norm": 1.6348752975463867,
        "learning_rate": 0.00011195091402221387,
        "epoch": 0.4891917940245078,
        "step": 3553
    },
    {
        "loss": 2.3451,
        "grad_norm": 1.3202431201934814,
        "learning_rate": 0.00011182162468535413,
        "epoch": 0.4893294781770618,
        "step": 3554
    },
    {
        "loss": 2.3915,
        "grad_norm": 1.8062931299209595,
        "learning_rate": 0.00011169231530459554,
        "epoch": 0.48946716232961585,
        "step": 3555
    },
    {
        "loss": 1.4807,
        "grad_norm": 2.3968498706817627,
        "learning_rate": 0.00011156298609918583,
        "epoch": 0.4896048464821699,
        "step": 3556
    },
    {
        "loss": 1.9462,
        "grad_norm": 2.0762181282043457,
        "learning_rate": 0.00011143363728840628,
        "epoch": 0.48974253063472395,
        "step": 3557
    },
    {
        "loss": 1.9862,
        "grad_norm": 2.0924978256225586,
        "learning_rate": 0.00011130426909157139,
        "epoch": 0.489880214787278,
        "step": 3558
    },
    {
        "loss": 1.0297,
        "grad_norm": 3.14176344871521,
        "learning_rate": 0.00011117488172802873,
        "epoch": 0.49001789893983205,
        "step": 3559
    },
    {
        "loss": 1.5006,
        "grad_norm": 3.825468063354492,
        "learning_rate": 0.00011104547541715804,
        "epoch": 0.49015558309238605,
        "step": 3560
    },
    {
        "loss": 1.4531,
        "grad_norm": 1.8621547222137451,
        "learning_rate": 0.00011091605037837157,
        "epoch": 0.4902932672449401,
        "step": 3561
    },
    {
        "loss": 2.3123,
        "grad_norm": 1.2823988199234009,
        "learning_rate": 0.00011078660683111309,
        "epoch": 0.49043095139749415,
        "step": 3562
    },
    {
        "loss": 2.0146,
        "grad_norm": 2.652580976486206,
        "learning_rate": 0.00011065714499485767,
        "epoch": 0.4905686355500482,
        "step": 3563
    },
    {
        "loss": 1.7516,
        "grad_norm": 1.5363168716430664,
        "learning_rate": 0.00011052766508911172,
        "epoch": 0.49070631970260226,
        "step": 3564
    },
    {
        "loss": 2.4507,
        "grad_norm": 2.050575017929077,
        "learning_rate": 0.00011039816733341195,
        "epoch": 0.49084400385515625,
        "step": 3565
    },
    {
        "loss": 1.9974,
        "grad_norm": 2.0444021224975586,
        "learning_rate": 0.00011026865194732541,
        "epoch": 0.4909816880077103,
        "step": 3566
    },
    {
        "loss": 2.5272,
        "grad_norm": 2.272416830062866,
        "learning_rate": 0.00011013911915044919,
        "epoch": 0.49111937216026436,
        "step": 3567
    },
    {
        "loss": 2.1574,
        "grad_norm": 2.2947964668273926,
        "learning_rate": 0.00011000956916240985,
        "epoch": 0.4912570563128184,
        "step": 3568
    },
    {
        "loss": 2.1652,
        "grad_norm": 1.4046179056167603,
        "learning_rate": 0.00010988000220286299,
        "epoch": 0.49139474046537246,
        "step": 3569
    },
    {
        "loss": 1.2924,
        "grad_norm": 2.7155473232269287,
        "learning_rate": 0.00010975041849149302,
        "epoch": 0.49153242461792646,
        "step": 3570
    },
    {
        "loss": 1.6676,
        "grad_norm": 2.3056106567382812,
        "learning_rate": 0.00010962081824801287,
        "epoch": 0.4916701087704805,
        "step": 3571
    },
    {
        "loss": 2.3467,
        "grad_norm": 1.684207797050476,
        "learning_rate": 0.00010949120169216331,
        "epoch": 0.49180779292303456,
        "step": 3572
    },
    {
        "loss": 2.2175,
        "grad_norm": 1.7864198684692383,
        "learning_rate": 0.00010936156904371295,
        "epoch": 0.4919454770755886,
        "step": 3573
    },
    {
        "loss": 2.0149,
        "grad_norm": 1.8453807830810547,
        "learning_rate": 0.0001092319205224577,
        "epoch": 0.49208316122814266,
        "step": 3574
    },
    {
        "loss": 1.2444,
        "grad_norm": 1.8149768114089966,
        "learning_rate": 0.00010910225634822006,
        "epoch": 0.49222084538069666,
        "step": 3575
    },
    {
        "loss": 2.2684,
        "grad_norm": 2.2011563777923584,
        "learning_rate": 0.00010897257674084955,
        "epoch": 0.4923585295332507,
        "step": 3576
    },
    {
        "loss": 2.006,
        "grad_norm": 2.276693105697632,
        "learning_rate": 0.00010884288192022152,
        "epoch": 0.49249621368580476,
        "step": 3577
    },
    {
        "loss": 2.4559,
        "grad_norm": 1.185805082321167,
        "learning_rate": 0.00010871317210623706,
        "epoch": 0.4926338978383588,
        "step": 3578
    },
    {
        "loss": 2.3902,
        "grad_norm": 2.4707038402557373,
        "learning_rate": 0.00010858344751882298,
        "epoch": 0.49277158199091287,
        "step": 3579
    },
    {
        "loss": 2.7196,
        "grad_norm": 2.027841091156006,
        "learning_rate": 0.00010845370837793105,
        "epoch": 0.49290926614346686,
        "step": 3580
    },
    {
        "loss": 1.8093,
        "grad_norm": 2.5040485858917236,
        "learning_rate": 0.00010832395490353752,
        "epoch": 0.4930469502960209,
        "step": 3581
    },
    {
        "loss": 2.1541,
        "grad_norm": 1.9077017307281494,
        "learning_rate": 0.00010819418731564313,
        "epoch": 0.49318463444857497,
        "step": 3582
    },
    {
        "loss": 2.0034,
        "grad_norm": 1.998443365097046,
        "learning_rate": 0.0001080644058342724,
        "epoch": 0.493322318601129,
        "step": 3583
    },
    {
        "loss": 1.7337,
        "grad_norm": 1.6931313276290894,
        "learning_rate": 0.0001079346106794736,
        "epoch": 0.49346000275368307,
        "step": 3584
    },
    {
        "loss": 2.3837,
        "grad_norm": 1.7999264001846313,
        "learning_rate": 0.0001078048020713181,
        "epoch": 0.49359768690623707,
        "step": 3585
    },
    {
        "loss": 1.6086,
        "grad_norm": 1.597777247428894,
        "learning_rate": 0.00010767498022989998,
        "epoch": 0.4937353710587911,
        "step": 3586
    },
    {
        "loss": 2.44,
        "grad_norm": 2.360614061355591,
        "learning_rate": 0.00010754514537533585,
        "epoch": 0.49387305521134517,
        "step": 3587
    },
    {
        "loss": 2.2248,
        "grad_norm": 2.7832705974578857,
        "learning_rate": 0.00010741529772776451,
        "epoch": 0.4940107393638992,
        "step": 3588
    },
    {
        "loss": 1.8598,
        "grad_norm": 1.9346243143081665,
        "learning_rate": 0.00010728543750734623,
        "epoch": 0.4941484235164533,
        "step": 3589
    },
    {
        "loss": 2.9524,
        "grad_norm": 2.0159239768981934,
        "learning_rate": 0.00010715556493426262,
        "epoch": 0.4942861076690073,
        "step": 3590
    },
    {
        "loss": 1.5898,
        "grad_norm": 3.6243956089019775,
        "learning_rate": 0.0001070256802287164,
        "epoch": 0.4944237918215613,
        "step": 3591
    },
    {
        "loss": 1.8715,
        "grad_norm": 2.4375336170196533,
        "learning_rate": 0.00010689578361093067,
        "epoch": 0.49456147597411537,
        "step": 3592
    },
    {
        "loss": 1.9918,
        "grad_norm": 1.7290525436401367,
        "learning_rate": 0.00010676587530114894,
        "epoch": 0.4946991601266694,
        "step": 3593
    },
    {
        "loss": 2.0192,
        "grad_norm": 1.7095520496368408,
        "learning_rate": 0.00010663595551963432,
        "epoch": 0.4948368442792235,
        "step": 3594
    },
    {
        "loss": 1.9307,
        "grad_norm": 2.39677357673645,
        "learning_rate": 0.00010650602448666945,
        "epoch": 0.4949745284317775,
        "step": 3595
    },
    {
        "loss": 2.3591,
        "grad_norm": 1.6823281049728394,
        "learning_rate": 0.00010637608242255614,
        "epoch": 0.4951122125843315,
        "step": 3596
    },
    {
        "loss": 2.0538,
        "grad_norm": 1.6100455522537231,
        "learning_rate": 0.00010624612954761484,
        "epoch": 0.4952498967368856,
        "step": 3597
    },
    {
        "loss": 2.0282,
        "grad_norm": 1.9275476932525635,
        "learning_rate": 0.00010611616608218426,
        "epoch": 0.4953875808894396,
        "step": 3598
    },
    {
        "loss": 0.6436,
        "grad_norm": 2.0608069896698,
        "learning_rate": 0.00010598619224662118,
        "epoch": 0.4955252650419937,
        "step": 3599
    },
    {
        "loss": 2.477,
        "grad_norm": 2.0791075229644775,
        "learning_rate": 0.00010585620826129994,
        "epoch": 0.49566294919454773,
        "step": 3600
    },
    {
        "loss": 1.9554,
        "grad_norm": 2.4824249744415283,
        "learning_rate": 0.00010572621434661203,
        "epoch": 0.4958006333471017,
        "step": 3601
    },
    {
        "loss": 2.0634,
        "grad_norm": 1.8220648765563965,
        "learning_rate": 0.00010559621072296575,
        "epoch": 0.4959383174996558,
        "step": 3602
    },
    {
        "loss": 2.2002,
        "grad_norm": 2.035342216491699,
        "learning_rate": 0.00010546619761078602,
        "epoch": 0.49607600165220983,
        "step": 3603
    },
    {
        "loss": 1.8183,
        "grad_norm": 2.336637020111084,
        "learning_rate": 0.00010533617523051365,
        "epoch": 0.4962136858047639,
        "step": 3604
    },
    {
        "loss": 2.4348,
        "grad_norm": 1.475877046585083,
        "learning_rate": 0.00010520614380260536,
        "epoch": 0.49635136995731793,
        "step": 3605
    },
    {
        "loss": 1.4434,
        "grad_norm": 2.612607479095459,
        "learning_rate": 0.00010507610354753308,
        "epoch": 0.49648905410987193,
        "step": 3606
    },
    {
        "loss": 1.8406,
        "grad_norm": 1.6689797639846802,
        "learning_rate": 0.00010494605468578362,
        "epoch": 0.496626738262426,
        "step": 3607
    },
    {
        "loss": 2.4184,
        "grad_norm": 1.6522094011306763,
        "learning_rate": 0.00010481599743785863,
        "epoch": 0.49676442241498003,
        "step": 3608
    },
    {
        "loss": 2.1241,
        "grad_norm": 2.2386057376861572,
        "learning_rate": 0.0001046859320242739,
        "epoch": 0.4969021065675341,
        "step": 3609
    },
    {
        "loss": 2.1225,
        "grad_norm": 1.4910506010055542,
        "learning_rate": 0.00010455585866555884,
        "epoch": 0.49703979072008814,
        "step": 3610
    },
    {
        "loss": 2.2968,
        "grad_norm": 1.9302388429641724,
        "learning_rate": 0.00010442577758225664,
        "epoch": 0.49717747487264213,
        "step": 3611
    },
    {
        "loss": 1.909,
        "grad_norm": 2.4226200580596924,
        "learning_rate": 0.00010429568899492348,
        "epoch": 0.4973151590251962,
        "step": 3612
    },
    {
        "loss": 2.073,
        "grad_norm": 1.383040189743042,
        "learning_rate": 0.00010416559312412814,
        "epoch": 0.49745284317775024,
        "step": 3613
    },
    {
        "loss": 2.2807,
        "grad_norm": 1.994701623916626,
        "learning_rate": 0.00010403549019045187,
        "epoch": 0.4975905273303043,
        "step": 3614
    },
    {
        "loss": 2.4842,
        "grad_norm": 1.4493958950042725,
        "learning_rate": 0.00010390538041448794,
        "epoch": 0.49772821148285834,
        "step": 3615
    },
    {
        "loss": 1.8793,
        "grad_norm": 2.6755075454711914,
        "learning_rate": 0.00010377526401684104,
        "epoch": 0.49786589563541234,
        "step": 3616
    },
    {
        "loss": 1.8728,
        "grad_norm": 3.3466145992279053,
        "learning_rate": 0.00010364514121812732,
        "epoch": 0.4980035797879664,
        "step": 3617
    },
    {
        "loss": 1.9409,
        "grad_norm": 3.373516082763672,
        "learning_rate": 0.00010351501223897362,
        "epoch": 0.49814126394052044,
        "step": 3618
    },
    {
        "loss": 1.6108,
        "grad_norm": 1.9607205390930176,
        "learning_rate": 0.0001033848773000172,
        "epoch": 0.4982789480930745,
        "step": 3619
    },
    {
        "loss": 2.0485,
        "grad_norm": 2.058610439300537,
        "learning_rate": 0.00010325473662190563,
        "epoch": 0.49841663224562854,
        "step": 3620
    },
    {
        "loss": 2.162,
        "grad_norm": 1.0020238161087036,
        "learning_rate": 0.00010312459042529619,
        "epoch": 0.4985543163981826,
        "step": 3621
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.4408217668533325,
        "learning_rate": 0.00010299443893085523,
        "epoch": 0.4986920005507366,
        "step": 3622
    },
    {
        "loss": 2.2774,
        "grad_norm": 1.9231226444244385,
        "learning_rate": 0.00010286428235925845,
        "epoch": 0.49882968470329064,
        "step": 3623
    },
    {
        "loss": 1.9757,
        "grad_norm": 1.8331927061080933,
        "learning_rate": 0.00010273412093119,
        "epoch": 0.4989673688558447,
        "step": 3624
    },
    {
        "loss": 1.7615,
        "grad_norm": 1.9060298204421997,
        "learning_rate": 0.00010260395486734218,
        "epoch": 0.49910505300839875,
        "step": 3625
    },
    {
        "loss": 1.8084,
        "grad_norm": 2.722620725631714,
        "learning_rate": 0.00010247378438841531,
        "epoch": 0.4992427371609528,
        "step": 3626
    },
    {
        "loss": 2.2277,
        "grad_norm": 1.4614890813827515,
        "learning_rate": 0.00010234360971511698,
        "epoch": 0.4993804213135068,
        "step": 3627
    },
    {
        "loss": 2.1625,
        "grad_norm": 1.9858759641647339,
        "learning_rate": 0.0001022134310681621,
        "epoch": 0.49951810546606085,
        "step": 3628
    },
    {
        "loss": 2.1916,
        "grad_norm": 1.3634904623031616,
        "learning_rate": 0.00010208324866827235,
        "epoch": 0.4996557896186149,
        "step": 3629
    },
    {
        "loss": 1.521,
        "grad_norm": 2.5343737602233887,
        "learning_rate": 0.0001019530627361756,
        "epoch": 0.49979347377116895,
        "step": 3630
    },
    {
        "loss": 2.4259,
        "grad_norm": 2.690349578857422,
        "learning_rate": 0.00010182287349260567,
        "epoch": 0.499931157923723,
        "step": 3631
    },
    {
        "loss": 2.1183,
        "grad_norm": 1.8158739805221558,
        "learning_rate": 0.00010169268115830228,
        "epoch": 0.500068842076277,
        "step": 3632
    },
    {
        "loss": 2.153,
        "grad_norm": 1.3016775846481323,
        "learning_rate": 0.00010156248595401005,
        "epoch": 0.5002065262288311,
        "step": 3633
    },
    {
        "loss": 1.9421,
        "grad_norm": 2.077698230743408,
        "learning_rate": 0.00010143228810047871,
        "epoch": 0.5003442103813851,
        "step": 3634
    },
    {
        "loss": 2.608,
        "grad_norm": 2.006187915802002,
        "learning_rate": 0.00010130208781846249,
        "epoch": 0.5004818945339391,
        "step": 3635
    },
    {
        "loss": 1.9153,
        "grad_norm": 2.3201847076416016,
        "learning_rate": 0.00010117188532871949,
        "epoch": 0.5006195786864932,
        "step": 3636
    },
    {
        "loss": 2.4312,
        "grad_norm": 2.705878734588623,
        "learning_rate": 0.00010104168085201184,
        "epoch": 0.5007572628390472,
        "step": 3637
    },
    {
        "loss": 2.1284,
        "grad_norm": 1.3313713073730469,
        "learning_rate": 0.00010091147460910483,
        "epoch": 0.5008949469916013,
        "step": 3638
    },
    {
        "loss": 1.8275,
        "grad_norm": 1.3453214168548584,
        "learning_rate": 0.00010078126682076681,
        "epoch": 0.5010326311441553,
        "step": 3639
    },
    {
        "loss": 2.008,
        "grad_norm": 2.5852417945861816,
        "learning_rate": 0.00010065105770776882,
        "epoch": 0.5011703152967093,
        "step": 3640
    },
    {
        "loss": 2.7363,
        "grad_norm": 1.579744815826416,
        "learning_rate": 0.0001005208474908841,
        "epoch": 0.5013079994492634,
        "step": 3641
    },
    {
        "loss": 1.0314,
        "grad_norm": 3.8087313175201416,
        "learning_rate": 0.00010039063639088777,
        "epoch": 0.5014456836018174,
        "step": 3642
    },
    {
        "loss": 1.4316,
        "grad_norm": 2.754612922668457,
        "learning_rate": 0.00010026042462855634,
        "epoch": 0.5015833677543715,
        "step": 3643
    },
    {
        "loss": 1.3099,
        "grad_norm": 1.4824241399765015,
        "learning_rate": 0.00010013021242466762,
        "epoch": 0.5017210519069255,
        "step": 3644
    },
    {
        "loss": 1.4666,
        "grad_norm": 2.5343761444091797,
        "learning_rate": 0.0001,
        "epoch": 0.5018587360594795,
        "step": 3645
    },
    {
        "loss": 1.7517,
        "grad_norm": 1.4848450422286987,
        "learning_rate": 9.986978757533247e-05,
        "epoch": 0.5019964202120336,
        "step": 3646
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.227011203765869,
        "learning_rate": 9.973957537144362e-05,
        "epoch": 0.5021341043645876,
        "step": 3647
    },
    {
        "loss": 1.5567,
        "grad_norm": 2.2677669525146484,
        "learning_rate": 9.960936360911224e-05,
        "epoch": 0.5022717885171417,
        "step": 3648
    },
    {
        "loss": 1.3138,
        "grad_norm": 1.910942554473877,
        "learning_rate": 9.947915250911592e-05,
        "epoch": 0.5024094726696957,
        "step": 3649
    },
    {
        "loss": 2.1345,
        "grad_norm": 1.5909836292266846,
        "learning_rate": 9.934894229223114e-05,
        "epoch": 0.5025471568222497,
        "step": 3650
    },
    {
        "loss": 1.942,
        "grad_norm": 1.923373818397522,
        "learning_rate": 9.92187331792332e-05,
        "epoch": 0.5026848409748038,
        "step": 3651
    },
    {
        "loss": 2.3872,
        "grad_norm": 2.082904100418091,
        "learning_rate": 9.908852539089526e-05,
        "epoch": 0.5028225251273578,
        "step": 3652
    },
    {
        "loss": 1.8949,
        "grad_norm": 3.701291799545288,
        "learning_rate": 9.895831914798819e-05,
        "epoch": 0.5029602092799119,
        "step": 3653
    },
    {
        "loss": 1.6811,
        "grad_norm": 1.8502284288406372,
        "learning_rate": 9.882811467128055e-05,
        "epoch": 0.5030978934324659,
        "step": 3654
    },
    {
        "loss": 2.5976,
        "grad_norm": 1.360418677330017,
        "learning_rate": 9.869791218153761e-05,
        "epoch": 0.5032355775850199,
        "step": 3655
    },
    {
        "loss": 2.3544,
        "grad_norm": 1.8394016027450562,
        "learning_rate": 9.856771189952122e-05,
        "epoch": 0.503373261737574,
        "step": 3656
    },
    {
        "loss": 2.3818,
        "grad_norm": 1.329214334487915,
        "learning_rate": 9.843751404598997e-05,
        "epoch": 0.503510945890128,
        "step": 3657
    },
    {
        "loss": 2.0144,
        "grad_norm": 2.4264729022979736,
        "learning_rate": 9.830731884169781e-05,
        "epoch": 0.5036486300426821,
        "step": 3658
    },
    {
        "loss": 2.5559,
        "grad_norm": 1.3069226741790771,
        "learning_rate": 9.817712650739427e-05,
        "epoch": 0.5037863141952361,
        "step": 3659
    },
    {
        "loss": 1.5738,
        "grad_norm": 4.060251712799072,
        "learning_rate": 9.804693726382443e-05,
        "epoch": 0.5039239983477901,
        "step": 3660
    },
    {
        "loss": 1.9972,
        "grad_norm": 1.2109906673431396,
        "learning_rate": 9.791675133172766e-05,
        "epoch": 0.5040616825003442,
        "step": 3661
    },
    {
        "loss": 2.2779,
        "grad_norm": 1.2015942335128784,
        "learning_rate": 9.778656893183783e-05,
        "epoch": 0.5041993666528982,
        "step": 3662
    },
    {
        "loss": 2.4285,
        "grad_norm": 2.1437249183654785,
        "learning_rate": 9.765639028488305e-05,
        "epoch": 0.5043370508054523,
        "step": 3663
    },
    {
        "loss": 2.1725,
        "grad_norm": 1.8299411535263062,
        "learning_rate": 9.75262156115848e-05,
        "epoch": 0.5044747349580063,
        "step": 3664
    },
    {
        "loss": 0.8476,
        "grad_norm": 2.057302236557007,
        "learning_rate": 9.739604513265784e-05,
        "epoch": 0.5046124191105604,
        "step": 3665
    },
    {
        "loss": 2.4187,
        "grad_norm": 1.4605480432510376,
        "learning_rate": 9.726587906881003e-05,
        "epoch": 0.5047501032631144,
        "step": 3666
    },
    {
        "loss": 2.0095,
        "grad_norm": 1.6718287467956543,
        "learning_rate": 9.713571764074159e-05,
        "epoch": 0.5048877874156684,
        "step": 3667
    },
    {
        "loss": 2.2022,
        "grad_norm": 1.743932843208313,
        "learning_rate": 9.700556106914473e-05,
        "epoch": 0.5050254715682225,
        "step": 3668
    },
    {
        "loss": 2.5583,
        "grad_norm": 1.906691551208496,
        "learning_rate": 9.687540957470383e-05,
        "epoch": 0.5051631557207765,
        "step": 3669
    },
    {
        "loss": 2.0374,
        "grad_norm": 1.9859809875488281,
        "learning_rate": 9.674526337809438e-05,
        "epoch": 0.5053008398733306,
        "step": 3670
    },
    {
        "loss": 2.0928,
        "grad_norm": 1.7509419918060303,
        "learning_rate": 9.661512269998277e-05,
        "epoch": 0.5054385240258846,
        "step": 3671
    },
    {
        "loss": 2.1812,
        "grad_norm": 2.7581841945648193,
        "learning_rate": 9.648498776102641e-05,
        "epoch": 0.5055762081784386,
        "step": 3672
    },
    {
        "loss": 1.6752,
        "grad_norm": 1.915225625038147,
        "learning_rate": 9.63548587818727e-05,
        "epoch": 0.5057138923309927,
        "step": 3673
    },
    {
        "loss": 2.128,
        "grad_norm": 2.887462615966797,
        "learning_rate": 9.622473598315893e-05,
        "epoch": 0.5058515764835467,
        "step": 3674
    },
    {
        "loss": 2.1992,
        "grad_norm": 1.2086472511291504,
        "learning_rate": 9.609461958551207e-05,
        "epoch": 0.5059892606361008,
        "step": 3675
    },
    {
        "loss": 2.1785,
        "grad_norm": 2.2732765674591064,
        "learning_rate": 9.596450980954815e-05,
        "epoch": 0.5061269447886548,
        "step": 3676
    },
    {
        "loss": 1.5374,
        "grad_norm": 2.394028663635254,
        "learning_rate": 9.583440687587187e-05,
        "epoch": 0.5062646289412088,
        "step": 3677
    },
    {
        "loss": 1.7713,
        "grad_norm": 2.5243101119995117,
        "learning_rate": 9.570431100507654e-05,
        "epoch": 0.5064023130937629,
        "step": 3678
    },
    {
        "loss": 1.8423,
        "grad_norm": 1.5343365669250488,
        "learning_rate": 9.557422241774339e-05,
        "epoch": 0.5065399972463169,
        "step": 3679
    },
    {
        "loss": 1.894,
        "grad_norm": 1.8114205598831177,
        "learning_rate": 9.544414133444112e-05,
        "epoch": 0.506677681398871,
        "step": 3680
    },
    {
        "loss": 2.1243,
        "grad_norm": 2.0975189208984375,
        "learning_rate": 9.531406797572611e-05,
        "epoch": 0.506815365551425,
        "step": 3681
    },
    {
        "loss": 1.9089,
        "grad_norm": 1.6007894277572632,
        "learning_rate": 9.518400256214138e-05,
        "epoch": 0.506953049703979,
        "step": 3682
    },
    {
        "loss": 1.7683,
        "grad_norm": 3.996680736541748,
        "learning_rate": 9.505394531421634e-05,
        "epoch": 0.5070907338565331,
        "step": 3683
    },
    {
        "loss": 2.4018,
        "grad_norm": 0.9478393197059631,
        "learning_rate": 9.492389645246695e-05,
        "epoch": 0.5072284180090871,
        "step": 3684
    },
    {
        "loss": 1.7887,
        "grad_norm": 1.6673599481582642,
        "learning_rate": 9.479385619739467e-05,
        "epoch": 0.5073661021616412,
        "step": 3685
    },
    {
        "loss": 2.029,
        "grad_norm": 1.6009306907653809,
        "learning_rate": 9.466382476948631e-05,
        "epoch": 0.5075037863141952,
        "step": 3686
    },
    {
        "loss": 1.7141,
        "grad_norm": 1.604538917541504,
        "learning_rate": 9.453380238921402e-05,
        "epoch": 0.5076414704667492,
        "step": 3687
    },
    {
        "loss": 1.93,
        "grad_norm": 1.8982230424880981,
        "learning_rate": 9.440378927703427e-05,
        "epoch": 0.5077791546193033,
        "step": 3688
    },
    {
        "loss": 2.1818,
        "grad_norm": 1.9624989032745361,
        "learning_rate": 9.427378565338805e-05,
        "epoch": 0.5079168387718573,
        "step": 3689
    },
    {
        "loss": 1.0011,
        "grad_norm": 1.6222366094589233,
        "learning_rate": 9.414379173870008e-05,
        "epoch": 0.5080545229244114,
        "step": 3690
    },
    {
        "loss": 2.7232,
        "grad_norm": 2.0307953357696533,
        "learning_rate": 9.401380775337884e-05,
        "epoch": 0.5081922070769654,
        "step": 3691
    },
    {
        "loss": 2.1301,
        "grad_norm": 2.5231499671936035,
        "learning_rate": 9.388383391781582e-05,
        "epoch": 0.5083298912295194,
        "step": 3692
    },
    {
        "loss": 2.3864,
        "grad_norm": 1.685200810432434,
        "learning_rate": 9.375387045238519e-05,
        "epoch": 0.5084675753820735,
        "step": 3693
    },
    {
        "loss": 1.3693,
        "grad_norm": 1.2532765865325928,
        "learning_rate": 9.362391757744388e-05,
        "epoch": 0.5086052595346275,
        "step": 3694
    },
    {
        "loss": 1.5318,
        "grad_norm": 1.548998236656189,
        "learning_rate": 9.349397551333063e-05,
        "epoch": 0.5087429436871816,
        "step": 3695
    },
    {
        "loss": 2.3166,
        "grad_norm": 1.1712563037872314,
        "learning_rate": 9.33640444803657e-05,
        "epoch": 0.5088806278397356,
        "step": 3696
    },
    {
        "loss": 2.4539,
        "grad_norm": 1.1833086013793945,
        "learning_rate": 9.32341246988511e-05,
        "epoch": 0.5090183119922896,
        "step": 3697
    },
    {
        "loss": 1.4624,
        "grad_norm": 2.351032018661499,
        "learning_rate": 9.310421638906935e-05,
        "epoch": 0.5091559961448437,
        "step": 3698
    },
    {
        "loss": 2.563,
        "grad_norm": 1.7820348739624023,
        "learning_rate": 9.297431977128362e-05,
        "epoch": 0.5092936802973977,
        "step": 3699
    },
    {
        "loss": 2.3972,
        "grad_norm": 1.8001017570495605,
        "learning_rate": 9.28444350657374e-05,
        "epoch": 0.5094313644499519,
        "step": 3700
    },
    {
        "loss": 0.8536,
        "grad_norm": 3.0502047538757324,
        "learning_rate": 9.271456249265386e-05,
        "epoch": 0.5095690486025058,
        "step": 3701
    },
    {
        "loss": 1.8209,
        "grad_norm": 2.6883046627044678,
        "learning_rate": 9.258470227223552e-05,
        "epoch": 0.5097067327550598,
        "step": 3702
    },
    {
        "loss": 2.2745,
        "grad_norm": 1.173895001411438,
        "learning_rate": 9.245485462466417e-05,
        "epoch": 0.509844416907614,
        "step": 3703
    },
    {
        "loss": 2.5469,
        "grad_norm": 1.5372083187103271,
        "learning_rate": 9.232501977010013e-05,
        "epoch": 0.509982101060168,
        "step": 3704
    },
    {
        "loss": 2.409,
        "grad_norm": 2.350713014602661,
        "learning_rate": 9.219519792868194e-05,
        "epoch": 0.510119785212722,
        "step": 3705
    },
    {
        "loss": 2.5446,
        "grad_norm": 1.5134505033493042,
        "learning_rate": 9.206538932052643e-05,
        "epoch": 0.510257469365276,
        "step": 3706
    },
    {
        "loss": 2.4399,
        "grad_norm": 1.8503806591033936,
        "learning_rate": 9.193559416572769e-05,
        "epoch": 0.51039515351783,
        "step": 3707
    },
    {
        "loss": 1.7358,
        "grad_norm": 2.9584126472473145,
        "learning_rate": 9.18058126843569e-05,
        "epoch": 0.5105328376703842,
        "step": 3708
    },
    {
        "loss": 2.3705,
        "grad_norm": 2.2544751167297363,
        "learning_rate": 9.167604509646249e-05,
        "epoch": 0.5106705218229382,
        "step": 3709
    },
    {
        "loss": 2.3098,
        "grad_norm": 1.9714711904525757,
        "learning_rate": 9.154629162206898e-05,
        "epoch": 0.5108082059754923,
        "step": 3710
    },
    {
        "loss": 1.5697,
        "grad_norm": 2.8089442253112793,
        "learning_rate": 9.141655248117697e-05,
        "epoch": 0.5109458901280463,
        "step": 3711
    },
    {
        "loss": 2.0311,
        "grad_norm": 2.5784993171691895,
        "learning_rate": 9.128682789376295e-05,
        "epoch": 0.5110835742806003,
        "step": 3712
    },
    {
        "loss": 2.402,
        "grad_norm": 1.8125239610671997,
        "learning_rate": 9.115711807977858e-05,
        "epoch": 0.5112212584331544,
        "step": 3713
    },
    {
        "loss": 1.804,
        "grad_norm": 1.70437490940094,
        "learning_rate": 9.102742325915046e-05,
        "epoch": 0.5113589425857084,
        "step": 3714
    },
    {
        "loss": 2.1875,
        "grad_norm": 1.4510936737060547,
        "learning_rate": 9.089774365177995e-05,
        "epoch": 0.5114966267382625,
        "step": 3715
    },
    {
        "loss": 2.4382,
        "grad_norm": 1.3017786741256714,
        "learning_rate": 9.07680794775424e-05,
        "epoch": 0.5116343108908165,
        "step": 3716
    },
    {
        "loss": 1.2053,
        "grad_norm": 2.220918655395508,
        "learning_rate": 9.0638430956287e-05,
        "epoch": 0.5117719950433705,
        "step": 3717
    },
    {
        "loss": 1.4787,
        "grad_norm": 2.2461917400360107,
        "learning_rate": 9.05087983078367e-05,
        "epoch": 0.5119096791959246,
        "step": 3718
    },
    {
        "loss": 2.4559,
        "grad_norm": 1.3950165510177612,
        "learning_rate": 9.037918175198721e-05,
        "epoch": 0.5120473633484786,
        "step": 3719
    },
    {
        "loss": 2.0865,
        "grad_norm": 2.4122989177703857,
        "learning_rate": 9.024958150850695e-05,
        "epoch": 0.5121850475010327,
        "step": 3720
    },
    {
        "loss": 1.6376,
        "grad_norm": 2.1121878623962402,
        "learning_rate": 9.011999779713704e-05,
        "epoch": 0.5123227316535867,
        "step": 3721
    },
    {
        "loss": 1.6916,
        "grad_norm": 3.0678110122680664,
        "learning_rate": 8.999043083759017e-05,
        "epoch": 0.5124604158061408,
        "step": 3722
    },
    {
        "loss": 1.2287,
        "grad_norm": 2.039854049682617,
        "learning_rate": 8.986088084955078e-05,
        "epoch": 0.5125980999586948,
        "step": 3723
    },
    {
        "loss": 0.5413,
        "grad_norm": 1.8045973777770996,
        "learning_rate": 8.973134805267461e-05,
        "epoch": 0.5127357841112488,
        "step": 3724
    },
    {
        "loss": 2.1191,
        "grad_norm": 2.666292667388916,
        "learning_rate": 8.960183266658813e-05,
        "epoch": 0.5128734682638029,
        "step": 3725
    },
    {
        "loss": 2.1753,
        "grad_norm": 1.532371997833252,
        "learning_rate": 8.947233491088832e-05,
        "epoch": 0.5130111524163569,
        "step": 3726
    },
    {
        "loss": 1.8846,
        "grad_norm": 1.7987630367279053,
        "learning_rate": 8.934285500514234e-05,
        "epoch": 0.513148836568911,
        "step": 3727
    },
    {
        "loss": 2.4717,
        "grad_norm": 1.3443951606750488,
        "learning_rate": 8.921339316888703e-05,
        "epoch": 0.513286520721465,
        "step": 3728
    },
    {
        "loss": 1.467,
        "grad_norm": 2.573413133621216,
        "learning_rate": 8.90839496216284e-05,
        "epoch": 0.513424204874019,
        "step": 3729
    },
    {
        "loss": 2.2626,
        "grad_norm": 2.2037508487701416,
        "learning_rate": 8.895452458284197e-05,
        "epoch": 0.5135618890265731,
        "step": 3730
    },
    {
        "loss": 2.4306,
        "grad_norm": 1.3872549533843994,
        "learning_rate": 8.88251182719714e-05,
        "epoch": 0.5136995731791271,
        "step": 3731
    },
    {
        "loss": 2.0475,
        "grad_norm": 1.8659889698028564,
        "learning_rate": 8.869573090842858e-05,
        "epoch": 0.5138372573316812,
        "step": 3732
    },
    {
        "loss": 2.1734,
        "grad_norm": 1.8399369716644287,
        "learning_rate": 8.856636271159376e-05,
        "epoch": 0.5139749414842352,
        "step": 3733
    },
    {
        "loss": 2.8204,
        "grad_norm": 1.7282105684280396,
        "learning_rate": 8.843701390081418e-05,
        "epoch": 0.5141126256367892,
        "step": 3734
    },
    {
        "loss": 2.0446,
        "grad_norm": 1.3135932683944702,
        "learning_rate": 8.830768469540443e-05,
        "epoch": 0.5142503097893433,
        "step": 3735
    },
    {
        "loss": 1.73,
        "grad_norm": 1.8812671899795532,
        "learning_rate": 8.817837531464591e-05,
        "epoch": 0.5143879939418973,
        "step": 3736
    },
    {
        "loss": 2.2482,
        "grad_norm": 2.353637218475342,
        "learning_rate": 8.804908597778625e-05,
        "epoch": 0.5145256780944514,
        "step": 3737
    },
    {
        "loss": 1.3618,
        "grad_norm": 2.2229692935943604,
        "learning_rate": 8.791981690403912e-05,
        "epoch": 0.5146633622470054,
        "step": 3738
    },
    {
        "loss": 2.3958,
        "grad_norm": 2.4653072357177734,
        "learning_rate": 8.779056831258406e-05,
        "epoch": 0.5148010463995594,
        "step": 3739
    },
    {
        "loss": 2.4357,
        "grad_norm": 3.542776107788086,
        "learning_rate": 8.766134042256561e-05,
        "epoch": 0.5149387305521135,
        "step": 3740
    },
    {
        "loss": 1.76,
        "grad_norm": 2.0757150650024414,
        "learning_rate": 8.753213345309317e-05,
        "epoch": 0.5150764147046675,
        "step": 3741
    },
    {
        "loss": 2.4989,
        "grad_norm": 1.9079246520996094,
        "learning_rate": 8.740294762324111e-05,
        "epoch": 0.5152140988572216,
        "step": 3742
    },
    {
        "loss": 2.2733,
        "grad_norm": 1.578991174697876,
        "learning_rate": 8.72737831520475e-05,
        "epoch": 0.5153517830097756,
        "step": 3743
    },
    {
        "loss": 2.2677,
        "grad_norm": 1.5954315662384033,
        "learning_rate": 8.71446402585142e-05,
        "epoch": 0.5154894671623296,
        "step": 3744
    },
    {
        "loss": 0.7266,
        "grad_norm": 1.947340726852417,
        "learning_rate": 8.701551916160697e-05,
        "epoch": 0.5156271513148837,
        "step": 3745
    },
    {
        "loss": 1.7677,
        "grad_norm": 2.997443914413452,
        "learning_rate": 8.688642008025408e-05,
        "epoch": 0.5157648354674377,
        "step": 3746
    },
    {
        "loss": 1.6294,
        "grad_norm": 2.6937317848205566,
        "learning_rate": 8.675734323334662e-05,
        "epoch": 0.5159025196199918,
        "step": 3747
    },
    {
        "loss": 1.6555,
        "grad_norm": 3.6149888038635254,
        "learning_rate": 8.66282888397382e-05,
        "epoch": 0.5160402037725458,
        "step": 3748
    },
    {
        "loss": 1.9719,
        "grad_norm": 1.538596272468567,
        "learning_rate": 8.649925711824411e-05,
        "epoch": 0.5161778879250998,
        "step": 3749
    },
    {
        "loss": 1.7007,
        "grad_norm": 3.17228364944458,
        "learning_rate": 8.637024828764118e-05,
        "epoch": 0.5163155720776539,
        "step": 3750
    },
    {
        "loss": 1.7176,
        "grad_norm": 2.47809100151062,
        "learning_rate": 8.624126256666771e-05,
        "epoch": 0.5164532562302079,
        "step": 3751
    },
    {
        "loss": 2.2938,
        "grad_norm": 2.340845823287964,
        "learning_rate": 8.611230017402255e-05,
        "epoch": 0.516590940382762,
        "step": 3752
    },
    {
        "loss": 2.1246,
        "grad_norm": 1.8481870889663696,
        "learning_rate": 8.598336132836494e-05,
        "epoch": 0.516728624535316,
        "step": 3753
    },
    {
        "loss": 2.2827,
        "grad_norm": 2.161550998687744,
        "learning_rate": 8.585444624831464e-05,
        "epoch": 0.51686630868787,
        "step": 3754
    },
    {
        "loss": 2.1579,
        "grad_norm": 2.544668674468994,
        "learning_rate": 8.572555515245064e-05,
        "epoch": 0.5170039928404241,
        "step": 3755
    },
    {
        "loss": 1.564,
        "grad_norm": 1.981188178062439,
        "learning_rate": 8.55966882593113e-05,
        "epoch": 0.5171416769929781,
        "step": 3756
    },
    {
        "loss": 2.1317,
        "grad_norm": 2.486894130706787,
        "learning_rate": 8.546784578739441e-05,
        "epoch": 0.5172793611455322,
        "step": 3757
    },
    {
        "loss": 1.7759,
        "grad_norm": 2.778803825378418,
        "learning_rate": 8.533902795515583e-05,
        "epoch": 0.5174170452980862,
        "step": 3758
    },
    {
        "loss": 1.8497,
        "grad_norm": 1.8265413045883179,
        "learning_rate": 8.521023498100977e-05,
        "epoch": 0.5175547294506402,
        "step": 3759
    },
    {
        "loss": 1.7169,
        "grad_norm": 2.1257197856903076,
        "learning_rate": 8.508146708332854e-05,
        "epoch": 0.5176924136031943,
        "step": 3760
    },
    {
        "loss": 2.4433,
        "grad_norm": 1.1814740896224976,
        "learning_rate": 8.495272448044163e-05,
        "epoch": 0.5178300977557483,
        "step": 3761
    },
    {
        "loss": 0.6907,
        "grad_norm": 2.4611525535583496,
        "learning_rate": 8.482400739063572e-05,
        "epoch": 0.5179677819083024,
        "step": 3762
    },
    {
        "loss": 1.5152,
        "grad_norm": 2.8961963653564453,
        "learning_rate": 8.469531603215443e-05,
        "epoch": 0.5181054660608564,
        "step": 3763
    },
    {
        "loss": 2.2688,
        "grad_norm": 2.2932639122009277,
        "learning_rate": 8.456665062319749e-05,
        "epoch": 0.5182431502134104,
        "step": 3764
    },
    {
        "loss": 1.8138,
        "grad_norm": 1.5151689052581787,
        "learning_rate": 8.443801138192063e-05,
        "epoch": 0.5183808343659645,
        "step": 3765
    },
    {
        "loss": 1.252,
        "grad_norm": 1.867808222770691,
        "learning_rate": 8.430939852643558e-05,
        "epoch": 0.5185185185185185,
        "step": 3766
    },
    {
        "loss": 1.9306,
        "grad_norm": 2.527000904083252,
        "learning_rate": 8.418081227480895e-05,
        "epoch": 0.5186562026710726,
        "step": 3767
    },
    {
        "loss": 2.0161,
        "grad_norm": 1.666422963142395,
        "learning_rate": 8.40522528450622e-05,
        "epoch": 0.5187938868236266,
        "step": 3768
    },
    {
        "loss": 2.0174,
        "grad_norm": 1.6037999391555786,
        "learning_rate": 8.39237204551718e-05,
        "epoch": 0.5189315709761806,
        "step": 3769
    },
    {
        "loss": 1.9705,
        "grad_norm": 1.5950729846954346,
        "learning_rate": 8.379521532306787e-05,
        "epoch": 0.5190692551287347,
        "step": 3770
    },
    {
        "loss": 2.1496,
        "grad_norm": 1.9753634929656982,
        "learning_rate": 8.366673766663444e-05,
        "epoch": 0.5192069392812887,
        "step": 3771
    },
    {
        "loss": 1.5288,
        "grad_norm": 1.3175567388534546,
        "learning_rate": 8.353828770370914e-05,
        "epoch": 0.5193446234338428,
        "step": 3772
    },
    {
        "loss": 1.5817,
        "grad_norm": 2.192351818084717,
        "learning_rate": 8.340986565208246e-05,
        "epoch": 0.5194823075863968,
        "step": 3773
    },
    {
        "loss": 2.0374,
        "grad_norm": 1.0172233581542969,
        "learning_rate": 8.32814717294976e-05,
        "epoch": 0.5196199917389508,
        "step": 3774
    },
    {
        "loss": 2.3959,
        "grad_norm": 2.580439329147339,
        "learning_rate": 8.315310615365003e-05,
        "epoch": 0.5197576758915049,
        "step": 3775
    },
    {
        "loss": 2.7568,
        "grad_norm": 1.8839547634124756,
        "learning_rate": 8.302476914218737e-05,
        "epoch": 0.5198953600440589,
        "step": 3776
    },
    {
        "loss": 2.0076,
        "grad_norm": 3.4978582859039307,
        "learning_rate": 8.289646091270855e-05,
        "epoch": 0.520033044196613,
        "step": 3777
    },
    {
        "loss": 2.0905,
        "grad_norm": 1.9217404127120972,
        "learning_rate": 8.276818168276366e-05,
        "epoch": 0.520170728349167,
        "step": 3778
    },
    {
        "loss": 2.1496,
        "grad_norm": 2.1571645736694336,
        "learning_rate": 8.263993166985406e-05,
        "epoch": 0.5203084125017211,
        "step": 3779
    },
    {
        "loss": 2.088,
        "grad_norm": 1.9866509437561035,
        "learning_rate": 8.251171109143114e-05,
        "epoch": 0.5204460966542751,
        "step": 3780
    },
    {
        "loss": 1.6969,
        "grad_norm": 2.6524500846862793,
        "learning_rate": 8.238352016489635e-05,
        "epoch": 0.5205837808068291,
        "step": 3781
    },
    {
        "loss": 1.0932,
        "grad_norm": 2.872408151626587,
        "learning_rate": 8.225535910760133e-05,
        "epoch": 0.5207214649593832,
        "step": 3782
    },
    {
        "loss": 2.0583,
        "grad_norm": 2.614954710006714,
        "learning_rate": 8.212722813684662e-05,
        "epoch": 0.5208591491119372,
        "step": 3783
    },
    {
        "loss": 2.5484,
        "grad_norm": 2.375628709793091,
        "learning_rate": 8.199912746988183e-05,
        "epoch": 0.5209968332644913,
        "step": 3784
    },
    {
        "loss": 2.2044,
        "grad_norm": 1.4886795282363892,
        "learning_rate": 8.187105732390543e-05,
        "epoch": 0.5211345174170453,
        "step": 3785
    },
    {
        "loss": 2.2128,
        "grad_norm": 1.3759329319000244,
        "learning_rate": 8.174301791606389e-05,
        "epoch": 0.5212722015695993,
        "step": 3786
    },
    {
        "loss": 2.267,
        "grad_norm": 1.450350284576416,
        "learning_rate": 8.161500946345157e-05,
        "epoch": 0.5214098857221534,
        "step": 3787
    },
    {
        "loss": 2.2022,
        "grad_norm": 1.6881775856018066,
        "learning_rate": 8.148703218311056e-05,
        "epoch": 0.5215475698747074,
        "step": 3788
    },
    {
        "loss": 1.9574,
        "grad_norm": 1.880164384841919,
        "learning_rate": 8.135908629202988e-05,
        "epoch": 0.5216852540272615,
        "step": 3789
    },
    {
        "loss": 2.3424,
        "grad_norm": 1.413509726524353,
        "learning_rate": 8.12311720071453e-05,
        "epoch": 0.5218229381798155,
        "step": 3790
    },
    {
        "loss": 1.812,
        "grad_norm": 1.6471737623214722,
        "learning_rate": 8.110328954533935e-05,
        "epoch": 0.5219606223323695,
        "step": 3791
    },
    {
        "loss": 1.5432,
        "grad_norm": 2.2473559379577637,
        "learning_rate": 8.097543912344025e-05,
        "epoch": 0.5220983064849236,
        "step": 3792
    },
    {
        "loss": 1.7851,
        "grad_norm": 2.3918135166168213,
        "learning_rate": 8.084762095822182e-05,
        "epoch": 0.5222359906374776,
        "step": 3793
    },
    {
        "loss": 2.587,
        "grad_norm": 2.5156426429748535,
        "learning_rate": 8.07198352664037e-05,
        "epoch": 0.5223736747900317,
        "step": 3794
    },
    {
        "loss": 2.1229,
        "grad_norm": 1.4566917419433594,
        "learning_rate": 8.059208226464998e-05,
        "epoch": 0.5225113589425857,
        "step": 3795
    },
    {
        "loss": 2.4263,
        "grad_norm": 1.99540376663208,
        "learning_rate": 8.046436216956943e-05,
        "epoch": 0.5226490430951397,
        "step": 3796
    },
    {
        "loss": 1.9294,
        "grad_norm": 3.157233476638794,
        "learning_rate": 8.033667519771519e-05,
        "epoch": 0.5227867272476938,
        "step": 3797
    },
    {
        "loss": 1.5246,
        "grad_norm": 2.422827959060669,
        "learning_rate": 8.02090215655841e-05,
        "epoch": 0.5229244114002478,
        "step": 3798
    },
    {
        "loss": 1.5048,
        "grad_norm": 2.0486819744110107,
        "learning_rate": 8.008140148961641e-05,
        "epoch": 0.5230620955528019,
        "step": 3799
    },
    {
        "loss": 1.6721,
        "grad_norm": 1.9578588008880615,
        "learning_rate": 7.99538151861957e-05,
        "epoch": 0.5231997797053559,
        "step": 3800
    },
    {
        "loss": 2.1238,
        "grad_norm": 1.2227855920791626,
        "learning_rate": 7.982626287164812e-05,
        "epoch": 0.5233374638579099,
        "step": 3801
    },
    {
        "loss": 2.0127,
        "grad_norm": 2.14798641204834,
        "learning_rate": 7.969874476224207e-05,
        "epoch": 0.523475148010464,
        "step": 3802
    },
    {
        "loss": 2.3829,
        "grad_norm": 1.5477195978164673,
        "learning_rate": 7.957126107418834e-05,
        "epoch": 0.523612832163018,
        "step": 3803
    },
    {
        "loss": 1.8936,
        "grad_norm": 1.5232818126678467,
        "learning_rate": 7.944381202363906e-05,
        "epoch": 0.5237505163155721,
        "step": 3804
    },
    {
        "loss": 2.1505,
        "grad_norm": 2.2131786346435547,
        "learning_rate": 7.931639782668754e-05,
        "epoch": 0.5238882004681261,
        "step": 3805
    },
    {
        "loss": 1.9813,
        "grad_norm": 2.8221893310546875,
        "learning_rate": 7.918901869936839e-05,
        "epoch": 0.5240258846206801,
        "step": 3806
    },
    {
        "loss": 2.5012,
        "grad_norm": 2.747062921524048,
        "learning_rate": 7.906167485765643e-05,
        "epoch": 0.5241635687732342,
        "step": 3807
    },
    {
        "loss": 2.1096,
        "grad_norm": 2.191197633743286,
        "learning_rate": 7.893436651746667e-05,
        "epoch": 0.5243012529257882,
        "step": 3808
    },
    {
        "loss": 2.3737,
        "grad_norm": 1.4495269060134888,
        "learning_rate": 7.880709389465409e-05,
        "epoch": 0.5244389370783423,
        "step": 3809
    },
    {
        "loss": 1.7335,
        "grad_norm": 1.6149179935455322,
        "learning_rate": 7.867985720501302e-05,
        "epoch": 0.5245766212308963,
        "step": 3810
    },
    {
        "loss": 1.908,
        "grad_norm": 2.1179778575897217,
        "learning_rate": 7.855265666427676e-05,
        "epoch": 0.5247143053834503,
        "step": 3811
    },
    {
        "loss": 1.8916,
        "grad_norm": 1.7151343822479248,
        "learning_rate": 7.842549248811757e-05,
        "epoch": 0.5248519895360044,
        "step": 3812
    },
    {
        "loss": 0.9985,
        "grad_norm": 1.7159628868103027,
        "learning_rate": 7.82983648921458e-05,
        "epoch": 0.5249896736885584,
        "step": 3813
    },
    {
        "loss": 1.8853,
        "grad_norm": 0.9717121720314026,
        "learning_rate": 7.817127409190981e-05,
        "epoch": 0.5251273578411125,
        "step": 3814
    },
    {
        "loss": 1.7694,
        "grad_norm": 2.495818853378296,
        "learning_rate": 7.80442203028959e-05,
        "epoch": 0.5252650419936665,
        "step": 3815
    },
    {
        "loss": 2.0296,
        "grad_norm": 1.4614537954330444,
        "learning_rate": 7.791720374052715e-05,
        "epoch": 0.5254027261462205,
        "step": 3816
    },
    {
        "loss": 1.0284,
        "grad_norm": 2.422245740890503,
        "learning_rate": 7.779022462016371e-05,
        "epoch": 0.5255404102987746,
        "step": 3817
    },
    {
        "loss": 1.9748,
        "grad_norm": 2.54183292388916,
        "learning_rate": 7.766328315710245e-05,
        "epoch": 0.5256780944513286,
        "step": 3818
    },
    {
        "loss": 2.1882,
        "grad_norm": 2.1423254013061523,
        "learning_rate": 7.753637956657614e-05,
        "epoch": 0.5258157786038827,
        "step": 3819
    },
    {
        "loss": 1.442,
        "grad_norm": 2.4383487701416016,
        "learning_rate": 7.74095140637533e-05,
        "epoch": 0.5259534627564367,
        "step": 3820
    },
    {
        "loss": 2.0475,
        "grad_norm": 1.6992220878601074,
        "learning_rate": 7.728268686373814e-05,
        "epoch": 0.5260911469089907,
        "step": 3821
    },
    {
        "loss": 0.8806,
        "grad_norm": 3.75656795501709,
        "learning_rate": 7.715589818156964e-05,
        "epoch": 0.5262288310615448,
        "step": 3822
    },
    {
        "loss": 1.8877,
        "grad_norm": 2.0816197395324707,
        "learning_rate": 7.702914823222159e-05,
        "epoch": 0.5263665152140988,
        "step": 3823
    },
    {
        "loss": 1.519,
        "grad_norm": 2.0658798217773438,
        "learning_rate": 7.690243723060223e-05,
        "epoch": 0.5265041993666529,
        "step": 3824
    },
    {
        "loss": 1.6765,
        "grad_norm": 1.5429816246032715,
        "learning_rate": 7.677576539155355e-05,
        "epoch": 0.5266418835192069,
        "step": 3825
    },
    {
        "loss": 1.2484,
        "grad_norm": 2.5028746128082275,
        "learning_rate": 7.664913292985113e-05,
        "epoch": 0.5267795676717609,
        "step": 3826
    },
    {
        "loss": 2.4889,
        "grad_norm": 1.3370018005371094,
        "learning_rate": 7.652254006020417e-05,
        "epoch": 0.526917251824315,
        "step": 3827
    },
    {
        "loss": 1.9939,
        "grad_norm": 2.3847620487213135,
        "learning_rate": 7.639598699725426e-05,
        "epoch": 0.527054935976869,
        "step": 3828
    },
    {
        "loss": 2.3715,
        "grad_norm": 1.6290433406829834,
        "learning_rate": 7.626947395557563e-05,
        "epoch": 0.5271926201294231,
        "step": 3829
    },
    {
        "loss": 2.3885,
        "grad_norm": 1.7165844440460205,
        "learning_rate": 7.614300114967498e-05,
        "epoch": 0.5273303042819771,
        "step": 3830
    },
    {
        "loss": 2.5111,
        "grad_norm": 1.83820641040802,
        "learning_rate": 7.601656879399034e-05,
        "epoch": 0.5274679884345311,
        "step": 3831
    },
    {
        "loss": 1.7655,
        "grad_norm": 1.5053085088729858,
        "learning_rate": 7.589017710289135e-05,
        "epoch": 0.5276056725870852,
        "step": 3832
    },
    {
        "loss": 1.6308,
        "grad_norm": 1.8317468166351318,
        "learning_rate": 7.576382629067877e-05,
        "epoch": 0.5277433567396392,
        "step": 3833
    },
    {
        "loss": 1.9199,
        "grad_norm": 1.9386359453201294,
        "learning_rate": 7.563751657158394e-05,
        "epoch": 0.5278810408921933,
        "step": 3834
    },
    {
        "loss": 2.2693,
        "grad_norm": 1.438774824142456,
        "learning_rate": 7.551124815976848e-05,
        "epoch": 0.5280187250447473,
        "step": 3835
    },
    {
        "loss": 2.195,
        "grad_norm": 2.1585607528686523,
        "learning_rate": 7.538502126932419e-05,
        "epoch": 0.5281564091973013,
        "step": 3836
    },
    {
        "loss": 2.2488,
        "grad_norm": 1.3733434677124023,
        "learning_rate": 7.525883611427228e-05,
        "epoch": 0.5282940933498554,
        "step": 3837
    },
    {
        "loss": 1.6879,
        "grad_norm": 2.1859498023986816,
        "learning_rate": 7.513269290856307e-05,
        "epoch": 0.5284317775024094,
        "step": 3838
    },
    {
        "loss": 2.721,
        "grad_norm": 1.5113352537155151,
        "learning_rate": 7.500659186607626e-05,
        "epoch": 0.5285694616549635,
        "step": 3839
    },
    {
        "loss": 1.572,
        "grad_norm": 3.1366841793060303,
        "learning_rate": 7.488053320061955e-05,
        "epoch": 0.5287071458075175,
        "step": 3840
    },
    {
        "loss": 2.7425,
        "grad_norm": 1.87868070602417,
        "learning_rate": 7.475451712592885e-05,
        "epoch": 0.5288448299600716,
        "step": 3841
    },
    {
        "loss": 2.1779,
        "grad_norm": 1.7000181674957275,
        "learning_rate": 7.462854385566825e-05,
        "epoch": 0.5289825141126256,
        "step": 3842
    },
    {
        "loss": 1.9741,
        "grad_norm": 1.2344955205917358,
        "learning_rate": 7.450261360342888e-05,
        "epoch": 0.5291201982651796,
        "step": 3843
    },
    {
        "loss": 2.4868,
        "grad_norm": 1.5313791036605835,
        "learning_rate": 7.437672658272891e-05,
        "epoch": 0.5292578824177337,
        "step": 3844
    },
    {
        "loss": 2.3817,
        "grad_norm": 2.43581223487854,
        "learning_rate": 7.425088300701355e-05,
        "epoch": 0.5293955665702877,
        "step": 3845
    },
    {
        "loss": 2.4168,
        "grad_norm": 1.545271396636963,
        "learning_rate": 7.412508308965404e-05,
        "epoch": 0.5295332507228419,
        "step": 3846
    },
    {
        "loss": 1.9022,
        "grad_norm": 1.92232346534729,
        "learning_rate": 7.399932704394762e-05,
        "epoch": 0.5296709348753958,
        "step": 3847
    },
    {
        "loss": 1.8705,
        "grad_norm": 2.267449378967285,
        "learning_rate": 7.387361508311739e-05,
        "epoch": 0.5298086190279498,
        "step": 3848
    },
    {
        "loss": 2.2817,
        "grad_norm": 1.8086731433868408,
        "learning_rate": 7.374794742031143e-05,
        "epoch": 0.529946303180504,
        "step": 3849
    },
    {
        "loss": 2.1185,
        "grad_norm": 2.0264368057250977,
        "learning_rate": 7.362232426860269e-05,
        "epoch": 0.530083987333058,
        "step": 3850
    },
    {
        "loss": 2.2075,
        "grad_norm": 1.219844102859497,
        "learning_rate": 7.349674584098906e-05,
        "epoch": 0.5302216714856121,
        "step": 3851
    },
    {
        "loss": 1.5439,
        "grad_norm": 1.9060958623886108,
        "learning_rate": 7.337121235039217e-05,
        "epoch": 0.530359355638166,
        "step": 3852
    },
    {
        "loss": 2.2074,
        "grad_norm": 1.4352365732192993,
        "learning_rate": 7.324572400965745e-05,
        "epoch": 0.53049703979072,
        "step": 3853
    },
    {
        "loss": 2.3056,
        "grad_norm": 1.554126262664795,
        "learning_rate": 7.312028103155423e-05,
        "epoch": 0.5306347239432742,
        "step": 3854
    },
    {
        "loss": 2.3487,
        "grad_norm": 2.592517137527466,
        "learning_rate": 7.299488362877449e-05,
        "epoch": 0.5307724080958282,
        "step": 3855
    },
    {
        "loss": 2.5042,
        "grad_norm": 1.3159924745559692,
        "learning_rate": 7.286953201393303e-05,
        "epoch": 0.5309100922483823,
        "step": 3856
    },
    {
        "loss": 1.9756,
        "grad_norm": 1.8363860845565796,
        "learning_rate": 7.274422639956704e-05,
        "epoch": 0.5310477764009363,
        "step": 3857
    },
    {
        "loss": 2.9349,
        "grad_norm": 1.4517896175384521,
        "learning_rate": 7.261896699813584e-05,
        "epoch": 0.5311854605534903,
        "step": 3858
    },
    {
        "loss": 1.693,
        "grad_norm": 2.6174774169921875,
        "learning_rate": 7.249375402202024e-05,
        "epoch": 0.5313231447060444,
        "step": 3859
    },
    {
        "loss": 1.5043,
        "grad_norm": 2.3390095233917236,
        "learning_rate": 7.236858768352226e-05,
        "epoch": 0.5314608288585984,
        "step": 3860
    },
    {
        "loss": 1.9733,
        "grad_norm": 1.874277114868164,
        "learning_rate": 7.224346819486517e-05,
        "epoch": 0.5315985130111525,
        "step": 3861
    },
    {
        "loss": 1.3933,
        "grad_norm": 1.8539353609085083,
        "learning_rate": 7.21183957681925e-05,
        "epoch": 0.5317361971637065,
        "step": 3862
    },
    {
        "loss": 2.3708,
        "grad_norm": 1.9430698156356812,
        "learning_rate": 7.199337061556794e-05,
        "epoch": 0.5318738813162605,
        "step": 3863
    },
    {
        "loss": 2.1134,
        "grad_norm": 2.8150012493133545,
        "learning_rate": 7.186839294897545e-05,
        "epoch": 0.5320115654688146,
        "step": 3864
    },
    {
        "loss": 1.3659,
        "grad_norm": 1.1571779251098633,
        "learning_rate": 7.174346298031809e-05,
        "epoch": 0.5321492496213686,
        "step": 3865
    },
    {
        "loss": 1.9442,
        "grad_norm": 1.700577974319458,
        "learning_rate": 7.161858092141796e-05,
        "epoch": 0.5322869337739227,
        "step": 3866
    },
    {
        "loss": 2.5125,
        "grad_norm": 2.200587749481201,
        "learning_rate": 7.149374698401645e-05,
        "epoch": 0.5324246179264767,
        "step": 3867
    },
    {
        "loss": 2.1175,
        "grad_norm": 2.293922185897827,
        "learning_rate": 7.136896137977287e-05,
        "epoch": 0.5325623020790307,
        "step": 3868
    },
    {
        "loss": 1.8016,
        "grad_norm": 1.1155354976654053,
        "learning_rate": 7.124422432026466e-05,
        "epoch": 0.5326999862315848,
        "step": 3869
    },
    {
        "loss": 1.9512,
        "grad_norm": 1.9592951536178589,
        "learning_rate": 7.111953601698719e-05,
        "epoch": 0.5328376703841388,
        "step": 3870
    },
    {
        "loss": 1.1205,
        "grad_norm": 3.2497546672821045,
        "learning_rate": 7.099489668135293e-05,
        "epoch": 0.5329753545366929,
        "step": 3871
    },
    {
        "loss": 1.8641,
        "grad_norm": 1.7448153495788574,
        "learning_rate": 7.087030652469131e-05,
        "epoch": 0.5331130386892469,
        "step": 3872
    },
    {
        "loss": 2.0704,
        "grad_norm": 2.6506223678588867,
        "learning_rate": 7.074576575824863e-05,
        "epoch": 0.5332507228418009,
        "step": 3873
    },
    {
        "loss": 2.2536,
        "grad_norm": 2.124117136001587,
        "learning_rate": 7.062127459318719e-05,
        "epoch": 0.533388406994355,
        "step": 3874
    },
    {
        "loss": 2.6169,
        "grad_norm": 1.188998818397522,
        "learning_rate": 7.049683324058516e-05,
        "epoch": 0.533526091146909,
        "step": 3875
    },
    {
        "loss": 1.4716,
        "grad_norm": 2.7511472702026367,
        "learning_rate": 7.037244191143661e-05,
        "epoch": 0.5336637752994631,
        "step": 3876
    },
    {
        "loss": 2.2991,
        "grad_norm": 2.2091567516326904,
        "learning_rate": 7.024810081665047e-05,
        "epoch": 0.5338014594520171,
        "step": 3877
    },
    {
        "loss": 1.8211,
        "grad_norm": 1.7081488370895386,
        "learning_rate": 7.012381016705041e-05,
        "epoch": 0.5339391436045711,
        "step": 3878
    },
    {
        "loss": 2.4228,
        "grad_norm": 2.253223419189453,
        "learning_rate": 6.999957017337502e-05,
        "epoch": 0.5340768277571252,
        "step": 3879
    },
    {
        "loss": 2.2741,
        "grad_norm": 1.916795253753662,
        "learning_rate": 6.98753810462766e-05,
        "epoch": 0.5342145119096792,
        "step": 3880
    },
    {
        "loss": 1.9295,
        "grad_norm": 2.5254364013671875,
        "learning_rate": 6.975124299632122e-05,
        "epoch": 0.5343521960622333,
        "step": 3881
    },
    {
        "loss": 1.7916,
        "grad_norm": 2.0737740993499756,
        "learning_rate": 6.962715623398864e-05,
        "epoch": 0.5344898802147873,
        "step": 3882
    },
    {
        "loss": 1.8168,
        "grad_norm": 2.4540796279907227,
        "learning_rate": 6.950312096967137e-05,
        "epoch": 0.5346275643673413,
        "step": 3883
    },
    {
        "loss": 1.3289,
        "grad_norm": 2.2462100982666016,
        "learning_rate": 6.937913741367464e-05,
        "epoch": 0.5347652485198954,
        "step": 3884
    },
    {
        "loss": 2.039,
        "grad_norm": 2.2535150051116943,
        "learning_rate": 6.925520577621624e-05,
        "epoch": 0.5349029326724494,
        "step": 3885
    },
    {
        "loss": 2.2195,
        "grad_norm": 2.105436086654663,
        "learning_rate": 6.913132626742567e-05,
        "epoch": 0.5350406168250035,
        "step": 3886
    },
    {
        "loss": 1.4461,
        "grad_norm": 2.2947089672088623,
        "learning_rate": 6.9007499097344e-05,
        "epoch": 0.5351783009775575,
        "step": 3887
    },
    {
        "loss": 1.9132,
        "grad_norm": 2.124558448791504,
        "learning_rate": 6.888372447592402e-05,
        "epoch": 0.5353159851301115,
        "step": 3888
    },
    {
        "loss": 2.4546,
        "grad_norm": 1.7152103185653687,
        "learning_rate": 6.876000261302889e-05,
        "epoch": 0.5354536692826656,
        "step": 3889
    },
    {
        "loss": 2.4984,
        "grad_norm": 1.8196057081222534,
        "learning_rate": 6.863633371843243e-05,
        "epoch": 0.5355913534352196,
        "step": 3890
    },
    {
        "loss": 1.1372,
        "grad_norm": 1.9371616840362549,
        "learning_rate": 6.851271800181902e-05,
        "epoch": 0.5357290375877737,
        "step": 3891
    },
    {
        "loss": 1.5482,
        "grad_norm": 2.2509334087371826,
        "learning_rate": 6.838915567278248e-05,
        "epoch": 0.5358667217403277,
        "step": 3892
    },
    {
        "loss": 1.9866,
        "grad_norm": 2.302032709121704,
        "learning_rate": 6.826564694082612e-05,
        "epoch": 0.5360044058928817,
        "step": 3893
    },
    {
        "loss": 2.0505,
        "grad_norm": 2.1355254650115967,
        "learning_rate": 6.814219201536265e-05,
        "epoch": 0.5361420900454358,
        "step": 3894
    },
    {
        "loss": 2.0503,
        "grad_norm": 3.9879150390625,
        "learning_rate": 6.801879110571333e-05,
        "epoch": 0.5362797741979898,
        "step": 3895
    },
    {
        "loss": 2.2954,
        "grad_norm": 0.9245132803916931,
        "learning_rate": 6.789544442110782e-05,
        "epoch": 0.5364174583505439,
        "step": 3896
    },
    {
        "loss": 2.256,
        "grad_norm": 1.7813800573349,
        "learning_rate": 6.7772152170684e-05,
        "epoch": 0.5365551425030979,
        "step": 3897
    },
    {
        "loss": 1.5278,
        "grad_norm": 2.4258177280426025,
        "learning_rate": 6.764891456348736e-05,
        "epoch": 0.536692826655652,
        "step": 3898
    },
    {
        "loss": 2.242,
        "grad_norm": 1.7520406246185303,
        "learning_rate": 6.752573180847061e-05,
        "epoch": 0.536830510808206,
        "step": 3899
    },
    {
        "loss": 1.9316,
        "grad_norm": 1.6890535354614258,
        "learning_rate": 6.74026041144938e-05,
        "epoch": 0.53696819496076,
        "step": 3900
    },
    {
        "loss": 1.0086,
        "grad_norm": 2.374720811843872,
        "learning_rate": 6.727953169032335e-05,
        "epoch": 0.5371058791133141,
        "step": 3901
    },
    {
        "loss": 2.1544,
        "grad_norm": 1.4428386688232422,
        "learning_rate": 6.715651474463185e-05,
        "epoch": 0.5372435632658681,
        "step": 3902
    },
    {
        "loss": 2.5213,
        "grad_norm": 1.3377065658569336,
        "learning_rate": 6.703355348599832e-05,
        "epoch": 0.5373812474184222,
        "step": 3903
    },
    {
        "loss": 2.1027,
        "grad_norm": 2.1529388427734375,
        "learning_rate": 6.69106481229069e-05,
        "epoch": 0.5375189315709762,
        "step": 3904
    },
    {
        "loss": 2.0552,
        "grad_norm": 2.271204948425293,
        "learning_rate": 6.678779886374704e-05,
        "epoch": 0.5376566157235302,
        "step": 3905
    },
    {
        "loss": 1.775,
        "grad_norm": 2.051915168762207,
        "learning_rate": 6.666500591681325e-05,
        "epoch": 0.5377942998760843,
        "step": 3906
    },
    {
        "loss": 2.1238,
        "grad_norm": 2.476149559020996,
        "learning_rate": 6.65422694903044e-05,
        "epoch": 0.5379319840286383,
        "step": 3907
    },
    {
        "loss": 1.6619,
        "grad_norm": 2.75299334526062,
        "learning_rate": 6.641958979232352e-05,
        "epoch": 0.5380696681811924,
        "step": 3908
    },
    {
        "loss": 0.7934,
        "grad_norm": 2.3583614826202393,
        "learning_rate": 6.629696703087756e-05,
        "epoch": 0.5382073523337464,
        "step": 3909
    },
    {
        "loss": 1.1202,
        "grad_norm": 2.6008198261260986,
        "learning_rate": 6.61744014138769e-05,
        "epoch": 0.5383450364863004,
        "step": 3910
    },
    {
        "loss": 2.419,
        "grad_norm": 1.9156794548034668,
        "learning_rate": 6.605189314913487e-05,
        "epoch": 0.5384827206388545,
        "step": 3911
    },
    {
        "loss": 1.4882,
        "grad_norm": 2.1083102226257324,
        "learning_rate": 6.592944244436788e-05,
        "epoch": 0.5386204047914085,
        "step": 3912
    },
    {
        "loss": 2.1237,
        "grad_norm": 2.0432932376861572,
        "learning_rate": 6.580704950719452e-05,
        "epoch": 0.5387580889439626,
        "step": 3913
    },
    {
        "loss": 1.5428,
        "grad_norm": 2.1729400157928467,
        "learning_rate": 6.568471454513532e-05,
        "epoch": 0.5388957730965166,
        "step": 3914
    },
    {
        "loss": 2.377,
        "grad_norm": 1.263625979423523,
        "learning_rate": 6.556243776561288e-05,
        "epoch": 0.5390334572490706,
        "step": 3915
    },
    {
        "loss": 0.9472,
        "grad_norm": 2.2917990684509277,
        "learning_rate": 6.544021937595086e-05,
        "epoch": 0.5391711414016247,
        "step": 3916
    },
    {
        "loss": 1.6139,
        "grad_norm": 1.9713279008865356,
        "learning_rate": 6.531805958337393e-05,
        "epoch": 0.5393088255541787,
        "step": 3917
    },
    {
        "loss": 1.6846,
        "grad_norm": 1.8511717319488525,
        "learning_rate": 6.519595859500756e-05,
        "epoch": 0.5394465097067328,
        "step": 3918
    },
    {
        "loss": 2.1663,
        "grad_norm": 2.184256076812744,
        "learning_rate": 6.50739166178774e-05,
        "epoch": 0.5395841938592868,
        "step": 3919
    },
    {
        "loss": 1.3269,
        "grad_norm": 2.7797513008117676,
        "learning_rate": 6.495193385890901e-05,
        "epoch": 0.5397218780118408,
        "step": 3920
    },
    {
        "loss": 1.8445,
        "grad_norm": 2.7947089672088623,
        "learning_rate": 6.483001052492772e-05,
        "epoch": 0.5398595621643949,
        "step": 3921
    },
    {
        "loss": 1.902,
        "grad_norm": 2.5604894161224365,
        "learning_rate": 6.470814682265792e-05,
        "epoch": 0.5399972463169489,
        "step": 3922
    },
    {
        "loss": 1.6962,
        "grad_norm": 3.079280138015747,
        "learning_rate": 6.458634295872283e-05,
        "epoch": 0.540134930469503,
        "step": 3923
    },
    {
        "loss": 1.9638,
        "grad_norm": 2.4549639225006104,
        "learning_rate": 6.446459913964465e-05,
        "epoch": 0.540272614622057,
        "step": 3924
    },
    {
        "loss": 1.646,
        "grad_norm": 3.1799330711364746,
        "learning_rate": 6.434291557184325e-05,
        "epoch": 0.540410298774611,
        "step": 3925
    },
    {
        "loss": 1.9041,
        "grad_norm": 2.7726495265960693,
        "learning_rate": 6.422129246163653e-05,
        "epoch": 0.5405479829271651,
        "step": 3926
    },
    {
        "loss": 0.8335,
        "grad_norm": 1.9638371467590332,
        "learning_rate": 6.40997300152401e-05,
        "epoch": 0.5406856670797191,
        "step": 3927
    },
    {
        "loss": 2.2966,
        "grad_norm": 2.0939958095550537,
        "learning_rate": 6.397822843876641e-05,
        "epoch": 0.5408233512322732,
        "step": 3928
    },
    {
        "loss": 1.2427,
        "grad_norm": 2.2826364040374756,
        "learning_rate": 6.385678793822479e-05,
        "epoch": 0.5409610353848272,
        "step": 3929
    },
    {
        "loss": 2.171,
        "grad_norm": 2.2298364639282227,
        "learning_rate": 6.373540871952114e-05,
        "epoch": 0.5410987195373812,
        "step": 3930
    },
    {
        "loss": 1.7791,
        "grad_norm": 1.797353744506836,
        "learning_rate": 6.361409098845728e-05,
        "epoch": 0.5412364036899353,
        "step": 3931
    },
    {
        "loss": 2.2368,
        "grad_norm": 1.2478224039077759,
        "learning_rate": 6.349283495073081e-05,
        "epoch": 0.5413740878424893,
        "step": 3932
    },
    {
        "loss": 2.5462,
        "grad_norm": 1.9085341691970825,
        "learning_rate": 6.33716408119349e-05,
        "epoch": 0.5415117719950434,
        "step": 3933
    },
    {
        "loss": 2.1723,
        "grad_norm": 1.8168015480041504,
        "learning_rate": 6.325050877755755e-05,
        "epoch": 0.5416494561475974,
        "step": 3934
    },
    {
        "loss": 2.3923,
        "grad_norm": 1.4119313955307007,
        "learning_rate": 6.312943905298146e-05,
        "epoch": 0.5417871403001514,
        "step": 3935
    },
    {
        "loss": 2.046,
        "grad_norm": 2.0820837020874023,
        "learning_rate": 6.300843184348399e-05,
        "epoch": 0.5419248244527055,
        "step": 3936
    },
    {
        "loss": 2.3761,
        "grad_norm": 1.844064712524414,
        "learning_rate": 6.288748735423617e-05,
        "epoch": 0.5420625086052595,
        "step": 3937
    },
    {
        "loss": 1.9328,
        "grad_norm": 2.706583023071289,
        "learning_rate": 6.276660579030272e-05,
        "epoch": 0.5422001927578136,
        "step": 3938
    },
    {
        "loss": 1.9393,
        "grad_norm": 2.148592233657837,
        "learning_rate": 6.264578735664193e-05,
        "epoch": 0.5423378769103676,
        "step": 3939
    },
    {
        "loss": 2.4653,
        "grad_norm": 1.8275612592697144,
        "learning_rate": 6.252503225810475e-05,
        "epoch": 0.5424755610629216,
        "step": 3940
    },
    {
        "loss": 2.3085,
        "grad_norm": 1.2705739736557007,
        "learning_rate": 6.24043406994349e-05,
        "epoch": 0.5426132452154757,
        "step": 3941
    },
    {
        "loss": 2.0636,
        "grad_norm": 3.153392791748047,
        "learning_rate": 6.228371288526829e-05,
        "epoch": 0.5427509293680297,
        "step": 3942
    },
    {
        "loss": 2.4059,
        "grad_norm": 2.030039072036743,
        "learning_rate": 6.216314902013284e-05,
        "epoch": 0.5428886135205838,
        "step": 3943
    },
    {
        "loss": 2.3484,
        "grad_norm": 1.6420671939849854,
        "learning_rate": 6.204264930844796e-05,
        "epoch": 0.5430262976731378,
        "step": 3944
    },
    {
        "loss": 1.2177,
        "grad_norm": 1.985564112663269,
        "learning_rate": 6.19222139545243e-05,
        "epoch": 0.5431639818256918,
        "step": 3945
    },
    {
        "loss": 2.2178,
        "grad_norm": 2.6948611736297607,
        "learning_rate": 6.180184316256348e-05,
        "epoch": 0.5433016659782459,
        "step": 3946
    },
    {
        "loss": 1.833,
        "grad_norm": 2.1447391510009766,
        "learning_rate": 6.168153713665752e-05,
        "epoch": 0.5434393501307999,
        "step": 3947
    },
    {
        "loss": 1.8198,
        "grad_norm": 2.9957492351531982,
        "learning_rate": 6.15612960807886e-05,
        "epoch": 0.543577034283354,
        "step": 3948
    },
    {
        "loss": 1.072,
        "grad_norm": 2.343475341796875,
        "learning_rate": 6.144112019882906e-05,
        "epoch": 0.543714718435908,
        "step": 3949
    },
    {
        "loss": 2.2311,
        "grad_norm": 1.5536221265792847,
        "learning_rate": 6.132100969454035e-05,
        "epoch": 0.543852402588462,
        "step": 3950
    },
    {
        "loss": 2.1961,
        "grad_norm": 1.270261526107788,
        "learning_rate": 6.120096477157316e-05,
        "epoch": 0.5439900867410161,
        "step": 3951
    },
    {
        "loss": 1.6188,
        "grad_norm": 1.823164463043213,
        "learning_rate": 6.108098563346726e-05,
        "epoch": 0.5441277708935701,
        "step": 3952
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.5305215120315552,
        "learning_rate": 6.0961072483650526e-05,
        "epoch": 0.5442654550461242,
        "step": 3953
    },
    {
        "loss": 1.7865,
        "grad_norm": 1.6173434257507324,
        "learning_rate": 6.084122552543909e-05,
        "epoch": 0.5444031391986782,
        "step": 3954
    },
    {
        "loss": 2.6794,
        "grad_norm": 1.2864972352981567,
        "learning_rate": 6.072144496203697e-05,
        "epoch": 0.5445408233512323,
        "step": 3955
    },
    {
        "loss": 2.1586,
        "grad_norm": 1.8629814386367798,
        "learning_rate": 6.060173099653539e-05,
        "epoch": 0.5446785075037863,
        "step": 3956
    },
    {
        "loss": 1.2895,
        "grad_norm": 2.312886953353882,
        "learning_rate": 6.048208383191274e-05,
        "epoch": 0.5448161916563403,
        "step": 3957
    },
    {
        "loss": 2.1053,
        "grad_norm": 1.9448689222335815,
        "learning_rate": 6.0362503671034285e-05,
        "epoch": 0.5449538758088944,
        "step": 3958
    },
    {
        "loss": 2.0197,
        "grad_norm": 2.2407727241516113,
        "learning_rate": 6.0242990716651515e-05,
        "epoch": 0.5450915599614484,
        "step": 3959
    },
    {
        "loss": 2.415,
        "grad_norm": 1.499668002128601,
        "learning_rate": 6.012354517140187e-05,
        "epoch": 0.5452292441140025,
        "step": 3960
    },
    {
        "loss": 1.1038,
        "grad_norm": 1.6568479537963867,
        "learning_rate": 6.000416723780894e-05,
        "epoch": 0.5453669282665565,
        "step": 3961
    },
    {
        "loss": 1.5014,
        "grad_norm": 2.482482433319092,
        "learning_rate": 5.9884857118281214e-05,
        "epoch": 0.5455046124191105,
        "step": 3962
    },
    {
        "loss": 2.0151,
        "grad_norm": 2.0929183959960938,
        "learning_rate": 5.97656150151123e-05,
        "epoch": 0.5456422965716646,
        "step": 3963
    },
    {
        "loss": 1.8994,
        "grad_norm": 1.7383188009262085,
        "learning_rate": 5.964644113048077e-05,
        "epoch": 0.5457799807242186,
        "step": 3964
    },
    {
        "loss": 1.8962,
        "grad_norm": 2.6099228858947754,
        "learning_rate": 5.952733566644918e-05,
        "epoch": 0.5459176648767727,
        "step": 3965
    },
    {
        "loss": 2.1564,
        "grad_norm": 2.3021042346954346,
        "learning_rate": 5.9408298824964195e-05,
        "epoch": 0.5460553490293267,
        "step": 3966
    },
    {
        "loss": 2.4548,
        "grad_norm": 2.4270267486572266,
        "learning_rate": 5.9289330807856234e-05,
        "epoch": 0.5461930331818807,
        "step": 3967
    },
    {
        "loss": 2.3302,
        "grad_norm": 1.730658769607544,
        "learning_rate": 5.917043181683889e-05,
        "epoch": 0.5463307173344348,
        "step": 3968
    },
    {
        "loss": 2.3136,
        "grad_norm": 1.6774177551269531,
        "learning_rate": 5.905160205350866e-05,
        "epoch": 0.5464684014869888,
        "step": 3969
    },
    {
        "loss": 2.204,
        "grad_norm": 2.5038962364196777,
        "learning_rate": 5.893284171934496e-05,
        "epoch": 0.5466060856395429,
        "step": 3970
    },
    {
        "loss": 2.2126,
        "grad_norm": 1.9722179174423218,
        "learning_rate": 5.881415101570917e-05,
        "epoch": 0.5467437697920969,
        "step": 3971
    },
    {
        "loss": 1.8673,
        "grad_norm": 3.380497455596924,
        "learning_rate": 5.8695530143844635e-05,
        "epoch": 0.5468814539446509,
        "step": 3972
    },
    {
        "loss": 2.3908,
        "grad_norm": 1.6867066621780396,
        "learning_rate": 5.857697930487667e-05,
        "epoch": 0.547019138097205,
        "step": 3973
    },
    {
        "loss": 2.4183,
        "grad_norm": 2.235435962677002,
        "learning_rate": 5.8458498699811415e-05,
        "epoch": 0.547156822249759,
        "step": 3974
    },
    {
        "loss": 1.7614,
        "grad_norm": 1.9669965505599976,
        "learning_rate": 5.834008852953594e-05,
        "epoch": 0.5472945064023131,
        "step": 3975
    },
    {
        "loss": 2.3467,
        "grad_norm": 1.9926023483276367,
        "learning_rate": 5.822174899481836e-05,
        "epoch": 0.5474321905548671,
        "step": 3976
    },
    {
        "loss": 1.9408,
        "grad_norm": 2.4864370822906494,
        "learning_rate": 5.810348029630652e-05,
        "epoch": 0.5475698747074211,
        "step": 3977
    },
    {
        "loss": 2.2225,
        "grad_norm": 2.471511125564575,
        "learning_rate": 5.798528263452827e-05,
        "epoch": 0.5477075588599752,
        "step": 3978
    },
    {
        "loss": 2.2481,
        "grad_norm": 1.8150877952575684,
        "learning_rate": 5.786715620989127e-05,
        "epoch": 0.5478452430125292,
        "step": 3979
    },
    {
        "loss": 1.7572,
        "grad_norm": 2.2031607627868652,
        "learning_rate": 5.774910122268211e-05,
        "epoch": 0.5479829271650833,
        "step": 3980
    },
    {
        "loss": 1.1068,
        "grad_norm": 2.004645586013794,
        "learning_rate": 5.763111787306628e-05,
        "epoch": 0.5481206113176373,
        "step": 3981
    },
    {
        "loss": 1.9629,
        "grad_norm": 1.41458261013031,
        "learning_rate": 5.751320636108806e-05,
        "epoch": 0.5482582954701913,
        "step": 3982
    },
    {
        "loss": 2.4695,
        "grad_norm": 1.4893553256988525,
        "learning_rate": 5.7395366886669654e-05,
        "epoch": 0.5483959796227454,
        "step": 3983
    },
    {
        "loss": 2.4604,
        "grad_norm": 2.147428512573242,
        "learning_rate": 5.72775996496111e-05,
        "epoch": 0.5485336637752994,
        "step": 3984
    },
    {
        "loss": 2.2766,
        "grad_norm": 2.4035749435424805,
        "learning_rate": 5.715990484959035e-05,
        "epoch": 0.5486713479278535,
        "step": 3985
    },
    {
        "loss": 2.2268,
        "grad_norm": 1.704901099205017,
        "learning_rate": 5.704228268616208e-05,
        "epoch": 0.5488090320804075,
        "step": 3986
    },
    {
        "loss": 2.2108,
        "grad_norm": 1.8289645910263062,
        "learning_rate": 5.692473335875794e-05,
        "epoch": 0.5489467162329615,
        "step": 3987
    },
    {
        "loss": 1.9111,
        "grad_norm": 1.6178925037384033,
        "learning_rate": 5.680725706668637e-05,
        "epoch": 0.5490844003855156,
        "step": 3988
    },
    {
        "loss": 1.7432,
        "grad_norm": 2.907928466796875,
        "learning_rate": 5.6689854009131606e-05,
        "epoch": 0.5492220845380696,
        "step": 3989
    },
    {
        "loss": 2.7056,
        "grad_norm": 1.3420722484588623,
        "learning_rate": 5.657252438515377e-05,
        "epoch": 0.5493597686906238,
        "step": 3990
    },
    {
        "loss": 2.2765,
        "grad_norm": 2.188958168029785,
        "learning_rate": 5.645526839368873e-05,
        "epoch": 0.5494974528431777,
        "step": 3991
    },
    {
        "loss": 1.8217,
        "grad_norm": 2.2535500526428223,
        "learning_rate": 5.633808623354724e-05,
        "epoch": 0.5496351369957317,
        "step": 3992
    },
    {
        "loss": 2.5258,
        "grad_norm": 1.768996000289917,
        "learning_rate": 5.62209781034149e-05,
        "epoch": 0.5497728211482859,
        "step": 3993
    },
    {
        "loss": 2.2365,
        "grad_norm": 2.0045034885406494,
        "learning_rate": 5.6103944201852e-05,
        "epoch": 0.5499105053008398,
        "step": 3994
    },
    {
        "loss": 2.2267,
        "grad_norm": 3.152296781539917,
        "learning_rate": 5.598698472729276e-05,
        "epoch": 0.550048189453394,
        "step": 3995
    },
    {
        "loss": 2.0299,
        "grad_norm": 2.0120489597320557,
        "learning_rate": 5.5870099878045144e-05,
        "epoch": 0.550185873605948,
        "step": 3996
    },
    {
        "loss": 2.3486,
        "grad_norm": 1.509053349494934,
        "learning_rate": 5.575328985229098e-05,
        "epoch": 0.550323557758502,
        "step": 3997
    },
    {
        "loss": 1.9286,
        "grad_norm": 1.4953773021697998,
        "learning_rate": 5.563655484808479e-05,
        "epoch": 0.550461241911056,
        "step": 3998
    },
    {
        "loss": 2.0857,
        "grad_norm": 1.301711082458496,
        "learning_rate": 5.5519895063354036e-05,
        "epoch": 0.55059892606361,
        "step": 3999
    },
    {
        "loss": 1.7685,
        "grad_norm": 1.2627044916152954,
        "learning_rate": 5.540331069589888e-05,
        "epoch": 0.5507366102161642,
        "step": 4000
    },
    {
        "loss": 2.2796,
        "grad_norm": 1.0414620637893677,
        "learning_rate": 5.528680194339131e-05,
        "epoch": 0.5508742943687182,
        "step": 4001
    },
    {
        "loss": 1.5132,
        "grad_norm": 1.7486051321029663,
        "learning_rate": 5.517036900337511e-05,
        "epoch": 0.5510119785212721,
        "step": 4002
    },
    {
        "loss": 2.4239,
        "grad_norm": 1.919819951057434,
        "learning_rate": 5.505401207326581e-05,
        "epoch": 0.5511496626738263,
        "step": 4003
    },
    {
        "loss": 2.3552,
        "grad_norm": 1.6570369005203247,
        "learning_rate": 5.493773135034975e-05,
        "epoch": 0.5512873468263803,
        "step": 4004
    },
    {
        "loss": 0.8813,
        "grad_norm": 3.638211965560913,
        "learning_rate": 5.4821527031784136e-05,
        "epoch": 0.5514250309789344,
        "step": 4005
    },
    {
        "loss": 1.9145,
        "grad_norm": 1.5510358810424805,
        "learning_rate": 5.470539931459683e-05,
        "epoch": 0.5515627151314884,
        "step": 4006
    },
    {
        "loss": 2.2626,
        "grad_norm": 1.3842211961746216,
        "learning_rate": 5.458934839568557e-05,
        "epoch": 0.5517003992840424,
        "step": 4007
    },
    {
        "loss": 1.7261,
        "grad_norm": 2.010425090789795,
        "learning_rate": 5.4473374471817865e-05,
        "epoch": 0.5518380834365965,
        "step": 4008
    },
    {
        "loss": 2.2313,
        "grad_norm": 1.225076675415039,
        "learning_rate": 5.435747773963101e-05,
        "epoch": 0.5519757675891505,
        "step": 4009
    },
    {
        "loss": 1.3852,
        "grad_norm": 2.133741855621338,
        "learning_rate": 5.4241658395631076e-05,
        "epoch": 0.5521134517417046,
        "step": 4010
    },
    {
        "loss": 1.6351,
        "grad_norm": 2.7228963375091553,
        "learning_rate": 5.412591663619289e-05,
        "epoch": 0.5522511358942586,
        "step": 4011
    },
    {
        "loss": 1.9161,
        "grad_norm": 1.7323676347732544,
        "learning_rate": 5.401025265756014e-05,
        "epoch": 0.5523888200468126,
        "step": 4012
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.9203217029571533,
        "learning_rate": 5.3894666655844294e-05,
        "epoch": 0.5525265041993667,
        "step": 4013
    },
    {
        "loss": 1.8944,
        "grad_norm": 1.9708428382873535,
        "learning_rate": 5.377915882702456e-05,
        "epoch": 0.5526641883519207,
        "step": 4014
    },
    {
        "loss": 1.6844,
        "grad_norm": 1.9429662227630615,
        "learning_rate": 5.366372936694788e-05,
        "epoch": 0.5528018725044748,
        "step": 4015
    },
    {
        "loss": 2.6484,
        "grad_norm": 1.4319002628326416,
        "learning_rate": 5.35483784713281e-05,
        "epoch": 0.5529395566570288,
        "step": 4016
    },
    {
        "loss": 2.0893,
        "grad_norm": 1.9416555166244507,
        "learning_rate": 5.3433106335745854e-05,
        "epoch": 0.5530772408095829,
        "step": 4017
    },
    {
        "loss": 1.9306,
        "grad_norm": 4.333351135253906,
        "learning_rate": 5.3317913155648446e-05,
        "epoch": 0.5532149249621369,
        "step": 4018
    },
    {
        "loss": 1.9539,
        "grad_norm": 2.805767297744751,
        "learning_rate": 5.32027991263491e-05,
        "epoch": 0.5533526091146909,
        "step": 4019
    },
    {
        "loss": 2.1679,
        "grad_norm": 2.2364413738250732,
        "learning_rate": 5.30877644430268e-05,
        "epoch": 0.553490293267245,
        "step": 4020
    },
    {
        "loss": 2.2892,
        "grad_norm": 1.9332064390182495,
        "learning_rate": 5.297280930072632e-05,
        "epoch": 0.553627977419799,
        "step": 4021
    },
    {
        "loss": 0.9395,
        "grad_norm": 1.6540859937667847,
        "learning_rate": 5.285793389435726e-05,
        "epoch": 0.5537656615723531,
        "step": 4022
    },
    {
        "loss": 1.9932,
        "grad_norm": 1.3816437721252441,
        "learning_rate": 5.274313841869394e-05,
        "epoch": 0.5539033457249071,
        "step": 4023
    },
    {
        "loss": 2.2098,
        "grad_norm": 1.8708854913711548,
        "learning_rate": 5.262842306837565e-05,
        "epoch": 0.5540410298774611,
        "step": 4024
    },
    {
        "loss": 1.6696,
        "grad_norm": 1.9736038446426392,
        "learning_rate": 5.2513788037905374e-05,
        "epoch": 0.5541787140300152,
        "step": 4025
    },
    {
        "loss": 2.1964,
        "grad_norm": 1.1929658651351929,
        "learning_rate": 5.239923352165004e-05,
        "epoch": 0.5543163981825692,
        "step": 4026
    },
    {
        "loss": 1.9826,
        "grad_norm": 2.033726692199707,
        "learning_rate": 5.228475971383997e-05,
        "epoch": 0.5544540823351233,
        "step": 4027
    },
    {
        "loss": 1.8515,
        "grad_norm": 1.6110029220581055,
        "learning_rate": 5.21703668085689e-05,
        "epoch": 0.5545917664876773,
        "step": 4028
    },
    {
        "loss": 2.0827,
        "grad_norm": 1.9325376749038696,
        "learning_rate": 5.205605499979318e-05,
        "epoch": 0.5547294506402313,
        "step": 4029
    },
    {
        "loss": 2.0576,
        "grad_norm": 1.9903054237365723,
        "learning_rate": 5.1941824481331626e-05,
        "epoch": 0.5548671347927854,
        "step": 4030
    },
    {
        "loss": 2.307,
        "grad_norm": 2.362417697906494,
        "learning_rate": 5.182767544686546e-05,
        "epoch": 0.5550048189453394,
        "step": 4031
    },
    {
        "loss": 1.9911,
        "grad_norm": 1.703581690788269,
        "learning_rate": 5.1713608089937546e-05,
        "epoch": 0.5551425030978935,
        "step": 4032
    },
    {
        "loss": 2.346,
        "grad_norm": 1.7771426439285278,
        "learning_rate": 5.159962260395216e-05,
        "epoch": 0.5552801872504475,
        "step": 4033
    },
    {
        "loss": 1.7068,
        "grad_norm": 2.3299052715301514,
        "learning_rate": 5.148571918217516e-05,
        "epoch": 0.5554178714030015,
        "step": 4034
    },
    {
        "loss": 1.4518,
        "grad_norm": 1.6310473680496216,
        "learning_rate": 5.137189801773291e-05,
        "epoch": 0.5555555555555556,
        "step": 4035
    },
    {
        "loss": 1.7663,
        "grad_norm": 3.2790966033935547,
        "learning_rate": 5.125815930361226e-05,
        "epoch": 0.5556932397081096,
        "step": 4036
    },
    {
        "loss": 2.0254,
        "grad_norm": 1.5321317911148071,
        "learning_rate": 5.114450323266071e-05,
        "epoch": 0.5558309238606637,
        "step": 4037
    },
    {
        "loss": 2.6083,
        "grad_norm": 1.918803095817566,
        "learning_rate": 5.10309299975852e-05,
        "epoch": 0.5559686080132177,
        "step": 4038
    },
    {
        "loss": 1.9197,
        "grad_norm": 1.5876785516738892,
        "learning_rate": 5.091743979095227e-05,
        "epoch": 0.5561062921657717,
        "step": 4039
    },
    {
        "loss": 2.1993,
        "grad_norm": 2.332210063934326,
        "learning_rate": 5.080403280518795e-05,
        "epoch": 0.5562439763183258,
        "step": 4040
    },
    {
        "loss": 2.1913,
        "grad_norm": 1.472352385520935,
        "learning_rate": 5.069070923257688e-05,
        "epoch": 0.5563816604708798,
        "step": 4041
    },
    {
        "loss": 2.4139,
        "grad_norm": 1.9934256076812744,
        "learning_rate": 5.0577469265262344e-05,
        "epoch": 0.5565193446234339,
        "step": 4042
    },
    {
        "loss": 2.6504,
        "grad_norm": 1.8327877521514893,
        "learning_rate": 5.046431309524604e-05,
        "epoch": 0.5566570287759879,
        "step": 4043
    },
    {
        "loss": 2.2687,
        "grad_norm": 1.7552849054336548,
        "learning_rate": 5.035124091438739e-05,
        "epoch": 0.5567947129285419,
        "step": 4044
    },
    {
        "loss": 2.0869,
        "grad_norm": 2.6252071857452393,
        "learning_rate": 5.023825291440334e-05,
        "epoch": 0.556932397081096,
        "step": 4045
    },
    {
        "loss": 0.7841,
        "grad_norm": 4.1072211265563965,
        "learning_rate": 5.012534928686847e-05,
        "epoch": 0.55707008123365,
        "step": 4046
    },
    {
        "loss": 0.9037,
        "grad_norm": 2.359945058822632,
        "learning_rate": 5.001253022321397e-05,
        "epoch": 0.5572077653862041,
        "step": 4047
    },
    {
        "loss": 2.1097,
        "grad_norm": 3.6633291244506836,
        "learning_rate": 4.9899795914727596e-05,
        "epoch": 0.5573454495387581,
        "step": 4048
    },
    {
        "loss": 2.2113,
        "grad_norm": 2.5105624198913574,
        "learning_rate": 4.9787146552553855e-05,
        "epoch": 0.5574831336913121,
        "step": 4049
    },
    {
        "loss": 1.9748,
        "grad_norm": 2.8841397762298584,
        "learning_rate": 4.967458232769277e-05,
        "epoch": 0.5576208178438662,
        "step": 4050
    },
    {
        "loss": 1.4782,
        "grad_norm": 2.160133123397827,
        "learning_rate": 4.9562103431000104e-05,
        "epoch": 0.5577585019964202,
        "step": 4051
    },
    {
        "loss": 2.4892,
        "grad_norm": 2.1391708850860596,
        "learning_rate": 4.944971005318716e-05,
        "epoch": 0.5578961861489743,
        "step": 4052
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.1510565280914307,
        "learning_rate": 4.9337402384819995e-05,
        "epoch": 0.5580338703015283,
        "step": 4053
    },
    {
        "loss": 2.5671,
        "grad_norm": 1.5597046613693237,
        "learning_rate": 4.922518061631937e-05,
        "epoch": 0.5581715544540823,
        "step": 4054
    },
    {
        "loss": 2.1516,
        "grad_norm": 2.3253653049468994,
        "learning_rate": 4.911304493796061e-05,
        "epoch": 0.5583092386066364,
        "step": 4055
    },
    {
        "loss": 1.6076,
        "grad_norm": 2.366504192352295,
        "learning_rate": 4.900099553987286e-05,
        "epoch": 0.5584469227591904,
        "step": 4056
    },
    {
        "loss": 2.2735,
        "grad_norm": 1.3867241144180298,
        "learning_rate": 4.888903261203893e-05,
        "epoch": 0.5585846069117445,
        "step": 4057
    },
    {
        "loss": 2.2847,
        "grad_norm": 1.6281498670578003,
        "learning_rate": 4.877715634429535e-05,
        "epoch": 0.5587222910642985,
        "step": 4058
    },
    {
        "loss": 1.846,
        "grad_norm": 1.6418414115905762,
        "learning_rate": 4.866536692633139e-05,
        "epoch": 0.5588599752168525,
        "step": 4059
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.8609614372253418,
        "learning_rate": 4.8553664547689014e-05,
        "epoch": 0.5589976593694066,
        "step": 4060
    },
    {
        "loss": 2.1829,
        "grad_norm": 1.1634118556976318,
        "learning_rate": 4.8442049397763056e-05,
        "epoch": 0.5591353435219606,
        "step": 4061
    },
    {
        "loss": 1.727,
        "grad_norm": 2.843173027038574,
        "learning_rate": 4.8330521665800035e-05,
        "epoch": 0.5592730276745147,
        "step": 4062
    },
    {
        "loss": 2.2013,
        "grad_norm": 2.0283782482147217,
        "learning_rate": 4.821908154089826e-05,
        "epoch": 0.5594107118270687,
        "step": 4063
    },
    {
        "loss": 1.9945,
        "grad_norm": 1.983571171760559,
        "learning_rate": 4.810772921200779e-05,
        "epoch": 0.5595483959796227,
        "step": 4064
    },
    {
        "loss": 1.8558,
        "grad_norm": 1.8141913414001465,
        "learning_rate": 4.799646486792956e-05,
        "epoch": 0.5596860801321768,
        "step": 4065
    },
    {
        "loss": 2.1041,
        "grad_norm": 2.5263144969940186,
        "learning_rate": 4.7885288697315375e-05,
        "epoch": 0.5598237642847308,
        "step": 4066
    },
    {
        "loss": 2.0283,
        "grad_norm": 2.2662265300750732,
        "learning_rate": 4.777420088866763e-05,
        "epoch": 0.5599614484372849,
        "step": 4067
    },
    {
        "loss": 1.3706,
        "grad_norm": 2.1270089149475098,
        "learning_rate": 4.7663201630338864e-05,
        "epoch": 0.5600991325898389,
        "step": 4068
    },
    {
        "loss": 1.2334,
        "grad_norm": 2.3238208293914795,
        "learning_rate": 4.75522911105313e-05,
        "epoch": 0.5602368167423929,
        "step": 4069
    },
    {
        "loss": 2.5048,
        "grad_norm": 2.1241180896759033,
        "learning_rate": 4.744146951729711e-05,
        "epoch": 0.560374500894947,
        "step": 4070
    },
    {
        "loss": 2.106,
        "grad_norm": 2.300119161605835,
        "learning_rate": 4.733073703853731e-05,
        "epoch": 0.560512185047501,
        "step": 4071
    },
    {
        "loss": 2.0186,
        "grad_norm": 2.697035312652588,
        "learning_rate": 4.72200938620018e-05,
        "epoch": 0.5606498692000551,
        "step": 4072
    },
    {
        "loss": 1.6827,
        "grad_norm": 1.5036060810089111,
        "learning_rate": 4.710954017528952e-05,
        "epoch": 0.5607875533526091,
        "step": 4073
    },
    {
        "loss": 1.7406,
        "grad_norm": 2.3562428951263428,
        "learning_rate": 4.699907616584721e-05,
        "epoch": 0.5609252375051632,
        "step": 4074
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.2302556037902832,
        "learning_rate": 4.68887020209697e-05,
        "epoch": 0.5610629216577172,
        "step": 4075
    },
    {
        "loss": 2.1343,
        "grad_norm": 2.472360372543335,
        "learning_rate": 4.67784179277995e-05,
        "epoch": 0.5612006058102712,
        "step": 4076
    },
    {
        "loss": 2.04,
        "grad_norm": 1.8689466714859009,
        "learning_rate": 4.666822407332645e-05,
        "epoch": 0.5613382899628253,
        "step": 4077
    },
    {
        "loss": 1.9833,
        "grad_norm": 1.8089898824691772,
        "learning_rate": 4.655812064438726e-05,
        "epoch": 0.5614759741153793,
        "step": 4078
    },
    {
        "loss": 1.6077,
        "grad_norm": 2.5017309188842773,
        "learning_rate": 4.644810782766546e-05,
        "epoch": 0.5616136582679334,
        "step": 4079
    },
    {
        "loss": 2.3958,
        "grad_norm": 5.799130439758301,
        "learning_rate": 4.633818580969091e-05,
        "epoch": 0.5617513424204874,
        "step": 4080
    },
    {
        "loss": 2.3825,
        "grad_norm": 2.568213701248169,
        "learning_rate": 4.6228354776839375e-05,
        "epoch": 0.5618890265730414,
        "step": 4081
    },
    {
        "loss": 1.2998,
        "grad_norm": 1.771543025970459,
        "learning_rate": 4.6118614915332684e-05,
        "epoch": 0.5620267107255955,
        "step": 4082
    },
    {
        "loss": 2.6282,
        "grad_norm": 1.7218724489212036,
        "learning_rate": 4.6008966411237755e-05,
        "epoch": 0.5621643948781495,
        "step": 4083
    },
    {
        "loss": 2.1403,
        "grad_norm": 2.055572986602783,
        "learning_rate": 4.5899409450466624e-05,
        "epoch": 0.5623020790307036,
        "step": 4084
    },
    {
        "loss": 1.9422,
        "grad_norm": 1.8025933504104614,
        "learning_rate": 4.5789944218776414e-05,
        "epoch": 0.5624397631832576,
        "step": 4085
    },
    {
        "loss": 2.5308,
        "grad_norm": 1.161948561668396,
        "learning_rate": 4.568057090176846e-05,
        "epoch": 0.5625774473358116,
        "step": 4086
    },
    {
        "loss": 2.1124,
        "grad_norm": 4.4151177406311035,
        "learning_rate": 4.5571289684888254e-05,
        "epoch": 0.5627151314883657,
        "step": 4087
    },
    {
        "loss": 1.6113,
        "grad_norm": 1.7229115962982178,
        "learning_rate": 4.546210075342523e-05,
        "epoch": 0.5628528156409197,
        "step": 4088
    },
    {
        "loss": 2.1844,
        "grad_norm": 1.8795183897018433,
        "learning_rate": 4.535300429251233e-05,
        "epoch": 0.5629904997934738,
        "step": 4089
    },
    {
        "loss": 1.6899,
        "grad_norm": 2.503932237625122,
        "learning_rate": 4.5244000487125615e-05,
        "epoch": 0.5631281839460278,
        "step": 4090
    },
    {
        "loss": 1.8139,
        "grad_norm": 2.1453824043273926,
        "learning_rate": 4.5135089522084204e-05,
        "epoch": 0.5632658680985818,
        "step": 4091
    },
    {
        "loss": 2.488,
        "grad_norm": 1.260310173034668,
        "learning_rate": 4.5026271582049654e-05,
        "epoch": 0.5634035522511359,
        "step": 4092
    },
    {
        "loss": 1.9954,
        "grad_norm": 2.6834933757781982,
        "learning_rate": 4.4917546851525783e-05,
        "epoch": 0.5635412364036899,
        "step": 4093
    },
    {
        "loss": 2.4062,
        "grad_norm": 2.2835543155670166,
        "learning_rate": 4.480891551485866e-05,
        "epoch": 0.563678920556244,
        "step": 4094
    },
    {
        "loss": 1.9846,
        "grad_norm": 1.7783114910125732,
        "learning_rate": 4.470037775623568e-05,
        "epoch": 0.563816604708798,
        "step": 4095
    },
    {
        "loss": 2.028,
        "grad_norm": 1.9624353647232056,
        "learning_rate": 4.4591933759685554e-05,
        "epoch": 0.563954288861352,
        "step": 4096
    },
    {
        "loss": 1.3027,
        "grad_norm": 2.891596794128418,
        "learning_rate": 4.448358370907838e-05,
        "epoch": 0.5640919730139061,
        "step": 4097
    },
    {
        "loss": 1.871,
        "grad_norm": 1.4161688089370728,
        "learning_rate": 4.437532778812458e-05,
        "epoch": 0.5642296571664601,
        "step": 4098
    },
    {
        "loss": 2.2608,
        "grad_norm": 2.635105609893799,
        "learning_rate": 4.4267166180375153e-05,
        "epoch": 0.5643673413190142,
        "step": 4099
    },
    {
        "loss": 2.3374,
        "grad_norm": 1.3194072246551514,
        "learning_rate": 4.415909906922121e-05,
        "epoch": 0.5645050254715682,
        "step": 4100
    },
    {
        "loss": 2.0076,
        "grad_norm": 1.7312519550323486,
        "learning_rate": 4.4051126637893526e-05,
        "epoch": 0.5646427096241222,
        "step": 4101
    },
    {
        "loss": 2.4262,
        "grad_norm": 1.8015272617340088,
        "learning_rate": 4.394324906946243e-05,
        "epoch": 0.5647803937766763,
        "step": 4102
    },
    {
        "loss": 1.742,
        "grad_norm": 2.892975330352783,
        "learning_rate": 4.383546654683745e-05,
        "epoch": 0.5649180779292303,
        "step": 4103
    },
    {
        "loss": 1.5049,
        "grad_norm": 1.9126157760620117,
        "learning_rate": 4.3727779252766796e-05,
        "epoch": 0.5650557620817844,
        "step": 4104
    },
    {
        "loss": 2.5065,
        "grad_norm": 0.9257777333259583,
        "learning_rate": 4.3620187369837316e-05,
        "epoch": 0.5651934462343384,
        "step": 4105
    },
    {
        "loss": 1.589,
        "grad_norm": 2.877185344696045,
        "learning_rate": 4.351269108047425e-05,
        "epoch": 0.5653311303868924,
        "step": 4106
    },
    {
        "loss": 2.2461,
        "grad_norm": 2.307035207748413,
        "learning_rate": 4.3405290566940506e-05,
        "epoch": 0.5654688145394465,
        "step": 4107
    },
    {
        "loss": 2.151,
        "grad_norm": 1.4850150346755981,
        "learning_rate": 4.3297986011336675e-05,
        "epoch": 0.5656064986920005,
        "step": 4108
    },
    {
        "loss": 1.2413,
        "grad_norm": 2.791919469833374,
        "learning_rate": 4.319077759560055e-05,
        "epoch": 0.5657441828445546,
        "step": 4109
    },
    {
        "loss": 2.1029,
        "grad_norm": 1.6906999349594116,
        "learning_rate": 4.308366550150723e-05,
        "epoch": 0.5658818669971086,
        "step": 4110
    },
    {
        "loss": 2.8688,
        "grad_norm": 1.7660305500030518,
        "learning_rate": 4.2976649910668254e-05,
        "epoch": 0.5660195511496626,
        "step": 4111
    },
    {
        "loss": 1.3263,
        "grad_norm": 1.7186205387115479,
        "learning_rate": 4.286973100453151e-05,
        "epoch": 0.5661572353022167,
        "step": 4112
    },
    {
        "loss": 2.1159,
        "grad_norm": 2.1122822761535645,
        "learning_rate": 4.27629089643811e-05,
        "epoch": 0.5662949194547707,
        "step": 4113
    },
    {
        "loss": 1.2897,
        "grad_norm": 1.8588911294937134,
        "learning_rate": 4.265618397133674e-05,
        "epoch": 0.5664326036073248,
        "step": 4114
    },
    {
        "loss": 1.3088,
        "grad_norm": 2.040351390838623,
        "learning_rate": 4.25495562063537e-05,
        "epoch": 0.5665702877598788,
        "step": 4115
    },
    {
        "loss": 2.791,
        "grad_norm": 1.66463303565979,
        "learning_rate": 4.2443025850222414e-05,
        "epoch": 0.5667079719124328,
        "step": 4116
    },
    {
        "loss": 1.6255,
        "grad_norm": 1.1788291931152344,
        "learning_rate": 4.233659308356805e-05,
        "epoch": 0.5668456560649869,
        "step": 4117
    },
    {
        "loss": 2.2249,
        "grad_norm": 1.7427875995635986,
        "learning_rate": 4.2230258086850326e-05,
        "epoch": 0.5669833402175409,
        "step": 4118
    },
    {
        "loss": 2.3704,
        "grad_norm": 1.7681666612625122,
        "learning_rate": 4.2124021040363427e-05,
        "epoch": 0.567121024370095,
        "step": 4119
    },
    {
        "loss": 2.814,
        "grad_norm": 2.123538017272949,
        "learning_rate": 4.201788212423514e-05,
        "epoch": 0.567258708522649,
        "step": 4120
    },
    {
        "loss": 1.6253,
        "grad_norm": 2.3478505611419678,
        "learning_rate": 4.191184151842691e-05,
        "epoch": 0.567396392675203,
        "step": 4121
    },
    {
        "loss": 1.8894,
        "grad_norm": 1.221799612045288,
        "learning_rate": 4.1805899402733785e-05,
        "epoch": 0.5675340768277571,
        "step": 4122
    },
    {
        "loss": 1.9484,
        "grad_norm": 2.7994275093078613,
        "learning_rate": 4.170005595678348e-05,
        "epoch": 0.5676717609803111,
        "step": 4123
    },
    {
        "loss": 2.3726,
        "grad_norm": 1.8571158647537231,
        "learning_rate": 4.1594311360036585e-05,
        "epoch": 0.5678094451328652,
        "step": 4124
    },
    {
        "loss": 2.436,
        "grad_norm": 1.2077444791793823,
        "learning_rate": 4.148866579178608e-05,
        "epoch": 0.5679471292854192,
        "step": 4125
    },
    {
        "loss": 2.3396,
        "grad_norm": 2.8052713871002197,
        "learning_rate": 4.138311943115696e-05,
        "epoch": 0.5680848134379732,
        "step": 4126
    },
    {
        "loss": 2.2371,
        "grad_norm": 1.0620191097259521,
        "learning_rate": 4.127767245710609e-05,
        "epoch": 0.5682224975905273,
        "step": 4127
    },
    {
        "loss": 2.0927,
        "grad_norm": 1.4714446067810059,
        "learning_rate": 4.1172325048421825e-05,
        "epoch": 0.5683601817430813,
        "step": 4128
    },
    {
        "loss": 1.9517,
        "grad_norm": 1.3969268798828125,
        "learning_rate": 4.1067077383723654e-05,
        "epoch": 0.5684978658956354,
        "step": 4129
    },
    {
        "loss": 1.3773,
        "grad_norm": 2.744136333465576,
        "learning_rate": 4.09619296414618e-05,
        "epoch": 0.5686355500481894,
        "step": 4130
    },
    {
        "loss": 2.5428,
        "grad_norm": 1.5736314058303833,
        "learning_rate": 4.085688199991755e-05,
        "epoch": 0.5687732342007435,
        "step": 4131
    },
    {
        "loss": 2.1571,
        "grad_norm": 1.3692353963851929,
        "learning_rate": 4.075193463720198e-05,
        "epoch": 0.5689109183532975,
        "step": 4132
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.9144625663757324,
        "learning_rate": 4.0647087731256225e-05,
        "epoch": 0.5690486025058515,
        "step": 4133
    },
    {
        "loss": 2.4106,
        "grad_norm": 1.9435957670211792,
        "learning_rate": 4.054234145985139e-05,
        "epoch": 0.5691862866584056,
        "step": 4134
    },
    {
        "loss": 1.9056,
        "grad_norm": 2.7284352779388428,
        "learning_rate": 4.043769600058758e-05,
        "epoch": 0.5693239708109596,
        "step": 4135
    },
    {
        "loss": 1.9304,
        "grad_norm": 2.8924672603607178,
        "learning_rate": 4.033315153089418e-05,
        "epoch": 0.5694616549635138,
        "step": 4136
    },
    {
        "loss": 1.6438,
        "grad_norm": 1.8357232809066772,
        "learning_rate": 4.022870822802931e-05,
        "epoch": 0.5695993391160677,
        "step": 4137
    },
    {
        "loss": 1.7,
        "grad_norm": 2.510512113571167,
        "learning_rate": 4.012436626907954e-05,
        "epoch": 0.5697370232686217,
        "step": 4138
    },
    {
        "loss": 2.49,
        "grad_norm": 2.4594526290893555,
        "learning_rate": 4.002012583095951e-05,
        "epoch": 0.5698747074211759,
        "step": 4139
    },
    {
        "loss": 2.3295,
        "grad_norm": 2.3203957080841064,
        "learning_rate": 3.9915987090411985e-05,
        "epoch": 0.5700123915737298,
        "step": 4140
    },
    {
        "loss": 1.8638,
        "grad_norm": 1.9349138736724854,
        "learning_rate": 3.9811950224007086e-05,
        "epoch": 0.570150075726284,
        "step": 4141
    },
    {
        "loss": 1.9018,
        "grad_norm": 1.688780426979065,
        "learning_rate": 3.9708015408142095e-05,
        "epoch": 0.570287759878838,
        "step": 4142
    },
    {
        "loss": 1.817,
        "grad_norm": 2.3574931621551514,
        "learning_rate": 3.9604182819041734e-05,
        "epoch": 0.570425444031392,
        "step": 4143
    },
    {
        "loss": 2.1348,
        "grad_norm": 1.149618148803711,
        "learning_rate": 3.9500452632756937e-05,
        "epoch": 0.5705631281839461,
        "step": 4144
    },
    {
        "loss": 1.8386,
        "grad_norm": 2.8283021450042725,
        "learning_rate": 3.939682502516506e-05,
        "epoch": 0.5707008123365,
        "step": 4145
    },
    {
        "loss": 2.0936,
        "grad_norm": 2.212409734725952,
        "learning_rate": 3.929330017196987e-05,
        "epoch": 0.5708384964890542,
        "step": 4146
    },
    {
        "loss": 2.4847,
        "grad_norm": 1.3412531614303589,
        "learning_rate": 3.918987824870057e-05,
        "epoch": 0.5709761806416082,
        "step": 4147
    },
    {
        "loss": 1.7472,
        "grad_norm": 1.3495243787765503,
        "learning_rate": 3.908655943071188e-05,
        "epoch": 0.5711138647941622,
        "step": 4148
    },
    {
        "loss": 2.2292,
        "grad_norm": 2.3266520500183105,
        "learning_rate": 3.8983343893183925e-05,
        "epoch": 0.5712515489467163,
        "step": 4149
    },
    {
        "loss": 0.5244,
        "grad_norm": 1.552935242652893,
        "learning_rate": 3.888023181112148e-05,
        "epoch": 0.5713892330992703,
        "step": 4150
    },
    {
        "loss": 1.9199,
        "grad_norm": 2.2592198848724365,
        "learning_rate": 3.877722335935394e-05,
        "epoch": 0.5715269172518244,
        "step": 4151
    },
    {
        "loss": 2.3516,
        "grad_norm": 1.2097313404083252,
        "learning_rate": 3.8674318712535194e-05,
        "epoch": 0.5716646014043784,
        "step": 4152
    },
    {
        "loss": 1.6576,
        "grad_norm": 2.560802936553955,
        "learning_rate": 3.8571518045142896e-05,
        "epoch": 0.5718022855569324,
        "step": 4153
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.5075663328170776,
        "learning_rate": 3.8468821531478335e-05,
        "epoch": 0.5719399697094865,
        "step": 4154
    },
    {
        "loss": 1.2858,
        "grad_norm": 2.151904344558716,
        "learning_rate": 3.836622934566666e-05,
        "epoch": 0.5720776538620405,
        "step": 4155
    },
    {
        "loss": 1.7561,
        "grad_norm": 2.6724326610565186,
        "learning_rate": 3.826374166165567e-05,
        "epoch": 0.5722153380145946,
        "step": 4156
    },
    {
        "loss": 2.5056,
        "grad_norm": 1.6711286306381226,
        "learning_rate": 3.816135865321603e-05,
        "epoch": 0.5723530221671486,
        "step": 4157
    },
    {
        "loss": 1.4086,
        "grad_norm": 2.55241322517395,
        "learning_rate": 3.8059080493941214e-05,
        "epoch": 0.5724907063197026,
        "step": 4158
    },
    {
        "loss": 2.2171,
        "grad_norm": 2.0113003253936768,
        "learning_rate": 3.795690735724665e-05,
        "epoch": 0.5726283904722567,
        "step": 4159
    },
    {
        "loss": 2.378,
        "grad_norm": 1.4871190786361694,
        "learning_rate": 3.785483941636971e-05,
        "epoch": 0.5727660746248107,
        "step": 4160
    },
    {
        "loss": 1.6292,
        "grad_norm": 2.317676305770874,
        "learning_rate": 3.77528768443696e-05,
        "epoch": 0.5729037587773648,
        "step": 4161
    },
    {
        "loss": 1.7821,
        "grad_norm": 3.0324137210845947,
        "learning_rate": 3.765101981412669e-05,
        "epoch": 0.5730414429299188,
        "step": 4162
    },
    {
        "loss": 2.0728,
        "grad_norm": 5.188248634338379,
        "learning_rate": 3.754926849834237e-05,
        "epoch": 0.5731791270824728,
        "step": 4163
    },
    {
        "loss": 2.4767,
        "grad_norm": 1.6037241220474243,
        "learning_rate": 3.744762306953902e-05,
        "epoch": 0.5733168112350269,
        "step": 4164
    },
    {
        "loss": 1.9616,
        "grad_norm": 2.8246781826019287,
        "learning_rate": 3.7346083700059265e-05,
        "epoch": 0.5734544953875809,
        "step": 4165
    },
    {
        "loss": 2.0246,
        "grad_norm": 2.6267271041870117,
        "learning_rate": 3.724465056206584e-05,
        "epoch": 0.573592179540135,
        "step": 4166
    },
    {
        "loss": 1.4619,
        "grad_norm": 1.4744688272476196,
        "learning_rate": 3.7143323827541774e-05,
        "epoch": 0.573729863692689,
        "step": 4167
    },
    {
        "loss": 1.2231,
        "grad_norm": 2.5187745094299316,
        "learning_rate": 3.704210366828929e-05,
        "epoch": 0.573867547845243,
        "step": 4168
    },
    {
        "loss": 1.91,
        "grad_norm": 2.1651411056518555,
        "learning_rate": 3.6940990255929884e-05,
        "epoch": 0.5740052319977971,
        "step": 4169
    },
    {
        "loss": 2.0089,
        "grad_norm": 2.540127754211426,
        "learning_rate": 3.683998376190445e-05,
        "epoch": 0.5741429161503511,
        "step": 4170
    },
    {
        "loss": 1.9777,
        "grad_norm": 1.188281536102295,
        "learning_rate": 3.673908435747223e-05,
        "epoch": 0.5742806003029052,
        "step": 4171
    },
    {
        "loss": 1.8458,
        "grad_norm": 2.096665620803833,
        "learning_rate": 3.663829221371097e-05,
        "epoch": 0.5744182844554592,
        "step": 4172
    },
    {
        "loss": 2.0552,
        "grad_norm": 1.9117203950881958,
        "learning_rate": 3.6537607501516715e-05,
        "epoch": 0.5745559686080132,
        "step": 4173
    },
    {
        "loss": 2.2805,
        "grad_norm": 2.2508251667022705,
        "learning_rate": 3.643703039160318e-05,
        "epoch": 0.5746936527605673,
        "step": 4174
    },
    {
        "loss": 2.2467,
        "grad_norm": 1.3435169458389282,
        "learning_rate": 3.633656105450163e-05,
        "epoch": 0.5748313369131213,
        "step": 4175
    },
    {
        "loss": 0.8582,
        "grad_norm": 2.974830389022827,
        "learning_rate": 3.623619966056081e-05,
        "epoch": 0.5749690210656754,
        "step": 4176
    },
    {
        "loss": 2.1687,
        "grad_norm": 1.8232057094573975,
        "learning_rate": 3.613594637994622e-05,
        "epoch": 0.5751067052182294,
        "step": 4177
    },
    {
        "loss": 2.2237,
        "grad_norm": 1.948416829109192,
        "learning_rate": 3.603580138264005e-05,
        "epoch": 0.5752443893707834,
        "step": 4178
    },
    {
        "loss": 2.2603,
        "grad_norm": 1.3488528728485107,
        "learning_rate": 3.593576483844111e-05,
        "epoch": 0.5753820735233375,
        "step": 4179
    },
    {
        "loss": 1.7315,
        "grad_norm": 2.135603904724121,
        "learning_rate": 3.583583691696418e-05,
        "epoch": 0.5755197576758915,
        "step": 4180
    },
    {
        "loss": 1.7383,
        "grad_norm": 4.144712448120117,
        "learning_rate": 3.573601778763974e-05,
        "epoch": 0.5756574418284456,
        "step": 4181
    },
    {
        "loss": 2.1369,
        "grad_norm": 1.7934457063674927,
        "learning_rate": 3.563630761971415e-05,
        "epoch": 0.5757951259809996,
        "step": 4182
    },
    {
        "loss": 1.9577,
        "grad_norm": 2.8975932598114014,
        "learning_rate": 3.553670658224871e-05,
        "epoch": 0.5759328101335536,
        "step": 4183
    },
    {
        "loss": 2.0076,
        "grad_norm": 1.070217251777649,
        "learning_rate": 3.543721484411973e-05,
        "epoch": 0.5760704942861077,
        "step": 4184
    },
    {
        "loss": 2.0594,
        "grad_norm": 2.10131573677063,
        "learning_rate": 3.533783257401839e-05,
        "epoch": 0.5762081784386617,
        "step": 4185
    },
    {
        "loss": 2.4964,
        "grad_norm": 0.9536445736885071,
        "learning_rate": 3.52385599404501e-05,
        "epoch": 0.5763458625912158,
        "step": 4186
    },
    {
        "loss": 2.1009,
        "grad_norm": 0.9788001179695129,
        "learning_rate": 3.513939711173432e-05,
        "epoch": 0.5764835467437698,
        "step": 4187
    },
    {
        "loss": 2.5765,
        "grad_norm": 1.281313180923462,
        "learning_rate": 3.5040344256004566e-05,
        "epoch": 0.5766212308963239,
        "step": 4188
    },
    {
        "loss": 2.3792,
        "grad_norm": 1.5484464168548584,
        "learning_rate": 3.494140154120771e-05,
        "epoch": 0.5767589150488779,
        "step": 4189
    },
    {
        "loss": 1.6729,
        "grad_norm": 1.4363141059875488,
        "learning_rate": 3.48425691351038e-05,
        "epoch": 0.5768965992014319,
        "step": 4190
    },
    {
        "loss": 1.4188,
        "grad_norm": 1.9452625513076782,
        "learning_rate": 3.4743847205266156e-05,
        "epoch": 0.577034283353986,
        "step": 4191
    },
    {
        "loss": 2.3361,
        "grad_norm": 1.8279229402542114,
        "learning_rate": 3.464523591908057e-05,
        "epoch": 0.57717196750654,
        "step": 4192
    },
    {
        "loss": 2.8837,
        "grad_norm": 1.2833541631698608,
        "learning_rate": 3.45467354437452e-05,
        "epoch": 0.5773096516590941,
        "step": 4193
    },
    {
        "loss": 2.4183,
        "grad_norm": 1.1182565689086914,
        "learning_rate": 3.4448345946270335e-05,
        "epoch": 0.5774473358116481,
        "step": 4194
    },
    {
        "loss": 2.4469,
        "grad_norm": 1.5124002695083618,
        "learning_rate": 3.435006759347832e-05,
        "epoch": 0.5775850199642021,
        "step": 4195
    },
    {
        "loss": 2.517,
        "grad_norm": 1.6335275173187256,
        "learning_rate": 3.425190055200278e-05,
        "epoch": 0.5777227041167562,
        "step": 4196
    },
    {
        "loss": 2.7128,
        "grad_norm": 2.188605546951294,
        "learning_rate": 3.415384498828865e-05,
        "epoch": 0.5778603882693102,
        "step": 4197
    },
    {
        "loss": 1.433,
        "grad_norm": 2.980954647064209,
        "learning_rate": 3.4055901068592054e-05,
        "epoch": 0.5779980724218643,
        "step": 4198
    },
    {
        "loss": 1.7797,
        "grad_norm": 1.3734723329544067,
        "learning_rate": 3.3958068958979595e-05,
        "epoch": 0.5781357565744183,
        "step": 4199
    },
    {
        "loss": 2.4653,
        "grad_norm": 2.4537856578826904,
        "learning_rate": 3.3860348825328314e-05,
        "epoch": 0.5782734407269723,
        "step": 4200
    },
    {
        "loss": 2.1963,
        "grad_norm": 1.4683815240859985,
        "learning_rate": 3.37627408333256e-05,
        "epoch": 0.5784111248795264,
        "step": 4201
    },
    {
        "loss": 2.2952,
        "grad_norm": 1.9177272319793701,
        "learning_rate": 3.3665245148468505e-05,
        "epoch": 0.5785488090320804,
        "step": 4202
    },
    {
        "loss": 1.936,
        "grad_norm": 1.8791393041610718,
        "learning_rate": 3.3567861936063605e-05,
        "epoch": 0.5786864931846345,
        "step": 4203
    },
    {
        "loss": 2.6001,
        "grad_norm": 1.5399971008300781,
        "learning_rate": 3.347059136122705e-05,
        "epoch": 0.5788241773371885,
        "step": 4204
    },
    {
        "loss": 2.0942,
        "grad_norm": 1.399284839630127,
        "learning_rate": 3.337343358888384e-05,
        "epoch": 0.5789618614897425,
        "step": 4205
    },
    {
        "loss": 1.0552,
        "grad_norm": 1.200064778327942,
        "learning_rate": 3.3276388783767575e-05,
        "epoch": 0.5790995456422966,
        "step": 4206
    },
    {
        "loss": 2.0673,
        "grad_norm": 1.1498303413391113,
        "learning_rate": 3.3179457110420664e-05,
        "epoch": 0.5792372297948506,
        "step": 4207
    },
    {
        "loss": 1.5804,
        "grad_norm": 2.490145206451416,
        "learning_rate": 3.3082638733193404e-05,
        "epoch": 0.5793749139474047,
        "step": 4208
    },
    {
        "loss": 2.1929,
        "grad_norm": 1.7079039812088013,
        "learning_rate": 3.298593381624403e-05,
        "epoch": 0.5795125980999587,
        "step": 4209
    },
    {
        "loss": 2.0432,
        "grad_norm": 1.8476355075836182,
        "learning_rate": 3.288934252353859e-05,
        "epoch": 0.5796502822525127,
        "step": 4210
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.5396019220352173,
        "learning_rate": 3.279286501885029e-05,
        "epoch": 0.5797879664050668,
        "step": 4211
    },
    {
        "loss": 2.0732,
        "grad_norm": 1.753666877746582,
        "learning_rate": 3.26965014657594e-05,
        "epoch": 0.5799256505576208,
        "step": 4212
    },
    {
        "loss": 1.412,
        "grad_norm": 2.2419629096984863,
        "learning_rate": 3.260025202765317e-05,
        "epoch": 0.5800633347101749,
        "step": 4213
    },
    {
        "loss": 1.268,
        "grad_norm": 1.502615213394165,
        "learning_rate": 3.2504116867725186e-05,
        "epoch": 0.5802010188627289,
        "step": 4214
    },
    {
        "loss": 2.2478,
        "grad_norm": 1.6988822221755981,
        "learning_rate": 3.240809614897523e-05,
        "epoch": 0.5803387030152829,
        "step": 4215
    },
    {
        "loss": 2.2729,
        "grad_norm": 1.7969574928283691,
        "learning_rate": 3.231219003420932e-05,
        "epoch": 0.580476387167837,
        "step": 4216
    },
    {
        "loss": 2.5072,
        "grad_norm": 1.3200795650482178,
        "learning_rate": 3.2216398686038926e-05,
        "epoch": 0.580614071320391,
        "step": 4217
    },
    {
        "loss": 1.3355,
        "grad_norm": 2.671046733856201,
        "learning_rate": 3.212072226688089e-05,
        "epoch": 0.5807517554729451,
        "step": 4218
    },
    {
        "loss": 1.825,
        "grad_norm": 1.9321544170379639,
        "learning_rate": 3.202516093895745e-05,
        "epoch": 0.5808894396254991,
        "step": 4219
    },
    {
        "loss": 1.4191,
        "grad_norm": 1.3507579565048218,
        "learning_rate": 3.192971486429547e-05,
        "epoch": 0.5810271237780531,
        "step": 4220
    },
    {
        "loss": 2.0166,
        "grad_norm": 1.7651562690734863,
        "learning_rate": 3.183438420472638e-05,
        "epoch": 0.5811648079306072,
        "step": 4221
    },
    {
        "loss": 1.1722,
        "grad_norm": 1.493462324142456,
        "learning_rate": 3.173916912188615e-05,
        "epoch": 0.5813024920831612,
        "step": 4222
    },
    {
        "loss": 2.333,
        "grad_norm": 2.08949613571167,
        "learning_rate": 3.164406977721457e-05,
        "epoch": 0.5814401762357153,
        "step": 4223
    },
    {
        "loss": 1.6667,
        "grad_norm": 3.8781142234802246,
        "learning_rate": 3.1549086331955215e-05,
        "epoch": 0.5815778603882693,
        "step": 4224
    },
    {
        "loss": 1.9103,
        "grad_norm": 3.550279378890991,
        "learning_rate": 3.145421894715532e-05,
        "epoch": 0.5817155445408233,
        "step": 4225
    },
    {
        "loss": 1.8083,
        "grad_norm": 2.583043336868286,
        "learning_rate": 3.135946778366517e-05,
        "epoch": 0.5818532286933774,
        "step": 4226
    },
    {
        "loss": 2.3545,
        "grad_norm": 1.160117506980896,
        "learning_rate": 3.126483300213793e-05,
        "epoch": 0.5819909128459314,
        "step": 4227
    },
    {
        "loss": 2.2341,
        "grad_norm": 1.8445194959640503,
        "learning_rate": 3.117031476302975e-05,
        "epoch": 0.5821285969984855,
        "step": 4228
    },
    {
        "loss": 1.48,
        "grad_norm": 1.6485663652420044,
        "learning_rate": 3.107591322659884e-05,
        "epoch": 0.5822662811510395,
        "step": 4229
    },
    {
        "loss": 1.9093,
        "grad_norm": 2.2766287326812744,
        "learning_rate": 3.098162855290564e-05,
        "epoch": 0.5824039653035935,
        "step": 4230
    },
    {
        "loss": 2.4114,
        "grad_norm": 1.115417718887329,
        "learning_rate": 3.088746090181265e-05,
        "epoch": 0.5825416494561476,
        "step": 4231
    },
    {
        "loss": 2.1811,
        "grad_norm": 4.021580219268799,
        "learning_rate": 3.079341043298368e-05,
        "epoch": 0.5826793336087016,
        "step": 4232
    },
    {
        "loss": 2.4501,
        "grad_norm": 2.3351151943206787,
        "learning_rate": 3.06994773058839e-05,
        "epoch": 0.5828170177612557,
        "step": 4233
    },
    {
        "loss": 1.9532,
        "grad_norm": 1.4427224397659302,
        "learning_rate": 3.0605661679779716e-05,
        "epoch": 0.5829547019138097,
        "step": 4234
    },
    {
        "loss": 2.2064,
        "grad_norm": 1.5461314916610718,
        "learning_rate": 3.051196371373811e-05,
        "epoch": 0.5830923860663637,
        "step": 4235
    },
    {
        "loss": 1.1364,
        "grad_norm": 1.8228329420089722,
        "learning_rate": 3.0418383566626553e-05,
        "epoch": 0.5832300702189178,
        "step": 4236
    },
    {
        "loss": 2.4111,
        "grad_norm": 1.1687389612197876,
        "learning_rate": 3.0324921397112972e-05,
        "epoch": 0.5833677543714718,
        "step": 4237
    },
    {
        "loss": 1.348,
        "grad_norm": 2.962946891784668,
        "learning_rate": 3.023157736366504e-05,
        "epoch": 0.5835054385240259,
        "step": 4238
    },
    {
        "loss": 1.8314,
        "grad_norm": 2.4701452255249023,
        "learning_rate": 3.0138351624550133e-05,
        "epoch": 0.5836431226765799,
        "step": 4239
    },
    {
        "loss": 1.9845,
        "grad_norm": 1.2230474948883057,
        "learning_rate": 3.0045244337835298e-05,
        "epoch": 0.5837808068291339,
        "step": 4240
    },
    {
        "loss": 1.8309,
        "grad_norm": 1.7228995561599731,
        "learning_rate": 2.9952255661386485e-05,
        "epoch": 0.583918490981688,
        "step": 4241
    },
    {
        "loss": 1.6129,
        "grad_norm": 1.744454264640808,
        "learning_rate": 2.9859385752868484e-05,
        "epoch": 0.584056175134242,
        "step": 4242
    },
    {
        "loss": 1.8968,
        "grad_norm": 1.63149893283844,
        "learning_rate": 2.9766634769745128e-05,
        "epoch": 0.5841938592867961,
        "step": 4243
    },
    {
        "loss": 1.7487,
        "grad_norm": 1.3282636404037476,
        "learning_rate": 2.9674002869278184e-05,
        "epoch": 0.5843315434393501,
        "step": 4244
    },
    {
        "loss": 1.7163,
        "grad_norm": 1.735153079032898,
        "learning_rate": 2.9581490208527606e-05,
        "epoch": 0.5844692275919041,
        "step": 4245
    },
    {
        "loss": 2.2403,
        "grad_norm": 1.224888801574707,
        "learning_rate": 2.948909694435136e-05,
        "epoch": 0.5846069117444582,
        "step": 4246
    },
    {
        "loss": 2.0334,
        "grad_norm": 1.4716790914535522,
        "learning_rate": 2.9396823233404746e-05,
        "epoch": 0.5847445958970122,
        "step": 4247
    },
    {
        "loss": 2.2744,
        "grad_norm": 2.1704788208007812,
        "learning_rate": 2.9304669232140414e-05,
        "epoch": 0.5848822800495663,
        "step": 4248
    },
    {
        "loss": 1.3036,
        "grad_norm": 2.273927927017212,
        "learning_rate": 2.9212635096808183e-05,
        "epoch": 0.5850199642021203,
        "step": 4249
    },
    {
        "loss": 2.5357,
        "grad_norm": 1.1620140075683594,
        "learning_rate": 2.9120720983454497e-05,
        "epoch": 0.5851576483546744,
        "step": 4250
    },
    {
        "loss": 2.3795,
        "grad_norm": 1.829302430152893,
        "learning_rate": 2.9028927047922207e-05,
        "epoch": 0.5852953325072284,
        "step": 4251
    },
    {
        "loss": 1.3027,
        "grad_norm": 2.1443159580230713,
        "learning_rate": 2.8937253445850743e-05,
        "epoch": 0.5854330166597824,
        "step": 4252
    },
    {
        "loss": 2.3911,
        "grad_norm": 1.936509132385254,
        "learning_rate": 2.8845700332675176e-05,
        "epoch": 0.5855707008123365,
        "step": 4253
    },
    {
        "loss": 2.1131,
        "grad_norm": 4.965620994567871,
        "learning_rate": 2.8754267863626272e-05,
        "epoch": 0.5857083849648905,
        "step": 4254
    },
    {
        "loss": 2.4372,
        "grad_norm": 1.9337334632873535,
        "learning_rate": 2.8662956193730627e-05,
        "epoch": 0.5858460691174446,
        "step": 4255
    },
    {
        "loss": 1.3303,
        "grad_norm": 2.6682779788970947,
        "learning_rate": 2.8571765477809643e-05,
        "epoch": 0.5859837532699986,
        "step": 4256
    },
    {
        "loss": 2.4533,
        "grad_norm": 2.453640937805176,
        "learning_rate": 2.848069587047967e-05,
        "epoch": 0.5861214374225526,
        "step": 4257
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.7577310800552368,
        "learning_rate": 2.8389747526151943e-05,
        "epoch": 0.5862591215751067,
        "step": 4258
    },
    {
        "loss": 2.1327,
        "grad_norm": 1.7987803220748901,
        "learning_rate": 2.8298920599031887e-05,
        "epoch": 0.5863968057276607,
        "step": 4259
    },
    {
        "loss": 1.8448,
        "grad_norm": 2.3335766792297363,
        "learning_rate": 2.8208215243119062e-05,
        "epoch": 0.5865344898802148,
        "step": 4260
    },
    {
        "loss": 2.0118,
        "grad_norm": 2.081775665283203,
        "learning_rate": 2.8117631612207084e-05,
        "epoch": 0.5866721740327688,
        "step": 4261
    },
    {
        "loss": 1.9475,
        "grad_norm": 2.0631699562072754,
        "learning_rate": 2.8027169859882996e-05,
        "epoch": 0.5868098581853228,
        "step": 4262
    },
    {
        "loss": 1.7052,
        "grad_norm": 2.0168838500976562,
        "learning_rate": 2.793683013952717e-05,
        "epoch": 0.5869475423378769,
        "step": 4263
    },
    {
        "loss": 2.2847,
        "grad_norm": 1.2483024597167969,
        "learning_rate": 2.784661260431335e-05,
        "epoch": 0.5870852264904309,
        "step": 4264
    },
    {
        "loss": 0.4379,
        "grad_norm": 2.0276756286621094,
        "learning_rate": 2.7756517407207804e-05,
        "epoch": 0.587222910642985,
        "step": 4265
    },
    {
        "loss": 2.566,
        "grad_norm": 1.7008464336395264,
        "learning_rate": 2.7666544700969377e-05,
        "epoch": 0.587360594795539,
        "step": 4266
    },
    {
        "loss": 1.9915,
        "grad_norm": 1.7582453489303589,
        "learning_rate": 2.757669463814958e-05,
        "epoch": 0.587498278948093,
        "step": 4267
    },
    {
        "loss": 2.2977,
        "grad_norm": 2.537539482116699,
        "learning_rate": 2.7486967371091622e-05,
        "epoch": 0.5876359631006471,
        "step": 4268
    },
    {
        "loss": 2.1592,
        "grad_norm": 1.271198034286499,
        "learning_rate": 2.7397363051930535e-05,
        "epoch": 0.5877736472532011,
        "step": 4269
    },
    {
        "loss": 2.2132,
        "grad_norm": 2.672116279602051,
        "learning_rate": 2.7307881832593107e-05,
        "epoch": 0.5879113314057552,
        "step": 4270
    },
    {
        "loss": 1.8961,
        "grad_norm": 1.4287792444229126,
        "learning_rate": 2.721852386479723e-05,
        "epoch": 0.5880490155583092,
        "step": 4271
    },
    {
        "loss": 2.6676,
        "grad_norm": 2.2748806476593018,
        "learning_rate": 2.7129289300051787e-05,
        "epoch": 0.5881866997108632,
        "step": 4272
    },
    {
        "loss": 1.9349,
        "grad_norm": 2.494828701019287,
        "learning_rate": 2.7040178289656637e-05,
        "epoch": 0.5883243838634173,
        "step": 4273
    },
    {
        "loss": 1.2101,
        "grad_norm": 1.7172960042953491,
        "learning_rate": 2.6951190984701947e-05,
        "epoch": 0.5884620680159713,
        "step": 4274
    },
    {
        "loss": 2.1727,
        "grad_norm": 2.702073097229004,
        "learning_rate": 2.686232753606812e-05,
        "epoch": 0.5885997521685254,
        "step": 4275
    },
    {
        "loss": 1.9912,
        "grad_norm": 2.449328899383545,
        "learning_rate": 2.6773588094425838e-05,
        "epoch": 0.5887374363210794,
        "step": 4276
    },
    {
        "loss": 2.2707,
        "grad_norm": 1.951162338256836,
        "learning_rate": 2.668497281023523e-05,
        "epoch": 0.5888751204736334,
        "step": 4277
    },
    {
        "loss": 1.3665,
        "grad_norm": 2.4184601306915283,
        "learning_rate": 2.6596481833746022e-05,
        "epoch": 0.5890128046261875,
        "step": 4278
    },
    {
        "loss": 2.349,
        "grad_norm": 2.5264155864715576,
        "learning_rate": 2.650811531499705e-05,
        "epoch": 0.5891504887787415,
        "step": 4279
    },
    {
        "loss": 0.8647,
        "grad_norm": 2.30491304397583,
        "learning_rate": 2.6419873403816477e-05,
        "epoch": 0.5892881729312957,
        "step": 4280
    },
    {
        "loss": 1.8445,
        "grad_norm": 1.9007086753845215,
        "learning_rate": 2.633175624982084e-05,
        "epoch": 0.5894258570838496,
        "step": 4281
    },
    {
        "loss": 2.1369,
        "grad_norm": 1.5335825681686401,
        "learning_rate": 2.6243764002415195e-05,
        "epoch": 0.5895635412364036,
        "step": 4282
    },
    {
        "loss": 2.1209,
        "grad_norm": 1.7908004522323608,
        "learning_rate": 2.6155896810793036e-05,
        "epoch": 0.5897012253889578,
        "step": 4283
    },
    {
        "loss": 1.3168,
        "grad_norm": 2.4108810424804688,
        "learning_rate": 2.6068154823935575e-05,
        "epoch": 0.5898389095415117,
        "step": 4284
    },
    {
        "loss": 1.3317,
        "grad_norm": 2.4456443786621094,
        "learning_rate": 2.5980538190611792e-05,
        "epoch": 0.5899765936940659,
        "step": 4285
    },
    {
        "loss": 1.625,
        "grad_norm": 2.5564043521881104,
        "learning_rate": 2.5893047059378283e-05,
        "epoch": 0.5901142778466199,
        "step": 4286
    },
    {
        "loss": 1.933,
        "grad_norm": 2.22528076171875,
        "learning_rate": 2.5805681578578666e-05,
        "epoch": 0.5902519619991738,
        "step": 4287
    },
    {
        "loss": 1.7667,
        "grad_norm": 2.399445056915283,
        "learning_rate": 2.5718441896343502e-05,
        "epoch": 0.590389646151728,
        "step": 4288
    },
    {
        "loss": 1.5393,
        "grad_norm": 3.2077317237854004,
        "learning_rate": 2.5631328160590318e-05,
        "epoch": 0.590527330304282,
        "step": 4289
    },
    {
        "loss": 2.1182,
        "grad_norm": 1.807757019996643,
        "learning_rate": 2.5544340519022825e-05,
        "epoch": 0.5906650144568361,
        "step": 4290
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.4846467971801758,
        "learning_rate": 2.5457479119130912e-05,
        "epoch": 0.59080269860939,
        "step": 4291
    },
    {
        "loss": 2.3178,
        "grad_norm": 2.121814012527466,
        "learning_rate": 2.537074410819078e-05,
        "epoch": 0.590940382761944,
        "step": 4292
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.4708250761032104,
        "learning_rate": 2.528413563326395e-05,
        "epoch": 0.5910780669144982,
        "step": 4293
    },
    {
        "loss": 2.3043,
        "grad_norm": 2.7264387607574463,
        "learning_rate": 2.519765384119751e-05,
        "epoch": 0.5912157510670522,
        "step": 4294
    },
    {
        "loss": 1.829,
        "grad_norm": 2.60138201713562,
        "learning_rate": 2.511129887862389e-05,
        "epoch": 0.5913534352196063,
        "step": 4295
    },
    {
        "loss": 2.1408,
        "grad_norm": 1.1352503299713135,
        "learning_rate": 2.5025070891960313e-05,
        "epoch": 0.5914911193721603,
        "step": 4296
    },
    {
        "loss": 2.6847,
        "grad_norm": 2.1778292655944824,
        "learning_rate": 2.4938970027408727e-05,
        "epoch": 0.5916288035247143,
        "step": 4297
    },
    {
        "loss": 1.8469,
        "grad_norm": 2.436028480529785,
        "learning_rate": 2.485299643095562e-05,
        "epoch": 0.5917664876772684,
        "step": 4298
    },
    {
        "loss": 1.2227,
        "grad_norm": 2.17067551612854,
        "learning_rate": 2.4767150248371696e-05,
        "epoch": 0.5919041718298224,
        "step": 4299
    },
    {
        "loss": 2.4061,
        "grad_norm": 2.196988821029663,
        "learning_rate": 2.468143162521145e-05,
        "epoch": 0.5920418559823765,
        "step": 4300
    },
    {
        "loss": 1.3256,
        "grad_norm": 3.092132091522217,
        "learning_rate": 2.459584070681342e-05,
        "epoch": 0.5921795401349305,
        "step": 4301
    },
    {
        "loss": 2.6408,
        "grad_norm": 1.6973766088485718,
        "learning_rate": 2.451037763829933e-05,
        "epoch": 0.5923172242874845,
        "step": 4302
    },
    {
        "loss": 2.2594,
        "grad_norm": 1.95566725730896,
        "learning_rate": 2.442504256457412e-05,
        "epoch": 0.5924549084400386,
        "step": 4303
    },
    {
        "loss": 2.1705,
        "grad_norm": 1.9710441827774048,
        "learning_rate": 2.433983563032606e-05,
        "epoch": 0.5925925925925926,
        "step": 4304
    },
    {
        "loss": 2.425,
        "grad_norm": 1.733045220375061,
        "learning_rate": 2.4254756980025773e-05,
        "epoch": 0.5927302767451467,
        "step": 4305
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.1443642377853394,
        "learning_rate": 2.4169806757926504e-05,
        "epoch": 0.5928679608977007,
        "step": 4306
    },
    {
        "loss": 1.7342,
        "grad_norm": 1.9647235870361328,
        "learning_rate": 2.4084985108063796e-05,
        "epoch": 0.5930056450502548,
        "step": 4307
    },
    {
        "loss": 2.3982,
        "grad_norm": 1.6702133417129517,
        "learning_rate": 2.4000292174255168e-05,
        "epoch": 0.5931433292028088,
        "step": 4308
    },
    {
        "loss": 1.9924,
        "grad_norm": 1.1711921691894531,
        "learning_rate": 2.391572810009983e-05,
        "epoch": 0.5932810133553628,
        "step": 4309
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.7698701620101929,
        "learning_rate": 2.3831293028978584e-05,
        "epoch": 0.5934186975079169,
        "step": 4310
    },
    {
        "loss": 2.0087,
        "grad_norm": 2.1517882347106934,
        "learning_rate": 2.374698710405352e-05,
        "epoch": 0.5935563816604709,
        "step": 4311
    },
    {
        "loss": 2.2415,
        "grad_norm": 1.4037483930587769,
        "learning_rate": 2.3662810468267603e-05,
        "epoch": 0.593694065813025,
        "step": 4312
    },
    {
        "loss": 1.6775,
        "grad_norm": 2.0106523036956787,
        "learning_rate": 2.3578763264344862e-05,
        "epoch": 0.593831749965579,
        "step": 4313
    },
    {
        "loss": 2.1368,
        "grad_norm": 1.5382362604141235,
        "learning_rate": 2.349484563478962e-05,
        "epoch": 0.593969434118133,
        "step": 4314
    },
    {
        "loss": 1.4583,
        "grad_norm": 2.8742456436157227,
        "learning_rate": 2.341105772188651e-05,
        "epoch": 0.5941071182706871,
        "step": 4315
    },
    {
        "loss": 2.0852,
        "grad_norm": 1.5608675479888916,
        "learning_rate": 2.3327399667700443e-05,
        "epoch": 0.5942448024232411,
        "step": 4316
    },
    {
        "loss": 1.7135,
        "grad_norm": 1.8449167013168335,
        "learning_rate": 2.3243871614076007e-05,
        "epoch": 0.5943824865757952,
        "step": 4317
    },
    {
        "loss": 1.8592,
        "grad_norm": 1.9629020690917969,
        "learning_rate": 2.31604737026373e-05,
        "epoch": 0.5945201707283492,
        "step": 4318
    },
    {
        "loss": 2.1519,
        "grad_norm": 1.8504008054733276,
        "learning_rate": 2.3077206074787872e-05,
        "epoch": 0.5946578548809032,
        "step": 4319
    },
    {
        "loss": 2.4262,
        "grad_norm": 2.0472140312194824,
        "learning_rate": 2.299406887171037e-05,
        "epoch": 0.5947955390334573,
        "step": 4320
    },
    {
        "loss": 2.3299,
        "grad_norm": 1.8823106288909912,
        "learning_rate": 2.291106223436622e-05,
        "epoch": 0.5949332231860113,
        "step": 4321
    },
    {
        "loss": 1.834,
        "grad_norm": 2.553209066390991,
        "learning_rate": 2.282818630349558e-05,
        "epoch": 0.5950709073385654,
        "step": 4322
    },
    {
        "loss": 2.2945,
        "grad_norm": 1.2408242225646973,
        "learning_rate": 2.274544121961687e-05,
        "epoch": 0.5952085914911194,
        "step": 4323
    },
    {
        "loss": 1.4491,
        "grad_norm": 2.2027697563171387,
        "learning_rate": 2.2662827123026697e-05,
        "epoch": 0.5953462756436734,
        "step": 4324
    },
    {
        "loss": 1.3114,
        "grad_norm": 3.186338186264038,
        "learning_rate": 2.2580344153799728e-05,
        "epoch": 0.5954839597962275,
        "step": 4325
    },
    {
        "loss": 2.1319,
        "grad_norm": 2.668039083480835,
        "learning_rate": 2.2497992451788097e-05,
        "epoch": 0.5956216439487815,
        "step": 4326
    },
    {
        "loss": 2.4563,
        "grad_norm": 1.913500428199768,
        "learning_rate": 2.2415772156621362e-05,
        "epoch": 0.5957593281013356,
        "step": 4327
    },
    {
        "loss": 1.8488,
        "grad_norm": 2.603790521621704,
        "learning_rate": 2.2333683407706474e-05,
        "epoch": 0.5958970122538896,
        "step": 4328
    },
    {
        "loss": 2.1911,
        "grad_norm": 1.4590741395950317,
        "learning_rate": 2.225172634422723e-05,
        "epoch": 0.5960346964064436,
        "step": 4329
    },
    {
        "loss": 2.5262,
        "grad_norm": 0.9573328495025635,
        "learning_rate": 2.2169901105144075e-05,
        "epoch": 0.5961723805589977,
        "step": 4330
    },
    {
        "loss": 0.7001,
        "grad_norm": 2.6629221439361572,
        "learning_rate": 2.2088207829194063e-05,
        "epoch": 0.5963100647115517,
        "step": 4331
    },
    {
        "loss": 1.3814,
        "grad_norm": 2.0590696334838867,
        "learning_rate": 2.2006646654890418e-05,
        "epoch": 0.5964477488641058,
        "step": 4332
    },
    {
        "loss": 2.3578,
        "grad_norm": 1.1650465726852417,
        "learning_rate": 2.192521772052244e-05,
        "epoch": 0.5965854330166598,
        "step": 4333
    },
    {
        "loss": 2.0957,
        "grad_norm": 2.686490297317505,
        "learning_rate": 2.1843921164155212e-05,
        "epoch": 0.5967231171692138,
        "step": 4334
    },
    {
        "loss": 2.098,
        "grad_norm": 1.8616201877593994,
        "learning_rate": 2.1762757123629283e-05,
        "epoch": 0.5968608013217679,
        "step": 4335
    },
    {
        "loss": 1.6675,
        "grad_norm": 1.9422149658203125,
        "learning_rate": 2.1681725736560553e-05,
        "epoch": 0.5969984854743219,
        "step": 4336
    },
    {
        "loss": 1.6171,
        "grad_norm": 2.4725961685180664,
        "learning_rate": 2.1600827140340185e-05,
        "epoch": 0.597136169626876,
        "step": 4337
    },
    {
        "loss": 1.677,
        "grad_norm": 2.4791765213012695,
        "learning_rate": 2.1520061472133923e-05,
        "epoch": 0.59727385377943,
        "step": 4338
    },
    {
        "loss": 0.7847,
        "grad_norm": 2.9438116550445557,
        "learning_rate": 2.1439428868882193e-05,
        "epoch": 0.597411537931984,
        "step": 4339
    },
    {
        "loss": 2.2454,
        "grad_norm": 1.9053012132644653,
        "learning_rate": 2.1358929467299992e-05,
        "epoch": 0.5975492220845381,
        "step": 4340
    },
    {
        "loss": 1.2069,
        "grad_norm": 3.3086349964141846,
        "learning_rate": 2.1278563403876228e-05,
        "epoch": 0.5976869062370921,
        "step": 4341
    },
    {
        "loss": 1.9503,
        "grad_norm": 1.935067892074585,
        "learning_rate": 2.119833081487387e-05,
        "epoch": 0.5978245903896462,
        "step": 4342
    },
    {
        "loss": 2.3421,
        "grad_norm": 1.7903796434402466,
        "learning_rate": 2.111823183632956e-05,
        "epoch": 0.5979622745422002,
        "step": 4343
    },
    {
        "loss": 2.2598,
        "grad_norm": 1.429175853729248,
        "learning_rate": 2.1038266604053325e-05,
        "epoch": 0.5980999586947542,
        "step": 4344
    },
    {
        "loss": 1.3054,
        "grad_norm": 2.321349859237671,
        "learning_rate": 2.0958435253628507e-05,
        "epoch": 0.5982376428473083,
        "step": 4345
    },
    {
        "loss": 2.6545,
        "grad_norm": 1.3398923873901367,
        "learning_rate": 2.087873792041144e-05,
        "epoch": 0.5983753269998623,
        "step": 4346
    },
    {
        "loss": 0.822,
        "grad_norm": 2.1086106300354004,
        "learning_rate": 2.079917473953119e-05,
        "epoch": 0.5985130111524164,
        "step": 4347
    },
    {
        "loss": 1.8815,
        "grad_norm": 2.391835927963257,
        "learning_rate": 2.071974584588927e-05,
        "epoch": 0.5986506953049704,
        "step": 4348
    },
    {
        "loss": 2.154,
        "grad_norm": 1.5447845458984375,
        "learning_rate": 2.064045137415982e-05,
        "epoch": 0.5987883794575244,
        "step": 4349
    },
    {
        "loss": 1.8092,
        "grad_norm": 1.3175617456436157,
        "learning_rate": 2.0561291458788756e-05,
        "epoch": 0.5989260636100785,
        "step": 4350
    },
    {
        "loss": 1.3146,
        "grad_norm": 2.9281561374664307,
        "learning_rate": 2.0482266233993874e-05,
        "epoch": 0.5990637477626325,
        "step": 4351
    },
    {
        "loss": 1.4074,
        "grad_norm": 4.094700336456299,
        "learning_rate": 2.0403375833764837e-05,
        "epoch": 0.5992014319151866,
        "step": 4352
    },
    {
        "loss": 0.4921,
        "grad_norm": 2.201803207397461,
        "learning_rate": 2.032462039186247e-05,
        "epoch": 0.5993391160677406,
        "step": 4353
    },
    {
        "loss": 1.6392,
        "grad_norm": 1.8082058429718018,
        "learning_rate": 2.024600004181879e-05,
        "epoch": 0.5994768002202946,
        "step": 4354
    },
    {
        "loss": 2.6223,
        "grad_norm": 1.0815060138702393,
        "learning_rate": 2.0167514916936948e-05,
        "epoch": 0.5996144843728487,
        "step": 4355
    },
    {
        "loss": 1.7189,
        "grad_norm": 1.6841329336166382,
        "learning_rate": 2.0089165150290623e-05,
        "epoch": 0.5997521685254027,
        "step": 4356
    },
    {
        "loss": 1.8305,
        "grad_norm": 1.3361175060272217,
        "learning_rate": 2.0010950874723998e-05,
        "epoch": 0.5998898526779568,
        "step": 4357
    },
    {
        "loss": 1.5658,
        "grad_norm": 2.6912336349487305,
        "learning_rate": 1.9932872222851716e-05,
        "epoch": 0.6000275368305108,
        "step": 4358
    },
    {
        "loss": 2.4228,
        "grad_norm": 2.277193784713745,
        "learning_rate": 1.985492932705828e-05,
        "epoch": 0.6001652209830648,
        "step": 4359
    },
    {
        "loss": 2.1427,
        "grad_norm": 2.3466718196868896,
        "learning_rate": 1.9777122319497966e-05,
        "epoch": 0.6003029051356189,
        "step": 4360
    },
    {
        "loss": 1.983,
        "grad_norm": 1.1818865537643433,
        "learning_rate": 1.9699451332094963e-05,
        "epoch": 0.6004405892881729,
        "step": 4361
    },
    {
        "loss": 2.2494,
        "grad_norm": 1.5912988185882568,
        "learning_rate": 1.9621916496542514e-05,
        "epoch": 0.600578273440727,
        "step": 4362
    },
    {
        "loss": 1.3841,
        "grad_norm": 1.9186850786209106,
        "learning_rate": 1.9544517944303143e-05,
        "epoch": 0.600715957593281,
        "step": 4363
    },
    {
        "loss": 2.1466,
        "grad_norm": 2.5416033267974854,
        "learning_rate": 1.9467255806608198e-05,
        "epoch": 0.6008536417458351,
        "step": 4364
    },
    {
        "loss": 1.9536,
        "grad_norm": 1.2746427059173584,
        "learning_rate": 1.939013021445796e-05,
        "epoch": 0.6009913258983891,
        "step": 4365
    },
    {
        "loss": 2.0965,
        "grad_norm": 2.497326612472534,
        "learning_rate": 1.9313141298620963e-05,
        "epoch": 0.6011290100509431,
        "step": 4366
    },
    {
        "loss": 2.0625,
        "grad_norm": 2.014042615890503,
        "learning_rate": 1.9236289189634116e-05,
        "epoch": 0.6012666942034972,
        "step": 4367
    },
    {
        "loss": 2.0733,
        "grad_norm": 2.472242832183838,
        "learning_rate": 1.9159574017802363e-05,
        "epoch": 0.6014043783560512,
        "step": 4368
    },
    {
        "loss": 2.2872,
        "grad_norm": 1.8134729862213135,
        "learning_rate": 1.908299591319842e-05,
        "epoch": 0.6015420625086053,
        "step": 4369
    },
    {
        "loss": 2.3189,
        "grad_norm": 1.7437959909439087,
        "learning_rate": 1.9006555005662587e-05,
        "epoch": 0.6016797466611593,
        "step": 4370
    },
    {
        "loss": 1.8594,
        "grad_norm": 1.585392951965332,
        "learning_rate": 1.893025142480267e-05,
        "epoch": 0.6018174308137133,
        "step": 4371
    },
    {
        "loss": 2.335,
        "grad_norm": 1.4610698223114014,
        "learning_rate": 1.885408529999351e-05,
        "epoch": 0.6019551149662674,
        "step": 4372
    },
    {
        "loss": 2.2151,
        "grad_norm": 1.630654215812683,
        "learning_rate": 1.8778056760376817e-05,
        "epoch": 0.6020927991188214,
        "step": 4373
    },
    {
        "loss": 2.5286,
        "grad_norm": 2.8862385749816895,
        "learning_rate": 1.870216593486134e-05,
        "epoch": 0.6022304832713755,
        "step": 4374
    },
    {
        "loss": 1.2709,
        "grad_norm": 3.3749120235443115,
        "learning_rate": 1.862641295212201e-05,
        "epoch": 0.6023681674239295,
        "step": 4375
    },
    {
        "loss": 1.6853,
        "grad_norm": 1.4256823062896729,
        "learning_rate": 1.8550797940600063e-05,
        "epoch": 0.6025058515764835,
        "step": 4376
    },
    {
        "loss": 2.3393,
        "grad_norm": 2.3388636112213135,
        "learning_rate": 1.847532102850307e-05,
        "epoch": 0.6026435357290376,
        "step": 4377
    },
    {
        "loss": 1.9601,
        "grad_norm": 1.5309292078018188,
        "learning_rate": 1.839998234380418e-05,
        "epoch": 0.6027812198815916,
        "step": 4378
    },
    {
        "loss": 1.396,
        "grad_norm": 4.659789562225342,
        "learning_rate": 1.8324782014242204e-05,
        "epoch": 0.6029189040341457,
        "step": 4379
    },
    {
        "loss": 2.2294,
        "grad_norm": 1.7211343050003052,
        "learning_rate": 1.8249720167321537e-05,
        "epoch": 0.6030565881866997,
        "step": 4380
    },
    {
        "loss": 2.4918,
        "grad_norm": 1.7012313604354858,
        "learning_rate": 1.817479693031161e-05,
        "epoch": 0.6031942723392537,
        "step": 4381
    },
    {
        "loss": 2.3033,
        "grad_norm": 1.0794428586959839,
        "learning_rate": 1.8100012430246837e-05,
        "epoch": 0.6033319564918078,
        "step": 4382
    },
    {
        "loss": 1.8551,
        "grad_norm": 1.9738092422485352,
        "learning_rate": 1.8025366793926555e-05,
        "epoch": 0.6034696406443618,
        "step": 4383
    },
    {
        "loss": 2.4516,
        "grad_norm": 1.7276158332824707,
        "learning_rate": 1.7950860147914506e-05,
        "epoch": 0.6036073247969159,
        "step": 4384
    },
    {
        "loss": 2.0919,
        "grad_norm": 2.084237575531006,
        "learning_rate": 1.787649261853871e-05,
        "epoch": 0.6037450089494699,
        "step": 4385
    },
    {
        "loss": 1.3446,
        "grad_norm": 2.3433094024658203,
        "learning_rate": 1.7802264331891604e-05,
        "epoch": 0.6038826931020239,
        "step": 4386
    },
    {
        "loss": 1.1434,
        "grad_norm": 2.5084915161132812,
        "learning_rate": 1.7728175413829262e-05,
        "epoch": 0.604020377254578,
        "step": 4387
    },
    {
        "loss": 1.8039,
        "grad_norm": 1.9782649278640747,
        "learning_rate": 1.765422598997146e-05,
        "epoch": 0.604158061407132,
        "step": 4388
    },
    {
        "loss": 1.9596,
        "grad_norm": 2.6855597496032715,
        "learning_rate": 1.7580416185701697e-05,
        "epoch": 0.6042957455596861,
        "step": 4389
    },
    {
        "loss": 2.1837,
        "grad_norm": 1.588786244392395,
        "learning_rate": 1.7506746126166485e-05,
        "epoch": 0.6044334297122401,
        "step": 4390
    },
    {
        "loss": 2.0983,
        "grad_norm": 1.2356609106063843,
        "learning_rate": 1.743321593627546e-05,
        "epoch": 0.6045711138647941,
        "step": 4391
    },
    {
        "loss": 2.5802,
        "grad_norm": 1.693127155303955,
        "learning_rate": 1.735982574070123e-05,
        "epoch": 0.6047087980173482,
        "step": 4392
    },
    {
        "loss": 1.7728,
        "grad_norm": 1.2408888339996338,
        "learning_rate": 1.72865756638789e-05,
        "epoch": 0.6048464821699022,
        "step": 4393
    },
    {
        "loss": 2.3307,
        "grad_norm": 0.9482520818710327,
        "learning_rate": 1.7213465830006004e-05,
        "epoch": 0.6049841663224563,
        "step": 4394
    },
    {
        "loss": 2.1353,
        "grad_norm": 1.7442185878753662,
        "learning_rate": 1.7140496363042413e-05,
        "epoch": 0.6051218504750103,
        "step": 4395
    },
    {
        "loss": 2.4678,
        "grad_norm": 2.5357189178466797,
        "learning_rate": 1.7067667386709896e-05,
        "epoch": 0.6052595346275643,
        "step": 4396
    },
    {
        "loss": 1.8962,
        "grad_norm": 2.314730644226074,
        "learning_rate": 1.699497902449192e-05,
        "epoch": 0.6053972187801184,
        "step": 4397
    },
    {
        "loss": 2.3003,
        "grad_norm": 1.1949865818023682,
        "learning_rate": 1.6922431399633865e-05,
        "epoch": 0.6055349029326724,
        "step": 4398
    },
    {
        "loss": 1.7294,
        "grad_norm": 1.4037007093429565,
        "learning_rate": 1.6850024635142182e-05,
        "epoch": 0.6056725870852265,
        "step": 4399
    },
    {
        "loss": 2.1742,
        "grad_norm": 2.2973670959472656,
        "learning_rate": 1.677775885378451e-05,
        "epoch": 0.6058102712377805,
        "step": 4400
    },
    {
        "loss": 1.6435,
        "grad_norm": 2.250824213027954,
        "learning_rate": 1.670563417808968e-05,
        "epoch": 0.6059479553903345,
        "step": 4401
    },
    {
        "loss": 2.1728,
        "grad_norm": 1.7329912185668945,
        "learning_rate": 1.663365073034704e-05,
        "epoch": 0.6060856395428886,
        "step": 4402
    },
    {
        "loss": 2.1693,
        "grad_norm": 3.0594000816345215,
        "learning_rate": 1.6561808632606512e-05,
        "epoch": 0.6062233236954426,
        "step": 4403
    },
    {
        "loss": 2.0873,
        "grad_norm": 1.5109750032424927,
        "learning_rate": 1.6490108006678494e-05,
        "epoch": 0.6063610078479967,
        "step": 4404
    },
    {
        "loss": 1.7365,
        "grad_norm": 2.3678038120269775,
        "learning_rate": 1.6418548974133385e-05,
        "epoch": 0.6064986920005507,
        "step": 4405
    },
    {
        "loss": 2.0521,
        "grad_norm": 1.8336633443832397,
        "learning_rate": 1.6347131656301507e-05,
        "epoch": 0.6066363761531047,
        "step": 4406
    },
    {
        "loss": 1.9253,
        "grad_norm": 1.8502609729766846,
        "learning_rate": 1.6275856174273018e-05,
        "epoch": 0.6067740603056588,
        "step": 4407
    },
    {
        "loss": 2.1829,
        "grad_norm": 1.6307190656661987,
        "learning_rate": 1.6204722648897475e-05,
        "epoch": 0.6069117444582128,
        "step": 4408
    },
    {
        "loss": 1.4617,
        "grad_norm": 2.0431594848632812,
        "learning_rate": 1.6133731200783708e-05,
        "epoch": 0.6070494286107669,
        "step": 4409
    },
    {
        "loss": 2.3932,
        "grad_norm": 1.8580843210220337,
        "learning_rate": 1.6062881950299856e-05,
        "epoch": 0.6071871127633209,
        "step": 4410
    },
    {
        "loss": 2.3361,
        "grad_norm": 3.977872371673584,
        "learning_rate": 1.5992175017572787e-05,
        "epoch": 0.6073247969158749,
        "step": 4411
    },
    {
        "loss": 1.824,
        "grad_norm": 2.321730613708496,
        "learning_rate": 1.5921610522488005e-05,
        "epoch": 0.607462481068429,
        "step": 4412
    },
    {
        "loss": 1.5077,
        "grad_norm": 2.4653780460357666,
        "learning_rate": 1.585118858468977e-05,
        "epoch": 0.607600165220983,
        "step": 4413
    },
    {
        "loss": 1.5968,
        "grad_norm": 1.1296604871749878,
        "learning_rate": 1.5780909323580384e-05,
        "epoch": 0.6077378493735371,
        "step": 4414
    },
    {
        "loss": 2.2382,
        "grad_norm": 1.2740463018417358,
        "learning_rate": 1.5710772858320277e-05,
        "epoch": 0.6078755335260911,
        "step": 4415
    },
    {
        "loss": 1.8149,
        "grad_norm": 1.3706921339035034,
        "learning_rate": 1.56407793078279e-05,
        "epoch": 0.6080132176786451,
        "step": 4416
    },
    {
        "loss": 2.6104,
        "grad_norm": 1.7764047384262085,
        "learning_rate": 1.557092879077925e-05,
        "epoch": 0.6081509018311992,
        "step": 4417
    },
    {
        "loss": 2.1095,
        "grad_norm": 1.2983498573303223,
        "learning_rate": 1.5501221425607802e-05,
        "epoch": 0.6082885859837532,
        "step": 4418
    },
    {
        "loss": 0.8169,
        "grad_norm": 3.154998779296875,
        "learning_rate": 1.543165733050448e-05,
        "epoch": 0.6084262701363073,
        "step": 4419
    },
    {
        "loss": 2.3489,
        "grad_norm": 2.216610908508301,
        "learning_rate": 1.5362236623417105e-05,
        "epoch": 0.6085639542888613,
        "step": 4420
    },
    {
        "loss": 1.4097,
        "grad_norm": 2.237940549850464,
        "learning_rate": 1.529295942205038e-05,
        "epoch": 0.6087016384414153,
        "step": 4421
    },
    {
        "loss": 2.3709,
        "grad_norm": 2.021775007247925,
        "learning_rate": 1.5223825843865879e-05,
        "epoch": 0.6088393225939694,
        "step": 4422
    },
    {
        "loss": 2.0084,
        "grad_norm": 1.2870410680770874,
        "learning_rate": 1.5154836006081518e-05,
        "epoch": 0.6089770067465234,
        "step": 4423
    },
    {
        "loss": 1.8547,
        "grad_norm": 2.172051429748535,
        "learning_rate": 1.5085990025671426e-05,
        "epoch": 0.6091146908990775,
        "step": 4424
    },
    {
        "loss": 2.1808,
        "grad_norm": 2.330936908721924,
        "learning_rate": 1.5017288019366038e-05,
        "epoch": 0.6092523750516315,
        "step": 4425
    },
    {
        "loss": 1.5644,
        "grad_norm": 2.535541296005249,
        "learning_rate": 1.4948730103651498e-05,
        "epoch": 0.6093900592041857,
        "step": 4426
    },
    {
        "loss": 2.2819,
        "grad_norm": 1.400164246559143,
        "learning_rate": 1.4880316394769623e-05,
        "epoch": 0.6095277433567396,
        "step": 4427
    },
    {
        "loss": 2.0723,
        "grad_norm": 1.8207426071166992,
        "learning_rate": 1.4812047008717888e-05,
        "epoch": 0.6096654275092936,
        "step": 4428
    },
    {
        "loss": 1.8536,
        "grad_norm": 1.9786089658737183,
        "learning_rate": 1.4743922061248927e-05,
        "epoch": 0.6098031116618478,
        "step": 4429
    },
    {
        "loss": 1.2421,
        "grad_norm": 2.6148576736450195,
        "learning_rate": 1.4675941667870475e-05,
        "epoch": 0.6099407958144017,
        "step": 4430
    },
    {
        "loss": 1.0048,
        "grad_norm": 3.3254756927490234,
        "learning_rate": 1.4608105943845285e-05,
        "epoch": 0.6100784799669559,
        "step": 4431
    },
    {
        "loss": 2.1909,
        "grad_norm": 2.399167537689209,
        "learning_rate": 1.454041500419071e-05,
        "epoch": 0.6102161641195099,
        "step": 4432
    },
    {
        "loss": 1.701,
        "grad_norm": 2.8370978832244873,
        "learning_rate": 1.4472868963678587e-05,
        "epoch": 0.6103538482720638,
        "step": 4433
    },
    {
        "loss": 0.7769,
        "grad_norm": 1.210817813873291,
        "learning_rate": 1.4405467936835271e-05,
        "epoch": 0.610491532424618,
        "step": 4434
    },
    {
        "loss": 2.3281,
        "grad_norm": 1.9006773233413696,
        "learning_rate": 1.4338212037941002e-05,
        "epoch": 0.610629216577172,
        "step": 4435
    },
    {
        "loss": 2.1783,
        "grad_norm": 1.8806891441345215,
        "learning_rate": 1.427110138103006e-05,
        "epoch": 0.6107669007297261,
        "step": 4436
    },
    {
        "loss": 1.7426,
        "grad_norm": 1.777857780456543,
        "learning_rate": 1.4204136079890562e-05,
        "epoch": 0.61090458488228,
        "step": 4437
    },
    {
        "loss": 2.0004,
        "grad_norm": 2.6800856590270996,
        "learning_rate": 1.4137316248064003e-05,
        "epoch": 0.611042269034834,
        "step": 4438
    },
    {
        "loss": 1.846,
        "grad_norm": 2.4205591678619385,
        "learning_rate": 1.4070641998845247e-05,
        "epoch": 0.6111799531873882,
        "step": 4439
    },
    {
        "loss": 2.0356,
        "grad_norm": 1.4788166284561157,
        "learning_rate": 1.4004113445282463e-05,
        "epoch": 0.6113176373399422,
        "step": 4440
    },
    {
        "loss": 1.0145,
        "grad_norm": 1.6112380027770996,
        "learning_rate": 1.3937730700176654e-05,
        "epoch": 0.6114553214924963,
        "step": 4441
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.7362678050994873,
        "learning_rate": 1.3871493876081598e-05,
        "epoch": 0.6115930056450503,
        "step": 4442
    },
    {
        "loss": 1.4759,
        "grad_norm": 1.8301637172698975,
        "learning_rate": 1.380540308530377e-05,
        "epoch": 0.6117306897976043,
        "step": 4443
    },
    {
        "loss": 1.9759,
        "grad_norm": 1.3053631782531738,
        "learning_rate": 1.3739458439901965e-05,
        "epoch": 0.6118683739501584,
        "step": 4444
    },
    {
        "loss": 1.7837,
        "grad_norm": 2.6148276329040527,
        "learning_rate": 1.3673660051687165e-05,
        "epoch": 0.6120060581027124,
        "step": 4445
    },
    {
        "loss": 2.3756,
        "grad_norm": 2.289869785308838,
        "learning_rate": 1.360800803222234e-05,
        "epoch": 0.6121437422552665,
        "step": 4446
    },
    {
        "loss": 1.8385,
        "grad_norm": 2.2850935459136963,
        "learning_rate": 1.3542502492822463e-05,
        "epoch": 0.6122814264078205,
        "step": 4447
    },
    {
        "loss": 1.8703,
        "grad_norm": 3.0473976135253906,
        "learning_rate": 1.3477143544553995e-05,
        "epoch": 0.6124191105603745,
        "step": 4448
    },
    {
        "loss": 2.2937,
        "grad_norm": 1.9407891035079956,
        "learning_rate": 1.3411931298234793e-05,
        "epoch": 0.6125567947129286,
        "step": 4449
    },
    {
        "loss": 1.6406,
        "grad_norm": 2.713789939880371,
        "learning_rate": 1.3346865864434211e-05,
        "epoch": 0.6126944788654826,
        "step": 4450
    },
    {
        "loss": 2.1003,
        "grad_norm": 1.5007027387619019,
        "learning_rate": 1.3281947353472445e-05,
        "epoch": 0.6128321630180367,
        "step": 4451
    },
    {
        "loss": 2.1348,
        "grad_norm": 1.6537877321243286,
        "learning_rate": 1.321717587542064e-05,
        "epoch": 0.6129698471705907,
        "step": 4452
    },
    {
        "loss": 2.1018,
        "grad_norm": 1.0919079780578613,
        "learning_rate": 1.3152551540100744e-05,
        "epoch": 0.6131075313231447,
        "step": 4453
    },
    {
        "loss": 1.8872,
        "grad_norm": 2.0017051696777344,
        "learning_rate": 1.3088074457085109e-05,
        "epoch": 0.6132452154756988,
        "step": 4454
    },
    {
        "loss": 2.1583,
        "grad_norm": 1.267978549003601,
        "learning_rate": 1.3023744735696408e-05,
        "epoch": 0.6133828996282528,
        "step": 4455
    },
    {
        "loss": 2.1143,
        "grad_norm": 1.4031237363815308,
        "learning_rate": 1.295956248500758e-05,
        "epoch": 0.6135205837808069,
        "step": 4456
    },
    {
        "loss": 1.8938,
        "grad_norm": 1.3003263473510742,
        "learning_rate": 1.2895527813841401e-05,
        "epoch": 0.6136582679333609,
        "step": 4457
    },
    {
        "loss": 1.0502,
        "grad_norm": 1.1774386167526245,
        "learning_rate": 1.283164083077042e-05,
        "epoch": 0.6137959520859149,
        "step": 4458
    },
    {
        "loss": 1.129,
        "grad_norm": 1.9611347913742065,
        "learning_rate": 1.2767901644116941e-05,
        "epoch": 0.613933636238469,
        "step": 4459
    },
    {
        "loss": 2.308,
        "grad_norm": 2.3605854511260986,
        "learning_rate": 1.2704310361952464e-05,
        "epoch": 0.614071320391023,
        "step": 4460
    },
    {
        "loss": 1.5676,
        "grad_norm": 2.0855884552001953,
        "learning_rate": 1.2640867092097797e-05,
        "epoch": 0.6142090045435771,
        "step": 4461
    },
    {
        "loss": 1.7374,
        "grad_norm": 1.6221060752868652,
        "learning_rate": 1.257757194212289e-05,
        "epoch": 0.6143466886961311,
        "step": 4462
    },
    {
        "loss": 2.3822,
        "grad_norm": 1.4996792078018188,
        "learning_rate": 1.251442501934642e-05,
        "epoch": 0.6144843728486851,
        "step": 4463
    },
    {
        "loss": 2.3758,
        "grad_norm": 2.410163164138794,
        "learning_rate": 1.2451426430835711e-05,
        "epoch": 0.6146220570012392,
        "step": 4464
    },
    {
        "loss": 2.2893,
        "grad_norm": 2.748589277267456,
        "learning_rate": 1.238857628340675e-05,
        "epoch": 0.6147597411537932,
        "step": 4465
    },
    {
        "loss": 1.9273,
        "grad_norm": 2.2918972969055176,
        "learning_rate": 1.2325874683623696e-05,
        "epoch": 0.6148974253063473,
        "step": 4466
    },
    {
        "loss": 1.943,
        "grad_norm": 1.1449373960494995,
        "learning_rate": 1.2263321737798827e-05,
        "epoch": 0.6150351094589013,
        "step": 4467
    },
    {
        "loss": 2.0053,
        "grad_norm": 1.094903826713562,
        "learning_rate": 1.220091755199254e-05,
        "epoch": 0.6151727936114553,
        "step": 4468
    },
    {
        "loss": 1.8734,
        "grad_norm": 2.436927318572998,
        "learning_rate": 1.2138662232012843e-05,
        "epoch": 0.6153104777640094,
        "step": 4469
    },
    {
        "loss": 1.8236,
        "grad_norm": 1.7091397047042847,
        "learning_rate": 1.2076555883415319e-05,
        "epoch": 0.6154481619165634,
        "step": 4470
    },
    {
        "loss": 1.7352,
        "grad_norm": 2.3450920581817627,
        "learning_rate": 1.201459861150317e-05,
        "epoch": 0.6155858460691175,
        "step": 4471
    },
    {
        "loss": 2.6384,
        "grad_norm": 1.2078441381454468,
        "learning_rate": 1.195279052132665e-05,
        "epoch": 0.6157235302216715,
        "step": 4472
    },
    {
        "loss": 2.1346,
        "grad_norm": 2.5641512870788574,
        "learning_rate": 1.1891131717683024e-05,
        "epoch": 0.6158612143742255,
        "step": 4473
    },
    {
        "loss": 1.8359,
        "grad_norm": 1.7760779857635498,
        "learning_rate": 1.1829622305116717e-05,
        "epoch": 0.6159988985267796,
        "step": 4474
    },
    {
        "loss": 1.447,
        "grad_norm": 2.3273863792419434,
        "learning_rate": 1.1768262387918582e-05,
        "epoch": 0.6161365826793336,
        "step": 4475
    },
    {
        "loss": 2.3396,
        "grad_norm": 1.6062371730804443,
        "learning_rate": 1.1707052070126056e-05,
        "epoch": 0.6162742668318877,
        "step": 4476
    },
    {
        "loss": 1.6028,
        "grad_norm": 2.280012369155884,
        "learning_rate": 1.1645991455523054e-05,
        "epoch": 0.6164119509844417,
        "step": 4477
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.2148125171661377,
        "learning_rate": 1.1585080647639523e-05,
        "epoch": 0.6165496351369957,
        "step": 4478
    },
    {
        "loss": 1.7075,
        "grad_norm": 2.0116734504699707,
        "learning_rate": 1.1524319749751423e-05,
        "epoch": 0.6166873192895498,
        "step": 4479
    },
    {
        "loss": 1.9679,
        "grad_norm": 2.5508930683135986,
        "learning_rate": 1.1463708864880652e-05,
        "epoch": 0.6168250034421038,
        "step": 4480
    },
    {
        "loss": 2.0189,
        "grad_norm": 1.706786870956421,
        "learning_rate": 1.140324809579465e-05,
        "epoch": 0.6169626875946579,
        "step": 4481
    },
    {
        "loss": 1.9988,
        "grad_norm": 2.035466432571411,
        "learning_rate": 1.1342937545006283e-05,
        "epoch": 0.6171003717472119,
        "step": 4482
    },
    {
        "loss": 1.4907,
        "grad_norm": 2.7025561332702637,
        "learning_rate": 1.1282777314773928e-05,
        "epoch": 0.617238055899766,
        "step": 4483
    },
    {
        "loss": 1.9872,
        "grad_norm": 2.1375186443328857,
        "learning_rate": 1.1222767507100896e-05,
        "epoch": 0.61737574005232,
        "step": 4484
    },
    {
        "loss": 1.9191,
        "grad_norm": 1.4723390340805054,
        "learning_rate": 1.1162908223735413e-05,
        "epoch": 0.617513424204874,
        "step": 4485
    },
    {
        "loss": 1.4763,
        "grad_norm": 2.119140625,
        "learning_rate": 1.1103199566170774e-05,
        "epoch": 0.6176511083574281,
        "step": 4486
    },
    {
        "loss": 1.947,
        "grad_norm": 1.819075345993042,
        "learning_rate": 1.1043641635644609e-05,
        "epoch": 0.6177887925099821,
        "step": 4487
    },
    {
        "loss": 2.2816,
        "grad_norm": 1.651716947555542,
        "learning_rate": 1.0984234533139026e-05,
        "epoch": 0.6179264766625362,
        "step": 4488
    },
    {
        "loss": 1.6813,
        "grad_norm": 3.4392619132995605,
        "learning_rate": 1.0924978359380522e-05,
        "epoch": 0.6180641608150902,
        "step": 4489
    },
    {
        "loss": 2.1681,
        "grad_norm": 1.1812046766281128,
        "learning_rate": 1.0865873214839583e-05,
        "epoch": 0.6182018449676442,
        "step": 4490
    },
    {
        "loss": 2.1879,
        "grad_norm": 1.437644600868225,
        "learning_rate": 1.0806919199730615e-05,
        "epoch": 0.6183395291201983,
        "step": 4491
    },
    {
        "loss": 2.4289,
        "grad_norm": 1.1334173679351807,
        "learning_rate": 1.0748116414011888e-05,
        "epoch": 0.6184772132727523,
        "step": 4492
    },
    {
        "loss": 2.3832,
        "grad_norm": 2.0685126781463623,
        "learning_rate": 1.0689464957385165e-05,
        "epoch": 0.6186148974253064,
        "step": 4493
    },
    {
        "loss": 1.7188,
        "grad_norm": 1.9242451190948486,
        "learning_rate": 1.0630964929295573e-05,
        "epoch": 0.6187525815778604,
        "step": 4494
    },
    {
        "loss": 2.0991,
        "grad_norm": 2.069094181060791,
        "learning_rate": 1.057261642893168e-05,
        "epoch": 0.6188902657304144,
        "step": 4495
    },
    {
        "loss": 2.0599,
        "grad_norm": 2.4307713508605957,
        "learning_rate": 1.0514419555224975e-05,
        "epoch": 0.6190279498829685,
        "step": 4496
    },
    {
        "loss": 2.3522,
        "grad_norm": 1.5169520378112793,
        "learning_rate": 1.0456374406849812e-05,
        "epoch": 0.6191656340355225,
        "step": 4497
    },
    {
        "loss": 2.0419,
        "grad_norm": 1.4740718603134155,
        "learning_rate": 1.0398481082223533e-05,
        "epoch": 0.6193033181880766,
        "step": 4498
    },
    {
        "loss": 2.6008,
        "grad_norm": 1.954946756362915,
        "learning_rate": 1.034073967950585e-05,
        "epoch": 0.6194410023406306,
        "step": 4499
    },
    {
        "loss": 2.546,
        "grad_norm": 1.9818326234817505,
        "learning_rate": 1.0283150296598886e-05,
        "epoch": 0.6195786864931846,
        "step": 4500
    },
    {
        "loss": 2.0362,
        "grad_norm": 2.1120221614837646,
        "learning_rate": 1.0225713031147155e-05,
        "epoch": 0.6197163706457387,
        "step": 4501
    },
    {
        "loss": 1.903,
        "grad_norm": 1.9358129501342773,
        "learning_rate": 1.0168427980537122e-05,
        "epoch": 0.6198540547982927,
        "step": 4502
    },
    {
        "loss": 1.8021,
        "grad_norm": 2.406654119491577,
        "learning_rate": 1.0111295241897157e-05,
        "epoch": 0.6199917389508468,
        "step": 4503
    },
    {
        "loss": 1.6949,
        "grad_norm": 3.6087794303894043,
        "learning_rate": 1.005431491209754e-05,
        "epoch": 0.6201294231034008,
        "step": 4504
    },
    {
        "loss": 2.1139,
        "grad_norm": 1.495240330696106,
        "learning_rate": 9.997487087749969e-06,
        "epoch": 0.6202671072559548,
        "step": 4505
    },
    {
        "loss": 1.7592,
        "grad_norm": 2.999271869659424,
        "learning_rate": 9.940811865207589e-06,
        "epoch": 0.6204047914085089,
        "step": 4506
    },
    {
        "loss": 1.7734,
        "grad_norm": 2.5621283054351807,
        "learning_rate": 9.884289340564934e-06,
        "epoch": 0.6205424755610629,
        "step": 4507
    },
    {
        "loss": 2.1814,
        "grad_norm": 1.661689281463623,
        "learning_rate": 9.82791960965751e-06,
        "epoch": 0.620680159713617,
        "step": 4508
    },
    {
        "loss": 2.1648,
        "grad_norm": 1.6844168901443481,
        "learning_rate": 9.771702768061686e-06,
        "epoch": 0.620817843866171,
        "step": 4509
    },
    {
        "loss": 2.2188,
        "grad_norm": 1.8580172061920166,
        "learning_rate": 9.715638911094882e-06,
        "epoch": 0.620955528018725,
        "step": 4510
    },
    {
        "loss": 1.7587,
        "grad_norm": 1.5305346250534058,
        "learning_rate": 9.65972813381486e-06,
        "epoch": 0.6210932121712791,
        "step": 4511
    },
    {
        "loss": 1.9514,
        "grad_norm": 2.1587822437286377,
        "learning_rate": 9.603970531019923e-06,
        "epoch": 0.6212308963238331,
        "step": 4512
    },
    {
        "loss": 1.8851,
        "grad_norm": 2.612009048461914,
        "learning_rate": 9.548366197248671e-06,
        "epoch": 0.6213685804763872,
        "step": 4513
    },
    {
        "loss": 2.0396,
        "grad_norm": 1.9694535732269287,
        "learning_rate": 9.49291522677983e-06,
        "epoch": 0.6215062646289412,
        "step": 4514
    },
    {
        "loss": 2.0012,
        "grad_norm": 2.065189838409424,
        "learning_rate": 9.437617713632074e-06,
        "epoch": 0.6216439487814952,
        "step": 4515
    },
    {
        "loss": 1.6714,
        "grad_norm": 2.375497341156006,
        "learning_rate": 9.382473751563903e-06,
        "epoch": 0.6217816329340493,
        "step": 4516
    },
    {
        "loss": 1.9528,
        "grad_norm": 1.850508689880371,
        "learning_rate": 9.327483434073491e-06,
        "epoch": 0.6219193170866033,
        "step": 4517
    },
    {
        "loss": 2.2328,
        "grad_norm": 2.2784476280212402,
        "learning_rate": 9.272646854398393e-06,
        "epoch": 0.6220570012391574,
        "step": 4518
    },
    {
        "loss": 0.597,
        "grad_norm": 2.2404041290283203,
        "learning_rate": 9.217964105515698e-06,
        "epoch": 0.6221946853917114,
        "step": 4519
    },
    {
        "loss": 2.3626,
        "grad_norm": 1.111883282661438,
        "learning_rate": 9.163435280141507e-06,
        "epoch": 0.6223323695442654,
        "step": 4520
    },
    {
        "loss": 1.8394,
        "grad_norm": 1.8123458623886108,
        "learning_rate": 9.109060470730901e-06,
        "epoch": 0.6224700536968195,
        "step": 4521
    },
    {
        "loss": 2.0722,
        "grad_norm": 1.6644169092178345,
        "learning_rate": 9.05483976947803e-06,
        "epoch": 0.6226077378493735,
        "step": 4522
    },
    {
        "loss": 2.0496,
        "grad_norm": 1.6805007457733154,
        "learning_rate": 9.000773268315587e-06,
        "epoch": 0.6227454220019276,
        "step": 4523
    },
    {
        "loss": 1.9213,
        "grad_norm": 1.8655505180358887,
        "learning_rate": 8.946861058914823e-06,
        "epoch": 0.6228831061544816,
        "step": 4524
    },
    {
        "loss": 2.3843,
        "grad_norm": 1.375117301940918,
        "learning_rate": 8.89310323268544e-06,
        "epoch": 0.6230207903070356,
        "step": 4525
    },
    {
        "loss": 2.2057,
        "grad_norm": 1.5948123931884766,
        "learning_rate": 8.83949988077536e-06,
        "epoch": 0.6231584744595897,
        "step": 4526
    },
    {
        "loss": 2.6148,
        "grad_norm": 2.405755043029785,
        "learning_rate": 8.786051094070547e-06,
        "epoch": 0.6232961586121437,
        "step": 4527
    },
    {
        "loss": 2.4814,
        "grad_norm": 2.2568256855010986,
        "learning_rate": 8.732756963194966e-06,
        "epoch": 0.6234338427646978,
        "step": 4528
    },
    {
        "loss": 2.2935,
        "grad_norm": 1.3046241998672485,
        "learning_rate": 8.67961757851028e-06,
        "epoch": 0.6235715269172518,
        "step": 4529
    },
    {
        "loss": 2.2225,
        "grad_norm": 1.126417875289917,
        "learning_rate": 8.626633030115872e-06,
        "epoch": 0.6237092110698058,
        "step": 4530
    },
    {
        "loss": 1.6789,
        "grad_norm": 2.6412551403045654,
        "learning_rate": 8.573803407848457e-06,
        "epoch": 0.6238468952223599,
        "step": 4531
    },
    {
        "loss": 1.1167,
        "grad_norm": 2.386272668838501,
        "learning_rate": 8.521128801282264e-06,
        "epoch": 0.6239845793749139,
        "step": 4532
    },
    {
        "loss": 1.4859,
        "grad_norm": 2.0253446102142334,
        "learning_rate": 8.468609299728514e-06,
        "epoch": 0.624122263527468,
        "step": 4533
    },
    {
        "loss": 2.0669,
        "grad_norm": 1.7432427406311035,
        "learning_rate": 8.416244992235456e-06,
        "epoch": 0.624259947680022,
        "step": 4534
    },
    {
        "loss": 2.4549,
        "grad_norm": 1.4267078638076782,
        "learning_rate": 8.36403596758838e-06,
        "epoch": 0.624397631832576,
        "step": 4535
    },
    {
        "loss": 2.2118,
        "grad_norm": 1.8071995973587036,
        "learning_rate": 8.311982314309109e-06,
        "epoch": 0.6245353159851301,
        "step": 4536
    },
    {
        "loss": 2.0237,
        "grad_norm": 1.2925890684127808,
        "learning_rate": 8.260084120656042e-06,
        "epoch": 0.6246730001376841,
        "step": 4537
    },
    {
        "loss": 2.035,
        "grad_norm": 2.532599449157715,
        "learning_rate": 8.208341474624071e-06,
        "epoch": 0.6248106842902382,
        "step": 4538
    },
    {
        "loss": 1.1962,
        "grad_norm": 1.143985390663147,
        "learning_rate": 8.156754463944316e-06,
        "epoch": 0.6249483684427922,
        "step": 4539
    },
    {
        "loss": 2.0061,
        "grad_norm": 1.5382349491119385,
        "learning_rate": 8.10532317608399e-06,
        "epoch": 0.6250860525953463,
        "step": 4540
    },
    {
        "loss": 1.2567,
        "grad_norm": 2.3818769454956055,
        "learning_rate": 8.054047698246314e-06,
        "epoch": 0.6252237367479003,
        "step": 4541
    },
    {
        "loss": 1.5244,
        "grad_norm": 1.0994075536727905,
        "learning_rate": 8.002928117370322e-06,
        "epoch": 0.6253614209004543,
        "step": 4542
    },
    {
        "loss": 1.3362,
        "grad_norm": 2.0137784481048584,
        "learning_rate": 7.95196452013064e-06,
        "epoch": 0.6254991050530084,
        "step": 4543
    },
    {
        "loss": 2.5529,
        "grad_norm": 1.0601072311401367,
        "learning_rate": 7.901156992937597e-06,
        "epoch": 0.6256367892055624,
        "step": 4544
    },
    {
        "loss": 2.3213,
        "grad_norm": 1.2032270431518555,
        "learning_rate": 7.850505621936732e-06,
        "epoch": 0.6257744733581165,
        "step": 4545
    },
    {
        "loss": 2.412,
        "grad_norm": 2.194694995880127,
        "learning_rate": 7.800010493008824e-06,
        "epoch": 0.6259121575106705,
        "step": 4546
    },
    {
        "loss": 1.9947,
        "grad_norm": 1.7662609815597534,
        "learning_rate": 7.749671691769889e-06,
        "epoch": 0.6260498416632245,
        "step": 4547
    },
    {
        "loss": 2.4762,
        "grad_norm": 1.4720039367675781,
        "learning_rate": 7.699489303570762e-06,
        "epoch": 0.6261875258157786,
        "step": 4548
    },
    {
        "loss": 2.1575,
        "grad_norm": 1.688645601272583,
        "learning_rate": 7.64946341349707e-06,
        "epoch": 0.6263252099683326,
        "step": 4549
    },
    {
        "loss": 1.7797,
        "grad_norm": 1.2968902587890625,
        "learning_rate": 7.599594106369135e-06,
        "epoch": 0.6264628941208867,
        "step": 4550
    },
    {
        "loss": 1.8683,
        "grad_norm": 2.2634944915771484,
        "learning_rate": 7.549881466741804e-06,
        "epoch": 0.6266005782734407,
        "step": 4551
    },
    {
        "loss": 1.7098,
        "grad_norm": 2.1939306259155273,
        "learning_rate": 7.5003255789042215e-06,
        "epoch": 0.6267382624259947,
        "step": 4552
    },
    {
        "loss": 1.9859,
        "grad_norm": 1.3822436332702637,
        "learning_rate": 7.450926526879831e-06,
        "epoch": 0.6268759465785488,
        "step": 4553
    },
    {
        "loss": 2.1258,
        "grad_norm": 1.2158472537994385,
        "learning_rate": 7.4016843944260895e-06,
        "epoch": 0.6270136307311028,
        "step": 4554
    },
    {
        "loss": 2.0069,
        "grad_norm": 1.3932056427001953,
        "learning_rate": 7.35259926503441e-06,
        "epoch": 0.6271513148836569,
        "step": 4555
    },
    {
        "loss": 2.1061,
        "grad_norm": 3.701629161834717,
        "learning_rate": 7.303671221930097e-06,
        "epoch": 0.6272889990362109,
        "step": 4556
    },
    {
        "loss": 1.9067,
        "grad_norm": 2.1830716133117676,
        "learning_rate": 7.254900348071992e-06,
        "epoch": 0.6274266831887649,
        "step": 4557
    },
    {
        "loss": 1.6309,
        "grad_norm": 2.2588820457458496,
        "learning_rate": 7.206286726152422e-06,
        "epoch": 0.627564367341319,
        "step": 4558
    },
    {
        "loss": 1.3397,
        "grad_norm": 2.666485548019409,
        "learning_rate": 7.157830438597246e-06,
        "epoch": 0.627702051493873,
        "step": 4559
    },
    {
        "loss": 2.1159,
        "grad_norm": 3.2011923789978027,
        "learning_rate": 7.109531567565464e-06,
        "epoch": 0.6278397356464271,
        "step": 4560
    },
    {
        "loss": 2.1343,
        "grad_norm": 2.2742888927459717,
        "learning_rate": 7.061390194949136e-06,
        "epoch": 0.6279774197989811,
        "step": 4561
    },
    {
        "loss": 1.9577,
        "grad_norm": 1.54286789894104,
        "learning_rate": 7.013406402373357e-06,
        "epoch": 0.6281151039515351,
        "step": 4562
    },
    {
        "loss": 2.2255,
        "grad_norm": 1.5542858839035034,
        "learning_rate": 6.965580271195959e-06,
        "epoch": 0.6282527881040892,
        "step": 4563
    },
    {
        "loss": 2.0305,
        "grad_norm": 1.3692854642868042,
        "learning_rate": 6.917911882507533e-06,
        "epoch": 0.6283904722566432,
        "step": 4564
    },
    {
        "loss": 2.1254,
        "grad_norm": 1.234932780265808,
        "learning_rate": 6.870401317131192e-06,
        "epoch": 0.6285281564091973,
        "step": 4565
    },
    {
        "loss": 1.9149,
        "grad_norm": 1.7372610569000244,
        "learning_rate": 6.8230486556223995e-06,
        "epoch": 0.6286658405617513,
        "step": 4566
    },
    {
        "loss": 1.2307,
        "grad_norm": 3.1083850860595703,
        "learning_rate": 6.775853978268932e-06,
        "epoch": 0.6288035247143053,
        "step": 4567
    },
    {
        "loss": 2.3547,
        "grad_norm": 1.673786997795105,
        "learning_rate": 6.728817365090756e-06,
        "epoch": 0.6289412088668594,
        "step": 4568
    },
    {
        "loss": 2.2555,
        "grad_norm": 1.7880116701126099,
        "learning_rate": 6.681938895839757e-06,
        "epoch": 0.6290788930194134,
        "step": 4569
    },
    {
        "loss": 2.1768,
        "grad_norm": 1.4492218494415283,
        "learning_rate": 6.635218649999642e-06,
        "epoch": 0.6292165771719676,
        "step": 4570
    },
    {
        "loss": 2.1058,
        "grad_norm": 1.7538985013961792,
        "learning_rate": 6.588656706786011e-06,
        "epoch": 0.6293542613245215,
        "step": 4571
    },
    {
        "loss": 1.8257,
        "grad_norm": 1.8626450300216675,
        "learning_rate": 6.542253145145894e-06,
        "epoch": 0.6294919454770755,
        "step": 4572
    },
    {
        "loss": 1.8135,
        "grad_norm": 2.874755620956421,
        "learning_rate": 6.496008043757839e-06,
        "epoch": 0.6296296296296297,
        "step": 4573
    },
    {
        "loss": 1.3591,
        "grad_norm": 2.6305418014526367,
        "learning_rate": 6.449921481031762e-06,
        "epoch": 0.6297673137821836,
        "step": 4574
    },
    {
        "loss": 1.8137,
        "grad_norm": 1.8692257404327393,
        "learning_rate": 6.403993535108688e-06,
        "epoch": 0.6299049979347378,
        "step": 4575
    },
    {
        "loss": 2.1603,
        "grad_norm": 1.3359053134918213,
        "learning_rate": 6.358224283860759e-06,
        "epoch": 0.6300426820872917,
        "step": 4576
    },
    {
        "loss": 2.1797,
        "grad_norm": 1.202024221420288,
        "learning_rate": 6.312613804891065e-06,
        "epoch": 0.6301803662398457,
        "step": 4577
    },
    {
        "loss": 2.0477,
        "grad_norm": 1.6708675622940063,
        "learning_rate": 6.267162175533436e-06,
        "epoch": 0.6303180503923999,
        "step": 4578
    },
    {
        "loss": 1.8278,
        "grad_norm": 1.689815640449524,
        "learning_rate": 6.221869472852349e-06,
        "epoch": 0.6304557345449538,
        "step": 4579
    },
    {
        "loss": 1.0755,
        "grad_norm": 2.9714832305908203,
        "learning_rate": 6.176735773642961e-06,
        "epoch": 0.630593418697508,
        "step": 4580
    },
    {
        "loss": 2.0874,
        "grad_norm": 1.303503394126892,
        "learning_rate": 6.131761154430693e-06,
        "epoch": 0.630731102850062,
        "step": 4581
    },
    {
        "loss": 1.5271,
        "grad_norm": 2.730783700942993,
        "learning_rate": 6.086945691471224e-06,
        "epoch": 0.630868787002616,
        "step": 4582
    },
    {
        "loss": 2.017,
        "grad_norm": 1.5375158786773682,
        "learning_rate": 6.042289460750539e-06,
        "epoch": 0.6310064711551701,
        "step": 4583
    },
    {
        "loss": 2.1623,
        "grad_norm": 2.0400137901306152,
        "learning_rate": 5.997792537984493e-06,
        "epoch": 0.631144155307724,
        "step": 4584
    },
    {
        "loss": 1.546,
        "grad_norm": 2.2633230686187744,
        "learning_rate": 5.953454998618846e-06,
        "epoch": 0.6312818394602782,
        "step": 4585
    },
    {
        "loss": 1.6115,
        "grad_norm": 2.3789308071136475,
        "learning_rate": 5.909276917829198e-06,
        "epoch": 0.6314195236128322,
        "step": 4586
    },
    {
        "loss": 2.2826,
        "grad_norm": 1.5774483680725098,
        "learning_rate": 5.865258370520721e-06,
        "epoch": 0.6315572077653862,
        "step": 4587
    },
    {
        "loss": 1.5312,
        "grad_norm": 2.362003803253174,
        "learning_rate": 5.821399431328079e-06,
        "epoch": 0.6316948919179403,
        "step": 4588
    },
    {
        "loss": 1.4447,
        "grad_norm": 1.6347683668136597,
        "learning_rate": 5.777700174615386e-06,
        "epoch": 0.6318325760704943,
        "step": 4589
    },
    {
        "loss": 2.3835,
        "grad_norm": 1.3523679971694946,
        "learning_rate": 5.734160674475941e-06,
        "epoch": 0.6319702602230484,
        "step": 4590
    },
    {
        "loss": 2.3463,
        "grad_norm": 2.370272159576416,
        "learning_rate": 5.690781004732149e-06,
        "epoch": 0.6321079443756024,
        "step": 4591
    },
    {
        "loss": 2.1052,
        "grad_norm": 1.4830248355865479,
        "learning_rate": 5.647561238935573e-06,
        "epoch": 0.6322456285281564,
        "step": 4592
    },
    {
        "loss": 1.5857,
        "grad_norm": 1.5327873229980469,
        "learning_rate": 5.604501450366495e-06,
        "epoch": 0.6323833126807105,
        "step": 4593
    },
    {
        "loss": 2.0604,
        "grad_norm": 1.281455636024475,
        "learning_rate": 5.56160171203397e-06,
        "epoch": 0.6325209968332645,
        "step": 4594
    },
    {
        "loss": 2.5253,
        "grad_norm": 1.5200637578964233,
        "learning_rate": 5.518862096675781e-06,
        "epoch": 0.6326586809858186,
        "step": 4595
    },
    {
        "loss": 2.1892,
        "grad_norm": 2.8092734813690186,
        "learning_rate": 5.4762826767581265e-06,
        "epoch": 0.6327963651383726,
        "step": 4596
    },
    {
        "loss": 1.2301,
        "grad_norm": 4.272039413452148,
        "learning_rate": 5.433863524475591e-06,
        "epoch": 0.6329340492909266,
        "step": 4597
    },
    {
        "loss": 1.7112,
        "grad_norm": 1.4501292705535889,
        "learning_rate": 5.391604711751097e-06,
        "epoch": 0.6330717334434807,
        "step": 4598
    },
    {
        "loss": 2.05,
        "grad_norm": 2.1017067432403564,
        "learning_rate": 5.349506310235619e-06,
        "epoch": 0.6332094175960347,
        "step": 4599
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.3676153421401978,
        "learning_rate": 5.307568391308182e-06,
        "epoch": 0.6333471017485888,
        "step": 4600
    },
    {
        "loss": 2.1998,
        "grad_norm": 1.6461845636367798,
        "learning_rate": 5.26579102607575e-06,
        "epoch": 0.6334847859011428,
        "step": 4601
    },
    {
        "loss": 1.5486,
        "grad_norm": 2.1930651664733887,
        "learning_rate": 5.224174285373007e-06,
        "epoch": 0.6336224700536969,
        "step": 4602
    },
    {
        "loss": 1.5478,
        "grad_norm": 2.955063819885254,
        "learning_rate": 5.182718239762252e-06,
        "epoch": 0.6337601542062509,
        "step": 4603
    },
    {
        "loss": 1.6065,
        "grad_norm": 1.7688997983932495,
        "learning_rate": 5.141422959533493e-06,
        "epoch": 0.6338978383588049,
        "step": 4604
    },
    {
        "loss": 1.688,
        "grad_norm": 1.5203876495361328,
        "learning_rate": 5.100288514703999e-06,
        "epoch": 0.634035522511359,
        "step": 4605
    },
    {
        "loss": 2.2403,
        "grad_norm": 3.1066410541534424,
        "learning_rate": 5.059314975018336e-06,
        "epoch": 0.634173206663913,
        "step": 4606
    },
    {
        "loss": 1.4234,
        "grad_norm": 2.0407662391662598,
        "learning_rate": 5.018502409948389e-06,
        "epoch": 0.6343108908164671,
        "step": 4607
    },
    {
        "loss": 1.9879,
        "grad_norm": 5.375146865844727,
        "learning_rate": 4.9778508886929695e-06,
        "epoch": 0.6344485749690211,
        "step": 4608
    },
    {
        "loss": 2.3875,
        "grad_norm": 1.4710447788238525,
        "learning_rate": 4.937360480177844e-06,
        "epoch": 0.6345862591215751,
        "step": 4609
    },
    {
        "loss": 2.1537,
        "grad_norm": 2.5951266288757324,
        "learning_rate": 4.8970312530557175e-06,
        "epoch": 0.6347239432741292,
        "step": 4610
    },
    {
        "loss": 1.5638,
        "grad_norm": 2.689582109451294,
        "learning_rate": 4.856863275705892e-06,
        "epoch": 0.6348616274266832,
        "step": 4611
    },
    {
        "loss": 1.3969,
        "grad_norm": 2.0532050132751465,
        "learning_rate": 4.816856616234289e-06,
        "epoch": 0.6349993115792373,
        "step": 4612
    },
    {
        "loss": 2.2419,
        "grad_norm": 1.4585477113723755,
        "learning_rate": 4.777011342473392e-06,
        "epoch": 0.6351369957317913,
        "step": 4613
    },
    {
        "loss": 2.4251,
        "grad_norm": 1.9332135915756226,
        "learning_rate": 4.737327521981938e-06,
        "epoch": 0.6352746798843453,
        "step": 4614
    },
    {
        "loss": 0.898,
        "grad_norm": 2.6533944606781006,
        "learning_rate": 4.697805222044993e-06,
        "epoch": 0.6354123640368994,
        "step": 4615
    },
    {
        "loss": 2.1818,
        "grad_norm": 1.790795922279358,
        "learning_rate": 4.658444509673676e-06,
        "epoch": 0.6355500481894534,
        "step": 4616
    },
    {
        "loss": 2.2028,
        "grad_norm": 2.321167230606079,
        "learning_rate": 4.619245451605281e-06,
        "epoch": 0.6356877323420075,
        "step": 4617
    },
    {
        "loss": 2.4458,
        "grad_norm": 1.6646183729171753,
        "learning_rate": 4.5802081143028755e-06,
        "epoch": 0.6358254164945615,
        "step": 4618
    },
    {
        "loss": 2.0904,
        "grad_norm": 2.8929800987243652,
        "learning_rate": 4.541332563955336e-06,
        "epoch": 0.6359631006471155,
        "step": 4619
    },
    {
        "loss": 1.5027,
        "grad_norm": 2.8098833560943604,
        "learning_rate": 4.502618866477315e-06,
        "epoch": 0.6361007847996696,
        "step": 4620
    },
    {
        "loss": 2.2989,
        "grad_norm": 2.19179368019104,
        "learning_rate": 4.464067087508961e-06,
        "epoch": 0.6362384689522236,
        "step": 4621
    },
    {
        "loss": 1.9232,
        "grad_norm": 3.681715726852417,
        "learning_rate": 4.425677292415864e-06,
        "epoch": 0.6363761531047777,
        "step": 4622
    },
    {
        "loss": 1.9692,
        "grad_norm": 1.4343841075897217,
        "learning_rate": 4.387449546289079e-06,
        "epoch": 0.6365138372573317,
        "step": 4623
    },
    {
        "loss": 2.0113,
        "grad_norm": 3.104304075241089,
        "learning_rate": 4.349383913944782e-06,
        "epoch": 0.6366515214098857,
        "step": 4624
    },
    {
        "loss": 1.8374,
        "grad_norm": 2.849475622177124,
        "learning_rate": 4.311480459924322e-06,
        "epoch": 0.6367892055624398,
        "step": 4625
    },
    {
        "loss": 0.8471,
        "grad_norm": 2.2510080337524414,
        "learning_rate": 4.273739248494113e-06,
        "epoch": 0.6369268897149938,
        "step": 4626
    },
    {
        "loss": 1.7606,
        "grad_norm": 2.3044729232788086,
        "learning_rate": 4.236160343645434e-06,
        "epoch": 0.6370645738675479,
        "step": 4627
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.645039677619934,
        "learning_rate": 4.198743809094319e-06,
        "epoch": 0.6372022580201019,
        "step": 4628
    },
    {
        "loss": 1.6048,
        "grad_norm": 2.157836437225342,
        "learning_rate": 4.161489708281674e-06,
        "epoch": 0.6373399421726559,
        "step": 4629
    },
    {
        "loss": 1.4115,
        "grad_norm": 1.6034904718399048,
        "learning_rate": 4.124398104372806e-06,
        "epoch": 0.63747762632521,
        "step": 4630
    },
    {
        "loss": 1.2185,
        "grad_norm": 2.0203933715820312,
        "learning_rate": 4.087469060257565e-06,
        "epoch": 0.637615310477764,
        "step": 4631
    },
    {
        "loss": 1.5628,
        "grad_norm": 1.8728383779525757,
        "learning_rate": 4.050702638550264e-06,
        "epoch": 0.6377529946303181,
        "step": 4632
    },
    {
        "loss": 1.8733,
        "grad_norm": 2.590223789215088,
        "learning_rate": 4.014098901589347e-06,
        "epoch": 0.6378906787828721,
        "step": 4633
    },
    {
        "loss": 2.2112,
        "grad_norm": 1.2375496625900269,
        "learning_rate": 3.977657911437494e-06,
        "epoch": 0.6380283629354261,
        "step": 4634
    },
    {
        "loss": 1.6023,
        "grad_norm": 3.7947592735290527,
        "learning_rate": 3.941379729881456e-06,
        "epoch": 0.6381660470879802,
        "step": 4635
    },
    {
        "loss": 2.2723,
        "grad_norm": 2.585592746734619,
        "learning_rate": 3.905264418431909e-06,
        "epoch": 0.6383037312405342,
        "step": 4636
    },
    {
        "loss": 2.2252,
        "grad_norm": 1.9854590892791748,
        "learning_rate": 3.869312038323347e-06,
        "epoch": 0.6384414153930883,
        "step": 4637
    },
    {
        "loss": 1.9798,
        "grad_norm": 1.3201707601547241,
        "learning_rate": 3.833522650514099e-06,
        "epoch": 0.6385790995456423,
        "step": 4638
    },
    {
        "loss": 1.5617,
        "grad_norm": 1.8705615997314453,
        "learning_rate": 3.797896315686067e-06,
        "epoch": 0.6387167836981963,
        "step": 4639
    },
    {
        "loss": 1.052,
        "grad_norm": 2.8906993865966797,
        "learning_rate": 3.762433094244677e-06,
        "epoch": 0.6388544678507504,
        "step": 4640
    },
    {
        "loss": 2.2365,
        "grad_norm": 2.630162477493286,
        "learning_rate": 3.727133046318865e-06,
        "epoch": 0.6389921520033044,
        "step": 4641
    },
    {
        "loss": 2.6914,
        "grad_norm": 1.6221339702606201,
        "learning_rate": 3.6919962317608524e-06,
        "epoch": 0.6391298361558585,
        "step": 4642
    },
    {
        "loss": 1.1074,
        "grad_norm": 1.6037274599075317,
        "learning_rate": 3.657022710146074e-06,
        "epoch": 0.6392675203084125,
        "step": 4643
    },
    {
        "loss": 2.1205,
        "grad_norm": 1.6294392347335815,
        "learning_rate": 3.6222125407731734e-06,
        "epoch": 0.6394052044609665,
        "step": 4644
    },
    {
        "loss": 1.8712,
        "grad_norm": 2.0634844303131104,
        "learning_rate": 3.587565782663749e-06,
        "epoch": 0.6395428886135206,
        "step": 4645
    },
    {
        "loss": 0.9652,
        "grad_norm": 4.012012004852295,
        "learning_rate": 3.553082494562332e-06,
        "epoch": 0.6396805727660746,
        "step": 4646
    },
    {
        "loss": 1.7503,
        "grad_norm": 1.3437997102737427,
        "learning_rate": 3.518762734936376e-06,
        "epoch": 0.6398182569186287,
        "step": 4647
    },
    {
        "loss": 1.4059,
        "grad_norm": 3.084547519683838,
        "learning_rate": 3.484606561975956e-06,
        "epoch": 0.6399559410711827,
        "step": 4648
    },
    {
        "loss": 1.9731,
        "grad_norm": 2.303262710571289,
        "learning_rate": 3.4506140335938243e-06,
        "epoch": 0.6400936252237367,
        "step": 4649
    },
    {
        "loss": 1.8356,
        "grad_norm": 1.479166030883789,
        "learning_rate": 3.4167852074253103e-06,
        "epoch": 0.6402313093762908,
        "step": 4650
    },
    {
        "loss": 2.6658,
        "grad_norm": 1.5364421606063843,
        "learning_rate": 3.383120140828122e-06,
        "epoch": 0.6403689935288448,
        "step": 4651
    },
    {
        "loss": 1.9479,
        "grad_norm": 1.8473249673843384,
        "learning_rate": 3.3496188908823002e-06,
        "epoch": 0.6405066776813989,
        "step": 4652
    },
    {
        "loss": 2.0901,
        "grad_norm": 3.3308489322662354,
        "learning_rate": 3.3162815143902183e-06,
        "epoch": 0.6406443618339529,
        "step": 4653
    },
    {
        "loss": 1.0,
        "grad_norm": 3.3609325885772705,
        "learning_rate": 3.283108067876328e-06,
        "epoch": 0.6407820459865069,
        "step": 4654
    },
    {
        "loss": 1.725,
        "grad_norm": 1.783476710319519,
        "learning_rate": 3.250098607587104e-06,
        "epoch": 0.640919730139061,
        "step": 4655
    },
    {
        "loss": 1.7513,
        "grad_norm": 1.9323456287384033,
        "learning_rate": 3.2172531894910873e-06,
        "epoch": 0.641057414291615,
        "step": 4656
    },
    {
        "loss": 2.2418,
        "grad_norm": 1.8208717107772827,
        "learning_rate": 3.184571869278574e-06,
        "epoch": 0.6411950984441691,
        "step": 4657
    },
    {
        "loss": 2.2324,
        "grad_norm": 2.1400697231292725,
        "learning_rate": 3.15205470236164e-06,
        "epoch": 0.6413327825967231,
        "step": 4658
    },
    {
        "loss": 1.8603,
        "grad_norm": 1.7836966514587402,
        "learning_rate": 3.119701743874126e-06,
        "epoch": 0.6414704667492772,
        "step": 4659
    },
    {
        "loss": 1.5366,
        "grad_norm": 1.2068904638290405,
        "learning_rate": 3.087513048671342e-06,
        "epoch": 0.6416081509018312,
        "step": 4660
    },
    {
        "loss": 1.6084,
        "grad_norm": 2.7091541290283203,
        "learning_rate": 3.05548867133012e-06,
        "epoch": 0.6417458350543852,
        "step": 4661
    },
    {
        "loss": 2.218,
        "grad_norm": 1.7780946493148804,
        "learning_rate": 3.0236286661487366e-06,
        "epoch": 0.6418835192069393,
        "step": 4662
    },
    {
        "loss": 1.6994,
        "grad_norm": 3.184785842895508,
        "learning_rate": 2.9919330871467255e-06,
        "epoch": 0.6420212033594933,
        "step": 4663
    },
    {
        "loss": 2.1566,
        "grad_norm": 1.9192132949829102,
        "learning_rate": 2.960401988064776e-06,
        "epoch": 0.6421588875120474,
        "step": 4664
    },
    {
        "loss": 1.5975,
        "grad_norm": 2.512543201446533,
        "learning_rate": 2.9290354223648452e-06,
        "epoch": 0.6422965716646014,
        "step": 4665
    },
    {
        "loss": 2.3217,
        "grad_norm": 1.1443548202514648,
        "learning_rate": 2.8978334432297915e-06,
        "epoch": 0.6424342558171554,
        "step": 4666
    },
    {
        "loss": 1.6848,
        "grad_norm": 2.390092372894287,
        "learning_rate": 2.866796103563418e-06,
        "epoch": 0.6425719399697095,
        "step": 4667
    },
    {
        "loss": 1.1777,
        "grad_norm": 2.1737160682678223,
        "learning_rate": 2.835923455990497e-06,
        "epoch": 0.6427096241222635,
        "step": 4668
    },
    {
        "loss": 2.234,
        "grad_norm": 1.4848395586013794,
        "learning_rate": 2.8052155528564104e-06,
        "epoch": 0.6428473082748176,
        "step": 4669
    },
    {
        "loss": 2.787,
        "grad_norm": 1.879272699356079,
        "learning_rate": 2.7746724462272777e-06,
        "epoch": 0.6429849924273716,
        "step": 4670
    },
    {
        "loss": 1.2027,
        "grad_norm": 2.90328049659729,
        "learning_rate": 2.7442941878898398e-06,
        "epoch": 0.6431226765799256,
        "step": 4671
    },
    {
        "loss": 2.2182,
        "grad_norm": 1.1184141635894775,
        "learning_rate": 2.7140808293512622e-06,
        "epoch": 0.6432603607324797,
        "step": 4672
    },
    {
        "loss": 1.3409,
        "grad_norm": 2.8009233474731445,
        "learning_rate": 2.6840324218391223e-06,
        "epoch": 0.6433980448850337,
        "step": 4673
    },
    {
        "loss": 1.5851,
        "grad_norm": 2.38287353515625,
        "learning_rate": 2.6541490163014104e-06,
        "epoch": 0.6435357290375878,
        "step": 4674
    },
    {
        "loss": 2.0884,
        "grad_norm": 1.079591989517212,
        "learning_rate": 2.624430663406241e-06,
        "epoch": 0.6436734131901418,
        "step": 4675
    },
    {
        "loss": 1.4288,
        "grad_norm": 1.4561594724655151,
        "learning_rate": 2.5948774135419076e-06,
        "epoch": 0.6438110973426958,
        "step": 4676
    },
    {
        "loss": 0.9802,
        "grad_norm": 2.483853340148926,
        "learning_rate": 2.565489316816838e-06,
        "epoch": 0.6439487814952499,
        "step": 4677
    },
    {
        "loss": 2.3695,
        "grad_norm": 1.89858877658844,
        "learning_rate": 2.5362664230593305e-06,
        "epoch": 0.6440864656478039,
        "step": 4678
    },
    {
        "loss": 1.379,
        "grad_norm": 1.9036213159561157,
        "learning_rate": 2.5072087818176272e-06,
        "epoch": 0.644224149800358,
        "step": 4679
    },
    {
        "loss": 1.857,
        "grad_norm": 1.46160089969635,
        "learning_rate": 2.478316442359818e-06,
        "epoch": 0.644361833952912,
        "step": 4680
    },
    {
        "loss": 1.9047,
        "grad_norm": 2.648737907409668,
        "learning_rate": 2.449589453673673e-06,
        "epoch": 0.644499518105466,
        "step": 4681
    },
    {
        "loss": 1.68,
        "grad_norm": 1.8903871774673462,
        "learning_rate": 2.421027864466574e-06,
        "epoch": 0.6446372022580201,
        "step": 4682
    },
    {
        "loss": 2.1317,
        "grad_norm": 1.1840567588806152,
        "learning_rate": 2.3926317231655617e-06,
        "epoch": 0.6447748864105741,
        "step": 4683
    },
    {
        "loss": 1.914,
        "grad_norm": 1.8028072118759155,
        "learning_rate": 2.3644010779170556e-06,
        "epoch": 0.6449125705631282,
        "step": 4684
    },
    {
        "loss": 1.5374,
        "grad_norm": 2.611818313598633,
        "learning_rate": 2.336335976586912e-06,
        "epoch": 0.6450502547156822,
        "step": 4685
    },
    {
        "loss": 2.0138,
        "grad_norm": 2.09541916847229,
        "learning_rate": 2.308436466760322e-06,
        "epoch": 0.6451879388682362,
        "step": 4686
    },
    {
        "loss": 1.726,
        "grad_norm": 1.2938183546066284,
        "learning_rate": 2.2807025957416907e-06,
        "epoch": 0.6453256230207903,
        "step": 4687
    },
    {
        "loss": 2.2601,
        "grad_norm": 1.2307074069976807,
        "learning_rate": 2.253134410554558e-06,
        "epoch": 0.6454633071733443,
        "step": 4688
    },
    {
        "loss": 1.8705,
        "grad_norm": 1.848573923110962,
        "learning_rate": 2.22573195794159e-06,
        "epoch": 0.6456009913258984,
        "step": 4689
    },
    {
        "loss": 2.1086,
        "grad_norm": 1.9492614269256592,
        "learning_rate": 2.198495284364421e-06,
        "epoch": 0.6457386754784524,
        "step": 4690
    },
    {
        "loss": 2.15,
        "grad_norm": 2.352255344390869,
        "learning_rate": 2.171424436003544e-06,
        "epoch": 0.6458763596310064,
        "step": 4691
    },
    {
        "loss": 1.7939,
        "grad_norm": 2.1222586631774902,
        "learning_rate": 2.1445194587584427e-06,
        "epoch": 0.6460140437835605,
        "step": 4692
    },
    {
        "loss": 1.5731,
        "grad_norm": 2.13507080078125,
        "learning_rate": 2.1177803982472046e-06,
        "epoch": 0.6461517279361145,
        "step": 4693
    },
    {
        "loss": 1.55,
        "grad_norm": 2.4178414344787598,
        "learning_rate": 2.0912072998066522e-06,
        "epoch": 0.6462894120886686,
        "step": 4694
    },
    {
        "loss": 2.1482,
        "grad_norm": 1.807697057723999,
        "learning_rate": 2.064800208492279e-06,
        "epoch": 0.6464270962412226,
        "step": 4695
    },
    {
        "loss": 2.003,
        "grad_norm": 2.2087481021881104,
        "learning_rate": 2.0385591690780137e-06,
        "epoch": 0.6465647803937766,
        "step": 4696
    },
    {
        "loss": 0.9086,
        "grad_norm": 1.9798775911331177,
        "learning_rate": 2.0124842260562772e-06,
        "epoch": 0.6467024645463307,
        "step": 4697
    },
    {
        "loss": 2.1193,
        "grad_norm": 1.653973937034607,
        "learning_rate": 1.986575423637904e-06,
        "epoch": 0.6468401486988847,
        "step": 4698
    },
    {
        "loss": 0.7267,
        "grad_norm": 2.1088297367095947,
        "learning_rate": 1.960832805751989e-06,
        "epoch": 0.6469778328514388,
        "step": 4699
    },
    {
        "loss": 1.2012,
        "grad_norm": 3.5001380443573,
        "learning_rate": 1.9352564160458497e-06,
        "epoch": 0.6471155170039928,
        "step": 4700
    },
    {
        "loss": 2.5247,
        "grad_norm": 2.332465410232544,
        "learning_rate": 1.9098462978849762e-06,
        "epoch": 0.6472532011565468,
        "step": 4701
    },
    {
        "loss": 2.0017,
        "grad_norm": 2.2455215454101562,
        "learning_rate": 1.8846024943529828e-06,
        "epoch": 0.6473908853091009,
        "step": 4702
    },
    {
        "loss": 2.1709,
        "grad_norm": 2.1395859718322754,
        "learning_rate": 1.8595250482514093e-06,
        "epoch": 0.6475285694616549,
        "step": 4703
    },
    {
        "loss": 1.8565,
        "grad_norm": 2.1795384883880615,
        "learning_rate": 1.834614002099777e-06,
        "epoch": 0.647666253614209,
        "step": 4704
    },
    {
        "loss": 2.8563,
        "grad_norm": 1.469718337059021,
        "learning_rate": 1.8098693981355109e-06,
        "epoch": 0.647803937766763,
        "step": 4705
    },
    {
        "loss": 1.8538,
        "grad_norm": 1.912894368171692,
        "learning_rate": 1.785291278313761e-06,
        "epoch": 0.647941621919317,
        "step": 4706
    },
    {
        "loss": 2.167,
        "grad_norm": 1.6683934926986694,
        "learning_rate": 1.7608796843074039e-06,
        "epoch": 0.6480793060718711,
        "step": 4707
    },
    {
        "loss": 1.4516,
        "grad_norm": 2.6353867053985596,
        "learning_rate": 1.7366346575070192e-06,
        "epoch": 0.6482169902244251,
        "step": 4708
    },
    {
        "loss": 1.0524,
        "grad_norm": 3.221017837524414,
        "learning_rate": 1.712556239020735e-06,
        "epoch": 0.6483546743769792,
        "step": 4709
    },
    {
        "loss": 1.9234,
        "grad_norm": 1.7793715000152588,
        "learning_rate": 1.68864446967415e-06,
        "epoch": 0.6484923585295332,
        "step": 4710
    },
    {
        "loss": 1.299,
        "grad_norm": 2.628784656524658,
        "learning_rate": 1.6648993900103883e-06,
        "epoch": 0.6486300426820872,
        "step": 4711
    },
    {
        "loss": 2.299,
        "grad_norm": 1.5781749486923218,
        "learning_rate": 1.6413210402898893e-06,
        "epoch": 0.6487677268346413,
        "step": 4712
    },
    {
        "loss": 2.5754,
        "grad_norm": 2.1242973804473877,
        "learning_rate": 1.6179094604903965e-06,
        "epoch": 0.6489054109871953,
        "step": 4713
    },
    {
        "loss": 1.6267,
        "grad_norm": 2.4044981002807617,
        "learning_rate": 1.594664690306935e-06,
        "epoch": 0.6490430951397494,
        "step": 4714
    },
    {
        "loss": 1.7752,
        "grad_norm": 2.622817039489746,
        "learning_rate": 1.571586769151656e-06,
        "epoch": 0.6491807792923034,
        "step": 4715
    },
    {
        "loss": 2.4875,
        "grad_norm": 1.8665542602539062,
        "learning_rate": 1.548675736153793e-06,
        "epoch": 0.6493184634448576,
        "step": 4716
    },
    {
        "loss": 2.1078,
        "grad_norm": 1.7460795640945435,
        "learning_rate": 1.525931630159716e-06,
        "epoch": 0.6494561475974115,
        "step": 4717
    },
    {
        "loss": 2.3462,
        "grad_norm": 1.4002959728240967,
        "learning_rate": 1.5033544897326668e-06,
        "epoch": 0.6495938317499655,
        "step": 4718
    },
    {
        "loss": 2.0492,
        "grad_norm": 1.510799765586853,
        "learning_rate": 1.4809443531528021e-06,
        "epoch": 0.6497315159025197,
        "step": 4719
    },
    {
        "loss": 2.4482,
        "grad_norm": 1.5793362855911255,
        "learning_rate": 1.4587012584171944e-06,
        "epoch": 0.6498692000550736,
        "step": 4720
    },
    {
        "loss": 2.0245,
        "grad_norm": 2.59580397605896,
        "learning_rate": 1.4366252432395976e-06,
        "epoch": 0.6500068842076278,
        "step": 4721
    },
    {
        "loss": 2.0416,
        "grad_norm": 2.102144956588745,
        "learning_rate": 1.4147163450505262e-06,
        "epoch": 0.6501445683601818,
        "step": 4722
    },
    {
        "loss": 1.6475,
        "grad_norm": 2.3749237060546875,
        "learning_rate": 1.3929746009971545e-06,
        "epoch": 0.6502822525127357,
        "step": 4723
    },
    {
        "loss": 2.5208,
        "grad_norm": 1.3072402477264404,
        "learning_rate": 1.3714000479432055e-06,
        "epoch": 0.6504199366652899,
        "step": 4724
    },
    {
        "loss": 1.7424,
        "grad_norm": 1.7911286354064941,
        "learning_rate": 1.3499927224689402e-06,
        "epoch": 0.6505576208178439,
        "step": 4725
    },
    {
        "loss": 2.1506,
        "grad_norm": 1.897802472114563,
        "learning_rate": 1.3287526608711131e-06,
        "epoch": 0.650695304970398,
        "step": 4726
    },
    {
        "loss": 2.0393,
        "grad_norm": 1.9051977396011353,
        "learning_rate": 1.3076798991628281e-06,
        "epoch": 0.650832989122952,
        "step": 4727
    },
    {
        "loss": 1.7863,
        "grad_norm": 1.518178939819336,
        "learning_rate": 1.2867744730735264e-06,
        "epoch": 0.650970673275506,
        "step": 4728
    },
    {
        "loss": 1.996,
        "grad_norm": 1.9693007469177246,
        "learning_rate": 1.2660364180489992e-06,
        "epoch": 0.6511083574280601,
        "step": 4729
    },
    {
        "loss": 2.6942,
        "grad_norm": 1.1452208757400513,
        "learning_rate": 1.2454657692511861e-06,
        "epoch": 0.651246041580614,
        "step": 4730
    },
    {
        "loss": 2.0735,
        "grad_norm": 1.3446365594863892,
        "learning_rate": 1.2250625615581769e-06,
        "epoch": 0.6513837257331682,
        "step": 4731
    },
    {
        "loss": 2.1273,
        "grad_norm": 2.160224437713623,
        "learning_rate": 1.2048268295642206e-06,
        "epoch": 0.6515214098857222,
        "step": 4732
    },
    {
        "loss": 1.3824,
        "grad_norm": 2.1307685375213623,
        "learning_rate": 1.1847586075795613e-06,
        "epoch": 0.6516590940382762,
        "step": 4733
    },
    {
        "loss": 1.7001,
        "grad_norm": 1.3229717016220093,
        "learning_rate": 1.1648579296304253e-06,
        "epoch": 0.6517967781908303,
        "step": 4734
    },
    {
        "loss": 1.6397,
        "grad_norm": 1.813964605331421,
        "learning_rate": 1.145124829458988e-06,
        "epoch": 0.6519344623433843,
        "step": 4735
    },
    {
        "loss": 2.2902,
        "grad_norm": 2.0406618118286133,
        "learning_rate": 1.1255593405232857e-06,
        "epoch": 0.6520721464959384,
        "step": 4736
    },
    {
        "loss": 2.0957,
        "grad_norm": 2.2391304969787598,
        "learning_rate": 1.1061614959971266e-06,
        "epoch": 0.6522098306484924,
        "step": 4737
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.2276850938796997,
        "learning_rate": 1.0869313287701466e-06,
        "epoch": 0.6523475148010464,
        "step": 4738
    },
    {
        "loss": 1.794,
        "grad_norm": 1.9995300769805908,
        "learning_rate": 1.0678688714476082e-06,
        "epoch": 0.6524851989536005,
        "step": 4739
    },
    {
        "loss": 1.551,
        "grad_norm": 2.2154102325439453,
        "learning_rate": 1.0489741563504352e-06,
        "epoch": 0.6526228831061545,
        "step": 4740
    },
    {
        "loss": 0.7801,
        "grad_norm": 2.7075464725494385,
        "learning_rate": 1.030247215515201e-06,
        "epoch": 0.6527605672587086,
        "step": 4741
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.8420764207839966,
        "learning_rate": 1.011688080693929e-06,
        "epoch": 0.6528982514112626,
        "step": 4742
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.9691420793533325,
        "learning_rate": 9.932967833541474e-07,
        "epoch": 0.6530359355638166,
        "step": 4743
    },
    {
        "loss": 1.8328,
        "grad_norm": 2.011221170425415,
        "learning_rate": 9.75073354678846e-07,
        "epoch": 0.6531736197163707,
        "step": 4744
    },
    {
        "loss": 2.7579,
        "grad_norm": 1.0868992805480957,
        "learning_rate": 9.570178255663642e-07,
        "epoch": 0.6533113038689247,
        "step": 4745
    },
    {
        "loss": 1.8478,
        "grad_norm": 2.6770052909851074,
        "learning_rate": 9.391302266303359e-07,
        "epoch": 0.6534489880214788,
        "step": 4746
    },
    {
        "loss": 2.1548,
        "grad_norm": 2.253396987915039,
        "learning_rate": 9.214105881997004e-07,
        "epoch": 0.6535866721740328,
        "step": 4747
    },
    {
        "loss": 2.5026,
        "grad_norm": 1.3840141296386719,
        "learning_rate": 9.038589403186249e-07,
        "epoch": 0.6537243563265868,
        "step": 4748
    },
    {
        "loss": 1.546,
        "grad_norm": 1.9507050514221191,
        "learning_rate": 8.864753127463932e-07,
        "epoch": 0.6538620404791409,
        "step": 4749
    },
    {
        "loss": 2.3418,
        "grad_norm": 1.186560869216919,
        "learning_rate": 8.692597349574616e-07,
        "epoch": 0.6539997246316949,
        "step": 4750
    },
    {
        "loss": 2.6107,
        "grad_norm": 1.0985136032104492,
        "learning_rate": 8.522122361413365e-07,
        "epoch": 0.654137408784249,
        "step": 4751
    },
    {
        "loss": 2.5107,
        "grad_norm": 1.3380414247512817,
        "learning_rate": 8.353328452024967e-07,
        "epoch": 0.654275092936803,
        "step": 4752
    },
    {
        "loss": 2.1703,
        "grad_norm": 1.7465128898620605,
        "learning_rate": 8.186215907604489e-07,
        "epoch": 0.654412777089357,
        "step": 4753
    },
    {
        "loss": 1.4524,
        "grad_norm": 2.5220015048980713,
        "learning_rate": 8.020785011496168e-07,
        "epoch": 0.6545504612419111,
        "step": 4754
    },
    {
        "loss": 2.0182,
        "grad_norm": 1.6917487382888794,
        "learning_rate": 7.857036044192523e-07,
        "epoch": 0.6546881453944651,
        "step": 4755
    },
    {
        "loss": 2.0204,
        "grad_norm": 1.1692345142364502,
        "learning_rate": 7.694969283334575e-07,
        "epoch": 0.6548258295470192,
        "step": 4756
    },
    {
        "loss": 2.0887,
        "grad_norm": 2.4327168464660645,
        "learning_rate": 7.53458500371118e-07,
        "epoch": 0.6549635136995732,
        "step": 4757
    },
    {
        "loss": 1.6561,
        "grad_norm": 4.070888042449951,
        "learning_rate": 7.375883477258372e-07,
        "epoch": 0.6551011978521272,
        "step": 4758
    },
    {
        "loss": 1.8403,
        "grad_norm": 2.0108768939971924,
        "learning_rate": 7.218864973059015e-07,
        "epoch": 0.6552388820046813,
        "step": 4759
    },
    {
        "loss": 2.3306,
        "grad_norm": 2.3921396732330322,
        "learning_rate": 7.063529757342591e-07,
        "epoch": 0.6553765661572353,
        "step": 4760
    },
    {
        "loss": 2.5393,
        "grad_norm": 2.8609628677368164,
        "learning_rate": 6.90987809348398e-07,
        "epoch": 0.6555142503097894,
        "step": 4761
    },
    {
        "loss": 2.5049,
        "grad_norm": 1.0851528644561768,
        "learning_rate": 6.757910242004451e-07,
        "epoch": 0.6556519344623434,
        "step": 4762
    },
    {
        "loss": 2.5031,
        "grad_norm": 2.6387827396392822,
        "learning_rate": 6.60762646056945e-07,
        "epoch": 0.6557896186148974,
        "step": 4763
    },
    {
        "loss": 1.9372,
        "grad_norm": 1.157459020614624,
        "learning_rate": 6.45902700398937e-07,
        "epoch": 0.6559273027674515,
        "step": 4764
    },
    {
        "loss": 1.842,
        "grad_norm": 2.231477975845337,
        "learning_rate": 6.312112124219005e-07,
        "epoch": 0.6560649869200055,
        "step": 4765
    },
    {
        "loss": 2.2298,
        "grad_norm": 2.199625253677368,
        "learning_rate": 6.166882070356539e-07,
        "epoch": 0.6562026710725596,
        "step": 4766
    },
    {
        "loss": 1.7712,
        "grad_norm": 1.8640607595443726,
        "learning_rate": 6.023337088643665e-07,
        "epoch": 0.6563403552251136,
        "step": 4767
    },
    {
        "loss": 1.4818,
        "grad_norm": 1.4435662031173706,
        "learning_rate": 5.881477422464921e-07,
        "epoch": 0.6564780393776676,
        "step": 4768
    },
    {
        "loss": 2.3674,
        "grad_norm": 1.36307692527771,
        "learning_rate": 5.741303312347457e-07,
        "epoch": 0.6566157235302217,
        "step": 4769
    },
    {
        "loss": 1.8784,
        "grad_norm": 1.4591768980026245,
        "learning_rate": 5.602814995960381e-07,
        "epoch": 0.6567534076827757,
        "step": 4770
    },
    {
        "loss": 1.3551,
        "grad_norm": 3.1896839141845703,
        "learning_rate": 5.466012708114532e-07,
        "epoch": 0.6568910918353298,
        "step": 4771
    },
    {
        "loss": 1.9216,
        "grad_norm": 2.376307964324951,
        "learning_rate": 5.330896680762032e-07,
        "epoch": 0.6570287759878838,
        "step": 4772
    },
    {
        "loss": 1.9194,
        "grad_norm": 1.9557479619979858,
        "learning_rate": 5.19746714299596e-07,
        "epoch": 0.6571664601404378,
        "step": 4773
    },
    {
        "loss": 2.4621,
        "grad_norm": 1.152933955192566,
        "learning_rate": 5.065724321050014e-07,
        "epoch": 0.6573041442929919,
        "step": 4774
    },
    {
        "loss": 1.0193,
        "grad_norm": 3.1126837730407715,
        "learning_rate": 4.935668438297625e-07,
        "epoch": 0.6574418284455459,
        "step": 4775
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.6024301052093506,
        "learning_rate": 4.807299715252289e-07,
        "epoch": 0.6575795125981,
        "step": 4776
    },
    {
        "loss": 1.9672,
        "grad_norm": 2.193834066390991,
        "learning_rate": 4.680618369567014e-07,
        "epoch": 0.657717196750654,
        "step": 4777
    },
    {
        "loss": 1.4669,
        "grad_norm": 2.7016899585723877,
        "learning_rate": 4.555624616033427e-07,
        "epoch": 0.6578548809032081,
        "step": 4778
    },
    {
        "loss": 1.8811,
        "grad_norm": 1.4276947975158691,
        "learning_rate": 4.4323186665820026e-07,
        "epoch": 0.6579925650557621,
        "step": 4779
    },
    {
        "loss": 1.7756,
        "grad_norm": 2.4618096351623535,
        "learning_rate": 4.310700730281392e-07,
        "epoch": 0.6581302492083161,
        "step": 4780
    },
    {
        "loss": 2.1026,
        "grad_norm": 1.2002772092819214,
        "learning_rate": 4.190771013338313e-07,
        "epoch": 0.6582679333608702,
        "step": 4781
    },
    {
        "loss": 1.7393,
        "grad_norm": 1.7923166751861572,
        "learning_rate": 4.072529719096996e-07,
        "epoch": 0.6584056175134242,
        "step": 4782
    },
    {
        "loss": 1.7771,
        "grad_norm": 3.0612447261810303,
        "learning_rate": 3.9559770480388504e-07,
        "epoch": 0.6585433016659783,
        "step": 4783
    },
    {
        "loss": 1.9122,
        "grad_norm": 2.2954657077789307,
        "learning_rate": 3.8411131977823533e-07,
        "epoch": 0.6586809858185323,
        "step": 4784
    },
    {
        "loss": 1.6826,
        "grad_norm": 2.139934539794922,
        "learning_rate": 3.727938363082273e-07,
        "epoch": 0.6588186699710863,
        "step": 4785
    },
    {
        "loss": 2.2434,
        "grad_norm": 1.4373804330825806,
        "learning_rate": 3.6164527358296674e-07,
        "epoch": 0.6589563541236404,
        "step": 4786
    },
    {
        "loss": 1.954,
        "grad_norm": 1.991045355796814,
        "learning_rate": 3.5066565050517754e-07,
        "epoch": 0.6590940382761944,
        "step": 4787
    },
    {
        "loss": 1.2191,
        "grad_norm": 3.576700448989868,
        "learning_rate": 3.3985498569111265e-07,
        "epoch": 0.6592317224287485,
        "step": 4788
    },
    {
        "loss": 2.1674,
        "grad_norm": 2.111924886703491,
        "learning_rate": 3.292132974705542e-07,
        "epoch": 0.6593694065813025,
        "step": 4789
    },
    {
        "loss": 1.7151,
        "grad_norm": 1.5024386644363403,
        "learning_rate": 3.187406038868024e-07,
        "epoch": 0.6595070907338565,
        "step": 4790
    },
    {
        "loss": 2.1084,
        "grad_norm": 1.8404555320739746,
        "learning_rate": 3.0843692269658664e-07,
        "epoch": 0.6596447748864106,
        "step": 4791
    },
    {
        "loss": 2.7144,
        "grad_norm": 1.31842839717865,
        "learning_rate": 2.983022713700989e-07,
        "epoch": 0.6597824590389646,
        "step": 4792
    },
    {
        "loss": 1.8887,
        "grad_norm": 2.115241765975952,
        "learning_rate": 2.883366670909271e-07,
        "epoch": 0.6599201431915187,
        "step": 4793
    },
    {
        "loss": 1.9746,
        "grad_norm": 2.9330263137817383,
        "learning_rate": 2.785401267560217e-07,
        "epoch": 0.6600578273440727,
        "step": 4794
    },
    {
        "loss": 1.3545,
        "grad_norm": 2.0410544872283936,
        "learning_rate": 2.6891266697571803e-07,
        "epoch": 0.6601955114966267,
        "step": 4795
    },
    {
        "loss": 1.6737,
        "grad_norm": 2.867661714553833,
        "learning_rate": 2.594543040736253e-07,
        "epoch": 0.6603331956491808,
        "step": 4796
    },
    {
        "loss": 1.7462,
        "grad_norm": 2.9438118934631348,
        "learning_rate": 2.5016505408667066e-07,
        "epoch": 0.6604708798017348,
        "step": 4797
    },
    {
        "loss": 1.4972,
        "grad_norm": 2.1069552898406982,
        "learning_rate": 2.410449327650444e-07,
        "epoch": 0.6606085639542889,
        "step": 4798
    },
    {
        "loss": 2.1576,
        "grad_norm": 1.1588292121887207,
        "learning_rate": 2.3209395557217683e-07,
        "epoch": 0.6607462481068429,
        "step": 4799
    },
    {
        "loss": 1.8387,
        "grad_norm": 1.703193187713623,
        "learning_rate": 2.2331213768468363e-07,
        "epoch": 0.6608839322593969,
        "step": 4800
    },
    {
        "loss": 2.2132,
        "grad_norm": 1.7309257984161377,
        "learning_rate": 2.1469949399239852e-07,
        "epoch": 0.661021616411951,
        "step": 4801
    },
    {
        "loss": 2.5323,
        "grad_norm": 2.1115951538085938,
        "learning_rate": 2.0625603909830704e-07,
        "epoch": 0.661159300564505,
        "step": 4802
    },
    {
        "loss": 1.101,
        "grad_norm": 2.304966449737549,
        "learning_rate": 1.9798178731851303e-07,
        "epoch": 0.6612969847170591,
        "step": 4803
    },
    {
        "loss": 2.3554,
        "grad_norm": 1.0925796031951904,
        "learning_rate": 1.898767526822498e-07,
        "epoch": 0.6614346688696131,
        "step": 4804
    },
    {
        "loss": 2.1235,
        "grad_norm": 2.019016981124878,
        "learning_rate": 1.8194094893183577e-07,
        "epoch": 0.6615723530221671,
        "step": 4805
    },
    {
        "loss": 1.4852,
        "grad_norm": 2.009725570678711,
        "learning_rate": 1.741743895226411e-07,
        "epoch": 0.6617100371747212,
        "step": 4806
    },
    {
        "loss": 1.8178,
        "grad_norm": 1.4159739017486572,
        "learning_rate": 1.6657708762309877e-07,
        "epoch": 0.6618477213272752,
        "step": 4807
    },
    {
        "loss": 1.6047,
        "grad_norm": 1.8517346382141113,
        "learning_rate": 1.5914905611463803e-07,
        "epoch": 0.6619854054798293,
        "step": 4808
    },
    {
        "loss": 1.9561,
        "grad_norm": 2.485194683074951,
        "learning_rate": 1.5189030759170664e-07,
        "epoch": 0.6621230896323833,
        "step": 4809
    },
    {
        "loss": 1.8849,
        "grad_norm": 2.0278944969177246,
        "learning_rate": 1.4480085436170409e-07,
        "epoch": 0.6622607737849373,
        "step": 4810
    },
    {
        "loss": 1.223,
        "grad_norm": 2.4805266857147217,
        "learning_rate": 1.378807084450151e-07,
        "epoch": 0.6623984579374914,
        "step": 4811
    },
    {
        "loss": 1.5476,
        "grad_norm": 2.0260212421417236,
        "learning_rate": 1.3112988157493177e-07,
        "epoch": 0.6625361420900454,
        "step": 4812
    },
    {
        "loss": 2.6196,
        "grad_norm": 2.1303870677948,
        "learning_rate": 1.2454838519767587e-07,
        "epoch": 0.6626738262425995,
        "step": 4813
    },
    {
        "loss": 2.2654,
        "grad_norm": 2.299773693084717,
        "learning_rate": 1.1813623047236544e-07,
        "epoch": 0.6628115103951535,
        "step": 4814
    },
    {
        "loss": 2.7413,
        "grad_norm": 1.6377609968185425,
        "learning_rate": 1.1189342827098159e-07,
        "epoch": 0.6629491945477075,
        "step": 4815
    },
    {
        "loss": 2.1932,
        "grad_norm": 1.6610616445541382,
        "learning_rate": 1.0581998917836844e-07,
        "epoch": 0.6630868787002616,
        "step": 4816
    },
    {
        "loss": 2.5095,
        "grad_norm": 1.2173811197280884,
        "learning_rate": 9.991592349223311e-08,
        "epoch": 0.6632245628528156,
        "step": 4817
    },
    {
        "loss": 2.2196,
        "grad_norm": 1.3945099115371704,
        "learning_rate": 9.418124122306803e-08,
        "epoch": 0.6633622470053697,
        "step": 4818
    },
    {
        "loss": 2.3334,
        "grad_norm": 2.4318442344665527,
        "learning_rate": 8.861595209419537e-08,
        "epoch": 0.6634999311579237,
        "step": 4819
    },
    {
        "loss": 1.9088,
        "grad_norm": 2.455570936203003,
        "learning_rate": 8.322006554171146e-08,
        "epoch": 0.6636376153104777,
        "step": 4820
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.3809878826141357,
        "learning_rate": 7.799359071450907e-08,
        "epoch": 0.6637752994630318,
        "step": 4821
    },
    {
        "loss": 2.2602,
        "grad_norm": 1.948536992073059,
        "learning_rate": 7.293653647421073e-08,
        "epoch": 0.6639129836155858,
        "step": 4822
    },
    {
        "loss": 0.878,
        "grad_norm": 2.6670854091644287,
        "learning_rate": 6.804891139521318e-08,
        "epoch": 0.6640506677681399,
        "step": 4823
    },
    {
        "loss": 1.8121,
        "grad_norm": 1.6558467149734497,
        "learning_rate": 6.333072376459859e-08,
        "epoch": 0.6641883519206939,
        "step": 4824
    },
    {
        "loss": 1.3581,
        "grad_norm": 1.8910349607467651,
        "learning_rate": 5.878198158220105e-08,
        "epoch": 0.6643260360732479,
        "step": 4825
    },
    {
        "loss": 2.2729,
        "grad_norm": 1.2385178804397583,
        "learning_rate": 5.440269256055119e-08,
        "epoch": 0.664463720225802,
        "step": 4826
    },
    {
        "loss": 0.4694,
        "grad_norm": 1.9980823993682861,
        "learning_rate": 5.019286412484281e-08,
        "epoch": 0.664601404378356,
        "step": 4827
    },
    {
        "loss": 2.4494,
        "grad_norm": 1.9958178997039795,
        "learning_rate": 4.615250341295507e-08,
        "epoch": 0.6647390885309101,
        "step": 4828
    },
    {
        "loss": 2.2253,
        "grad_norm": 2.2851922512054443,
        "learning_rate": 4.2281617275441444e-08,
        "epoch": 0.6648767726834641,
        "step": 4829
    },
    {
        "loss": 2.2764,
        "grad_norm": 1.3970271348953247,
        "learning_rate": 3.858021227550745e-08,
        "epoch": 0.6650144568360181,
        "step": 4830
    },
    {
        "loss": 1.8578,
        "grad_norm": 2.093965530395508,
        "learning_rate": 3.504829468897741e-08,
        "epoch": 0.6651521409885722,
        "step": 4831
    },
    {
        "loss": 1.478,
        "grad_norm": 1.5278325080871582,
        "learning_rate": 3.1685870504316596e-08,
        "epoch": 0.6652898251411262,
        "step": 4832
    },
    {
        "loss": 1.4645,
        "grad_norm": 1.9545924663543701,
        "learning_rate": 2.8492945422620155e-08,
        "epoch": 0.6654275092936803,
        "step": 4833
    },
    {
        "loss": 1.4343,
        "grad_norm": 2.2005374431610107,
        "learning_rate": 2.5469524857579807e-08,
        "epoch": 0.6655651934462343,
        "step": 4834
    },
    {
        "loss": 1.2802,
        "grad_norm": 2.587083578109741,
        "learning_rate": 2.2615613935483838e-08,
        "epoch": 0.6657028775987884,
        "step": 4835
    },
    {
        "loss": 1.9162,
        "grad_norm": 3.1687216758728027,
        "learning_rate": 1.9931217495228193e-08,
        "epoch": 0.6658405617513424,
        "step": 4836
    },
    {
        "loss": 2.2687,
        "grad_norm": 1.6699090003967285,
        "learning_rate": 1.7416340088272087e-08,
        "epoch": 0.6659782459038964,
        "step": 4837
    },
    {
        "loss": 1.8855,
        "grad_norm": 2.344465732574463,
        "learning_rate": 1.5070985978671292e-08,
        "epoch": 0.6661159300564505,
        "step": 4838
    },
    {
        "loss": 2.3594,
        "grad_norm": 2.0424017906188965,
        "learning_rate": 1.2895159143033742e-08,
        "epoch": 0.6662536142090045,
        "step": 4839
    },
    {
        "loss": 2.1302,
        "grad_norm": 2.8382318019866943,
        "learning_rate": 1.0888863270541728e-08,
        "epoch": 0.6663912983615586,
        "step": 4840
    },
    {
        "loss": 2.1543,
        "grad_norm": 1.7733683586120605,
        "learning_rate": 9.052101762918597e-09,
        "epoch": 0.6665289825141126,
        "step": 4841
    },
    {
        "loss": 1.6479,
        "grad_norm": 2.6378254890441895,
        "learning_rate": 7.384877734450957e-09,
        "epoch": 0.6666666666666666,
        "step": 4842
    },
    {
        "loss": 1.5748,
        "grad_norm": 1.848215103149414,
        "learning_rate": 5.887194011966468e-09,
        "epoch": 0.6668043508192207,
        "step": 4843
    },
    {
        "loss": 1.5524,
        "grad_norm": 2.9363138675689697,
        "learning_rate": 4.559053134822744e-09,
        "epoch": 0.6669420349717747,
        "step": 4844
    },
    {
        "loss": 2.5746,
        "grad_norm": 1.9280790090560913,
        "learning_rate": 3.400457354929554e-09,
        "epoch": 0.6670797191243288,
        "step": 4845
    },
    {
        "loss": 1.524,
        "grad_norm": 1.0495840311050415,
        "learning_rate": 2.4114086367044154e-09,
        "epoch": 0.6672174032768828,
        "step": 4846
    },
    {
        "loss": 2.1524,
        "grad_norm": 1.4000893831253052,
        "learning_rate": 1.5919086571170028e-09,
        "epoch": 0.6673550874294368,
        "step": 4847
    },
    {
        "loss": 1.6026,
        "grad_norm": 1.7758492231369019,
        "learning_rate": 9.419588056558404e-10,
        "epoch": 0.6674927715819909,
        "step": 4848
    },
    {
        "loss": 1.28,
        "grad_norm": 3.843421697616577,
        "learning_rate": 4.615601843283024e-10,
        "epoch": 0.6676304557345449,
        "step": 4849
    },
    {
        "loss": 1.6055,
        "grad_norm": 2.765040397644043,
        "learning_rate": 1.5071360766061305e-10,
        "epoch": 0.667768139887099,
        "step": 4850
    },
    {
        "loss": 1.8465,
        "grad_norm": 1.098371982574463,
        "learning_rate": 9.419602697846585e-12,
        "epoch": 0.667905824039653,
        "step": 4851
    },
    {
        "loss": 2.0471,
        "grad_norm": 2.5549798011779785,
        "learning_rate": 0.000199999962321591,
        "epoch": 0.668043508192207,
        "step": 4852
    },
    {
        "loss": 1.8443,
        "grad_norm": 2.0322041511535645,
        "learning_rate": 0.0001999997645100213,
        "epoch": 0.6681811923447611,
        "step": 4853
    },
    {
        "loss": 2.0717,
        "grad_norm": 1.502540111541748,
        "learning_rate": 0.00019999939714602366,
        "epoch": 0.6683188764973151,
        "step": 4854
    },
    {
        "loss": 1.9226,
        "grad_norm": 1.5289784669876099,
        "learning_rate": 0.0001999988602302209,
        "epoch": 0.6684565606498692,
        "step": 4855
    },
    {
        "loss": 2.5996,
        "grad_norm": 1.5636110305786133,
        "learning_rate": 0.00019999815376352346,
        "epoch": 0.6685942448024232,
        "step": 4856
    },
    {
        "loss": 1.9125,
        "grad_norm": 1.9723930358886719,
        "learning_rate": 0.00019999727774712905,
        "epoch": 0.6687319289549772,
        "step": 4857
    },
    {
        "loss": 1.8574,
        "grad_norm": 1.6726235151290894,
        "learning_rate": 0.0001999962321825231,
        "epoch": 0.6688696131075313,
        "step": 4858
    },
    {
        "loss": 2.3937,
        "grad_norm": 1.8804618120193481,
        "learning_rate": 0.00019999501707147833,
        "epoch": 0.6690072972600853,
        "step": 4859
    },
    {
        "loss": 2.3913,
        "grad_norm": 1.9621028900146484,
        "learning_rate": 0.000199993632416055,
        "epoch": 0.6691449814126395,
        "step": 4860
    },
    {
        "loss": 1.9303,
        "grad_norm": 1.7463557720184326,
        "learning_rate": 0.00019999207821860083,
        "epoch": 0.6692826655651934,
        "step": 4861
    },
    {
        "loss": 2.0081,
        "grad_norm": 2.3360185623168945,
        "learning_rate": 0.00019999035448175101,
        "epoch": 0.6694203497177474,
        "step": 4862
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.71932852268219,
        "learning_rate": 0.00019998846120842823,
        "epoch": 0.6695580338703015,
        "step": 4863
    },
    {
        "loss": 1.8059,
        "grad_norm": 2.6710424423217773,
        "learning_rate": 0.0001999863984018425,
        "epoch": 0.6696957180228555,
        "step": 4864
    },
    {
        "loss": 1.1694,
        "grad_norm": 2.5283639430999756,
        "learning_rate": 0.00019998416606549144,
        "epoch": 0.6698334021754097,
        "step": 4865
    },
    {
        "loss": 1.2341,
        "grad_norm": 3.1127986907958984,
        "learning_rate": 0.00019998176420316002,
        "epoch": 0.6699710863279636,
        "step": 4866
    },
    {
        "loss": 2.5664,
        "grad_norm": 1.0633113384246826,
        "learning_rate": 0.00019997919281892067,
        "epoch": 0.6701087704805176,
        "step": 4867
    },
    {
        "loss": 2.0728,
        "grad_norm": 1.877479076385498,
        "learning_rate": 0.0001999764519171332,
        "epoch": 0.6702464546330718,
        "step": 4868
    },
    {
        "loss": 0.996,
        "grad_norm": 2.3753609657287598,
        "learning_rate": 0.00019997354150244493,
        "epoch": 0.6703841387856257,
        "step": 4869
    },
    {
        "loss": 0.9054,
        "grad_norm": 2.052438735961914,
        "learning_rate": 0.00019997046157979058,
        "epoch": 0.6705218229381799,
        "step": 4870
    },
    {
        "loss": 1.879,
        "grad_norm": 2.6997621059417725,
        "learning_rate": 0.00019996721215439218,
        "epoch": 0.6706595070907339,
        "step": 4871
    },
    {
        "loss": 2.1085,
        "grad_norm": 2.4274840354919434,
        "learning_rate": 0.00019996379323175924,
        "epoch": 0.6707971912432878,
        "step": 4872
    },
    {
        "loss": 1.0771,
        "grad_norm": 3.6062378883361816,
        "learning_rate": 0.0001999602048176887,
        "epoch": 0.670934875395842,
        "step": 4873
    },
    {
        "loss": 1.9842,
        "grad_norm": 2.1177918910980225,
        "learning_rate": 0.0001999564469182647,
        "epoch": 0.671072559548396,
        "step": 4874
    },
    {
        "loss": 2.3055,
        "grad_norm": 1.732854962348938,
        "learning_rate": 0.00019995251953985896,
        "epoch": 0.6712102437009501,
        "step": 4875
    },
    {
        "loss": 1.2656,
        "grad_norm": 2.3510167598724365,
        "learning_rate": 0.00019994842268913045,
        "epoch": 0.6713479278535041,
        "step": 4876
    },
    {
        "loss": 1.9786,
        "grad_norm": 1.8941643238067627,
        "learning_rate": 0.00019994415637302547,
        "epoch": 0.671485612006058,
        "step": 4877
    },
    {
        "loss": 2.098,
        "grad_norm": 1.7540550231933594,
        "learning_rate": 0.00019993972059877767,
        "epoch": 0.6716232961586122,
        "step": 4878
    },
    {
        "loss": 1.8011,
        "grad_norm": 2.038581609725952,
        "learning_rate": 0.0001999351153739081,
        "epoch": 0.6717609803111662,
        "step": 4879
    },
    {
        "loss": 1.8287,
        "grad_norm": 1.869220495223999,
        "learning_rate": 0.00019993034070622497,
        "epoch": 0.6718986644637203,
        "step": 4880
    },
    {
        "loss": 2.5804,
        "grad_norm": 2.5918729305267334,
        "learning_rate": 0.0001999253966038239,
        "epoch": 0.6720363486162743,
        "step": 4881
    },
    {
        "loss": 2.3993,
        "grad_norm": 1.4238777160644531,
        "learning_rate": 0.00019992028307508773,
        "epoch": 0.6721740327688283,
        "step": 4882
    },
    {
        "loss": 2.2179,
        "grad_norm": 1.7641979455947876,
        "learning_rate": 0.0001999150001286866,
        "epoch": 0.6723117169213824,
        "step": 4883
    },
    {
        "loss": 2.0479,
        "grad_norm": 1.8380118608474731,
        "learning_rate": 0.00019990954777357795,
        "epoch": 0.6724494010739364,
        "step": 4884
    },
    {
        "loss": 2.4565,
        "grad_norm": 2.5672237873077393,
        "learning_rate": 0.00019990392601900635,
        "epoch": 0.6725870852264905,
        "step": 4885
    },
    {
        "loss": 2.1551,
        "grad_norm": 2.2112979888916016,
        "learning_rate": 0.00019989813487450363,
        "epoch": 0.6727247693790445,
        "step": 4886
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.1961359977722168,
        "learning_rate": 0.00019989217434988885,
        "epoch": 0.6728624535315985,
        "step": 4887
    },
    {
        "loss": 2.2016,
        "grad_norm": 1.8471471071243286,
        "learning_rate": 0.00019988604445526827,
        "epoch": 0.6730001376841526,
        "step": 4888
    },
    {
        "loss": 1.8429,
        "grad_norm": 2.8190958499908447,
        "learning_rate": 0.00019987974520103535,
        "epoch": 0.6731378218367066,
        "step": 4889
    },
    {
        "loss": 1.721,
        "grad_norm": 1.908125877380371,
        "learning_rate": 0.00019987327659787052,
        "epoch": 0.6732755059892607,
        "step": 4890
    },
    {
        "loss": 1.1215,
        "grad_norm": 4.5156941413879395,
        "learning_rate": 0.00019986663865674157,
        "epoch": 0.6734131901418147,
        "step": 4891
    },
    {
        "loss": 2.0978,
        "grad_norm": 2.635511875152588,
        "learning_rate": 0.00019985983138890332,
        "epoch": 0.6735508742943688,
        "step": 4892
    },
    {
        "loss": 2.1414,
        "grad_norm": 1.8740814924240112,
        "learning_rate": 0.00019985285480589763,
        "epoch": 0.6736885584469228,
        "step": 4893
    },
    {
        "loss": 2.8932,
        "grad_norm": 1.5273573398590088,
        "learning_rate": 0.00019984570891955358,
        "epoch": 0.6738262425994768,
        "step": 4894
    },
    {
        "loss": 1.3383,
        "grad_norm": 3.6877217292785645,
        "learning_rate": 0.00019983839374198713,
        "epoch": 0.6739639267520309,
        "step": 4895
    },
    {
        "loss": 1.9535,
        "grad_norm": 2.176872491836548,
        "learning_rate": 0.00019983090928560142,
        "epoch": 0.6741016109045849,
        "step": 4896
    },
    {
        "loss": 2.8145,
        "grad_norm": 1.4455463886260986,
        "learning_rate": 0.00019982325556308658,
        "epoch": 0.674239295057139,
        "step": 4897
    },
    {
        "loss": 1.9795,
        "grad_norm": 2.0121383666992188,
        "learning_rate": 0.0001998154325874196,
        "epoch": 0.674376979209693,
        "step": 4898
    },
    {
        "loss": 2.2483,
        "grad_norm": 1.7616627216339111,
        "learning_rate": 0.00019980744037186469,
        "epoch": 0.674514663362247,
        "step": 4899
    },
    {
        "loss": 2.0859,
        "grad_norm": 1.2362446784973145,
        "learning_rate": 0.00019979927892997283,
        "epoch": 0.6746523475148011,
        "step": 4900
    },
    {
        "loss": 1.8833,
        "grad_norm": 1.5121850967407227,
        "learning_rate": 0.00019979094827558198,
        "epoch": 0.6747900316673551,
        "step": 4901
    },
    {
        "loss": 1.3205,
        "grad_norm": 2.104560613632202,
        "learning_rate": 0.00019978244842281696,
        "epoch": 0.6749277158199092,
        "step": 4902
    },
    {
        "loss": 2.2419,
        "grad_norm": 2.099139451980591,
        "learning_rate": 0.00019977377938608956,
        "epoch": 0.6750653999724632,
        "step": 4903
    },
    {
        "loss": 2.1565,
        "grad_norm": 1.7280021905899048,
        "learning_rate": 0.00019976494118009836,
        "epoch": 0.6752030841250172,
        "step": 4904
    },
    {
        "loss": 2.188,
        "grad_norm": 1.526064395904541,
        "learning_rate": 0.00019975593381982878,
        "epoch": 0.6753407682775713,
        "step": 4905
    },
    {
        "loss": 1.9031,
        "grad_norm": 1.143996000289917,
        "learning_rate": 0.00019974675732055305,
        "epoch": 0.6754784524301253,
        "step": 4906
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.2341833114624023,
        "learning_rate": 0.0001997374116978302,
        "epoch": 0.6756161365826794,
        "step": 4907
    },
    {
        "loss": 1.8605,
        "grad_norm": 1.9857193231582642,
        "learning_rate": 0.000199727896967506,
        "epoch": 0.6757538207352334,
        "step": 4908
    },
    {
        "loss": 1.9116,
        "grad_norm": 1.6243634223937988,
        "learning_rate": 0.00019971821314571295,
        "epoch": 0.6758915048877874,
        "step": 4909
    },
    {
        "loss": 1.4872,
        "grad_norm": 2.529909610748291,
        "learning_rate": 0.0001997083602488702,
        "epoch": 0.6760291890403415,
        "step": 4910
    },
    {
        "loss": 1.7097,
        "grad_norm": 1.616527795791626,
        "learning_rate": 0.00019969833829368367,
        "epoch": 0.6761668731928955,
        "step": 4911
    },
    {
        "loss": 2.065,
        "grad_norm": 2.689579486846924,
        "learning_rate": 0.0001996881472971458,
        "epoch": 0.6763045573454496,
        "step": 4912
    },
    {
        "loss": 1.8714,
        "grad_norm": 2.2307322025299072,
        "learning_rate": 0.0001996777872765358,
        "epoch": 0.6764422414980036,
        "step": 4913
    },
    {
        "loss": 2.7372,
        "grad_norm": 1.532064437866211,
        "learning_rate": 0.00019966725824941932,
        "epoch": 0.6765799256505576,
        "step": 4914
    },
    {
        "loss": 2.217,
        "grad_norm": 1.624536156654358,
        "learning_rate": 0.00019965656023364866,
        "epoch": 0.6767176098031117,
        "step": 4915
    },
    {
        "loss": 1.7183,
        "grad_norm": 2.677884817123413,
        "learning_rate": 0.00019964569324736254,
        "epoch": 0.6768552939556657,
        "step": 4916
    },
    {
        "loss": 1.9128,
        "grad_norm": 2.0724806785583496,
        "learning_rate": 0.0001996346573089863,
        "epoch": 0.6769929781082198,
        "step": 4917
    },
    {
        "loss": 1.4996,
        "grad_norm": 3.333881378173828,
        "learning_rate": 0.0001996234524372317,
        "epoch": 0.6771306622607738,
        "step": 4918
    },
    {
        "loss": 1.879,
        "grad_norm": 2.2309560775756836,
        "learning_rate": 0.0001996120786510969,
        "epoch": 0.6772683464133278,
        "step": 4919
    },
    {
        "loss": 1.8637,
        "grad_norm": 2.1239612102508545,
        "learning_rate": 0.00019960053596986642,
        "epoch": 0.6774060305658819,
        "step": 4920
    },
    {
        "loss": 2.2055,
        "grad_norm": 1.7501764297485352,
        "learning_rate": 0.00019958882441311126,
        "epoch": 0.6775437147184359,
        "step": 4921
    },
    {
        "loss": 2.3448,
        "grad_norm": 2.071244478225708,
        "learning_rate": 0.0001995769440006887,
        "epoch": 0.67768139887099,
        "step": 4922
    },
    {
        "loss": 1.8925,
        "grad_norm": 3.113314390182495,
        "learning_rate": 0.0001995648947527423,
        "epoch": 0.677819083023544,
        "step": 4923
    },
    {
        "loss": 1.7315,
        "grad_norm": 2.6312663555145264,
        "learning_rate": 0.0001995526766897019,
        "epoch": 0.677956767176098,
        "step": 4924
    },
    {
        "loss": 2.3769,
        "grad_norm": 1.7479232549667358,
        "learning_rate": 0.00019954028983228356,
        "epoch": 0.6780944513286521,
        "step": 4925
    },
    {
        "loss": 2.1262,
        "grad_norm": 2.2554609775543213,
        "learning_rate": 0.00019952773420148955,
        "epoch": 0.6782321354812061,
        "step": 4926
    },
    {
        "loss": 2.5647,
        "grad_norm": 1.8787559270858765,
        "learning_rate": 0.00019951500981860835,
        "epoch": 0.6783698196337602,
        "step": 4927
    },
    {
        "loss": 2.2511,
        "grad_norm": 2.5343332290649414,
        "learning_rate": 0.00019950211670521442,
        "epoch": 0.6785075037863142,
        "step": 4928
    },
    {
        "loss": 2.4779,
        "grad_norm": 1.6299328804016113,
        "learning_rate": 0.00019948905488316844,
        "epoch": 0.6786451879388682,
        "step": 4929
    },
    {
        "loss": 1.8455,
        "grad_norm": 2.043957471847534,
        "learning_rate": 0.00019947582437461714,
        "epoch": 0.6787828720914223,
        "step": 4930
    },
    {
        "loss": 1.973,
        "grad_norm": 1.8059495687484741,
        "learning_rate": 0.00019946242520199313,
        "epoch": 0.6789205562439763,
        "step": 4931
    },
    {
        "loss": 1.3515,
        "grad_norm": 3.0551488399505615,
        "learning_rate": 0.00019944885738801518,
        "epoch": 0.6790582403965304,
        "step": 4932
    },
    {
        "loss": 1.7443,
        "grad_norm": Infinity,
        "learning_rate": 0.00019944885738801518,
        "epoch": 0.6791959245490844,
        "step": 4933
    },
    {
        "loss": 1.7414,
        "grad_norm": 2.63249135017395,
        "learning_rate": 0.00019943512095568785,
        "epoch": 0.6793336087016384,
        "step": 4934
    },
    {
        "loss": 2.3286,
        "grad_norm": 2.0271129608154297,
        "learning_rate": 0.0001994212159283017,
        "epoch": 0.6794712928541925,
        "step": 4935
    },
    {
        "loss": 1.6361,
        "grad_norm": 1.7295188903808594,
        "learning_rate": 0.00019940714232943299,
        "epoch": 0.6796089770067465,
        "step": 4936
    },
    {
        "loss": 1.9226,
        "grad_norm": 2.1268064975738525,
        "learning_rate": 0.000199392900182944,
        "epoch": 0.6797466611593006,
        "step": 4937
    },
    {
        "loss": 1.9691,
        "grad_norm": 2.226113796234131,
        "learning_rate": 0.00019937848951298267,
        "epoch": 0.6798843453118546,
        "step": 4938
    },
    {
        "loss": 2.0151,
        "grad_norm": 1.721388578414917,
        "learning_rate": 0.0001993639103439827,
        "epoch": 0.6800220294644086,
        "step": 4939
    },
    {
        "loss": 2.1376,
        "grad_norm": 1.969407320022583,
        "learning_rate": 0.0001993491627006635,
        "epoch": 0.6801597136169627,
        "step": 4940
    },
    {
        "loss": 2.3894,
        "grad_norm": 1.5080957412719727,
        "learning_rate": 0.00019933424660803007,
        "epoch": 0.6802973977695167,
        "step": 4941
    },
    {
        "loss": 2.0801,
        "grad_norm": 4.648683071136475,
        "learning_rate": 0.00019931916209137306,
        "epoch": 0.6804350819220708,
        "step": 4942
    },
    {
        "loss": 1.4201,
        "grad_norm": 2.22337007522583,
        "learning_rate": 0.0001993039091762688,
        "epoch": 0.6805727660746248,
        "step": 4943
    },
    {
        "loss": 2.5326,
        "grad_norm": 1.6342928409576416,
        "learning_rate": 0.00019928848788857887,
        "epoch": 0.6807104502271788,
        "step": 4944
    },
    {
        "loss": 2.2859,
        "grad_norm": 2.3331339359283447,
        "learning_rate": 0.00019927289825445065,
        "epoch": 0.6808481343797329,
        "step": 4945
    },
    {
        "loss": 2.0838,
        "grad_norm": 2.373378038406372,
        "learning_rate": 0.00019925714030031674,
        "epoch": 0.6809858185322869,
        "step": 4946
    },
    {
        "loss": 1.4934,
        "grad_norm": 1.607679009437561,
        "learning_rate": 0.00019924121405289519,
        "epoch": 0.681123502684841,
        "step": 4947
    },
    {
        "loss": 2.1452,
        "grad_norm": 2.383728504180908,
        "learning_rate": 0.00019922511953918942,
        "epoch": 0.681261186837395,
        "step": 4948
    },
    {
        "loss": 1.6399,
        "grad_norm": 3.272311210632324,
        "learning_rate": 0.00019920885678648817,
        "epoch": 0.681398870989949,
        "step": 4949
    },
    {
        "loss": 1.8614,
        "grad_norm": 2.4573986530303955,
        "learning_rate": 0.00019919242582236532,
        "epoch": 0.6815365551425031,
        "step": 4950
    },
    {
        "loss": 2.136,
        "grad_norm": 1.8489998579025269,
        "learning_rate": 0.00019917582667468007,
        "epoch": 0.6816742392950571,
        "step": 4951
    },
    {
        "loss": 2.2126,
        "grad_norm": 2.341280221939087,
        "learning_rate": 0.00019915905937157677,
        "epoch": 0.6818119234476112,
        "step": 4952
    },
    {
        "loss": 1.9379,
        "grad_norm": 1.9141961336135864,
        "learning_rate": 0.00019914212394148484,
        "epoch": 0.6819496076001652,
        "step": 4953
    },
    {
        "loss": 2.1585,
        "grad_norm": 1.867492437362671,
        "learning_rate": 0.00019912502041311873,
        "epoch": 0.6820872917527193,
        "step": 4954
    },
    {
        "loss": 2.4677,
        "grad_norm": 1.465903639793396,
        "learning_rate": 0.000199107748815478,
        "epoch": 0.6822249759052733,
        "step": 4955
    },
    {
        "loss": 2.3219,
        "grad_norm": 1.5111113786697388,
        "learning_rate": 0.0001990903091778472,
        "epoch": 0.6823626600578273,
        "step": 4956
    },
    {
        "loss": 1.4982,
        "grad_norm": 2.2086403369903564,
        "learning_rate": 0.00019907270152979558,
        "epoch": 0.6825003442103814,
        "step": 4957
    },
    {
        "loss": 1.5366,
        "grad_norm": 2.7359135150909424,
        "learning_rate": 0.0001990549259011775,
        "epoch": 0.6826380283629354,
        "step": 4958
    },
    {
        "loss": 2.0029,
        "grad_norm": 2.1822662353515625,
        "learning_rate": 0.00019903698232213207,
        "epoch": 0.6827757125154895,
        "step": 4959
    },
    {
        "loss": 2.2162,
        "grad_norm": 1.5782077312469482,
        "learning_rate": 0.00019901887082308305,
        "epoch": 0.6829133966680435,
        "step": 4960
    },
    {
        "loss": 2.1995,
        "grad_norm": 1.7885441780090332,
        "learning_rate": 0.00019900059143473903,
        "epoch": 0.6830510808205975,
        "step": 4961
    },
    {
        "loss": 1.3325,
        "grad_norm": 2.7654879093170166,
        "learning_rate": 0.0001989821441880933,
        "epoch": 0.6831887649731516,
        "step": 4962
    },
    {
        "loss": 2.0648,
        "grad_norm": 1.5614607334136963,
        "learning_rate": 0.00019896352911442358,
        "epoch": 0.6833264491257056,
        "step": 4963
    },
    {
        "loss": 1.9358,
        "grad_norm": 2.0680856704711914,
        "learning_rate": 0.00019894474624529228,
        "epoch": 0.6834641332782597,
        "step": 4964
    },
    {
        "loss": 2.305,
        "grad_norm": 2.561199903488159,
        "learning_rate": 0.00019892579561254636,
        "epoch": 0.6836018174308137,
        "step": 4965
    },
    {
        "loss": 1.6815,
        "grad_norm": 1.596044898033142,
        "learning_rate": 0.00019890667724831707,
        "epoch": 0.6837395015833677,
        "step": 4966
    },
    {
        "loss": 2.078,
        "grad_norm": 1.412976622581482,
        "learning_rate": 0.0001988873911850202,
        "epoch": 0.6838771857359218,
        "step": 4967
    },
    {
        "loss": 2.5553,
        "grad_norm": 2.4105618000030518,
        "learning_rate": 0.00019886793745535575,
        "epoch": 0.6840148698884758,
        "step": 4968
    },
    {
        "loss": 1.9237,
        "grad_norm": 2.79018497467041,
        "learning_rate": 0.00019884831609230812,
        "epoch": 0.6841525540410299,
        "step": 4969
    },
    {
        "loss": 1.7446,
        "grad_norm": 2.15921688079834,
        "learning_rate": 0.00019882852712914585,
        "epoch": 0.6842902381935839,
        "step": 4970
    },
    {
        "loss": 2.1121,
        "grad_norm": 2.8850579261779785,
        "learning_rate": 0.00019880857059942174,
        "epoch": 0.6844279223461379,
        "step": 4971
    },
    {
        "loss": 0.8673,
        "grad_norm": 2.3781256675720215,
        "learning_rate": 0.00019878844653697258,
        "epoch": 0.684565606498692,
        "step": 4972
    },
    {
        "loss": 1.8747,
        "grad_norm": 2.760631799697876,
        "learning_rate": 0.00019876815497591932,
        "epoch": 0.684703290651246,
        "step": 4973
    },
    {
        "loss": 2.2276,
        "grad_norm": 1.516941785812378,
        "learning_rate": 0.0001987476959506669,
        "epoch": 0.6848409748038001,
        "step": 4974
    },
    {
        "loss": 2.0556,
        "grad_norm": 2.669621706008911,
        "learning_rate": 0.00019872706949590415,
        "epoch": 0.6849786589563541,
        "step": 4975
    },
    {
        "loss": 1.8647,
        "grad_norm": 2.0398566722869873,
        "learning_rate": 0.0001987062756466038,
        "epoch": 0.6851163431089081,
        "step": 4976
    },
    {
        "loss": 2.9031,
        "grad_norm": 1.4711600542068481,
        "learning_rate": 0.00019868531443802243,
        "epoch": 0.6852540272614622,
        "step": 4977
    },
    {
        "loss": 2.6479,
        "grad_norm": 3.003993034362793,
        "learning_rate": 0.00019866418590570032,
        "epoch": 0.6853917114140162,
        "step": 4978
    },
    {
        "loss": 1.0952,
        "grad_norm": 3.2941224575042725,
        "learning_rate": 0.00019864289008546154,
        "epoch": 0.6855293955665703,
        "step": 4979
    },
    {
        "loss": 2.0799,
        "grad_norm": 1.1263794898986816,
        "learning_rate": 0.00019862142701341379,
        "epoch": 0.6856670797191243,
        "step": 4980
    },
    {
        "loss": 1.8355,
        "grad_norm": 2.170200824737549,
        "learning_rate": 0.00019859979672594817,
        "epoch": 0.6858047638716783,
        "step": 4981
    },
    {
        "loss": 1.897,
        "grad_norm": 1.756867527961731,
        "learning_rate": 0.00019857799925973962,
        "epoch": 0.6859424480242324,
        "step": 4982
    },
    {
        "loss": 1.7724,
        "grad_norm": 1.2387689352035522,
        "learning_rate": 0.00019855603465174625,
        "epoch": 0.6860801321767864,
        "step": 4983
    },
    {
        "loss": 0.7145,
        "grad_norm": 2.875605583190918,
        "learning_rate": 0.0001985339029392097,
        "epoch": 0.6862178163293405,
        "step": 4984
    },
    {
        "loss": 1.5718,
        "grad_norm": 3.2506744861602783,
        "learning_rate": 0.0001985116041596549,
        "epoch": 0.6863555004818945,
        "step": 4985
    },
    {
        "loss": 1.7141,
        "grad_norm": 1.9226531982421875,
        "learning_rate": 0.00019848913835089014,
        "epoch": 0.6864931846344485,
        "step": 4986
    },
    {
        "loss": 2.6612,
        "grad_norm": 1.8914731740951538,
        "learning_rate": 0.00019846650555100672,
        "epoch": 0.6866308687870026,
        "step": 4987
    },
    {
        "loss": 1.3259,
        "grad_norm": 2.606123685836792,
        "learning_rate": 0.00019844370579837927,
        "epoch": 0.6867685529395566,
        "step": 4988
    },
    {
        "loss": 2.3517,
        "grad_norm": 2.373715400695801,
        "learning_rate": 0.0001984207391316654,
        "epoch": 0.6869062370921107,
        "step": 4989
    },
    {
        "loss": 1.7698,
        "grad_norm": 1.8499729633331299,
        "learning_rate": 0.0001983976055898057,
        "epoch": 0.6870439212446647,
        "step": 4990
    },
    {
        "loss": 2.0293,
        "grad_norm": 3.5150530338287354,
        "learning_rate": 0.0001983743052120238,
        "epoch": 0.6871816053972187,
        "step": 4991
    },
    {
        "loss": 2.1304,
        "grad_norm": 2.806128740310669,
        "learning_rate": 0.00019835083803782614,
        "epoch": 0.6873192895497728,
        "step": 4992
    },
    {
        "loss": 1.5731,
        "grad_norm": 2.0235443115234375,
        "learning_rate": 0.00019832720410700193,
        "epoch": 0.6874569737023268,
        "step": 4993
    },
    {
        "loss": 1.8229,
        "grad_norm": 3.0039925575256348,
        "learning_rate": 0.00019830340345962323,
        "epoch": 0.6875946578548809,
        "step": 4994
    },
    {
        "loss": 2.4496,
        "grad_norm": 1.417353868484497,
        "learning_rate": 0.00019827943613604467,
        "epoch": 0.6877323420074349,
        "step": 4995
    },
    {
        "loss": 2.197,
        "grad_norm": 2.490222215652466,
        "learning_rate": 0.00019825530217690353,
        "epoch": 0.6878700261599889,
        "step": 4996
    },
    {
        "loss": 2.2065,
        "grad_norm": 1.692603588104248,
        "learning_rate": 0.00019823100162311965,
        "epoch": 0.688007710312543,
        "step": 4997
    },
    {
        "loss": 2.2434,
        "grad_norm": 1.533456563949585,
        "learning_rate": 0.00019820653451589526,
        "epoch": 0.688145394465097,
        "step": 4998
    },
    {
        "loss": 0.8501,
        "grad_norm": 2.9997777938842773,
        "learning_rate": 0.00019818190089671508,
        "epoch": 0.6882830786176511,
        "step": 4999
    },
    {
        "loss": 2.3658,
        "grad_norm": 1.550857663154602,
        "learning_rate": 0.00019815710080734608,
        "epoch": 0.6884207627702051,
        "step": 5000
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.7565354108810425,
        "learning_rate": 0.0001981321342898375,
        "epoch": 0.6885584469227591,
        "step": 5001
    },
    {
        "loss": 2.4182,
        "grad_norm": 2.339992046356201,
        "learning_rate": 0.00019810700138652077,
        "epoch": 0.6886961310753132,
        "step": 5002
    },
    {
        "loss": 1.4295,
        "grad_norm": 2.9770219326019287,
        "learning_rate": 0.0001980817021400095,
        "epoch": 0.6888338152278672,
        "step": 5003
    },
    {
        "loss": 2.6141,
        "grad_norm": 1.4099652767181396,
        "learning_rate": 0.00019805623659319922,
        "epoch": 0.6889714993804213,
        "step": 5004
    },
    {
        "loss": 2.4609,
        "grad_norm": 2.724687099456787,
        "learning_rate": 0.00019803060478926752,
        "epoch": 0.6891091835329753,
        "step": 5005
    },
    {
        "loss": 2.3376,
        "grad_norm": 2.6487812995910645,
        "learning_rate": 0.00019800480677167382,
        "epoch": 0.6892468676855293,
        "step": 5006
    },
    {
        "loss": 2.4296,
        "grad_norm": 1.6955329179763794,
        "learning_rate": 0.00019797884258415942,
        "epoch": 0.6893845518380834,
        "step": 5007
    },
    {
        "loss": 2.1663,
        "grad_norm": 2.1365180015563965,
        "learning_rate": 0.00019795271227074726,
        "epoch": 0.6895222359906374,
        "step": 5008
    },
    {
        "loss": 2.3644,
        "grad_norm": 2.2270922660827637,
        "learning_rate": 0.00019792641587574212,
        "epoch": 0.6896599201431916,
        "step": 5009
    },
    {
        "loss": 1.5449,
        "grad_norm": 3.772660493850708,
        "learning_rate": 0.00019789995344373024,
        "epoch": 0.6897976042957455,
        "step": 5010
    },
    {
        "loss": 2.0266,
        "grad_norm": 2.4377448558807373,
        "learning_rate": 0.0001978733250195794,
        "epoch": 0.6899352884482997,
        "step": 5011
    },
    {
        "loss": 2.4724,
        "grad_norm": 1.3786382675170898,
        "learning_rate": 0.00019784653064843885,
        "epoch": 0.6900729726008537,
        "step": 5012
    },
    {
        "loss": 2.6775,
        "grad_norm": 2.256408452987671,
        "learning_rate": 0.00019781957037573927,
        "epoch": 0.6902106567534076,
        "step": 5013
    },
    {
        "loss": 1.6431,
        "grad_norm": 2.6618125438690186,
        "learning_rate": 0.00019779244424719252,
        "epoch": 0.6903483409059618,
        "step": 5014
    },
    {
        "loss": 2.819,
        "grad_norm": 1.6862725019454956,
        "learning_rate": 0.0001977651523087917,
        "epoch": 0.6904860250585158,
        "step": 5015
    },
    {
        "loss": 1.3743,
        "grad_norm": 3.123222589492798,
        "learning_rate": 0.00019773769460681106,
        "epoch": 0.6906237092110699,
        "step": 5016
    },
    {
        "loss": 2.3251,
        "grad_norm": 1.771797776222229,
        "learning_rate": 0.0001977100711878059,
        "epoch": 0.6907613933636239,
        "step": 5017
    },
    {
        "loss": 1.8148,
        "grad_norm": 1.3484787940979004,
        "learning_rate": 0.00019768228209861256,
        "epoch": 0.6908990775161779,
        "step": 5018
    },
    {
        "loss": 1.4369,
        "grad_norm": 2.453899621963501,
        "learning_rate": 0.00019765432738634824,
        "epoch": 0.691036761668732,
        "step": 5019
    },
    {
        "loss": 1.6489,
        "grad_norm": 2.0120017528533936,
        "learning_rate": 0.00019762620709841083,
        "epoch": 0.691174445821286,
        "step": 5020
    },
    {
        "loss": 2.3569,
        "grad_norm": 2.0071780681610107,
        "learning_rate": 0.00019759792128247922,
        "epoch": 0.6913121299738401,
        "step": 5021
    },
    {
        "loss": 1.759,
        "grad_norm": 2.307189464569092,
        "learning_rate": 0.0001975694699865127,
        "epoch": 0.6914498141263941,
        "step": 5022
    },
    {
        "loss": 2.1981,
        "grad_norm": 1.7066459655761719,
        "learning_rate": 0.00019754085325875132,
        "epoch": 0.691587498278948,
        "step": 5023
    },
    {
        "loss": 2.3091,
        "grad_norm": 3.283599615097046,
        "learning_rate": 0.00019751207114771552,
        "epoch": 0.6917251824315022,
        "step": 5024
    },
    {
        "loss": 1.7377,
        "grad_norm": 2.848209857940674,
        "learning_rate": 0.00019748312370220617,
        "epoch": 0.6918628665840562,
        "step": 5025
    },
    {
        "loss": 2.1864,
        "grad_norm": 1.935770034790039,
        "learning_rate": 0.00019745401097130441,
        "epoch": 0.6920005507366103,
        "step": 5026
    },
    {
        "loss": 1.731,
        "grad_norm": 2.51419734954834,
        "learning_rate": 0.00019742473300437188,
        "epoch": 0.6921382348891643,
        "step": 5027
    },
    {
        "loss": 2.1449,
        "grad_norm": 2.027010440826416,
        "learning_rate": 0.0001973952898510501,
        "epoch": 0.6922759190417183,
        "step": 5028
    },
    {
        "loss": 2.0364,
        "grad_norm": 1.4963972568511963,
        "learning_rate": 0.0001973656815612607,
        "epoch": 0.6924136031942724,
        "step": 5029
    },
    {
        "loss": 1.8105,
        "grad_norm": 1.8791284561157227,
        "learning_rate": 0.00019733590818520549,
        "epoch": 0.6925512873468264,
        "step": 5030
    },
    {
        "loss": 2.0637,
        "grad_norm": 2.0231735706329346,
        "learning_rate": 0.00019730596977336604,
        "epoch": 0.6926889714993805,
        "step": 5031
    },
    {
        "loss": 1.1954,
        "grad_norm": 3.0916647911071777,
        "learning_rate": 0.0001972758663765037,
        "epoch": 0.6928266556519345,
        "step": 5032
    },
    {
        "loss": 2.0762,
        "grad_norm": 1.8615986108779907,
        "learning_rate": 0.00019724559804565973,
        "epoch": 0.6929643398044885,
        "step": 5033
    },
    {
        "loss": 2.6312,
        "grad_norm": 3.5510501861572266,
        "learning_rate": 0.0001972151648321549,
        "epoch": 0.6931020239570426,
        "step": 5034
    },
    {
        "loss": 1.9777,
        "grad_norm": 1.8344980478286743,
        "learning_rate": 0.00019718456678758961,
        "epoch": 0.6932397081095966,
        "step": 5035
    },
    {
        "loss": 2.0604,
        "grad_norm": 1.7574998140335083,
        "learning_rate": 0.0001971538039638436,
        "epoch": 0.6933773922621507,
        "step": 5036
    },
    {
        "loss": 2.1439,
        "grad_norm": 1.9565112590789795,
        "learning_rate": 0.00019712287641307627,
        "epoch": 0.6935150764147047,
        "step": 5037
    },
    {
        "loss": 1.9812,
        "grad_norm": 1.4756295680999756,
        "learning_rate": 0.00019709178418772605,
        "epoch": 0.6936527605672587,
        "step": 5038
    },
    {
        "loss": 2.2694,
        "grad_norm": 1.7463195323944092,
        "learning_rate": 0.0001970605273405107,
        "epoch": 0.6937904447198128,
        "step": 5039
    },
    {
        "loss": 1.6408,
        "grad_norm": 2.7484967708587646,
        "learning_rate": 0.00019702910592442715,
        "epoch": 0.6939281288723668,
        "step": 5040
    },
    {
        "loss": 1.6767,
        "grad_norm": 2.7629857063293457,
        "learning_rate": 0.0001969975199927512,
        "epoch": 0.6940658130249209,
        "step": 5041
    },
    {
        "loss": 1.9777,
        "grad_norm": 1.6653554439544678,
        "learning_rate": 0.00019696576959903772,
        "epoch": 0.6942034971774749,
        "step": 5042
    },
    {
        "loss": 2.0655,
        "grad_norm": 3.146857738494873,
        "learning_rate": 0.00019693385479712048,
        "epoch": 0.6943411813300289,
        "step": 5043
    },
    {
        "loss": 2.345,
        "grad_norm": 1.7693603038787842,
        "learning_rate": 0.00019690177564111186,
        "epoch": 0.694478865482583,
        "step": 5044
    },
    {
        "loss": 2.1995,
        "grad_norm": 1.3100885152816772,
        "learning_rate": 0.00019686953218540294,
        "epoch": 0.694616549635137,
        "step": 5045
    },
    {
        "loss": 1.7166,
        "grad_norm": 2.445286750793457,
        "learning_rate": 0.0001968371244846635,
        "epoch": 0.6947542337876911,
        "step": 5046
    },
    {
        "loss": 2.1017,
        "grad_norm": 1.6063843965530396,
        "learning_rate": 0.00019680455259384164,
        "epoch": 0.6948919179402451,
        "step": 5047
    },
    {
        "loss": 2.1339,
        "grad_norm": 1.7417800426483154,
        "learning_rate": 0.00019677181656816401,
        "epoch": 0.6950296020927991,
        "step": 5048
    },
    {
        "loss": 2.2847,
        "grad_norm": 3.2150468826293945,
        "learning_rate": 0.00019673891646313537,
        "epoch": 0.6951672862453532,
        "step": 5049
    },
    {
        "loss": 1.3546,
        "grad_norm": 3.1865341663360596,
        "learning_rate": 0.0001967058523345388,
        "epoch": 0.6953049703979072,
        "step": 5050
    },
    {
        "loss": 2.5417,
        "grad_norm": 1.8763339519500732,
        "learning_rate": 0.00019667262423843552,
        "epoch": 0.6954426545504613,
        "step": 5051
    },
    {
        "loss": 1.7791,
        "grad_norm": 2.0681262016296387,
        "learning_rate": 0.00019663923223116464,
        "epoch": 0.6955803387030153,
        "step": 5052
    },
    {
        "loss": 1.4476,
        "grad_norm": 2.3605589866638184,
        "learning_rate": 0.0001966056763693433,
        "epoch": 0.6957180228555693,
        "step": 5053
    },
    {
        "loss": 2.1871,
        "grad_norm": 2.160013437271118,
        "learning_rate": 0.00019657195670986637,
        "epoch": 0.6958557070081234,
        "step": 5054
    },
    {
        "loss": 2.1241,
        "grad_norm": 1.7953368425369263,
        "learning_rate": 0.00019653807330990652,
        "epoch": 0.6959933911606774,
        "step": 5055
    },
    {
        "loss": 2.0615,
        "grad_norm": 1.3920261859893799,
        "learning_rate": 0.00019650402622691402,
        "epoch": 0.6961310753132315,
        "step": 5056
    },
    {
        "loss": 1.7965,
        "grad_norm": 1.660447359085083,
        "learning_rate": 0.00019646981551861662,
        "epoch": 0.6962687594657855,
        "step": 5057
    },
    {
        "loss": 2.4065,
        "grad_norm": 2.761460065841675,
        "learning_rate": 0.00019643544124301957,
        "epoch": 0.6964064436183395,
        "step": 5058
    },
    {
        "loss": 1.4754,
        "grad_norm": 2.4495701789855957,
        "learning_rate": 0.00019640090345840542,
        "epoch": 0.6965441277708936,
        "step": 5059
    },
    {
        "loss": 1.9187,
        "grad_norm": 1.7323511838912964,
        "learning_rate": 0.00019636620222333395,
        "epoch": 0.6966818119234476,
        "step": 5060
    },
    {
        "loss": 1.6581,
        "grad_norm": 2.6924960613250732,
        "learning_rate": 0.0001963313375966421,
        "epoch": 0.6968194960760017,
        "step": 5061
    },
    {
        "loss": 2.1127,
        "grad_norm": 1.462790608406067,
        "learning_rate": 0.00019629630963744388,
        "epoch": 0.6969571802285557,
        "step": 5062
    },
    {
        "loss": 1.8739,
        "grad_norm": 2.5337746143341064,
        "learning_rate": 0.0001962611184051301,
        "epoch": 0.6970948643811097,
        "step": 5063
    },
    {
        "loss": 2.1266,
        "grad_norm": 1.9131205081939697,
        "learning_rate": 0.00019622576395936854,
        "epoch": 0.6972325485336638,
        "step": 5064
    },
    {
        "loss": 2.1415,
        "grad_norm": 1.6176379919052124,
        "learning_rate": 0.00019619024636010363,
        "epoch": 0.6973702326862178,
        "step": 5065
    },
    {
        "loss": 1.9657,
        "grad_norm": 2.7590677738189697,
        "learning_rate": 0.00019615456566755645,
        "epoch": 0.6975079168387719,
        "step": 5066
    },
    {
        "loss": 2.1594,
        "grad_norm": 2.6843903064727783,
        "learning_rate": 0.00019611872194222468,
        "epoch": 0.6976456009913259,
        "step": 5067
    },
    {
        "loss": 2.2422,
        "grad_norm": 1.460432767868042,
        "learning_rate": 0.00019608271524488237,
        "epoch": 0.69778328514388,
        "step": 5068
    },
    {
        "loss": 2.1985,
        "grad_norm": 1.5561274290084839,
        "learning_rate": 0.0001960465456365798,
        "epoch": 0.697920969296434,
        "step": 5069
    },
    {
        "loss": 1.4766,
        "grad_norm": 2.6677019596099854,
        "learning_rate": 0.0001960102131786437,
        "epoch": 0.698058653448988,
        "step": 5070
    },
    {
        "loss": 2.0186,
        "grad_norm": 1.4268546104431152,
        "learning_rate": 0.00019597371793267666,
        "epoch": 0.6981963376015421,
        "step": 5071
    },
    {
        "loss": 2.6488,
        "grad_norm": 1.4148287773132324,
        "learning_rate": 0.0001959370599605575,
        "epoch": 0.6983340217540961,
        "step": 5072
    },
    {
        "loss": 2.1118,
        "grad_norm": 1.8259848356246948,
        "learning_rate": 0.00019590023932444074,
        "epoch": 0.6984717059066502,
        "step": 5073
    },
    {
        "loss": 2.4305,
        "grad_norm": 1.7956786155700684,
        "learning_rate": 0.00019586325608675692,
        "epoch": 0.6986093900592042,
        "step": 5074
    },
    {
        "loss": 1.9172,
        "grad_norm": 1.911681056022644,
        "learning_rate": 0.00019582611031021205,
        "epoch": 0.6987470742117582,
        "step": 5075
    },
    {
        "loss": 2.2416,
        "grad_norm": 1.8652608394622803,
        "learning_rate": 0.00019578880205778796,
        "epoch": 0.6988847583643123,
        "step": 5076
    },
    {
        "loss": 1.7728,
        "grad_norm": 2.7015540599823,
        "learning_rate": 0.00019575133139274176,
        "epoch": 0.6990224425168663,
        "step": 5077
    },
    {
        "loss": 1.6868,
        "grad_norm": 5.2126569747924805,
        "learning_rate": 0.00019571369837860606,
        "epoch": 0.6991601266694204,
        "step": 5078
    },
    {
        "loss": 1.8583,
        "grad_norm": 2.722898006439209,
        "learning_rate": 0.00019567590307918869,
        "epoch": 0.6992978108219744,
        "step": 5079
    },
    {
        "loss": 2.4791,
        "grad_norm": 2.5883491039276123,
        "learning_rate": 0.00019563794555857265,
        "epoch": 0.6994354949745284,
        "step": 5080
    },
    {
        "loss": 2.2754,
        "grad_norm": 1.5881969928741455,
        "learning_rate": 0.00019559982588111601,
        "epoch": 0.6995731791270825,
        "step": 5081
    },
    {
        "loss": 2.1351,
        "grad_norm": 1.5569344758987427,
        "learning_rate": 0.00019556154411145174,
        "epoch": 0.6997108632796365,
        "step": 5082
    },
    {
        "loss": 2.2877,
        "grad_norm": 2.2768306732177734,
        "learning_rate": 0.0001955231003144877,
        "epoch": 0.6998485474321906,
        "step": 5083
    },
    {
        "loss": 2.0579,
        "grad_norm": 1.9931057691574097,
        "learning_rate": 0.0001954844945554063,
        "epoch": 0.6999862315847446,
        "step": 5084
    },
    {
        "loss": 1.7384,
        "grad_norm": 1.9013558626174927,
        "learning_rate": 0.00019544572689966493,
        "epoch": 0.7001239157372986,
        "step": 5085
    },
    {
        "loss": 2.061,
        "grad_norm": 2.3298909664154053,
        "learning_rate": 0.00019540679741299503,
        "epoch": 0.7002615998898527,
        "step": 5086
    },
    {
        "loss": 1.977,
        "grad_norm": 1.2900604009628296,
        "learning_rate": 0.00019536770616140276,
        "epoch": 0.7003992840424067,
        "step": 5087
    },
    {
        "loss": 2.0464,
        "grad_norm": 2.5825254917144775,
        "learning_rate": 0.00019532845321116842,
        "epoch": 0.7005369681949608,
        "step": 5088
    },
    {
        "loss": 2.1075,
        "grad_norm": 2.025120735168457,
        "learning_rate": 0.00019528903862884648,
        "epoch": 0.7006746523475148,
        "step": 5089
    },
    {
        "loss": 2.1012,
        "grad_norm": 2.504234790802002,
        "learning_rate": 0.00019524946248126546,
        "epoch": 0.7008123365000688,
        "step": 5090
    },
    {
        "loss": 2.269,
        "grad_norm": 2.416316270828247,
        "learning_rate": 0.0001952097248355279,
        "epoch": 0.7009500206526229,
        "step": 5091
    },
    {
        "loss": 2.4385,
        "grad_norm": 1.646684169769287,
        "learning_rate": 0.00019516982575901003,
        "epoch": 0.7010877048051769,
        "step": 5092
    },
    {
        "loss": 2.2661,
        "grad_norm": 2.1111748218536377,
        "learning_rate": 0.0001951297653193619,
        "epoch": 0.701225388957731,
        "step": 5093
    },
    {
        "loss": 2.7129,
        "grad_norm": 1.5615054368972778,
        "learning_rate": 0.00019508954358450712,
        "epoch": 0.701363073110285,
        "step": 5094
    },
    {
        "loss": 2.4143,
        "grad_norm": 1.1941964626312256,
        "learning_rate": 0.00019504916062264283,
        "epoch": 0.701500757262839,
        "step": 5095
    },
    {
        "loss": 2.1254,
        "grad_norm": 1.1657757759094238,
        "learning_rate": 0.00019500861650223938,
        "epoch": 0.7016384414153931,
        "step": 5096
    },
    {
        "loss": 1.1261,
        "grad_norm": 3.2308688163757324,
        "learning_rate": 0.00019496791129204052,
        "epoch": 0.7017761255679471,
        "step": 5097
    },
    {
        "loss": 1.8708,
        "grad_norm": 2.3313145637512207,
        "learning_rate": 0.00019492704506106313,
        "epoch": 0.7019138097205012,
        "step": 5098
    },
    {
        "loss": 2.1251,
        "grad_norm": 2.2359211444854736,
        "learning_rate": 0.00019488601787859692,
        "epoch": 0.7020514938730552,
        "step": 5099
    },
    {
        "loss": 2.2163,
        "grad_norm": 1.578473448753357,
        "learning_rate": 0.0001948448298142048,
        "epoch": 0.7021891780256092,
        "step": 5100
    },
    {
        "loss": 1.4563,
        "grad_norm": 3.087415933609009,
        "learning_rate": 0.00019480348093772225,
        "epoch": 0.7023268621781633,
        "step": 5101
    },
    {
        "loss": 1.6374,
        "grad_norm": 2.637052536010742,
        "learning_rate": 0.00019476197131925737,
        "epoch": 0.7024645463307173,
        "step": 5102
    },
    {
        "loss": 2.0025,
        "grad_norm": 1.822729229927063,
        "learning_rate": 0.000194720301029191,
        "epoch": 0.7026022304832714,
        "step": 5103
    },
    {
        "loss": 1.8432,
        "grad_norm": 3.180967330932617,
        "learning_rate": 0.0001946784701381763,
        "epoch": 0.7027399146358254,
        "step": 5104
    },
    {
        "loss": 1.9793,
        "grad_norm": 2.061858654022217,
        "learning_rate": 0.0001946364787171386,
        "epoch": 0.7028775987883794,
        "step": 5105
    },
    {
        "loss": 1.83,
        "grad_norm": 2.3757073879241943,
        "learning_rate": 0.00019459432683727567,
        "epoch": 0.7030152829409335,
        "step": 5106
    },
    {
        "loss": 2.6074,
        "grad_norm": 2.4777963161468506,
        "learning_rate": 0.00019455201457005717,
        "epoch": 0.7031529670934875,
        "step": 5107
    },
    {
        "loss": 1.9786,
        "grad_norm": 1.3759714365005493,
        "learning_rate": 0.00019450954198722473,
        "epoch": 0.7032906512460416,
        "step": 5108
    },
    {
        "loss": 1.5915,
        "grad_norm": 1.871311068534851,
        "learning_rate": 0.00019446690916079186,
        "epoch": 0.7034283353985956,
        "step": 5109
    },
    {
        "loss": 2.1201,
        "grad_norm": 1.7611961364746094,
        "learning_rate": 0.0001944241161630437,
        "epoch": 0.7035660195511496,
        "step": 5110
    },
    {
        "loss": 1.2953,
        "grad_norm": 3.7088382244110107,
        "learning_rate": 0.00019438116306653698,
        "epoch": 0.7037037037037037,
        "step": 5111
    },
    {
        "loss": 1.7747,
        "grad_norm": 4.432312488555908,
        "learning_rate": 0.00019433804994409986,
        "epoch": 0.7038413878562577,
        "step": 5112
    },
    {
        "loss": 2.3404,
        "grad_norm": 1.7870895862579346,
        "learning_rate": 0.00019429477686883193,
        "epoch": 0.7039790720088118,
        "step": 5113
    },
    {
        "loss": 2.3073,
        "grad_norm": 2.802460193634033,
        "learning_rate": 0.00019425134391410381,
        "epoch": 0.7041167561613658,
        "step": 5114
    },
    {
        "loss": 2.0434,
        "grad_norm": 1.7065447568893433,
        "learning_rate": 0.0001942077511535574,
        "epoch": 0.7042544403139198,
        "step": 5115
    },
    {
        "loss": 2.1002,
        "grad_norm": 3.837038278579712,
        "learning_rate": 0.00019416399866110544,
        "epoch": 0.7043921244664739,
        "step": 5116
    },
    {
        "loss": 1.9716,
        "grad_norm": 2.422931671142578,
        "learning_rate": 0.00019412008651093144,
        "epoch": 0.7045298086190279,
        "step": 5117
    },
    {
        "loss": 1.8534,
        "grad_norm": 1.7294399738311768,
        "learning_rate": 0.00019407601477748984,
        "epoch": 0.704667492771582,
        "step": 5118
    },
    {
        "loss": 2.3862,
        "grad_norm": 1.302409052848816,
        "learning_rate": 0.00019403178353550536,
        "epoch": 0.704805176924136,
        "step": 5119
    },
    {
        "loss": 1.9759,
        "grad_norm": 2.1365127563476562,
        "learning_rate": 0.0001939873928599734,
        "epoch": 0.70494286107669,
        "step": 5120
    },
    {
        "loss": 2.2082,
        "grad_norm": 2.363356113433838,
        "learning_rate": 0.0001939428428261596,
        "epoch": 0.7050805452292441,
        "step": 5121
    },
    {
        "loss": 1.7717,
        "grad_norm": 2.1307668685913086,
        "learning_rate": 0.00019389813350959977,
        "epoch": 0.7052182293817981,
        "step": 5122
    },
    {
        "loss": 1.8739,
        "grad_norm": 1.9634528160095215,
        "learning_rate": 0.0001938532649860999,
        "epoch": 0.7053559135343522,
        "step": 5123
    },
    {
        "loss": 2.0143,
        "grad_norm": 1.2494605779647827,
        "learning_rate": 0.00019380823733173568,
        "epoch": 0.7054935976869062,
        "step": 5124
    },
    {
        "loss": 2.3771,
        "grad_norm": 2.221693754196167,
        "learning_rate": 0.00019376305062285295,
        "epoch": 0.7056312818394602,
        "step": 5125
    },
    {
        "loss": 2.1429,
        "grad_norm": 1.7826998233795166,
        "learning_rate": 0.00019371770493606692,
        "epoch": 0.7057689659920143,
        "step": 5126
    },
    {
        "loss": 1.236,
        "grad_norm": 2.5374958515167236,
        "learning_rate": 0.00019367220034826254,
        "epoch": 0.7059066501445683,
        "step": 5127
    },
    {
        "loss": 2.4279,
        "grad_norm": 1.6062465906143188,
        "learning_rate": 0.00019362653693659416,
        "epoch": 0.7060443342971224,
        "step": 5128
    },
    {
        "loss": 1.7628,
        "grad_norm": 2.323093891143799,
        "learning_rate": 0.00019358071477848534,
        "epoch": 0.7061820184496764,
        "step": 5129
    },
    {
        "loss": 1.8186,
        "grad_norm": 2.539621353149414,
        "learning_rate": 0.0001935347339516288,
        "epoch": 0.7063197026022305,
        "step": 5130
    },
    {
        "loss": 1.4199,
        "grad_norm": 3.6525635719299316,
        "learning_rate": 0.00019348859453398646,
        "epoch": 0.7064573867547845,
        "step": 5131
    },
    {
        "loss": 1.6106,
        "grad_norm": 2.5125558376312256,
        "learning_rate": 0.00019344229660378892,
        "epoch": 0.7065950709073385,
        "step": 5132
    },
    {
        "loss": 2.5014,
        "grad_norm": 1.8038115501403809,
        "learning_rate": 0.0001933958402395356,
        "epoch": 0.7067327550598926,
        "step": 5133
    },
    {
        "loss": 2.0334,
        "grad_norm": 2.122361660003662,
        "learning_rate": 0.00019334922551999465,
        "epoch": 0.7068704392124466,
        "step": 5134
    },
    {
        "loss": 1.4515,
        "grad_norm": 2.061720848083496,
        "learning_rate": 0.00019330245252420264,
        "epoch": 0.7070081233650007,
        "step": 5135
    },
    {
        "loss": 1.6113,
        "grad_norm": 2.38150691986084,
        "learning_rate": 0.00019325552133146447,
        "epoch": 0.7071458075175547,
        "step": 5136
    },
    {
        "loss": 1.6601,
        "grad_norm": 1.5466828346252441,
        "learning_rate": 0.0001932084320213533,
        "epoch": 0.7072834916701087,
        "step": 5137
    },
    {
        "loss": 1.5474,
        "grad_norm": 2.2966012954711914,
        "learning_rate": 0.00019316118467371045,
        "epoch": 0.7074211758226628,
        "step": 5138
    },
    {
        "loss": 2.2875,
        "grad_norm": 1.4630464315414429,
        "learning_rate": 0.000193113779368645,
        "epoch": 0.7075588599752168,
        "step": 5139
    },
    {
        "loss": 2.235,
        "grad_norm": 1.6125237941741943,
        "learning_rate": 0.00019306621618653415,
        "epoch": 0.7076965441277709,
        "step": 5140
    },
    {
        "loss": 0.3789,
        "grad_norm": 5.589758396148682,
        "learning_rate": 0.0001930184952080225,
        "epoch": 0.7078342282803249,
        "step": 5141
    },
    {
        "loss": 2.3489,
        "grad_norm": 2.2846994400024414,
        "learning_rate": 0.00019297061651402236,
        "epoch": 0.7079719124328789,
        "step": 5142
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.609022617340088,
        "learning_rate": 0.00019292258018571343,
        "epoch": 0.708109596585433,
        "step": 5143
    },
    {
        "loss": 1.5987,
        "grad_norm": 3.0018491744995117,
        "learning_rate": 0.00019287438630454266,
        "epoch": 0.708247280737987,
        "step": 5144
    },
    {
        "loss": 2.1744,
        "grad_norm": 1.3019808530807495,
        "learning_rate": 0.0001928260349522241,
        "epoch": 0.7083849648905411,
        "step": 5145
    },
    {
        "loss": 1.5146,
        "grad_norm": 2.779003381729126,
        "learning_rate": 0.00019277752621073887,
        "epoch": 0.7085226490430951,
        "step": 5146
    },
    {
        "loss": 1.7744,
        "grad_norm": 2.1214616298675537,
        "learning_rate": 0.0001927288601623349,
        "epoch": 0.7086603331956491,
        "step": 5147
    },
    {
        "loss": 2.1607,
        "grad_norm": 2.282608985900879,
        "learning_rate": 0.0001926800368895268,
        "epoch": 0.7087980173482032,
        "step": 5148
    },
    {
        "loss": 2.5068,
        "grad_norm": 9.679214477539062,
        "learning_rate": 0.0001926310564750959,
        "epoch": 0.7089357015007572,
        "step": 5149
    },
    {
        "loss": 2.2164,
        "grad_norm": 2.3225419521331787,
        "learning_rate": 0.00019258191900208983,
        "epoch": 0.7090733856533113,
        "step": 5150
    },
    {
        "loss": 1.7781,
        "grad_norm": 2.9921302795410156,
        "learning_rate": 0.00019253262455382254,
        "epoch": 0.7092110698058653,
        "step": 5151
    },
    {
        "loss": 1.7014,
        "grad_norm": 3.254704475402832,
        "learning_rate": 0.00019248317321387423,
        "epoch": 0.7093487539584193,
        "step": 5152
    },
    {
        "loss": 2.0398,
        "grad_norm": 2.17065691947937,
        "learning_rate": 0.00019243356506609102,
        "epoch": 0.7094864381109734,
        "step": 5153
    },
    {
        "loss": 2.1108,
        "grad_norm": 2.5052335262298584,
        "learning_rate": 0.00019238380019458485,
        "epoch": 0.7096241222635274,
        "step": 5154
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.1995893716812134,
        "learning_rate": 0.0001923338786837336,
        "epoch": 0.7097618064160816,
        "step": 5155
    },
    {
        "loss": 1.8168,
        "grad_norm": 1.8555045127868652,
        "learning_rate": 0.0001922838006181805,
        "epoch": 0.7098994905686355,
        "step": 5156
    },
    {
        "loss": 1.4441,
        "grad_norm": 3.0846729278564453,
        "learning_rate": 0.00019223356608283432,
        "epoch": 0.7100371747211895,
        "step": 5157
    },
    {
        "loss": 2.3148,
        "grad_norm": 2.3395440578460693,
        "learning_rate": 0.00019218317516286922,
        "epoch": 0.7101748588737437,
        "step": 5158
    },
    {
        "loss": 1.966,
        "grad_norm": 1.65771484375,
        "learning_rate": 0.0001921326279437244,
        "epoch": 0.7103125430262976,
        "step": 5159
    },
    {
        "loss": 1.7829,
        "grad_norm": 2.416266441345215,
        "learning_rate": 0.00019208192451110413,
        "epoch": 0.7104502271788518,
        "step": 5160
    },
    {
        "loss": 1.8704,
        "grad_norm": 1.727271556854248,
        "learning_rate": 0.00019203106495097738,
        "epoch": 0.7105879113314058,
        "step": 5161
    },
    {
        "loss": 2.5537,
        "grad_norm": 3.135768413543701,
        "learning_rate": 0.00019198004934957812,
        "epoch": 0.7107255954839597,
        "step": 5162
    },
    {
        "loss": 1.1655,
        "grad_norm": 2.756253242492676,
        "learning_rate": 0.00019192887779340457,
        "epoch": 0.7108632796365139,
        "step": 5163
    },
    {
        "loss": 1.5497,
        "grad_norm": 1.7194205522537231,
        "learning_rate": 0.00019187755036921978,
        "epoch": 0.7110009637890679,
        "step": 5164
    },
    {
        "loss": 1.873,
        "grad_norm": 1.8528203964233398,
        "learning_rate": 0.0001918260671640507,
        "epoch": 0.711138647941622,
        "step": 5165
    },
    {
        "loss": 2.5981,
        "grad_norm": 1.9199645519256592,
        "learning_rate": 0.00019177442826518855,
        "epoch": 0.711276332094176,
        "step": 5166
    },
    {
        "loss": 2.3736,
        "grad_norm": 2.0385899543762207,
        "learning_rate": 0.0001917226337601886,
        "epoch": 0.71141401624673,
        "step": 5167
    },
    {
        "loss": 1.7215,
        "grad_norm": 2.4213767051696777,
        "learning_rate": 0.00019167068373686995,
        "epoch": 0.7115517003992841,
        "step": 5168
    },
    {
        "loss": 1.3238,
        "grad_norm": 2.489492416381836,
        "learning_rate": 0.00019161857828331523,
        "epoch": 0.7116893845518381,
        "step": 5169
    },
    {
        "loss": 2.051,
        "grad_norm": 2.0739638805389404,
        "learning_rate": 0.00019156631748787074,
        "epoch": 0.7118270687043922,
        "step": 5170
    },
    {
        "loss": 2.48,
        "grad_norm": 1.9626989364624023,
        "learning_rate": 0.00019151390143914613,
        "epoch": 0.7119647528569462,
        "step": 5171
    },
    {
        "loss": 2.3304,
        "grad_norm": 2.0435421466827393,
        "learning_rate": 0.00019146133022601426,
        "epoch": 0.7121024370095002,
        "step": 5172
    },
    {
        "loss": 1.5822,
        "grad_norm": 3.5786972045898438,
        "learning_rate": 0.00019140860393761125,
        "epoch": 0.7122401211620543,
        "step": 5173
    },
    {
        "loss": 2.3959,
        "grad_norm": 3.2547667026519775,
        "learning_rate": 0.00019135572266333587,
        "epoch": 0.7123778053146083,
        "step": 5174
    },
    {
        "loss": 2.1931,
        "grad_norm": 1.8487309217453003,
        "learning_rate": 0.0001913026864928498,
        "epoch": 0.7125154894671624,
        "step": 5175
    },
    {
        "loss": 2.1162,
        "grad_norm": 2.210513114929199,
        "learning_rate": 0.00019124949551607752,
        "epoch": 0.7126531736197164,
        "step": 5176
    },
    {
        "loss": 2.0075,
        "grad_norm": 1.3157196044921875,
        "learning_rate": 0.0001911961498232057,
        "epoch": 0.7127908577722704,
        "step": 5177
    },
    {
        "loss": 2.2214,
        "grad_norm": 1.9046815633773804,
        "learning_rate": 0.00019114264950468348,
        "epoch": 0.7129285419248245,
        "step": 5178
    },
    {
        "loss": 1.8784,
        "grad_norm": 2.4609286785125732,
        "learning_rate": 0.00019108899465122226,
        "epoch": 0.7130662260773785,
        "step": 5179
    },
    {
        "loss": 1.9512,
        "grad_norm": 2.3176097869873047,
        "learning_rate": 0.00019103518535379526,
        "epoch": 0.7132039102299326,
        "step": 5180
    },
    {
        "loss": 2.1355,
        "grad_norm": 2.0826210975646973,
        "learning_rate": 0.00019098122170363771,
        "epoch": 0.7133415943824866,
        "step": 5181
    },
    {
        "loss": 1.8068,
        "grad_norm": 2.3200466632843018,
        "learning_rate": 0.0001909271037922465,
        "epoch": 0.7134792785350406,
        "step": 5182
    },
    {
        "loss": 2.5923,
        "grad_norm": 1.6121541261672974,
        "learning_rate": 0.0001908728317113801,
        "epoch": 0.7136169626875947,
        "step": 5183
    },
    {
        "loss": 1.7274,
        "grad_norm": 2.9431142807006836,
        "learning_rate": 0.00019081840555305833,
        "epoch": 0.7137546468401487,
        "step": 5184
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.5111736059188843,
        "learning_rate": 0.00019076382540956225,
        "epoch": 0.7138923309927028,
        "step": 5185
    },
    {
        "loss": 2.1638,
        "grad_norm": 1.495172142982483,
        "learning_rate": 0.0001907090913734341,
        "epoch": 0.7140300151452568,
        "step": 5186
    },
    {
        "loss": 2.1486,
        "grad_norm": 1.4478100538253784,
        "learning_rate": 0.0001906542035374769,
        "epoch": 0.7141676992978109,
        "step": 5187
    },
    {
        "loss": 2.3929,
        "grad_norm": 2.155407667160034,
        "learning_rate": 0.00019059916199475464,
        "epoch": 0.7143053834503649,
        "step": 5188
    },
    {
        "loss": 1.9024,
        "grad_norm": 1.7490440607070923,
        "learning_rate": 0.0001905439668385917,
        "epoch": 0.7144430676029189,
        "step": 5189
    },
    {
        "loss": 2.3589,
        "grad_norm": 2.156101942062378,
        "learning_rate": 0.00019048861816257313,
        "epoch": 0.714580751755473,
        "step": 5190
    },
    {
        "loss": 2.4862,
        "grad_norm": 2.6227095127105713,
        "learning_rate": 0.0001904331160605441,
        "epoch": 0.714718435908027,
        "step": 5191
    },
    {
        "loss": 1.9502,
        "grad_norm": 2.116901397705078,
        "learning_rate": 0.00019037746062661005,
        "epoch": 0.7148561200605811,
        "step": 5192
    },
    {
        "loss": 2.1909,
        "grad_norm": 2.040271520614624,
        "learning_rate": 0.0001903216519551363,
        "epoch": 0.7149938042131351,
        "step": 5193
    },
    {
        "loss": 2.4736,
        "grad_norm": 8.14989948272705,
        "learning_rate": 0.00019026569014074805,
        "epoch": 0.7151314883656891,
        "step": 5194
    },
    {
        "loss": 1.733,
        "grad_norm": 3.2197318077087402,
        "learning_rate": 0.00019020957527833018,
        "epoch": 0.7152691725182432,
        "step": 5195
    },
    {
        "loss": 2.2211,
        "grad_norm": 1.9175292253494263,
        "learning_rate": 0.00019015330746302683,
        "epoch": 0.7154068566707972,
        "step": 5196
    },
    {
        "loss": 2.7531,
        "grad_norm": 1.2210806608200073,
        "learning_rate": 0.00019009688679024194,
        "epoch": 0.7155445408233513,
        "step": 5197
    },
    {
        "loss": 1.3257,
        "grad_norm": 3.9613029956817627,
        "learning_rate": 0.0001900403133556382,
        "epoch": 0.7156822249759053,
        "step": 5198
    },
    {
        "loss": 1.7242,
        "grad_norm": 1.9649547338485718,
        "learning_rate": 0.00018998358725513746,
        "epoch": 0.7158199091284593,
        "step": 5199
    },
    {
        "loss": 2.5841,
        "grad_norm": 2.288106918334961,
        "learning_rate": 0.0001899267085849205,
        "epoch": 0.7159575932810134,
        "step": 5200
    },
    {
        "loss": 1.7151,
        "grad_norm": 1.749064564704895,
        "learning_rate": 0.00018986967744142672,
        "epoch": 0.7160952774335674,
        "step": 5201
    },
    {
        "loss": 1.914,
        "grad_norm": 1.9758882522583008,
        "learning_rate": 0.0001898124939213539,
        "epoch": 0.7162329615861215,
        "step": 5202
    },
    {
        "loss": 2.3393,
        "grad_norm": 1.806307315826416,
        "learning_rate": 0.00018975515812165854,
        "epoch": 0.7163706457386755,
        "step": 5203
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.9762425422668457,
        "learning_rate": 0.00018969767013955492,
        "epoch": 0.7165083298912295,
        "step": 5204
    },
    {
        "loss": 2.2684,
        "grad_norm": 2.0522170066833496,
        "learning_rate": 0.00018964003007251564,
        "epoch": 0.7166460140437836,
        "step": 5205
    },
    {
        "loss": 1.8557,
        "grad_norm": 1.5574766397476196,
        "learning_rate": 0.00018958223801827096,
        "epoch": 0.7167836981963376,
        "step": 5206
    },
    {
        "loss": 2.1863,
        "grad_norm": 2.2111644744873047,
        "learning_rate": 0.00018952429407480904,
        "epoch": 0.7169213823488917,
        "step": 5207
    },
    {
        "loss": 2.0921,
        "grad_norm": 1.981189489364624,
        "learning_rate": 0.00018946619834037546,
        "epoch": 0.7170590665014457,
        "step": 5208
    },
    {
        "loss": 1.3712,
        "grad_norm": 3.0755162239074707,
        "learning_rate": 0.00018940795091347318,
        "epoch": 0.7171967506539997,
        "step": 5209
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.424326777458191,
        "learning_rate": 0.0001893495518928623,
        "epoch": 0.7173344348065538,
        "step": 5210
    },
    {
        "loss": 2.0391,
        "grad_norm": 1.0661321878433228,
        "learning_rate": 0.00018929100137756008,
        "epoch": 0.7174721189591078,
        "step": 5211
    },
    {
        "loss": 2.5022,
        "grad_norm": 1.1017186641693115,
        "learning_rate": 0.00018923229946684052,
        "epoch": 0.7176098031116619,
        "step": 5212
    },
    {
        "loss": 1.9253,
        "grad_norm": 3.1471292972564697,
        "learning_rate": 0.0001891734462602344,
        "epoch": 0.7177474872642159,
        "step": 5213
    },
    {
        "loss": 2.4837,
        "grad_norm": 1.7118101119995117,
        "learning_rate": 0.000189114441857529,
        "epoch": 0.7178851714167699,
        "step": 5214
    },
    {
        "loss": 2.0961,
        "grad_norm": 1.4206289052963257,
        "learning_rate": 0.00018905528635876787,
        "epoch": 0.718022855569324,
        "step": 5215
    },
    {
        "loss": 2.4245,
        "grad_norm": 2.3659121990203857,
        "learning_rate": 0.000188995979864251,
        "epoch": 0.718160539721878,
        "step": 5216
    },
    {
        "loss": 2.3347,
        "grad_norm": 1.2315882444381714,
        "learning_rate": 0.00018893652247453413,
        "epoch": 0.7182982238744321,
        "step": 5217
    },
    {
        "loss": 2.6044,
        "grad_norm": 1.720046877861023,
        "learning_rate": 0.00018887691429042894,
        "epoch": 0.7184359080269861,
        "step": 5218
    },
    {
        "loss": 1.8756,
        "grad_norm": 3.008997917175293,
        "learning_rate": 0.0001888171554130028,
        "epoch": 0.7185735921795401,
        "step": 5219
    },
    {
        "loss": 2.0978,
        "grad_norm": 2.0347015857696533,
        "learning_rate": 0.00018875724594357855,
        "epoch": 0.7187112763320942,
        "step": 5220
    },
    {
        "loss": 2.5828,
        "grad_norm": 1.4458272457122803,
        "learning_rate": 0.0001886971859837344,
        "epoch": 0.7188489604846482,
        "step": 5221
    },
    {
        "loss": 1.1683,
        "grad_norm": 3.5972118377685547,
        "learning_rate": 0.00018863697563530375,
        "epoch": 0.7189866446372023,
        "step": 5222
    },
    {
        "loss": 1.5569,
        "grad_norm": 2.8784713745117188,
        "learning_rate": 0.00018857661500037493,
        "epoch": 0.7191243287897563,
        "step": 5223
    },
    {
        "loss": 1.9028,
        "grad_norm": 1.6874279975891113,
        "learning_rate": 0.00018851610418129104,
        "epoch": 0.7192620129423103,
        "step": 5224
    },
    {
        "loss": 2.63,
        "grad_norm": 2.4696009159088135,
        "learning_rate": 0.00018845544328065,
        "epoch": 0.7193996970948644,
        "step": 5225
    },
    {
        "loss": 1.8845,
        "grad_norm": 2.6450741291046143,
        "learning_rate": 0.00018839463240130403,
        "epoch": 0.7195373812474184,
        "step": 5226
    },
    {
        "loss": 1.7855,
        "grad_norm": 3.5986430644989014,
        "learning_rate": 0.0001883336716463596,
        "epoch": 0.7196750653999725,
        "step": 5227
    },
    {
        "loss": 1.9715,
        "grad_norm": 1.409485101699829,
        "learning_rate": 0.00018827256111917756,
        "epoch": 0.7198127495525265,
        "step": 5228
    },
    {
        "loss": 1.4021,
        "grad_norm": 1.7951381206512451,
        "learning_rate": 0.00018821130092337246,
        "epoch": 0.7199504337050805,
        "step": 5229
    },
    {
        "loss": 1.9081,
        "grad_norm": 1.335204839706421,
        "learning_rate": 0.00018814989116281263,
        "epoch": 0.7200881178576346,
        "step": 5230
    },
    {
        "loss": 2.1229,
        "grad_norm": 2.80935001373291,
        "learning_rate": 0.00018808833194162018,
        "epoch": 0.7202258020101886,
        "step": 5231
    },
    {
        "loss": 1.2408,
        "grad_norm": 2.664102077484131,
        "learning_rate": 0.00018802662336417044,
        "epoch": 0.7203634861627427,
        "step": 5232
    },
    {
        "loss": 1.2181,
        "grad_norm": 3.352893590927124,
        "learning_rate": 0.00018796476553509207,
        "epoch": 0.7205011703152967,
        "step": 5233
    },
    {
        "loss": 2.5667,
        "grad_norm": 2.249927520751953,
        "learning_rate": 0.00018790275855926677,
        "epoch": 0.7206388544678507,
        "step": 5234
    },
    {
        "loss": 2.0374,
        "grad_norm": 1.5226365327835083,
        "learning_rate": 0.00018784060254182906,
        "epoch": 0.7207765386204048,
        "step": 5235
    },
    {
        "loss": 2.1337,
        "grad_norm": 2.060896635055542,
        "learning_rate": 0.0001877782975881662,
        "epoch": 0.7209142227729588,
        "step": 5236
    },
    {
        "loss": 2.1182,
        "grad_norm": 1.4603239297866821,
        "learning_rate": 0.00018771584380391815,
        "epoch": 0.7210519069255129,
        "step": 5237
    },
    {
        "loss": 1.5537,
        "grad_norm": 1.541513204574585,
        "learning_rate": 0.000187653241294977,
        "epoch": 0.7211895910780669,
        "step": 5238
    },
    {
        "loss": 2.0262,
        "grad_norm": 1.8481062650680542,
        "learning_rate": 0.000187590490167487,
        "epoch": 0.7213272752306209,
        "step": 5239
    },
    {
        "loss": 2.216,
        "grad_norm": 1.4342447519302368,
        "learning_rate": 0.00018752759052784456,
        "epoch": 0.721464959383175,
        "step": 5240
    },
    {
        "loss": 2.4569,
        "grad_norm": 1.7257825136184692,
        "learning_rate": 0.00018746454248269777,
        "epoch": 0.721602643535729,
        "step": 5241
    },
    {
        "loss": 1.7234,
        "grad_norm": 2.313772201538086,
        "learning_rate": 0.00018740134613894633,
        "epoch": 0.7217403276882831,
        "step": 5242
    },
    {
        "loss": 2.1935,
        "grad_norm": 2.748053550720215,
        "learning_rate": 0.0001873380016037415,
        "epoch": 0.7218780118408371,
        "step": 5243
    },
    {
        "loss": 2.0344,
        "grad_norm": 2.6102631092071533,
        "learning_rate": 0.00018727450898448566,
        "epoch": 0.7220156959933912,
        "step": 5244
    },
    {
        "loss": 2.0643,
        "grad_norm": 0.985725462436676,
        "learning_rate": 0.00018721086838883232,
        "epoch": 0.7221533801459452,
        "step": 5245
    },
    {
        "loss": 1.9867,
        "grad_norm": 2.43782114982605,
        "learning_rate": 0.00018714707992468604,
        "epoch": 0.7222910642984992,
        "step": 5246
    },
    {
        "loss": 1.682,
        "grad_norm": 3.2203099727630615,
        "learning_rate": 0.00018708314370020185,
        "epoch": 0.7224287484510533,
        "step": 5247
    },
    {
        "loss": 2.5729,
        "grad_norm": 1.9715657234191895,
        "learning_rate": 0.0001870190598237854,
        "epoch": 0.7225664326036073,
        "step": 5248
    },
    {
        "loss": 2.2595,
        "grad_norm": 2.876453161239624,
        "learning_rate": 0.00018695482840409285,
        "epoch": 0.7227041167561614,
        "step": 5249
    },
    {
        "loss": 2.3465,
        "grad_norm": 1.1328098773956299,
        "learning_rate": 0.00018689044955003033,
        "epoch": 0.7228418009087154,
        "step": 5250
    },
    {
        "loss": 2.5125,
        "grad_norm": 2.4866745471954346,
        "learning_rate": 0.00018682592337075388,
        "epoch": 0.7229794850612694,
        "step": 5251
    },
    {
        "loss": 2.2821,
        "grad_norm": 2.391488552093506,
        "learning_rate": 0.0001867612499756697,
        "epoch": 0.7231171692138235,
        "step": 5252
    },
    {
        "loss": 1.9953,
        "grad_norm": 1.9540959596633911,
        "learning_rate": 0.00018669642947443315,
        "epoch": 0.7232548533663775,
        "step": 5253
    },
    {
        "loss": 2.5768,
        "grad_norm": 1.3192737102508545,
        "learning_rate": 0.00018663146197694927,
        "epoch": 0.7233925375189316,
        "step": 5254
    },
    {
        "loss": 1.6213,
        "grad_norm": 3.534554958343506,
        "learning_rate": 0.00018656634759337242,
        "epoch": 0.7235302216714856,
        "step": 5255
    },
    {
        "loss": 1.9592,
        "grad_norm": 1.592907428741455,
        "learning_rate": 0.00018650108643410574,
        "epoch": 0.7236679058240396,
        "step": 5256
    },
    {
        "loss": 0.8033,
        "grad_norm": 2.4097301959991455,
        "learning_rate": 0.00018643567860980142,
        "epoch": 0.7238055899765937,
        "step": 5257
    },
    {
        "loss": 1.5397,
        "grad_norm": 2.9648051261901855,
        "learning_rate": 0.0001863701242313603,
        "epoch": 0.7239432741291477,
        "step": 5258
    },
    {
        "loss": 1.5276,
        "grad_norm": 2.503713369369507,
        "learning_rate": 0.00018630442340993158,
        "epoch": 0.7240809582817018,
        "step": 5259
    },
    {
        "loss": 1.8101,
        "grad_norm": 2.6570379734039307,
        "learning_rate": 0.0001862385762569129,
        "epoch": 0.7242186424342558,
        "step": 5260
    },
    {
        "loss": 2.2894,
        "grad_norm": 1.4033373594284058,
        "learning_rate": 0.00018617258288395008,
        "epoch": 0.7243563265868098,
        "step": 5261
    },
    {
        "loss": 1.9442,
        "grad_norm": 1.9391621351242065,
        "learning_rate": 0.00018610644340293664,
        "epoch": 0.7244940107393639,
        "step": 5262
    },
    {
        "loss": 2.1939,
        "grad_norm": 2.140662908554077,
        "learning_rate": 0.0001860401579260139,
        "epoch": 0.7246316948919179,
        "step": 5263
    },
    {
        "loss": 2.4685,
        "grad_norm": 2.11356258392334,
        "learning_rate": 0.0001859737265655709,
        "epoch": 0.724769379044472,
        "step": 5264
    },
    {
        "loss": 2.4243,
        "grad_norm": 1.2243719100952148,
        "learning_rate": 0.00018590714943424384,
        "epoch": 0.724907063197026,
        "step": 5265
    },
    {
        "loss": 2.3595,
        "grad_norm": 1.6151467561721802,
        "learning_rate": 0.0001858404266449161,
        "epoch": 0.72504474734958,
        "step": 5266
    },
    {
        "loss": 1.2994,
        "grad_norm": 2.12605619430542,
        "learning_rate": 0.00018577355831071802,
        "epoch": 0.7251824315021341,
        "step": 5267
    },
    {
        "loss": 1.3018,
        "grad_norm": 1.4024763107299805,
        "learning_rate": 0.00018570654454502683,
        "epoch": 0.7253201156546881,
        "step": 5268
    },
    {
        "loss": 1.1394,
        "grad_norm": 2.7191271781921387,
        "learning_rate": 0.00018563938546146616,
        "epoch": 0.7254577998072422,
        "step": 5269
    },
    {
        "loss": 1.549,
        "grad_norm": 3.9493825435638428,
        "learning_rate": 0.00018557208117390625,
        "epoch": 0.7255954839597962,
        "step": 5270
    },
    {
        "loss": 2.0341,
        "grad_norm": 2.026989459991455,
        "learning_rate": 0.0001855046317964634,
        "epoch": 0.7257331681123502,
        "step": 5271
    },
    {
        "loss": 2.1904,
        "grad_norm": 2.231663703918457,
        "learning_rate": 0.00018543703744349982,
        "epoch": 0.7258708522649043,
        "step": 5272
    },
    {
        "loss": 1.9573,
        "grad_norm": 1.9277278184890747,
        "learning_rate": 0.00018536929822962378,
        "epoch": 0.7260085364174583,
        "step": 5273
    },
    {
        "loss": 1.0647,
        "grad_norm": 1.914968490600586,
        "learning_rate": 0.00018530141426968902,
        "epoch": 0.7261462205700124,
        "step": 5274
    },
    {
        "loss": 2.3474,
        "grad_norm": 2.462256908416748,
        "learning_rate": 0.00018523338567879463,
        "epoch": 0.7262839047225664,
        "step": 5275
    },
    {
        "loss": 2.3977,
        "grad_norm": 1.7796517610549927,
        "learning_rate": 0.00018516521257228513,
        "epoch": 0.7264215888751204,
        "step": 5276
    },
    {
        "loss": 1.9164,
        "grad_norm": 1.9685555696487427,
        "learning_rate": 0.00018509689506574986,
        "epoch": 0.7265592730276745,
        "step": 5277
    },
    {
        "loss": 2.2223,
        "grad_norm": 1.4085725545883179,
        "learning_rate": 0.00018502843327502304,
        "epoch": 0.7266969571802285,
        "step": 5278
    },
    {
        "loss": 2.2257,
        "grad_norm": 1.3948673009872437,
        "learning_rate": 0.00018495982731618372,
        "epoch": 0.7268346413327826,
        "step": 5279
    },
    {
        "loss": 1.7257,
        "grad_norm": 1.457889437675476,
        "learning_rate": 0.00018489107730555512,
        "epoch": 0.7269723254853366,
        "step": 5280
    },
    {
        "loss": 1.4784,
        "grad_norm": 2.8718321323394775,
        "learning_rate": 0.0001848221833597049,
        "epoch": 0.7271100096378906,
        "step": 5281
    },
    {
        "loss": 2.0795,
        "grad_norm": 2.8744096755981445,
        "learning_rate": 0.00018475314559544467,
        "epoch": 0.7272476937904447,
        "step": 5282
    },
    {
        "loss": 1.6598,
        "grad_norm": 2.3392293453216553,
        "learning_rate": 0.00018468396412982992,
        "epoch": 0.7273853779429987,
        "step": 5283
    },
    {
        "loss": 1.1959,
        "grad_norm": 2.9344725608825684,
        "learning_rate": 0.00018461463908015963,
        "epoch": 0.7275230620955528,
        "step": 5284
    },
    {
        "loss": 1.425,
        "grad_norm": 3.220336437225342,
        "learning_rate": 0.0001845451705639766,
        "epoch": 0.7276607462481068,
        "step": 5285
    },
    {
        "loss": 2.2602,
        "grad_norm": 1.9895031452178955,
        "learning_rate": 0.0001844755586990666,
        "epoch": 0.7277984304006608,
        "step": 5286
    },
    {
        "loss": 1.8301,
        "grad_norm": 2.7579452991485596,
        "learning_rate": 0.00018440580360345842,
        "epoch": 0.7279361145532149,
        "step": 5287
    },
    {
        "loss": 1.9374,
        "grad_norm": 1.7291862964630127,
        "learning_rate": 0.00018433590539542393,
        "epoch": 0.7280737987057689,
        "step": 5288
    },
    {
        "loss": 1.8781,
        "grad_norm": 1.532679557800293,
        "learning_rate": 0.00018426586419347744,
        "epoch": 0.728211482858323,
        "step": 5289
    },
    {
        "loss": 2.1532,
        "grad_norm": 1.7116508483886719,
        "learning_rate": 0.00018419568011637582,
        "epoch": 0.728349167010877,
        "step": 5290
    },
    {
        "loss": 1.6622,
        "grad_norm": 2.792921781539917,
        "learning_rate": 0.00018412535328311816,
        "epoch": 0.728486851163431,
        "step": 5291
    },
    {
        "loss": 2.2433,
        "grad_norm": 1.8830201625823975,
        "learning_rate": 0.00018405488381294556,
        "epoch": 0.7286245353159851,
        "step": 5292
    },
    {
        "loss": 2.0336,
        "grad_norm": 2.4991748332977295,
        "learning_rate": 0.000183984271825341,
        "epoch": 0.7287622194685391,
        "step": 5293
    },
    {
        "loss": 2.1833,
        "grad_norm": 2.4566805362701416,
        "learning_rate": 0.00018391351744002904,
        "epoch": 0.7288999036210932,
        "step": 5294
    },
    {
        "loss": 1.9021,
        "grad_norm": 3.5944244861602783,
        "learning_rate": 0.00018384262077697595,
        "epoch": 0.7290375877736472,
        "step": 5295
    },
    {
        "loss": 2.2646,
        "grad_norm": 1.3983789682388306,
        "learning_rate": 0.0001837715819563888,
        "epoch": 0.7291752719262012,
        "step": 5296
    },
    {
        "loss": 2.2546,
        "grad_norm": 3.7301876544952393,
        "learning_rate": 0.000183700401098716,
        "epoch": 0.7293129560787553,
        "step": 5297
    },
    {
        "loss": 2.1596,
        "grad_norm": 1.7749470472335815,
        "learning_rate": 0.0001836290783246468,
        "epoch": 0.7294506402313093,
        "step": 5298
    },
    {
        "loss": 1.2019,
        "grad_norm": 2.0614171028137207,
        "learning_rate": 0.00018355761375511088,
        "epoch": 0.7295883243838635,
        "step": 5299
    },
    {
        "loss": 2.4852,
        "grad_norm": 1.9783916473388672,
        "learning_rate": 0.00018348600751127837,
        "epoch": 0.7297260085364174,
        "step": 5300
    },
    {
        "loss": 2.0942,
        "grad_norm": 2.7143335342407227,
        "learning_rate": 0.00018341425971455985,
        "epoch": 0.7298636926889716,
        "step": 5301
    },
    {
        "loss": 1.8508,
        "grad_norm": 2.2732975482940674,
        "learning_rate": 0.00018334237048660566,
        "epoch": 0.7300013768415256,
        "step": 5302
    },
    {
        "loss": 2.6872,
        "grad_norm": 1.8358221054077148,
        "learning_rate": 0.00018327033994930595,
        "epoch": 0.7301390609940795,
        "step": 5303
    },
    {
        "loss": 2.4573,
        "grad_norm": 1.7330514192581177,
        "learning_rate": 0.00018319816822479065,
        "epoch": 0.7302767451466337,
        "step": 5304
    },
    {
        "loss": 1.559,
        "grad_norm": 1.6137230396270752,
        "learning_rate": 0.00018312585543542892,
        "epoch": 0.7304144292991877,
        "step": 5305
    },
    {
        "loss": 2.207,
        "grad_norm": 1.3349519968032837,
        "learning_rate": 0.00018305340170382913,
        "epoch": 0.7305521134517418,
        "step": 5306
    },
    {
        "loss": 0.985,
        "grad_norm": 2.3987975120544434,
        "learning_rate": 0.0001829808071528386,
        "epoch": 0.7306897976042958,
        "step": 5307
    },
    {
        "loss": 1.521,
        "grad_norm": 3.3866794109344482,
        "learning_rate": 0.00018290807190554352,
        "epoch": 0.7308274817568498,
        "step": 5308
    },
    {
        "loss": 1.7187,
        "grad_norm": 2.454190254211426,
        "learning_rate": 0.0001828351960852684,
        "epoch": 0.7309651659094039,
        "step": 5309
    },
    {
        "loss": 1.7809,
        "grad_norm": 2.778125524520874,
        "learning_rate": 0.0001827621798155764,
        "epoch": 0.7311028500619579,
        "step": 5310
    },
    {
        "loss": 1.569,
        "grad_norm": 2.699252128601074,
        "learning_rate": 0.00018268902322026866,
        "epoch": 0.731240534214512,
        "step": 5311
    },
    {
        "loss": 2.017,
        "grad_norm": 2.360860586166382,
        "learning_rate": 0.00018261572642338416,
        "epoch": 0.731378218367066,
        "step": 5312
    },
    {
        "loss": 1.6885,
        "grad_norm": 2.0130844116210938,
        "learning_rate": 0.0001825422895491998,
        "epoch": 0.73151590251962,
        "step": 5313
    },
    {
        "loss": 1.758,
        "grad_norm": 4.866580963134766,
        "learning_rate": 0.0001824687127222299,
        "epoch": 0.7316535866721741,
        "step": 5314
    },
    {
        "loss": 1.0388,
        "grad_norm": 3.8543202877044678,
        "learning_rate": 0.00018239499606722594,
        "epoch": 0.7317912708247281,
        "step": 5315
    },
    {
        "loss": 2.5791,
        "grad_norm": 1.7070317268371582,
        "learning_rate": 0.00018232113970917665,
        "epoch": 0.7319289549772822,
        "step": 5316
    },
    {
        "loss": 2.3258,
        "grad_norm": 2.6470415592193604,
        "learning_rate": 0.00018224714377330754,
        "epoch": 0.7320666391298362,
        "step": 5317
    },
    {
        "loss": 1.5029,
        "grad_norm": 2.806898355484009,
        "learning_rate": 0.00018217300838508073,
        "epoch": 0.7322043232823902,
        "step": 5318
    },
    {
        "loss": 2.0244,
        "grad_norm": 2.039015531539917,
        "learning_rate": 0.0001820987336701951,
        "epoch": 0.7323420074349443,
        "step": 5319
    },
    {
        "loss": 1.7243,
        "grad_norm": 2.235435724258423,
        "learning_rate": 0.0001820243197545854,
        "epoch": 0.7324796915874983,
        "step": 5320
    },
    {
        "loss": 2.3005,
        "grad_norm": 2.2773120403289795,
        "learning_rate": 0.00018194976676442242,
        "epoch": 0.7326173757400524,
        "step": 5321
    },
    {
        "loss": 0.6775,
        "grad_norm": 2.5421302318573,
        "learning_rate": 0.00018187507482611304,
        "epoch": 0.7327550598926064,
        "step": 5322
    },
    {
        "loss": 2.0073,
        "grad_norm": 2.3126137256622314,
        "learning_rate": 0.00018180024406629948,
        "epoch": 0.7328927440451604,
        "step": 5323
    },
    {
        "loss": 1.8969,
        "grad_norm": 2.1259608268737793,
        "learning_rate": 0.00018172527461185936,
        "epoch": 0.7330304281977145,
        "step": 5324
    },
    {
        "loss": 2.0703,
        "grad_norm": 1.9900227785110474,
        "learning_rate": 0.0001816501665899056,
        "epoch": 0.7331681123502685,
        "step": 5325
    },
    {
        "loss": 1.9463,
        "grad_norm": 1.2088100910186768,
        "learning_rate": 0.00018157492012778597,
        "epoch": 0.7333057965028226,
        "step": 5326
    },
    {
        "loss": 0.7072,
        "grad_norm": 2.000291109085083,
        "learning_rate": 0.0001814995353530828,
        "epoch": 0.7334434806553766,
        "step": 5327
    },
    {
        "loss": 2.086,
        "grad_norm": 2.346115827560425,
        "learning_rate": 0.0001814240123936134,
        "epoch": 0.7335811648079306,
        "step": 5328
    },
    {
        "loss": 2.4159,
        "grad_norm": 1.9537631273269653,
        "learning_rate": 0.00018134835137742887,
        "epoch": 0.7337188489604847,
        "step": 5329
    },
    {
        "loss": 2.0255,
        "grad_norm": 2.9911487102508545,
        "learning_rate": 0.00018127255243281467,
        "epoch": 0.7338565331130387,
        "step": 5330
    },
    {
        "loss": 1.8032,
        "grad_norm": 1.5769374370574951,
        "learning_rate": 0.00018119661568829007,
        "epoch": 0.7339942172655928,
        "step": 5331
    },
    {
        "loss": 2.6618,
        "grad_norm": 2.342578172683716,
        "learning_rate": 0.00018112054127260793,
        "epoch": 0.7341319014181468,
        "step": 5332
    },
    {
        "loss": 1.7237,
        "grad_norm": 2.6797091960906982,
        "learning_rate": 0.00018104432931475447,
        "epoch": 0.7342695855707008,
        "step": 5333
    },
    {
        "loss": 2.1559,
        "grad_norm": 1.8955023288726807,
        "learning_rate": 0.00018096797994394948,
        "epoch": 0.7344072697232549,
        "step": 5334
    },
    {
        "loss": 2.1013,
        "grad_norm": 2.4584884643554688,
        "learning_rate": 0.00018089149328964527,
        "epoch": 0.7345449538758089,
        "step": 5335
    },
    {
        "loss": 2.0008,
        "grad_norm": 1.6023905277252197,
        "learning_rate": 0.00018081486948152712,
        "epoch": 0.734682638028363,
        "step": 5336
    },
    {
        "loss": 2.1612,
        "grad_norm": 1.7233593463897705,
        "learning_rate": 0.00018073810864951303,
        "epoch": 0.734820322180917,
        "step": 5337
    },
    {
        "loss": 1.1539,
        "grad_norm": 1.8333641290664673,
        "learning_rate": 0.000180661210923753,
        "epoch": 0.734958006333471,
        "step": 5338
    },
    {
        "loss": 1.6622,
        "grad_norm": 2.398892879486084,
        "learning_rate": 0.0001805841764346294,
        "epoch": 0.7350956904860251,
        "step": 5339
    },
    {
        "loss": 2.1543,
        "grad_norm": 1.735108494758606,
        "learning_rate": 0.00018050700531275634,
        "epoch": 0.7352333746385791,
        "step": 5340
    },
    {
        "loss": 2.307,
        "grad_norm": 3.1033823490142822,
        "learning_rate": 0.00018042969768897963,
        "epoch": 0.7353710587911332,
        "step": 5341
    },
    {
        "loss": 1.8914,
        "grad_norm": 1.5374834537506104,
        "learning_rate": 0.00018035225369437648,
        "epoch": 0.7355087429436872,
        "step": 5342
    },
    {
        "loss": 2.4538,
        "grad_norm": 1.3437366485595703,
        "learning_rate": 0.0001802746734602555,
        "epoch": 0.7356464270962412,
        "step": 5343
    },
    {
        "loss": 2.0594,
        "grad_norm": 1.5485953092575073,
        "learning_rate": 0.0001801969571181561,
        "epoch": 0.7357841112487953,
        "step": 5344
    },
    {
        "loss": 1.8866,
        "grad_norm": 1.6988929510116577,
        "learning_rate": 0.00018011910479984845,
        "epoch": 0.7359217954013493,
        "step": 5345
    },
    {
        "loss": 2.422,
        "grad_norm": 1.9264509677886963,
        "learning_rate": 0.0001800411166373335,
        "epoch": 0.7360594795539034,
        "step": 5346
    },
    {
        "loss": 2.2724,
        "grad_norm": 2.1600749492645264,
        "learning_rate": 0.00017996299276284237,
        "epoch": 0.7361971637064574,
        "step": 5347
    },
    {
        "loss": 1.2988,
        "grad_norm": 2.6164474487304688,
        "learning_rate": 0.0001798847333088362,
        "epoch": 0.7363348478590114,
        "step": 5348
    },
    {
        "loss": 2.1166,
        "grad_norm": 1.9733598232269287,
        "learning_rate": 0.0001798063384080062,
        "epoch": 0.7364725320115655,
        "step": 5349
    },
    {
        "loss": 1.8624,
        "grad_norm": 2.2448744773864746,
        "learning_rate": 0.00017972780819327312,
        "epoch": 0.7366102161641195,
        "step": 5350
    },
    {
        "loss": 2.0922,
        "grad_norm": 3.641456127166748,
        "learning_rate": 0.0001796491427977871,
        "epoch": 0.7367479003166736,
        "step": 5351
    },
    {
        "loss": 1.5497,
        "grad_norm": 3.232041358947754,
        "learning_rate": 0.00017957034235492763,
        "epoch": 0.7368855844692276,
        "step": 5352
    },
    {
        "loss": 2.0181,
        "grad_norm": 2.504436731338501,
        "learning_rate": 0.00017949140699830305,
        "epoch": 0.7370232686217816,
        "step": 5353
    },
    {
        "loss": 1.7601,
        "grad_norm": 2.723776340484619,
        "learning_rate": 0.00017941233686175049,
        "epoch": 0.7371609527743357,
        "step": 5354
    },
    {
        "loss": 1.8176,
        "grad_norm": 2.3569462299346924,
        "learning_rate": 0.00017933313207933559,
        "epoch": 0.7372986369268897,
        "step": 5355
    },
    {
        "loss": 2.0259,
        "grad_norm": 2.303743839263916,
        "learning_rate": 0.00017925379278535227,
        "epoch": 0.7374363210794438,
        "step": 5356
    },
    {
        "loss": 2.0618,
        "grad_norm": 2.0209407806396484,
        "learning_rate": 0.00017917431911432258,
        "epoch": 0.7375740052319978,
        "step": 5357
    },
    {
        "loss": 2.2036,
        "grad_norm": 2.0545859336853027,
        "learning_rate": 0.0001790947112009964,
        "epoch": 0.7377116893845518,
        "step": 5358
    },
    {
        "loss": 2.2476,
        "grad_norm": 1.890246033668518,
        "learning_rate": 0.0001790149691803512,
        "epoch": 0.7378493735371059,
        "step": 5359
    },
    {
        "loss": 0.9267,
        "grad_norm": 2.9519898891448975,
        "learning_rate": 0.0001789350931875917,
        "epoch": 0.7379870576896599,
        "step": 5360
    },
    {
        "loss": 2.1523,
        "grad_norm": 1.5421669483184814,
        "learning_rate": 0.00017885508335815012,
        "epoch": 0.738124741842214,
        "step": 5361
    },
    {
        "loss": 2.2961,
        "grad_norm": 1.6159158945083618,
        "learning_rate": 0.00017877493982768525,
        "epoch": 0.738262425994768,
        "step": 5362
    },
    {
        "loss": 1.4749,
        "grad_norm": 1.9000321626663208,
        "learning_rate": 0.00017869466273208275,
        "epoch": 0.7384001101473221,
        "step": 5363
    },
    {
        "loss": 2.446,
        "grad_norm": 2.166363000869751,
        "learning_rate": 0.00017861425220745472,
        "epoch": 0.7385377942998761,
        "step": 5364
    },
    {
        "loss": 2.6394,
        "grad_norm": 1.455568790435791,
        "learning_rate": 0.00017853370839013941,
        "epoch": 0.7386754784524301,
        "step": 5365
    },
    {
        "loss": 2.1602,
        "grad_norm": 1.348142385482788,
        "learning_rate": 0.0001784530314167011,
        "epoch": 0.7388131626049842,
        "step": 5366
    },
    {
        "loss": 1.7825,
        "grad_norm": 2.33186674118042,
        "learning_rate": 0.00017837222142393007,
        "epoch": 0.7389508467575382,
        "step": 5367
    },
    {
        "loss": 2.6936,
        "grad_norm": 1.7506999969482422,
        "learning_rate": 0.00017829127854884183,
        "epoch": 0.7390885309100923,
        "step": 5368
    },
    {
        "loss": 2.3991,
        "grad_norm": 1.1877892017364502,
        "learning_rate": 0.00017821020292867722,
        "epoch": 0.7392262150626463,
        "step": 5369
    },
    {
        "loss": 2.1058,
        "grad_norm": 1.7542243003845215,
        "learning_rate": 0.00017812899470090248,
        "epoch": 0.7393638992152003,
        "step": 5370
    },
    {
        "loss": 2.2993,
        "grad_norm": 2.515404224395752,
        "learning_rate": 0.00017804765400320828,
        "epoch": 0.7395015833677544,
        "step": 5371
    },
    {
        "loss": 2.0284,
        "grad_norm": 2.2560079097747803,
        "learning_rate": 0.00017796618097351013,
        "epoch": 0.7396392675203084,
        "step": 5372
    },
    {
        "loss": 2.253,
        "grad_norm": 1.5027230978012085,
        "learning_rate": 0.00017788457574994775,
        "epoch": 0.7397769516728625,
        "step": 5373
    },
    {
        "loss": 2.3543,
        "grad_norm": 2.217189073562622,
        "learning_rate": 0.00017780283847088527,
        "epoch": 0.7399146358254165,
        "step": 5374
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.2589640617370605,
        "learning_rate": 0.0001777209692749105,
        "epoch": 0.7400523199779705,
        "step": 5375
    },
    {
        "loss": 2.0868,
        "grad_norm": 1.790079951286316,
        "learning_rate": 0.00017763896830083487,
        "epoch": 0.7401900041305246,
        "step": 5376
    },
    {
        "loss": 1.3019,
        "grad_norm": 2.4978199005126953,
        "learning_rate": 0.00017755683568769352,
        "epoch": 0.7403276882830786,
        "step": 5377
    },
    {
        "loss": 1.4175,
        "grad_norm": 1.8096801042556763,
        "learning_rate": 0.00017747457157474457,
        "epoch": 0.7404653724356327,
        "step": 5378
    },
    {
        "loss": 2.1268,
        "grad_norm": 2.3845670223236084,
        "learning_rate": 0.00017739217610146909,
        "epoch": 0.7406030565881867,
        "step": 5379
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.5916002988815308,
        "learning_rate": 0.00017730964940757096,
        "epoch": 0.7407407407407407,
        "step": 5380
    },
    {
        "loss": 1.7717,
        "grad_norm": 1.4486099481582642,
        "learning_rate": 0.00017722699163297652,
        "epoch": 0.7408784248932948,
        "step": 5381
    },
    {
        "loss": 1.4497,
        "grad_norm": 1.3440483808517456,
        "learning_rate": 0.00017714420291783427,
        "epoch": 0.7410161090458488,
        "step": 5382
    },
    {
        "loss": 2.3793,
        "grad_norm": 1.8423705101013184,
        "learning_rate": 0.00017706128340251503,
        "epoch": 0.7411537931984029,
        "step": 5383
    },
    {
        "loss": 2.3464,
        "grad_norm": 3.6113336086273193,
        "learning_rate": 0.00017697823322761105,
        "epoch": 0.7412914773509569,
        "step": 5384
    },
    {
        "loss": 2.1994,
        "grad_norm": 1.8204221725463867,
        "learning_rate": 0.00017689505253393618,
        "epoch": 0.7414291615035109,
        "step": 5385
    },
    {
        "loss": 2.2748,
        "grad_norm": 2.3294172286987305,
        "learning_rate": 0.0001768117414625258,
        "epoch": 0.741566845656065,
        "step": 5386
    },
    {
        "loss": 2.0768,
        "grad_norm": 3.017885208129883,
        "learning_rate": 0.00017672830015463614,
        "epoch": 0.741704529808619,
        "step": 5387
    },
    {
        "loss": 2.2945,
        "grad_norm": 2.969574451446533,
        "learning_rate": 0.00017664472875174425,
        "epoch": 0.7418422139611731,
        "step": 5388
    },
    {
        "loss": 1.5831,
        "grad_norm": 1.8295668363571167,
        "learning_rate": 0.00017656102739554782,
        "epoch": 0.7419798981137271,
        "step": 5389
    },
    {
        "loss": 2.4775,
        "grad_norm": 1.5187289714813232,
        "learning_rate": 0.0001764771962279649,
        "epoch": 0.7421175822662811,
        "step": 5390
    },
    {
        "loss": 1.5906,
        "grad_norm": 1.9747626781463623,
        "learning_rate": 0.00017639323539113345,
        "epoch": 0.7422552664188352,
        "step": 5391
    },
    {
        "loss": 2.091,
        "grad_norm": 1.427337408065796,
        "learning_rate": 0.0001763091450274117,
        "epoch": 0.7423929505713892,
        "step": 5392
    },
    {
        "loss": 2.6222,
        "grad_norm": 1.6449706554412842,
        "learning_rate": 0.00017622492527937707,
        "epoch": 0.7425306347239433,
        "step": 5393
    },
    {
        "loss": 1.8661,
        "grad_norm": 1.8528258800506592,
        "learning_rate": 0.00017614057628982646,
        "epoch": 0.7426683188764973,
        "step": 5394
    },
    {
        "loss": 2.0124,
        "grad_norm": 2.717578887939453,
        "learning_rate": 0.00017605609820177617,
        "epoch": 0.7428060030290513,
        "step": 5395
    },
    {
        "loss": 1.9109,
        "grad_norm": 2.180738687515259,
        "learning_rate": 0.0001759714911584611,
        "epoch": 0.7429436871816054,
        "step": 5396
    },
    {
        "loss": 2.435,
        "grad_norm": 1.9906553030014038,
        "learning_rate": 0.00017588675530333476,
        "epoch": 0.7430813713341594,
        "step": 5397
    },
    {
        "loss": 2.2459,
        "grad_norm": 1.9938551187515259,
        "learning_rate": 0.00017580189078006942,
        "epoch": 0.7432190554867135,
        "step": 5398
    },
    {
        "loss": 2.4666,
        "grad_norm": 1.526175856590271,
        "learning_rate": 0.00017571689773255512,
        "epoch": 0.7433567396392675,
        "step": 5399
    },
    {
        "loss": 1.8955,
        "grad_norm": 2.2244508266448975,
        "learning_rate": 0.00017563177630489993,
        "epoch": 0.7434944237918215,
        "step": 5400
    },
    {
        "loss": 1.4476,
        "grad_norm": 2.3951926231384277,
        "learning_rate": 0.00017554652664142982,
        "epoch": 0.7436321079443756,
        "step": 5401
    },
    {
        "loss": 2.3106,
        "grad_norm": 1.460619568824768,
        "learning_rate": 0.00017546114888668788,
        "epoch": 0.7437697920969296,
        "step": 5402
    },
    {
        "loss": 1.2206,
        "grad_norm": 3.575068950653076,
        "learning_rate": 0.00017537564318543454,
        "epoch": 0.7439074762494837,
        "step": 5403
    },
    {
        "loss": 2.0794,
        "grad_norm": 2.3288655281066895,
        "learning_rate": 0.0001752900096826471,
        "epoch": 0.7440451604020377,
        "step": 5404
    },
    {
        "loss": 1.5769,
        "grad_norm": 2.558990240097046,
        "learning_rate": 0.00017520424852351965,
        "epoch": 0.7441828445545917,
        "step": 5405
    },
    {
        "loss": 1.8198,
        "grad_norm": 2.2243354320526123,
        "learning_rate": 0.00017511835985346247,
        "epoch": 0.7443205287071458,
        "step": 5406
    },
    {
        "loss": 2.0873,
        "grad_norm": 1.880822777748108,
        "learning_rate": 0.00017503234381810253,
        "epoch": 0.7444582128596998,
        "step": 5407
    },
    {
        "loss": 2.3502,
        "grad_norm": 1.9522253274917603,
        "learning_rate": 0.00017494620056328225,
        "epoch": 0.7445958970122539,
        "step": 5408
    },
    {
        "loss": 1.0523,
        "grad_norm": 2.945996046066284,
        "learning_rate": 0.00017485993023505994,
        "epoch": 0.7447335811648079,
        "step": 5409
    },
    {
        "loss": 1.7073,
        "grad_norm": 1.8371124267578125,
        "learning_rate": 0.00017477353297970952,
        "epoch": 0.7448712653173619,
        "step": 5410
    },
    {
        "loss": 2.0213,
        "grad_norm": 1.4994643926620483,
        "learning_rate": 0.00017468700894371987,
        "epoch": 0.745008949469916,
        "step": 5411
    },
    {
        "loss": 1.3984,
        "grad_norm": 2.5239741802215576,
        "learning_rate": 0.000174600358273795,
        "epoch": 0.74514663362247,
        "step": 5412
    },
    {
        "loss": 2.3836,
        "grad_norm": 1.363838791847229,
        "learning_rate": 0.00017451358111685353,
        "epoch": 0.7452843177750241,
        "step": 5413
    },
    {
        "loss": 1.7768,
        "grad_norm": 1.9534554481506348,
        "learning_rate": 0.0001744266776200286,
        "epoch": 0.7454220019275781,
        "step": 5414
    },
    {
        "loss": 2.5417,
        "grad_norm": 2.1374006271362305,
        "learning_rate": 0.00017433964793066745,
        "epoch": 0.7455596860801321,
        "step": 5415
    },
    {
        "loss": 1.4058,
        "grad_norm": 2.9822604656219482,
        "learning_rate": 0.00017425249219633153,
        "epoch": 0.7456973702326862,
        "step": 5416
    },
    {
        "loss": 2.721,
        "grad_norm": 1.7858906984329224,
        "learning_rate": 0.00017416521056479582,
        "epoch": 0.7458350543852402,
        "step": 5417
    },
    {
        "loss": 1.631,
        "grad_norm": 2.3848438262939453,
        "learning_rate": 0.00017407780318404864,
        "epoch": 0.7459727385377943,
        "step": 5418
    },
    {
        "loss": 0.9973,
        "grad_norm": 3.6137330532073975,
        "learning_rate": 0.00017399027020229192,
        "epoch": 0.7461104226903483,
        "step": 5419
    },
    {
        "loss": 1.5562,
        "grad_norm": 2.43520450592041,
        "learning_rate": 0.00017390261176794018,
        "epoch": 0.7462481068429024,
        "step": 5420
    },
    {
        "loss": 1.3352,
        "grad_norm": 3.3189432621002197,
        "learning_rate": 0.00017381482802962065,
        "epoch": 0.7463857909954564,
        "step": 5421
    },
    {
        "loss": 2.1385,
        "grad_norm": 1.9726614952087402,
        "learning_rate": 0.00017372691913617342,
        "epoch": 0.7465234751480104,
        "step": 5422
    },
    {
        "loss": 2.4516,
        "grad_norm": 1.5750768184661865,
        "learning_rate": 0.0001736388852366503,
        "epoch": 0.7466611593005645,
        "step": 5423
    },
    {
        "loss": 1.2508,
        "grad_norm": 2.0384483337402344,
        "learning_rate": 0.00017355072648031524,
        "epoch": 0.7467988434531185,
        "step": 5424
    },
    {
        "loss": 2.206,
        "grad_norm": 1.9529415369033813,
        "learning_rate": 0.00017346244301664405,
        "epoch": 0.7469365276056726,
        "step": 5425
    },
    {
        "loss": 2.3357,
        "grad_norm": 2.097731113433838,
        "learning_rate": 0.00017337403499532376,
        "epoch": 0.7470742117582266,
        "step": 5426
    },
    {
        "loss": 2.1414,
        "grad_norm": 1.5208282470703125,
        "learning_rate": 0.00017328550256625266,
        "epoch": 0.7472118959107806,
        "step": 5427
    },
    {
        "loss": 1.9754,
        "grad_norm": 1.920979380607605,
        "learning_rate": 0.00017319684587954002,
        "epoch": 0.7473495800633347,
        "step": 5428
    },
    {
        "loss": 1.4076,
        "grad_norm": 2.3475356101989746,
        "learning_rate": 0.00017310806508550572,
        "epoch": 0.7474872642158887,
        "step": 5429
    },
    {
        "loss": 2.0326,
        "grad_norm": 2.28242564201355,
        "learning_rate": 0.00017301916033468003,
        "epoch": 0.7476249483684428,
        "step": 5430
    },
    {
        "loss": 1.9054,
        "grad_norm": 1.7179688215255737,
        "learning_rate": 0.00017293013177780366,
        "epoch": 0.7477626325209968,
        "step": 5431
    },
    {
        "loss": 1.4366,
        "grad_norm": 1.1728681325912476,
        "learning_rate": 0.00017284097956582692,
        "epoch": 0.7479003166735508,
        "step": 5432
    },
    {
        "loss": 1.8303,
        "grad_norm": 1.4662138223648071,
        "learning_rate": 0.00017275170384990987,
        "epoch": 0.7480380008261049,
        "step": 5433
    },
    {
        "loss": 1.2531,
        "grad_norm": 2.822676658630371,
        "learning_rate": 0.00017266230478142215,
        "epoch": 0.7481756849786589,
        "step": 5434
    },
    {
        "loss": 2.0104,
        "grad_norm": 2.208301544189453,
        "learning_rate": 0.00017257278251194233,
        "epoch": 0.748313369131213,
        "step": 5435
    },
    {
        "loss": 2.5775,
        "grad_norm": 2.1250216960906982,
        "learning_rate": 0.00017248313719325794,
        "epoch": 0.748451053283767,
        "step": 5436
    },
    {
        "loss": 2.3075,
        "grad_norm": 1.7257839441299438,
        "learning_rate": 0.00017239336897736517,
        "epoch": 0.748588737436321,
        "step": 5437
    },
    {
        "loss": 1.7096,
        "grad_norm": 2.372300386428833,
        "learning_rate": 0.00017230347801646856,
        "epoch": 0.7487264215888751,
        "step": 5438
    },
    {
        "loss": 1.2715,
        "grad_norm": 3.7704520225524902,
        "learning_rate": 0.0001722134644629807,
        "epoch": 0.7488641057414291,
        "step": 5439
    },
    {
        "loss": 1.4766,
        "grad_norm": 3.0152831077575684,
        "learning_rate": 0.00017212332846952224,
        "epoch": 0.7490017898939832,
        "step": 5440
    },
    {
        "loss": 2.1588,
        "grad_norm": 2.4804792404174805,
        "learning_rate": 0.00017203307018892128,
        "epoch": 0.7491394740465372,
        "step": 5441
    },
    {
        "loss": 1.7866,
        "grad_norm": 2.3082473278045654,
        "learning_rate": 0.00017194268977421316,
        "epoch": 0.7492771581990912,
        "step": 5442
    },
    {
        "loss": 2.1638,
        "grad_norm": 3.0463430881500244,
        "learning_rate": 0.00017185218737864063,
        "epoch": 0.7494148423516453,
        "step": 5443
    },
    {
        "loss": 1.5507,
        "grad_norm": 2.309509754180908,
        "learning_rate": 0.00017176156315565293,
        "epoch": 0.7495525265041993,
        "step": 5444
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.3340139389038086,
        "learning_rate": 0.00017167081725890596,
        "epoch": 0.7496902106567535,
        "step": 5445
    },
    {
        "loss": 2.3421,
        "grad_norm": 1.9248193502426147,
        "learning_rate": 0.0001715799498422621,
        "epoch": 0.7498278948093074,
        "step": 5446
    },
    {
        "loss": 2.0341,
        "grad_norm": 2.036389112472534,
        "learning_rate": 0.00017148896105978953,
        "epoch": 0.7499655789618614,
        "step": 5447
    },
    {
        "loss": 2.0876,
        "grad_norm": 2.227698802947998,
        "learning_rate": 0.00017139785106576224,
        "epoch": 0.7501032631144156,
        "step": 5448
    },
    {
        "loss": 1.0173,
        "grad_norm": 4.825929164886475,
        "learning_rate": 0.00017130662001465995,
        "epoch": 0.7502409472669695,
        "step": 5449
    },
    {
        "loss": 1.7462,
        "grad_norm": 1.9343680143356323,
        "learning_rate": 0.00017121526806116743,
        "epoch": 0.7503786314195237,
        "step": 5450
    },
    {
        "loss": 1.6954,
        "grad_norm": 2.7902872562408447,
        "learning_rate": 0.00017112379536017447,
        "epoch": 0.7505163155720777,
        "step": 5451
    },
    {
        "loss": 2.1348,
        "grad_norm": 2.0420923233032227,
        "learning_rate": 0.0001710322020667756,
        "epoch": 0.7506539997246316,
        "step": 5452
    },
    {
        "loss": 1.4939,
        "grad_norm": 2.1294105052948,
        "learning_rate": 0.00017094048833626993,
        "epoch": 0.7507916838771858,
        "step": 5453
    },
    {
        "loss": 1.94,
        "grad_norm": 1.8447376489639282,
        "learning_rate": 0.00017084865432416047,
        "epoch": 0.7509293680297398,
        "step": 5454
    },
    {
        "loss": 2.3538,
        "grad_norm": 1.9498157501220703,
        "learning_rate": 0.00017075670018615465,
        "epoch": 0.7510670521822939,
        "step": 5455
    },
    {
        "loss": 1.1496,
        "grad_norm": 4.038906097412109,
        "learning_rate": 0.0001706646260781632,
        "epoch": 0.7512047363348479,
        "step": 5456
    },
    {
        "loss": 1.9195,
        "grad_norm": 2.6553268432617188,
        "learning_rate": 0.00017057243215630038,
        "epoch": 0.7513424204874019,
        "step": 5457
    },
    {
        "loss": 1.9572,
        "grad_norm": 1.6073230504989624,
        "learning_rate": 0.00017048011857688348,
        "epoch": 0.751480104639956,
        "step": 5458
    },
    {
        "loss": 2.117,
        "grad_norm": 1.4029524326324463,
        "learning_rate": 0.00017038768549643298,
        "epoch": 0.75161778879251,
        "step": 5459
    },
    {
        "loss": 1.813,
        "grad_norm": 2.456766366958618,
        "learning_rate": 0.00017029513307167165,
        "epoch": 0.7517554729450641,
        "step": 5460
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.319843053817749,
        "learning_rate": 0.00017020246145952483,
        "epoch": 0.7518931570976181,
        "step": 5461
    },
    {
        "loss": 0.9473,
        "grad_norm": 2.658173084259033,
        "learning_rate": 0.00017010967081711978,
        "epoch": 0.752030841250172,
        "step": 5462
    },
    {
        "loss": 1.4463,
        "grad_norm": 2.066915988922119,
        "learning_rate": 0.0001700167613017857,
        "epoch": 0.7521685254027262,
        "step": 5463
    },
    {
        "loss": 2.2218,
        "grad_norm": 1.8541171550750732,
        "learning_rate": 0.00016992373307105318,
        "epoch": 0.7523062095552802,
        "step": 5464
    },
    {
        "loss": 1.1806,
        "grad_norm": 2.255204439163208,
        "learning_rate": 0.00016983058628265442,
        "epoch": 0.7524438937078343,
        "step": 5465
    },
    {
        "loss": 2.17,
        "grad_norm": 1.8930310010910034,
        "learning_rate": 0.00016973732109452233,
        "epoch": 0.7525815778603883,
        "step": 5466
    },
    {
        "loss": 1.9983,
        "grad_norm": 1.5915026664733887,
        "learning_rate": 0.0001696439376647906,
        "epoch": 0.7527192620129423,
        "step": 5467
    },
    {
        "loss": 2.4927,
        "grad_norm": 1.8849953413009644,
        "learning_rate": 0.0001695504361517936,
        "epoch": 0.7528569461654964,
        "step": 5468
    },
    {
        "loss": 1.9699,
        "grad_norm": 1.9020599126815796,
        "learning_rate": 0.0001694568167140658,
        "epoch": 0.7529946303180504,
        "step": 5469
    },
    {
        "loss": 2.168,
        "grad_norm": 1.482457160949707,
        "learning_rate": 0.0001693630795103414,
        "epoch": 0.7531323144706045,
        "step": 5470
    },
    {
        "loss": 1.7489,
        "grad_norm": 1.994707703590393,
        "learning_rate": 0.00016926922469955472,
        "epoch": 0.7532699986231585,
        "step": 5471
    },
    {
        "loss": 2.143,
        "grad_norm": 3.2522475719451904,
        "learning_rate": 0.0001691752524408392,
        "epoch": 0.7534076827757125,
        "step": 5472
    },
    {
        "loss": 1.9993,
        "grad_norm": 3.9866154193878174,
        "learning_rate": 0.00016908116289352727,
        "epoch": 0.7535453669282666,
        "step": 5473
    },
    {
        "loss": 1.4243,
        "grad_norm": 4.095659255981445,
        "learning_rate": 0.00016898695621715067,
        "epoch": 0.7536830510808206,
        "step": 5474
    },
    {
        "loss": 1.972,
        "grad_norm": 1.6645779609680176,
        "learning_rate": 0.00016889263257143937,
        "epoch": 0.7538207352333747,
        "step": 5475
    },
    {
        "loss": 0.7657,
        "grad_norm": 2.7546207904815674,
        "learning_rate": 0.00016879819211632177,
        "epoch": 0.7539584193859287,
        "step": 5476
    },
    {
        "loss": 2.3153,
        "grad_norm": 1.7210133075714111,
        "learning_rate": 0.00016870363501192434,
        "epoch": 0.7540961035384828,
        "step": 5477
    },
    {
        "loss": 1.5791,
        "grad_norm": 2.3395025730133057,
        "learning_rate": 0.00016860896141857135,
        "epoch": 0.7542337876910368,
        "step": 5478
    },
    {
        "loss": 2.2051,
        "grad_norm": 1.39772367477417,
        "learning_rate": 0.00016851417149678442,
        "epoch": 0.7543714718435908,
        "step": 5479
    },
    {
        "loss": 1.6298,
        "grad_norm": 2.5116381645202637,
        "learning_rate": 0.00016841926540728274,
        "epoch": 0.7545091559961449,
        "step": 5480
    },
    {
        "loss": 2.4299,
        "grad_norm": 2.239759922027588,
        "learning_rate": 0.00016832424331098217,
        "epoch": 0.7546468401486989,
        "step": 5481
    },
    {
        "loss": 2.2202,
        "grad_norm": 1.2909891605377197,
        "learning_rate": 0.00016822910536899529,
        "epoch": 0.754784524301253,
        "step": 5482
    },
    {
        "loss": 1.7188,
        "grad_norm": 1.8632572889328003,
        "learning_rate": 0.00016813385174263134,
        "epoch": 0.754922208453807,
        "step": 5483
    },
    {
        "loss": 2.2198,
        "grad_norm": 2.2062790393829346,
        "learning_rate": 0.00016803848259339544,
        "epoch": 0.755059892606361,
        "step": 5484
    },
    {
        "loss": 2.0962,
        "grad_norm": 2.0342795848846436,
        "learning_rate": 0.0001679429980829887,
        "epoch": 0.7551975767589151,
        "step": 5485
    },
    {
        "loss": 1.5248,
        "grad_norm": 1.617535948753357,
        "learning_rate": 0.0001678473983733078,
        "epoch": 0.7553352609114691,
        "step": 5486
    },
    {
        "loss": 2.0176,
        "grad_norm": 1.5805851221084595,
        "learning_rate": 0.00016775168362644473,
        "epoch": 0.7554729450640232,
        "step": 5487
    },
    {
        "loss": 1.3214,
        "grad_norm": 3.3522021770477295,
        "learning_rate": 0.00016765585400468647,
        "epoch": 0.7556106292165772,
        "step": 5488
    },
    {
        "loss": 2.3368,
        "grad_norm": 1.8162307739257812,
        "learning_rate": 0.00016755990967051503,
        "epoch": 0.7557483133691312,
        "step": 5489
    },
    {
        "loss": 1.949,
        "grad_norm": 2.4302890300750732,
        "learning_rate": 0.00016746385078660668,
        "epoch": 0.7558859975216853,
        "step": 5490
    },
    {
        "loss": 1.207,
        "grad_norm": 2.877112865447998,
        "learning_rate": 0.00016736767751583185,
        "epoch": 0.7560236816742393,
        "step": 5491
    },
    {
        "loss": 1.5518,
        "grad_norm": 2.1339120864868164,
        "learning_rate": 0.0001672713900212552,
        "epoch": 0.7561613658267934,
        "step": 5492
    },
    {
        "loss": 1.7245,
        "grad_norm": 1.6583480834960938,
        "learning_rate": 0.0001671749884661349,
        "epoch": 0.7562990499793474,
        "step": 5493
    },
    {
        "loss": 1.9801,
        "grad_norm": 2.0206594467163086,
        "learning_rate": 0.00016707847301392233,
        "epoch": 0.7564367341319014,
        "step": 5494
    },
    {
        "loss": 1.5266,
        "grad_norm": 2.765923261642456,
        "learning_rate": 0.00016698184382826236,
        "epoch": 0.7565744182844555,
        "step": 5495
    },
    {
        "loss": 1.7594,
        "grad_norm": 1.808099389076233,
        "learning_rate": 0.00016688510107299248,
        "epoch": 0.7567121024370095,
        "step": 5496
    },
    {
        "loss": 2.5062,
        "grad_norm": 2.524714708328247,
        "learning_rate": 0.00016678824491214256,
        "epoch": 0.7568497865895636,
        "step": 5497
    },
    {
        "loss": 2.1084,
        "grad_norm": 1.6783339977264404,
        "learning_rate": 0.0001666912755099353,
        "epoch": 0.7569874707421176,
        "step": 5498
    },
    {
        "loss": 2.1684,
        "grad_norm": 1.4103920459747314,
        "learning_rate": 0.00016659419303078482,
        "epoch": 0.7571251548946716,
        "step": 5499
    },
    {
        "loss": 0.8201,
        "grad_norm": 2.7503950595855713,
        "learning_rate": 0.0001664969976392973,
        "epoch": 0.7572628390472257,
        "step": 5500
    },
    {
        "loss": 2.3402,
        "grad_norm": 2.773879051208496,
        "learning_rate": 0.00016639968950027024,
        "epoch": 0.7574005231997797,
        "step": 5501
    },
    {
        "loss": 2.7042,
        "grad_norm": 2.4712109565734863,
        "learning_rate": 0.00016630226877869237,
        "epoch": 0.7575382073523338,
        "step": 5502
    },
    {
        "loss": 1.4575,
        "grad_norm": 4.268956184387207,
        "learning_rate": 0.00016620473563974311,
        "epoch": 0.7576758915048878,
        "step": 5503
    },
    {
        "loss": 2.1826,
        "grad_norm": 1.7396125793457031,
        "learning_rate": 0.00016610709024879295,
        "epoch": 0.7578135756574418,
        "step": 5504
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.8664982318878174,
        "learning_rate": 0.00016600933277140226,
        "epoch": 0.7579512598099959,
        "step": 5505
    },
    {
        "loss": 1.2369,
        "grad_norm": 1.8200311660766602,
        "learning_rate": 0.00016591146337332146,
        "epoch": 0.7580889439625499,
        "step": 5506
    },
    {
        "loss": 1.2648,
        "grad_norm": 3.107175827026367,
        "learning_rate": 0.0001658134822204912,
        "epoch": 0.758226628115104,
        "step": 5507
    },
    {
        "loss": 1.7122,
        "grad_norm": 2.301945209503174,
        "learning_rate": 0.00016571538947904104,
        "epoch": 0.758364312267658,
        "step": 5508
    },
    {
        "loss": 1.8417,
        "grad_norm": 2.147840976715088,
        "learning_rate": 0.00016561718531529014,
        "epoch": 0.758501996420212,
        "step": 5509
    },
    {
        "loss": 2.3222,
        "grad_norm": 1.2710150480270386,
        "learning_rate": 0.00016551886989574636,
        "epoch": 0.7586396805727661,
        "step": 5510
    },
    {
        "loss": 1.8413,
        "grad_norm": 1.6063076257705688,
        "learning_rate": 0.00016542044338710633,
        "epoch": 0.7587773647253201,
        "step": 5511
    },
    {
        "loss": 1.4891,
        "grad_norm": 2.719968557357788,
        "learning_rate": 0.0001653219059562548,
        "epoch": 0.7589150488778742,
        "step": 5512
    },
    {
        "loss": 2.5094,
        "grad_norm": 1.8802765607833862,
        "learning_rate": 0.00016522325777026497,
        "epoch": 0.7590527330304282,
        "step": 5513
    },
    {
        "loss": 1.9882,
        "grad_norm": 1.3368796110153198,
        "learning_rate": 0.00016512449899639764,
        "epoch": 0.7591904171829822,
        "step": 5514
    },
    {
        "loss": 2.3479,
        "grad_norm": 1.249955177307129,
        "learning_rate": 0.00016502562980210087,
        "epoch": 0.7593281013355363,
        "step": 5515
    },
    {
        "loss": 2.0148,
        "grad_norm": 2.6928036212921143,
        "learning_rate": 0.00016492665035501046,
        "epoch": 0.7594657854880903,
        "step": 5516
    },
    {
        "loss": 1.9186,
        "grad_norm": 2.5380349159240723,
        "learning_rate": 0.00016482756082294875,
        "epoch": 0.7596034696406444,
        "step": 5517
    },
    {
        "loss": 2.3746,
        "grad_norm": 1.6744590997695923,
        "learning_rate": 0.00016472836137392468,
        "epoch": 0.7597411537931984,
        "step": 5518
    },
    {
        "loss": 2.1979,
        "grad_norm": 1.7637953758239746,
        "learning_rate": 0.00016462905217613403,
        "epoch": 0.7598788379457524,
        "step": 5519
    },
    {
        "loss": 2.2394,
        "grad_norm": 2.2107791900634766,
        "learning_rate": 0.00016452963339795817,
        "epoch": 0.7600165220983065,
        "step": 5520
    },
    {
        "loss": 1.318,
        "grad_norm": 2.4519972801208496,
        "learning_rate": 0.00016443010520796435,
        "epoch": 0.7601542062508605,
        "step": 5521
    },
    {
        "loss": 1.2671,
        "grad_norm": 1.628630518913269,
        "learning_rate": 0.00016433046777490566,
        "epoch": 0.7602918904034146,
        "step": 5522
    },
    {
        "loss": 2.2548,
        "grad_norm": 1.7588828802108765,
        "learning_rate": 0.0001642307212677201,
        "epoch": 0.7604295745559686,
        "step": 5523
    },
    {
        "loss": 2.1494,
        "grad_norm": 2.0760679244995117,
        "learning_rate": 0.00016413086585553063,
        "epoch": 0.7605672587085226,
        "step": 5524
    },
    {
        "loss": 2.0009,
        "grad_norm": 3.959930896759033,
        "learning_rate": 0.000164030901707645,
        "epoch": 0.7607049428610767,
        "step": 5525
    },
    {
        "loss": 2.255,
        "grad_norm": 2.1611180305480957,
        "learning_rate": 0.0001639308289935552,
        "epoch": 0.7608426270136307,
        "step": 5526
    },
    {
        "loss": 2.1774,
        "grad_norm": 2.059964656829834,
        "learning_rate": 0.00016383064788293725,
        "epoch": 0.7609803111661848,
        "step": 5527
    },
    {
        "loss": 2.327,
        "grad_norm": 2.6193180084228516,
        "learning_rate": 0.00016373035854565126,
        "epoch": 0.7611179953187388,
        "step": 5528
    },
    {
        "loss": 1.978,
        "grad_norm": 2.704348087310791,
        "learning_rate": 0.00016362996115174057,
        "epoch": 0.7612556794712928,
        "step": 5529
    },
    {
        "loss": 1.5594,
        "grad_norm": 2.9952335357666016,
        "learning_rate": 0.00016352945587143163,
        "epoch": 0.7613933636238469,
        "step": 5530
    },
    {
        "loss": 1.9031,
        "grad_norm": 3.113987922668457,
        "learning_rate": 0.00016342884287513424,
        "epoch": 0.7615310477764009,
        "step": 5531
    },
    {
        "loss": 2.7518,
        "grad_norm": 1.8268635272979736,
        "learning_rate": 0.00016332812233344046,
        "epoch": 0.761668731928955,
        "step": 5532
    },
    {
        "loss": 1.9814,
        "grad_norm": 2.645327091217041,
        "learning_rate": 0.00016322729441712482,
        "epoch": 0.761806416081509,
        "step": 5533
    },
    {
        "loss": 1.201,
        "grad_norm": 1.5710371732711792,
        "learning_rate": 0.00016312635929714394,
        "epoch": 0.761944100234063,
        "step": 5534
    },
    {
        "loss": 1.7173,
        "grad_norm": 1.5430656671524048,
        "learning_rate": 0.0001630253171446361,
        "epoch": 0.7620817843866171,
        "step": 5535
    },
    {
        "loss": 2.1408,
        "grad_norm": 2.323634386062622,
        "learning_rate": 0.00016292416813092105,
        "epoch": 0.7622194685391711,
        "step": 5536
    },
    {
        "loss": 1.7343,
        "grad_norm": 2.9486136436462402,
        "learning_rate": 0.00016282291242750003,
        "epoch": 0.7623571526917252,
        "step": 5537
    },
    {
        "loss": 2.3771,
        "grad_norm": 1.8666560649871826,
        "learning_rate": 0.00016272155020605482,
        "epoch": 0.7624948368442792,
        "step": 5538
    },
    {
        "loss": 2.1207,
        "grad_norm": 2.368881940841675,
        "learning_rate": 0.00016262008163844783,
        "epoch": 0.7626325209968333,
        "step": 5539
    },
    {
        "loss": 2.5053,
        "grad_norm": 2.139871597290039,
        "learning_rate": 0.0001625185068967221,
        "epoch": 0.7627702051493873,
        "step": 5540
    },
    {
        "loss": 2.0307,
        "grad_norm": 1.2204047441482544,
        "learning_rate": 0.0001624168261531004,
        "epoch": 0.7629078893019413,
        "step": 5541
    },
    {
        "loss": 2.5097,
        "grad_norm": 1.0946071147918701,
        "learning_rate": 0.0001623150395799853,
        "epoch": 0.7630455734544954,
        "step": 5542
    },
    {
        "loss": 2.0709,
        "grad_norm": 1.8618865013122559,
        "learning_rate": 0.00016221314734995862,
        "epoch": 0.7631832576070494,
        "step": 5543
    },
    {
        "loss": 2.0587,
        "grad_norm": 1.9974825382232666,
        "learning_rate": 0.0001621111496357818,
        "epoch": 0.7633209417596035,
        "step": 5544
    },
    {
        "loss": 2.0513,
        "grad_norm": 2.788567543029785,
        "learning_rate": 0.00016200904661039478,
        "epoch": 0.7634586259121575,
        "step": 5545
    },
    {
        "loss": 1.4945,
        "grad_norm": 1.6116962432861328,
        "learning_rate": 0.00016190683844691597,
        "epoch": 0.7635963100647115,
        "step": 5546
    },
    {
        "loss": 1.888,
        "grad_norm": 2.968615770339966,
        "learning_rate": 0.00016180452531864238,
        "epoch": 0.7637339942172656,
        "step": 5547
    },
    {
        "loss": 1.9083,
        "grad_norm": 2.6781952381134033,
        "learning_rate": 0.00016170210739904878,
        "epoch": 0.7638716783698196,
        "step": 5548
    },
    {
        "loss": 1.6834,
        "grad_norm": 1.1627651453018188,
        "learning_rate": 0.0001615995848617876,
        "epoch": 0.7640093625223737,
        "step": 5549
    },
    {
        "loss": 2.3087,
        "grad_norm": 1.1810996532440186,
        "learning_rate": 0.00016149695788068874,
        "epoch": 0.7641470466749277,
        "step": 5550
    },
    {
        "loss": 1.7983,
        "grad_norm": 1.9402885437011719,
        "learning_rate": 0.00016139422662975905,
        "epoch": 0.7642847308274817,
        "step": 5551
    },
    {
        "loss": 2.1164,
        "grad_norm": 2.1046597957611084,
        "learning_rate": 0.0001612913912831823,
        "epoch": 0.7644224149800358,
        "step": 5552
    },
    {
        "loss": 2.4797,
        "grad_norm": 2.464294195175171,
        "learning_rate": 0.00016118845201531885,
        "epoch": 0.7645600991325898,
        "step": 5553
    },
    {
        "loss": 1.3321,
        "grad_norm": 3.086251735687256,
        "learning_rate": 0.00016108540900070507,
        "epoch": 0.7646977832851439,
        "step": 5554
    },
    {
        "loss": 2.0422,
        "grad_norm": 1.2786784172058105,
        "learning_rate": 0.00016098226241405316,
        "epoch": 0.7648354674376979,
        "step": 5555
    },
    {
        "loss": 1.2263,
        "grad_norm": 1.806975245475769,
        "learning_rate": 0.00016087901243025136,
        "epoch": 0.7649731515902519,
        "step": 5556
    },
    {
        "loss": 1.9666,
        "grad_norm": 2.1143107414245605,
        "learning_rate": 0.00016077565922436275,
        "epoch": 0.765110835742806,
        "step": 5557
    },
    {
        "loss": 1.4679,
        "grad_norm": 3.846588373184204,
        "learning_rate": 0.00016067220297162567,
        "epoch": 0.76524851989536,
        "step": 5558
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.1316368579864502,
        "learning_rate": 0.0001605686438474531,
        "epoch": 0.7653862040479141,
        "step": 5559
    },
    {
        "loss": 1.7302,
        "grad_norm": 1.794156789779663,
        "learning_rate": 0.00016046498202743244,
        "epoch": 0.7655238882004681,
        "step": 5560
    },
    {
        "loss": 1.5155,
        "grad_norm": 2.8157589435577393,
        "learning_rate": 0.00016036121768732512,
        "epoch": 0.7656615723530221,
        "step": 5561
    },
    {
        "loss": 1.6163,
        "grad_norm": 1.8121414184570312,
        "learning_rate": 0.0001602573510030668,
        "epoch": 0.7657992565055762,
        "step": 5562
    },
    {
        "loss": 2.2157,
        "grad_norm": 1.9666731357574463,
        "learning_rate": 0.00016015338215076623,
        "epoch": 0.7659369406581302,
        "step": 5563
    },
    {
        "loss": 2.034,
        "grad_norm": 1.734731912612915,
        "learning_rate": 0.00016004931130670543,
        "epoch": 0.7660746248106843,
        "step": 5564
    },
    {
        "loss": 2.39,
        "grad_norm": 1.8869903087615967,
        "learning_rate": 0.00015994513864733964,
        "epoch": 0.7662123089632383,
        "step": 5565
    },
    {
        "loss": 2.3678,
        "grad_norm": 2.151292562484741,
        "learning_rate": 0.0001598408643492965,
        "epoch": 0.7663499931157923,
        "step": 5566
    },
    {
        "loss": 1.7174,
        "grad_norm": 2.0138466358184814,
        "learning_rate": 0.0001597364885893759,
        "epoch": 0.7664876772683464,
        "step": 5567
    },
    {
        "loss": 1.6498,
        "grad_norm": 3.3240058422088623,
        "learning_rate": 0.0001596320115445501,
        "epoch": 0.7666253614209004,
        "step": 5568
    },
    {
        "loss": 1.7462,
        "grad_norm": 2.7847414016723633,
        "learning_rate": 0.0001595274333919628,
        "epoch": 0.7667630455734545,
        "step": 5569
    },
    {
        "loss": 2.4428,
        "grad_norm": 1.956358790397644,
        "learning_rate": 0.00015942275430892906,
        "epoch": 0.7669007297260085,
        "step": 5570
    },
    {
        "loss": 2.1581,
        "grad_norm": 2.645583391189575,
        "learning_rate": 0.00015931797447293547,
        "epoch": 0.7670384138785625,
        "step": 5571
    },
    {
        "loss": 2.1016,
        "grad_norm": 2.036824941635132,
        "learning_rate": 0.0001592130940616391,
        "epoch": 0.7671760980311166,
        "step": 5572
    },
    {
        "loss": 1.9766,
        "grad_norm": 2.862516403198242,
        "learning_rate": 0.00015910811325286768,
        "epoch": 0.7673137821836706,
        "step": 5573
    },
    {
        "loss": 2.192,
        "grad_norm": 2.2533750534057617,
        "learning_rate": 0.0001590030322246191,
        "epoch": 0.7674514663362247,
        "step": 5574
    },
    {
        "loss": 1.4443,
        "grad_norm": 3.165104866027832,
        "learning_rate": 0.00015889785115506117,
        "epoch": 0.7675891504887787,
        "step": 5575
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.9229440689086914,
        "learning_rate": 0.00015879257022253132,
        "epoch": 0.7677268346413327,
        "step": 5576
    },
    {
        "loss": 2.6284,
        "grad_norm": 2.1448731422424316,
        "learning_rate": 0.0001586871896055366,
        "epoch": 0.7678645187938868,
        "step": 5577
    },
    {
        "loss": 2.0552,
        "grad_norm": 3.023716688156128,
        "learning_rate": 0.00015858170948275264,
        "epoch": 0.7680022029464408,
        "step": 5578
    },
    {
        "loss": 2.559,
        "grad_norm": 1.5034099817276,
        "learning_rate": 0.00015847613003302388,
        "epoch": 0.768139887098995,
        "step": 5579
    },
    {
        "loss": 1.0927,
        "grad_norm": 2.44897723197937,
        "learning_rate": 0.0001583704514353635,
        "epoch": 0.7682775712515489,
        "step": 5580
    },
    {
        "loss": 1.2224,
        "grad_norm": 2.8167200088500977,
        "learning_rate": 0.00015826467386895247,
        "epoch": 0.7684152554041029,
        "step": 5581
    },
    {
        "loss": 2.3058,
        "grad_norm": 1.8771281242370605,
        "learning_rate": 0.00015815879751313955,
        "epoch": 0.768552939556657,
        "step": 5582
    },
    {
        "loss": 2.2013,
        "grad_norm": 2.0037503242492676,
        "learning_rate": 0.0001580528225474412,
        "epoch": 0.768690623709211,
        "step": 5583
    },
    {
        "loss": 1.5715,
        "grad_norm": 2.3097987174987793,
        "learning_rate": 0.00015794674915154091,
        "epoch": 0.7688283078617651,
        "step": 5584
    },
    {
        "loss": 2.1783,
        "grad_norm": 3.4299044609069824,
        "learning_rate": 0.00015784057750528903,
        "epoch": 0.7689659920143191,
        "step": 5585
    },
    {
        "loss": 2.3819,
        "grad_norm": 1.8477586507797241,
        "learning_rate": 0.0001577343077887028,
        "epoch": 0.7691036761668731,
        "step": 5586
    },
    {
        "loss": 1.1768,
        "grad_norm": 3.4193789958953857,
        "learning_rate": 0.0001576279401819654,
        "epoch": 0.7692413603194272,
        "step": 5587
    },
    {
        "loss": 1.9413,
        "grad_norm": 2.5907936096191406,
        "learning_rate": 0.00015752147486542605,
        "epoch": 0.7693790444719812,
        "step": 5588
    },
    {
        "loss": 2.3538,
        "grad_norm": 2.0347046852111816,
        "learning_rate": 0.00015741491201959988,
        "epoch": 0.7695167286245354,
        "step": 5589
    },
    {
        "loss": 1.8518,
        "grad_norm": 3.2469587326049805,
        "learning_rate": 0.00015730825182516713,
        "epoch": 0.7696544127770893,
        "step": 5590
    },
    {
        "loss": 1.2302,
        "grad_norm": 2.4468607902526855,
        "learning_rate": 0.00015720149446297308,
        "epoch": 0.7697920969296433,
        "step": 5591
    },
    {
        "loss": 2.2733,
        "grad_norm": 2.0310521125793457,
        "learning_rate": 0.00015709464011402798,
        "epoch": 0.7699297810821975,
        "step": 5592
    },
    {
        "loss": 1.7159,
        "grad_norm": 2.569706439971924,
        "learning_rate": 0.00015698768895950642,
        "epoch": 0.7700674652347514,
        "step": 5593
    },
    {
        "loss": 2.0923,
        "grad_norm": 2.291550397872925,
        "learning_rate": 0.00015688064118074695,
        "epoch": 0.7702051493873056,
        "step": 5594
    },
    {
        "loss": 1.3497,
        "grad_norm": 2.74873423576355,
        "learning_rate": 0.00015677349695925226,
        "epoch": 0.7703428335398596,
        "step": 5595
    },
    {
        "loss": 1.0477,
        "grad_norm": 2.217498779296875,
        "learning_rate": 0.00015666625647668838,
        "epoch": 0.7704805176924137,
        "step": 5596
    },
    {
        "loss": 1.4405,
        "grad_norm": 2.9188950061798096,
        "learning_rate": 0.00015655891991488453,
        "epoch": 0.7706182018449677,
        "step": 5597
    },
    {
        "loss": 2.4009,
        "grad_norm": 2.251365900039673,
        "learning_rate": 0.0001564514874558329,
        "epoch": 0.7707558859975217,
        "step": 5598
    },
    {
        "loss": 2.3159,
        "grad_norm": 1.5904136896133423,
        "learning_rate": 0.00015634395928168827,
        "epoch": 0.7708935701500758,
        "step": 5599
    },
    {
        "loss": 2.1484,
        "grad_norm": 5.2405686378479,
        "learning_rate": 0.0001562363355747676,
        "epoch": 0.7710312543026298,
        "step": 5600
    },
    {
        "loss": 2.8491,
        "grad_norm": 1.4955309629440308,
        "learning_rate": 0.00015612861651755013,
        "epoch": 0.7711689384551839,
        "step": 5601
    },
    {
        "loss": 2.1925,
        "grad_norm": 2.153186559677124,
        "learning_rate": 0.00015602080229267643,
        "epoch": 0.7713066226077379,
        "step": 5602
    },
    {
        "loss": 1.9845,
        "grad_norm": 2.0602285861968994,
        "learning_rate": 0.00015591289308294852,
        "epoch": 0.7714443067602919,
        "step": 5603
    },
    {
        "loss": 1.1813,
        "grad_norm": 3.2227044105529785,
        "learning_rate": 0.0001558048890713297,
        "epoch": 0.771581990912846,
        "step": 5604
    },
    {
        "loss": 2.2716,
        "grad_norm": 1.2800283432006836,
        "learning_rate": 0.0001556967904409437,
        "epoch": 0.7717196750654,
        "step": 5605
    },
    {
        "loss": 2.0845,
        "grad_norm": 2.7176754474639893,
        "learning_rate": 0.0001555885973750748,
        "epoch": 0.7718573592179541,
        "step": 5606
    },
    {
        "loss": 1.5237,
        "grad_norm": 2.0382001399993896,
        "learning_rate": 0.00015548031005716746,
        "epoch": 0.7719950433705081,
        "step": 5607
    },
    {
        "loss": 2.1023,
        "grad_norm": 1.5554265975952148,
        "learning_rate": 0.00015537192867082585,
        "epoch": 0.7721327275230621,
        "step": 5608
    },
    {
        "loss": 2.0918,
        "grad_norm": 1.3196440935134888,
        "learning_rate": 0.00015526345339981354,
        "epoch": 0.7722704116756162,
        "step": 5609
    },
    {
        "loss": 1.9061,
        "grad_norm": 2.2599148750305176,
        "learning_rate": 0.00015515488442805367,
        "epoch": 0.7724080958281702,
        "step": 5610
    },
    {
        "loss": 2.1839,
        "grad_norm": 2.617568254470825,
        "learning_rate": 0.0001550462219396279,
        "epoch": 0.7725457799807243,
        "step": 5611
    },
    {
        "loss": 2.5142,
        "grad_norm": 1.77340567111969,
        "learning_rate": 0.00015493746611877645,
        "epoch": 0.7726834641332783,
        "step": 5612
    },
    {
        "loss": 1.9836,
        "grad_norm": 1.473494529724121,
        "learning_rate": 0.00015482861714989806,
        "epoch": 0.7728211482858323,
        "step": 5613
    },
    {
        "loss": 2.3888,
        "grad_norm": 2.118175983428955,
        "learning_rate": 0.00015471967521754913,
        "epoch": 0.7729588324383864,
        "step": 5614
    },
    {
        "loss": 2.0981,
        "grad_norm": 1.788514256477356,
        "learning_rate": 0.0001546106405064437,
        "epoch": 0.7730965165909404,
        "step": 5615
    },
    {
        "loss": 2.4482,
        "grad_norm": 1.643083095550537,
        "learning_rate": 0.00015450151320145348,
        "epoch": 0.7732342007434945,
        "step": 5616
    },
    {
        "loss": 2.4702,
        "grad_norm": 1.5144712924957275,
        "learning_rate": 0.0001543922934876067,
        "epoch": 0.7733718848960485,
        "step": 5617
    },
    {
        "loss": 2.0783,
        "grad_norm": 1.6493569612503052,
        "learning_rate": 0.00015428298155008838,
        "epoch": 0.7735095690486025,
        "step": 5618
    },
    {
        "loss": 2.5674,
        "grad_norm": 1.3507853746414185,
        "learning_rate": 0.00015417357757424024,
        "epoch": 0.7736472532011566,
        "step": 5619
    },
    {
        "loss": 2.1399,
        "grad_norm": 2.1580593585968018,
        "learning_rate": 0.00015406408174555974,
        "epoch": 0.7737849373537106,
        "step": 5620
    },
    {
        "loss": 2.6459,
        "grad_norm": 2.371796131134033,
        "learning_rate": 0.00015395449424970007,
        "epoch": 0.7739226215062647,
        "step": 5621
    },
    {
        "loss": 1.5206,
        "grad_norm": 2.8095414638519287,
        "learning_rate": 0.00015384481527247,
        "epoch": 0.7740603056588187,
        "step": 5622
    },
    {
        "loss": 2.2467,
        "grad_norm": 2.2394540309906006,
        "learning_rate": 0.0001537350449998333,
        "epoch": 0.7741979898113727,
        "step": 5623
    },
    {
        "loss": 1.6658,
        "grad_norm": 2.1302168369293213,
        "learning_rate": 0.00015362518361790856,
        "epoch": 0.7743356739639268,
        "step": 5624
    },
    {
        "loss": 1.9485,
        "grad_norm": 2.3578827381134033,
        "learning_rate": 0.00015351523131296882,
        "epoch": 0.7744733581164808,
        "step": 5625
    },
    {
        "loss": 2.5288,
        "grad_norm": 1.5478166341781616,
        "learning_rate": 0.00015340518827144145,
        "epoch": 0.7746110422690349,
        "step": 5626
    },
    {
        "loss": 2.1596,
        "grad_norm": 2.3206849098205566,
        "learning_rate": 0.00015329505467990745,
        "epoch": 0.7747487264215889,
        "step": 5627
    },
    {
        "loss": 1.5435,
        "grad_norm": 3.3729043006896973,
        "learning_rate": 0.00015318483072510137,
        "epoch": 0.7748864105741429,
        "step": 5628
    },
    {
        "loss": 1.3949,
        "grad_norm": 2.2209277153015137,
        "learning_rate": 0.00015307451659391124,
        "epoch": 0.775024094726697,
        "step": 5629
    },
    {
        "loss": 1.9777,
        "grad_norm": 2.473207473754883,
        "learning_rate": 0.00015296411247337768,
        "epoch": 0.775161778879251,
        "step": 5630
    },
    {
        "loss": 1.8172,
        "grad_norm": 1.6463091373443604,
        "learning_rate": 0.000152853618550694,
        "epoch": 0.7752994630318051,
        "step": 5631
    },
    {
        "loss": 1.8826,
        "grad_norm": 2.5512092113494873,
        "learning_rate": 0.00015274303501320574,
        "epoch": 0.7754371471843591,
        "step": 5632
    },
    {
        "loss": 1.5537,
        "grad_norm": 1.9602223634719849,
        "learning_rate": 0.00015263236204841044,
        "epoch": 0.7755748313369131,
        "step": 5633
    },
    {
        "loss": 1.385,
        "grad_norm": 2.634342908859253,
        "learning_rate": 0.00015252159984395716,
        "epoch": 0.7757125154894672,
        "step": 5634
    },
    {
        "loss": 2.3431,
        "grad_norm": 1.395887851715088,
        "learning_rate": 0.00015241074858764653,
        "epoch": 0.7758501996420212,
        "step": 5635
    },
    {
        "loss": 1.7927,
        "grad_norm": 1.3695465326309204,
        "learning_rate": 0.00015229980846742986,
        "epoch": 0.7759878837945753,
        "step": 5636
    },
    {
        "loss": 1.0663,
        "grad_norm": 2.4703145027160645,
        "learning_rate": 0.0001521887796714092,
        "epoch": 0.7761255679471293,
        "step": 5637
    },
    {
        "loss": 1.6549,
        "grad_norm": 1.7265536785125732,
        "learning_rate": 0.00015207766238783724,
        "epoch": 0.7762632520996833,
        "step": 5638
    },
    {
        "loss": 2.2369,
        "grad_norm": 2.3101541996002197,
        "learning_rate": 0.00015196645680511626,
        "epoch": 0.7764009362522374,
        "step": 5639
    },
    {
        "loss": 1.8202,
        "grad_norm": 2.4701621532440186,
        "learning_rate": 0.00015185516311179848,
        "epoch": 0.7765386204047914,
        "step": 5640
    },
    {
        "loss": 2.3449,
        "grad_norm": 2.703112840652466,
        "learning_rate": 0.00015174378149658568,
        "epoch": 0.7766763045573455,
        "step": 5641
    },
    {
        "loss": 1.8791,
        "grad_norm": 2.7490503787994385,
        "learning_rate": 0.0001516323121483284,
        "epoch": 0.7768139887098995,
        "step": 5642
    },
    {
        "loss": 1.6255,
        "grad_norm": 2.4894156455993652,
        "learning_rate": 0.00015152075525602605,
        "epoch": 0.7769516728624535,
        "step": 5643
    },
    {
        "loss": 2.2371,
        "grad_norm": 2.8411219120025635,
        "learning_rate": 0.0001514091110088266,
        "epoch": 0.7770893570150076,
        "step": 5644
    },
    {
        "loss": 2.2461,
        "grad_norm": 1.680579423904419,
        "learning_rate": 0.000151297379596026,
        "epoch": 0.7772270411675616,
        "step": 5645
    },
    {
        "loss": 1.5647,
        "grad_norm": 2.276185989379883,
        "learning_rate": 0.00015118556120706798,
        "epoch": 0.7773647253201157,
        "step": 5646
    },
    {
        "loss": 2.1243,
        "grad_norm": 2.5781853199005127,
        "learning_rate": 0.0001510736560315438,
        "epoch": 0.7775024094726697,
        "step": 5647
    },
    {
        "loss": 1.9483,
        "grad_norm": 1.5983833074569702,
        "learning_rate": 0.00015096166425919183,
        "epoch": 0.7776400936252237,
        "step": 5648
    },
    {
        "loss": 2.0,
        "grad_norm": 1.650352954864502,
        "learning_rate": 0.00015084958607989717,
        "epoch": 0.7777777777777778,
        "step": 5649
    },
    {
        "loss": 2.6192,
        "grad_norm": 1.914541482925415,
        "learning_rate": 0.00015073742168369178,
        "epoch": 0.7779154619303318,
        "step": 5650
    },
    {
        "loss": 1.8035,
        "grad_norm": 1.5826243162155151,
        "learning_rate": 0.00015062517126075345,
        "epoch": 0.7780531460828859,
        "step": 5651
    },
    {
        "loss": 2.0507,
        "grad_norm": 1.5336121320724487,
        "learning_rate": 0.00015051283500140588,
        "epoch": 0.7781908302354399,
        "step": 5652
    },
    {
        "loss": 1.9932,
        "grad_norm": 2.653507947921753,
        "learning_rate": 0.0001504004130961185,
        "epoch": 0.778328514387994,
        "step": 5653
    },
    {
        "loss": 2.0987,
        "grad_norm": 1.759506106376648,
        "learning_rate": 0.0001502879057355058,
        "epoch": 0.778466198540548,
        "step": 5654
    },
    {
        "loss": 1.6062,
        "grad_norm": 2.96675181388855,
        "learning_rate": 0.00015017531311032717,
        "epoch": 0.778603882693102,
        "step": 5655
    },
    {
        "loss": 1.8804,
        "grad_norm": 1.1581588983535767,
        "learning_rate": 0.00015006263541148657,
        "epoch": 0.7787415668456561,
        "step": 5656
    },
    {
        "loss": 1.9482,
        "grad_norm": 2.0009829998016357,
        "learning_rate": 0.00014994987283003223,
        "epoch": 0.7788792509982101,
        "step": 5657
    },
    {
        "loss": 1.9154,
        "grad_norm": 2.6349358558654785,
        "learning_rate": 0.00014983702555715619,
        "epoch": 0.7790169351507642,
        "step": 5658
    },
    {
        "loss": 2.1459,
        "grad_norm": 2.3296732902526855,
        "learning_rate": 0.00014972409378419442,
        "epoch": 0.7791546193033182,
        "step": 5659
    },
    {
        "loss": 1.7452,
        "grad_norm": 2.7163233757019043,
        "learning_rate": 0.00014961107770262582,
        "epoch": 0.7792923034558722,
        "step": 5660
    },
    {
        "loss": 2.2265,
        "grad_norm": 1.9175488948822021,
        "learning_rate": 0.00014949797750407222,
        "epoch": 0.7794299876084263,
        "step": 5661
    },
    {
        "loss": 2.5548,
        "grad_norm": 2.162423849105835,
        "learning_rate": 0.00014938479338029846,
        "epoch": 0.7795676717609803,
        "step": 5662
    },
    {
        "loss": 2.4445,
        "grad_norm": 1.860656499862671,
        "learning_rate": 0.00014927152552321127,
        "epoch": 0.7797053559135344,
        "step": 5663
    },
    {
        "loss": 2.1676,
        "grad_norm": 1.8554937839508057,
        "learning_rate": 0.0001491581741248594,
        "epoch": 0.7798430400660884,
        "step": 5664
    },
    {
        "loss": 2.5497,
        "grad_norm": 2.270256519317627,
        "learning_rate": 0.0001490447393774336,
        "epoch": 0.7799807242186424,
        "step": 5665
    },
    {
        "loss": 1.6579,
        "grad_norm": 1.607624888420105,
        "learning_rate": 0.0001489312214732656,
        "epoch": 0.7801184083711965,
        "step": 5666
    },
    {
        "loss": 2.0359,
        "grad_norm": 2.456524610519409,
        "learning_rate": 0.00014881762060482803,
        "epoch": 0.7802560925237505,
        "step": 5667
    },
    {
        "loss": 1.3039,
        "grad_norm": 2.190000534057617,
        "learning_rate": 0.00014870393696473467,
        "epoch": 0.7803937766763046,
        "step": 5668
    },
    {
        "loss": 2.173,
        "grad_norm": 1.63498055934906,
        "learning_rate": 0.0001485901707457392,
        "epoch": 0.7805314608288586,
        "step": 5669
    },
    {
        "loss": 1.9612,
        "grad_norm": 1.8867396116256714,
        "learning_rate": 0.00014847632214073548,
        "epoch": 0.7806691449814126,
        "step": 5670
    },
    {
        "loss": 0.8155,
        "grad_norm": 2.7008519172668457,
        "learning_rate": 0.00014836239134275703,
        "epoch": 0.7808068291339667,
        "step": 5671
    },
    {
        "loss": 1.5882,
        "grad_norm": 2.196702718734741,
        "learning_rate": 0.00014824837854497672,
        "epoch": 0.7809445132865207,
        "step": 5672
    },
    {
        "loss": 1.0117,
        "grad_norm": 1.8336539268493652,
        "learning_rate": 0.00014813428394070633,
        "epoch": 0.7810821974390748,
        "step": 5673
    },
    {
        "loss": 2.2774,
        "grad_norm": 1.7217576503753662,
        "learning_rate": 0.00014802010772339684,
        "epoch": 0.7812198815916288,
        "step": 5674
    },
    {
        "loss": 1.792,
        "grad_norm": 1.8163330554962158,
        "learning_rate": 0.000147905850086637,
        "epoch": 0.7813575657441828,
        "step": 5675
    },
    {
        "loss": 1.861,
        "grad_norm": 2.678389310836792,
        "learning_rate": 0.0001477915112241538,
        "epoch": 0.7814952498967369,
        "step": 5676
    },
    {
        "loss": 2.0827,
        "grad_norm": 4.294375896453857,
        "learning_rate": 0.00014767709132981226,
        "epoch": 0.7816329340492909,
        "step": 5677
    },
    {
        "loss": 1.98,
        "grad_norm": 1.6731032133102417,
        "learning_rate": 0.00014756259059761444,
        "epoch": 0.781770618201845,
        "step": 5678
    },
    {
        "loss": 1.8082,
        "grad_norm": 2.120821475982666,
        "learning_rate": 0.00014744800922169952,
        "epoch": 0.781908302354399,
        "step": 5679
    },
    {
        "loss": 2.27,
        "grad_norm": 1.9162739515304565,
        "learning_rate": 0.00014733334739634352,
        "epoch": 0.782045986506953,
        "step": 5680
    },
    {
        "loss": 1.1196,
        "grad_norm": 4.3486328125,
        "learning_rate": 0.00014721860531595877,
        "epoch": 0.7821836706595071,
        "step": 5681
    },
    {
        "loss": 2.46,
        "grad_norm": 1.750961184501648,
        "learning_rate": 0.0001471037831750936,
        "epoch": 0.7823213548120611,
        "step": 5682
    },
    {
        "loss": 1.9033,
        "grad_norm": 1.6029223203659058,
        "learning_rate": 0.0001469888811684324,
        "epoch": 0.7824590389646152,
        "step": 5683
    },
    {
        "loss": 1.2071,
        "grad_norm": 2.3104541301727295,
        "learning_rate": 0.00014687389949079473,
        "epoch": 0.7825967231171692,
        "step": 5684
    },
    {
        "loss": 1.4326,
        "grad_norm": 2.125884532928467,
        "learning_rate": 0.0001467588383371351,
        "epoch": 0.7827344072697232,
        "step": 5685
    },
    {
        "loss": 1.468,
        "grad_norm": 1.8954370021820068,
        "learning_rate": 0.00014664369790254322,
        "epoch": 0.7828720914222773,
        "step": 5686
    },
    {
        "loss": 2.0655,
        "grad_norm": 1.4841185808181763,
        "learning_rate": 0.0001465284783822428,
        "epoch": 0.7830097755748313,
        "step": 5687
    },
    {
        "loss": 2.7071,
        "grad_norm": 1.8623883724212646,
        "learning_rate": 0.00014641317997159166,
        "epoch": 0.7831474597273854,
        "step": 5688
    },
    {
        "loss": 2.1456,
        "grad_norm": 2.7407734394073486,
        "learning_rate": 0.0001462978028660818,
        "epoch": 0.7832851438799394,
        "step": 5689
    },
    {
        "loss": 2.0273,
        "grad_norm": 1.8217086791992188,
        "learning_rate": 0.00014618234726133825,
        "epoch": 0.7834228280324934,
        "step": 5690
    },
    {
        "loss": 2.3231,
        "grad_norm": 1.8337976932525635,
        "learning_rate": 0.00014606681335311908,
        "epoch": 0.7835605121850475,
        "step": 5691
    },
    {
        "loss": 1.7883,
        "grad_norm": 2.4678685665130615,
        "learning_rate": 0.00014595120133731557,
        "epoch": 0.7836981963376015,
        "step": 5692
    },
    {
        "loss": 2.5105,
        "grad_norm": 1.5015406608581543,
        "learning_rate": 0.00014583551140995104,
        "epoch": 0.7838358804901556,
        "step": 5693
    },
    {
        "loss": 1.6264,
        "grad_norm": 1.5428931713104248,
        "learning_rate": 0.00014571974376718106,
        "epoch": 0.7839735646427096,
        "step": 5694
    },
    {
        "loss": 2.2263,
        "grad_norm": 1.432390809059143,
        "learning_rate": 0.00014560389860529286,
        "epoch": 0.7841112487952636,
        "step": 5695
    },
    {
        "loss": 2.0466,
        "grad_norm": 2.0847671031951904,
        "learning_rate": 0.00014548797612070526,
        "epoch": 0.7842489329478177,
        "step": 5696
    },
    {
        "loss": 2.1154,
        "grad_norm": 1.8413310050964355,
        "learning_rate": 0.00014537197650996795,
        "epoch": 0.7843866171003717,
        "step": 5697
    },
    {
        "loss": 2.0334,
        "grad_norm": 2.1422176361083984,
        "learning_rate": 0.00014525589996976186,
        "epoch": 0.7845243012529258,
        "step": 5698
    },
    {
        "loss": 1.8293,
        "grad_norm": 2.2300968170166016,
        "learning_rate": 0.00014513974669689786,
        "epoch": 0.7846619854054798,
        "step": 5699
    },
    {
        "loss": 1.7873,
        "grad_norm": 3.072124719619751,
        "learning_rate": 0.00014502351688831696,
        "epoch": 0.7847996695580338,
        "step": 5700
    },
    {
        "loss": 1.7263,
        "grad_norm": 2.6180624961853027,
        "learning_rate": 0.0001449072107410904,
        "epoch": 0.7849373537105879,
        "step": 5701
    },
    {
        "loss": 1.7647,
        "grad_norm": 1.8596094846725464,
        "learning_rate": 0.0001447908284524184,
        "epoch": 0.7850750378631419,
        "step": 5702
    },
    {
        "loss": 1.9669,
        "grad_norm": 2.2446627616882324,
        "learning_rate": 0.0001446743702196304,
        "epoch": 0.785212722015696,
        "step": 5703
    },
    {
        "loss": 1.7364,
        "grad_norm": 3.2535741329193115,
        "learning_rate": 0.00014455783624018462,
        "epoch": 0.78535040616825,
        "step": 5704
    },
    {
        "loss": 2.1834,
        "grad_norm": 2.0723156929016113,
        "learning_rate": 0.00014444122671166776,
        "epoch": 0.785488090320804,
        "step": 5705
    },
    {
        "loss": 1.5701,
        "grad_norm": 1.2332327365875244,
        "learning_rate": 0.00014432454183179442,
        "epoch": 0.7856257744733581,
        "step": 5706
    },
    {
        "loss": 1.435,
        "grad_norm": 2.690223217010498,
        "learning_rate": 0.00014420778179840735,
        "epoch": 0.7857634586259121,
        "step": 5707
    },
    {
        "loss": 2.2687,
        "grad_norm": 2.2425882816314697,
        "learning_rate": 0.0001440909468094764,
        "epoch": 0.7859011427784662,
        "step": 5708
    },
    {
        "loss": 1.7882,
        "grad_norm": 1.743639588356018,
        "learning_rate": 0.00014397403706309855,
        "epoch": 0.7860388269310202,
        "step": 5709
    },
    {
        "loss": 2.2532,
        "grad_norm": 1.6922082901000977,
        "learning_rate": 0.00014385705275749748,
        "epoch": 0.7861765110835742,
        "step": 5710
    },
    {
        "loss": 2.312,
        "grad_norm": 2.133348226547241,
        "learning_rate": 0.00014373999409102365,
        "epoch": 0.7863141952361283,
        "step": 5711
    },
    {
        "loss": 2.0751,
        "grad_norm": 2.2584757804870605,
        "learning_rate": 0.00014362286126215322,
        "epoch": 0.7864518793886823,
        "step": 5712
    },
    {
        "loss": 1.6055,
        "grad_norm": 2.201014518737793,
        "learning_rate": 0.00014350565446948803,
        "epoch": 0.7865895635412364,
        "step": 5713
    },
    {
        "loss": 2.0483,
        "grad_norm": 1.7689054012298584,
        "learning_rate": 0.00014338837391175582,
        "epoch": 0.7867272476937904,
        "step": 5714
    },
    {
        "loss": 1.4226,
        "grad_norm": 1.5185573101043701,
        "learning_rate": 0.00014327101978780891,
        "epoch": 0.7868649318463445,
        "step": 5715
    },
    {
        "loss": 2.0952,
        "grad_norm": 2.2613883018493652,
        "learning_rate": 0.00014315359229662448,
        "epoch": 0.7870026159988985,
        "step": 5716
    },
    {
        "loss": 1.1689,
        "grad_norm": 2.7488996982574463,
        "learning_rate": 0.00014303609163730438,
        "epoch": 0.7871403001514525,
        "step": 5717
    },
    {
        "loss": 1.8784,
        "grad_norm": 1.9091949462890625,
        "learning_rate": 0.00014291851800907416,
        "epoch": 0.7872779843040066,
        "step": 5718
    },
    {
        "loss": 2.4956,
        "grad_norm": 1.874071478843689,
        "learning_rate": 0.00014280087161128323,
        "epoch": 0.7874156684565606,
        "step": 5719
    },
    {
        "loss": 2.3407,
        "grad_norm": 1.9592183828353882,
        "learning_rate": 0.00014268315264340443,
        "epoch": 0.7875533526091147,
        "step": 5720
    },
    {
        "loss": 1.5851,
        "grad_norm": 2.525346517562866,
        "learning_rate": 0.00014256536130503354,
        "epoch": 0.7876910367616687,
        "step": 5721
    },
    {
        "loss": 1.9779,
        "grad_norm": 1.8373103141784668,
        "learning_rate": 0.000142447497795889,
        "epoch": 0.7878287209142227,
        "step": 5722
    },
    {
        "loss": 2.3543,
        "grad_norm": 3.0634777545928955,
        "learning_rate": 0.00014232956231581198,
        "epoch": 0.7879664050667768,
        "step": 5723
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.63326358795166,
        "learning_rate": 0.0001422115550647653,
        "epoch": 0.7881040892193308,
        "step": 5724
    },
    {
        "loss": 2.3138,
        "grad_norm": 1.8904591798782349,
        "learning_rate": 0.00014209347624283345,
        "epoch": 0.788241773371885,
        "step": 5725
    },
    {
        "loss": 1.8029,
        "grad_norm": 3.255594253540039,
        "learning_rate": 0.00014197532605022265,
        "epoch": 0.7883794575244389,
        "step": 5726
    },
    {
        "loss": 1.4018,
        "grad_norm": 2.08329701423645,
        "learning_rate": 0.00014185710468725984,
        "epoch": 0.7885171416769929,
        "step": 5727
    },
    {
        "loss": 1.4087,
        "grad_norm": 1.9659359455108643,
        "learning_rate": 0.00014173881235439258,
        "epoch": 0.788654825829547,
        "step": 5728
    },
    {
        "loss": 2.0536,
        "grad_norm": 1.7183550596237183,
        "learning_rate": 0.0001416204492521889,
        "epoch": 0.788792509982101,
        "step": 5729
    },
    {
        "loss": 1.9073,
        "grad_norm": 2.019801378250122,
        "learning_rate": 0.00014150201558133687,
        "epoch": 0.7889301941346551,
        "step": 5730
    },
    {
        "loss": 1.2299,
        "grad_norm": 2.323981761932373,
        "learning_rate": 0.00014138351154264388,
        "epoch": 0.7890678782872091,
        "step": 5731
    },
    {
        "loss": 2.4291,
        "grad_norm": 1.8666393756866455,
        "learning_rate": 0.0001412649373370372,
        "epoch": 0.7892055624397631,
        "step": 5732
    },
    {
        "loss": 2.1826,
        "grad_norm": 3.173069953918457,
        "learning_rate": 0.00014114629316556258,
        "epoch": 0.7893432465923172,
        "step": 5733
    },
    {
        "loss": 1.2192,
        "grad_norm": 1.8035836219787598,
        "learning_rate": 0.00014102757922938448,
        "epoch": 0.7894809307448712,
        "step": 5734
    },
    {
        "loss": 1.6797,
        "grad_norm": 2.6169493198394775,
        "learning_rate": 0.00014090879572978594,
        "epoch": 0.7896186148974254,
        "step": 5735
    },
    {
        "loss": 2.4211,
        "grad_norm": 1.9806567430496216,
        "learning_rate": 0.00014078994286816768,
        "epoch": 0.7897562990499793,
        "step": 5736
    },
    {
        "loss": 2.0225,
        "grad_norm": 1.6526551246643066,
        "learning_rate": 0.00014067102084604796,
        "epoch": 0.7898939832025333,
        "step": 5737
    },
    {
        "loss": 2.1568,
        "grad_norm": 1.1529650688171387,
        "learning_rate": 0.00014055202986506258,
        "epoch": 0.7900316673550875,
        "step": 5738
    },
    {
        "loss": 2.2809,
        "grad_norm": 1.6836042404174805,
        "learning_rate": 0.00014043297012696407,
        "epoch": 0.7901693515076414,
        "step": 5739
    },
    {
        "loss": 1.6215,
        "grad_norm": 1.740168809890747,
        "learning_rate": 0.00014031384183362149,
        "epoch": 0.7903070356601956,
        "step": 5740
    },
    {
        "loss": 2.0067,
        "grad_norm": 2.9566972255706787,
        "learning_rate": 0.00014019464518702038,
        "epoch": 0.7904447198127496,
        "step": 5741
    },
    {
        "loss": 2.3601,
        "grad_norm": 2.3077597618103027,
        "learning_rate": 0.000140075380389262,
        "epoch": 0.7905824039653035,
        "step": 5742
    },
    {
        "loss": 2.1906,
        "grad_norm": 2.9181125164031982,
        "learning_rate": 0.00013995604764256313,
        "epoch": 0.7907200881178577,
        "step": 5743
    },
    {
        "loss": 2.0288,
        "grad_norm": 2.1030287742614746,
        "learning_rate": 0.00013983664714925582,
        "epoch": 0.7908577722704117,
        "step": 5744
    },
    {
        "loss": 2.3315,
        "grad_norm": 1.7302144765853882,
        "learning_rate": 0.00013971717911178696,
        "epoch": 0.7909954564229658,
        "step": 5745
    },
    {
        "loss": 1.8263,
        "grad_norm": 1.838350534439087,
        "learning_rate": 0.0001395976437327179,
        "epoch": 0.7911331405755198,
        "step": 5746
    },
    {
        "loss": 1.8215,
        "grad_norm": 1.9508360624313354,
        "learning_rate": 0.0001394780412147245,
        "epoch": 0.7912708247280738,
        "step": 5747
    },
    {
        "loss": 1.9877,
        "grad_norm": 2.050046920776367,
        "learning_rate": 0.0001393583717605961,
        "epoch": 0.7914085088806279,
        "step": 5748
    },
    {
        "loss": 1.3435,
        "grad_norm": 1.9049326181411743,
        "learning_rate": 0.00013923863557323548,
        "epoch": 0.7915461930331819,
        "step": 5749
    },
    {
        "loss": 1.9592,
        "grad_norm": 2.1989877223968506,
        "learning_rate": 0.00013911883285565898,
        "epoch": 0.791683877185736,
        "step": 5750
    },
    {
        "loss": 2.21,
        "grad_norm": 2.0042529106140137,
        "learning_rate": 0.0001389989638109954,
        "epoch": 0.79182156133829,
        "step": 5751
    },
    {
        "loss": 1.9892,
        "grad_norm": 2.616241455078125,
        "learning_rate": 0.00013887902864248607,
        "epoch": 0.791959245490844,
        "step": 5752
    },
    {
        "loss": 2.102,
        "grad_norm": 1.7808959484100342,
        "learning_rate": 0.0001387590275534845,
        "epoch": 0.7920969296433981,
        "step": 5753
    },
    {
        "loss": 1.9057,
        "grad_norm": 2.054969549179077,
        "learning_rate": 0.00013863896074745589,
        "epoch": 0.7922346137959521,
        "step": 5754
    },
    {
        "loss": 2.2165,
        "grad_norm": 1.2623695135116577,
        "learning_rate": 0.00013851882842797673,
        "epoch": 0.7923722979485062,
        "step": 5755
    },
    {
        "loss": 2.3651,
        "grad_norm": 1.8804469108581543,
        "learning_rate": 0.00013839863079873513,
        "epoch": 0.7925099821010602,
        "step": 5756
    },
    {
        "loss": 1.8316,
        "grad_norm": 2.6426661014556885,
        "learning_rate": 0.0001382783680635294,
        "epoch": 0.7926476662536142,
        "step": 5757
    },
    {
        "loss": 1.8016,
        "grad_norm": 1.7334108352661133,
        "learning_rate": 0.00013815804042626822,
        "epoch": 0.7927853504061683,
        "step": 5758
    },
    {
        "loss": 1.9801,
        "grad_norm": 1.7789323329925537,
        "learning_rate": 0.00013803764809097084,
        "epoch": 0.7929230345587223,
        "step": 5759
    },
    {
        "loss": 1.0039,
        "grad_norm": 2.419853448867798,
        "learning_rate": 0.00013791719126176566,
        "epoch": 0.7930607187112764,
        "step": 5760
    },
    {
        "loss": 1.925,
        "grad_norm": 1.9879252910614014,
        "learning_rate": 0.0001377966701428906,
        "epoch": 0.7931984028638304,
        "step": 5761
    },
    {
        "loss": 1.826,
        "grad_norm": 1.5815184116363525,
        "learning_rate": 0.0001376760849386928,
        "epoch": 0.7933360870163844,
        "step": 5762
    },
    {
        "loss": 2.2732,
        "grad_norm": 1.9565057754516602,
        "learning_rate": 0.00013755543585362783,
        "epoch": 0.7934737711689385,
        "step": 5763
    },
    {
        "loss": 2.1504,
        "grad_norm": 2.4606943130493164,
        "learning_rate": 0.0001374347230922594,
        "epoch": 0.7936114553214925,
        "step": 5764
    },
    {
        "loss": 2.1539,
        "grad_norm": 2.866936445236206,
        "learning_rate": 0.00013731394685925977,
        "epoch": 0.7937491394740466,
        "step": 5765
    },
    {
        "loss": 2.388,
        "grad_norm": 2.3175227642059326,
        "learning_rate": 0.00013719310735940826,
        "epoch": 0.7938868236266006,
        "step": 5766
    },
    {
        "loss": 1.4503,
        "grad_norm": 3.2186119556427,
        "learning_rate": 0.00013707220479759172,
        "epoch": 0.7940245077791546,
        "step": 5767
    },
    {
        "loss": 1.19,
        "grad_norm": 3.859982967376709,
        "learning_rate": 0.0001369512393788038,
        "epoch": 0.7941621919317087,
        "step": 5768
    },
    {
        "loss": 2.0816,
        "grad_norm": 1.3960331678390503,
        "learning_rate": 0.00013683021130814479,
        "epoch": 0.7942998760842627,
        "step": 5769
    },
    {
        "loss": 1.5305,
        "grad_norm": 1.6518170833587646,
        "learning_rate": 0.0001367091207908211,
        "epoch": 0.7944375602368168,
        "step": 5770
    },
    {
        "loss": 2.1373,
        "grad_norm": 2.2545087337493896,
        "learning_rate": 0.00013658796803214538,
        "epoch": 0.7945752443893708,
        "step": 5771
    },
    {
        "loss": 2.2768,
        "grad_norm": 1.9601614475250244,
        "learning_rate": 0.00013646675323753543,
        "epoch": 0.7947129285419249,
        "step": 5772
    },
    {
        "loss": 2.4178,
        "grad_norm": 2.065091609954834,
        "learning_rate": 0.00013634547661251425,
        "epoch": 0.7948506126944789,
        "step": 5773
    },
    {
        "loss": 1.6947,
        "grad_norm": 2.7222299575805664,
        "learning_rate": 0.00013622413836271,
        "epoch": 0.7949882968470329,
        "step": 5774
    },
    {
        "loss": 2.3121,
        "grad_norm": 2.3189094066619873,
        "learning_rate": 0.00013610273869385503,
        "epoch": 0.795125980999587,
        "step": 5775
    },
    {
        "loss": 1.769,
        "grad_norm": 1.4279775619506836,
        "learning_rate": 0.00013598127781178586,
        "epoch": 0.795263665152141,
        "step": 5776
    },
    {
        "loss": 2.4136,
        "grad_norm": 1.2805142402648926,
        "learning_rate": 0.00013585975592244292,
        "epoch": 0.7954013493046951,
        "step": 5777
    },
    {
        "loss": 1.3844,
        "grad_norm": 2.069023847579956,
        "learning_rate": 0.00013573817323186998,
        "epoch": 0.7955390334572491,
        "step": 5778
    },
    {
        "loss": 1.4038,
        "grad_norm": 3.095992088317871,
        "learning_rate": 0.00013561652994621373,
        "epoch": 0.7956767176098031,
        "step": 5779
    },
    {
        "loss": 0.9149,
        "grad_norm": 2.2140607833862305,
        "learning_rate": 0.00013549482627172416,
        "epoch": 0.7958144017623572,
        "step": 5780
    },
    {
        "loss": 1.8833,
        "grad_norm": 1.9846417903900146,
        "learning_rate": 0.00013537306241475313,
        "epoch": 0.7959520859149112,
        "step": 5781
    },
    {
        "loss": 2.2229,
        "grad_norm": 1.7655972242355347,
        "learning_rate": 0.0001352512385817545,
        "epoch": 0.7960897700674653,
        "step": 5782
    },
    {
        "loss": 2.1089,
        "grad_norm": 3.43463397026062,
        "learning_rate": 0.00013512935497928438,
        "epoch": 0.7962274542200193,
        "step": 5783
    },
    {
        "loss": 2.0666,
        "grad_norm": 1.5230246782302856,
        "learning_rate": 0.0001350074118139997,
        "epoch": 0.7963651383725733,
        "step": 5784
    },
    {
        "loss": 1.6069,
        "grad_norm": 2.355194330215454,
        "learning_rate": 0.00013488540929265842,
        "epoch": 0.7965028225251274,
        "step": 5785
    },
    {
        "loss": 1.6799,
        "grad_norm": 2.608689308166504,
        "learning_rate": 0.00013476334762211952,
        "epoch": 0.7966405066776814,
        "step": 5786
    },
    {
        "loss": 1.8292,
        "grad_norm": 3.4335033893585205,
        "learning_rate": 0.00013464122700934187,
        "epoch": 0.7967781908302355,
        "step": 5787
    },
    {
        "loss": 1.137,
        "grad_norm": 4.584320545196533,
        "learning_rate": 0.0001345190476613843,
        "epoch": 0.7969158749827895,
        "step": 5788
    },
    {
        "loss": 1.7934,
        "grad_norm": 1.5412373542785645,
        "learning_rate": 0.00013439680978540562,
        "epoch": 0.7970535591353435,
        "step": 5789
    },
    {
        "loss": 1.8581,
        "grad_norm": 2.1697068214416504,
        "learning_rate": 0.00013427451358866347,
        "epoch": 0.7971912432878976,
        "step": 5790
    },
    {
        "loss": 1.7229,
        "grad_norm": 1.9466021060943604,
        "learning_rate": 0.0001341521592785145,
        "epoch": 0.7973289274404516,
        "step": 5791
    },
    {
        "loss": 2.2188,
        "grad_norm": 1.6219193935394287,
        "learning_rate": 0.00013402974706241388,
        "epoch": 0.7974666115930057,
        "step": 5792
    },
    {
        "loss": 2.4426,
        "grad_norm": 1.2137320041656494,
        "learning_rate": 0.00013390727714791505,
        "epoch": 0.7976042957455597,
        "step": 5793
    },
    {
        "loss": 2.1632,
        "grad_norm": 3.2625818252563477,
        "learning_rate": 0.00013378474974266914,
        "epoch": 0.7977419798981137,
        "step": 5794
    },
    {
        "loss": 0.901,
        "grad_norm": 2.4369912147521973,
        "learning_rate": 0.00013366216505442474,
        "epoch": 0.7978796640506678,
        "step": 5795
    },
    {
        "loss": 1.2419,
        "grad_norm": 3.1673991680145264,
        "learning_rate": 0.00013353952329102792,
        "epoch": 0.7980173482032218,
        "step": 5796
    },
    {
        "loss": 1.3302,
        "grad_norm": 2.6134910583496094,
        "learning_rate": 0.00013341682466042114,
        "epoch": 0.7981550323557759,
        "step": 5797
    },
    {
        "loss": 1.9105,
        "grad_norm": 2.4375534057617188,
        "learning_rate": 0.00013329406937064318,
        "epoch": 0.7982927165083299,
        "step": 5798
    },
    {
        "loss": 1.9451,
        "grad_norm": 1.8505624532699585,
        "learning_rate": 0.00013317125762982946,
        "epoch": 0.7984304006608839,
        "step": 5799
    },
    {
        "loss": 0.8784,
        "grad_norm": 3.6374363899230957,
        "learning_rate": 0.0001330483896462106,
        "epoch": 0.798568084813438,
        "step": 5800
    },
    {
        "loss": 2.5133,
        "grad_norm": 1.2743515968322754,
        "learning_rate": 0.00013292546562811272,
        "epoch": 0.798705768965992,
        "step": 5801
    },
    {
        "loss": 2.5141,
        "grad_norm": 2.083806037902832,
        "learning_rate": 0.00013280248578395699,
        "epoch": 0.7988434531185461,
        "step": 5802
    },
    {
        "loss": 2.128,
        "grad_norm": 2.5921261310577393,
        "learning_rate": 0.0001326794503222592,
        "epoch": 0.7989811372711001,
        "step": 5803
    },
    {
        "loss": 2.5019,
        "grad_norm": 2.03731369972229,
        "learning_rate": 0.00013255635945162933,
        "epoch": 0.7991188214236541,
        "step": 5804
    },
    {
        "loss": 2.3227,
        "grad_norm": 1.6174609661102295,
        "learning_rate": 0.0001324332133807717,
        "epoch": 0.7992565055762082,
        "step": 5805
    },
    {
        "loss": 2.0167,
        "grad_norm": 2.383183002471924,
        "learning_rate": 0.00013231001231848384,
        "epoch": 0.7993941897287622,
        "step": 5806
    },
    {
        "loss": 1.9533,
        "grad_norm": 2.3951659202575684,
        "learning_rate": 0.00013218675647365643,
        "epoch": 0.7995318738813163,
        "step": 5807
    },
    {
        "loss": 1.9798,
        "grad_norm": 1.6425373554229736,
        "learning_rate": 0.00013206344605527355,
        "epoch": 0.7996695580338703,
        "step": 5808
    },
    {
        "loss": 1.5349,
        "grad_norm": 2.4065282344818115,
        "learning_rate": 0.00013194008127241142,
        "epoch": 0.7998072421864243,
        "step": 5809
    },
    {
        "loss": 1.586,
        "grad_norm": 2.2600057125091553,
        "learning_rate": 0.00013181666233423828,
        "epoch": 0.7999449263389784,
        "step": 5810
    },
    {
        "loss": 2.2095,
        "grad_norm": 2.089783191680908,
        "learning_rate": 0.00013169318945001479,
        "epoch": 0.8000826104915324,
        "step": 5811
    },
    {
        "loss": 1.5153,
        "grad_norm": 1.9118932485580444,
        "learning_rate": 0.00013156966282909253,
        "epoch": 0.8002202946440865,
        "step": 5812
    },
    {
        "loss": 1.1114,
        "grad_norm": 2.2701339721679688,
        "learning_rate": 0.00013144608268091424,
        "epoch": 0.8003579787966405,
        "step": 5813
    },
    {
        "loss": 2.7013,
        "grad_norm": 1.1559655666351318,
        "learning_rate": 0.00013132244921501386,
        "epoch": 0.8004956629491945,
        "step": 5814
    },
    {
        "loss": 2.4999,
        "grad_norm": 1.6002600193023682,
        "learning_rate": 0.00013119876264101527,
        "epoch": 0.8006333471017486,
        "step": 5815
    },
    {
        "loss": 2.2073,
        "grad_norm": 1.8370617628097534,
        "learning_rate": 0.00013107502316863254,
        "epoch": 0.8007710312543026,
        "step": 5816
    },
    {
        "loss": 2.4229,
        "grad_norm": 1.0488474369049072,
        "learning_rate": 0.00013095123100766946,
        "epoch": 0.8009087154068567,
        "step": 5817
    },
    {
        "loss": 1.9143,
        "grad_norm": 1.1390269994735718,
        "learning_rate": 0.00013082738636801918,
        "epoch": 0.8010463995594107,
        "step": 5818
    },
    {
        "loss": 2.3101,
        "grad_norm": 2.0896565914154053,
        "learning_rate": 0.00013070348945966355,
        "epoch": 0.8011840837119647,
        "step": 5819
    },
    {
        "loss": 1.7909,
        "grad_norm": 1.998672604560852,
        "learning_rate": 0.00013057954049267365,
        "epoch": 0.8013217678645188,
        "step": 5820
    },
    {
        "loss": 1.6906,
        "grad_norm": 2.202955961227417,
        "learning_rate": 0.00013045553967720827,
        "epoch": 0.8014594520170728,
        "step": 5821
    },
    {
        "loss": 1.4664,
        "grad_norm": 1.7001153230667114,
        "learning_rate": 0.0001303314872235142,
        "epoch": 0.8015971361696269,
        "step": 5822
    },
    {
        "loss": 1.9228,
        "grad_norm": 1.343707799911499,
        "learning_rate": 0.0001302073833419261,
        "epoch": 0.8017348203221809,
        "step": 5823
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.3910421133041382,
        "learning_rate": 0.00013008322824286555,
        "epoch": 0.8018725044747349,
        "step": 5824
    },
    {
        "loss": 1.908,
        "grad_norm": 2.075301170349121,
        "learning_rate": 0.00012995902213684094,
        "epoch": 0.802010188627289,
        "step": 5825
    },
    {
        "loss": 2.1898,
        "grad_norm": 1.4014073610305786,
        "learning_rate": 0.0001298347652344473,
        "epoch": 0.802147872779843,
        "step": 5826
    },
    {
        "loss": 2.1462,
        "grad_norm": 1.510521411895752,
        "learning_rate": 0.0001297104577463657,
        "epoch": 0.8022855569323971,
        "step": 5827
    },
    {
        "loss": 2.3924,
        "grad_norm": 2.802877902984619,
        "learning_rate": 0.00012958609988336287,
        "epoch": 0.8024232410849511,
        "step": 5828
    },
    {
        "loss": 1.3449,
        "grad_norm": 3.0031495094299316,
        "learning_rate": 0.00012946169185629135,
        "epoch": 0.8025609252375052,
        "step": 5829
    },
    {
        "loss": 2.3707,
        "grad_norm": 2.111689567565918,
        "learning_rate": 0.00012933723387608832,
        "epoch": 0.8026986093900592,
        "step": 5830
    },
    {
        "loss": 2.0101,
        "grad_norm": 2.224846124649048,
        "learning_rate": 0.00012921272615377572,
        "epoch": 0.8028362935426132,
        "step": 5831
    },
    {
        "loss": 2.5724,
        "grad_norm": 1.3575072288513184,
        "learning_rate": 0.0001290881689004601,
        "epoch": 0.8029739776951673,
        "step": 5832
    },
    {
        "loss": 1.9946,
        "grad_norm": 2.6596996784210205,
        "learning_rate": 0.00012896356232733173,
        "epoch": 0.8031116618477213,
        "step": 5833
    },
    {
        "loss": 1.5578,
        "grad_norm": 2.2713656425476074,
        "learning_rate": 0.00012883890664566443,
        "epoch": 0.8032493460002754,
        "step": 5834
    },
    {
        "loss": 2.5305,
        "grad_norm": 1.455846905708313,
        "learning_rate": 0.0001287142020668157,
        "epoch": 0.8033870301528294,
        "step": 5835
    },
    {
        "loss": 2.0614,
        "grad_norm": 1.406886339187622,
        "learning_rate": 0.00012858944880222555,
        "epoch": 0.8035247143053834,
        "step": 5836
    },
    {
        "loss": 2.3074,
        "grad_norm": 2.5979421138763428,
        "learning_rate": 0.00012846464706341653,
        "epoch": 0.8036623984579375,
        "step": 5837
    },
    {
        "loss": 1.7572,
        "grad_norm": 2.2377941608428955,
        "learning_rate": 0.00012833979706199374,
        "epoch": 0.8038000826104915,
        "step": 5838
    },
    {
        "loss": 1.3295,
        "grad_norm": 2.56834077835083,
        "learning_rate": 0.00012821489900964386,
        "epoch": 0.8039377667630456,
        "step": 5839
    },
    {
        "loss": 2.5091,
        "grad_norm": 1.5333865880966187,
        "learning_rate": 0.00012808995311813494,
        "epoch": 0.8040754509155996,
        "step": 5840
    },
    {
        "loss": 2.1296,
        "grad_norm": 1.7497520446777344,
        "learning_rate": 0.00012796495959931637,
        "epoch": 0.8042131350681536,
        "step": 5841
    },
    {
        "loss": 2.147,
        "grad_norm": 3.2558186054229736,
        "learning_rate": 0.00012783991866511816,
        "epoch": 0.8043508192207077,
        "step": 5842
    },
    {
        "loss": 2.1843,
        "grad_norm": 2.1236307621002197,
        "learning_rate": 0.0001277148305275506,
        "epoch": 0.8044885033732617,
        "step": 5843
    },
    {
        "loss": 1.9245,
        "grad_norm": 1.756569504737854,
        "learning_rate": 0.00012758969539870447,
        "epoch": 0.8046261875258158,
        "step": 5844
    },
    {
        "loss": 1.2858,
        "grad_norm": 1.9009289741516113,
        "learning_rate": 0.00012746451349074974,
        "epoch": 0.8047638716783698,
        "step": 5845
    },
    {
        "loss": 1.5478,
        "grad_norm": 3.0827760696411133,
        "learning_rate": 0.0001273392850159358,
        "epoch": 0.8049015558309238,
        "step": 5846
    },
    {
        "loss": 1.9561,
        "grad_norm": 3.6901729106903076,
        "learning_rate": 0.00012721401018659124,
        "epoch": 0.8050392399834779,
        "step": 5847
    },
    {
        "loss": 1.9825,
        "grad_norm": 1.3205186128616333,
        "learning_rate": 0.00012708868921512304,
        "epoch": 0.8051769241360319,
        "step": 5848
    },
    {
        "loss": 2.058,
        "grad_norm": 2.0089242458343506,
        "learning_rate": 0.00012696332231401642,
        "epoch": 0.805314608288586,
        "step": 5849
    },
    {
        "loss": 2.6386,
        "grad_norm": 1.5530771017074585,
        "learning_rate": 0.00012683790969583448,
        "epoch": 0.80545229244114,
        "step": 5850
    },
    {
        "loss": 1.7437,
        "grad_norm": 1.6692874431610107,
        "learning_rate": 0.00012671245157321788,
        "epoch": 0.805589976593694,
        "step": 5851
    },
    {
        "loss": 1.2987,
        "grad_norm": 2.8426690101623535,
        "learning_rate": 0.0001265869481588843,
        "epoch": 0.8057276607462481,
        "step": 5852
    },
    {
        "loss": 2.3224,
        "grad_norm": 2.0780768394470215,
        "learning_rate": 0.00012646139966562852,
        "epoch": 0.8058653448988021,
        "step": 5853
    },
    {
        "loss": 2.0196,
        "grad_norm": 2.1820244789123535,
        "learning_rate": 0.0001263358063063215,
        "epoch": 0.8060030290513562,
        "step": 5854
    },
    {
        "loss": 2.4609,
        "grad_norm": 2.09256649017334,
        "learning_rate": 0.0001262101682939102,
        "epoch": 0.8061407132039102,
        "step": 5855
    },
    {
        "loss": 2.1916,
        "grad_norm": 2.589406967163086,
        "learning_rate": 0.00012608448584141763,
        "epoch": 0.8062783973564642,
        "step": 5856
    },
    {
        "loss": 1.9392,
        "grad_norm": 2.325655937194824,
        "learning_rate": 0.00012595875916194188,
        "epoch": 0.8064160815090183,
        "step": 5857
    },
    {
        "loss": 1.6864,
        "grad_norm": 2.9214773178100586,
        "learning_rate": 0.00012583298846865587,
        "epoch": 0.8065537656615723,
        "step": 5858
    },
    {
        "loss": 0.7483,
        "grad_norm": 2.101762533187866,
        "learning_rate": 0.00012570717397480773,
        "epoch": 0.8066914498141264,
        "step": 5859
    },
    {
        "loss": 0.9631,
        "grad_norm": 2.7188146114349365,
        "learning_rate": 0.00012558131589371927,
        "epoch": 0.8068291339666804,
        "step": 5860
    },
    {
        "loss": 1.8422,
        "grad_norm": 1.6094504594802856,
        "learning_rate": 0.00012545541443878633,
        "epoch": 0.8069668181192344,
        "step": 5861
    },
    {
        "loss": 2.5496,
        "grad_norm": 2.1631405353546143,
        "learning_rate": 0.00012532946982347868,
        "epoch": 0.8071045022717885,
        "step": 5862
    },
    {
        "loss": 1.8821,
        "grad_norm": 2.30493426322937,
        "learning_rate": 0.00012520348226133883,
        "epoch": 0.8072421864243425,
        "step": 5863
    },
    {
        "loss": 2.5085,
        "grad_norm": 1.3640737533569336,
        "learning_rate": 0.0001250774519659822,
        "epoch": 0.8073798705768966,
        "step": 5864
    },
    {
        "loss": 2.2585,
        "grad_norm": 1.9402607679367065,
        "learning_rate": 0.0001249513791510968,
        "epoch": 0.8075175547294506,
        "step": 5865
    },
    {
        "loss": 1.9593,
        "grad_norm": 2.278014659881592,
        "learning_rate": 0.0001248252640304426,
        "epoch": 0.8076552388820046,
        "step": 5866
    },
    {
        "loss": 2.4836,
        "grad_norm": 1.5128732919692993,
        "learning_rate": 0.00012469910681785123,
        "epoch": 0.8077929230345587,
        "step": 5867
    },
    {
        "loss": 2.1958,
        "grad_norm": 2.2275803089141846,
        "learning_rate": 0.00012457290772722605,
        "epoch": 0.8079306071871127,
        "step": 5868
    },
    {
        "loss": 2.8031,
        "grad_norm": 1.522911548614502,
        "learning_rate": 0.00012444666697254111,
        "epoch": 0.8080682913396668,
        "step": 5869
    },
    {
        "loss": 2.0488,
        "grad_norm": 1.238664984703064,
        "learning_rate": 0.000124320384767841,
        "epoch": 0.8082059754922208,
        "step": 5870
    },
    {
        "loss": 2.1837,
        "grad_norm": 1.5624563694000244,
        "learning_rate": 0.00012419406132724108,
        "epoch": 0.8083436596447748,
        "step": 5871
    },
    {
        "loss": 1.4768,
        "grad_norm": 2.061495304107666,
        "learning_rate": 0.00012406769686492612,
        "epoch": 0.8084813437973289,
        "step": 5872
    },
    {
        "loss": 2.3101,
        "grad_norm": 1.538439393043518,
        "learning_rate": 0.0001239412915951507,
        "epoch": 0.8086190279498829,
        "step": 5873
    },
    {
        "loss": 1.5243,
        "grad_norm": 3.0727500915527344,
        "learning_rate": 0.00012381484573223848,
        "epoch": 0.808756712102437,
        "step": 5874
    },
    {
        "loss": 2.4944,
        "grad_norm": 1.6551011800765991,
        "learning_rate": 0.00012368835949058205,
        "epoch": 0.808894396254991,
        "step": 5875
    },
    {
        "loss": 2.3443,
        "grad_norm": 1.9512211084365845,
        "learning_rate": 0.0001235618330846422,
        "epoch": 0.809032080407545,
        "step": 5876
    },
    {
        "loss": 2.5231,
        "grad_norm": 2.0321834087371826,
        "learning_rate": 0.00012343526672894834,
        "epoch": 0.8091697645600991,
        "step": 5877
    },
    {
        "loss": 1.7954,
        "grad_norm": 2.409586191177368,
        "learning_rate": 0.00012330866063809718,
        "epoch": 0.8093074487126531,
        "step": 5878
    },
    {
        "loss": 1.7035,
        "grad_norm": 2.2091891765594482,
        "learning_rate": 0.0001231820150267529,
        "epoch": 0.8094451328652073,
        "step": 5879
    },
    {
        "loss": 2.0238,
        "grad_norm": 1.6436450481414795,
        "learning_rate": 0.00012305533010964663,
        "epoch": 0.8095828170177612,
        "step": 5880
    },
    {
        "loss": 1.4444,
        "grad_norm": 2.178873300552368,
        "learning_rate": 0.00012292860610157647,
        "epoch": 0.8097205011703152,
        "step": 5881
    },
    {
        "loss": 1.3437,
        "grad_norm": 2.71018123626709,
        "learning_rate": 0.00012280184321740635,
        "epoch": 0.8098581853228694,
        "step": 5882
    },
    {
        "loss": 2.1633,
        "grad_norm": 1.9531974792480469,
        "learning_rate": 0.0001226750416720663,
        "epoch": 0.8099958694754233,
        "step": 5883
    },
    {
        "loss": 2.0239,
        "grad_norm": 1.4020482301712036,
        "learning_rate": 0.00012254820168055213,
        "epoch": 0.8101335536279775,
        "step": 5884
    },
    {
        "loss": 1.8169,
        "grad_norm": 1.7445766925811768,
        "learning_rate": 0.00012242132345792453,
        "epoch": 0.8102712377805315,
        "step": 5885
    },
    {
        "loss": 2.1855,
        "grad_norm": 2.1716601848602295,
        "learning_rate": 0.00012229440721930898,
        "epoch": 0.8104089219330854,
        "step": 5886
    },
    {
        "loss": 2.0973,
        "grad_norm": 2.940972089767456,
        "learning_rate": 0.0001221674531798958,
        "epoch": 0.8105466060856396,
        "step": 5887
    },
    {
        "loss": 2.4536,
        "grad_norm": 1.838004469871521,
        "learning_rate": 0.00012204046155493901,
        "epoch": 0.8106842902381936,
        "step": 5888
    },
    {
        "loss": 1.3404,
        "grad_norm": 2.315613269805908,
        "learning_rate": 0.00012191343255975654,
        "epoch": 0.8108219743907477,
        "step": 5889
    },
    {
        "loss": 2.1621,
        "grad_norm": 1.2657909393310547,
        "learning_rate": 0.00012178636640972959,
        "epoch": 0.8109596585433017,
        "step": 5890
    },
    {
        "loss": 0.9905,
        "grad_norm": 2.460824728012085,
        "learning_rate": 0.00012165926332030246,
        "epoch": 0.8110973426958558,
        "step": 5891
    },
    {
        "loss": 1.813,
        "grad_norm": 2.196277379989624,
        "learning_rate": 0.00012153212350698184,
        "epoch": 0.8112350268484098,
        "step": 5892
    },
    {
        "loss": 1.9175,
        "grad_norm": 2.483083963394165,
        "learning_rate": 0.00012140494718533721,
        "epoch": 0.8113727110009638,
        "step": 5893
    },
    {
        "loss": 2.0157,
        "grad_norm": 1.4804303646087646,
        "learning_rate": 0.00012127773457099944,
        "epoch": 0.8115103951535179,
        "step": 5894
    },
    {
        "loss": 2.1281,
        "grad_norm": 2.4554929733276367,
        "learning_rate": 0.000121150485879661,
        "epoch": 0.8116480793060719,
        "step": 5895
    },
    {
        "loss": 2.1673,
        "grad_norm": 3.7676055431365967,
        "learning_rate": 0.00012102320132707592,
        "epoch": 0.811785763458626,
        "step": 5896
    },
    {
        "loss": 1.4915,
        "grad_norm": 2.5456666946411133,
        "learning_rate": 0.00012089588112905863,
        "epoch": 0.81192344761118,
        "step": 5897
    },
    {
        "loss": 1.7939,
        "grad_norm": 2.430981397628784,
        "learning_rate": 0.00012076852550148405,
        "epoch": 0.812061131763734,
        "step": 5898
    },
    {
        "loss": 2.5252,
        "grad_norm": 1.525554895401001,
        "learning_rate": 0.00012064113466028734,
        "epoch": 0.8121988159162881,
        "step": 5899
    },
    {
        "loss": 1.8989,
        "grad_norm": 1.5069398880004883,
        "learning_rate": 0.00012051370882146327,
        "epoch": 0.8123365000688421,
        "step": 5900
    },
    {
        "loss": 1.3893,
        "grad_norm": 4.413422584533691,
        "learning_rate": 0.00012038624820106572,
        "epoch": 0.8124741842213962,
        "step": 5901
    },
    {
        "loss": 1.2516,
        "grad_norm": 1.8908206224441528,
        "learning_rate": 0.00012025875301520814,
        "epoch": 0.8126118683739502,
        "step": 5902
    },
    {
        "loss": 1.6312,
        "grad_norm": 2.1097559928894043,
        "learning_rate": 0.00012013122348006207,
        "epoch": 0.8127495525265042,
        "step": 5903
    },
    {
        "loss": 1.8974,
        "grad_norm": 2.1412289142608643,
        "learning_rate": 0.00012000365981185729,
        "epoch": 0.8128872366790583,
        "step": 5904
    },
    {
        "loss": 2.3949,
        "grad_norm": 1.6850507259368896,
        "learning_rate": 0.00011987606222688185,
        "epoch": 0.8130249208316123,
        "step": 5905
    },
    {
        "loss": 1.6301,
        "grad_norm": 4.197201251983643,
        "learning_rate": 0.000119748430941481,
        "epoch": 0.8131626049841664,
        "step": 5906
    },
    {
        "loss": 1.7814,
        "grad_norm": 1.6816972494125366,
        "learning_rate": 0.00011962076617205697,
        "epoch": 0.8133002891367204,
        "step": 5907
    },
    {
        "loss": 1.9748,
        "grad_norm": 1.3314590454101562,
        "learning_rate": 0.00011949306813506935,
        "epoch": 0.8134379732892744,
        "step": 5908
    },
    {
        "loss": 1.189,
        "grad_norm": 2.4896962642669678,
        "learning_rate": 0.00011936533704703359,
        "epoch": 0.8135756574418285,
        "step": 5909
    },
    {
        "loss": 1.1369,
        "grad_norm": 3.0366482734680176,
        "learning_rate": 0.00011923757312452117,
        "epoch": 0.8137133415943825,
        "step": 5910
    },
    {
        "loss": 1.9138,
        "grad_norm": 1.6203747987747192,
        "learning_rate": 0.0001191097765841598,
        "epoch": 0.8138510257469366,
        "step": 5911
    },
    {
        "loss": 1.8166,
        "grad_norm": 2.0561513900756836,
        "learning_rate": 0.00011898194764263197,
        "epoch": 0.8139887098994906,
        "step": 5912
    },
    {
        "loss": 0.7725,
        "grad_norm": 3.676506519317627,
        "learning_rate": 0.0001188540865166752,
        "epoch": 0.8141263940520446,
        "step": 5913
    },
    {
        "loss": 1.8284,
        "grad_norm": 1.5252267122268677,
        "learning_rate": 0.00011872619342308171,
        "epoch": 0.8142640782045987,
        "step": 5914
    },
    {
        "loss": 1.2693,
        "grad_norm": 3.0612456798553467,
        "learning_rate": 0.00011859826857869785,
        "epoch": 0.8144017623571527,
        "step": 5915
    },
    {
        "loss": 1.2703,
        "grad_norm": 4.318853378295898,
        "learning_rate": 0.00011847031220042364,
        "epoch": 0.8145394465097068,
        "step": 5916
    },
    {
        "loss": 2.0995,
        "grad_norm": 1.916958212852478,
        "learning_rate": 0.00011834232450521304,
        "epoch": 0.8146771306622608,
        "step": 5917
    },
    {
        "loss": 2.8174,
        "grad_norm": 1.916439414024353,
        "learning_rate": 0.00011821430571007263,
        "epoch": 0.8148148148148148,
        "step": 5918
    },
    {
        "loss": 2.4505,
        "grad_norm": 2.1618664264678955,
        "learning_rate": 0.0001180862560320618,
        "epoch": 0.8149524989673689,
        "step": 5919
    },
    {
        "loss": 2.2632,
        "grad_norm": 3.1701231002807617,
        "learning_rate": 0.00011795817568829269,
        "epoch": 0.8150901831199229,
        "step": 5920
    },
    {
        "loss": 2.1064,
        "grad_norm": 2.218892812728882,
        "learning_rate": 0.00011783006489592896,
        "epoch": 0.815227867272477,
        "step": 5921
    },
    {
        "loss": 1.4539,
        "grad_norm": 2.4779398441314697,
        "learning_rate": 0.0001177019238721861,
        "epoch": 0.815365551425031,
        "step": 5922
    },
    {
        "loss": 1.6211,
        "grad_norm": 3.0910189151763916,
        "learning_rate": 0.00011757375283433083,
        "epoch": 0.815503235577585,
        "step": 5923
    },
    {
        "loss": 2.4538,
        "grad_norm": 1.4004571437835693,
        "learning_rate": 0.00011744555199968077,
        "epoch": 0.8156409197301391,
        "step": 5924
    },
    {
        "loss": 1.7144,
        "grad_norm": 2.8786556720733643,
        "learning_rate": 0.00011731732158560396,
        "epoch": 0.8157786038826931,
        "step": 5925
    },
    {
        "loss": 2.425,
        "grad_norm": 1.1037046909332275,
        "learning_rate": 0.0001171890618095189,
        "epoch": 0.8159162880352472,
        "step": 5926
    },
    {
        "loss": 2.3027,
        "grad_norm": 1.2778559923171997,
        "learning_rate": 0.00011706077288889359,
        "epoch": 0.8160539721878012,
        "step": 5927
    },
    {
        "loss": 1.654,
        "grad_norm": 2.6700189113616943,
        "learning_rate": 0.00011693245504124532,
        "epoch": 0.8161916563403552,
        "step": 5928
    },
    {
        "loss": 1.1115,
        "grad_norm": 2.1647233963012695,
        "learning_rate": 0.00011680410848414098,
        "epoch": 0.8163293404929093,
        "step": 5929
    },
    {
        "loss": 1.3103,
        "grad_norm": 3.303069591522217,
        "learning_rate": 0.0001166757334351956,
        "epoch": 0.8164670246454633,
        "step": 5930
    },
    {
        "loss": 2.6444,
        "grad_norm": 1.689761757850647,
        "learning_rate": 0.00011654733011207257,
        "epoch": 0.8166047087980174,
        "step": 5931
    },
    {
        "loss": 0.6435,
        "grad_norm": 1.5682270526885986,
        "learning_rate": 0.00011641889873248367,
        "epoch": 0.8167423929505714,
        "step": 5932
    },
    {
        "loss": 1.8074,
        "grad_norm": 2.80307936668396,
        "learning_rate": 0.00011629043951418774,
        "epoch": 0.8168800771031254,
        "step": 5933
    },
    {
        "loss": 2.0375,
        "grad_norm": 1.1529979705810547,
        "learning_rate": 0.0001161619526749909,
        "epoch": 0.8170177612556795,
        "step": 5934
    },
    {
        "loss": 2.2022,
        "grad_norm": 2.9605813026428223,
        "learning_rate": 0.0001160334384327465,
        "epoch": 0.8171554454082335,
        "step": 5935
    },
    {
        "loss": 2.4225,
        "grad_norm": 1.8221780061721802,
        "learning_rate": 0.00011590489700535397,
        "epoch": 0.8172931295607876,
        "step": 5936
    },
    {
        "loss": 1.8467,
        "grad_norm": 3.45731258392334,
        "learning_rate": 0.00011577632861075884,
        "epoch": 0.8174308137133416,
        "step": 5937
    },
    {
        "loss": 1.7078,
        "grad_norm": 1.9408148527145386,
        "learning_rate": 0.00011564773346695255,
        "epoch": 0.8175684978658956,
        "step": 5938
    },
    {
        "loss": 2.3953,
        "grad_norm": 1.8305366039276123,
        "learning_rate": 0.00011551911179197175,
        "epoch": 0.8177061820184497,
        "step": 5939
    },
    {
        "loss": 1.2159,
        "grad_norm": 3.1491000652313232,
        "learning_rate": 0.00011539046380389802,
        "epoch": 0.8178438661710037,
        "step": 5940
    },
    {
        "loss": 1.4406,
        "grad_norm": 3.2168917655944824,
        "learning_rate": 0.0001152617897208579,
        "epoch": 0.8179815503235578,
        "step": 5941
    },
    {
        "loss": 2.1053,
        "grad_norm": 2.303802967071533,
        "learning_rate": 0.00011513308976102184,
        "epoch": 0.8181192344761118,
        "step": 5942
    },
    {
        "loss": 1.4029,
        "grad_norm": 2.261082887649536,
        "learning_rate": 0.00011500436414260412,
        "epoch": 0.8182569186286658,
        "step": 5943
    },
    {
        "loss": 1.8103,
        "grad_norm": 1.6451301574707031,
        "learning_rate": 0.00011487561308386288,
        "epoch": 0.8183946027812199,
        "step": 5944
    },
    {
        "loss": 1.8627,
        "grad_norm": 2.946073293685913,
        "learning_rate": 0.0001147468368030991,
        "epoch": 0.8185322869337739,
        "step": 5945
    },
    {
        "loss": 1.9552,
        "grad_norm": 3.079747200012207,
        "learning_rate": 0.0001146180355186566,
        "epoch": 0.818669971086328,
        "step": 5946
    },
    {
        "loss": 2.1259,
        "grad_norm": 3.1130738258361816,
        "learning_rate": 0.00011448920944892158,
        "epoch": 0.818807655238882,
        "step": 5947
    },
    {
        "loss": 2.4148,
        "grad_norm": 2.569654941558838,
        "learning_rate": 0.0001143603588123223,
        "epoch": 0.8189453393914361,
        "step": 5948
    },
    {
        "loss": 0.7952,
        "grad_norm": 3.9469923973083496,
        "learning_rate": 0.00011423148382732851,
        "epoch": 0.8190830235439901,
        "step": 5949
    },
    {
        "loss": 1.8818,
        "grad_norm": 2.313769817352295,
        "learning_rate": 0.00011410258471245167,
        "epoch": 0.8192207076965441,
        "step": 5950
    },
    {
        "loss": 2.0231,
        "grad_norm": 1.8926059007644653,
        "learning_rate": 0.00011397366168624372,
        "epoch": 0.8193583918490982,
        "step": 5951
    },
    {
        "loss": 2.2999,
        "grad_norm": 2.5168745517730713,
        "learning_rate": 0.0001138447149672972,
        "epoch": 0.8194960760016522,
        "step": 5952
    },
    {
        "loss": 1.5713,
        "grad_norm": 2.485116481781006,
        "learning_rate": 0.00011371574477424518,
        "epoch": 0.8196337601542063,
        "step": 5953
    },
    {
        "loss": 1.1164,
        "grad_norm": 3.373734951019287,
        "learning_rate": 0.00011358675132576006,
        "epoch": 0.8197714443067603,
        "step": 5954
    },
    {
        "loss": 2.3203,
        "grad_norm": 1.2457634210586548,
        "learning_rate": 0.00011345773484055384,
        "epoch": 0.8199091284593143,
        "step": 5955
    },
    {
        "loss": 2.0634,
        "grad_norm": 2.100414514541626,
        "learning_rate": 0.00011332869553737784,
        "epoch": 0.8200468126118684,
        "step": 5956
    },
    {
        "loss": 1.9553,
        "grad_norm": 3.1214988231658936,
        "learning_rate": 0.00011319963363502173,
        "epoch": 0.8201844967644224,
        "step": 5957
    },
    {
        "loss": 1.8961,
        "grad_norm": 2.0937163829803467,
        "learning_rate": 0.00011307054935231348,
        "epoch": 0.8203221809169765,
        "step": 5958
    },
    {
        "loss": 2.4516,
        "grad_norm": 2.3089141845703125,
        "learning_rate": 0.00011294144290811935,
        "epoch": 0.8204598650695305,
        "step": 5959
    },
    {
        "loss": 2.0819,
        "grad_norm": 1.5454679727554321,
        "learning_rate": 0.00011281231452134293,
        "epoch": 0.8205975492220845,
        "step": 5960
    },
    {
        "loss": 2.2353,
        "grad_norm": 2.6813724040985107,
        "learning_rate": 0.000112683164410925,
        "epoch": 0.8207352333746386,
        "step": 5961
    },
    {
        "loss": 2.1482,
        "grad_norm": 2.0334529876708984,
        "learning_rate": 0.00011255399279584322,
        "epoch": 0.8208729175271926,
        "step": 5962
    },
    {
        "loss": 2.2896,
        "grad_norm": 2.437284469604492,
        "learning_rate": 0.0001124247998951117,
        "epoch": 0.8210106016797467,
        "step": 5963
    },
    {
        "loss": 1.8728,
        "grad_norm": 1.2500098943710327,
        "learning_rate": 0.0001122955859277807,
        "epoch": 0.8211482858323007,
        "step": 5964
    },
    {
        "loss": 2.4697,
        "grad_norm": 2.5442206859588623,
        "learning_rate": 0.000112166351112936,
        "epoch": 0.8212859699848547,
        "step": 5965
    },
    {
        "loss": 1.4226,
        "grad_norm": 2.776555061340332,
        "learning_rate": 0.00011203709566969913,
        "epoch": 0.8214236541374088,
        "step": 5966
    },
    {
        "loss": 2.0912,
        "grad_norm": 1.813712239265442,
        "learning_rate": 0.00011190781981722627,
        "epoch": 0.8215613382899628,
        "step": 5967
    },
    {
        "loss": 1.9628,
        "grad_norm": 1.8382338285446167,
        "learning_rate": 0.00011177852377470808,
        "epoch": 0.8216990224425169,
        "step": 5968
    },
    {
        "loss": 1.9593,
        "grad_norm": 1.5282132625579834,
        "learning_rate": 0.00011164920776136996,
        "epoch": 0.8218367065950709,
        "step": 5969
    },
    {
        "loss": 2.3464,
        "grad_norm": 2.2605724334716797,
        "learning_rate": 0.00011151987199647073,
        "epoch": 0.8219743907476249,
        "step": 5970
    },
    {
        "loss": 2.4017,
        "grad_norm": 2.391860008239746,
        "learning_rate": 0.00011139051669930285,
        "epoch": 0.822112074900179,
        "step": 5971
    },
    {
        "loss": 2.2028,
        "grad_norm": 1.65144944190979,
        "learning_rate": 0.00011126114208919184,
        "epoch": 0.822249759052733,
        "step": 5972
    },
    {
        "loss": 2.5175,
        "grad_norm": 1.5745922327041626,
        "learning_rate": 0.00011113174838549607,
        "epoch": 0.8223874432052871,
        "step": 5973
    },
    {
        "loss": 1.5478,
        "grad_norm": 1.9754695892333984,
        "learning_rate": 0.00011100233580760604,
        "epoch": 0.8225251273578411,
        "step": 5974
    },
    {
        "loss": 1.7908,
        "grad_norm": 1.8789892196655273,
        "learning_rate": 0.00011087290457494473,
        "epoch": 0.8226628115103951,
        "step": 5975
    },
    {
        "loss": 1.7705,
        "grad_norm": 2.10355806350708,
        "learning_rate": 0.00011074345490696627,
        "epoch": 0.8228004956629492,
        "step": 5976
    },
    {
        "loss": 1.8735,
        "grad_norm": 2.2580513954162598,
        "learning_rate": 0.00011061398702315618,
        "epoch": 0.8229381798155032,
        "step": 5977
    },
    {
        "loss": 2.0548,
        "grad_norm": 1.6775081157684326,
        "learning_rate": 0.00011048450114303113,
        "epoch": 0.8230758639680573,
        "step": 5978
    },
    {
        "loss": 2.3917,
        "grad_norm": 1.3701629638671875,
        "learning_rate": 0.00011035499748613795,
        "epoch": 0.8232135481206113,
        "step": 5979
    },
    {
        "loss": 2.6149,
        "grad_norm": 2.1960911750793457,
        "learning_rate": 0.00011022547627205367,
        "epoch": 0.8233512322731653,
        "step": 5980
    },
    {
        "loss": 2.222,
        "grad_norm": 1.9066280126571655,
        "learning_rate": 0.0001100959377203854,
        "epoch": 0.8234889164257194,
        "step": 5981
    },
    {
        "loss": 1.9091,
        "grad_norm": 2.109410285949707,
        "learning_rate": 0.00010996638205076926,
        "epoch": 0.8236266005782734,
        "step": 5982
    },
    {
        "loss": 2.3378,
        "grad_norm": 1.9330440759658813,
        "learning_rate": 0.00010983680948287045,
        "epoch": 0.8237642847308275,
        "step": 5983
    },
    {
        "loss": 1.933,
        "grad_norm": 3.0947344303131104,
        "learning_rate": 0.00010970722023638318,
        "epoch": 0.8239019688833815,
        "step": 5984
    },
    {
        "loss": 1.6734,
        "grad_norm": 2.48335337638855,
        "learning_rate": 0.00010957761453102959,
        "epoch": 0.8240396530359355,
        "step": 5985
    },
    {
        "loss": 1.146,
        "grad_norm": 3.4062209129333496,
        "learning_rate": 0.00010944799258655975,
        "epoch": 0.8241773371884896,
        "step": 5986
    },
    {
        "loss": 2.3961,
        "grad_norm": 0.9783623814582825,
        "learning_rate": 0.00010931835462275137,
        "epoch": 0.8243150213410436,
        "step": 5987
    },
    {
        "loss": 2.043,
        "grad_norm": 2.4053194522857666,
        "learning_rate": 0.00010918870085940927,
        "epoch": 0.8244527054935977,
        "step": 5988
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.2069108486175537,
        "learning_rate": 0.00010905903151636496,
        "epoch": 0.8245903896461517,
        "step": 5989
    },
    {
        "loss": 1.5923,
        "grad_norm": 2.2267754077911377,
        "learning_rate": 0.00010892934681347671,
        "epoch": 0.8247280737987057,
        "step": 5990
    },
    {
        "loss": 2.0872,
        "grad_norm": 1.600274920463562,
        "learning_rate": 0.00010879964697062848,
        "epoch": 0.8248657579512598,
        "step": 5991
    },
    {
        "loss": 2.044,
        "grad_norm": 3.855018377304077,
        "learning_rate": 0.0001086699322077299,
        "epoch": 0.8250034421038138,
        "step": 5992
    },
    {
        "loss": 2.4319,
        "grad_norm": 1.3963563442230225,
        "learning_rate": 0.00010854020274471625,
        "epoch": 0.8251411262563679,
        "step": 5993
    },
    {
        "loss": 1.4512,
        "grad_norm": 2.4686107635498047,
        "learning_rate": 0.00010841045880154741,
        "epoch": 0.8252788104089219,
        "step": 5994
    },
    {
        "loss": 1.0181,
        "grad_norm": 2.1399619579315186,
        "learning_rate": 0.00010828070059820789,
        "epoch": 0.8254164945614759,
        "step": 5995
    },
    {
        "loss": 2.3388,
        "grad_norm": 1.4989116191864014,
        "learning_rate": 0.0001081509283547064,
        "epoch": 0.82555417871403,
        "step": 5996
    },
    {
        "loss": 0.9184,
        "grad_norm": 1.4699747562408447,
        "learning_rate": 0.00010802114229107542,
        "epoch": 0.825691862866584,
        "step": 5997
    },
    {
        "loss": 1.3541,
        "grad_norm": 1.6464414596557617,
        "learning_rate": 0.00010789134262737077,
        "epoch": 0.8258295470191381,
        "step": 5998
    },
    {
        "loss": 1.2137,
        "grad_norm": 1.6901962757110596,
        "learning_rate": 0.00010776152958367176,
        "epoch": 0.8259672311716921,
        "step": 5999
    },
    {
        "loss": 1.6249,
        "grad_norm": 1.9508211612701416,
        "learning_rate": 0.00010763170338007986,
        "epoch": 0.8261049153242461,
        "step": 6000
    },
    {
        "loss": 1.1839,
        "grad_norm": 1.3323160409927368,
        "learning_rate": 0.000107501864236719,
        "epoch": 0.8262425994768002,
        "step": 6001
    },
    {
        "loss": 2.2108,
        "grad_norm": 1.5123729705810547,
        "learning_rate": 0.00010737201237373535,
        "epoch": 0.8263802836293542,
        "step": 6002
    },
    {
        "loss": 2.0083,
        "grad_norm": 1.3573142290115356,
        "learning_rate": 0.00010724214801129631,
        "epoch": 0.8265179677819083,
        "step": 6003
    },
    {
        "loss": 2.2475,
        "grad_norm": 2.168482780456543,
        "learning_rate": 0.00010711227136959041,
        "epoch": 0.8266556519344623,
        "step": 6004
    },
    {
        "loss": 2.3256,
        "grad_norm": 2.8231940269470215,
        "learning_rate": 0.00010698238266882748,
        "epoch": 0.8267933360870164,
        "step": 6005
    },
    {
        "loss": 1.3857,
        "grad_norm": 3.632497787475586,
        "learning_rate": 0.0001068524821292373,
        "epoch": 0.8269310202395704,
        "step": 6006
    },
    {
        "loss": 2.3576,
        "grad_norm": 1.9166688919067383,
        "learning_rate": 0.00010672256997106983,
        "epoch": 0.8270687043921244,
        "step": 6007
    },
    {
        "loss": 1.417,
        "grad_norm": 3.334899425506592,
        "learning_rate": 0.00010659264641459501,
        "epoch": 0.8272063885446785,
        "step": 6008
    },
    {
        "loss": 1.2403,
        "grad_norm": 4.481107234954834,
        "learning_rate": 0.00010646271168010182,
        "epoch": 0.8273440726972325,
        "step": 6009
    },
    {
        "loss": 2.278,
        "grad_norm": 1.6762502193450928,
        "learning_rate": 0.00010633276598789826,
        "epoch": 0.8274817568497866,
        "step": 6010
    },
    {
        "loss": 1.3595,
        "grad_norm": 2.84437894821167,
        "learning_rate": 0.00010620280955831093,
        "epoch": 0.8276194410023406,
        "step": 6011
    },
    {
        "loss": 2.0062,
        "grad_norm": 1.9813076257705688,
        "learning_rate": 0.00010607284261168462,
        "epoch": 0.8277571251548946,
        "step": 6012
    },
    {
        "loss": 2.2577,
        "grad_norm": 1.662818431854248,
        "learning_rate": 0.00010594286536838182,
        "epoch": 0.8278948093074487,
        "step": 6013
    },
    {
        "loss": 1.9899,
        "grad_norm": 2.9291880130767822,
        "learning_rate": 0.00010581287804878293,
        "epoch": 0.8280324934600027,
        "step": 6014
    },
    {
        "loss": 2.1789,
        "grad_norm": 2.1004273891448975,
        "learning_rate": 0.00010568288087328495,
        "epoch": 0.8281701776125568,
        "step": 6015
    },
    {
        "loss": 2.2959,
        "grad_norm": 2.1878364086151123,
        "learning_rate": 0.00010555287406230163,
        "epoch": 0.8283078617651108,
        "step": 6016
    },
    {
        "loss": 2.318,
        "grad_norm": 1.4089056253433228,
        "learning_rate": 0.00010542285783626348,
        "epoch": 0.8284455459176648,
        "step": 6017
    },
    {
        "loss": 1.9535,
        "grad_norm": 1.6255221366882324,
        "learning_rate": 0.00010529283241561659,
        "epoch": 0.828583230070219,
        "step": 6018
    },
    {
        "loss": 2.0059,
        "grad_norm": 2.4607150554656982,
        "learning_rate": 0.00010516279802082265,
        "epoch": 0.8287209142227729,
        "step": 6019
    },
    {
        "loss": 2.2168,
        "grad_norm": 1.277862787246704,
        "learning_rate": 0.00010503275487235873,
        "epoch": 0.828858598375327,
        "step": 6020
    },
    {
        "loss": 2.1284,
        "grad_norm": 2.976029872894287,
        "learning_rate": 0.00010490270319071663,
        "epoch": 0.828996282527881,
        "step": 6021
    },
    {
        "loss": 1.0749,
        "grad_norm": 2.2513585090637207,
        "learning_rate": 0.00010477264319640252,
        "epoch": 0.829133966680435,
        "step": 6022
    },
    {
        "loss": 1.8067,
        "grad_norm": 1.2818553447723389,
        "learning_rate": 0.00010464257510993707,
        "epoch": 0.8292716508329891,
        "step": 6023
    },
    {
        "loss": 2.2555,
        "grad_norm": 1.4396705627441406,
        "learning_rate": 0.00010451249915185428,
        "epoch": 0.8294093349855431,
        "step": 6024
    },
    {
        "loss": 2.175,
        "grad_norm": 3.853773593902588,
        "learning_rate": 0.00010438241554270142,
        "epoch": 0.8295470191380973,
        "step": 6025
    },
    {
        "loss": 1.7959,
        "grad_norm": 1.4999486207962036,
        "learning_rate": 0.00010425232450303928,
        "epoch": 0.8296847032906512,
        "step": 6026
    },
    {
        "loss": 2.6633,
        "grad_norm": 1.5284520387649536,
        "learning_rate": 0.00010412222625344072,
        "epoch": 0.8298223874432052,
        "step": 6027
    },
    {
        "loss": 1.1992,
        "grad_norm": 3.2758989334106445,
        "learning_rate": 0.00010399212101449086,
        "epoch": 0.8299600715957594,
        "step": 6028
    },
    {
        "loss": 2.0687,
        "grad_norm": 1.6973848342895508,
        "learning_rate": 0.00010386200900678709,
        "epoch": 0.8300977557483133,
        "step": 6029
    },
    {
        "loss": 2.4622,
        "grad_norm": 2.0798237323760986,
        "learning_rate": 0.00010373189045093785,
        "epoch": 0.8302354399008675,
        "step": 6030
    },
    {
        "loss": 1.721,
        "grad_norm": 0.987307071685791,
        "learning_rate": 0.00010360176556756269,
        "epoch": 0.8303731240534215,
        "step": 6031
    },
    {
        "loss": 1.9965,
        "grad_norm": 1.866761326789856,
        "learning_rate": 0.00010347163457729234,
        "epoch": 0.8305108082059754,
        "step": 6032
    },
    {
        "loss": 2.0203,
        "grad_norm": 1.794771671295166,
        "learning_rate": 0.00010334149770076742,
        "epoch": 0.8306484923585296,
        "step": 6033
    },
    {
        "loss": 2.8241,
        "grad_norm": 2.5604841709136963,
        "learning_rate": 0.00010321135515863866,
        "epoch": 0.8307861765110836,
        "step": 6034
    },
    {
        "loss": 2.8575,
        "grad_norm": 2.2077057361602783,
        "learning_rate": 0.00010308120717156644,
        "epoch": 0.8309238606636377,
        "step": 6035
    },
    {
        "loss": 2.2455,
        "grad_norm": 1.622519612312317,
        "learning_rate": 0.00010295105396022035,
        "epoch": 0.8310615448161917,
        "step": 6036
    },
    {
        "loss": 1.8255,
        "grad_norm": 3.245539665222168,
        "learning_rate": 0.00010282089574527871,
        "epoch": 0.8311992289687457,
        "step": 6037
    },
    {
        "loss": 1.3375,
        "grad_norm": 3.0466182231903076,
        "learning_rate": 0.00010269073274742869,
        "epoch": 0.8313369131212998,
        "step": 6038
    },
    {
        "loss": 1.1101,
        "grad_norm": 2.13948655128479,
        "learning_rate": 0.00010256056518736526,
        "epoch": 0.8314745972738538,
        "step": 6039
    },
    {
        "loss": 2.5123,
        "grad_norm": 2.020334005355835,
        "learning_rate": 0.00010243039328579099,
        "epoch": 0.8316122814264079,
        "step": 6040
    },
    {
        "loss": 2.1371,
        "grad_norm": 1.1255073547363281,
        "learning_rate": 0.00010230021726341632,
        "epoch": 0.8317499655789619,
        "step": 6041
    },
    {
        "loss": 2.0885,
        "grad_norm": 2.0347015857696533,
        "learning_rate": 0.0001021700373409583,
        "epoch": 0.8318876497315159,
        "step": 6042
    },
    {
        "loss": 1.9142,
        "grad_norm": 1.7987749576568604,
        "learning_rate": 0.00010203985373914058,
        "epoch": 0.83202533388407,
        "step": 6043
    },
    {
        "loss": 2.3294,
        "grad_norm": 1.7413129806518555,
        "learning_rate": 0.00010190966667869314,
        "epoch": 0.832163018036624,
        "step": 6044
    },
    {
        "loss": 2.132,
        "grad_norm": 2.6614553928375244,
        "learning_rate": 0.0001017794763803519,
        "epoch": 0.8323007021891781,
        "step": 6045
    },
    {
        "loss": 2.1527,
        "grad_norm": 1.4346821308135986,
        "learning_rate": 0.00010164928306485815,
        "epoch": 0.8324383863417321,
        "step": 6046
    },
    {
        "loss": 1.8382,
        "grad_norm": 2.4467248916625977,
        "learning_rate": 0.00010151908695295818,
        "epoch": 0.8325760704942861,
        "step": 6047
    },
    {
        "loss": 2.2935,
        "grad_norm": 2.0216314792633057,
        "learning_rate": 0.00010138888826540346,
        "epoch": 0.8327137546468402,
        "step": 6048
    },
    {
        "loss": 2.3399,
        "grad_norm": 2.331782579421997,
        "learning_rate": 0.00010125868722294947,
        "epoch": 0.8328514387993942,
        "step": 6049
    },
    {
        "loss": 2.4341,
        "grad_norm": 2.2580442428588867,
        "learning_rate": 0.00010112848404635551,
        "epoch": 0.8329891229519483,
        "step": 6050
    },
    {
        "loss": 2.4258,
        "grad_norm": 1.551544189453125,
        "learning_rate": 0.00010099827895638512,
        "epoch": 0.8331268071045023,
        "step": 6051
    },
    {
        "loss": 2.0103,
        "grad_norm": 1.6951584815979004,
        "learning_rate": 0.00010086807217380451,
        "epoch": 0.8332644912570563,
        "step": 6052
    },
    {
        "loss": 1.8142,
        "grad_norm": 2.7380173206329346,
        "learning_rate": 0.00010073786391938287,
        "epoch": 0.8334021754096104,
        "step": 6053
    },
    {
        "loss": 2.2201,
        "grad_norm": 1.2984551191329956,
        "learning_rate": 0.00010060765441389221,
        "epoch": 0.8335398595621644,
        "step": 6054
    },
    {
        "loss": 1.1862,
        "grad_norm": 2.5100021362304688,
        "learning_rate": 0.00010047744387810634,
        "epoch": 0.8336775437147185,
        "step": 6055
    },
    {
        "loss": 2.1629,
        "grad_norm": 1.2524003982543945,
        "learning_rate": 0.00010034723253280069,
        "epoch": 0.8338152278672725,
        "step": 6056
    },
    {
        "loss": 2.0037,
        "grad_norm": 2.4705002307891846,
        "learning_rate": 0.00010021702059875266,
        "epoch": 0.8339529120198265,
        "step": 6057
    },
    {
        "loss": 1.9515,
        "grad_norm": 1.2709141969680786,
        "learning_rate": 0.00010008680829674004,
        "epoch": 0.8340905961723806,
        "step": 6058
    },
    {
        "loss": 2.0252,
        "grad_norm": 1.5137630701065063,
        "learning_rate": 9.995659584754152e-05,
        "epoch": 0.8342282803249346,
        "step": 6059
    },
    {
        "loss": 1.1172,
        "grad_norm": 2.784562826156616,
        "learning_rate": 9.982638347193593e-05,
        "epoch": 0.8343659644774887,
        "step": 6060
    },
    {
        "loss": 2.4512,
        "grad_norm": 1.7658584117889404,
        "learning_rate": 9.969617139070209e-05,
        "epoch": 0.8345036486300427,
        "step": 6061
    },
    {
        "loss": 2.0383,
        "grad_norm": 2.0463368892669678,
        "learning_rate": 9.956595982461811e-05,
        "epoch": 0.8346413327825967,
        "step": 6062
    },
    {
        "loss": 2.0283,
        "grad_norm": 2.505077362060547,
        "learning_rate": 9.943574899446164e-05,
        "epoch": 0.8347790169351508,
        "step": 6063
    },
    {
        "loss": 1.8903,
        "grad_norm": 2.5301029682159424,
        "learning_rate": 9.930553912100873e-05,
        "epoch": 0.8349167010877048,
        "step": 6064
    },
    {
        "loss": 1.8081,
        "grad_norm": 2.2844274044036865,
        "learning_rate": 9.917533042503373e-05,
        "epoch": 0.8350543852402589,
        "step": 6065
    },
    {
        "loss": 2.2518,
        "grad_norm": 3.58178973197937,
        "learning_rate": 9.904512312730944e-05,
        "epoch": 0.8351920693928129,
        "step": 6066
    },
    {
        "loss": 1.947,
        "grad_norm": 2.7406346797943115,
        "learning_rate": 9.891491744860588e-05,
        "epoch": 0.835329753545367,
        "step": 6067
    },
    {
        "loss": 2.1151,
        "grad_norm": 1.3143310546875,
        "learning_rate": 9.878471360969049e-05,
        "epoch": 0.835467437697921,
        "step": 6068
    },
    {
        "loss": 2.5462,
        "grad_norm": 1.7337284088134766,
        "learning_rate": 9.86545118313275e-05,
        "epoch": 0.835605121850475,
        "step": 6069
    },
    {
        "loss": 1.93,
        "grad_norm": 1.8743422031402588,
        "learning_rate": 9.852431233427775e-05,
        "epoch": 0.8357428060030291,
        "step": 6070
    },
    {
        "loss": 1.6979,
        "grad_norm": 2.168156385421753,
        "learning_rate": 9.839411533929804e-05,
        "epoch": 0.8358804901555831,
        "step": 6071
    },
    {
        "loss": 0.9578,
        "grad_norm": 2.3716940879821777,
        "learning_rate": 9.826392106714129e-05,
        "epoch": 0.8360181743081372,
        "step": 6072
    },
    {
        "loss": 0.9326,
        "grad_norm": 2.9857664108276367,
        "learning_rate": 9.813372973855546e-05,
        "epoch": 0.8361558584606912,
        "step": 6073
    },
    {
        "loss": 1.6619,
        "grad_norm": 1.4001520872116089,
        "learning_rate": 9.80035415742835e-05,
        "epoch": 0.8362935426132452,
        "step": 6074
    },
    {
        "loss": 1.4143,
        "grad_norm": 2.9306533336639404,
        "learning_rate": 9.787335679506336e-05,
        "epoch": 0.8364312267657993,
        "step": 6075
    },
    {
        "loss": 2.1765,
        "grad_norm": 1.9730846881866455,
        "learning_rate": 9.77431756216269e-05,
        "epoch": 0.8365689109183533,
        "step": 6076
    },
    {
        "loss": 2.6101,
        "grad_norm": 2.3891587257385254,
        "learning_rate": 9.761299827469987e-05,
        "epoch": 0.8367065950709074,
        "step": 6077
    },
    {
        "loss": 1.9038,
        "grad_norm": 1.9648357629776,
        "learning_rate": 9.748282497500186e-05,
        "epoch": 0.8368442792234614,
        "step": 6078
    },
    {
        "loss": 1.9846,
        "grad_norm": 1.3482156991958618,
        "learning_rate": 9.735265594324529e-05,
        "epoch": 0.8369819633760154,
        "step": 6079
    },
    {
        "loss": 2.2443,
        "grad_norm": 1.8295683860778809,
        "learning_rate": 9.722249140013528e-05,
        "epoch": 0.8371196475285695,
        "step": 6080
    },
    {
        "loss": 1.1737,
        "grad_norm": 1.531162977218628,
        "learning_rate": 9.709233156636974e-05,
        "epoch": 0.8372573316811235,
        "step": 6081
    },
    {
        "loss": 1.564,
        "grad_norm": 2.5876619815826416,
        "learning_rate": 9.696217666263825e-05,
        "epoch": 0.8373950158336776,
        "step": 6082
    },
    {
        "loss": 1.5313,
        "grad_norm": 2.195401906967163,
        "learning_rate": 9.683202690962212e-05,
        "epoch": 0.8375326999862316,
        "step": 6083
    },
    {
        "loss": 1.6135,
        "grad_norm": 2.1263034343719482,
        "learning_rate": 9.670188252799394e-05,
        "epoch": 0.8376703841387856,
        "step": 6084
    },
    {
        "loss": 1.6819,
        "grad_norm": 2.0384361743927,
        "learning_rate": 9.657174373841715e-05,
        "epoch": 0.8378080682913397,
        "step": 6085
    },
    {
        "loss": 1.8572,
        "grad_norm": 2.2623238563537598,
        "learning_rate": 9.644161076154568e-05,
        "epoch": 0.8379457524438937,
        "step": 6086
    },
    {
        "loss": 0.7745,
        "grad_norm": 3.866434097290039,
        "learning_rate": 9.631148381802389e-05,
        "epoch": 0.8380834365964478,
        "step": 6087
    },
    {
        "loss": 1.9146,
        "grad_norm": 2.3480312824249268,
        "learning_rate": 9.618136312848554e-05,
        "epoch": 0.8382211207490018,
        "step": 6088
    },
    {
        "loss": 2.1288,
        "grad_norm": 1.643660306930542,
        "learning_rate": 9.605124891355386e-05,
        "epoch": 0.8383588049015558,
        "step": 6089
    },
    {
        "loss": 2.025,
        "grad_norm": 2.1344308853149414,
        "learning_rate": 9.592114139384142e-05,
        "epoch": 0.8384964890541099,
        "step": 6090
    },
    {
        "loss": 2.3565,
        "grad_norm": 2.8582632541656494,
        "learning_rate": 9.579104078994906e-05,
        "epoch": 0.8386341732066639,
        "step": 6091
    },
    {
        "loss": 1.6115,
        "grad_norm": 1.8973838090896606,
        "learning_rate": 9.566094732246604e-05,
        "epoch": 0.838771857359218,
        "step": 6092
    },
    {
        "loss": 1.6566,
        "grad_norm": 2.489333152770996,
        "learning_rate": 9.553086121196952e-05,
        "epoch": 0.838909541511772,
        "step": 6093
    },
    {
        "loss": 1.2451,
        "grad_norm": 2.3033714294433594,
        "learning_rate": 9.540078267902419e-05,
        "epoch": 0.839047225664326,
        "step": 6094
    },
    {
        "loss": 1.946,
        "grad_norm": 3.2692933082580566,
        "learning_rate": 9.527071194418167e-05,
        "epoch": 0.8391849098168801,
        "step": 6095
    },
    {
        "loss": 2.356,
        "grad_norm": 1.1258180141448975,
        "learning_rate": 9.514064922798093e-05,
        "epoch": 0.8393225939694341,
        "step": 6096
    },
    {
        "loss": 1.7985,
        "grad_norm": 1.9555132389068604,
        "learning_rate": 9.50105947509468e-05,
        "epoch": 0.8394602781219882,
        "step": 6097
    },
    {
        "loss": 2.2398,
        "grad_norm": 2.3074097633361816,
        "learning_rate": 9.48805487335902e-05,
        "epoch": 0.8395979622745422,
        "step": 6098
    },
    {
        "loss": 1.9094,
        "grad_norm": 1.7162261009216309,
        "learning_rate": 9.475051139640809e-05,
        "epoch": 0.8397356464270962,
        "step": 6099
    },
    {
        "loss": 0.8792,
        "grad_norm": 2.004347801208496,
        "learning_rate": 9.46204829598823e-05,
        "epoch": 0.8398733305796503,
        "step": 6100
    },
    {
        "loss": 2.0012,
        "grad_norm": 1.80429208278656,
        "learning_rate": 9.449046364447966e-05,
        "epoch": 0.8400110147322043,
        "step": 6101
    },
    {
        "loss": 1.5123,
        "grad_norm": 1.7978184223175049,
        "learning_rate": 9.43604536706518e-05,
        "epoch": 0.8401486988847584,
        "step": 6102
    },
    {
        "loss": 2.1851,
        "grad_norm": 1.6244449615478516,
        "learning_rate": 9.423045325883415e-05,
        "epoch": 0.8402863830373124,
        "step": 6103
    },
    {
        "loss": 1.8384,
        "grad_norm": 2.1164588928222656,
        "learning_rate": 9.410046262944602e-05,
        "epoch": 0.8404240671898664,
        "step": 6104
    },
    {
        "loss": 2.1275,
        "grad_norm": 4.056552886962891,
        "learning_rate": 9.397048200289042e-05,
        "epoch": 0.8405617513424205,
        "step": 6105
    },
    {
        "loss": 1.5467,
        "grad_norm": 1.9577643871307373,
        "learning_rate": 9.384051159955305e-05,
        "epoch": 0.8406994354949745,
        "step": 6106
    },
    {
        "loss": 1.756,
        "grad_norm": 1.5117430686950684,
        "learning_rate": 9.371055163980241e-05,
        "epoch": 0.8408371196475286,
        "step": 6107
    },
    {
        "loss": 1.2264,
        "grad_norm": 2.386446475982666,
        "learning_rate": 9.35806023439893e-05,
        "epoch": 0.8409748038000826,
        "step": 6108
    },
    {
        "loss": 1.1707,
        "grad_norm": 2.4739890098571777,
        "learning_rate": 9.34506639324464e-05,
        "epoch": 0.8411124879526366,
        "step": 6109
    },
    {
        "loss": 2.1503,
        "grad_norm": 2.458806276321411,
        "learning_rate": 9.33207366254878e-05,
        "epoch": 0.8412501721051907,
        "step": 6110
    },
    {
        "loss": 1.623,
        "grad_norm": 2.2804582118988037,
        "learning_rate": 9.319082064340927e-05,
        "epoch": 0.8413878562577447,
        "step": 6111
    },
    {
        "loss": 1.8621,
        "grad_norm": 1.0848970413208008,
        "learning_rate": 9.306091620648684e-05,
        "epoch": 0.8415255404102988,
        "step": 6112
    },
    {
        "loss": 2.2661,
        "grad_norm": 1.522627592086792,
        "learning_rate": 9.293102353497701e-05,
        "epoch": 0.8416632245628528,
        "step": 6113
    },
    {
        "loss": 1.1676,
        "grad_norm": 2.7848527431488037,
        "learning_rate": 9.28011428491168e-05,
        "epoch": 0.8418009087154068,
        "step": 6114
    },
    {
        "loss": 2.1863,
        "grad_norm": 1.0662709474563599,
        "learning_rate": 9.26712743691224e-05,
        "epoch": 0.8419385928679609,
        "step": 6115
    },
    {
        "loss": 2.1972,
        "grad_norm": 1.8678009510040283,
        "learning_rate": 9.25414183151895e-05,
        "epoch": 0.8420762770205149,
        "step": 6116
    },
    {
        "loss": 2.145,
        "grad_norm": 2.1752984523773193,
        "learning_rate": 9.241157490749274e-05,
        "epoch": 0.842213961173069,
        "step": 6117
    },
    {
        "loss": 2.0486,
        "grad_norm": 1.7632482051849365,
        "learning_rate": 9.228174436618526e-05,
        "epoch": 0.842351645325623,
        "step": 6118
    },
    {
        "loss": 2.1822,
        "grad_norm": 2.739980459213257,
        "learning_rate": 9.215192691139827e-05,
        "epoch": 0.842489329478177,
        "step": 6119
    },
    {
        "loss": 2.244,
        "grad_norm": 2.4566099643707275,
        "learning_rate": 9.202212276324124e-05,
        "epoch": 0.8426270136307311,
        "step": 6120
    },
    {
        "loss": 2.1727,
        "grad_norm": 1.5688138008117676,
        "learning_rate": 9.189233214180063e-05,
        "epoch": 0.8427646977832851,
        "step": 6121
    },
    {
        "loss": 2.4339,
        "grad_norm": 1.4249534606933594,
        "learning_rate": 9.176255526713993e-05,
        "epoch": 0.8429023819358392,
        "step": 6122
    },
    {
        "loss": 1.6616,
        "grad_norm": 1.8988438844680786,
        "learning_rate": 9.163279235929983e-05,
        "epoch": 0.8430400660883932,
        "step": 6123
    },
    {
        "loss": 2.287,
        "grad_norm": 1.5314621925354004,
        "learning_rate": 9.150304363829684e-05,
        "epoch": 0.8431777502409473,
        "step": 6124
    },
    {
        "loss": 1.4652,
        "grad_norm": 2.3728559017181396,
        "learning_rate": 9.137330932412345e-05,
        "epoch": 0.8433154343935013,
        "step": 6125
    },
    {
        "loss": 2.1309,
        "grad_norm": 2.6636276245117188,
        "learning_rate": 9.124358963674814e-05,
        "epoch": 0.8434531185460553,
        "step": 6126
    },
    {
        "loss": 2.0526,
        "grad_norm": 1.8792905807495117,
        "learning_rate": 9.111388479611418e-05,
        "epoch": 0.8435908026986094,
        "step": 6127
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.3573532104492188,
        "learning_rate": 9.098419502213961e-05,
        "epoch": 0.8437284868511634,
        "step": 6128
    },
    {
        "loss": 1.8968,
        "grad_norm": 1.596100926399231,
        "learning_rate": 9.085452053471744e-05,
        "epoch": 0.8438661710037175,
        "step": 6129
    },
    {
        "loss": 2.3104,
        "grad_norm": 2.011958360671997,
        "learning_rate": 9.072486155371427e-05,
        "epoch": 0.8440038551562715,
        "step": 6130
    },
    {
        "loss": 2.1162,
        "grad_norm": 2.1131768226623535,
        "learning_rate": 9.059521829897056e-05,
        "epoch": 0.8441415393088255,
        "step": 6131
    },
    {
        "loss": 2.0103,
        "grad_norm": 1.7676938772201538,
        "learning_rate": 9.046559099030012e-05,
        "epoch": 0.8442792234613796,
        "step": 6132
    },
    {
        "loss": 1.8052,
        "grad_norm": 2.3066811561584473,
        "learning_rate": 9.033597984748972e-05,
        "epoch": 0.8444169076139336,
        "step": 6133
    },
    {
        "loss": 1.771,
        "grad_norm": 2.0400912761688232,
        "learning_rate": 9.020638509029872e-05,
        "epoch": 0.8445545917664877,
        "step": 6134
    },
    {
        "loss": 1.6472,
        "grad_norm": 1.984334945678711,
        "learning_rate": 9.007680693845854e-05,
        "epoch": 0.8446922759190417,
        "step": 6135
    },
    {
        "loss": 2.2515,
        "grad_norm": 1.796033501625061,
        "learning_rate": 8.99472456116729e-05,
        "epoch": 0.8448299600715957,
        "step": 6136
    },
    {
        "loss": 1.514,
        "grad_norm": 2.088735818862915,
        "learning_rate": 8.981770132961653e-05,
        "epoch": 0.8449676442241498,
        "step": 6137
    },
    {
        "loss": 2.1987,
        "grad_norm": 1.7941008806228638,
        "learning_rate": 8.968817431193529e-05,
        "epoch": 0.8451053283767038,
        "step": 6138
    },
    {
        "loss": 2.0359,
        "grad_norm": 1.30092191696167,
        "learning_rate": 8.955866477824619e-05,
        "epoch": 0.8452430125292579,
        "step": 6139
    },
    {
        "loss": 2.0334,
        "grad_norm": 1.4332809448242188,
        "learning_rate": 8.942917294813617e-05,
        "epoch": 0.8453806966818119,
        "step": 6140
    },
    {
        "loss": 1.4087,
        "grad_norm": 1.6017982959747314,
        "learning_rate": 8.929969904116226e-05,
        "epoch": 0.8455183808343659,
        "step": 6141
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.6629254817962646,
        "learning_rate": 8.917024327685117e-05,
        "epoch": 0.84565606498692,
        "step": 6142
    },
    {
        "loss": 1.05,
        "grad_norm": 3.6413562297821045,
        "learning_rate": 8.90408058746988e-05,
        "epoch": 0.845793749139474,
        "step": 6143
    },
    {
        "loss": 2.0372,
        "grad_norm": 1.836536169052124,
        "learning_rate": 8.891138705416974e-05,
        "epoch": 0.8459314332920281,
        "step": 6144
    },
    {
        "loss": 2.4491,
        "grad_norm": 1.5589758157730103,
        "learning_rate": 8.878198703469759e-05,
        "epoch": 0.8460691174445821,
        "step": 6145
    },
    {
        "loss": 1.0169,
        "grad_norm": 2.9202003479003906,
        "learning_rate": 8.86526060356836e-05,
        "epoch": 0.8462068015971361,
        "step": 6146
    },
    {
        "loss": 2.3706,
        "grad_norm": 2.652992010116577,
        "learning_rate": 8.852324427649671e-05,
        "epoch": 0.8463444857496902,
        "step": 6147
    },
    {
        "loss": 1.6643,
        "grad_norm": 5.761599063873291,
        "learning_rate": 8.839390197647381e-05,
        "epoch": 0.8464821699022442,
        "step": 6148
    },
    {
        "loss": 2.0522,
        "grad_norm": 1.8055294752120972,
        "learning_rate": 8.826457935491823e-05,
        "epoch": 0.8466198540547983,
        "step": 6149
    },
    {
        "loss": 2.2432,
        "grad_norm": 2.1138577461242676,
        "learning_rate": 8.813527663109998e-05,
        "epoch": 0.8467575382073523,
        "step": 6150
    },
    {
        "loss": 2.2016,
        "grad_norm": 1.554972767829895,
        "learning_rate": 8.80059940242558e-05,
        "epoch": 0.8468952223599063,
        "step": 6151
    },
    {
        "loss": 1.8422,
        "grad_norm": 1.4327425956726074,
        "learning_rate": 8.787673175358788e-05,
        "epoch": 0.8470329065124604,
        "step": 6152
    },
    {
        "loss": 1.6267,
        "grad_norm": 2.807943105697632,
        "learning_rate": 8.774749003826386e-05,
        "epoch": 0.8471705906650144,
        "step": 6153
    },
    {
        "loss": 1.3239,
        "grad_norm": 2.4574201107025146,
        "learning_rate": 8.761826909741703e-05,
        "epoch": 0.8473082748175685,
        "step": 6154
    },
    {
        "loss": 1.7535,
        "grad_norm": 2.684490919113159,
        "learning_rate": 8.748906915014502e-05,
        "epoch": 0.8474459589701225,
        "step": 6155
    },
    {
        "loss": 2.4339,
        "grad_norm": 1.8534290790557861,
        "learning_rate": 8.735989041550996e-05,
        "epoch": 0.8475836431226765,
        "step": 6156
    },
    {
        "loss": 1.7211,
        "grad_norm": 3.103924512863159,
        "learning_rate": 8.723073311253807e-05,
        "epoch": 0.8477213272752306,
        "step": 6157
    },
    {
        "loss": 2.1241,
        "grad_norm": 2.2284820079803467,
        "learning_rate": 8.710159746021923e-05,
        "epoch": 0.8478590114277846,
        "step": 6158
    },
    {
        "loss": 2.3444,
        "grad_norm": 1.4797639846801758,
        "learning_rate": 8.69724836775064e-05,
        "epoch": 0.8479966955803387,
        "step": 6159
    },
    {
        "loss": 1.6535,
        "grad_norm": 2.6211698055267334,
        "learning_rate": 8.684339198331599e-05,
        "epoch": 0.8481343797328927,
        "step": 6160
    },
    {
        "loss": 2.1494,
        "grad_norm": 2.1995718479156494,
        "learning_rate": 8.67143225965264e-05,
        "epoch": 0.8482720638854467,
        "step": 6161
    },
    {
        "loss": 1.6237,
        "grad_norm": 3.8789443969726562,
        "learning_rate": 8.658527573597835e-05,
        "epoch": 0.8484097480380008,
        "step": 6162
    },
    {
        "loss": 1.8812,
        "grad_norm": 2.5378267765045166,
        "learning_rate": 8.64562516204747e-05,
        "epoch": 0.8485474321905548,
        "step": 6163
    },
    {
        "loss": 2.0855,
        "grad_norm": 2.8702847957611084,
        "learning_rate": 8.632725046877935e-05,
        "epoch": 0.848685116343109,
        "step": 6164
    },
    {
        "loss": 2.4358,
        "grad_norm": 2.2349729537963867,
        "learning_rate": 8.619827249961737e-05,
        "epoch": 0.8488228004956629,
        "step": 6165
    },
    {
        "loss": 1.6418,
        "grad_norm": 2.1550354957580566,
        "learning_rate": 8.606931793167458e-05,
        "epoch": 0.8489604846482169,
        "step": 6166
    },
    {
        "loss": 2.2725,
        "grad_norm": 2.9588229656219482,
        "learning_rate": 8.59403869835971e-05,
        "epoch": 0.849098168800771,
        "step": 6167
    },
    {
        "loss": 1.5301,
        "grad_norm": 1.5774402618408203,
        "learning_rate": 8.581147987399085e-05,
        "epoch": 0.849235852953325,
        "step": 6168
    },
    {
        "loss": 2.1043,
        "grad_norm": 1.2605457305908203,
        "learning_rate": 8.568259682142175e-05,
        "epoch": 0.8493735371058792,
        "step": 6169
    },
    {
        "loss": 1.5662,
        "grad_norm": 2.188809633255005,
        "learning_rate": 8.555373804441454e-05,
        "epoch": 0.8495112212584331,
        "step": 6170
    },
    {
        "loss": 1.7656,
        "grad_norm": 3.771470546722412,
        "learning_rate": 8.542490376145277e-05,
        "epoch": 0.8496489054109871,
        "step": 6171
    },
    {
        "loss": 1.9056,
        "grad_norm": 2.5515575408935547,
        "learning_rate": 8.529609419097886e-05,
        "epoch": 0.8497865895635413,
        "step": 6172
    },
    {
        "loss": 1.7659,
        "grad_norm": 2.443507194519043,
        "learning_rate": 8.516730955139299e-05,
        "epoch": 0.8499242737160952,
        "step": 6173
    },
    {
        "loss": 1.0027,
        "grad_norm": 1.8580135107040405,
        "learning_rate": 8.503855006105299e-05,
        "epoch": 0.8500619578686494,
        "step": 6174
    },
    {
        "loss": 2.2496,
        "grad_norm": 2.677077293395996,
        "learning_rate": 8.490981593827449e-05,
        "epoch": 0.8501996420212034,
        "step": 6175
    },
    {
        "loss": 1.631,
        "grad_norm": 2.6833431720733643,
        "learning_rate": 8.47811074013297e-05,
        "epoch": 0.8503373261737573,
        "step": 6176
    },
    {
        "loss": 1.3774,
        "grad_norm": 2.2292354106903076,
        "learning_rate": 8.465242466844748e-05,
        "epoch": 0.8504750103263115,
        "step": 6177
    },
    {
        "loss": 2.0629,
        "grad_norm": 1.6890021562576294,
        "learning_rate": 8.452376795781324e-05,
        "epoch": 0.8506126944788654,
        "step": 6178
    },
    {
        "loss": 2.5368,
        "grad_norm": 1.502102017402649,
        "learning_rate": 8.4395137487568e-05,
        "epoch": 0.8507503786314196,
        "step": 6179
    },
    {
        "loss": 0.4686,
        "grad_norm": 4.2705078125,
        "learning_rate": 8.426653347580835e-05,
        "epoch": 0.8508880627839736,
        "step": 6180
    },
    {
        "loss": 2.1388,
        "grad_norm": 1.6726932525634766,
        "learning_rate": 8.413795614058602e-05,
        "epoch": 0.8510257469365277,
        "step": 6181
    },
    {
        "loss": 1.5496,
        "grad_norm": 3.7072300910949707,
        "learning_rate": 8.400940569990747e-05,
        "epoch": 0.8511634310890817,
        "step": 6182
    },
    {
        "loss": 2.2322,
        "grad_norm": 2.0502870082855225,
        "learning_rate": 8.388088237173356e-05,
        "epoch": 0.8513011152416357,
        "step": 6183
    },
    {
        "loss": 2.5647,
        "grad_norm": 1.2481701374053955,
        "learning_rate": 8.375238637397942e-05,
        "epoch": 0.8514387993941898,
        "step": 6184
    },
    {
        "loss": 1.5455,
        "grad_norm": 2.517160654067993,
        "learning_rate": 8.362391792451356e-05,
        "epoch": 0.8515764835467438,
        "step": 6185
    },
    {
        "loss": 1.7201,
        "grad_norm": 2.5303425788879395,
        "learning_rate": 8.349547724115771e-05,
        "epoch": 0.8517141676992979,
        "step": 6186
    },
    {
        "loss": 1.9779,
        "grad_norm": 2.6098384857177734,
        "learning_rate": 8.336706454168695e-05,
        "epoch": 0.8518518518518519,
        "step": 6187
    },
    {
        "loss": 1.6747,
        "grad_norm": 1.4207637310028076,
        "learning_rate": 8.323868004382853e-05,
        "epoch": 0.8519895360044059,
        "step": 6188
    },
    {
        "loss": 2.1823,
        "grad_norm": 1.9463878870010376,
        "learning_rate": 8.311032396526199e-05,
        "epoch": 0.85212722015696,
        "step": 6189
    },
    {
        "loss": 1.2493,
        "grad_norm": 3.0419259071350098,
        "learning_rate": 8.298199652361867e-05,
        "epoch": 0.852264904309514,
        "step": 6190
    },
    {
        "loss": 1.3553,
        "grad_norm": 3.577559471130371,
        "learning_rate": 8.285369793648137e-05,
        "epoch": 0.8524025884620681,
        "step": 6191
    },
    {
        "loss": 2.1393,
        "grad_norm": 2.344351291656494,
        "learning_rate": 8.272542842138386e-05,
        "epoch": 0.8525402726146221,
        "step": 6192
    },
    {
        "loss": 1.4465,
        "grad_norm": 3.2640790939331055,
        "learning_rate": 8.259718819581101e-05,
        "epoch": 0.8526779567671761,
        "step": 6193
    },
    {
        "loss": 1.9236,
        "grad_norm": 2.510605573654175,
        "learning_rate": 8.246897747719756e-05,
        "epoch": 0.8528156409197302,
        "step": 6194
    },
    {
        "loss": 2.1899,
        "grad_norm": 1.476320743560791,
        "learning_rate": 8.234079648292833e-05,
        "epoch": 0.8529533250722842,
        "step": 6195
    },
    {
        "loss": 2.9302,
        "grad_norm": 1.617003321647644,
        "learning_rate": 8.221264543033807e-05,
        "epoch": 0.8530910092248383,
        "step": 6196
    },
    {
        "loss": 0.384,
        "grad_norm": 3.33894419670105,
        "learning_rate": 8.208452453671037e-05,
        "epoch": 0.8532286933773923,
        "step": 6197
    },
    {
        "loss": 1.0144,
        "grad_norm": 2.683777332305908,
        "learning_rate": 8.19564340192777e-05,
        "epoch": 0.8533663775299463,
        "step": 6198
    },
    {
        "loss": 2.1555,
        "grad_norm": 2.1423892974853516,
        "learning_rate": 8.182837409522141e-05,
        "epoch": 0.8535040616825004,
        "step": 6199
    },
    {
        "loss": 2.2523,
        "grad_norm": 2.5282185077667236,
        "learning_rate": 8.170034498167061e-05,
        "epoch": 0.8536417458350544,
        "step": 6200
    },
    {
        "loss": 2.0989,
        "grad_norm": 3.851916551589966,
        "learning_rate": 8.157234689570213e-05,
        "epoch": 0.8537794299876085,
        "step": 6201
    },
    {
        "loss": 1.8524,
        "grad_norm": 2.280789613723755,
        "learning_rate": 8.144438005434063e-05,
        "epoch": 0.8539171141401625,
        "step": 6202
    },
    {
        "loss": 2.0348,
        "grad_norm": 2.070803642272949,
        "learning_rate": 8.131644467455734e-05,
        "epoch": 0.8540547982927165,
        "step": 6203
    },
    {
        "loss": 1.3453,
        "grad_norm": 4.851687908172607,
        "learning_rate": 8.118854097327035e-05,
        "epoch": 0.8541924824452706,
        "step": 6204
    },
    {
        "loss": 2.1089,
        "grad_norm": 2.0172667503356934,
        "learning_rate": 8.1060669167344e-05,
        "epoch": 0.8543301665978246,
        "step": 6205
    },
    {
        "loss": 1.7181,
        "grad_norm": 1.6906332969665527,
        "learning_rate": 8.093282947358852e-05,
        "epoch": 0.8544678507503787,
        "step": 6206
    },
    {
        "loss": 1.7794,
        "grad_norm": 2.07711124420166,
        "learning_rate": 8.08050221087596e-05,
        "epoch": 0.8546055349029327,
        "step": 6207
    },
    {
        "loss": 1.7105,
        "grad_norm": 3.5743136405944824,
        "learning_rate": 8.067724728955854e-05,
        "epoch": 0.8547432190554867,
        "step": 6208
    },
    {
        "loss": 2.0914,
        "grad_norm": 1.3578109741210938,
        "learning_rate": 8.0549505232631e-05,
        "epoch": 0.8548809032080408,
        "step": 6209
    },
    {
        "loss": 2.0877,
        "grad_norm": 2.737964630126953,
        "learning_rate": 8.042179615456711e-05,
        "epoch": 0.8550185873605948,
        "step": 6210
    },
    {
        "loss": 1.6566,
        "grad_norm": 1.5538380146026611,
        "learning_rate": 8.02941202719015e-05,
        "epoch": 0.8551562715131489,
        "step": 6211
    },
    {
        "loss": 1.7697,
        "grad_norm": 1.6361263990402222,
        "learning_rate": 8.016647780111209e-05,
        "epoch": 0.8552939556657029,
        "step": 6212
    },
    {
        "loss": 1.8206,
        "grad_norm": 1.1309607028961182,
        "learning_rate": 8.003886895862035e-05,
        "epoch": 0.8554316398182569,
        "step": 6213
    },
    {
        "loss": 1.4781,
        "grad_norm": 1.8136757612228394,
        "learning_rate": 7.99112939607906e-05,
        "epoch": 0.855569323970811,
        "step": 6214
    },
    {
        "loss": 2.2238,
        "grad_norm": 2.820319414138794,
        "learning_rate": 7.978375302392993e-05,
        "epoch": 0.855707008123365,
        "step": 6215
    },
    {
        "loss": 2.1126,
        "grad_norm": 2.473470687866211,
        "learning_rate": 7.965624636428753e-05,
        "epoch": 0.8558446922759191,
        "step": 6216
    },
    {
        "loss": 1.7489,
        "grad_norm": 1.3207963705062866,
        "learning_rate": 7.952877419805449e-05,
        "epoch": 0.8559823764284731,
        "step": 6217
    },
    {
        "loss": 2.1991,
        "grad_norm": 1.6344484090805054,
        "learning_rate": 7.94013367413637e-05,
        "epoch": 0.8561200605810271,
        "step": 6218
    },
    {
        "loss": 2.0077,
        "grad_norm": 2.6791670322418213,
        "learning_rate": 7.927393421028881e-05,
        "epoch": 0.8562577447335812,
        "step": 6219
    },
    {
        "loss": 1.7129,
        "grad_norm": 2.84574294090271,
        "learning_rate": 7.914656682084435e-05,
        "epoch": 0.8563954288861352,
        "step": 6220
    },
    {
        "loss": 1.7295,
        "grad_norm": 1.2209782600402832,
        "learning_rate": 7.901923478898555e-05,
        "epoch": 0.8565331130386893,
        "step": 6221
    },
    {
        "loss": 1.8776,
        "grad_norm": 1.1954697370529175,
        "learning_rate": 7.889193833060732e-05,
        "epoch": 0.8566707971912433,
        "step": 6222
    },
    {
        "loss": 1.9562,
        "grad_norm": 1.7903599739074707,
        "learning_rate": 7.876467766154432e-05,
        "epoch": 0.8568084813437973,
        "step": 6223
    },
    {
        "loss": 2.1823,
        "grad_norm": 1.8988183736801147,
        "learning_rate": 7.863745299757084e-05,
        "epoch": 0.8569461654963514,
        "step": 6224
    },
    {
        "loss": 1.8763,
        "grad_norm": 3.161339044570923,
        "learning_rate": 7.851026455439975e-05,
        "epoch": 0.8570838496489054,
        "step": 6225
    },
    {
        "loss": 1.5242,
        "grad_norm": 3.5243706703186035,
        "learning_rate": 7.83831125476826e-05,
        "epoch": 0.8572215338014595,
        "step": 6226
    },
    {
        "loss": 2.0471,
        "grad_norm": 2.499523639678955,
        "learning_rate": 7.825599719300939e-05,
        "epoch": 0.8573592179540135,
        "step": 6227
    },
    {
        "loss": 1.5438,
        "grad_norm": 2.6569409370422363,
        "learning_rate": 7.812891870590769e-05,
        "epoch": 0.8574969021065675,
        "step": 6228
    },
    {
        "loss": 1.8959,
        "grad_norm": 2.6703643798828125,
        "learning_rate": 7.800187730184268e-05,
        "epoch": 0.8576345862591216,
        "step": 6229
    },
    {
        "loss": 2.2802,
        "grad_norm": 2.251950740814209,
        "learning_rate": 7.787487319621666e-05,
        "epoch": 0.8577722704116756,
        "step": 6230
    },
    {
        "loss": 2.2517,
        "grad_norm": 1.664616584777832,
        "learning_rate": 7.774790660436866e-05,
        "epoch": 0.8579099545642297,
        "step": 6231
    },
    {
        "loss": 1.9312,
        "grad_norm": 2.3030307292938232,
        "learning_rate": 7.762097774157399e-05,
        "epoch": 0.8580476387167837,
        "step": 6232
    },
    {
        "loss": 1.503,
        "grad_norm": 3.5086565017700195,
        "learning_rate": 7.749408682304441e-05,
        "epoch": 0.8581853228693377,
        "step": 6233
    },
    {
        "loss": 2.232,
        "grad_norm": 2.2768704891204834,
        "learning_rate": 7.736723406392692e-05,
        "epoch": 0.8583230070218918,
        "step": 6234
    },
    {
        "loss": 1.4942,
        "grad_norm": 2.887777805328369,
        "learning_rate": 7.724041967930383e-05,
        "epoch": 0.8584606911744458,
        "step": 6235
    },
    {
        "loss": 2.1461,
        "grad_norm": 1.9729262590408325,
        "learning_rate": 7.711364388419277e-05,
        "epoch": 0.8585983753269999,
        "step": 6236
    },
    {
        "loss": 1.8131,
        "grad_norm": 1.4290251731872559,
        "learning_rate": 7.698690689354557e-05,
        "epoch": 0.8587360594795539,
        "step": 6237
    },
    {
        "loss": 1.4418,
        "grad_norm": 1.9894654750823975,
        "learning_rate": 7.686020892224835e-05,
        "epoch": 0.858873743632108,
        "step": 6238
    },
    {
        "loss": 2.2948,
        "grad_norm": 1.684943437576294,
        "learning_rate": 7.673355018512113e-05,
        "epoch": 0.859011427784662,
        "step": 6239
    },
    {
        "loss": 2.4406,
        "grad_norm": 2.2416975498199463,
        "learning_rate": 7.660693089691738e-05,
        "epoch": 0.859149111937216,
        "step": 6240
    },
    {
        "loss": 2.2396,
        "grad_norm": 1.9772106409072876,
        "learning_rate": 7.648035127232351e-05,
        "epoch": 0.8592867960897701,
        "step": 6241
    },
    {
        "loss": 1.9679,
        "grad_norm": 2.6677091121673584,
        "learning_rate": 7.635381152595919e-05,
        "epoch": 0.8594244802423241,
        "step": 6242
    },
    {
        "loss": 1.5963,
        "grad_norm": 1.2280943393707275,
        "learning_rate": 7.622731187237595e-05,
        "epoch": 0.8595621643948782,
        "step": 6243
    },
    {
        "loss": 1.9732,
        "grad_norm": 3.046217918395996,
        "learning_rate": 7.61008525260574e-05,
        "epoch": 0.8596998485474322,
        "step": 6244
    },
    {
        "loss": 1.5706,
        "grad_norm": 2.506593704223633,
        "learning_rate": 7.597443370141923e-05,
        "epoch": 0.8598375326999862,
        "step": 6245
    },
    {
        "loss": 2.293,
        "grad_norm": 1.2571465969085693,
        "learning_rate": 7.584805561280799e-05,
        "epoch": 0.8599752168525403,
        "step": 6246
    },
    {
        "loss": 0.6531,
        "grad_norm": 1.9483753442764282,
        "learning_rate": 7.572171847450113e-05,
        "epoch": 0.8601129010050943,
        "step": 6247
    },
    {
        "loss": 1.9491,
        "grad_norm": 1.4368236064910889,
        "learning_rate": 7.559542250070716e-05,
        "epoch": 0.8602505851576484,
        "step": 6248
    },
    {
        "loss": 1.8348,
        "grad_norm": 1.5597281455993652,
        "learning_rate": 7.546916790556431e-05,
        "epoch": 0.8603882693102024,
        "step": 6249
    },
    {
        "loss": 1.8862,
        "grad_norm": 1.8088855743408203,
        "learning_rate": 7.534295490314069e-05,
        "epoch": 0.8605259534627564,
        "step": 6250
    },
    {
        "loss": 1.9052,
        "grad_norm": 3.098184823989868,
        "learning_rate": 7.521678370743428e-05,
        "epoch": 0.8606636376153105,
        "step": 6251
    },
    {
        "loss": 2.2367,
        "grad_norm": 2.1486830711364746,
        "learning_rate": 7.509065453237178e-05,
        "epoch": 0.8608013217678645,
        "step": 6252
    },
    {
        "loss": 1.5451,
        "grad_norm": 2.1869754791259766,
        "learning_rate": 7.496456759180875e-05,
        "epoch": 0.8609390059204186,
        "step": 6253
    },
    {
        "loss": 1.8957,
        "grad_norm": 1.7984347343444824,
        "learning_rate": 7.48385230995292e-05,
        "epoch": 0.8610766900729726,
        "step": 6254
    },
    {
        "loss": 2.326,
        "grad_norm": 1.159373164176941,
        "learning_rate": 7.471252126924513e-05,
        "epoch": 0.8612143742255266,
        "step": 6255
    },
    {
        "loss": 2.2021,
        "grad_norm": 2.4518237113952637,
        "learning_rate": 7.458656231459602e-05,
        "epoch": 0.8613520583780807,
        "step": 6256
    },
    {
        "loss": 1.6045,
        "grad_norm": 2.584235429763794,
        "learning_rate": 7.446064644914916e-05,
        "epoch": 0.8614897425306347,
        "step": 6257
    },
    {
        "loss": 2.1906,
        "grad_norm": 2.7142324447631836,
        "learning_rate": 7.433477388639831e-05,
        "epoch": 0.8616274266831888,
        "step": 6258
    },
    {
        "loss": 1.2645,
        "grad_norm": 5.455132961273193,
        "learning_rate": 7.420894483976382e-05,
        "epoch": 0.8617651108357428,
        "step": 6259
    },
    {
        "loss": 1.6909,
        "grad_norm": 3.2075135707855225,
        "learning_rate": 7.408315952259266e-05,
        "epoch": 0.8619027949882968,
        "step": 6260
    },
    {
        "loss": 1.6171,
        "grad_norm": 2.2082319259643555,
        "learning_rate": 7.395741814815726e-05,
        "epoch": 0.8620404791408509,
        "step": 6261
    },
    {
        "loss": 1.8474,
        "grad_norm": 2.953950881958008,
        "learning_rate": 7.383172092965569e-05,
        "epoch": 0.8621781632934049,
        "step": 6262
    },
    {
        "loss": 2.0125,
        "grad_norm": 2.1355700492858887,
        "learning_rate": 7.37060680802111e-05,
        "epoch": 0.862315847445959,
        "step": 6263
    },
    {
        "loss": 1.3372,
        "grad_norm": 1.905733346939087,
        "learning_rate": 7.35804598128715e-05,
        "epoch": 0.862453531598513,
        "step": 6264
    },
    {
        "loss": 2.1042,
        "grad_norm": 3.1389265060424805,
        "learning_rate": 7.345489634060911e-05,
        "epoch": 0.862591215751067,
        "step": 6265
    },
    {
        "loss": 2.1993,
        "grad_norm": 1.535979151725769,
        "learning_rate": 7.332937787632057e-05,
        "epoch": 0.8627288999036211,
        "step": 6266
    },
    {
        "loss": 1.8979,
        "grad_norm": 1.597724199295044,
        "learning_rate": 7.320390463282592e-05,
        "epoch": 0.8628665840561751,
        "step": 6267
    },
    {
        "loss": 1.6242,
        "grad_norm": 3.0137417316436768,
        "learning_rate": 7.307847682286837e-05,
        "epoch": 0.8630042682087292,
        "step": 6268
    },
    {
        "loss": 0.6189,
        "grad_norm": 2.1594953536987305,
        "learning_rate": 7.295309465911468e-05,
        "epoch": 0.8631419523612832,
        "step": 6269
    },
    {
        "loss": 2.1518,
        "grad_norm": 2.8307526111602783,
        "learning_rate": 7.28277583541537e-05,
        "epoch": 0.8632796365138372,
        "step": 6270
    },
    {
        "loss": 1.8028,
        "grad_norm": 2.032094955444336,
        "learning_rate": 7.270246812049648e-05,
        "epoch": 0.8634173206663913,
        "step": 6271
    },
    {
        "loss": 2.215,
        "grad_norm": 1.2844709157943726,
        "learning_rate": 7.257722417057649e-05,
        "epoch": 0.8635550048189453,
        "step": 6272
    },
    {
        "loss": 2.1358,
        "grad_norm": 3.1612377166748047,
        "learning_rate": 7.245202671674823e-05,
        "epoch": 0.8636926889714994,
        "step": 6273
    },
    {
        "loss": 2.2561,
        "grad_norm": 2.634411334991455,
        "learning_rate": 7.232687597128735e-05,
        "epoch": 0.8638303731240534,
        "step": 6274
    },
    {
        "loss": 2.3819,
        "grad_norm": 3.046082019805908,
        "learning_rate": 7.220177214639082e-05,
        "epoch": 0.8639680572766074,
        "step": 6275
    },
    {
        "loss": 1.9616,
        "grad_norm": 2.7798573970794678,
        "learning_rate": 7.207671545417554e-05,
        "epoch": 0.8641057414291615,
        "step": 6276
    },
    {
        "loss": 1.168,
        "grad_norm": 3.1242969036102295,
        "learning_rate": 7.195170610667866e-05,
        "epoch": 0.8642434255817155,
        "step": 6277
    },
    {
        "loss": 2.1651,
        "grad_norm": 2.700852632522583,
        "learning_rate": 7.182674431585705e-05,
        "epoch": 0.8643811097342696,
        "step": 6278
    },
    {
        "loss": 1.9056,
        "grad_norm": 2.3149335384368896,
        "learning_rate": 7.170183029358701e-05,
        "epoch": 0.8645187938868236,
        "step": 6279
    },
    {
        "loss": 2.1138,
        "grad_norm": 1.583585500717163,
        "learning_rate": 7.157696425166363e-05,
        "epoch": 0.8646564780393776,
        "step": 6280
    },
    {
        "loss": 1.4673,
        "grad_norm": 2.4975342750549316,
        "learning_rate": 7.145214640180106e-05,
        "epoch": 0.8647941621919317,
        "step": 6281
    },
    {
        "loss": 1.9892,
        "grad_norm": 3.156067371368408,
        "learning_rate": 7.132737695563143e-05,
        "epoch": 0.8649318463444857,
        "step": 6282
    },
    {
        "loss": 1.679,
        "grad_norm": 2.4947991371154785,
        "learning_rate": 7.120265612470466e-05,
        "epoch": 0.8650695304970398,
        "step": 6283
    },
    {
        "loss": 0.4053,
        "grad_norm": 1.3170430660247803,
        "learning_rate": 7.107798412048871e-05,
        "epoch": 0.8652072146495938,
        "step": 6284
    },
    {
        "loss": 2.2702,
        "grad_norm": 3.585681200027466,
        "learning_rate": 7.095336115436841e-05,
        "epoch": 0.8653448988021478,
        "step": 6285
    },
    {
        "loss": 1.3268,
        "grad_norm": 1.6413118839263916,
        "learning_rate": 7.082878743764549e-05,
        "epoch": 0.8654825829547019,
        "step": 6286
    },
    {
        "loss": 2.0468,
        "grad_norm": 1.8968745470046997,
        "learning_rate": 7.070426318153823e-05,
        "epoch": 0.8656202671072559,
        "step": 6287
    },
    {
        "loss": 1.5832,
        "grad_norm": 3.037271738052368,
        "learning_rate": 7.057978859718097e-05,
        "epoch": 0.86575795125981,
        "step": 6288
    },
    {
        "loss": 1.5215,
        "grad_norm": 1.5654445886611938,
        "learning_rate": 7.045536389562381e-05,
        "epoch": 0.865895635412364,
        "step": 6289
    },
    {
        "loss": 2.181,
        "grad_norm": 2.6388099193573,
        "learning_rate": 7.033098928783262e-05,
        "epoch": 0.866033319564918,
        "step": 6290
    },
    {
        "loss": 2.1222,
        "grad_norm": 1.3663839101791382,
        "learning_rate": 7.020666498468786e-05,
        "epoch": 0.8661710037174721,
        "step": 6291
    },
    {
        "loss": 1.944,
        "grad_norm": 1.1995705366134644,
        "learning_rate": 7.008239119698488e-05,
        "epoch": 0.8663086878700261,
        "step": 6292
    },
    {
        "loss": 1.834,
        "grad_norm": 2.335895538330078,
        "learning_rate": 6.995816813543356e-05,
        "epoch": 0.8664463720225802,
        "step": 6293
    },
    {
        "loss": 2.1007,
        "grad_norm": 1.7929638624191284,
        "learning_rate": 6.983399601065752e-05,
        "epoch": 0.8665840561751342,
        "step": 6294
    },
    {
        "loss": 2.02,
        "grad_norm": 1.3407341241836548,
        "learning_rate": 6.970987503319401e-05,
        "epoch": 0.8667217403276882,
        "step": 6295
    },
    {
        "loss": 1.9092,
        "grad_norm": 2.2869369983673096,
        "learning_rate": 6.958580541349385e-05,
        "epoch": 0.8668594244802423,
        "step": 6296
    },
    {
        "loss": 2.1694,
        "grad_norm": 1.4663951396942139,
        "learning_rate": 6.946178736192053e-05,
        "epoch": 0.8669971086327963,
        "step": 6297
    },
    {
        "loss": 2.0085,
        "grad_norm": 2.716639757156372,
        "learning_rate": 6.933782108875014e-05,
        "epoch": 0.8671347927853504,
        "step": 6298
    },
    {
        "loss": 2.5501,
        "grad_norm": 1.7281818389892578,
        "learning_rate": 6.921390680417082e-05,
        "epoch": 0.8672724769379044,
        "step": 6299
    },
    {
        "loss": 1.8208,
        "grad_norm": 2.2275643348693848,
        "learning_rate": 6.909004471828306e-05,
        "epoch": 0.8674101610904585,
        "step": 6300
    },
    {
        "loss": 2.4289,
        "grad_norm": 1.6475086212158203,
        "learning_rate": 6.896623504109838e-05,
        "epoch": 0.8675478452430125,
        "step": 6301
    },
    {
        "loss": 2.1464,
        "grad_norm": 1.8741439580917358,
        "learning_rate": 6.88424779825396e-05,
        "epoch": 0.8676855293955665,
        "step": 6302
    },
    {
        "loss": 1.8724,
        "grad_norm": 2.741027355194092,
        "learning_rate": 6.87187737524403e-05,
        "epoch": 0.8678232135481206,
        "step": 6303
    },
    {
        "loss": 2.0446,
        "grad_norm": 2.515698194503784,
        "learning_rate": 6.85951225605445e-05,
        "epoch": 0.8679608977006746,
        "step": 6304
    },
    {
        "loss": 2.2142,
        "grad_norm": 1.3760261535644531,
        "learning_rate": 6.847152461650615e-05,
        "epoch": 0.8680985818532287,
        "step": 6305
    },
    {
        "loss": 1.3426,
        "grad_norm": 2.912888288497925,
        "learning_rate": 6.834798012988932e-05,
        "epoch": 0.8682362660057827,
        "step": 6306
    },
    {
        "loss": 2.2953,
        "grad_norm": 1.5138682126998901,
        "learning_rate": 6.82244893101671e-05,
        "epoch": 0.8683739501583367,
        "step": 6307
    },
    {
        "loss": 1.9561,
        "grad_norm": 3.187016725540161,
        "learning_rate": 6.810105236672149e-05,
        "epoch": 0.8685116343108908,
        "step": 6308
    },
    {
        "loss": 1.9456,
        "grad_norm": 2.4640755653381348,
        "learning_rate": 6.797766950884355e-05,
        "epoch": 0.8686493184634448,
        "step": 6309
    },
    {
        "loss": 2.5141,
        "grad_norm": 1.872883915901184,
        "learning_rate": 6.785434094573233e-05,
        "epoch": 0.868787002615999,
        "step": 6310
    },
    {
        "loss": 2.0753,
        "grad_norm": 3.1083180904388428,
        "learning_rate": 6.773106688649487e-05,
        "epoch": 0.868924686768553,
        "step": 6311
    },
    {
        "loss": 1.6949,
        "grad_norm": 2.75339937210083,
        "learning_rate": 6.760784754014584e-05,
        "epoch": 0.8690623709211069,
        "step": 6312
    },
    {
        "loss": 2.3157,
        "grad_norm": 1.6030055284500122,
        "learning_rate": 6.74846831156071e-05,
        "epoch": 0.869200055073661,
        "step": 6313
    },
    {
        "loss": 2.3056,
        "grad_norm": 2.4943573474884033,
        "learning_rate": 6.736157382170731e-05,
        "epoch": 0.869337739226215,
        "step": 6314
    },
    {
        "loss": 1.7787,
        "grad_norm": 2.3791415691375732,
        "learning_rate": 6.723851986718199e-05,
        "epoch": 0.8694754233787692,
        "step": 6315
    },
    {
        "loss": 2.6944,
        "grad_norm": 1.7873976230621338,
        "learning_rate": 6.711552146067248e-05,
        "epoch": 0.8696131075313231,
        "step": 6316
    },
    {
        "loss": 2.4307,
        "grad_norm": 1.347680687904358,
        "learning_rate": 6.699257881072594e-05,
        "epoch": 0.8697507916838771,
        "step": 6317
    },
    {
        "loss": 2.3193,
        "grad_norm": 2.4038705825805664,
        "learning_rate": 6.686969212579537e-05,
        "epoch": 0.8698884758364313,
        "step": 6318
    },
    {
        "loss": 2.6979,
        "grad_norm": 1.7640485763549805,
        "learning_rate": 6.674686161423844e-05,
        "epoch": 0.8700261599889852,
        "step": 6319
    },
    {
        "loss": 2.0486,
        "grad_norm": 2.1420042514801025,
        "learning_rate": 6.66240874843177e-05,
        "epoch": 0.8701638441415394,
        "step": 6320
    },
    {
        "loss": 1.1117,
        "grad_norm": 1.0591799020767212,
        "learning_rate": 6.650136994420036e-05,
        "epoch": 0.8703015282940934,
        "step": 6321
    },
    {
        "loss": 1.6011,
        "grad_norm": 2.313011407852173,
        "learning_rate": 6.637870920195737e-05,
        "epoch": 0.8704392124466473,
        "step": 6322
    },
    {
        "loss": 1.8515,
        "grad_norm": 2.1365749835968018,
        "learning_rate": 6.625610546556335e-05,
        "epoch": 0.8705768965992015,
        "step": 6323
    },
    {
        "loss": 2.1812,
        "grad_norm": 1.9371696710586548,
        "learning_rate": 6.613355894289667e-05,
        "epoch": 0.8707145807517555,
        "step": 6324
    },
    {
        "loss": 1.1587,
        "grad_norm": 2.720529079437256,
        "learning_rate": 6.601106984173831e-05,
        "epoch": 0.8708522649043096,
        "step": 6325
    },
    {
        "loss": 2.3798,
        "grad_norm": 2.6694557666778564,
        "learning_rate": 6.588863836977198e-05,
        "epoch": 0.8709899490568636,
        "step": 6326
    },
    {
        "loss": 1.9836,
        "grad_norm": 1.3092762231826782,
        "learning_rate": 6.57662647345837e-05,
        "epoch": 0.8711276332094176,
        "step": 6327
    },
    {
        "loss": 1.282,
        "grad_norm": 1.4953128099441528,
        "learning_rate": 6.56439491436614e-05,
        "epoch": 0.8712653173619717,
        "step": 6328
    },
    {
        "loss": 2.5506,
        "grad_norm": 1.941314458847046,
        "learning_rate": 6.552169180439458e-05,
        "epoch": 0.8714030015145257,
        "step": 6329
    },
    {
        "loss": 1.9798,
        "grad_norm": 1.8531118631362915,
        "learning_rate": 6.539949292407421e-05,
        "epoch": 0.8715406856670798,
        "step": 6330
    },
    {
        "loss": 2.1301,
        "grad_norm": 1.72721266746521,
        "learning_rate": 6.527735270989182e-05,
        "epoch": 0.8716783698196338,
        "step": 6331
    },
    {
        "loss": 1.8149,
        "grad_norm": 2.387911319732666,
        "learning_rate": 6.515527136893948e-05,
        "epoch": 0.8718160539721878,
        "step": 6332
    },
    {
        "loss": 1.6767,
        "grad_norm": 3.2635319232940674,
        "learning_rate": 6.503324910820982e-05,
        "epoch": 0.8719537381247419,
        "step": 6333
    },
    {
        "loss": 2.103,
        "grad_norm": 2.3018181324005127,
        "learning_rate": 6.491128613459488e-05,
        "epoch": 0.8720914222772959,
        "step": 6334
    },
    {
        "loss": 1.4363,
        "grad_norm": 2.131643533706665,
        "learning_rate": 6.478938265488634e-05,
        "epoch": 0.87222910642985,
        "step": 6335
    },
    {
        "loss": 2.5248,
        "grad_norm": 0.9389228224754333,
        "learning_rate": 6.466753887577503e-05,
        "epoch": 0.872366790582404,
        "step": 6336
    },
    {
        "loss": 1.6317,
        "grad_norm": 2.913499593734741,
        "learning_rate": 6.454575500385047e-05,
        "epoch": 0.872504474734958,
        "step": 6337
    },
    {
        "loss": 2.0827,
        "grad_norm": 1.782894253730774,
        "learning_rate": 6.442403124560054e-05,
        "epoch": 0.8726421588875121,
        "step": 6338
    },
    {
        "loss": 1.9066,
        "grad_norm": 3.702788829803467,
        "learning_rate": 6.430236780741161e-05,
        "epoch": 0.8727798430400661,
        "step": 6339
    },
    {
        "loss": 1.9687,
        "grad_norm": 1.978732943534851,
        "learning_rate": 6.418076489556731e-05,
        "epoch": 0.8729175271926202,
        "step": 6340
    },
    {
        "loss": 1.0707,
        "grad_norm": 2.197051525115967,
        "learning_rate": 6.405922271624869e-05,
        "epoch": 0.8730552113451742,
        "step": 6341
    },
    {
        "loss": 0.7161,
        "grad_norm": 1.6916314363479614,
        "learning_rate": 6.393774147553425e-05,
        "epoch": 0.8731928954977282,
        "step": 6342
    },
    {
        "loss": 2.1911,
        "grad_norm": 1.8978281021118164,
        "learning_rate": 6.381632137939868e-05,
        "epoch": 0.8733305796502823,
        "step": 6343
    },
    {
        "loss": 1.3882,
        "grad_norm": 2.3679697513580322,
        "learning_rate": 6.369496263371309e-05,
        "epoch": 0.8734682638028363,
        "step": 6344
    },
    {
        "loss": 2.1877,
        "grad_norm": 1.8105366230010986,
        "learning_rate": 6.357366544424491e-05,
        "epoch": 0.8736059479553904,
        "step": 6345
    },
    {
        "loss": 2.3786,
        "grad_norm": 2.071514368057251,
        "learning_rate": 6.345243001665683e-05,
        "epoch": 0.8737436321079444,
        "step": 6346
    },
    {
        "loss": 1.5325,
        "grad_norm": 2.119328260421753,
        "learning_rate": 6.333125655650684e-05,
        "epoch": 0.8738813162604984,
        "step": 6347
    },
    {
        "loss": 2.51,
        "grad_norm": 1.2487139701843262,
        "learning_rate": 6.32101452692482e-05,
        "epoch": 0.8740190004130525,
        "step": 6348
    },
    {
        "loss": 2.7462,
        "grad_norm": 1.900813102722168,
        "learning_rate": 6.308909636022838e-05,
        "epoch": 0.8741566845656065,
        "step": 6349
    },
    {
        "loss": 1.5157,
        "grad_norm": 3.2295610904693604,
        "learning_rate": 6.296811003468924e-05,
        "epoch": 0.8742943687181606,
        "step": 6350
    },
    {
        "loss": 1.3176,
        "grad_norm": 2.803663492202759,
        "learning_rate": 6.284718649776655e-05,
        "epoch": 0.8744320528707146,
        "step": 6351
    },
    {
        "loss": 1.8881,
        "grad_norm": 2.671708583831787,
        "learning_rate": 6.272632595448952e-05,
        "epoch": 0.8745697370232686,
        "step": 6352
    },
    {
        "loss": 1.3494,
        "grad_norm": 2.5860445499420166,
        "learning_rate": 6.260552860978056e-05,
        "epoch": 0.8747074211758227,
        "step": 6353
    },
    {
        "loss": 1.0947,
        "grad_norm": 3.2348780632019043,
        "learning_rate": 6.248479466845518e-05,
        "epoch": 0.8748451053283767,
        "step": 6354
    },
    {
        "loss": 2.5126,
        "grad_norm": 1.5615620613098145,
        "learning_rate": 6.23641243352211e-05,
        "epoch": 0.8749827894809308,
        "step": 6355
    },
    {
        "loss": 1.6535,
        "grad_norm": 5.149742603302002,
        "learning_rate": 6.224351781467813e-05,
        "epoch": 0.8751204736334848,
        "step": 6356
    },
    {
        "loss": 2.5755,
        "grad_norm": 1.17693030834198,
        "learning_rate": 6.212297531131832e-05,
        "epoch": 0.8752581577860389,
        "step": 6357
    },
    {
        "loss": 1.7212,
        "grad_norm": 1.4760947227478027,
        "learning_rate": 6.200249702952474e-05,
        "epoch": 0.8753958419385929,
        "step": 6358
    },
    {
        "loss": 2.5122,
        "grad_norm": 1.5528783798217773,
        "learning_rate": 6.188208317357171e-05,
        "epoch": 0.8755335260911469,
        "step": 6359
    },
    {
        "loss": 1.8387,
        "grad_norm": 2.494187355041504,
        "learning_rate": 6.176173394762436e-05,
        "epoch": 0.875671210243701,
        "step": 6360
    },
    {
        "loss": 1.1298,
        "grad_norm": 2.5529446601867676,
        "learning_rate": 6.164144955573822e-05,
        "epoch": 0.875808894396255,
        "step": 6361
    },
    {
        "loss": 2.2176,
        "grad_norm": 1.8998404741287231,
        "learning_rate": 6.15212302018587e-05,
        "epoch": 0.8759465785488091,
        "step": 6362
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.7341129779815674,
        "learning_rate": 6.140107608982139e-05,
        "epoch": 0.8760842627013631,
        "step": 6363
    },
    {
        "loss": 2.1683,
        "grad_norm": 1.8492790460586548,
        "learning_rate": 6.128098742335087e-05,
        "epoch": 0.8762219468539171,
        "step": 6364
    },
    {
        "loss": 2.0951,
        "grad_norm": 2.642421007156372,
        "learning_rate": 6.11609644060607e-05,
        "epoch": 0.8763596310064712,
        "step": 6365
    },
    {
        "loss": 2.0695,
        "grad_norm": 2.2987143993377686,
        "learning_rate": 6.104100724145361e-05,
        "epoch": 0.8764973151590252,
        "step": 6366
    },
    {
        "loss": 2.3224,
        "grad_norm": 1.4616084098815918,
        "learning_rate": 6.09211161329202e-05,
        "epoch": 0.8766349993115793,
        "step": 6367
    },
    {
        "loss": 2.8002,
        "grad_norm": 2.292466878890991,
        "learning_rate": 6.08012912837391e-05,
        "epoch": 0.8767726834641333,
        "step": 6368
    },
    {
        "loss": 2.1266,
        "grad_norm": 3.144204616546631,
        "learning_rate": 6.068153289707703e-05,
        "epoch": 0.8769103676166873,
        "step": 6369
    },
    {
        "loss": 2.1538,
        "grad_norm": 1.3160392045974731,
        "learning_rate": 6.056184117598755e-05,
        "epoch": 0.8770480517692414,
        "step": 6370
    },
    {
        "loss": 2.2229,
        "grad_norm": 1.791448712348938,
        "learning_rate": 6.0442216323411226e-05,
        "epoch": 0.8771857359217954,
        "step": 6371
    },
    {
        "loss": 1.884,
        "grad_norm": 2.2204766273498535,
        "learning_rate": 6.032265854217568e-05,
        "epoch": 0.8773234200743495,
        "step": 6372
    },
    {
        "loss": 2.2343,
        "grad_norm": 2.0621066093444824,
        "learning_rate": 6.020316803499435e-05,
        "epoch": 0.8774611042269035,
        "step": 6373
    },
    {
        "loss": 1.5326,
        "grad_norm": 2.105931520462036,
        "learning_rate": 6.008374500446676e-05,
        "epoch": 0.8775987883794575,
        "step": 6374
    },
    {
        "loss": 1.8009,
        "grad_norm": 1.3007704019546509,
        "learning_rate": 5.996438965307804e-05,
        "epoch": 0.8777364725320116,
        "step": 6375
    },
    {
        "loss": 2.1199,
        "grad_norm": 1.2218101024627686,
        "learning_rate": 5.984510218319859e-05,
        "epoch": 0.8778741566845656,
        "step": 6376
    },
    {
        "loss": 2.0414,
        "grad_norm": 1.7834317684173584,
        "learning_rate": 5.972588279708353e-05,
        "epoch": 0.8780118408371197,
        "step": 6377
    },
    {
        "loss": 2.0956,
        "grad_norm": 1.7507692575454712,
        "learning_rate": 5.960673169687295e-05,
        "epoch": 0.8781495249896737,
        "step": 6378
    },
    {
        "loss": 1.9272,
        "grad_norm": 1.2684980630874634,
        "learning_rate": 5.948764908459081e-05,
        "epoch": 0.8782872091422277,
        "step": 6379
    },
    {
        "loss": 2.0248,
        "grad_norm": 1.5540499687194824,
        "learning_rate": 5.93686351621449e-05,
        "epoch": 0.8784248932947818,
        "step": 6380
    },
    {
        "loss": 1.2981,
        "grad_norm": 1.6753400564193726,
        "learning_rate": 5.924969013132695e-05,
        "epoch": 0.8785625774473358,
        "step": 6381
    },
    {
        "loss": 2.2724,
        "grad_norm": 1.864648461341858,
        "learning_rate": 5.913081419381151e-05,
        "epoch": 0.8787002615998899,
        "step": 6382
    },
    {
        "loss": 1.7762,
        "grad_norm": 2.4607279300689697,
        "learning_rate": 5.901200755115608e-05,
        "epoch": 0.8788379457524439,
        "step": 6383
    },
    {
        "loss": 1.3725,
        "grad_norm": 1.115620732307434,
        "learning_rate": 5.8893270404800636e-05,
        "epoch": 0.8789756299049979,
        "step": 6384
    },
    {
        "loss": 1.6012,
        "grad_norm": 2.3744051456451416,
        "learning_rate": 5.877460295606745e-05,
        "epoch": 0.879113314057552,
        "step": 6385
    },
    {
        "loss": 2.1608,
        "grad_norm": 2.82620906829834,
        "learning_rate": 5.86560054061605e-05,
        "epoch": 0.879250998210106,
        "step": 6386
    },
    {
        "loss": 2.5381,
        "grad_norm": 1.245882511138916,
        "learning_rate": 5.853747795616512e-05,
        "epoch": 0.8793886823626601,
        "step": 6387
    },
    {
        "loss": 2.4231,
        "grad_norm": 1.6403896808624268,
        "learning_rate": 5.841902080704826e-05,
        "epoch": 0.8795263665152141,
        "step": 6388
    },
    {
        "loss": 1.9039,
        "grad_norm": 2.916985273361206,
        "learning_rate": 5.830063415965721e-05,
        "epoch": 0.8796640506677681,
        "step": 6389
    },
    {
        "loss": 1.6577,
        "grad_norm": 2.4253528118133545,
        "learning_rate": 5.818231821471978e-05,
        "epoch": 0.8798017348203222,
        "step": 6390
    },
    {
        "loss": 1.1689,
        "grad_norm": 3.465672492980957,
        "learning_rate": 5.8064073172844233e-05,
        "epoch": 0.8799394189728762,
        "step": 6391
    },
    {
        "loss": 1.8333,
        "grad_norm": 2.5752594470977783,
        "learning_rate": 5.794589923451825e-05,
        "epoch": 0.8800771031254303,
        "step": 6392
    },
    {
        "loss": 1.8578,
        "grad_norm": 2.9092817306518555,
        "learning_rate": 5.782779660010898e-05,
        "epoch": 0.8802147872779843,
        "step": 6393
    },
    {
        "loss": 1.6662,
        "grad_norm": 1.5204819440841675,
        "learning_rate": 5.770976546986298e-05,
        "epoch": 0.8803524714305383,
        "step": 6394
    },
    {
        "loss": 1.2768,
        "grad_norm": 2.175297737121582,
        "learning_rate": 5.759180604390526e-05,
        "epoch": 0.8804901555830924,
        "step": 6395
    },
    {
        "loss": 2.1284,
        "grad_norm": 1.5822844505310059,
        "learning_rate": 5.747391852223929e-05,
        "epoch": 0.8806278397356464,
        "step": 6396
    },
    {
        "loss": 2.0905,
        "grad_norm": 2.4782180786132812,
        "learning_rate": 5.735610310474689e-05,
        "epoch": 0.8807655238882005,
        "step": 6397
    },
    {
        "loss": 1.2292,
        "grad_norm": 2.157925605773926,
        "learning_rate": 5.723835999118734e-05,
        "epoch": 0.8809032080407545,
        "step": 6398
    },
    {
        "loss": 1.7929,
        "grad_norm": 2.6706395149230957,
        "learning_rate": 5.712068938119741e-05,
        "epoch": 0.8810408921933085,
        "step": 6399
    },
    {
        "loss": 2.2181,
        "grad_norm": 1.8050636053085327,
        "learning_rate": 5.700309147429093e-05,
        "epoch": 0.8811785763458626,
        "step": 6400
    },
    {
        "loss": 1.7548,
        "grad_norm": 2.3764288425445557,
        "learning_rate": 5.6885566469858476e-05,
        "epoch": 0.8813162604984166,
        "step": 6401
    },
    {
        "loss": 1.9044,
        "grad_norm": 2.422305107116699,
        "learning_rate": 5.676811456716687e-05,
        "epoch": 0.8814539446509707,
        "step": 6402
    },
    {
        "loss": 2.0006,
        "grad_norm": 1.6387708187103271,
        "learning_rate": 5.665073596535946e-05,
        "epoch": 0.8815916288035247,
        "step": 6403
    },
    {
        "loss": 2.5849,
        "grad_norm": 1.9586652517318726,
        "learning_rate": 5.6533430863454794e-05,
        "epoch": 0.8817293129560787,
        "step": 6404
    },
    {
        "loss": 1.4942,
        "grad_norm": 2.1649422645568848,
        "learning_rate": 5.641619946034691e-05,
        "epoch": 0.8818669971086328,
        "step": 6405
    },
    {
        "loss": 2.4871,
        "grad_norm": 1.944278597831726,
        "learning_rate": 5.629904195480517e-05,
        "epoch": 0.8820046812611868,
        "step": 6406
    },
    {
        "loss": 1.8998,
        "grad_norm": 2.8529984951019287,
        "learning_rate": 5.6181958545473325e-05,
        "epoch": 0.8821423654137409,
        "step": 6407
    },
    {
        "loss": 2.1493,
        "grad_norm": 1.9340177774429321,
        "learning_rate": 5.606494943086957e-05,
        "epoch": 0.8822800495662949,
        "step": 6408
    },
    {
        "loss": 1.6944,
        "grad_norm": 2.348788022994995,
        "learning_rate": 5.594801480938625e-05,
        "epoch": 0.8824177337188489,
        "step": 6409
    },
    {
        "loss": 1.8422,
        "grad_norm": 1.566481113433838,
        "learning_rate": 5.583115487928929e-05,
        "epoch": 0.882555417871403,
        "step": 6410
    },
    {
        "loss": 1.6316,
        "grad_norm": 2.2689874172210693,
        "learning_rate": 5.5714369838717874e-05,
        "epoch": 0.882693102023957,
        "step": 6411
    },
    {
        "loss": 1.9858,
        "grad_norm": 1.7910746335983276,
        "learning_rate": 5.559765988568457e-05,
        "epoch": 0.8828307861765111,
        "step": 6412
    },
    {
        "loss": 2.3378,
        "grad_norm": 1.9064617156982422,
        "learning_rate": 5.54810252180743e-05,
        "epoch": 0.8829684703290651,
        "step": 6413
    },
    {
        "loss": 2.1348,
        "grad_norm": 2.087634801864624,
        "learning_rate": 5.536446603364428e-05,
        "epoch": 0.8831061544816192,
        "step": 6414
    },
    {
        "loss": 1.7458,
        "grad_norm": 2.024219512939453,
        "learning_rate": 5.5247982530024213e-05,
        "epoch": 0.8832438386341732,
        "step": 6415
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.2830537557601929,
        "learning_rate": 5.513157490471501e-05,
        "epoch": 0.8833815227867272,
        "step": 6416
    },
    {
        "loss": 2.3613,
        "grad_norm": 3.2335429191589355,
        "learning_rate": 5.5015243355088984e-05,
        "epoch": 0.8835192069392813,
        "step": 6417
    },
    {
        "loss": 1.3285,
        "grad_norm": 2.4841647148132324,
        "learning_rate": 5.48989880783898e-05,
        "epoch": 0.8836568910918353,
        "step": 6418
    },
    {
        "loss": 1.6937,
        "grad_norm": 2.197035312652588,
        "learning_rate": 5.478280927173145e-05,
        "epoch": 0.8837945752443894,
        "step": 6419
    },
    {
        "loss": 2.1693,
        "grad_norm": 2.5205047130584717,
        "learning_rate": 5.4666707132098226e-05,
        "epoch": 0.8839322593969434,
        "step": 6420
    },
    {
        "loss": 2.3165,
        "grad_norm": 1.6102076768875122,
        "learning_rate": 5.4550681856344896e-05,
        "epoch": 0.8840699435494974,
        "step": 6421
    },
    {
        "loss": 1.4306,
        "grad_norm": 3.0320851802825928,
        "learning_rate": 5.443473364119544e-05,
        "epoch": 0.8842076277020515,
        "step": 6422
    },
    {
        "loss": 1.7065,
        "grad_norm": 1.7701698541641235,
        "learning_rate": 5.4318862683243354e-05,
        "epoch": 0.8843453118546055,
        "step": 6423
    },
    {
        "loss": 2.0682,
        "grad_norm": 1.72017502784729,
        "learning_rate": 5.4203069178951104e-05,
        "epoch": 0.8844829960071596,
        "step": 6424
    },
    {
        "loss": 2.1514,
        "grad_norm": 1.8896888494491577,
        "learning_rate": 5.408735332464985e-05,
        "epoch": 0.8846206801597136,
        "step": 6425
    },
    {
        "loss": 1.9276,
        "grad_norm": 1.8467413187026978,
        "learning_rate": 5.397171531653893e-05,
        "epoch": 0.8847583643122676,
        "step": 6426
    },
    {
        "loss": 1.8056,
        "grad_norm": 1.3809083700180054,
        "learning_rate": 5.38561553506862e-05,
        "epoch": 0.8848960484648217,
        "step": 6427
    },
    {
        "loss": 2.4073,
        "grad_norm": 2.444572925567627,
        "learning_rate": 5.374067362302666e-05,
        "epoch": 0.8850337326173757,
        "step": 6428
    },
    {
        "loss": 1.9957,
        "grad_norm": 2.0571436882019043,
        "learning_rate": 5.3625270329362745e-05,
        "epoch": 0.8851714167699298,
        "step": 6429
    },
    {
        "loss": 1.5894,
        "grad_norm": 2.4790215492248535,
        "learning_rate": 5.350994566536424e-05,
        "epoch": 0.8853091009224838,
        "step": 6430
    },
    {
        "loss": 2.0592,
        "grad_norm": 1.9636515378952026,
        "learning_rate": 5.339469982656724e-05,
        "epoch": 0.8854467850750378,
        "step": 6431
    },
    {
        "loss": 2.1775,
        "grad_norm": 2.6076841354370117,
        "learning_rate": 5.327953300837434e-05,
        "epoch": 0.8855844692275919,
        "step": 6432
    },
    {
        "loss": 1.1624,
        "grad_norm": 3.159792184829712,
        "learning_rate": 5.3164445406054075e-05,
        "epoch": 0.8857221533801459,
        "step": 6433
    },
    {
        "loss": 1.8958,
        "grad_norm": 2.725465774536133,
        "learning_rate": 5.3049437214740825e-05,
        "epoch": 0.8858598375327,
        "step": 6434
    },
    {
        "loss": 2.0959,
        "grad_norm": 3.1668665409088135,
        "learning_rate": 5.2934508629434074e-05,
        "epoch": 0.885997521685254,
        "step": 6435
    },
    {
        "loss": 2.7428,
        "grad_norm": 1.2619630098342896,
        "learning_rate": 5.2819659844998726e-05,
        "epoch": 0.886135205837808,
        "step": 6436
    },
    {
        "loss": 1.6815,
        "grad_norm": 2.1191563606262207,
        "learning_rate": 5.270489105616401e-05,
        "epoch": 0.8862728899903621,
        "step": 6437
    },
    {
        "loss": 1.9245,
        "grad_norm": 2.5792219638824463,
        "learning_rate": 5.259020245752349e-05,
        "epoch": 0.8864105741429161,
        "step": 6438
    },
    {
        "loss": 1.988,
        "grad_norm": 1.5457477569580078,
        "learning_rate": 5.247559424353527e-05,
        "epoch": 0.8865482582954702,
        "step": 6439
    },
    {
        "loss": 2.3495,
        "grad_norm": 2.000102996826172,
        "learning_rate": 5.236106660852063e-05,
        "epoch": 0.8866859424480242,
        "step": 6440
    },
    {
        "loss": 1.7747,
        "grad_norm": 2.197497606277466,
        "learning_rate": 5.2246619746664314e-05,
        "epoch": 0.8868236266005782,
        "step": 6441
    },
    {
        "loss": 2.0002,
        "grad_norm": 1.372003197669983,
        "learning_rate": 5.2132253852014436e-05,
        "epoch": 0.8869613107531323,
        "step": 6442
    },
    {
        "loss": 1.6614,
        "grad_norm": 1.7452162504196167,
        "learning_rate": 5.20179691184815e-05,
        "epoch": 0.8870989949056863,
        "step": 6443
    },
    {
        "loss": 1.9727,
        "grad_norm": 3.0732016563415527,
        "learning_rate": 5.190376573983834e-05,
        "epoch": 0.8872366790582404,
        "step": 6444
    },
    {
        "loss": 1.358,
        "grad_norm": 1.8793457746505737,
        "learning_rate": 5.178964390972032e-05,
        "epoch": 0.8873743632107944,
        "step": 6445
    },
    {
        "loss": 1.7454,
        "grad_norm": 3.2583091259002686,
        "learning_rate": 5.167560382162407e-05,
        "epoch": 0.8875120473633484,
        "step": 6446
    },
    {
        "loss": 2.311,
        "grad_norm": 2.163956642150879,
        "learning_rate": 5.15616456689078e-05,
        "epoch": 0.8876497315159025,
        "step": 6447
    },
    {
        "loss": 1.7629,
        "grad_norm": 1.5131357908248901,
        "learning_rate": 5.144776964479078e-05,
        "epoch": 0.8877874156684565,
        "step": 6448
    },
    {
        "loss": 2.0175,
        "grad_norm": 1.619493007659912,
        "learning_rate": 5.1333975942353006e-05,
        "epoch": 0.8879250998210106,
        "step": 6449
    },
    {
        "loss": 0.6798,
        "grad_norm": 2.2806613445281982,
        "learning_rate": 5.122026475453482e-05,
        "epoch": 0.8880627839735646,
        "step": 6450
    },
    {
        "loss": 1.4414,
        "grad_norm": 1.4165481328964233,
        "learning_rate": 5.110663627413694e-05,
        "epoch": 0.8882004681261186,
        "step": 6451
    },
    {
        "loss": 2.3862,
        "grad_norm": 2.287571430206299,
        "learning_rate": 5.099309069381963e-05,
        "epoch": 0.8883381522786727,
        "step": 6452
    },
    {
        "loss": 0.8791,
        "grad_norm": 2.1555490493774414,
        "learning_rate": 5.087962820610249e-05,
        "epoch": 0.8884758364312267,
        "step": 6453
    },
    {
        "loss": 2.1178,
        "grad_norm": 2.6185216903686523,
        "learning_rate": 5.0766249003364594e-05,
        "epoch": 0.8886135205837808,
        "step": 6454
    },
    {
        "loss": 0.9672,
        "grad_norm": 2.0049045085906982,
        "learning_rate": 5.065295327784347e-05,
        "epoch": 0.8887512047363348,
        "step": 6455
    },
    {
        "loss": 1.0452,
        "grad_norm": 2.9938549995422363,
        "learning_rate": 5.053974122163523e-05,
        "epoch": 0.8888888888888888,
        "step": 6456
    },
    {
        "loss": 1.5714,
        "grad_norm": 2.5704753398895264,
        "learning_rate": 5.042661302669411e-05,
        "epoch": 0.889026573041443,
        "step": 6457
    },
    {
        "loss": 1.9265,
        "grad_norm": 2.7571263313293457,
        "learning_rate": 5.031356888483214e-05,
        "epoch": 0.8891642571939969,
        "step": 6458
    },
    {
        "loss": 1.7874,
        "grad_norm": 3.827422857284546,
        "learning_rate": 5.020060898771879e-05,
        "epoch": 0.889301941346551,
        "step": 6459
    },
    {
        "loss": 1.1304,
        "grad_norm": 2.8470804691314697,
        "learning_rate": 5.0087733526880983e-05,
        "epoch": 0.889439625499105,
        "step": 6460
    },
    {
        "loss": 1.3963,
        "grad_norm": 2.4617671966552734,
        "learning_rate": 4.997494269370212e-05,
        "epoch": 0.889577309651659,
        "step": 6461
    },
    {
        "loss": 1.562,
        "grad_norm": 1.3099466562271118,
        "learning_rate": 4.986223667942214e-05,
        "epoch": 0.8897149938042132,
        "step": 6462
    },
    {
        "loss": 2.2423,
        "grad_norm": 1.9197715520858765,
        "learning_rate": 4.9749615675137494e-05,
        "epoch": 0.8898526779567671,
        "step": 6463
    },
    {
        "loss": 1.2279,
        "grad_norm": 2.3082683086395264,
        "learning_rate": 4.963707987180014e-05,
        "epoch": 0.8899903621093213,
        "step": 6464
    },
    {
        "loss": 2.1278,
        "grad_norm": 2.4354019165039062,
        "learning_rate": 4.9524629460217675e-05,
        "epoch": 0.8901280462618752,
        "step": 6465
    },
    {
        "loss": 1.8152,
        "grad_norm": 2.9672563076019287,
        "learning_rate": 4.941226463105314e-05,
        "epoch": 0.8902657304144292,
        "step": 6466
    },
    {
        "loss": 1.6221,
        "grad_norm": 1.329329252243042,
        "learning_rate": 4.929998557482416e-05,
        "epoch": 0.8904034145669834,
        "step": 6467
    },
    {
        "loss": 2.3207,
        "grad_norm": 1.1856344938278198,
        "learning_rate": 4.918779248190306e-05,
        "epoch": 0.8905410987195373,
        "step": 6468
    },
    {
        "loss": 1.7874,
        "grad_norm": 1.705775499343872,
        "learning_rate": 4.9075685542516235e-05,
        "epoch": 0.8906787828720915,
        "step": 6469
    },
    {
        "loss": 1.5058,
        "grad_norm": 2.645554780960083,
        "learning_rate": 4.896366494674448e-05,
        "epoch": 0.8908164670246455,
        "step": 6470
    },
    {
        "loss": 1.8902,
        "grad_norm": 2.880216360092163,
        "learning_rate": 4.885173088452174e-05,
        "epoch": 0.8909541511771994,
        "step": 6471
    },
    {
        "loss": 2.191,
        "grad_norm": 2.1953814029693604,
        "learning_rate": 4.8739883545635366e-05,
        "epoch": 0.8910918353297536,
        "step": 6472
    },
    {
        "loss": 2.0697,
        "grad_norm": 2.23991060256958,
        "learning_rate": 4.86281231197257e-05,
        "epoch": 0.8912295194823076,
        "step": 6473
    },
    {
        "loss": 2.2315,
        "grad_norm": 1.658349633216858,
        "learning_rate": 4.851644979628569e-05,
        "epoch": 0.8913672036348617,
        "step": 6474
    },
    {
        "loss": 2.487,
        "grad_norm": 2.095160722732544,
        "learning_rate": 4.8404863764660504e-05,
        "epoch": 0.8915048877874157,
        "step": 6475
    },
    {
        "loss": 2.5046,
        "grad_norm": 1.329813838005066,
        "learning_rate": 4.82933652140476e-05,
        "epoch": 0.8916425719399698,
        "step": 6476
    },
    {
        "loss": 1.2631,
        "grad_norm": 1.9915555715560913,
        "learning_rate": 4.8181954333495894e-05,
        "epoch": 0.8917802560925238,
        "step": 6477
    },
    {
        "loss": 1.4244,
        "grad_norm": 3.367727041244507,
        "learning_rate": 4.8070631311905536e-05,
        "epoch": 0.8919179402450778,
        "step": 6478
    },
    {
        "loss": 2.4787,
        "grad_norm": 2.3146440982818604,
        "learning_rate": 4.7959396338028063e-05,
        "epoch": 0.8920556243976319,
        "step": 6479
    },
    {
        "loss": 2.0307,
        "grad_norm": 2.0598015785217285,
        "learning_rate": 4.784824960046541e-05,
        "epoch": 0.8921933085501859,
        "step": 6480
    },
    {
        "loss": 0.8961,
        "grad_norm": 3.5931503772735596,
        "learning_rate": 4.7737191287670024e-05,
        "epoch": 0.89233099270274,
        "step": 6481
    },
    {
        "loss": 2.0426,
        "grad_norm": 1.3046246767044067,
        "learning_rate": 4.762622158794442e-05,
        "epoch": 0.892468676855294,
        "step": 6482
    },
    {
        "loss": 2.0193,
        "grad_norm": 3.2370870113372803,
        "learning_rate": 4.75153406894408e-05,
        "epoch": 0.892606361007848,
        "step": 6483
    },
    {
        "loss": 2.2191,
        "grad_norm": 1.4613072872161865,
        "learning_rate": 4.740454878016084e-05,
        "epoch": 0.8927440451604021,
        "step": 6484
    },
    {
        "loss": 2.0538,
        "grad_norm": 2.03462290763855,
        "learning_rate": 4.7293846047955546e-05,
        "epoch": 0.8928817293129561,
        "step": 6485
    },
    {
        "loss": 1.6845,
        "grad_norm": 2.0993354320526123,
        "learning_rate": 4.71832326805244e-05,
        "epoch": 0.8930194134655102,
        "step": 6486
    },
    {
        "loss": 1.7651,
        "grad_norm": 2.182150363922119,
        "learning_rate": 4.707270886541541e-05,
        "epoch": 0.8931570976180642,
        "step": 6487
    },
    {
        "loss": 2.2765,
        "grad_norm": 1.280168056488037,
        "learning_rate": 4.6962274790025026e-05,
        "epoch": 0.8932947817706182,
        "step": 6488
    },
    {
        "loss": 1.7036,
        "grad_norm": 3.148822546005249,
        "learning_rate": 4.685193064159721e-05,
        "epoch": 0.8934324659231723,
        "step": 6489
    },
    {
        "loss": 2.4294,
        "grad_norm": 1.4413145780563354,
        "learning_rate": 4.674167660722354e-05,
        "epoch": 0.8935701500757263,
        "step": 6490
    },
    {
        "loss": 1.7851,
        "grad_norm": 2.715280771255493,
        "learning_rate": 4.663151287384304e-05,
        "epoch": 0.8937078342282804,
        "step": 6491
    },
    {
        "loss": 2.7717,
        "grad_norm": 2.5866358280181885,
        "learning_rate": 4.6521439628241326e-05,
        "epoch": 0.8938455183808344,
        "step": 6492
    },
    {
        "loss": 1.1528,
        "grad_norm": 1.9781200885772705,
        "learning_rate": 4.641145705705058e-05,
        "epoch": 0.8939832025333884,
        "step": 6493
    },
    {
        "loss": 1.3174,
        "grad_norm": 2.3775901794433594,
        "learning_rate": 4.6301565346749554e-05,
        "epoch": 0.8941208866859425,
        "step": 6494
    },
    {
        "loss": 2.1918,
        "grad_norm": 1.8278167247772217,
        "learning_rate": 4.619176468366271e-05,
        "epoch": 0.8942585708384965,
        "step": 6495
    },
    {
        "loss": 0.5987,
        "grad_norm": 5.401411056518555,
        "learning_rate": 4.608205525396012e-05,
        "epoch": 0.8943962549910506,
        "step": 6496
    },
    {
        "loss": 1.53,
        "grad_norm": 1.3495135307312012,
        "learning_rate": 4.597243724365726e-05,
        "epoch": 0.8945339391436046,
        "step": 6497
    },
    {
        "loss": 1.9696,
        "grad_norm": 1.928419828414917,
        "learning_rate": 4.5862910838614536e-05,
        "epoch": 0.8946716232961586,
        "step": 6498
    },
    {
        "loss": 2.2408,
        "grad_norm": 2.130866289138794,
        "learning_rate": 4.575347622453697e-05,
        "epoch": 0.8948093074487127,
        "step": 6499
    },
    {
        "loss": 1.9291,
        "grad_norm": 2.1396021842956543,
        "learning_rate": 4.564413358697426e-05,
        "epoch": 0.8949469916012667,
        "step": 6500
    },
    {
        "loss": 1.6354,
        "grad_norm": 3.2595629692077637,
        "learning_rate": 4.5534883111319784e-05,
        "epoch": 0.8950846757538208,
        "step": 6501
    },
    {
        "loss": 2.3795,
        "grad_norm": 2.5762157440185547,
        "learning_rate": 4.542572498281079e-05,
        "epoch": 0.8952223599063748,
        "step": 6502
    },
    {
        "loss": 2.4118,
        "grad_norm": 1.7331117391586304,
        "learning_rate": 4.531665938652815e-05,
        "epoch": 0.8953600440589288,
        "step": 6503
    },
    {
        "loss": 2.1061,
        "grad_norm": 2.5497138500213623,
        "learning_rate": 4.520768650739555e-05,
        "epoch": 0.8954977282114829,
        "step": 6504
    },
    {
        "loss": 0.8841,
        "grad_norm": 3.048600435256958,
        "learning_rate": 4.509880653017959e-05,
        "epoch": 0.8956354123640369,
        "step": 6505
    },
    {
        "loss": 1.9931,
        "grad_norm": 2.1719117164611816,
        "learning_rate": 4.4990019639489357e-05,
        "epoch": 0.895773096516591,
        "step": 6506
    },
    {
        "loss": 2.3404,
        "grad_norm": 1.986533761024475,
        "learning_rate": 4.4881326019776096e-05,
        "epoch": 0.895910780669145,
        "step": 6507
    },
    {
        "loss": 1.8182,
        "grad_norm": 2.679147720336914,
        "learning_rate": 4.477272585533275e-05,
        "epoch": 0.896048464821699,
        "step": 6508
    },
    {
        "loss": 2.4538,
        "grad_norm": 2.3809967041015625,
        "learning_rate": 4.466421933029429e-05,
        "epoch": 0.8961861489742531,
        "step": 6509
    },
    {
        "loss": 1.7307,
        "grad_norm": 2.6102712154388428,
        "learning_rate": 4.455580662863643e-05,
        "epoch": 0.8963238331268071,
        "step": 6510
    },
    {
        "loss": 1.6051,
        "grad_norm": 1.2659878730773926,
        "learning_rate": 4.444748793417587e-05,
        "epoch": 0.8964615172793612,
        "step": 6511
    },
    {
        "loss": 2.1736,
        "grad_norm": 1.5322519540786743,
        "learning_rate": 4.4339263430570235e-05,
        "epoch": 0.8965992014319152,
        "step": 6512
    },
    {
        "loss": 1.8067,
        "grad_norm": 1.5792758464813232,
        "learning_rate": 4.4231133301317105e-05,
        "epoch": 0.8967368855844692,
        "step": 6513
    },
    {
        "loss": 2.3801,
        "grad_norm": 2.3986010551452637,
        "learning_rate": 4.412309772975406e-05,
        "epoch": 0.8968745697370233,
        "step": 6514
    },
    {
        "loss": 2.5709,
        "grad_norm": 1.9597498178482056,
        "learning_rate": 4.401515689905873e-05,
        "epoch": 0.8970122538895773,
        "step": 6515
    },
    {
        "loss": 2.9612,
        "grad_norm": 1.364406704902649,
        "learning_rate": 4.3907310992247706e-05,
        "epoch": 0.8971499380421314,
        "step": 6516
    },
    {
        "loss": 2.042,
        "grad_norm": 2.3469889163970947,
        "learning_rate": 4.379956019217667e-05,
        "epoch": 0.8972876221946854,
        "step": 6517
    },
    {
        "loss": 2.2494,
        "grad_norm": 1.435778260231018,
        "learning_rate": 4.369190468154036e-05,
        "epoch": 0.8974253063472394,
        "step": 6518
    },
    {
        "loss": 2.1083,
        "grad_norm": 2.73858904838562,
        "learning_rate": 4.358434464287164e-05,
        "epoch": 0.8975629904997935,
        "step": 6519
    },
    {
        "loss": 1.7427,
        "grad_norm": 3.3127057552337646,
        "learning_rate": 4.347688025854155e-05,
        "epoch": 0.8977006746523475,
        "step": 6520
    },
    {
        "loss": 1.757,
        "grad_norm": 2.140197515487671,
        "learning_rate": 4.336951171075907e-05,
        "epoch": 0.8978383588049016,
        "step": 6521
    },
    {
        "loss": 2.4728,
        "grad_norm": 2.0703842639923096,
        "learning_rate": 4.3262239181570566e-05,
        "epoch": 0.8979760429574556,
        "step": 6522
    },
    {
        "loss": 1.9111,
        "grad_norm": 3.841081142425537,
        "learning_rate": 4.315506285285953e-05,
        "epoch": 0.8981137271100096,
        "step": 6523
    },
    {
        "loss": 2.5518,
        "grad_norm": 2.4993104934692383,
        "learning_rate": 4.304798290634668e-05,
        "epoch": 0.8982514112625637,
        "step": 6524
    },
    {
        "loss": 2.3733,
        "grad_norm": 1.590158224105835,
        "learning_rate": 4.294099952358901e-05,
        "epoch": 0.8983890954151177,
        "step": 6525
    },
    {
        "loss": 1.5812,
        "grad_norm": 1.913277506828308,
        "learning_rate": 4.283411288597968e-05,
        "epoch": 0.8985267795676718,
        "step": 6526
    },
    {
        "loss": 1.2005,
        "grad_norm": 1.6796475648880005,
        "learning_rate": 4.2727323174748335e-05,
        "epoch": 0.8986644637202258,
        "step": 6527
    },
    {
        "loss": 2.1912,
        "grad_norm": 1.7649513483047485,
        "learning_rate": 4.2620630570959775e-05,
        "epoch": 0.8988021478727798,
        "step": 6528
    },
    {
        "loss": 1.8463,
        "grad_norm": 1.699002981185913,
        "learning_rate": 4.251403525551435e-05,
        "epoch": 0.8989398320253339,
        "step": 6529
    },
    {
        "loss": 1.9493,
        "grad_norm": 2.326849937438965,
        "learning_rate": 4.240753740914741e-05,
        "epoch": 0.8990775161778879,
        "step": 6530
    },
    {
        "loss": 2.0988,
        "grad_norm": 1.8318259716033936,
        "learning_rate": 4.230113721242909e-05,
        "epoch": 0.899215200330442,
        "step": 6531
    },
    {
        "loss": 1.7768,
        "grad_norm": 2.235323905944824,
        "learning_rate": 4.219483484576375e-05,
        "epoch": 0.899352884482996,
        "step": 6532
    },
    {
        "loss": 2.2254,
        "grad_norm": 1.712645411491394,
        "learning_rate": 4.208863048939039e-05,
        "epoch": 0.8994905686355501,
        "step": 6533
    },
    {
        "loss": 2.2339,
        "grad_norm": 2.2842812538146973,
        "learning_rate": 4.198252432338132e-05,
        "epoch": 0.8996282527881041,
        "step": 6534
    },
    {
        "loss": 1.1529,
        "grad_norm": 2.280639886856079,
        "learning_rate": 4.187651652764246e-05,
        "epoch": 0.8997659369406581,
        "step": 6535
    },
    {
        "loss": 1.396,
        "grad_norm": 2.2814745903015137,
        "learning_rate": 4.177060728191322e-05,
        "epoch": 0.8999036210932122,
        "step": 6536
    },
    {
        "loss": 2.3137,
        "grad_norm": 2.586977958679199,
        "learning_rate": 4.1664796765765635e-05,
        "epoch": 0.9000413052457662,
        "step": 6537
    },
    {
        "loss": 1.6431,
        "grad_norm": 3.16066575050354,
        "learning_rate": 4.155908515860426e-05,
        "epoch": 0.9001789893983203,
        "step": 6538
    },
    {
        "loss": 1.3842,
        "grad_norm": 3.331876754760742,
        "learning_rate": 4.145347263966642e-05,
        "epoch": 0.9003166735508743,
        "step": 6539
    },
    {
        "loss": 1.9377,
        "grad_norm": 2.0233662128448486,
        "learning_rate": 4.1347959388020974e-05,
        "epoch": 0.9004543577034283,
        "step": 6540
    },
    {
        "loss": 1.267,
        "grad_norm": 5.9870924949646,
        "learning_rate": 4.124254558256851e-05,
        "epoch": 0.9005920418559824,
        "step": 6541
    },
    {
        "loss": 2.0339,
        "grad_norm": 3.2154417037963867,
        "learning_rate": 4.1137231402041344e-05,
        "epoch": 0.9007297260085364,
        "step": 6542
    },
    {
        "loss": 1.163,
        "grad_norm": 1.7631137371063232,
        "learning_rate": 4.103201702500253e-05,
        "epoch": 0.9008674101610905,
        "step": 6543
    },
    {
        "loss": 2.0442,
        "grad_norm": 2.0150086879730225,
        "learning_rate": 4.092690262984603e-05,
        "epoch": 0.9010050943136445,
        "step": 6544
    },
    {
        "loss": 1.0425,
        "grad_norm": 3.242116928100586,
        "learning_rate": 4.082188839479621e-05,
        "epoch": 0.9011427784661985,
        "step": 6545
    },
    {
        "loss": 1.6608,
        "grad_norm": 1.3313713073730469,
        "learning_rate": 4.071697449790779e-05,
        "epoch": 0.9012804626187526,
        "step": 6546
    },
    {
        "loss": 1.9636,
        "grad_norm": 2.2516720294952393,
        "learning_rate": 4.061216111706506e-05,
        "epoch": 0.9014181467713066,
        "step": 6547
    },
    {
        "loss": 1.0807,
        "grad_norm": 2.9054195880889893,
        "learning_rate": 4.0507448429982306e-05,
        "epoch": 0.9015558309238607,
        "step": 6548
    },
    {
        "loss": 2.4222,
        "grad_norm": 2.029526472091675,
        "learning_rate": 4.040283661420276e-05,
        "epoch": 0.9016935150764147,
        "step": 6549
    },
    {
        "loss": 1.6772,
        "grad_norm": 1.9637094736099243,
        "learning_rate": 4.029832584709867e-05,
        "epoch": 0.9018311992289687,
        "step": 6550
    },
    {
        "loss": 1.9276,
        "grad_norm": 2.3273510932922363,
        "learning_rate": 4.0193916305870894e-05,
        "epoch": 0.9019688833815228,
        "step": 6551
    },
    {
        "loss": 1.8905,
        "grad_norm": 2.5318191051483154,
        "learning_rate": 4.008960816754902e-05,
        "epoch": 0.9021065675340768,
        "step": 6552
    },
    {
        "loss": 1.8566,
        "grad_norm": 3.2091662883758545,
        "learning_rate": 3.9985401608990324e-05,
        "epoch": 0.9022442516866309,
        "step": 6553
    },
    {
        "loss": 2.0593,
        "grad_norm": 2.9298031330108643,
        "learning_rate": 3.9881296806879956e-05,
        "epoch": 0.9023819358391849,
        "step": 6554
    },
    {
        "loss": 2.0152,
        "grad_norm": 1.534247636795044,
        "learning_rate": 3.9777293937730556e-05,
        "epoch": 0.9025196199917389,
        "step": 6555
    },
    {
        "loss": 1.0662,
        "grad_norm": 2.4629464149475098,
        "learning_rate": 3.967339317788193e-05,
        "epoch": 0.902657304144293,
        "step": 6556
    },
    {
        "loss": 1.9176,
        "grad_norm": 2.275042772293091,
        "learning_rate": 3.9569594703500624e-05,
        "epoch": 0.902794988296847,
        "step": 6557
    },
    {
        "loss": 2.5651,
        "grad_norm": 1.6959965229034424,
        "learning_rate": 3.946589869058014e-05,
        "epoch": 0.9029326724494011,
        "step": 6558
    },
    {
        "loss": 2.0779,
        "grad_norm": 1.8764947652816772,
        "learning_rate": 3.936230531493988e-05,
        "epoch": 0.9030703566019551,
        "step": 6559
    },
    {
        "loss": 1.348,
        "grad_norm": 2.1670572757720947,
        "learning_rate": 3.9258814752225236e-05,
        "epoch": 0.9032080407545091,
        "step": 6560
    },
    {
        "loss": 2.141,
        "grad_norm": 1.6984504461288452,
        "learning_rate": 3.915542717790759e-05,
        "epoch": 0.9033457249070632,
        "step": 6561
    },
    {
        "loss": 2.1066,
        "grad_norm": 2.069552183151245,
        "learning_rate": 3.905214276728339e-05,
        "epoch": 0.9034834090596172,
        "step": 6562
    },
    {
        "loss": 1.1717,
        "grad_norm": 3.588963031768799,
        "learning_rate": 3.894896169547414e-05,
        "epoch": 0.9036210932121713,
        "step": 6563
    },
    {
        "loss": 1.3121,
        "grad_norm": 2.9976935386657715,
        "learning_rate": 3.884588413742654e-05,
        "epoch": 0.9037587773647253,
        "step": 6564
    },
    {
        "loss": 1.7932,
        "grad_norm": 3.5774877071380615,
        "learning_rate": 3.874291026791135e-05,
        "epoch": 0.9038964615172793,
        "step": 6565
    },
    {
        "loss": 1.3329,
        "grad_norm": 2.3496696949005127,
        "learning_rate": 3.864004026152359e-05,
        "epoch": 0.9040341456698334,
        "step": 6566
    },
    {
        "loss": 1.8574,
        "grad_norm": 1.821577548980713,
        "learning_rate": 3.853727429268248e-05,
        "epoch": 0.9041718298223874,
        "step": 6567
    },
    {
        "loss": 1.6981,
        "grad_norm": 2.6445138454437256,
        "learning_rate": 3.84346125356305e-05,
        "epoch": 0.9043095139749415,
        "step": 6568
    },
    {
        "loss": 2.2078,
        "grad_norm": 1.5459946393966675,
        "learning_rate": 3.833205516443358e-05,
        "epoch": 0.9044471981274955,
        "step": 6569
    },
    {
        "loss": 1.3607,
        "grad_norm": 3.4900121688842773,
        "learning_rate": 3.82296023529806e-05,
        "epoch": 0.9045848822800495,
        "step": 6570
    },
    {
        "loss": 2.3231,
        "grad_norm": 1.413567066192627,
        "learning_rate": 3.812725427498326e-05,
        "epoch": 0.9047225664326036,
        "step": 6571
    },
    {
        "loss": 2.2024,
        "grad_norm": 2.180528163909912,
        "learning_rate": 3.8025011103975496e-05,
        "epoch": 0.9048602505851576,
        "step": 6572
    },
    {
        "loss": 2.3579,
        "grad_norm": 2.4666354656219482,
        "learning_rate": 3.79228730133137e-05,
        "epoch": 0.9049979347377117,
        "step": 6573
    },
    {
        "loss": 2.3828,
        "grad_norm": 2.3857762813568115,
        "learning_rate": 3.782084017617578e-05,
        "epoch": 0.9051356188902657,
        "step": 6574
    },
    {
        "loss": 2.3819,
        "grad_norm": 2.527026891708374,
        "learning_rate": 3.771891276556119e-05,
        "epoch": 0.9052733030428197,
        "step": 6575
    },
    {
        "loss": 1.9968,
        "grad_norm": 2.0879414081573486,
        "learning_rate": 3.761709095429092e-05,
        "epoch": 0.9054109871953738,
        "step": 6576
    },
    {
        "loss": 1.3285,
        "grad_norm": 2.13315486907959,
        "learning_rate": 3.7515374915006696e-05,
        "epoch": 0.9055486713479278,
        "step": 6577
    },
    {
        "loss": 1.8855,
        "grad_norm": 1.3672281503677368,
        "learning_rate": 3.7413764820170896e-05,
        "epoch": 0.9056863555004819,
        "step": 6578
    },
    {
        "loss": 1.9165,
        "grad_norm": 2.4751434326171875,
        "learning_rate": 3.7312260842066314e-05,
        "epoch": 0.9058240396530359,
        "step": 6579
    },
    {
        "loss": 1.7074,
        "grad_norm": 2.1665658950805664,
        "learning_rate": 3.7210863152795804e-05,
        "epoch": 0.9059617238055899,
        "step": 6580
    },
    {
        "loss": 0.6836,
        "grad_norm": 3.1787030696868896,
        "learning_rate": 3.710957192428194e-05,
        "epoch": 0.906099407958144,
        "step": 6581
    },
    {
        "loss": 2.586,
        "grad_norm": 2.405203104019165,
        "learning_rate": 3.700838732826704e-05,
        "epoch": 0.906237092110698,
        "step": 6582
    },
    {
        "loss": 1.6522,
        "grad_norm": 2.3921163082122803,
        "learning_rate": 3.690730953631231e-05,
        "epoch": 0.9063747762632521,
        "step": 6583
    },
    {
        "loss": 0.9494,
        "grad_norm": 3.8790781497955322,
        "learning_rate": 3.6806338719797984e-05,
        "epoch": 0.9065124604158061,
        "step": 6584
    },
    {
        "loss": 1.8352,
        "grad_norm": 2.344597101211548,
        "learning_rate": 3.670547504992311e-05,
        "epoch": 0.9066501445683601,
        "step": 6585
    },
    {
        "loss": 2.2419,
        "grad_norm": 1.626267433166504,
        "learning_rate": 3.6604718697704774e-05,
        "epoch": 0.9067878287209142,
        "step": 6586
    },
    {
        "loss": 1.9637,
        "grad_norm": 1.428887128829956,
        "learning_rate": 3.650406983397813e-05,
        "epoch": 0.9069255128734682,
        "step": 6587
    },
    {
        "loss": 1.6661,
        "grad_norm": 2.2931721210479736,
        "learning_rate": 3.6403528629396344e-05,
        "epoch": 0.9070631970260223,
        "step": 6588
    },
    {
        "loss": 2.2442,
        "grad_norm": 1.4306666851043701,
        "learning_rate": 3.630309525442982e-05,
        "epoch": 0.9072008811785763,
        "step": 6589
    },
    {
        "loss": 1.43,
        "grad_norm": 3.3041419982910156,
        "learning_rate": 3.620276987936607e-05,
        "epoch": 0.9073385653311304,
        "step": 6590
    },
    {
        "loss": 2.0249,
        "grad_norm": 2.177210807800293,
        "learning_rate": 3.610255267430979e-05,
        "epoch": 0.9074762494836844,
        "step": 6591
    },
    {
        "loss": 1.3269,
        "grad_norm": 2.5249388217926025,
        "learning_rate": 3.6002443809181996e-05,
        "epoch": 0.9076139336362384,
        "step": 6592
    },
    {
        "loss": 1.7031,
        "grad_norm": 1.3989191055297852,
        "learning_rate": 3.590244345372009e-05,
        "epoch": 0.9077516177887925,
        "step": 6593
    },
    {
        "loss": 1.022,
        "grad_norm": 3.8275749683380127,
        "learning_rate": 3.580255177747751e-05,
        "epoch": 0.9078893019413465,
        "step": 6594
    },
    {
        "loss": 1.445,
        "grad_norm": 2.7478208541870117,
        "learning_rate": 3.5702768949823376e-05,
        "epoch": 0.9080269860939006,
        "step": 6595
    },
    {
        "loss": 1.935,
        "grad_norm": 1.2998825311660767,
        "learning_rate": 3.5603095139942255e-05,
        "epoch": 0.9081646702464546,
        "step": 6596
    },
    {
        "loss": 0.9684,
        "grad_norm": 4.415024280548096,
        "learning_rate": 3.550353051683409e-05,
        "epoch": 0.9083023543990086,
        "step": 6597
    },
    {
        "loss": 1.9017,
        "grad_norm": 2.3008131980895996,
        "learning_rate": 3.5404075249313415e-05,
        "epoch": 0.9084400385515627,
        "step": 6598
    },
    {
        "loss": 1.2084,
        "grad_norm": 4.942272186279297,
        "learning_rate": 3.530472950600935e-05,
        "epoch": 0.9085777227041167,
        "step": 6599
    },
    {
        "loss": 0.4272,
        "grad_norm": 1.965262532234192,
        "learning_rate": 3.520549345536559e-05,
        "epoch": 0.9087154068566708,
        "step": 6600
    },
    {
        "loss": 1.9395,
        "grad_norm": 2.001607656478882,
        "learning_rate": 3.510636726563954e-05,
        "epoch": 0.9088530910092248,
        "step": 6601
    },
    {
        "loss": 1.792,
        "grad_norm": 2.37078595161438,
        "learning_rate": 3.500735110490254e-05,
        "epoch": 0.9089907751617788,
        "step": 6602
    },
    {
        "loss": 0.7699,
        "grad_norm": 3.788963794708252,
        "learning_rate": 3.490844514103925e-05,
        "epoch": 0.909128459314333,
        "step": 6603
    },
    {
        "loss": 2.3563,
        "grad_norm": 1.3874986171722412,
        "learning_rate": 3.4809649541747515e-05,
        "epoch": 0.909266143466887,
        "step": 6604
    },
    {
        "loss": 2.1559,
        "grad_norm": 2.02490496635437,
        "learning_rate": 3.4710964474537966e-05,
        "epoch": 0.909403827619441,
        "step": 6605
    },
    {
        "loss": 1.4837,
        "grad_norm": 1.4463380575180054,
        "learning_rate": 3.461239010673415e-05,
        "epoch": 0.909541511771995,
        "step": 6606
    },
    {
        "loss": 2.4348,
        "grad_norm": 1.5422955751419067,
        "learning_rate": 3.451392660547157e-05,
        "epoch": 0.909679195924549,
        "step": 6607
    },
    {
        "loss": 2.3146,
        "grad_norm": 3.082198143005371,
        "learning_rate": 3.441557413769775e-05,
        "epoch": 0.9098168800771032,
        "step": 6608
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.5533115863800049,
        "learning_rate": 3.431733287017235e-05,
        "epoch": 0.9099545642296571,
        "step": 6609
    },
    {
        "loss": 2.4149,
        "grad_norm": 3.454730272293091,
        "learning_rate": 3.421920296946608e-05,
        "epoch": 0.9100922483822113,
        "step": 6610
    },
    {
        "loss": 2.446,
        "grad_norm": 1.491511344909668,
        "learning_rate": 3.412118460196087e-05,
        "epoch": 0.9102299325347653,
        "step": 6611
    },
    {
        "loss": 2.0247,
        "grad_norm": 3.036510705947876,
        "learning_rate": 3.4023277933849806e-05,
        "epoch": 0.9103676166873192,
        "step": 6612
    },
    {
        "loss": 1.9927,
        "grad_norm": 1.8951997756958008,
        "learning_rate": 3.392548313113631e-05,
        "epoch": 0.9105053008398734,
        "step": 6613
    },
    {
        "loss": 1.9747,
        "grad_norm": 2.195528745651245,
        "learning_rate": 3.3827800359634185e-05,
        "epoch": 0.9106429849924274,
        "step": 6614
    },
    {
        "loss": 1.6038,
        "grad_norm": 2.0622997283935547,
        "learning_rate": 3.373022978496751e-05,
        "epoch": 0.9107806691449815,
        "step": 6615
    },
    {
        "loss": 0.9558,
        "grad_norm": 4.918468475341797,
        "learning_rate": 3.3632771572569846e-05,
        "epoch": 0.9109183532975355,
        "step": 6616
    },
    {
        "loss": 1.8363,
        "grad_norm": 2.6459310054779053,
        "learning_rate": 3.353542588768435e-05,
        "epoch": 0.9110560374500895,
        "step": 6617
    },
    {
        "loss": 1.9091,
        "grad_norm": 2.9198153018951416,
        "learning_rate": 3.3438192895363366e-05,
        "epoch": 0.9111937216026436,
        "step": 6618
    },
    {
        "loss": 2.2229,
        "grad_norm": 3.788299560546875,
        "learning_rate": 3.334107276046821e-05,
        "epoch": 0.9113314057551976,
        "step": 6619
    },
    {
        "loss": 2.3211,
        "grad_norm": 1.6369049549102783,
        "learning_rate": 3.324406564766865e-05,
        "epoch": 0.9114690899077517,
        "step": 6620
    },
    {
        "loss": 1.6313,
        "grad_norm": 2.8079380989074707,
        "learning_rate": 3.314717172144329e-05,
        "epoch": 0.9116067740603057,
        "step": 6621
    },
    {
        "loss": 2.1848,
        "grad_norm": 2.363614559173584,
        "learning_rate": 3.305039114607836e-05,
        "epoch": 0.9117444582128597,
        "step": 6622
    },
    {
        "loss": 2.1624,
        "grad_norm": 1.716919183731079,
        "learning_rate": 3.295372408566799e-05,
        "epoch": 0.9118821423654138,
        "step": 6623
    },
    {
        "loss": 2.1162,
        "grad_norm": 2.5396132469177246,
        "learning_rate": 3.2857170704114094e-05,
        "epoch": 0.9120198265179678,
        "step": 6624
    },
    {
        "loss": 1.7578,
        "grad_norm": 2.351900339126587,
        "learning_rate": 3.276073116512556e-05,
        "epoch": 0.9121575106705219,
        "step": 6625
    },
    {
        "loss": 2.3373,
        "grad_norm": 2.283482789993286,
        "learning_rate": 3.266440563221832e-05,
        "epoch": 0.9122951948230759,
        "step": 6626
    },
    {
        "loss": 2.2149,
        "grad_norm": 1.4798532724380493,
        "learning_rate": 3.2568194268715105e-05,
        "epoch": 0.9124328789756299,
        "step": 6627
    },
    {
        "loss": 1.6185,
        "grad_norm": 3.0998973846435547,
        "learning_rate": 3.247209723774498e-05,
        "epoch": 0.912570563128184,
        "step": 6628
    },
    {
        "loss": 2.1761,
        "grad_norm": 1.2808266878128052,
        "learning_rate": 3.237611470224307e-05,
        "epoch": 0.912708247280738,
        "step": 6629
    },
    {
        "loss": 1.8647,
        "grad_norm": 2.5248944759368896,
        "learning_rate": 3.228024682495068e-05,
        "epoch": 0.9128459314332921,
        "step": 6630
    },
    {
        "loss": 2.0512,
        "grad_norm": 1.4213968515396118,
        "learning_rate": 3.218449376841439e-05,
        "epoch": 0.9129836155858461,
        "step": 6631
    },
    {
        "loss": 1.946,
        "grad_norm": 3.071678876876831,
        "learning_rate": 3.208885569498611e-05,
        "epoch": 0.9131212997384001,
        "step": 6632
    },
    {
        "loss": 2.2304,
        "grad_norm": 3.373396396636963,
        "learning_rate": 3.1993332766823134e-05,
        "epoch": 0.9132589838909542,
        "step": 6633
    },
    {
        "loss": 2.5763,
        "grad_norm": 1.1608655452728271,
        "learning_rate": 3.189792514588719e-05,
        "epoch": 0.9133966680435082,
        "step": 6634
    },
    {
        "loss": 1.2365,
        "grad_norm": 2.912012815475464,
        "learning_rate": 3.180263299394456e-05,
        "epoch": 0.9135343521960623,
        "step": 6635
    },
    {
        "loss": 2.4344,
        "grad_norm": 2.708096504211426,
        "learning_rate": 3.170745647256571e-05,
        "epoch": 0.9136720363486163,
        "step": 6636
    },
    {
        "loss": 1.7141,
        "grad_norm": 1.8474291563034058,
        "learning_rate": 3.1612395743125304e-05,
        "epoch": 0.9138097205011703,
        "step": 6637
    },
    {
        "loss": 1.9889,
        "grad_norm": 2.4819462299346924,
        "learning_rate": 3.15174509668014e-05,
        "epoch": 0.9139474046537244,
        "step": 6638
    },
    {
        "loss": 3.0399,
        "grad_norm": 2.445420980453491,
        "learning_rate": 3.142262230457542e-05,
        "epoch": 0.9140850888062784,
        "step": 6639
    },
    {
        "loss": 2.4735,
        "grad_norm": 1.9033766984939575,
        "learning_rate": 3.1327909917232314e-05,
        "epoch": 0.9142227729588325,
        "step": 6640
    },
    {
        "loss": 2.0266,
        "grad_norm": 2.837326765060425,
        "learning_rate": 3.1233313965359466e-05,
        "epoch": 0.9143604571113865,
        "step": 6641
    },
    {
        "loss": 2.3751,
        "grad_norm": 1.9922653436660767,
        "learning_rate": 3.1138834609347e-05,
        "epoch": 0.9144981412639405,
        "step": 6642
    },
    {
        "loss": 1.3781,
        "grad_norm": 3.461782217025757,
        "learning_rate": 3.1044472009387335e-05,
        "epoch": 0.9146358254164946,
        "step": 6643
    },
    {
        "loss": 1.8365,
        "grad_norm": 1.9362637996673584,
        "learning_rate": 3.0950226325474916e-05,
        "epoch": 0.9147735095690486,
        "step": 6644
    },
    {
        "loss": 2.57,
        "grad_norm": 2.1788125038146973,
        "learning_rate": 3.085609771740584e-05,
        "epoch": 0.9149111937216027,
        "step": 6645
    },
    {
        "loss": 1.9775,
        "grad_norm": 1.565222978591919,
        "learning_rate": 3.0762086344778097e-05,
        "epoch": 0.9150488778741567,
        "step": 6646
    },
    {
        "loss": 1.533,
        "grad_norm": 2.5342857837677,
        "learning_rate": 3.0668192366990465e-05,
        "epoch": 0.9151865620267107,
        "step": 6647
    },
    {
        "loss": 1.3254,
        "grad_norm": 1.7722336053848267,
        "learning_rate": 3.0574415943242776e-05,
        "epoch": 0.9153242461792648,
        "step": 6648
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.224822998046875,
        "learning_rate": 3.0480757232535772e-05,
        "epoch": 0.9154619303318188,
        "step": 6649
    },
    {
        "loss": 1.8614,
        "grad_norm": 1.830646276473999,
        "learning_rate": 3.038721639367037e-05,
        "epoch": 0.9155996144843729,
        "step": 6650
    },
    {
        "loss": 1.5898,
        "grad_norm": 2.065638542175293,
        "learning_rate": 3.0293793585247666e-05,
        "epoch": 0.9157372986369269,
        "step": 6651
    },
    {
        "loss": 2.027,
        "grad_norm": 1.9014701843261719,
        "learning_rate": 3.0200488965668738e-05,
        "epoch": 0.915874982789481,
        "step": 6652
    },
    {
        "loss": 1.5292,
        "grad_norm": 1.8962188959121704,
        "learning_rate": 3.0107302693134177e-05,
        "epoch": 0.916012666942035,
        "step": 6653
    },
    {
        "loss": 1.8432,
        "grad_norm": 1.9345571994781494,
        "learning_rate": 3.0014234925643837e-05,
        "epoch": 0.916150351094589,
        "step": 6654
    },
    {
        "loss": 1.9472,
        "grad_norm": 1.3439699411392212,
        "learning_rate": 2.9921285820996947e-05,
        "epoch": 0.9162880352471431,
        "step": 6655
    },
    {
        "loss": 0.9359,
        "grad_norm": 5.357852935791016,
        "learning_rate": 2.9828455536791255e-05,
        "epoch": 0.9164257193996971,
        "step": 6656
    },
    {
        "loss": 1.6443,
        "grad_norm": 2.3460183143615723,
        "learning_rate": 2.973574423042299e-05,
        "epoch": 0.9165634035522512,
        "step": 6657
    },
    {
        "loss": 1.8634,
        "grad_norm": 2.3203773498535156,
        "learning_rate": 2.9643152059087063e-05,
        "epoch": 0.9167010877048052,
        "step": 6658
    },
    {
        "loss": 1.6323,
        "grad_norm": 3.1723735332489014,
        "learning_rate": 2.9550679179775974e-05,
        "epoch": 0.9168387718573592,
        "step": 6659
    },
    {
        "loss": 2.0105,
        "grad_norm": 1.2974823713302612,
        "learning_rate": 2.9458325749280023e-05,
        "epoch": 0.9169764560099133,
        "step": 6660
    },
    {
        "loss": 1.7563,
        "grad_norm": 1.9464536905288696,
        "learning_rate": 2.9366091924187246e-05,
        "epoch": 0.9171141401624673,
        "step": 6661
    },
    {
        "loss": 1.8605,
        "grad_norm": 1.984754204750061,
        "learning_rate": 2.9273977860882605e-05,
        "epoch": 0.9172518243150214,
        "step": 6662
    },
    {
        "loss": 2.1758,
        "grad_norm": 2.420464038848877,
        "learning_rate": 2.918198371554799e-05,
        "epoch": 0.9173895084675754,
        "step": 6663
    },
    {
        "loss": 1.9804,
        "grad_norm": 1.4172358512878418,
        "learning_rate": 2.90901096441623e-05,
        "epoch": 0.9175271926201294,
        "step": 6664
    },
    {
        "loss": 1.2737,
        "grad_norm": 2.488287925720215,
        "learning_rate": 2.899835580250051e-05,
        "epoch": 0.9176648767726835,
        "step": 6665
    },
    {
        "loss": 1.4529,
        "grad_norm": 3.37410044670105,
        "learning_rate": 2.890672234613384e-05,
        "epoch": 0.9178025609252375,
        "step": 6666
    },
    {
        "loss": 2.1135,
        "grad_norm": 1.5251578092575073,
        "learning_rate": 2.8815209430429424e-05,
        "epoch": 0.9179402450777916,
        "step": 6667
    },
    {
        "loss": 1.9114,
        "grad_norm": 3.5112056732177734,
        "learning_rate": 2.8723817210549986e-05,
        "epoch": 0.9180779292303456,
        "step": 6668
    },
    {
        "loss": 0.5813,
        "grad_norm": 5.030342102050781,
        "learning_rate": 2.8632545841453508e-05,
        "epoch": 0.9182156133828996,
        "step": 6669
    },
    {
        "loss": 0.8532,
        "grad_norm": 4.059624195098877,
        "learning_rate": 2.8541395477893408e-05,
        "epoch": 0.9183532975354537,
        "step": 6670
    },
    {
        "loss": 2.0377,
        "grad_norm": 2.2092864513397217,
        "learning_rate": 2.8450366274417582e-05,
        "epoch": 0.9184909816880077,
        "step": 6671
    },
    {
        "loss": 2.1922,
        "grad_norm": 2.1172385215759277,
        "learning_rate": 2.8359458385368543e-05,
        "epoch": 0.9186286658405618,
        "step": 6672
    },
    {
        "loss": 2.2264,
        "grad_norm": 2.4643967151641846,
        "learning_rate": 2.826867196488335e-05,
        "epoch": 0.9187663499931158,
        "step": 6673
    },
    {
        "loss": 1.3898,
        "grad_norm": 3.2121880054473877,
        "learning_rate": 2.8178007166892873e-05,
        "epoch": 0.9189040341456698,
        "step": 6674
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.4281222820281982,
        "learning_rate": 2.8087464145121823e-05,
        "epoch": 0.9190417182982239,
        "step": 6675
    },
    {
        "loss": 1.8125,
        "grad_norm": 2.2287042140960693,
        "learning_rate": 2.7997043053088434e-05,
        "epoch": 0.9191794024507779,
        "step": 6676
    },
    {
        "loss": 1.9018,
        "grad_norm": 2.066983938217163,
        "learning_rate": 2.7906744044104272e-05,
        "epoch": 0.919317086603332,
        "step": 6677
    },
    {
        "loss": 2.3336,
        "grad_norm": 1.8715600967407227,
        "learning_rate": 2.7816567271273764e-05,
        "epoch": 0.919454770755886,
        "step": 6678
    },
    {
        "loss": 1.8256,
        "grad_norm": 1.79609215259552,
        "learning_rate": 2.7726512887494328e-05,
        "epoch": 0.91959245490844,
        "step": 6679
    },
    {
        "loss": 1.5019,
        "grad_norm": 2.8354036808013916,
        "learning_rate": 2.7636581045455658e-05,
        "epoch": 0.9197301390609941,
        "step": 6680
    },
    {
        "loss": 2.7992,
        "grad_norm": 2.3857619762420654,
        "learning_rate": 2.754677189763961e-05,
        "epoch": 0.9198678232135481,
        "step": 6681
    },
    {
        "loss": 2.3496,
        "grad_norm": 2.168231248855591,
        "learning_rate": 2.745708559632032e-05,
        "epoch": 0.9200055073661022,
        "step": 6682
    },
    {
        "loss": 1.6654,
        "grad_norm": 3.1660523414611816,
        "learning_rate": 2.7367522293563387e-05,
        "epoch": 0.9201431915186562,
        "step": 6683
    },
    {
        "loss": 1.1794,
        "grad_norm": 2.3899269104003906,
        "learning_rate": 2.7278082141225823e-05,
        "epoch": 0.9202808756712102,
        "step": 6684
    },
    {
        "loss": 1.1412,
        "grad_norm": 1.5108610391616821,
        "learning_rate": 2.718876529095611e-05,
        "epoch": 0.9204185598237643,
        "step": 6685
    },
    {
        "loss": 2.3948,
        "grad_norm": 2.7456042766571045,
        "learning_rate": 2.70995718941934e-05,
        "epoch": 0.9205562439763183,
        "step": 6686
    },
    {
        "loss": 1.3886,
        "grad_norm": 3.3661375045776367,
        "learning_rate": 2.7010502102167522e-05,
        "epoch": 0.9206939281288724,
        "step": 6687
    },
    {
        "loss": 1.9405,
        "grad_norm": 1.664543867111206,
        "learning_rate": 2.692155606589899e-05,
        "epoch": 0.9208316122814264,
        "step": 6688
    },
    {
        "loss": 2.1279,
        "grad_norm": 1.8733088970184326,
        "learning_rate": 2.6832733936198285e-05,
        "epoch": 0.9209692964339804,
        "step": 6689
    },
    {
        "loss": 1.8507,
        "grad_norm": 2.3907668590545654,
        "learning_rate": 2.6744035863665817e-05,
        "epoch": 0.9211069805865345,
        "step": 6690
    },
    {
        "loss": 2.2456,
        "grad_norm": 1.6661717891693115,
        "learning_rate": 2.6655461998691668e-05,
        "epoch": 0.9212446647390885,
        "step": 6691
    },
    {
        "loss": 1.2239,
        "grad_norm": 2.839268207550049,
        "learning_rate": 2.6567012491455324e-05,
        "epoch": 0.9213823488916426,
        "step": 6692
    },
    {
        "loss": 1.7792,
        "grad_norm": 1.7370171546936035,
        "learning_rate": 2.6478687491925357e-05,
        "epoch": 0.9215200330441966,
        "step": 6693
    },
    {
        "loss": 2.2111,
        "grad_norm": 2.464195728302002,
        "learning_rate": 2.6390487149859456e-05,
        "epoch": 0.9216577171967506,
        "step": 6694
    },
    {
        "loss": 1.4088,
        "grad_norm": 3.9139628410339355,
        "learning_rate": 2.630241161480369e-05,
        "epoch": 0.9217954013493047,
        "step": 6695
    },
    {
        "loss": 1.1902,
        "grad_norm": 3.749660015106201,
        "learning_rate": 2.6214461036092587e-05,
        "epoch": 0.9219330855018587,
        "step": 6696
    },
    {
        "loss": 1.8829,
        "grad_norm": 2.1378016471862793,
        "learning_rate": 2.612663556284899e-05,
        "epoch": 0.9220707696544128,
        "step": 6697
    },
    {
        "loss": 1.7294,
        "grad_norm": 2.9692611694335938,
        "learning_rate": 2.6038935343983385e-05,
        "epoch": 0.9222084538069668,
        "step": 6698
    },
    {
        "loss": 1.6979,
        "grad_norm": 2.575134754180908,
        "learning_rate": 2.5951360528193958e-05,
        "epoch": 0.9223461379595208,
        "step": 6699
    },
    {
        "loss": 1.1168,
        "grad_norm": 3.241769552230835,
        "learning_rate": 2.586391126396629e-05,
        "epoch": 0.9224838221120749,
        "step": 6700
    },
    {
        "loss": 2.3723,
        "grad_norm": 1.1773109436035156,
        "learning_rate": 2.5776587699573064e-05,
        "epoch": 0.9226215062646289,
        "step": 6701
    },
    {
        "loss": 2.5435,
        "grad_norm": 2.008993148803711,
        "learning_rate": 2.568938998307381e-05,
        "epoch": 0.922759190417183,
        "step": 6702
    },
    {
        "loss": 1.8752,
        "grad_norm": 1.6875405311584473,
        "learning_rate": 2.5602318262314885e-05,
        "epoch": 0.922896874569737,
        "step": 6703
    },
    {
        "loss": 2.0419,
        "grad_norm": 1.9305264949798584,
        "learning_rate": 2.551537268492874e-05,
        "epoch": 0.923034558722291,
        "step": 6704
    },
    {
        "loss": 2.0561,
        "grad_norm": 1.7651504278182983,
        "learning_rate": 2.5428553398333988e-05,
        "epoch": 0.9231722428748451,
        "step": 6705
    },
    {
        "loss": 2.2485,
        "grad_norm": 1.8313692808151245,
        "learning_rate": 2.5341860549735363e-05,
        "epoch": 0.9233099270273991,
        "step": 6706
    },
    {
        "loss": 2.3909,
        "grad_norm": 2.005915880203247,
        "learning_rate": 2.525529428612291e-05,
        "epoch": 0.9234476111799532,
        "step": 6707
    },
    {
        "loss": 1.1681,
        "grad_norm": 2.172865152359009,
        "learning_rate": 2.5168854754272163e-05,
        "epoch": 0.9235852953325072,
        "step": 6708
    },
    {
        "loss": 1.7461,
        "grad_norm": 2.5789549350738525,
        "learning_rate": 2.508254210074392e-05,
        "epoch": 0.9237229794850613,
        "step": 6709
    },
    {
        "loss": 0.649,
        "grad_norm": 2.859142780303955,
        "learning_rate": 2.4996356471883654e-05,
        "epoch": 0.9238606636376153,
        "step": 6710
    },
    {
        "loss": 1.8573,
        "grad_norm": 3.268677234649658,
        "learning_rate": 2.4910298013821442e-05,
        "epoch": 0.9239983477901693,
        "step": 6711
    },
    {
        "loss": 1.6046,
        "grad_norm": 1.6726051568984985,
        "learning_rate": 2.4824366872472016e-05,
        "epoch": 0.9241360319427234,
        "step": 6712
    },
    {
        "loss": 1.6146,
        "grad_norm": 1.63914155960083,
        "learning_rate": 2.4738563193533916e-05,
        "epoch": 0.9242737160952774,
        "step": 6713
    },
    {
        "loss": 1.1349,
        "grad_norm": 2.006868362426758,
        "learning_rate": 2.4652887122489787e-05,
        "epoch": 0.9244114002478315,
        "step": 6714
    },
    {
        "loss": 1.7815,
        "grad_norm": 3.107292890548706,
        "learning_rate": 2.4567338804605787e-05,
        "epoch": 0.9245490844003855,
        "step": 6715
    },
    {
        "loss": 2.4393,
        "grad_norm": 2.5579135417938232,
        "learning_rate": 2.4481918384931513e-05,
        "epoch": 0.9246867685529395,
        "step": 6716
    },
    {
        "loss": 2.2644,
        "grad_norm": 1.2899091243743896,
        "learning_rate": 2.4396626008299616e-05,
        "epoch": 0.9248244527054936,
        "step": 6717
    },
    {
        "loss": 1.9078,
        "grad_norm": 2.2261147499084473,
        "learning_rate": 2.431146181932591e-05,
        "epoch": 0.9249621368580476,
        "step": 6718
    },
    {
        "loss": 1.7423,
        "grad_norm": 2.6882920265197754,
        "learning_rate": 2.4226425962408583e-05,
        "epoch": 0.9250998210106017,
        "step": 6719
    },
    {
        "loss": 1.8692,
        "grad_norm": 1.326249122619629,
        "learning_rate": 2.4141518581728318e-05,
        "epoch": 0.9252375051631557,
        "step": 6720
    },
    {
        "loss": 2.0362,
        "grad_norm": 2.266808032989502,
        "learning_rate": 2.4056739821247953e-05,
        "epoch": 0.9253751893157097,
        "step": 6721
    },
    {
        "loss": 1.7324,
        "grad_norm": 1.6928459405899048,
        "learning_rate": 2.3972089824712406e-05,
        "epoch": 0.9255128734682638,
        "step": 6722
    },
    {
        "loss": 1.9246,
        "grad_norm": 2.2188003063201904,
        "learning_rate": 2.3887568735648058e-05,
        "epoch": 0.9256505576208178,
        "step": 6723
    },
    {
        "loss": 1.8144,
        "grad_norm": 1.385621190071106,
        "learning_rate": 2.3803176697362805e-05,
        "epoch": 0.9257882417733719,
        "step": 6724
    },
    {
        "loss": 1.6126,
        "grad_norm": 3.7242438793182373,
        "learning_rate": 2.3718913852945745e-05,
        "epoch": 0.9259259259259259,
        "step": 6725
    },
    {
        "loss": 1.8034,
        "grad_norm": 2.5031116008758545,
        "learning_rate": 2.3634780345266894e-05,
        "epoch": 0.9260636100784799,
        "step": 6726
    },
    {
        "loss": 1.812,
        "grad_norm": 2.275635004043579,
        "learning_rate": 2.3550776316976896e-05,
        "epoch": 0.926201294231034,
        "step": 6727
    },
    {
        "loss": 1.8787,
        "grad_norm": 4.04171085357666,
        "learning_rate": 2.3466901910507167e-05,
        "epoch": 0.926338978383588,
        "step": 6728
    },
    {
        "loss": 1.9937,
        "grad_norm": 2.0254104137420654,
        "learning_rate": 2.3383157268069034e-05,
        "epoch": 0.9264766625361421,
        "step": 6729
    },
    {
        "loss": 1.5591,
        "grad_norm": 1.9000358581542969,
        "learning_rate": 2.3299542531653817e-05,
        "epoch": 0.9266143466886961,
        "step": 6730
    },
    {
        "loss": 1.6505,
        "grad_norm": 1.8450711965560913,
        "learning_rate": 2.3216057843032836e-05,
        "epoch": 0.9267520308412501,
        "step": 6731
    },
    {
        "loss": 2.2999,
        "grad_norm": 1.677117109298706,
        "learning_rate": 2.3132703343756624e-05,
        "epoch": 0.9268897149938042,
        "step": 6732
    },
    {
        "loss": 2.8768,
        "grad_norm": 1.9294991493225098,
        "learning_rate": 2.3049479175155063e-05,
        "epoch": 0.9270273991463582,
        "step": 6733
    },
    {
        "loss": 1.5478,
        "grad_norm": 2.4683144092559814,
        "learning_rate": 2.296638547833724e-05,
        "epoch": 0.9271650832989123,
        "step": 6734
    },
    {
        "loss": 2.1515,
        "grad_norm": 2.497523546218872,
        "learning_rate": 2.2883422394190755e-05,
        "epoch": 0.9273027674514663,
        "step": 6735
    },
    {
        "loss": 1.6342,
        "grad_norm": 2.3821723461151123,
        "learning_rate": 2.28005900633818e-05,
        "epoch": 0.9274404516040203,
        "step": 6736
    },
    {
        "loss": 2.2964,
        "grad_norm": 1.4215562343597412,
        "learning_rate": 2.27178886263551e-05,
        "epoch": 0.9275781357565744,
        "step": 6737
    },
    {
        "loss": 0.9362,
        "grad_norm": 2.627612590789795,
        "learning_rate": 2.2635318223333158e-05,
        "epoch": 0.9277158199091284,
        "step": 6738
    },
    {
        "loss": 2.0507,
        "grad_norm": 1.9963929653167725,
        "learning_rate": 2.2552878994316495e-05,
        "epoch": 0.9278535040616825,
        "step": 6739
    },
    {
        "loss": 1.7535,
        "grad_norm": 1.8773397207260132,
        "learning_rate": 2.2470571079083115e-05,
        "epoch": 0.9279911882142365,
        "step": 6740
    },
    {
        "loss": 0.4357,
        "grad_norm": 1.915592908859253,
        "learning_rate": 2.2388394617188413e-05,
        "epoch": 0.9281288723667905,
        "step": 6741
    },
    {
        "loss": 1.8198,
        "grad_norm": 3.545180082321167,
        "learning_rate": 2.230634974796484e-05,
        "epoch": 0.9282665565193446,
        "step": 6742
    },
    {
        "loss": 2.1545,
        "grad_norm": 1.9374754428863525,
        "learning_rate": 2.2224436610521936e-05,
        "epoch": 0.9284042406718986,
        "step": 6743
    },
    {
        "loss": 2.2994,
        "grad_norm": 1.8987716436386108,
        "learning_rate": 2.2142655343745666e-05,
        "epoch": 0.9285419248244527,
        "step": 6744
    },
    {
        "loss": 2.0458,
        "grad_norm": 3.4874205589294434,
        "learning_rate": 2.2061006086298374e-05,
        "epoch": 0.9286796089770067,
        "step": 6745
    },
    {
        "loss": 1.665,
        "grad_norm": 1.948072075843811,
        "learning_rate": 2.19794889766189e-05,
        "epoch": 0.9288172931295607,
        "step": 6746
    },
    {
        "loss": 2.109,
        "grad_norm": 1.4516681432724,
        "learning_rate": 2.1898104152921706e-05,
        "epoch": 0.9289549772821148,
        "step": 6747
    },
    {
        "loss": 2.5265,
        "grad_norm": 2.697152853012085,
        "learning_rate": 2.1816851753197056e-05,
        "epoch": 0.9290926614346688,
        "step": 6748
    },
    {
        "loss": 2.3163,
        "grad_norm": 2.4686806201934814,
        "learning_rate": 2.1735731915210688e-05,
        "epoch": 0.929230345587223,
        "step": 6749
    },
    {
        "loss": 2.4057,
        "grad_norm": 1.2442190647125244,
        "learning_rate": 2.1654744776503578e-05,
        "epoch": 0.929368029739777,
        "step": 6750
    },
    {
        "loss": 1.8913,
        "grad_norm": 2.348447322845459,
        "learning_rate": 2.1573890474391613e-05,
        "epoch": 0.9295057138923309,
        "step": 6751
    },
    {
        "loss": 1.9939,
        "grad_norm": 1.5955957174301147,
        "learning_rate": 2.1493169145965754e-05,
        "epoch": 0.929643398044885,
        "step": 6752
    },
    {
        "loss": 1.7182,
        "grad_norm": 1.555038332939148,
        "learning_rate": 2.1412580928091186e-05,
        "epoch": 0.929781082197439,
        "step": 6753
    },
    {
        "loss": 2.1262,
        "grad_norm": 1.4110559225082397,
        "learning_rate": 2.1332125957407446e-05,
        "epoch": 0.9299187663499932,
        "step": 6754
    },
    {
        "loss": 1.2328,
        "grad_norm": 3.6800689697265625,
        "learning_rate": 2.1251804370328354e-05,
        "epoch": 0.9300564505025471,
        "step": 6755
    },
    {
        "loss": 1.9598,
        "grad_norm": 2.7231359481811523,
        "learning_rate": 2.1171616303041364e-05,
        "epoch": 0.9301941346551011,
        "step": 6756
    },
    {
        "loss": 1.705,
        "grad_norm": 2.2483136653900146,
        "learning_rate": 2.1091561891507493e-05,
        "epoch": 0.9303318188076553,
        "step": 6757
    },
    {
        "loss": 1.8391,
        "grad_norm": 1.8794238567352295,
        "learning_rate": 2.101164127146148e-05,
        "epoch": 0.9304695029602092,
        "step": 6758
    },
    {
        "loss": 1.7179,
        "grad_norm": 2.1391875743865967,
        "learning_rate": 2.0931854578410905e-05,
        "epoch": 0.9306071871127634,
        "step": 6759
    },
    {
        "loss": 2.1138,
        "grad_norm": 1.218963623046875,
        "learning_rate": 2.085220194763624e-05,
        "epoch": 0.9307448712653174,
        "step": 6760
    },
    {
        "loss": 2.2205,
        "grad_norm": 2.7766385078430176,
        "learning_rate": 2.077268351419097e-05,
        "epoch": 0.9308825554178713,
        "step": 6761
    },
    {
        "loss": 1.9679,
        "grad_norm": 2.765559673309326,
        "learning_rate": 2.0693299412900757e-05,
        "epoch": 0.9310202395704255,
        "step": 6762
    },
    {
        "loss": 1.8629,
        "grad_norm": 2.084225654602051,
        "learning_rate": 2.0614049778363563e-05,
        "epoch": 0.9311579237229795,
        "step": 6763
    },
    {
        "loss": 1.6007,
        "grad_norm": 2.9162988662719727,
        "learning_rate": 2.053493474494943e-05,
        "epoch": 0.9312956078755336,
        "step": 6764
    },
    {
        "loss": 2.2641,
        "grad_norm": 1.9183220863342285,
        "learning_rate": 2.0455954446800118e-05,
        "epoch": 0.9314332920280876,
        "step": 6765
    },
    {
        "loss": 1.9326,
        "grad_norm": 2.9222590923309326,
        "learning_rate": 2.0377109017828854e-05,
        "epoch": 0.9315709761806417,
        "step": 6766
    },
    {
        "loss": 2.5161,
        "grad_norm": 1.1728001832962036,
        "learning_rate": 2.029839859172048e-05,
        "epoch": 0.9317086603331957,
        "step": 6767
    },
    {
        "loss": 1.3647,
        "grad_norm": 1.585178017616272,
        "learning_rate": 2.0219823301930653e-05,
        "epoch": 0.9318463444857497,
        "step": 6768
    },
    {
        "loss": 1.78,
        "grad_norm": 1.3991446495056152,
        "learning_rate": 2.01413832816859e-05,
        "epoch": 0.9319840286383038,
        "step": 6769
    },
    {
        "loss": 1.689,
        "grad_norm": 2.707127094268799,
        "learning_rate": 2.0063078663983694e-05,
        "epoch": 0.9321217127908578,
        "step": 6770
    },
    {
        "loss": 2.1493,
        "grad_norm": 1.356558918952942,
        "learning_rate": 1.9984909581591626e-05,
        "epoch": 0.9322593969434119,
        "step": 6771
    },
    {
        "loss": 1.6718,
        "grad_norm": 2.486063241958618,
        "learning_rate": 1.9906876167047594e-05,
        "epoch": 0.9323970810959659,
        "step": 6772
    },
    {
        "loss": 1.9529,
        "grad_norm": 2.799795150756836,
        "learning_rate": 1.982897855265945e-05,
        "epoch": 0.9325347652485199,
        "step": 6773
    },
    {
        "loss": 1.2454,
        "grad_norm": 3.1497983932495117,
        "learning_rate": 1.9751216870504808e-05,
        "epoch": 0.932672449401074,
        "step": 6774
    },
    {
        "loss": 1.8972,
        "grad_norm": 3.042797803878784,
        "learning_rate": 1.9673591252430712e-05,
        "epoch": 0.932810133553628,
        "step": 6775
    },
    {
        "loss": 1.6508,
        "grad_norm": 2.2883763313293457,
        "learning_rate": 1.9596101830053724e-05,
        "epoch": 0.9329478177061821,
        "step": 6776
    },
    {
        "loss": 2.1463,
        "grad_norm": 2.062422752380371,
        "learning_rate": 1.9518748734759328e-05,
        "epoch": 0.9330855018587361,
        "step": 6777
    },
    {
        "loss": 0.2708,
        "grad_norm": 1.2209300994873047,
        "learning_rate": 1.9441532097701785e-05,
        "epoch": 0.9332231860112901,
        "step": 6778
    },
    {
        "loss": 1.7899,
        "grad_norm": 1.6964097023010254,
        "learning_rate": 1.9364452049804225e-05,
        "epoch": 0.9333608701638442,
        "step": 6779
    },
    {
        "loss": 2.272,
        "grad_norm": 1.3826947212219238,
        "learning_rate": 1.9287508721757997e-05,
        "epoch": 0.9334985543163982,
        "step": 6780
    },
    {
        "loss": 2.0787,
        "grad_norm": 1.7663052082061768,
        "learning_rate": 1.921070224402256e-05,
        "epoch": 0.9336362384689523,
        "step": 6781
    },
    {
        "loss": 1.911,
        "grad_norm": 2.0761096477508545,
        "learning_rate": 1.913403274682569e-05,
        "epoch": 0.9337739226215063,
        "step": 6782
    },
    {
        "loss": 1.8367,
        "grad_norm": 2.6894264221191406,
        "learning_rate": 1.9057500360162585e-05,
        "epoch": 0.9339116067740603,
        "step": 6783
    },
    {
        "loss": 1.6904,
        "grad_norm": 3.814094066619873,
        "learning_rate": 1.898110521379598e-05,
        "epoch": 0.9340492909266144,
        "step": 6784
    },
    {
        "loss": 2.2909,
        "grad_norm": 1.4864975214004517,
        "learning_rate": 1.890484743725617e-05,
        "epoch": 0.9341869750791684,
        "step": 6785
    },
    {
        "loss": 2.2809,
        "grad_norm": 2.6217172145843506,
        "learning_rate": 1.8828727159840286e-05,
        "epoch": 0.9343246592317225,
        "step": 6786
    },
    {
        "loss": 2.2828,
        "grad_norm": 1.6517937183380127,
        "learning_rate": 1.8752744510612417e-05,
        "epoch": 0.9344623433842765,
        "step": 6787
    },
    {
        "loss": 1.4283,
        "grad_norm": 3.896144390106201,
        "learning_rate": 1.8676899618403276e-05,
        "epoch": 0.9346000275368305,
        "step": 6788
    },
    {
        "loss": 1.4303,
        "grad_norm": 2.2603843212127686,
        "learning_rate": 1.860119261181006e-05,
        "epoch": 0.9347377116893846,
        "step": 6789
    },
    {
        "loss": 2.1861,
        "grad_norm": 1.8101475238800049,
        "learning_rate": 1.852562361919603e-05,
        "epoch": 0.9348753958419386,
        "step": 6790
    },
    {
        "loss": 1.7178,
        "grad_norm": 3.3885347843170166,
        "learning_rate": 1.845019276869072e-05,
        "epoch": 0.9350130799944927,
        "step": 6791
    },
    {
        "loss": 1.8181,
        "grad_norm": 2.5930988788604736,
        "learning_rate": 1.8374900188189192e-05,
        "epoch": 0.9351507641470467,
        "step": 6792
    },
    {
        "loss": 1.584,
        "grad_norm": 1.3817585706710815,
        "learning_rate": 1.829974600535206e-05,
        "epoch": 0.9352884482996007,
        "step": 6793
    },
    {
        "loss": 1.7589,
        "grad_norm": 2.6865150928497314,
        "learning_rate": 1.82247303476055e-05,
        "epoch": 0.9354261324521548,
        "step": 6794
    },
    {
        "loss": 1.2941,
        "grad_norm": 2.3471555709838867,
        "learning_rate": 1.8149853342140645e-05,
        "epoch": 0.9355638166047088,
        "step": 6795
    },
    {
        "loss": 2.0922,
        "grad_norm": 2.0247795581817627,
        "learning_rate": 1.8075115115913555e-05,
        "epoch": 0.9357015007572629,
        "step": 6796
    },
    {
        "loss": 1.6598,
        "grad_norm": 3.2954511642456055,
        "learning_rate": 1.8000515795645013e-05,
        "epoch": 0.9358391849098169,
        "step": 6797
    },
    {
        "loss": 1.5796,
        "grad_norm": 3.095942258834839,
        "learning_rate": 1.792605550782026e-05,
        "epoch": 0.9359768690623709,
        "step": 6798
    },
    {
        "loss": 2.2365,
        "grad_norm": 2.989638328552246,
        "learning_rate": 1.7851734378688756e-05,
        "epoch": 0.936114553214925,
        "step": 6799
    },
    {
        "loss": 2.1446,
        "grad_norm": 2.8309123516082764,
        "learning_rate": 1.7777552534264218e-05,
        "epoch": 0.936252237367479,
        "step": 6800
    },
    {
        "loss": 1.9523,
        "grad_norm": 2.2945713996887207,
        "learning_rate": 1.7703510100323973e-05,
        "epoch": 0.9363899215200331,
        "step": 6801
    },
    {
        "loss": 2.4439,
        "grad_norm": 1.3564518690109253,
        "learning_rate": 1.7629607202409027e-05,
        "epoch": 0.9365276056725871,
        "step": 6802
    },
    {
        "loss": 1.8637,
        "grad_norm": 1.9530726671218872,
        "learning_rate": 1.7555843965823958e-05,
        "epoch": 0.9366652898251411,
        "step": 6803
    },
    {
        "loss": 1.696,
        "grad_norm": 2.531383514404297,
        "learning_rate": 1.748222051563636e-05,
        "epoch": 0.9368029739776952,
        "step": 6804
    },
    {
        "loss": 1.9783,
        "grad_norm": 2.5020158290863037,
        "learning_rate": 1.7408736976676842e-05,
        "epoch": 0.9369406581302492,
        "step": 6805
    },
    {
        "loss": 1.6897,
        "grad_norm": 2.1962296962738037,
        "learning_rate": 1.7335393473538774e-05,
        "epoch": 0.9370783422828033,
        "step": 6806
    },
    {
        "loss": 0.9855,
        "grad_norm": 4.073400497436523,
        "learning_rate": 1.726219013057826e-05,
        "epoch": 0.9372160264353573,
        "step": 6807
    },
    {
        "loss": 0.6637,
        "grad_norm": 2.445880889892578,
        "learning_rate": 1.7189127071913604e-05,
        "epoch": 0.9373537105879113,
        "step": 6808
    },
    {
        "loss": 2.2765,
        "grad_norm": 1.349240779876709,
        "learning_rate": 1.7116204421425185e-05,
        "epoch": 0.9374913947404654,
        "step": 6809
    },
    {
        "loss": 2.3348,
        "grad_norm": 3.079866409301758,
        "learning_rate": 1.7043422302755586e-05,
        "epoch": 0.9376290788930194,
        "step": 6810
    },
    {
        "loss": 1.2644,
        "grad_norm": 2.4106457233428955,
        "learning_rate": 1.6970780839308853e-05,
        "epoch": 0.9377667630455735,
        "step": 6811
    },
    {
        "loss": 1.9608,
        "grad_norm": 2.0010502338409424,
        "learning_rate": 1.6898280154250645e-05,
        "epoch": 0.9379044471981275,
        "step": 6812
    },
    {
        "loss": 2.0867,
        "grad_norm": 1.9221978187561035,
        "learning_rate": 1.6825920370507907e-05,
        "epoch": 0.9380421313506815,
        "step": 6813
    },
    {
        "loss": 1.3339,
        "grad_norm": 1.139079213142395,
        "learning_rate": 1.6753701610768758e-05,
        "epoch": 0.9381798155032356,
        "step": 6814
    },
    {
        "loss": 1.9914,
        "grad_norm": 2.4478166103363037,
        "learning_rate": 1.668162399748202e-05,
        "epoch": 0.9383174996557896,
        "step": 6815
    },
    {
        "loss": 1.9374,
        "grad_norm": 2.997340202331543,
        "learning_rate": 1.6609687652857487e-05,
        "epoch": 0.9384551838083437,
        "step": 6816
    },
    {
        "loss": 1.2026,
        "grad_norm": 4.184372425079346,
        "learning_rate": 1.653789269886519e-05,
        "epoch": 0.9385928679608977,
        "step": 6817
    },
    {
        "loss": 1.6622,
        "grad_norm": 1.1039611101150513,
        "learning_rate": 1.6466239257235427e-05,
        "epoch": 0.9387305521134517,
        "step": 6818
    },
    {
        "loss": 1.0868,
        "grad_norm": 2.079514980316162,
        "learning_rate": 1.6394727449458756e-05,
        "epoch": 0.9388682362660058,
        "step": 6819
    },
    {
        "loss": 1.7084,
        "grad_norm": 1.9153311252593994,
        "learning_rate": 1.6323357396785465e-05,
        "epoch": 0.9390059204185598,
        "step": 6820
    },
    {
        "loss": 1.146,
        "grad_norm": 1.9151030778884888,
        "learning_rate": 1.625212922022544e-05,
        "epoch": 0.9391436045711139,
        "step": 6821
    },
    {
        "loss": 1.6004,
        "grad_norm": 2.822597026824951,
        "learning_rate": 1.618104304054808e-05,
        "epoch": 0.9392812887236679,
        "step": 6822
    },
    {
        "loss": 0.8202,
        "grad_norm": 2.1915788650512695,
        "learning_rate": 1.611009897828203e-05,
        "epoch": 0.9394189728762219,
        "step": 6823
    },
    {
        "loss": 1.4425,
        "grad_norm": 3.869155168533325,
        "learning_rate": 1.603929715371485e-05,
        "epoch": 0.939556657028776,
        "step": 6824
    },
    {
        "loss": 2.4729,
        "grad_norm": 2.236049175262451,
        "learning_rate": 1.5968637686893206e-05,
        "epoch": 0.93969434118133,
        "step": 6825
    },
    {
        "loss": 1.8514,
        "grad_norm": 2.0729665756225586,
        "learning_rate": 1.589812069762211e-05,
        "epoch": 0.9398320253338841,
        "step": 6826
    },
    {
        "loss": 1.9576,
        "grad_norm": 1.887710690498352,
        "learning_rate": 1.5827746305465095e-05,
        "epoch": 0.9399697094864381,
        "step": 6827
    },
    {
        "loss": 2.2334,
        "grad_norm": 1.8426873683929443,
        "learning_rate": 1.5757514629744054e-05,
        "epoch": 0.9401073936389922,
        "step": 6828
    },
    {
        "loss": 1.7202,
        "grad_norm": 2.814286231994629,
        "learning_rate": 1.568742578953869e-05,
        "epoch": 0.9402450777915462,
        "step": 6829
    },
    {
        "loss": 2.5913,
        "grad_norm": 1.9503216743469238,
        "learning_rate": 1.561747990368656e-05,
        "epoch": 0.9403827619441002,
        "step": 6830
    },
    {
        "loss": 2.1667,
        "grad_norm": 1.8942562341690063,
        "learning_rate": 1.554767709078303e-05,
        "epoch": 0.9405204460966543,
        "step": 6831
    },
    {
        "loss": 1.7537,
        "grad_norm": 1.619018316268921,
        "learning_rate": 1.547801746918065e-05,
        "epoch": 0.9406581302492083,
        "step": 6832
    },
    {
        "loss": 2.1079,
        "grad_norm": 1.8447004556655884,
        "learning_rate": 1.5408501156989252e-05,
        "epoch": 0.9407958144017624,
        "step": 6833
    },
    {
        "loss": 2.1008,
        "grad_norm": 1.8192609548568726,
        "learning_rate": 1.533912827207582e-05,
        "epoch": 0.9409334985543164,
        "step": 6834
    },
    {
        "loss": 2.2063,
        "grad_norm": 2.0168228149414062,
        "learning_rate": 1.5269898932063975e-05,
        "epoch": 0.9410711827068704,
        "step": 6835
    },
    {
        "loss": 1.8239,
        "grad_norm": 1.7857104539871216,
        "learning_rate": 1.5200813254334012e-05,
        "epoch": 0.9412088668594245,
        "step": 6836
    },
    {
        "loss": 1.5204,
        "grad_norm": 2.2474141120910645,
        "learning_rate": 1.5131871356022675e-05,
        "epoch": 0.9413465510119785,
        "step": 6837
    },
    {
        "loss": 1.9018,
        "grad_norm": 2.2087764739990234,
        "learning_rate": 1.5063073354022883e-05,
        "epoch": 0.9414842351645326,
        "step": 6838
    },
    {
        "loss": 1.4166,
        "grad_norm": 2.75543475151062,
        "learning_rate": 1.4994419364983502e-05,
        "epoch": 0.9416219193170866,
        "step": 6839
    },
    {
        "loss": 1.1983,
        "grad_norm": 2.59206223487854,
        "learning_rate": 1.4925909505309499e-05,
        "epoch": 0.9417596034696406,
        "step": 6840
    },
    {
        "loss": 2.5745,
        "grad_norm": 2.102825403213501,
        "learning_rate": 1.4857543891161186e-05,
        "epoch": 0.9418972876221947,
        "step": 6841
    },
    {
        "loss": 2.0541,
        "grad_norm": 2.0672247409820557,
        "learning_rate": 1.4789322638454327e-05,
        "epoch": 0.9420349717747487,
        "step": 6842
    },
    {
        "loss": 1.7122,
        "grad_norm": 2.186967372894287,
        "learning_rate": 1.4721245862860122e-05,
        "epoch": 0.9421726559273028,
        "step": 6843
    },
    {
        "loss": 1.9153,
        "grad_norm": 3.6243157386779785,
        "learning_rate": 1.465331367980457e-05,
        "epoch": 0.9423103400798568,
        "step": 6844
    },
    {
        "loss": 2.0781,
        "grad_norm": 1.5737180709838867,
        "learning_rate": 1.4585526204468658e-05,
        "epoch": 0.9424480242324108,
        "step": 6845
    },
    {
        "loss": 1.3776,
        "grad_norm": 2.5449416637420654,
        "learning_rate": 1.4517883551787959e-05,
        "epoch": 0.9425857083849649,
        "step": 6846
    },
    {
        "loss": 2.0551,
        "grad_norm": 2.368934392929077,
        "learning_rate": 1.4450385836452485e-05,
        "epoch": 0.9427233925375189,
        "step": 6847
    },
    {
        "loss": 1.673,
        "grad_norm": 1.4593368768692017,
        "learning_rate": 1.4383033172906446e-05,
        "epoch": 0.942861076690073,
        "step": 6848
    },
    {
        "loss": 1.4785,
        "grad_norm": 2.382871150970459,
        "learning_rate": 1.4315825675348349e-05,
        "epoch": 0.942998760842627,
        "step": 6849
    },
    {
        "loss": 2.013,
        "grad_norm": 2.115316867828369,
        "learning_rate": 1.4248763457730318e-05,
        "epoch": 0.943136444995181,
        "step": 6850
    },
    {
        "loss": 1.3349,
        "grad_norm": 4.023947715759277,
        "learning_rate": 1.4181846633758155e-05,
        "epoch": 0.9432741291477351,
        "step": 6851
    },
    {
        "loss": 2.0943,
        "grad_norm": 2.3993942737579346,
        "learning_rate": 1.4115075316891402e-05,
        "epoch": 0.9434118133002891,
        "step": 6852
    },
    {
        "loss": 2.1288,
        "grad_norm": 1.2287006378173828,
        "learning_rate": 1.4048449620342641e-05,
        "epoch": 0.9435494974528432,
        "step": 6853
    },
    {
        "loss": 2.2148,
        "grad_norm": 1.4398049116134644,
        "learning_rate": 1.3981969657077531e-05,
        "epoch": 0.9436871816053972,
        "step": 6854
    },
    {
        "loss": 1.7786,
        "grad_norm": 3.003295660018921,
        "learning_rate": 1.3915635539814887e-05,
        "epoch": 0.9438248657579512,
        "step": 6855
    },
    {
        "loss": 2.3022,
        "grad_norm": 1.4950578212738037,
        "learning_rate": 1.3849447381026004e-05,
        "epoch": 0.9439625499105053,
        "step": 6856
    },
    {
        "loss": 2.2879,
        "grad_norm": 1.2849644422531128,
        "learning_rate": 1.378340529293468e-05,
        "epoch": 0.9441002340630593,
        "step": 6857
    },
    {
        "loss": 2.1489,
        "grad_norm": 1.290358066558838,
        "learning_rate": 1.3717509387517336e-05,
        "epoch": 0.9442379182156134,
        "step": 6858
    },
    {
        "loss": 2.5644,
        "grad_norm": 1.332381248474121,
        "learning_rate": 1.3651759776502248e-05,
        "epoch": 0.9443756023681674,
        "step": 6859
    },
    {
        "loss": 2.3645,
        "grad_norm": 1.630252718925476,
        "learning_rate": 1.3586156571369736e-05,
        "epoch": 0.9445132865207214,
        "step": 6860
    },
    {
        "loss": 2.1029,
        "grad_norm": 1.6819323301315308,
        "learning_rate": 1.3520699883351895e-05,
        "epoch": 0.9446509706732755,
        "step": 6861
    },
    {
        "loss": 1.914,
        "grad_norm": 2.066201686859131,
        "learning_rate": 1.3455389823432373e-05,
        "epoch": 0.9447886548258295,
        "step": 6862
    },
    {
        "loss": 2.0574,
        "grad_norm": 2.3362321853637695,
        "learning_rate": 1.339022650234616e-05,
        "epoch": 0.9449263389783836,
        "step": 6863
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.8948910236358643,
        "learning_rate": 1.3325210030579671e-05,
        "epoch": 0.9450640231309376,
        "step": 6864
    },
    {
        "loss": 1.3671,
        "grad_norm": 2.3811659812927246,
        "learning_rate": 1.3260340518370085e-05,
        "epoch": 0.9452017072834916,
        "step": 6865
    },
    {
        "loss": 1.7372,
        "grad_norm": 2.3166213035583496,
        "learning_rate": 1.3195618075705428e-05,
        "epoch": 0.9453393914360457,
        "step": 6866
    },
    {
        "loss": 1.709,
        "grad_norm": 3.3298733234405518,
        "learning_rate": 1.3131042812324578e-05,
        "epoch": 0.9454770755885997,
        "step": 6867
    },
    {
        "loss": 2.1729,
        "grad_norm": 3.1453166007995605,
        "learning_rate": 1.3066614837716628e-05,
        "epoch": 0.9456147597411538,
        "step": 6868
    },
    {
        "loss": 0.9157,
        "grad_norm": 4.245405197143555,
        "learning_rate": 1.300233426112103e-05,
        "epoch": 0.9457524438937078,
        "step": 6869
    },
    {
        "loss": 1.8041,
        "grad_norm": 1.3983291387557983,
        "learning_rate": 1.2938201191527343e-05,
        "epoch": 0.9458901280462618,
        "step": 6870
    },
    {
        "loss": 2.2962,
        "grad_norm": 1.8145787715911865,
        "learning_rate": 1.2874215737674999e-05,
        "epoch": 0.9460278121988159,
        "step": 6871
    },
    {
        "loss": 1.4784,
        "grad_norm": 2.71047306060791,
        "learning_rate": 1.281037800805307e-05,
        "epoch": 0.9461654963513699,
        "step": 6872
    },
    {
        "loss": 1.6166,
        "grad_norm": 2.1415727138519287,
        "learning_rate": 1.2746688110900352e-05,
        "epoch": 0.946303180503924,
        "step": 6873
    },
    {
        "loss": 1.4616,
        "grad_norm": 1.597572922706604,
        "learning_rate": 1.2683146154204816e-05,
        "epoch": 0.946440864656478,
        "step": 6874
    },
    {
        "loss": 1.9568,
        "grad_norm": 3.1266982555389404,
        "learning_rate": 1.2619752245703553e-05,
        "epoch": 0.946578548809032,
        "step": 6875
    },
    {
        "loss": 2.4226,
        "grad_norm": 1.8579133749008179,
        "learning_rate": 1.255650649288289e-05,
        "epoch": 0.9467162329615861,
        "step": 6876
    },
    {
        "loss": 1.1722,
        "grad_norm": 2.863924741744995,
        "learning_rate": 1.249340900297773e-05,
        "epoch": 0.9468539171141401,
        "step": 6877
    },
    {
        "loss": 1.5534,
        "grad_norm": 2.240779399871826,
        "learning_rate": 1.2430459882971558e-05,
        "epoch": 0.9469916012666942,
        "step": 6878
    },
    {
        "loss": 2.3737,
        "grad_norm": 3.0013411045074463,
        "learning_rate": 1.236765923959654e-05,
        "epoch": 0.9471292854192482,
        "step": 6879
    },
    {
        "loss": 2.1352,
        "grad_norm": 1.4254173040390015,
        "learning_rate": 1.230500717933285e-05,
        "epoch": 0.9472669695718022,
        "step": 6880
    },
    {
        "loss": 2.2181,
        "grad_norm": 1.2378884553909302,
        "learning_rate": 1.2242503808408767e-05,
        "epoch": 0.9474046537243563,
        "step": 6881
    },
    {
        "loss": 2.0357,
        "grad_norm": 1.08453369140625,
        "learning_rate": 1.2180149232800686e-05,
        "epoch": 0.9475423378769103,
        "step": 6882
    },
    {
        "loss": 2.1345,
        "grad_norm": 3.3219192028045654,
        "learning_rate": 1.2117943558232458e-05,
        "epoch": 0.9476800220294644,
        "step": 6883
    },
    {
        "loss": 2.2431,
        "grad_norm": 1.9056452512741089,
        "learning_rate": 1.2055886890175572e-05,
        "epoch": 0.9478177061820184,
        "step": 6884
    },
    {
        "loss": 1.8343,
        "grad_norm": 1.7734010219573975,
        "learning_rate": 1.1993979333848871e-05,
        "epoch": 0.9479553903345725,
        "step": 6885
    },
    {
        "loss": 1.4425,
        "grad_norm": 3.003462791442871,
        "learning_rate": 1.1932220994218358e-05,
        "epoch": 0.9480930744871265,
        "step": 6886
    },
    {
        "loss": 2.0182,
        "grad_norm": 2.543809413909912,
        "learning_rate": 1.1870611975997037e-05,
        "epoch": 0.9482307586396805,
        "step": 6887
    },
    {
        "loss": 2.023,
        "grad_norm": 1.5589462518692017,
        "learning_rate": 1.1809152383644662e-05,
        "epoch": 0.9483684427922346,
        "step": 6888
    },
    {
        "loss": 1.4874,
        "grad_norm": 3.9667582511901855,
        "learning_rate": 1.1747842321367886e-05,
        "epoch": 0.9485061269447886,
        "step": 6889
    },
    {
        "loss": 1.942,
        "grad_norm": 2.7776827812194824,
        "learning_rate": 1.1686681893119555e-05,
        "epoch": 0.9486438110973427,
        "step": 6890
    },
    {
        "loss": 1.3059,
        "grad_norm": 1.7898929119110107,
        "learning_rate": 1.1625671202598853e-05,
        "epoch": 0.9487814952498967,
        "step": 6891
    },
    {
        "loss": 1.9404,
        "grad_norm": 2.5675058364868164,
        "learning_rate": 1.1564810353251254e-05,
        "epoch": 0.9489191794024507,
        "step": 6892
    },
    {
        "loss": 1.6581,
        "grad_norm": 2.3537843227386475,
        "learning_rate": 1.150409944826798e-05,
        "epoch": 0.9490568635550048,
        "step": 6893
    },
    {
        "loss": 1.293,
        "grad_norm": 2.875742197036743,
        "learning_rate": 1.144353859058609e-05,
        "epoch": 0.9491945477075588,
        "step": 6894
    },
    {
        "loss": 1.6038,
        "grad_norm": 1.7904036045074463,
        "learning_rate": 1.1383127882888211e-05,
        "epoch": 0.949332231860113,
        "step": 6895
    },
    {
        "loss": 1.943,
        "grad_norm": 1.3631240129470825,
        "learning_rate": 1.1322867427602446e-05,
        "epoch": 0.949469916012667,
        "step": 6896
    },
    {
        "loss": 1.9301,
        "grad_norm": 1.9279603958129883,
        "learning_rate": 1.1262757326902018e-05,
        "epoch": 0.9496076001652209,
        "step": 6897
    },
    {
        "loss": 1.4996,
        "grad_norm": 1.86091947555542,
        "learning_rate": 1.1202797682705424e-05,
        "epoch": 0.949745284317775,
        "step": 6898
    },
    {
        "loss": 2.2486,
        "grad_norm": 2.329115390777588,
        "learning_rate": 1.1142988596675873e-05,
        "epoch": 0.949882968470329,
        "step": 6899
    },
    {
        "loss": 2.0239,
        "grad_norm": 2.276793956756592,
        "learning_rate": 1.1083330170221296e-05,
        "epoch": 0.9500206526228832,
        "step": 6900
    },
    {
        "loss": 1.5875,
        "grad_norm": 2.765446662902832,
        "learning_rate": 1.1023822504494385e-05,
        "epoch": 0.9501583367754372,
        "step": 6901
    },
    {
        "loss": 1.401,
        "grad_norm": 2.593641519546509,
        "learning_rate": 1.0964465700392002e-05,
        "epoch": 0.9502960209279911,
        "step": 6902
    },
    {
        "loss": 1.9667,
        "grad_norm": 1.4133843183517456,
        "learning_rate": 1.0905259858555206e-05,
        "epoch": 0.9504337050805453,
        "step": 6903
    },
    {
        "loss": 1.9564,
        "grad_norm": 2.6752161979675293,
        "learning_rate": 1.084620507936932e-05,
        "epoch": 0.9505713892330993,
        "step": 6904
    },
    {
        "loss": 2.2037,
        "grad_norm": 1.6142762899398804,
        "learning_rate": 1.0787301462963317e-05,
        "epoch": 0.9507090733856534,
        "step": 6905
    },
    {
        "loss": 1.207,
        "grad_norm": 2.4203755855560303,
        "learning_rate": 1.0728549109209874e-05,
        "epoch": 0.9508467575382074,
        "step": 6906
    },
    {
        "loss": 1.6608,
        "grad_norm": 1.3038688898086548,
        "learning_rate": 1.0669948117725415e-05,
        "epoch": 0.9509844416907614,
        "step": 6907
    },
    {
        "loss": 1.2136,
        "grad_norm": 2.760496139526367,
        "learning_rate": 1.0611498587869506e-05,
        "epoch": 0.9511221258433155,
        "step": 6908
    },
    {
        "loss": 1.6995,
        "grad_norm": 3.1126210689544678,
        "learning_rate": 1.0553200618744985e-05,
        "epoch": 0.9512598099958695,
        "step": 6909
    },
    {
        "loss": 2.2332,
        "grad_norm": 1.7188915014266968,
        "learning_rate": 1.0495054309197683e-05,
        "epoch": 0.9513974941484236,
        "step": 6910
    },
    {
        "loss": 2.4845,
        "grad_norm": 1.8942679166793823,
        "learning_rate": 1.0437059757816337e-05,
        "epoch": 0.9515351783009776,
        "step": 6911
    },
    {
        "loss": 1.6824,
        "grad_norm": 2.1652183532714844,
        "learning_rate": 1.0379217062932278e-05,
        "epoch": 0.9516728624535316,
        "step": 6912
    },
    {
        "loss": 2.0224,
        "grad_norm": 2.144666910171509,
        "learning_rate": 1.0321526322619534e-05,
        "epoch": 0.9518105466060857,
        "step": 6913
    },
    {
        "loss": 1.8842,
        "grad_norm": 2.403771162033081,
        "learning_rate": 1.026398763469436e-05,
        "epoch": 0.9519482307586397,
        "step": 6914
    },
    {
        "loss": 2.3913,
        "grad_norm": 2.0282702445983887,
        "learning_rate": 1.0206601096715151e-05,
        "epoch": 0.9520859149111938,
        "step": 6915
    },
    {
        "loss": 2.1157,
        "grad_norm": 2.148151159286499,
        "learning_rate": 1.0149366805982519e-05,
        "epoch": 0.9522235990637478,
        "step": 6916
    },
    {
        "loss": 1.5952,
        "grad_norm": 2.5284159183502197,
        "learning_rate": 1.0092284859538782e-05,
        "epoch": 0.9523612832163018,
        "step": 6917
    },
    {
        "loss": 1.9075,
        "grad_norm": 1.6366583108901978,
        "learning_rate": 1.0035355354167964e-05,
        "epoch": 0.9524989673688559,
        "step": 6918
    },
    {
        "loss": 2.1455,
        "grad_norm": 1.8378831148147583,
        "learning_rate": 9.978578386395676e-06,
        "epoch": 0.9526366515214099,
        "step": 6919
    },
    {
        "loss": 1.3768,
        "grad_norm": 3.2317960262298584,
        "learning_rate": 9.921954052488846e-06,
        "epoch": 0.952774335673964,
        "step": 6920
    },
    {
        "loss": 1.865,
        "grad_norm": 2.2036685943603516,
        "learning_rate": 9.86548244845561e-06,
        "epoch": 0.952912019826518,
        "step": 6921
    },
    {
        "loss": 1.7014,
        "grad_norm": 2.173182725906372,
        "learning_rate": 9.809163670045263e-06,
        "epoch": 0.953049703979072,
        "step": 6922
    },
    {
        "loss": 1.8835,
        "grad_norm": 2.7403934001922607,
        "learning_rate": 9.752997812747833e-06,
        "epoch": 0.9531873881316261,
        "step": 6923
    },
    {
        "loss": 0.8946,
        "grad_norm": 3.6717770099639893,
        "learning_rate": 9.696984971794065e-06,
        "epoch": 0.9533250722841801,
        "step": 6924
    },
    {
        "loss": 1.9157,
        "grad_norm": 1.9115402698516846,
        "learning_rate": 9.641125242155402e-06,
        "epoch": 0.9534627564367342,
        "step": 6925
    },
    {
        "loss": 1.1722,
        "grad_norm": 1.9046826362609863,
        "learning_rate": 9.585418718543581e-06,
        "epoch": 0.9536004405892882,
        "step": 6926
    },
    {
        "loss": 2.4794,
        "grad_norm": 2.0835888385772705,
        "learning_rate": 9.529865495410507e-06,
        "epoch": 0.9537381247418422,
        "step": 6927
    },
    {
        "loss": 1.7129,
        "grad_norm": 2.7187445163726807,
        "learning_rate": 9.474465666948362e-06,
        "epoch": 0.9538758088943963,
        "step": 6928
    },
    {
        "loss": 2.2503,
        "grad_norm": 1.3839768171310425,
        "learning_rate": 9.419219327089069e-06,
        "epoch": 0.9540134930469503,
        "step": 6929
    },
    {
        "loss": 2.1129,
        "grad_norm": 2.8010568618774414,
        "learning_rate": 9.364126569504283e-06,
        "epoch": 0.9541511771995044,
        "step": 6930
    },
    {
        "loss": 1.6258,
        "grad_norm": 4.256551265716553,
        "learning_rate": 9.309187487605441e-06,
        "epoch": 0.9542888613520584,
        "step": 6931
    },
    {
        "loss": 2.0148,
        "grad_norm": 1.6681878566741943,
        "learning_rate": 9.254402174543286e-06,
        "epoch": 0.9544265455046124,
        "step": 6932
    },
    {
        "loss": 2.0357,
        "grad_norm": 2.401428461074829,
        "learning_rate": 9.199770723207846e-06,
        "epoch": 0.9545642296571665,
        "step": 6933
    },
    {
        "loss": 1.7829,
        "grad_norm": 2.376990795135498,
        "learning_rate": 9.145293226228302e-06,
        "epoch": 0.9547019138097205,
        "step": 6934
    },
    {
        "loss": 2.1579,
        "grad_norm": 1.440142035484314,
        "learning_rate": 9.090969775972768e-06,
        "epoch": 0.9548395979622746,
        "step": 6935
    },
    {
        "loss": 1.4899,
        "grad_norm": 1.7077667713165283,
        "learning_rate": 9.036800464548157e-06,
        "epoch": 0.9549772821148286,
        "step": 6936
    },
    {
        "loss": 1.498,
        "grad_norm": 1.7119394540786743,
        "learning_rate": 8.982785383800163e-06,
        "epoch": 0.9551149662673826,
        "step": 6937
    },
    {
        "loss": 1.6692,
        "grad_norm": 3.303520917892456,
        "learning_rate": 8.928924625312818e-06,
        "epoch": 0.9552526504199367,
        "step": 6938
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.5286529064178467,
        "learning_rate": 8.875218280408548e-06,
        "epoch": 0.9553903345724907,
        "step": 6939
    },
    {
        "loss": 1.8236,
        "grad_norm": 2.843414068222046,
        "learning_rate": 8.821666440148058e-06,
        "epoch": 0.9555280187250448,
        "step": 6940
    },
    {
        "loss": 2.356,
        "grad_norm": 2.1413583755493164,
        "learning_rate": 8.768269195329947e-06,
        "epoch": 0.9556657028775988,
        "step": 6941
    },
    {
        "loss": 2.1605,
        "grad_norm": 2.397390127182007,
        "learning_rate": 8.715026636490775e-06,
        "epoch": 0.9558033870301529,
        "step": 6942
    },
    {
        "loss": 2.3348,
        "grad_norm": 1.7110151052474976,
        "learning_rate": 8.661938853904783e-06,
        "epoch": 0.9559410711827069,
        "step": 6943
    },
    {
        "loss": 1.0359,
        "grad_norm": 2.0433948040008545,
        "learning_rate": 8.609005937583826e-06,
        "epoch": 0.9560787553352609,
        "step": 6944
    },
    {
        "loss": 1.1136,
        "grad_norm": 1.690829873085022,
        "learning_rate": 8.556227977277075e-06,
        "epoch": 0.956216439487815,
        "step": 6945
    },
    {
        "loss": 2.1851,
        "grad_norm": 1.8424746990203857,
        "learning_rate": 8.503605062471209e-06,
        "epoch": 0.956354123640369,
        "step": 6946
    },
    {
        "loss": 2.1511,
        "grad_norm": 2.694573163986206,
        "learning_rate": 8.451137282389787e-06,
        "epoch": 0.9564918077929231,
        "step": 6947
    },
    {
        "loss": 2.1354,
        "grad_norm": 2.281336784362793,
        "learning_rate": 8.39882472599336e-06,
        "epoch": 0.9566294919454771,
        "step": 6948
    },
    {
        "loss": 2.1642,
        "grad_norm": 1.479096531867981,
        "learning_rate": 8.346667481979464e-06,
        "epoch": 0.9567671760980311,
        "step": 6949
    },
    {
        "loss": 1.9313,
        "grad_norm": 1.7838784456253052,
        "learning_rate": 8.294665638782118e-06,
        "epoch": 0.9569048602505852,
        "step": 6950
    },
    {
        "loss": 1.8311,
        "grad_norm": 2.9516375064849854,
        "learning_rate": 8.242819284571846e-06,
        "epoch": 0.9570425444031392,
        "step": 6951
    },
    {
        "loss": 2.1035,
        "grad_norm": 1.6367262601852417,
        "learning_rate": 8.191128507255752e-06,
        "epoch": 0.9571802285556933,
        "step": 6952
    },
    {
        "loss": 2.3674,
        "grad_norm": 1.0854870080947876,
        "learning_rate": 8.13959339447693e-06,
        "epoch": 0.9573179127082473,
        "step": 6953
    },
    {
        "loss": 1.8526,
        "grad_norm": 1.5613373517990112,
        "learning_rate": 8.088214033614582e-06,
        "epoch": 0.9574555968608013,
        "step": 6954
    },
    {
        "loss": 1.0958,
        "grad_norm": 1.9323711395263672,
        "learning_rate": 8.03699051178397e-06,
        "epoch": 0.9575932810133554,
        "step": 6955
    },
    {
        "loss": 2.1479,
        "grad_norm": 1.5103031396865845,
        "learning_rate": 7.985922915835952e-06,
        "epoch": 0.9577309651659094,
        "step": 6956
    },
    {
        "loss": 1.9593,
        "grad_norm": 1.8745055198669434,
        "learning_rate": 7.935011332357112e-06,
        "epoch": 0.9578686493184635,
        "step": 6957
    },
    {
        "loss": 1.4443,
        "grad_norm": 2.65163254737854,
        "learning_rate": 7.884255847669474e-06,
        "epoch": 0.9580063334710175,
        "step": 6958
    },
    {
        "loss": 0.2659,
        "grad_norm": 1.4145599603652954,
        "learning_rate": 7.83365654783037e-06,
        "epoch": 0.9581440176235715,
        "step": 6959
    },
    {
        "loss": 0.99,
        "grad_norm": 2.39752459526062,
        "learning_rate": 7.783213518632327e-06,
        "epoch": 0.9582817017761256,
        "step": 6960
    },
    {
        "loss": 2.0735,
        "grad_norm": 1.3625928163528442,
        "learning_rate": 7.732926845602994e-06,
        "epoch": 0.9584193859286796,
        "step": 6961
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.670062780380249,
        "learning_rate": 7.682796614004828e-06,
        "epoch": 0.9585570700812337,
        "step": 6962
    },
    {
        "loss": 1.9004,
        "grad_norm": 1.6308963298797607,
        "learning_rate": 7.632822908834957e-06,
        "epoch": 0.9586947542337877,
        "step": 6963
    },
    {
        "loss": 2.2938,
        "grad_norm": 1.4221923351287842,
        "learning_rate": 7.583005814825373e-06,
        "epoch": 0.9588324383863417,
        "step": 6964
    },
    {
        "loss": 1.6122,
        "grad_norm": 1.7932411432266235,
        "learning_rate": 7.533345416442283e-06,
        "epoch": 0.9589701225388958,
        "step": 6965
    },
    {
        "loss": 1.2255,
        "grad_norm": 2.4198310375213623,
        "learning_rate": 7.483841797886304e-06,
        "epoch": 0.9591078066914498,
        "step": 6966
    },
    {
        "loss": 1.7625,
        "grad_norm": 1.724968433380127,
        "learning_rate": 7.434495043092226e-06,
        "epoch": 0.9592454908440039,
        "step": 6967
    },
    {
        "loss": 1.4693,
        "grad_norm": 2.2668919563293457,
        "learning_rate": 7.3853052357288455e-06,
        "epoch": 0.9593831749965579,
        "step": 6968
    },
    {
        "loss": 1.5255,
        "grad_norm": 3.344857931137085,
        "learning_rate": 7.336272459198823e-06,
        "epoch": 0.9595208591491119,
        "step": 6969
    },
    {
        "loss": 2.271,
        "grad_norm": 1.465864896774292,
        "learning_rate": 7.2873967966387566e-06,
        "epoch": 0.959658543301666,
        "step": 6970
    },
    {
        "loss": 1.577,
        "grad_norm": 2.5389158725738525,
        "learning_rate": 7.238678330918625e-06,
        "epoch": 0.95979622745422,
        "step": 6971
    },
    {
        "loss": 2.3667,
        "grad_norm": 1.3056823015213013,
        "learning_rate": 7.1901171446419836e-06,
        "epoch": 0.9599339116067741,
        "step": 6972
    },
    {
        "loss": 1.3489,
        "grad_norm": 2.687737464904785,
        "learning_rate": 7.141713320145638e-06,
        "epoch": 0.9600715957593281,
        "step": 6973
    },
    {
        "loss": 2.4779,
        "grad_norm": 2.3483619689941406,
        "learning_rate": 7.093466939499738e-06,
        "epoch": 0.9602092799118821,
        "step": 6974
    },
    {
        "loss": 1.9124,
        "grad_norm": 2.6949024200439453,
        "learning_rate": 7.045378084507337e-06,
        "epoch": 0.9603469640644362,
        "step": 6975
    },
    {
        "loss": 2.5217,
        "grad_norm": 1.5473681688308716,
        "learning_rate": 6.997446836704402e-06,
        "epoch": 0.9604846482169902,
        "step": 6976
    },
    {
        "loss": 1.5756,
        "grad_norm": 1.5036917924880981,
        "learning_rate": 6.949673277359814e-06,
        "epoch": 0.9606223323695443,
        "step": 6977
    },
    {
        "loss": 2.0384,
        "grad_norm": 2.1548235416412354,
        "learning_rate": 6.902057487474967e-06,
        "epoch": 0.9607600165220983,
        "step": 6978
    },
    {
        "loss": 2.0653,
        "grad_norm": 2.2411577701568604,
        "learning_rate": 6.8545995477837026e-06,
        "epoch": 0.9608977006746523,
        "step": 6979
    },
    {
        "loss": 1.9864,
        "grad_norm": 2.153351068496704,
        "learning_rate": 6.807299538752399e-06,
        "epoch": 0.9610353848272064,
        "step": 6980
    },
    {
        "loss": 1.8225,
        "grad_norm": 1.538212537765503,
        "learning_rate": 6.760157540579515e-06,
        "epoch": 0.9611730689797604,
        "step": 6981
    },
    {
        "loss": 1.6901,
        "grad_norm": 1.462532639503479,
        "learning_rate": 6.713173633195646e-06,
        "epoch": 0.9613107531323145,
        "step": 6982
    },
    {
        "loss": 2.2212,
        "grad_norm": 1.4842002391815186,
        "learning_rate": 6.666347896263337e-06,
        "epoch": 0.9614484372848685,
        "step": 6983
    },
    {
        "loss": 1.669,
        "grad_norm": 3.6428732872009277,
        "learning_rate": 6.6196804091769535e-06,
        "epoch": 0.9615861214374225,
        "step": 6984
    },
    {
        "loss": 1.3751,
        "grad_norm": 1.9453837871551514,
        "learning_rate": 6.573171251062471e-06,
        "epoch": 0.9617238055899766,
        "step": 6985
    },
    {
        "loss": 1.3978,
        "grad_norm": 2.7320423126220703,
        "learning_rate": 6.526820500777587e-06,
        "epoch": 0.9618614897425306,
        "step": 6986
    },
    {
        "loss": 1.8507,
        "grad_norm": 1.8798454999923706,
        "learning_rate": 6.480628236911268e-06,
        "epoch": 0.9619991738950847,
        "step": 6987
    },
    {
        "loss": 1.5473,
        "grad_norm": 2.717538833618164,
        "learning_rate": 6.4345945377837335e-06,
        "epoch": 0.9621368580476387,
        "step": 6988
    },
    {
        "loss": 2.1032,
        "grad_norm": 1.8890193700790405,
        "learning_rate": 6.388719481446559e-06,
        "epoch": 0.9622745422001927,
        "step": 6989
    },
    {
        "loss": 1.6686,
        "grad_norm": 1.7369344234466553,
        "learning_rate": 6.343003145682114e-06,
        "epoch": 0.9624122263527468,
        "step": 6990
    },
    {
        "loss": 1.2808,
        "grad_norm": 2.172102451324463,
        "learning_rate": 6.29744560800376e-06,
        "epoch": 0.9625499105053008,
        "step": 6991
    },
    {
        "loss": 1.749,
        "grad_norm": 2.14449143409729,
        "learning_rate": 6.252046945655576e-06,
        "epoch": 0.9626875946578549,
        "step": 6992
    },
    {
        "loss": 2.3553,
        "grad_norm": 2.5089809894561768,
        "learning_rate": 6.2068072356122885e-06,
        "epoch": 0.9628252788104089,
        "step": 6993
    },
    {
        "loss": 1.8657,
        "grad_norm": 1.5607454776763916,
        "learning_rate": 6.1617265545790285e-06,
        "epoch": 0.9629629629629629,
        "step": 6994
    },
    {
        "loss": 2.0961,
        "grad_norm": 2.4690768718719482,
        "learning_rate": 6.1168049789915085e-06,
        "epoch": 0.963100647115517,
        "step": 6995
    },
    {
        "loss": 2.5136,
        "grad_norm": 1.4614720344543457,
        "learning_rate": 6.072042585015448e-06,
        "epoch": 0.963238331268071,
        "step": 6996
    },
    {
        "loss": 2.2966,
        "grad_norm": 2.6787898540496826,
        "learning_rate": 6.027439448546723e-06,
        "epoch": 0.9633760154206251,
        "step": 6997
    },
    {
        "loss": 0.8658,
        "grad_norm": 2.001877784729004,
        "learning_rate": 5.982995645211298e-06,
        "epoch": 0.9635136995731791,
        "step": 6998
    },
    {
        "loss": 1.8391,
        "grad_norm": 2.208665609359741,
        "learning_rate": 5.938711250364848e-06,
        "epoch": 0.9636513837257331,
        "step": 6999
    },
    {
        "loss": 1.2397,
        "grad_norm": 2.8409194946289062,
        "learning_rate": 5.894586339092756e-06,
        "epoch": 0.9637890678782872,
        "step": 7000
    },
    {
        "loss": 2.4268,
        "grad_norm": 1.9242148399353027,
        "learning_rate": 5.850620986210187e-06,
        "epoch": 0.9639267520308412,
        "step": 7001
    },
    {
        "loss": 2.1815,
        "grad_norm": 2.6892874240875244,
        "learning_rate": 5.806815266261567e-06,
        "epoch": 0.9640644361833953,
        "step": 7002
    },
    {
        "loss": 2.1804,
        "grad_norm": 1.4927871227264404,
        "learning_rate": 5.763169253520695e-06,
        "epoch": 0.9642021203359493,
        "step": 7003
    },
    {
        "loss": 1.9306,
        "grad_norm": 1.5724345445632935,
        "learning_rate": 5.719683021990707e-06,
        "epoch": 0.9643398044885034,
        "step": 7004
    },
    {
        "loss": 2.1712,
        "grad_norm": 2.390089511871338,
        "learning_rate": 5.6763566454036905e-06,
        "epoch": 0.9644774886410574,
        "step": 7005
    },
    {
        "loss": 1.5699,
        "grad_norm": 2.3128697872161865,
        "learning_rate": 5.633190197220761e-06,
        "epoch": 0.9646151727936114,
        "step": 7006
    },
    {
        "loss": 1.4542,
        "grad_norm": 2.4950201511383057,
        "learning_rate": 5.590183750631805e-06,
        "epoch": 0.9647528569461655,
        "step": 7007
    },
    {
        "loss": 1.9066,
        "grad_norm": 2.139261245727539,
        "learning_rate": 5.547337378555517e-06,
        "epoch": 0.9648905410987195,
        "step": 7008
    },
    {
        "loss": 0.9168,
        "grad_norm": 1.1974847316741943,
        "learning_rate": 5.504651153639073e-06,
        "epoch": 0.9650282252512736,
        "step": 7009
    },
    {
        "loss": 1.2912,
        "grad_norm": 1.917349934577942,
        "learning_rate": 5.462125148258268e-06,
        "epoch": 0.9651659094038276,
        "step": 7010
    },
    {
        "loss": 1.6599,
        "grad_norm": 1.697023868560791,
        "learning_rate": 5.419759434517091e-06,
        "epoch": 0.9653035935563816,
        "step": 7011
    },
    {
        "loss": 1.475,
        "grad_norm": 3.4276161193847656,
        "learning_rate": 5.377554084247749e-06,
        "epoch": 0.9654412777089357,
        "step": 7012
    },
    {
        "loss": 2.2974,
        "grad_norm": 1.8296518325805664,
        "learning_rate": 5.3355091690107214e-06,
        "epoch": 0.9655789618614897,
        "step": 7013
    },
    {
        "loss": 2.2658,
        "grad_norm": 2.573591947555542,
        "learning_rate": 5.293624760094307e-06,
        "epoch": 0.9657166460140438,
        "step": 7014
    },
    {
        "loss": 1.1869,
        "grad_norm": 2.7415831089019775,
        "learning_rate": 5.251900928514697e-06,
        "epoch": 0.9658543301665978,
        "step": 7015
    },
    {
        "loss": 2.6131,
        "grad_norm": 1.3241342306137085,
        "learning_rate": 5.210337745015825e-06,
        "epoch": 0.9659920143191518,
        "step": 7016
    },
    {
        "loss": 1.8284,
        "grad_norm": 3.13089656829834,
        "learning_rate": 5.168935280069242e-06,
        "epoch": 0.9661296984717059,
        "step": 7017
    },
    {
        "loss": 2.0695,
        "grad_norm": 1.5633348226547241,
        "learning_rate": 5.127693603873962e-06,
        "epoch": 0.9662673826242599,
        "step": 7018
    },
    {
        "loss": 1.88,
        "grad_norm": 2.7164721488952637,
        "learning_rate": 5.086612786356482e-06,
        "epoch": 0.966405066776814,
        "step": 7019
    },
    {
        "loss": 2.0804,
        "grad_norm": 3.017627239227295,
        "learning_rate": 5.045692897170473e-06,
        "epoch": 0.966542750929368,
        "step": 7020
    },
    {
        "loss": 0.8602,
        "grad_norm": 2.785834312438965,
        "learning_rate": 5.004934005696716e-06,
        "epoch": 0.966680435081922,
        "step": 7021
    },
    {
        "loss": 2.0042,
        "grad_norm": 1.1117368936538696,
        "learning_rate": 4.9643361810431274e-06,
        "epoch": 0.9668181192344761,
        "step": 7022
    },
    {
        "loss": 1.9347,
        "grad_norm": 3.2489850521087646,
        "learning_rate": 4.923899492044448e-06,
        "epoch": 0.9669558033870301,
        "step": 7023
    },
    {
        "loss": 1.8962,
        "grad_norm": 3.2916290760040283,
        "learning_rate": 4.8836240072621665e-06,
        "epoch": 0.9670934875395842,
        "step": 7024
    },
    {
        "loss": 2.0422,
        "grad_norm": 2.8902361392974854,
        "learning_rate": 4.843509794984591e-06,
        "epoch": 0.9672311716921382,
        "step": 7025
    },
    {
        "loss": 2.099,
        "grad_norm": 1.537645697593689,
        "learning_rate": 4.803556923226482e-06,
        "epoch": 0.9673688558446922,
        "step": 7026
    },
    {
        "loss": 2.1763,
        "grad_norm": 1.6339205503463745,
        "learning_rate": 4.763765459729008e-06,
        "epoch": 0.9675065399972463,
        "step": 7027
    },
    {
        "loss": 2.168,
        "grad_norm": 2.509673595428467,
        "learning_rate": 4.724135471959812e-06,
        "epoch": 0.9676442241498003,
        "step": 7028
    },
    {
        "loss": 1.7585,
        "grad_norm": 2.683471202850342,
        "learning_rate": 4.684667027112599e-06,
        "epoch": 0.9677819083023544,
        "step": 7029
    },
    {
        "loss": 1.9286,
        "grad_norm": 2.606084108352661,
        "learning_rate": 4.6453601921072395e-06,
        "epoch": 0.9679195924549084,
        "step": 7030
    },
    {
        "loss": 0.9691,
        "grad_norm": 2.458188772201538,
        "learning_rate": 4.606215033589589e-06,
        "epoch": 0.9680572766074624,
        "step": 7031
    },
    {
        "loss": 1.4782,
        "grad_norm": 1.8447470664978027,
        "learning_rate": 4.567231617931367e-06,
        "epoch": 0.9681949607600165,
        "step": 7032
    },
    {
        "loss": 1.4533,
        "grad_norm": 2.368422269821167,
        "learning_rate": 4.528410011230022e-06,
        "epoch": 0.9683326449125705,
        "step": 7033
    },
    {
        "loss": 1.0691,
        "grad_norm": 2.22918438911438,
        "learning_rate": 4.489750279308757e-06,
        "epoch": 0.9684703290651246,
        "step": 7034
    },
    {
        "loss": 2.3265,
        "grad_norm": 1.3257862329483032,
        "learning_rate": 4.451252487716206e-06,
        "epoch": 0.9686080132176786,
        "step": 7035
    },
    {
        "loss": 1.5343,
        "grad_norm": 2.7383790016174316,
        "learning_rate": 4.4129167017264324e-06,
        "epoch": 0.9687456973702326,
        "step": 7036
    },
    {
        "loss": 2.1758,
        "grad_norm": 1.9770333766937256,
        "learning_rate": 4.3747429863388976e-06,
        "epoch": 0.9688833815227867,
        "step": 7037
    },
    {
        "loss": 1.8493,
        "grad_norm": 2.5971662998199463,
        "learning_rate": 4.336731406278205e-06,
        "epoch": 0.9690210656753407,
        "step": 7038
    },
    {
        "loss": 1.7693,
        "grad_norm": 1.4640748500823975,
        "learning_rate": 4.298882025994056e-06,
        "epoch": 0.9691587498278948,
        "step": 7039
    },
    {
        "loss": 2.1055,
        "grad_norm": 2.983274459838867,
        "learning_rate": 4.261194909661137e-06,
        "epoch": 0.9692964339804488,
        "step": 7040
    },
    {
        "loss": 2.1369,
        "grad_norm": 2.5334548950195312,
        "learning_rate": 4.223670121179013e-06,
        "epoch": 0.9694341181330028,
        "step": 7041
    },
    {
        "loss": 1.3929,
        "grad_norm": 2.8745200634002686,
        "learning_rate": 4.186307724172012e-06,
        "epoch": 0.969571802285557,
        "step": 7042
    },
    {
        "loss": 1.92,
        "grad_norm": 1.5336363315582275,
        "learning_rate": 4.149107781989181e-06,
        "epoch": 0.969709486438111,
        "step": 7043
    },
    {
        "loss": 1.8488,
        "grad_norm": 1.4779421091079712,
        "learning_rate": 4.112070357704045e-06,
        "epoch": 0.969847170590665,
        "step": 7044
    },
    {
        "loss": 1.7184,
        "grad_norm": 2.410230875015259,
        "learning_rate": 4.075195514114571e-06,
        "epoch": 0.969984854743219,
        "step": 7045
    },
    {
        "loss": 1.9607,
        "grad_norm": 1.29770827293396,
        "learning_rate": 4.038483313743191e-06,
        "epoch": 0.970122538895773,
        "step": 7046
    },
    {
        "loss": 1.47,
        "grad_norm": 2.6338093280792236,
        "learning_rate": 4.001933818836412e-06,
        "epoch": 0.9702602230483272,
        "step": 7047
    },
    {
        "loss": 2.2009,
        "grad_norm": 2.4915244579315186,
        "learning_rate": 3.965547091364907e-06,
        "epoch": 0.9703979072008811,
        "step": 7048
    },
    {
        "loss": 1.1981,
        "grad_norm": 3.2002387046813965,
        "learning_rate": 3.929323193023482e-06,
        "epoch": 0.9705355913534353,
        "step": 7049
    },
    {
        "loss": 2.2135,
        "grad_norm": 1.550718903541565,
        "learning_rate": 3.893262185230728e-06,
        "epoch": 0.9706732755059893,
        "step": 7050
    },
    {
        "loss": 2.0915,
        "grad_norm": 1.2529772520065308,
        "learning_rate": 3.857364129129093e-06,
        "epoch": 0.9708109596585432,
        "step": 7051
    },
    {
        "loss": 1.9605,
        "grad_norm": 1.4610551595687866,
        "learning_rate": 3.8216290855847995e-06,
        "epoch": 0.9709486438110974,
        "step": 7052
    },
    {
        "loss": 2.3316,
        "grad_norm": 1.8757438659667969,
        "learning_rate": 3.786057115187591e-06,
        "epoch": 0.9710863279636514,
        "step": 7053
    },
    {
        "loss": 1.9101,
        "grad_norm": 1.741836667060852,
        "learning_rate": 3.7506482782507566e-06,
        "epoch": 0.9712240121162055,
        "step": 7054
    },
    {
        "loss": 2.1866,
        "grad_norm": 1.7058539390563965,
        "learning_rate": 3.7154026348109714e-06,
        "epoch": 0.9713616962687595,
        "step": 7055
    },
    {
        "loss": 1.6444,
        "grad_norm": 1.9596699476242065,
        "learning_rate": 3.6803202446282324e-06,
        "epoch": 0.9714993804213135,
        "step": 7056
    },
    {
        "loss": 2.0133,
        "grad_norm": 2.2893033027648926,
        "learning_rate": 3.645401167185691e-06,
        "epoch": 0.9716370645738676,
        "step": 7057
    },
    {
        "loss": 0.7112,
        "grad_norm": 2.5808918476104736,
        "learning_rate": 3.6106454616896435e-06,
        "epoch": 0.9717747487264216,
        "step": 7058
    },
    {
        "loss": 0.6617,
        "grad_norm": 1.2329813241958618,
        "learning_rate": 3.576053187069406e-06,
        "epoch": 0.9719124328789757,
        "step": 7059
    },
    {
        "loss": 1.9601,
        "grad_norm": 2.356250047683716,
        "learning_rate": 3.5416244019771393e-06,
        "epoch": 0.9720501170315297,
        "step": 7060
    },
    {
        "loss": 1.6859,
        "grad_norm": 3.249715566635132,
        "learning_rate": 3.5073591647877823e-06,
        "epoch": 0.9721878011840838,
        "step": 7061
    },
    {
        "loss": 1.5312,
        "grad_norm": 2.632894515991211,
        "learning_rate": 3.4732575335990834e-06,
        "epoch": 0.9723254853366378,
        "step": 7062
    },
    {
        "loss": 1.8961,
        "grad_norm": 1.4789907932281494,
        "learning_rate": 3.4393195662312805e-06,
        "epoch": 0.9724631694891918,
        "step": 7063
    },
    {
        "loss": 1.5938,
        "grad_norm": 2.4626595973968506,
        "learning_rate": 3.4055453202271657e-06,
        "epoch": 0.9726008536417459,
        "step": 7064
    },
    {
        "loss": 2.3692,
        "grad_norm": 2.240100145339966,
        "learning_rate": 3.3719348528519323e-06,
        "epoch": 0.9727385377942999,
        "step": 7065
    },
    {
        "loss": 2.2091,
        "grad_norm": 1.5991332530975342,
        "learning_rate": 3.3384882210930612e-06,
        "epoch": 0.972876221946854,
        "step": 7066
    },
    {
        "loss": 1.9131,
        "grad_norm": 2.4372830390930176,
        "learning_rate": 3.3052054816602452e-06,
        "epoch": 0.973013906099408,
        "step": 7067
    },
    {
        "loss": 1.0968,
        "grad_norm": 2.1810343265533447,
        "learning_rate": 3.2720866909853765e-06,
        "epoch": 0.973151590251962,
        "step": 7068
    },
    {
        "loss": 1.8825,
        "grad_norm": 1.4388816356658936,
        "learning_rate": 3.239131905222248e-06,
        "epoch": 0.9732892744045161,
        "step": 7069
    },
    {
        "loss": 2.6454,
        "grad_norm": 1.4340640306472778,
        "learning_rate": 3.206341180246619e-06,
        "epoch": 0.9734269585570701,
        "step": 7070
    },
    {
        "loss": 1.6588,
        "grad_norm": 2.4823315143585205,
        "learning_rate": 3.17371457165615e-06,
        "epoch": 0.9735646427096242,
        "step": 7071
    },
    {
        "loss": 1.7284,
        "grad_norm": 2.4898128509521484,
        "learning_rate": 3.1412521347701453e-06,
        "epoch": 0.9737023268621782,
        "step": 7072
    },
    {
        "loss": 1.7001,
        "grad_norm": 3.510637044906616,
        "learning_rate": 3.1089539246295428e-06,
        "epoch": 0.9738400110147322,
        "step": 7073
    },
    {
        "loss": 1.47,
        "grad_norm": 1.4591548442840576,
        "learning_rate": 3.07681999599696e-06,
        "epoch": 0.9739776951672863,
        "step": 7074
    },
    {
        "loss": 2.0433,
        "grad_norm": 1.5625382661819458,
        "learning_rate": 3.044850403356314e-06,
        "epoch": 0.9741153793198403,
        "step": 7075
    },
    {
        "loss": 1.5965,
        "grad_norm": 2.1125564575195312,
        "learning_rate": 3.013045200912945e-06,
        "epoch": 0.9742530634723944,
        "step": 7076
    },
    {
        "loss": 1.6624,
        "grad_norm": 3.2039103507995605,
        "learning_rate": 2.981404442593538e-06,
        "epoch": 0.9743907476249484,
        "step": 7077
    },
    {
        "loss": 2.3356,
        "grad_norm": 2.0882039070129395,
        "learning_rate": 2.949928182045858e-06,
        "epoch": 0.9745284317775024,
        "step": 7078
    },
    {
        "loss": 1.3231,
        "grad_norm": 3.401613473892212,
        "learning_rate": 2.918616472638791e-06,
        "epoch": 0.9746661159300565,
        "step": 7079
    },
    {
        "loss": 2.4578,
        "grad_norm": 1.3665281534194946,
        "learning_rate": 2.8874693674622257e-06,
        "epoch": 0.9748038000826105,
        "step": 7080
    },
    {
        "loss": 1.4945,
        "grad_norm": 3.4560351371765137,
        "learning_rate": 2.8564869193269507e-06,
        "epoch": 0.9749414842351646,
        "step": 7081
    },
    {
        "loss": 2.4948,
        "grad_norm": 2.798366069793701,
        "learning_rate": 2.8256691807645674e-06,
        "epoch": 0.9750791683877186,
        "step": 7082
    },
    {
        "loss": 2.4958,
        "grad_norm": 1.643062949180603,
        "learning_rate": 2.7950162040274784e-06,
        "epoch": 0.9752168525402726,
        "step": 7083
    },
    {
        "loss": 1.5751,
        "grad_norm": 2.965224027633667,
        "learning_rate": 2.764528041088643e-06,
        "epoch": 0.9753545366928267,
        "step": 7084
    },
    {
        "loss": 1.974,
        "grad_norm": 3.4673287868499756,
        "learning_rate": 2.7342047436415774e-06,
        "epoch": 0.9754922208453807,
        "step": 7085
    },
    {
        "loss": 2.5504,
        "grad_norm": 1.387834906578064,
        "learning_rate": 2.7040463631003433e-06,
        "epoch": 0.9756299049979348,
        "step": 7086
    },
    {
        "loss": 1.5383,
        "grad_norm": 2.5540566444396973,
        "learning_rate": 2.674052950599293e-06,
        "epoch": 0.9757675891504888,
        "step": 7087
    },
    {
        "loss": 2.037,
        "grad_norm": 1.9866327047348022,
        "learning_rate": 2.6442245569931133e-06,
        "epoch": 0.9759052733030428,
        "step": 7088
    },
    {
        "loss": 1.6946,
        "grad_norm": 3.1295602321624756,
        "learning_rate": 2.6145612328566827e-06,
        "epoch": 0.9760429574555969,
        "step": 7089
    },
    {
        "loss": 1.8019,
        "grad_norm": 2.824153184890747,
        "learning_rate": 2.585063028485013e-06,
        "epoch": 0.9761806416081509,
        "step": 7090
    },
    {
        "loss": 1.8664,
        "grad_norm": 2.0604360103607178,
        "learning_rate": 2.555729993893119e-06,
        "epoch": 0.976318325760705,
        "step": 7091
    },
    {
        "loss": 1.5702,
        "grad_norm": 2.3563625812530518,
        "learning_rate": 2.5265621788160386e-06,
        "epoch": 0.976456009913259,
        "step": 7092
    },
    {
        "loss": 2.041,
        "grad_norm": 1.5267248153686523,
        "learning_rate": 2.4975596327086016e-06,
        "epoch": 0.976593694065813,
        "step": 7093
    },
    {
        "loss": 1.7041,
        "grad_norm": 2.7938551902770996,
        "learning_rate": 2.468722404745405e-06,
        "epoch": 0.9767313782183671,
        "step": 7094
    },
    {
        "loss": 2.0928,
        "grad_norm": 1.5294691324234009,
        "learning_rate": 2.4400505438208486e-06,
        "epoch": 0.9768690623709211,
        "step": 7095
    },
    {
        "loss": 2.258,
        "grad_norm": 1.601797342300415,
        "learning_rate": 2.411544098548868e-06,
        "epoch": 0.9770067465234752,
        "step": 7096
    },
    {
        "loss": 1.4025,
        "grad_norm": 1.5629385709762573,
        "learning_rate": 2.383203117262911e-06,
        "epoch": 0.9771444306760292,
        "step": 7097
    },
    {
        "loss": 0.5825,
        "grad_norm": 1.828012466430664,
        "learning_rate": 2.3550276480159617e-06,
        "epoch": 0.9772821148285832,
        "step": 7098
    },
    {
        "loss": 1.3502,
        "grad_norm": 3.50597882270813,
        "learning_rate": 2.327017738580306e-06,
        "epoch": 0.9774197989811373,
        "step": 7099
    },
    {
        "loss": 2.7345,
        "grad_norm": 1.394228458404541,
        "learning_rate": 2.299173436447488e-06,
        "epoch": 0.9775574831336913,
        "step": 7100
    },
    {
        "loss": 0.8851,
        "grad_norm": 2.1142094135284424,
        "learning_rate": 2.271494788828399e-06,
        "epoch": 0.9776951672862454,
        "step": 7101
    },
    {
        "loss": 2.1777,
        "grad_norm": 2.2315866947174072,
        "learning_rate": 2.24398184265292e-06,
        "epoch": 0.9778328514387994,
        "step": 7102
    },
    {
        "loss": 1.7264,
        "grad_norm": 2.266526699066162,
        "learning_rate": 2.216634644570026e-06,
        "epoch": 0.9779705355913534,
        "step": 7103
    },
    {
        "loss": 1.9157,
        "grad_norm": 1.8466168642044067,
        "learning_rate": 2.189453240947681e-06,
        "epoch": 0.9781082197439075,
        "step": 7104
    },
    {
        "loss": 2.0914,
        "grad_norm": 2.222811460494995,
        "learning_rate": 2.1624376778727087e-06,
        "epoch": 0.9782459038964615,
        "step": 7105
    },
    {
        "loss": 1.1148,
        "grad_norm": 2.8476579189300537,
        "learning_rate": 2.1355880011507235e-06,
        "epoch": 0.9783835880490156,
        "step": 7106
    },
    {
        "loss": 2.5583,
        "grad_norm": 1.8712712526321411,
        "learning_rate": 2.108904256306199e-06,
        "epoch": 0.9785212722015696,
        "step": 7107
    },
    {
        "loss": 2.1281,
        "grad_norm": 1.9046558141708374,
        "learning_rate": 2.082386488582122e-06,
        "epoch": 0.9786589563541236,
        "step": 7108
    },
    {
        "loss": 2.0078,
        "grad_norm": 1.466472864151001,
        "learning_rate": 2.056034742940105e-06,
        "epoch": 0.9787966405066777,
        "step": 7109
    },
    {
        "loss": 1.323,
        "grad_norm": 3.0145723819732666,
        "learning_rate": 2.0298490640603296e-06,
        "epoch": 0.9789343246592317,
        "step": 7110
    },
    {
        "loss": 2.205,
        "grad_norm": 2.11466383934021,
        "learning_rate": 2.003829496341325e-06,
        "epoch": 0.9790720088117858,
        "step": 7111
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.2018554210662842,
        "learning_rate": 1.977976083900002e-06,
        "epoch": 0.9792096929643398,
        "step": 7112
    },
    {
        "loss": 1.6041,
        "grad_norm": 2.3663103580474854,
        "learning_rate": 1.9522888705715503e-06,
        "epoch": 0.9793473771168938,
        "step": 7113
    },
    {
        "loss": 1.742,
        "grad_norm": 1.8129711151123047,
        "learning_rate": 1.9267678999093763e-06,
        "epoch": 0.9794850612694479,
        "step": 7114
    },
    {
        "loss": 2.026,
        "grad_norm": 1.5057921409606934,
        "learning_rate": 1.9014132151849773e-06,
        "epoch": 0.9796227454220019,
        "step": 7115
    },
    {
        "loss": 1.5794,
        "grad_norm": 4.353881359100342,
        "learning_rate": 1.8762248593879873e-06,
        "epoch": 0.979760429574556,
        "step": 7116
    },
    {
        "loss": 2.1554,
        "grad_norm": 1.655988097190857,
        "learning_rate": 1.8512028752259436e-06,
        "epoch": 0.97989811372711,
        "step": 7117
    },
    {
        "loss": 1.8139,
        "grad_norm": 1.6441011428833008,
        "learning_rate": 1.8263473051243207e-06,
        "epoch": 0.9800357978796641,
        "step": 7118
    },
    {
        "loss": 1.6103,
        "grad_norm": 1.2099964618682861,
        "learning_rate": 1.801658191226474e-06,
        "epoch": 0.9801734820322181,
        "step": 7119
    },
    {
        "loss": 1.8927,
        "grad_norm": 2.8862321376800537,
        "learning_rate": 1.7771355753934738e-06,
        "epoch": 0.9803111661847721,
        "step": 7120
    },
    {
        "loss": 2.1786,
        "grad_norm": 2.1235768795013428,
        "learning_rate": 1.7527794992040935e-06,
        "epoch": 0.9804488503373262,
        "step": 7121
    },
    {
        "loss": 2.2235,
        "grad_norm": 3.253727674484253,
        "learning_rate": 1.7285900039547886e-06,
        "epoch": 0.9805865344898802,
        "step": 7122
    },
    {
        "loss": 1.9968,
        "grad_norm": 2.5122296810150146,
        "learning_rate": 1.7045671306595068e-06,
        "epoch": 0.9807242186424343,
        "step": 7123
    },
    {
        "loss": 1.8209,
        "grad_norm": 2.765467405319214,
        "learning_rate": 1.6807109200496773e-06,
        "epoch": 0.9808619027949883,
        "step": 7124
    },
    {
        "loss": 1.4402,
        "grad_norm": 1.4223783016204834,
        "learning_rate": 1.6570214125742557e-06,
        "epoch": 0.9809995869475423,
        "step": 7125
    },
    {
        "loss": 1.6811,
        "grad_norm": 2.9907357692718506,
        "learning_rate": 1.633498648399412e-06,
        "epoch": 0.9811372711000964,
        "step": 7126
    },
    {
        "loss": 1.9214,
        "grad_norm": 1.2891573905944824,
        "learning_rate": 1.6101426674086872e-06,
        "epoch": 0.9812749552526504,
        "step": 7127
    },
    {
        "loss": 1.7099,
        "grad_norm": 5.010601997375488,
        "learning_rate": 1.5869535092027933e-06,
        "epoch": 0.9814126394052045,
        "step": 7128
    },
    {
        "loss": 1.8393,
        "grad_norm": 2.2982587814331055,
        "learning_rate": 1.563931213099612e-06,
        "epoch": 0.9815503235577585,
        "step": 7129
    },
    {
        "loss": 1.9648,
        "grad_norm": 1.7849229574203491,
        "learning_rate": 1.5410758181340634e-06,
        "epoch": 0.9816880077103125,
        "step": 7130
    },
    {
        "loss": 2.3193,
        "grad_norm": 1.605682611465454,
        "learning_rate": 1.5183873630581492e-06,
        "epoch": 0.9818256918628666,
        "step": 7131
    },
    {
        "loss": 2.1723,
        "grad_norm": 1.469681978225708,
        "learning_rate": 1.495865886340797e-06,
        "epoch": 0.9819633760154206,
        "step": 7132
    },
    {
        "loss": 0.6073,
        "grad_norm": 1.2307034730911255,
        "learning_rate": 1.473511426167773e-06,
        "epoch": 0.9821010601679747,
        "step": 7133
    },
    {
        "loss": 2.1829,
        "grad_norm": 1.3887068033218384,
        "learning_rate": 1.4513240204417356e-06,
        "epoch": 0.9822387443205287,
        "step": 7134
    },
    {
        "loss": 1.8029,
        "grad_norm": 1.7997734546661377,
        "learning_rate": 1.4293037067820258e-06,
        "epoch": 0.9823764284730827,
        "step": 7135
    },
    {
        "loss": 1.935,
        "grad_norm": 1.9968458414077759,
        "learning_rate": 1.4074505225247337e-06,
        "epoch": 0.9825141126256368,
        "step": 7136
    },
    {
        "loss": 1.5556,
        "grad_norm": 1.2924258708953857,
        "learning_rate": 1.3857645047225309e-06,
        "epoch": 0.9826517967781908,
        "step": 7137
    },
    {
        "loss": 2.0286,
        "grad_norm": 1.873576283454895,
        "learning_rate": 1.364245690144672e-06,
        "epoch": 0.9827894809307449,
        "step": 7138
    },
    {
        "loss": 1.6801,
        "grad_norm": 1.694469690322876,
        "learning_rate": 1.342894115276927e-06,
        "epoch": 0.9829271650832989,
        "step": 7139
    },
    {
        "loss": 1.5361,
        "grad_norm": 2.3773012161254883,
        "learning_rate": 1.3217098163214814e-06,
        "epoch": 0.9830648492358529,
        "step": 7140
    },
    {
        "loss": 0.9729,
        "grad_norm": 1.4119312763214111,
        "learning_rate": 1.3006928291969367e-06,
        "epoch": 0.983202533388407,
        "step": 7141
    },
    {
        "loss": 2.3669,
        "grad_norm": 1.886810541152954,
        "learning_rate": 1.2798431895381769e-06,
        "epoch": 0.983340217540961,
        "step": 7142
    },
    {
        "loss": 2.0821,
        "grad_norm": 3.493474006652832,
        "learning_rate": 1.2591609326963239e-06,
        "epoch": 0.9834779016935151,
        "step": 7143
    },
    {
        "loss": 2.0205,
        "grad_norm": 1.3641287088394165,
        "learning_rate": 1.2386460937387822e-06,
        "epoch": 0.9836155858460691,
        "step": 7144
    },
    {
        "loss": 0.8103,
        "grad_norm": 2.394681215286255,
        "learning_rate": 1.2182987074490172e-06,
        "epoch": 0.9837532699986231,
        "step": 7145
    },
    {
        "loss": 2.7603,
        "grad_norm": 1.8069854974746704,
        "learning_rate": 1.1981188083265872e-06,
        "epoch": 0.9838909541511772,
        "step": 7146
    },
    {
        "loss": 2.0567,
        "grad_norm": 1.5880111455917358,
        "learning_rate": 1.1781064305871004e-06,
        "epoch": 0.9840286383037312,
        "step": 7147
    },
    {
        "loss": 1.9446,
        "grad_norm": 2.985539197921753,
        "learning_rate": 1.1582616081621145e-06,
        "epoch": 0.9841663224562853,
        "step": 7148
    },
    {
        "loss": 2.1321,
        "grad_norm": 2.0283522605895996,
        "learning_rate": 1.1385843746990367e-06,
        "epoch": 0.9843040066088393,
        "step": 7149
    },
    {
        "loss": 1.6268,
        "grad_norm": 1.6037870645523071,
        "learning_rate": 1.1190747635612341e-06,
        "epoch": 0.9844416907613933,
        "step": 7150
    },
    {
        "loss": 1.385,
        "grad_norm": 3.233307361602783,
        "learning_rate": 1.0997328078277913e-06,
        "epoch": 0.9845793749139474,
        "step": 7151
    },
    {
        "loss": 1.7184,
        "grad_norm": 2.4182522296905518,
        "learning_rate": 1.0805585402935192e-06,
        "epoch": 0.9847170590665014,
        "step": 7152
    },
    {
        "loss": 2.001,
        "grad_norm": 2.174805164337158,
        "learning_rate": 1.0615519934689456e-06,
        "epoch": 0.9848547432190555,
        "step": 7153
    },
    {
        "loss": 1.9241,
        "grad_norm": 2.030691146850586,
        "learning_rate": 1.042713199580203e-06,
        "epoch": 0.9849924273716095,
        "step": 7154
    },
    {
        "loss": 2.1793,
        "grad_norm": 1.6264344453811646,
        "learning_rate": 1.0240421905689745e-06,
        "epoch": 0.9851301115241635,
        "step": 7155
    },
    {
        "loss": 0.9175,
        "grad_norm": 1.9225659370422363,
        "learning_rate": 1.0055389980925256e-06,
        "epoch": 0.9852677956767176,
        "step": 7156
    },
    {
        "loss": 2.1981,
        "grad_norm": 2.129715919494629,
        "learning_rate": 9.872036535235274e-07,
        "epoch": 0.9854054798292716,
        "step": 7157
    },
    {
        "loss": 1.4035,
        "grad_norm": 1.553769588470459,
        "learning_rate": 9.690361879500565e-07,
        "epoch": 0.9855431639818257,
        "step": 7158
    },
    {
        "loss": 1.6248,
        "grad_norm": 2.137765884399414,
        "learning_rate": 9.510366321755948e-07,
        "epoch": 0.9856808481343797,
        "step": 7159
    },
    {
        "loss": 2.5552,
        "grad_norm": 1.6718573570251465,
        "learning_rate": 9.332050167188966e-07,
        "epoch": 0.9858185322869337,
        "step": 7160
    },
    {
        "loss": 1.1198,
        "grad_norm": 1.6686358451843262,
        "learning_rate": 9.155413718139438e-07,
        "epoch": 0.9859562164394878,
        "step": 7161
    },
    {
        "loss": 2.0875,
        "grad_norm": 1.6075469255447388,
        "learning_rate": 8.980457274099574e-07,
        "epoch": 0.9860939005920418,
        "step": 7162
    },
    {
        "loss": 1.7546,
        "grad_norm": 2.5244243144989014,
        "learning_rate": 8.807181131712971e-07,
        "epoch": 0.9862315847445959,
        "step": 7163
    },
    {
        "loss": 0.8991,
        "grad_norm": 2.6652822494506836,
        "learning_rate": 8.635585584774175e-07,
        "epoch": 0.9863692688971499,
        "step": 7164
    },
    {
        "loss": 2.0092,
        "grad_norm": 1.6414053440093994,
        "learning_rate": 8.465670924228452e-07,
        "epoch": 0.9865069530497039,
        "step": 7165
    },
    {
        "loss": 2.6687,
        "grad_norm": 2.369241952896118,
        "learning_rate": 8.297437438170908e-07,
        "epoch": 0.986644637202258,
        "step": 7166
    },
    {
        "loss": 1.5982,
        "grad_norm": 2.3697404861450195,
        "learning_rate": 8.130885411845812e-07,
        "epoch": 0.986782321354812,
        "step": 7167
    },
    {
        "loss": 2.0512,
        "grad_norm": 2.873086929321289,
        "learning_rate": 7.966015127647386e-07,
        "epoch": 0.9869200055073661,
        "step": 7168
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.8434120416641235,
        "learning_rate": 7.802826865117463e-07,
        "epoch": 0.9870576896599201,
        "step": 7169
    },
    {
        "loss": 1.3293,
        "grad_norm": 2.020580768585205,
        "learning_rate": 7.64132090094627e-07,
        "epoch": 0.9871953738124741,
        "step": 7170
    },
    {
        "loss": 1.4891,
        "grad_norm": 2.0735907554626465,
        "learning_rate": 7.481497508972202e-07,
        "epoch": 0.9873330579650282,
        "step": 7171
    },
    {
        "loss": 2.2705,
        "grad_norm": 1.4128344058990479,
        "learning_rate": 7.32335696018005e-07,
        "epoch": 0.9874707421175822,
        "step": 7172
    },
    {
        "loss": 2.6444,
        "grad_norm": 2.262218475341797,
        "learning_rate": 7.166899522701442e-07,
        "epoch": 0.9876084262701363,
        "step": 7173
    },
    {
        "loss": 2.096,
        "grad_norm": 1.0199217796325684,
        "learning_rate": 7.012125461814734e-07,
        "epoch": 0.9877461104226903,
        "step": 7174
    },
    {
        "loss": 1.2754,
        "grad_norm": 2.78389310836792,
        "learning_rate": 6.859035039943562e-07,
        "epoch": 0.9878837945752443,
        "step": 7175
    },
    {
        "loss": 2.1435,
        "grad_norm": 2.2857301235198975,
        "learning_rate": 6.70762851665696e-07,
        "epoch": 0.9880214787277984,
        "step": 7176
    },
    {
        "loss": 2.299,
        "grad_norm": 1.4875179529190063,
        "learning_rate": 6.557906148669135e-07,
        "epoch": 0.9881591628803524,
        "step": 7177
    },
    {
        "loss": 2.3283,
        "grad_norm": 1.37210214138031,
        "learning_rate": 6.409868189838464e-07,
        "epoch": 0.9882968470329065,
        "step": 7178
    },
    {
        "loss": 1.4977,
        "grad_norm": 2.1322994232177734,
        "learning_rate": 6.263514891167388e-07,
        "epoch": 0.9884345311854605,
        "step": 7179
    },
    {
        "loss": 2.3991,
        "grad_norm": 3.472017526626587,
        "learning_rate": 6.11884650080241e-07,
        "epoch": 0.9885722153380146,
        "step": 7180
    },
    {
        "loss": 1.9042,
        "grad_norm": 1.8050107955932617,
        "learning_rate": 5.975863264032544e-07,
        "epoch": 0.9887098994905686,
        "step": 7181
    },
    {
        "loss": 1.652,
        "grad_norm": 1.6882342100143433,
        "learning_rate": 5.834565423289972e-07,
        "epoch": 0.9888475836431226,
        "step": 7182
    },
    {
        "loss": 1.8868,
        "grad_norm": 2.163248300552368,
        "learning_rate": 5.694953218149502e-07,
        "epoch": 0.9889852677956767,
        "step": 7183
    },
    {
        "loss": 1.5289,
        "grad_norm": 3.862319231033325,
        "learning_rate": 5.557026885327221e-07,
        "epoch": 0.9891229519482307,
        "step": 7184
    },
    {
        "loss": 0.6374,
        "grad_norm": 3.433178663253784,
        "learning_rate": 5.420786658681287e-07,
        "epoch": 0.9892606361007849,
        "step": 7185
    },
    {
        "loss": 1.8198,
        "grad_norm": 2.166616678237915,
        "learning_rate": 5.286232769210697e-07,
        "epoch": 0.9893983202533388,
        "step": 7186
    },
    {
        "loss": 1.5875,
        "grad_norm": 2.76249361038208,
        "learning_rate": 5.153365445055625e-07,
        "epoch": 0.9895360044058928,
        "step": 7187
    },
    {
        "loss": 1.6699,
        "grad_norm": 2.2694878578186035,
        "learning_rate": 5.022184911495864e-07,
        "epoch": 0.989673688558447,
        "step": 7188
    },
    {
        "loss": 2.003,
        "grad_norm": 2.082960605621338,
        "learning_rate": 4.892691390952387e-07,
        "epoch": 0.989811372711001,
        "step": 7189
    },
    {
        "loss": 1.9284,
        "grad_norm": 1.831303358078003,
        "learning_rate": 4.764885102984784e-07,
        "epoch": 0.989949056863555,
        "step": 7190
    },
    {
        "loss": 1.6493,
        "grad_norm": 2.033196210861206,
        "learning_rate": 4.6387662642921604e-07,
        "epoch": 0.990086741016109,
        "step": 7191
    },
    {
        "loss": 1.5795,
        "grad_norm": 3.098888635635376,
        "learning_rate": 4.5143350887127956e-07,
        "epoch": 0.990224425168663,
        "step": 7192
    },
    {
        "loss": 2.019,
        "grad_norm": 2.109905481338501,
        "learning_rate": 4.391591787223148e-07,
        "epoch": 0.9903621093212172,
        "step": 7193
    },
    {
        "loss": 2.4062,
        "grad_norm": 1.492464542388916,
        "learning_rate": 4.270536567937744e-07,
        "epoch": 0.9904997934737712,
        "step": 7194
    },
    {
        "loss": 1.3073,
        "grad_norm": 3.2839486598968506,
        "learning_rate": 4.1511696361095086e-07,
        "epoch": 0.9906374776263253,
        "step": 7195
    },
    {
        "loss": 1.2915,
        "grad_norm": 2.3391780853271484,
        "learning_rate": 4.033491194128325e-07,
        "epoch": 0.9907751617788793,
        "step": 7196
    },
    {
        "loss": 1.3256,
        "grad_norm": 1.616349220275879,
        "learning_rate": 3.917501441521032e-07,
        "epoch": 0.9909128459314333,
        "step": 7197
    },
    {
        "loss": 1.8274,
        "grad_norm": 1.787898063659668,
        "learning_rate": 3.803200574951982e-07,
        "epoch": 0.9910505300839874,
        "step": 7198
    },
    {
        "loss": 1.5437,
        "grad_norm": 3.0788164138793945,
        "learning_rate": 3.6905887882213717e-07,
        "epoch": 0.9911882142365414,
        "step": 7199
    },
    {
        "loss": 1.1982,
        "grad_norm": 3.507424831390381,
        "learning_rate": 3.5796662722654697e-07,
        "epoch": 0.9913258983890955,
        "step": 7200
    },
    {
        "loss": 2.4225,
        "grad_norm": 1.442734956741333,
        "learning_rate": 3.470433215156721e-07,
        "epoch": 0.9914635825416495,
        "step": 7201
    },
    {
        "loss": 1.592,
        "grad_norm": 2.9523918628692627,
        "learning_rate": 3.362889802102864e-07,
        "epoch": 0.9916012666942035,
        "step": 7202
    },
    {
        "loss": 2.6384,
        "grad_norm": 2.7203152179718018,
        "learning_rate": 3.257036215446596e-07,
        "epoch": 0.9917389508467576,
        "step": 7203
    },
    {
        "loss": 1.7505,
        "grad_norm": 1.9598097801208496,
        "learning_rate": 3.1528726346660153e-07,
        "epoch": 0.9918766349993116,
        "step": 7204
    },
    {
        "loss": 1.7299,
        "grad_norm": 3.58092999458313,
        "learning_rate": 3.0503992363731804e-07,
        "epoch": 0.9920143191518657,
        "step": 7205
    },
    {
        "loss": 2.085,
        "grad_norm": 1.716303825378418,
        "learning_rate": 2.949616194314553e-07,
        "epoch": 0.9921520033044197,
        "step": 7206
    },
    {
        "loss": 1.6493,
        "grad_norm": 3.079380750656128,
        "learning_rate": 2.850523679370887e-07,
        "epoch": 0.9922896874569737,
        "step": 7207
    },
    {
        "loss": 1.9285,
        "grad_norm": 2.1913793087005615,
        "learning_rate": 2.753121859556229e-07,
        "epoch": 0.9924273716095278,
        "step": 7208
    },
    {
        "loss": 0.9815,
        "grad_norm": 3.6736247539520264,
        "learning_rate": 2.657410900018142e-07,
        "epoch": 0.9925650557620818,
        "step": 7209
    },
    {
        "loss": 0.8301,
        "grad_norm": 1.682874083518982,
        "learning_rate": 2.5633909630371487e-07,
        "epoch": 0.9927027399146359,
        "step": 7210
    },
    {
        "loss": 1.7369,
        "grad_norm": 2.717097520828247,
        "learning_rate": 2.471062208026842e-07,
        "epoch": 0.9928404240671899,
        "step": 7211
    },
    {
        "loss": 1.3571,
        "grad_norm": 3.780292510986328,
        "learning_rate": 2.380424791533109e-07,
        "epoch": 0.9929781082197439,
        "step": 7212
    },
    {
        "loss": 2.0503,
        "grad_norm": 1.3266041278839111,
        "learning_rate": 2.2914788672343535e-07,
        "epoch": 0.993115792372298,
        "step": 7213
    },
    {
        "loss": 1.5324,
        "grad_norm": 2.1941113471984863,
        "learning_rate": 2.2042245859409393e-07,
        "epoch": 0.993253476524852,
        "step": 7214
    },
    {
        "loss": 1.3953,
        "grad_norm": 1.63412606716156,
        "learning_rate": 2.1186620955947478e-07,
        "epoch": 0.9933911606774061,
        "step": 7215
    },
    {
        "loss": 1.9381,
        "grad_norm": 1.9674373865127563,
        "learning_rate": 2.0347915412697317e-07,
        "epoch": 0.9935288448299601,
        "step": 7216
    },
    {
        "loss": 1.0516,
        "grad_norm": 3.031503915786743,
        "learning_rate": 1.9526130651704722e-07,
        "epoch": 0.9936665289825141,
        "step": 7217
    },
    {
        "loss": 2.1641,
        "grad_norm": 1.3920246362686157,
        "learning_rate": 1.8721268066330676e-07,
        "epoch": 0.9938042131350682,
        "step": 7218
    },
    {
        "loss": 0.928,
        "grad_norm": 2.9866437911987305,
        "learning_rate": 1.793332902124245e-07,
        "epoch": 0.9939418972876222,
        "step": 7219
    },
    {
        "loss": 2.2218,
        "grad_norm": 2.3754196166992188,
        "learning_rate": 1.7162314852413598e-07,
        "epoch": 0.9940795814401763,
        "step": 7220
    },
    {
        "loss": 1.7638,
        "grad_norm": 2.141005277633667,
        "learning_rate": 1.6408226867118403e-07,
        "epoch": 0.9942172655927303,
        "step": 7221
    },
    {
        "loss": 1.4721,
        "grad_norm": 2.585449457168579,
        "learning_rate": 1.5671066343936335e-07,
        "epoch": 0.9943549497452843,
        "step": 7222
    },
    {
        "loss": 1.7381,
        "grad_norm": 2.9862208366394043,
        "learning_rate": 1.4950834532742043e-07,
        "epoch": 0.9944926338978384,
        "step": 7223
    },
    {
        "loss": 2.2871,
        "grad_norm": 1.6420453786849976,
        "learning_rate": 1.4247532654710904e-07,
        "epoch": 0.9946303180503924,
        "step": 7224
    },
    {
        "loss": 1.2434,
        "grad_norm": 3.353693962097168,
        "learning_rate": 1.356116190231016e-07,
        "epoch": 0.9947680022029465,
        "step": 7225
    },
    {
        "loss": 1.922,
        "grad_norm": 2.8372819423675537,
        "learning_rate": 1.2891723439301118e-07,
        "epoch": 0.9949056863555005,
        "step": 7226
    },
    {
        "loss": 1.9618,
        "grad_norm": 1.519397258758545,
        "learning_rate": 1.223921840073472e-07,
        "epoch": 0.9950433705080545,
        "step": 7227
    },
    {
        "loss": 2.2207,
        "grad_norm": 2.2052009105682373,
        "learning_rate": 1.1603647892951541e-07,
        "epoch": 0.9951810546606086,
        "step": 7228
    },
    {
        "loss": 1.5853,
        "grad_norm": 2.472780227661133,
        "learning_rate": 1.098501299358179e-07,
        "epoch": 0.9953187388131626,
        "step": 7229
    },
    {
        "loss": 1.8874,
        "grad_norm": 1.8996706008911133,
        "learning_rate": 1.0383314751535311e-07,
        "epoch": 0.9954564229657167,
        "step": 7230
    },
    {
        "loss": 2.1128,
        "grad_norm": 1.298227071762085,
        "learning_rate": 9.798554187010478e-08,
        "epoch": 0.9955941071182707,
        "step": 7231
    },
    {
        "loss": 2.2623,
        "grad_norm": 1.6137670278549194,
        "learning_rate": 9.230732291485301e-08,
        "epoch": 0.9957317912708247,
        "step": 7232
    },
    {
        "loss": 2.0099,
        "grad_norm": 1.667215347290039,
        "learning_rate": 8.67985002771743e-08,
        "epoch": 0.9958694754233788,
        "step": 7233
    },
    {
        "loss": 1.2555,
        "grad_norm": 2.8365466594696045,
        "learning_rate": 8.145908329743046e-08,
        "epoch": 0.9960071595759328,
        "step": 7234
    },
    {
        "loss": 1.7412,
        "grad_norm": 2.8954570293426514,
        "learning_rate": 7.628908102875754e-08,
        "epoch": 0.9961448437284869,
        "step": 7235
    },
    {
        "loss": 1.3815,
        "grad_norm": 3.1742022037506104,
        "learning_rate": 7.128850223703243e-08,
        "epoch": 0.9962825278810409,
        "step": 7236
    },
    {
        "loss": 1.2963,
        "grad_norm": 3.1373131275177,
        "learning_rate": 6.645735540088405e-08,
        "epoch": 0.996420212033595,
        "step": 7237
    },
    {
        "loss": 2.4415,
        "grad_norm": 1.4191877841949463,
        "learning_rate": 6.17956487116711e-08,
        "epoch": 0.996557896186149,
        "step": 7238
    },
    {
        "loss": 1.4531,
        "grad_norm": 2.697519063949585,
        "learning_rate": 5.730339007342655e-08,
        "epoch": 0.996695580338703,
        "step": 7239
    },
    {
        "loss": 2.3467,
        "grad_norm": 1.9445295333862305,
        "learning_rate": 5.2980587102913207e-08,
        "epoch": 0.9968332644912571,
        "step": 7240
    },
    {
        "loss": 2.3705,
        "grad_norm": 1.3469783067703247,
        "learning_rate": 4.88272471295792e-08,
        "epoch": 0.9969709486438111,
        "step": 7241
    },
    {
        "loss": 2.4102,
        "grad_norm": 2.0039713382720947,
        "learning_rate": 4.484337719550258e-08,
        "epoch": 0.9971086327963652,
        "step": 7242
    },
    {
        "loss": 0.8488,
        "grad_norm": 2.7090983390808105,
        "learning_rate": 4.102898405545785e-08,
        "epoch": 0.9972463169489192,
        "step": 7243
    },
    {
        "loss": 1.8717,
        "grad_norm": 1.9859405755996704,
        "learning_rate": 3.7384074176860515e-08,
        "epoch": 0.9973840011014732,
        "step": 7244
    },
    {
        "loss": 1.8808,
        "grad_norm": 2.3857178688049316,
        "learning_rate": 3.390865373976704e-08,
        "epoch": 0.9975216852540273,
        "step": 7245
    },
    {
        "loss": 1.5094,
        "grad_norm": 1.9364808797836304,
        "learning_rate": 3.0602728636830444e-08,
        "epoch": 0.9976593694065813,
        "step": 7246
    },
    {
        "loss": 2.1885,
        "grad_norm": 1.5292538404464722,
        "learning_rate": 2.7466304473355853e-08,
        "epoch": 0.9977970535591354,
        "step": 7247
    },
    {
        "loss": 1.3577,
        "grad_norm": 2.1930251121520996,
        "learning_rate": 2.4499386567233828e-08,
        "epoch": 0.9979347377116894,
        "step": 7248
    },
    {
        "loss": 1.4792,
        "grad_norm": 3.150418281555176,
        "learning_rate": 2.1701979948962614e-08,
        "epoch": 0.9980724218642434,
        "step": 7249
    },
    {
        "loss": 0.5175,
        "grad_norm": 1.9607940912246704,
        "learning_rate": 1.907408936161481e-08,
        "epoch": 0.9982101060167975,
        "step": 7250
    },
    {
        "loss": 2.0947,
        "grad_norm": 1.2725167274475098,
        "learning_rate": 1.6615719260859586e-08,
        "epoch": 0.9983477901693515,
        "step": 7251
    },
    {
        "loss": 1.2442,
        "grad_norm": 2.401916265487671,
        "learning_rate": 1.4326873814918263e-08,
        "epoch": 0.9984854743219056,
        "step": 7252
    },
    {
        "loss": 2.4094,
        "grad_norm": 1.5849645137786865,
        "learning_rate": 1.2207556904619832e-08,
        "epoch": 0.9986231584744596,
        "step": 7253
    },
    {
        "loss": 1.737,
        "grad_norm": 2.8737359046936035,
        "learning_rate": 1.0257772123312137e-08,
        "epoch": 0.9987608426270136,
        "step": 7254
    },
    {
        "loss": 2.2857,
        "grad_norm": 1.173540472984314,
        "learning_rate": 8.477522776906277e-09,
        "epoch": 0.9988985267795677,
        "step": 7255
    },
    {
        "loss": 0.9037,
        "grad_norm": 3.071037769317627,
        "learning_rate": 6.86681188386551e-09,
        "epoch": 0.9990362109321217,
        "step": 7256
    },
    {
        "loss": 1.1976,
        "grad_norm": 2.086066246032715,
        "learning_rate": 5.425642175205248e-09,
        "epoch": 0.9991738950846758,
        "step": 7257
    },
    {
        "loss": 2.4968,
        "grad_norm": 2.7015304565429688,
        "learning_rate": 4.1540160944597565e-09,
        "epoch": 0.9993115792372298,
        "step": 7258
    },
    {
        "loss": 1.0213,
        "grad_norm": 2.9968185424804688,
        "learning_rate": 3.051935797715455e-09,
        "epoch": 0.9994492633897838,
        "step": 7259
    },
    {
        "loss": 1.875,
        "grad_norm": 1.232094645500183,
        "learning_rate": 2.119403153577615e-09,
        "epoch": 0.9995869475423379,
        "step": 7260
    },
    {
        "loss": 2.419,
        "grad_norm": 1.7684465646743774,
        "learning_rate": 1.3564197431703563e-09,
        "epoch": 0.9997246316948919,
        "step": 7261
    },
    {
        "loss": 2.2768,
        "grad_norm": 3.11551570892334,
        "learning_rate": 7.629868601699564e-10,
        "epoch": 0.999862315847446,
        "step": 7262
    },
    {
        "loss": 1.7794,
        "grad_norm": 3.3492705821990967,
        "learning_rate": 3.3910551076044015e-10,
        "epoch": 1.0,
        "step": 7263
    },
    {
        "train_runtime": 19550.8123,
        "train_samples_per_second": 0.743,
        "train_steps_per_second": 0.371,
        "total_flos": 2.910877328306995e+17,
        "train_loss": 2.033073419068251,
        "epoch": 1.0,
        "step": 7263
    }
]