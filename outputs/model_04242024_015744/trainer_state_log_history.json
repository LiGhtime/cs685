[
    {
        "loss": 4.0149,
        "grad_norm": 3.2245090007781982,
        "learning_rate": 6.666666666666667e-06,
        "epoch": 0.00012883277505797475,
        "step": 1
    },
    {
        "loss": 3.5414,
        "grad_norm": 2.057100296020508,
        "learning_rate": 1.3333333333333333e-05,
        "epoch": 0.0002576655501159495,
        "step": 2
    },
    {
        "loss": 3.8137,
        "grad_norm": 2.309344530105591,
        "learning_rate": 2e-05,
        "epoch": 0.00038649832517392423,
        "step": 3
    },
    {
        "loss": 3.7335,
        "grad_norm": 1.8257405757904053,
        "learning_rate": 2.6666666666666667e-05,
        "epoch": 0.000515331100231899,
        "step": 4
    },
    {
        "loss": 3.9239,
        "grad_norm": 2.8927900791168213,
        "learning_rate": 3.3333333333333335e-05,
        "epoch": 0.0006441638752898738,
        "step": 5
    },
    {
        "loss": 4.1986,
        "grad_norm": 3.650843858718872,
        "learning_rate": 4e-05,
        "epoch": 0.0007729966503478485,
        "step": 6
    },
    {
        "loss": 3.8112,
        "grad_norm": 3.0538244247436523,
        "learning_rate": 4.666666666666667e-05,
        "epoch": 0.0009018294254058232,
        "step": 7
    },
    {
        "loss": 4.4448,
        "grad_norm": 4.6073198318481445,
        "learning_rate": 5.333333333333333e-05,
        "epoch": 0.001030662200463798,
        "step": 8
    },
    {
        "loss": 3.7278,
        "grad_norm": 2.7018661499023438,
        "learning_rate": 6e-05,
        "epoch": 0.0011594949755217728,
        "step": 9
    },
    {
        "loss": 3.5365,
        "grad_norm": 2.4038021564483643,
        "learning_rate": 6.666666666666667e-05,
        "epoch": 0.0012883277505797476,
        "step": 10
    },
    {
        "loss": 3.699,
        "grad_norm": 2.5561304092407227,
        "learning_rate": 7.333333333333333e-05,
        "epoch": 0.0014171605256377221,
        "step": 11
    },
    {
        "loss": 3.4576,
        "grad_norm": 1.7482218742370605,
        "learning_rate": 8e-05,
        "epoch": 0.001545993300695697,
        "step": 12
    },
    {
        "loss": 3.7729,
        "grad_norm": 3.083221912384033,
        "learning_rate": 8.666666666666667e-05,
        "epoch": 0.0016748260757536717,
        "step": 13
    },
    {
        "loss": 2.9923,
        "grad_norm": 1.027418851852417,
        "learning_rate": 9.333333333333334e-05,
        "epoch": 0.0018036588508116465,
        "step": 14
    },
    {
        "loss": 3.0312,
        "grad_norm": 1.2651070356369019,
        "learning_rate": 0.0001,
        "epoch": 0.0019324916258696213,
        "step": 15
    },
    {
        "loss": 3.5081,
        "grad_norm": 2.9616971015930176,
        "learning_rate": 9.999996299886059e-05,
        "epoch": 0.002061324400927596,
        "step": 16
    },
    {
        "loss": 3.1588,
        "grad_norm": Infinity,
        "learning_rate": 9.999996299886059e-05,
        "epoch": 0.0021901571759855706,
        "step": 17
    },
    {
        "loss": 3.0546,
        "grad_norm": 2.500945568084717,
        "learning_rate": 9.999985199549709e-05,
        "epoch": 0.0023189899510435456,
        "step": 18
    },
    {
        "loss": 3.3017,
        "grad_norm": 3.08349871635437,
        "learning_rate": 9.999966699007381e-05,
        "epoch": 0.00244782272610152,
        "step": 19
    },
    {
        "loss": 2.7107,
        "grad_norm": 2.535407543182373,
        "learning_rate": 9.999940798286458e-05,
        "epoch": 0.002576655501159495,
        "step": 20
    },
    {
        "loss": 2.788,
        "grad_norm": 2.502821207046509,
        "learning_rate": 9.99990749742527e-05,
        "epoch": 0.0027054882762174697,
        "step": 21
    },
    {
        "loss": 2.7099,
        "grad_norm": 3.5131707191467285,
        "learning_rate": 9.999866796473108e-05,
        "epoch": 0.0028343210512754443,
        "step": 22
    },
    {
        "loss": 3.037,
        "grad_norm": 2.252744436264038,
        "learning_rate": 9.999818695490208e-05,
        "epoch": 0.0029631538263334193,
        "step": 23
    },
    {
        "loss": 3.0607,
        "grad_norm": 1.550294280052185,
        "learning_rate": 9.999763194547765e-05,
        "epoch": 0.003091986601391394,
        "step": 24
    },
    {
        "loss": 3.0019,
        "grad_norm": 2.390254497528076,
        "learning_rate": 9.99970029372792e-05,
        "epoch": 0.003220819376449369,
        "step": 25
    },
    {
        "loss": 2.9854,
        "grad_norm": 1.5864754915237427,
        "learning_rate": 9.999629993123772e-05,
        "epoch": 0.0033496521515073434,
        "step": 26
    },
    {
        "loss": 2.6773,
        "grad_norm": 1.9582209587097168,
        "learning_rate": 9.999552292839366e-05,
        "epoch": 0.0034784849265653184,
        "step": 27
    },
    {
        "loss": 2.7459,
        "grad_norm": 1.927025318145752,
        "learning_rate": 9.999467192989702e-05,
        "epoch": 0.003607317701623293,
        "step": 28
    },
    {
        "loss": 2.8666,
        "grad_norm": 1.6352674961090088,
        "learning_rate": 9.999374693700737e-05,
        "epoch": 0.0037361504766812675,
        "step": 29
    },
    {
        "loss": 2.5943,
        "grad_norm": 3.2671866416931152,
        "learning_rate": 9.999274795109365e-05,
        "epoch": 0.0038649832517392425,
        "step": 30
    },
    {
        "loss": 2.9397,
        "grad_norm": 1.9556325674057007,
        "learning_rate": 9.999167497363449e-05,
        "epoch": 0.003993816026797217,
        "step": 31
    },
    {
        "loss": 2.8288,
        "grad_norm": 1.5264798402786255,
        "learning_rate": 9.999052800621789e-05,
        "epoch": 0.004122648801855192,
        "step": 32
    },
    {
        "loss": 2.4648,
        "grad_norm": 3.1203174591064453,
        "learning_rate": 9.998930705054145e-05,
        "epoch": 0.004251481576913167,
        "step": 33
    },
    {
        "loss": 2.2611,
        "grad_norm": 2.6327404975891113,
        "learning_rate": 9.998801210841222e-05,
        "epoch": 0.004380314351971141,
        "step": 34
    },
    {
        "loss": 2.6318,
        "grad_norm": 1.9109227657318115,
        "learning_rate": 9.998664318174677e-05,
        "epoch": 0.004509147127029116,
        "step": 35
    },
    {
        "loss": 2.7465,
        "grad_norm": 2.080521583557129,
        "learning_rate": 9.998520027257122e-05,
        "epoch": 0.004637979902087091,
        "step": 36
    },
    {
        "loss": 2.664,
        "grad_norm": 1.869097352027893,
        "learning_rate": 9.998368338302106e-05,
        "epoch": 0.004766812677145065,
        "step": 37
    },
    {
        "loss": 2.4667,
        "grad_norm": 1.8718011379241943,
        "learning_rate": 9.998209251534142e-05,
        "epoch": 0.00489564545220304,
        "step": 38
    },
    {
        "loss": 2.6158,
        "grad_norm": 1.608787178993225,
        "learning_rate": 9.998042767188682e-05,
        "epoch": 0.005024478227261015,
        "step": 39
    },
    {
        "loss": 2.5644,
        "grad_norm": 2.2463252544403076,
        "learning_rate": 9.997868885512134e-05,
        "epoch": 0.00515331100231899,
        "step": 40
    },
    {
        "loss": 2.704,
        "grad_norm": 1.632853388786316,
        "learning_rate": 9.997687606761848e-05,
        "epoch": 0.0052821437773769644,
        "step": 41
    },
    {
        "loss": 2.3721,
        "grad_norm": 2.191293716430664,
        "learning_rate": 9.997498931206126e-05,
        "epoch": 0.0054109765524349394,
        "step": 42
    },
    {
        "loss": 2.6512,
        "grad_norm": 1.5807377099990845,
        "learning_rate": 9.997302859124215e-05,
        "epoch": 0.0055398093274929144,
        "step": 43
    },
    {
        "loss": 2.6967,
        "grad_norm": 1.342618465423584,
        "learning_rate": 9.997099390806313e-05,
        "epoch": 0.0056686421025508886,
        "step": 44
    },
    {
        "loss": 2.464,
        "grad_norm": 1.9804749488830566,
        "learning_rate": 9.996888526553562e-05,
        "epoch": 0.0057974748776088636,
        "step": 45
    },
    {
        "loss": 2.1601,
        "grad_norm": 1.8881677389144897,
        "learning_rate": 9.996670266678047e-05,
        "epoch": 0.0059263076526668386,
        "step": 46
    },
    {
        "loss": 2.6103,
        "grad_norm": 1.9037083387374878,
        "learning_rate": 9.996444611502809e-05,
        "epoch": 0.0060551404277248136,
        "step": 47
    },
    {
        "loss": 2.4382,
        "grad_norm": 1.7836438417434692,
        "learning_rate": 9.996211561361821e-05,
        "epoch": 0.006183973202782788,
        "step": 48
    },
    {
        "loss": 2.5603,
        "grad_norm": 1.8645169734954834,
        "learning_rate": 9.995971116600012e-05,
        "epoch": 0.006312805977840763,
        "step": 49
    },
    {
        "loss": 2.6027,
        "grad_norm": 1.801620364189148,
        "learning_rate": 9.99572327757325e-05,
        "epoch": 0.006441638752898738,
        "step": 50
    },
    {
        "loss": 2.49,
        "grad_norm": 1.8069795370101929,
        "learning_rate": 9.99546804464835e-05,
        "epoch": 0.006570471527956712,
        "step": 51
    },
    {
        "loss": 2.5879,
        "grad_norm": 1.48258638381958,
        "learning_rate": 9.995205418203067e-05,
        "epoch": 0.006699304303014687,
        "step": 52
    },
    {
        "loss": 2.6253,
        "grad_norm": 1.7820388078689575,
        "learning_rate": 9.994935398626097e-05,
        "epoch": 0.006828137078072662,
        "step": 53
    },
    {
        "loss": 2.5205,
        "grad_norm": 1.7092974185943604,
        "learning_rate": 9.994657986317086e-05,
        "epoch": 0.006956969853130637,
        "step": 54
    },
    {
        "loss": 2.5256,
        "grad_norm": 1.3615789413452148,
        "learning_rate": 9.994373181686615e-05,
        "epoch": 0.007085802628188611,
        "step": 55
    },
    {
        "loss": 2.4883,
        "grad_norm": 1.3152430057525635,
        "learning_rate": 9.99408098515621e-05,
        "epoch": 0.007214635403246586,
        "step": 56
    },
    {
        "loss": 2.431,
        "grad_norm": 1.309076189994812,
        "learning_rate": 9.993781397158328e-05,
        "epoch": 0.007343468178304561,
        "step": 57
    },
    {
        "loss": 2.5209,
        "grad_norm": 1.8083295822143555,
        "learning_rate": 9.993474418136381e-05,
        "epoch": 0.007472300953362535,
        "step": 58
    },
    {
        "loss": 2.185,
        "grad_norm": 2.04490065574646,
        "learning_rate": 9.99316004854471e-05,
        "epoch": 0.00760113372842051,
        "step": 59
    },
    {
        "loss": 2.2863,
        "grad_norm": 2.0925509929656982,
        "learning_rate": 9.992838288848593e-05,
        "epoch": 0.007729966503478485,
        "step": 60
    },
    {
        "loss": 2.7685,
        "grad_norm": 1.9092971086502075,
        "learning_rate": 9.992509139524252e-05,
        "epoch": 0.00785879927853646,
        "step": 61
    },
    {
        "loss": 2.2484,
        "grad_norm": 1.5271962881088257,
        "learning_rate": 9.992172601058842e-05,
        "epoch": 0.007987632053594434,
        "step": 62
    },
    {
        "loss": 2.6853,
        "grad_norm": 1.1606734991073608,
        "learning_rate": 9.991828673950455e-05,
        "epoch": 0.00811646482865241,
        "step": 63
    },
    {
        "loss": 2.5369,
        "grad_norm": 1.731136679649353,
        "learning_rate": 9.99147735870812e-05,
        "epoch": 0.008245297603710384,
        "step": 64
    },
    {
        "loss": 2.3227,
        "grad_norm": 2.2216055393218994,
        "learning_rate": 9.991118655851798e-05,
        "epoch": 0.00837413037876836,
        "step": 65
    },
    {
        "loss": 2.9367,
        "grad_norm": 2.0197856426239014,
        "learning_rate": 9.990752565912389e-05,
        "epoch": 0.008502963153826334,
        "step": 66
    },
    {
        "loss": 2.3982,
        "grad_norm": 2.3087968826293945,
        "learning_rate": 9.990379089431717e-05,
        "epoch": 0.008631795928884307,
        "step": 67
    },
    {
        "loss": 2.7647,
        "grad_norm": 1.5004626512527466,
        "learning_rate": 9.989998226962549e-05,
        "epoch": 0.008760628703942282,
        "step": 68
    },
    {
        "loss": 2.3675,
        "grad_norm": 1.5954655408859253,
        "learning_rate": 9.989609979068577e-05,
        "epoch": 0.008889461479000257,
        "step": 69
    },
    {
        "loss": 2.0665,
        "grad_norm": 2.525221347808838,
        "learning_rate": 9.989214346324425e-05,
        "epoch": 0.009018294254058232,
        "step": 70
    },
    {
        "loss": 2.6994,
        "grad_norm": 1.6555991172790527,
        "learning_rate": 9.988811329315649e-05,
        "epoch": 0.009147127029116207,
        "step": 71
    },
    {
        "loss": 2.7623,
        "grad_norm": 1.8387870788574219,
        "learning_rate": 9.988400928638733e-05,
        "epoch": 0.009275959804174182,
        "step": 72
    },
    {
        "loss": 2.4692,
        "grad_norm": 2.319812536239624,
        "learning_rate": 9.987983144901085e-05,
        "epoch": 0.009404792579232157,
        "step": 73
    },
    {
        "loss": 2.6025,
        "grad_norm": 1.5836330652236938,
        "learning_rate": 9.98755797872105e-05,
        "epoch": 0.00953362535429013,
        "step": 74
    },
    {
        "loss": 2.201,
        "grad_norm": 2.208174705505371,
        "learning_rate": 9.987125430727886e-05,
        "epoch": 0.009662458129348106,
        "step": 75
    },
    {
        "loss": 2.7879,
        "grad_norm": 1.1003392934799194,
        "learning_rate": 9.98668550156179e-05,
        "epoch": 0.00979129090440608,
        "step": 76
    },
    {
        "loss": 2.3257,
        "grad_norm": 1.7274519205093384,
        "learning_rate": 9.986238191873874e-05,
        "epoch": 0.009920123679464056,
        "step": 77
    },
    {
        "loss": 2.6705,
        "grad_norm": 1.1093530654907227,
        "learning_rate": 9.985783502326177e-05,
        "epoch": 0.01004895645452203,
        "step": 78
    },
    {
        "loss": 2.1549,
        "grad_norm": 1.3716269731521606,
        "learning_rate": 9.985321433591662e-05,
        "epoch": 0.010177789229580006,
        "step": 79
    },
    {
        "loss": 1.9998,
        "grad_norm": 2.4189038276672363,
        "learning_rate": 9.98485198635421e-05,
        "epoch": 0.01030662200463798,
        "step": 80
    },
    {
        "loss": 2.8393,
        "grad_norm": 1.379417896270752,
        "learning_rate": 9.984375161308626e-05,
        "epoch": 0.010435454779695954,
        "step": 81
    },
    {
        "loss": 2.3933,
        "grad_norm": 2.048875570297241,
        "learning_rate": 9.983890959160629e-05,
        "epoch": 0.010564287554753929,
        "step": 82
    },
    {
        "loss": 2.0276,
        "grad_norm": 2.7722713947296143,
        "learning_rate": 9.983399380626866e-05,
        "epoch": 0.010693120329811904,
        "step": 83
    },
    {
        "loss": 2.7159,
        "grad_norm": 2.424875020980835,
        "learning_rate": 9.98290042643489e-05,
        "epoch": 0.010821953104869879,
        "step": 84
    },
    {
        "loss": 2.5264,
        "grad_norm": 1.5187588930130005,
        "learning_rate": 9.98239409732318e-05,
        "epoch": 0.010950785879927854,
        "step": 85
    },
    {
        "loss": 2.3011,
        "grad_norm": 1.2789115905761719,
        "learning_rate": 9.981880394041124e-05,
        "epoch": 0.011079618654985829,
        "step": 86
    },
    {
        "loss": 2.4143,
        "grad_norm": 1.5901192426681519,
        "learning_rate": 9.981359317349027e-05,
        "epoch": 0.011208451430043804,
        "step": 87
    },
    {
        "loss": 2.468,
        "grad_norm": 2.3651130199432373,
        "learning_rate": 9.980830868018107e-05,
        "epoch": 0.011337284205101777,
        "step": 88
    },
    {
        "loss": 2.6672,
        "grad_norm": 1.7540349960327148,
        "learning_rate": 9.980295046830491e-05,
        "epoch": 0.011466116980159752,
        "step": 89
    },
    {
        "loss": 2.2127,
        "grad_norm": 2.4845850467681885,
        "learning_rate": 9.979751854579221e-05,
        "epoch": 0.011594949755217727,
        "step": 90
    },
    {
        "loss": 2.2887,
        "grad_norm": 1.4796843528747559,
        "learning_rate": 9.979201292068245e-05,
        "epoch": 0.011723782530275702,
        "step": 91
    },
    {
        "loss": 1.9356,
        "grad_norm": 2.7482831478118896,
        "learning_rate": 9.978643360112421e-05,
        "epoch": 0.011852615305333677,
        "step": 92
    },
    {
        "loss": 1.9406,
        "grad_norm": 3.6479575634002686,
        "learning_rate": 9.978078059537515e-05,
        "epoch": 0.011981448080391652,
        "step": 93
    },
    {
        "loss": 1.9149,
        "grad_norm": 2.241025686264038,
        "learning_rate": 9.977505391180194e-05,
        "epoch": 0.012110280855449627,
        "step": 94
    },
    {
        "loss": 2.7879,
        "grad_norm": 1.4326337575912476,
        "learning_rate": 9.976925355888035e-05,
        "epoch": 0.0122391136305076,
        "step": 95
    },
    {
        "loss": 1.9011,
        "grad_norm": 1.8670907020568848,
        "learning_rate": 9.976337954519521e-05,
        "epoch": 0.012367946405565575,
        "step": 96
    },
    {
        "loss": 2.7553,
        "grad_norm": 2.300898313522339,
        "learning_rate": 9.975743187944026e-05,
        "epoch": 0.01249677918062355,
        "step": 97
    },
    {
        "loss": 2.4346,
        "grad_norm": 2.041025161743164,
        "learning_rate": 9.975141057041836e-05,
        "epoch": 0.012625611955681525,
        "step": 98
    },
    {
        "loss": 2.6018,
        "grad_norm": 1.518113136291504,
        "learning_rate": 9.974531562704131e-05,
        "epoch": 0.0127544447307395,
        "step": 99
    },
    {
        "loss": 2.3051,
        "grad_norm": 2.4375452995300293,
        "learning_rate": 9.97391470583299e-05,
        "epoch": 0.012883277505797475,
        "step": 100
    },
    {
        "loss": 2.3532,
        "grad_norm": 2.258138656616211,
        "learning_rate": 9.97329048734139e-05,
        "epoch": 0.01301211028085545,
        "step": 101
    },
    {
        "loss": 2.7815,
        "grad_norm": 1.8968650102615356,
        "learning_rate": 9.9726589081532e-05,
        "epoch": 0.013140943055913424,
        "step": 102
    },
    {
        "loss": 2.3446,
        "grad_norm": 1.8525606393814087,
        "learning_rate": 9.97201996920319e-05,
        "epoch": 0.013269775830971399,
        "step": 103
    },
    {
        "loss": 2.7781,
        "grad_norm": 1.0024890899658203,
        "learning_rate": 9.971373671437017e-05,
        "epoch": 0.013398608606029374,
        "step": 104
    },
    {
        "loss": 2.3197,
        "grad_norm": 1.567887306213379,
        "learning_rate": 9.97072001581123e-05,
        "epoch": 0.013527441381087349,
        "step": 105
    },
    {
        "loss": 2.4923,
        "grad_norm": 1.5335198640823364,
        "learning_rate": 9.970059003293273e-05,
        "epoch": 0.013656274156145324,
        "step": 106
    },
    {
        "loss": 2.3229,
        "grad_norm": 2.3494508266448975,
        "learning_rate": 9.969390634861471e-05,
        "epoch": 0.013785106931203299,
        "step": 107
    },
    {
        "loss": 2.5913,
        "grad_norm": 1.950614333152771,
        "learning_rate": 9.96871491150504e-05,
        "epoch": 0.013913939706261274,
        "step": 108
    },
    {
        "loss": 1.5796,
        "grad_norm": 2.8312437534332275,
        "learning_rate": 9.968031834224083e-05,
        "epoch": 0.014042772481319247,
        "step": 109
    },
    {
        "loss": 2.171,
        "grad_norm": 1.4842281341552734,
        "learning_rate": 9.967341404029583e-05,
        "epoch": 0.014171605256377222,
        "step": 110
    },
    {
        "loss": 2.013,
        "grad_norm": 2.2596042156219482,
        "learning_rate": 9.966643621943411e-05,
        "epoch": 0.014300438031435197,
        "step": 111
    },
    {
        "loss": 2.4101,
        "grad_norm": 1.8054683208465576,
        "learning_rate": 9.965938488998316e-05,
        "epoch": 0.014429270806493172,
        "step": 112
    },
    {
        "loss": 2.6987,
        "grad_norm": 1.3497233390808105,
        "learning_rate": 9.965226006237924e-05,
        "epoch": 0.014558103581551147,
        "step": 113
    },
    {
        "loss": 2.5744,
        "grad_norm": 1.2198524475097656,
        "learning_rate": 9.964506174716745e-05,
        "epoch": 0.014686936356609122,
        "step": 114
    },
    {
        "loss": 2.7113,
        "grad_norm": 1.3009581565856934,
        "learning_rate": 9.963778995500162e-05,
        "epoch": 0.014815769131667097,
        "step": 115
    },
    {
        "loss": 2.2549,
        "grad_norm": 2.0284106731414795,
        "learning_rate": 9.963044469664432e-05,
        "epoch": 0.01494460190672507,
        "step": 116
    },
    {
        "loss": 2.0342,
        "grad_norm": 2.4295482635498047,
        "learning_rate": 9.962302598296688e-05,
        "epoch": 0.015073434681783045,
        "step": 117
    },
    {
        "loss": 2.4175,
        "grad_norm": 2.232548952102661,
        "learning_rate": 9.961553382494932e-05,
        "epoch": 0.01520226745684102,
        "step": 118
    },
    {
        "loss": 2.5943,
        "grad_norm": 2.0026047229766846,
        "learning_rate": 9.960796823368039e-05,
        "epoch": 0.015331100231898995,
        "step": 119
    },
    {
        "loss": 2.5119,
        "grad_norm": 1.9073607921600342,
        "learning_rate": 9.960032922035751e-05,
        "epoch": 0.01545993300695697,
        "step": 120
    },
    {
        "loss": 2.3068,
        "grad_norm": 2.3309054374694824,
        "learning_rate": 9.959261679628675e-05,
        "epoch": 0.015588765782014945,
        "step": 121
    },
    {
        "loss": 2.5845,
        "grad_norm": 1.9327796697616577,
        "learning_rate": 9.958483097288287e-05,
        "epoch": 0.01571759855707292,
        "step": 122
    },
    {
        "loss": 2.285,
        "grad_norm": 2.4650840759277344,
        "learning_rate": 9.957697176166924e-05,
        "epoch": 0.015846431332130893,
        "step": 123
    },
    {
        "loss": 2.6143,
        "grad_norm": 1.4789222478866577,
        "learning_rate": 9.956903917427784e-05,
        "epoch": 0.01597526410718887,
        "step": 124
    },
    {
        "loss": 2.6844,
        "grad_norm": 2.7746224403381348,
        "learning_rate": 9.956103322244928e-05,
        "epoch": 0.016104096882246843,
        "step": 125
    },
    {
        "loss": 2.4748,
        "grad_norm": 1.8560264110565186,
        "learning_rate": 9.95529539180327e-05,
        "epoch": 0.01623292965730482,
        "step": 126
    },
    {
        "loss": 2.1752,
        "grad_norm": 1.4975370168685913,
        "learning_rate": 9.954480127298586e-05,
        "epoch": 0.016361762432362793,
        "step": 127
    },
    {
        "loss": 1.9621,
        "grad_norm": 1.5996099710464478,
        "learning_rate": 9.953657529937505e-05,
        "epoch": 0.01649059520742077,
        "step": 128
    },
    {
        "loss": 2.5032,
        "grad_norm": 1.6217457056045532,
        "learning_rate": 9.952827600937509e-05,
        "epoch": 0.016619427982478743,
        "step": 129
    },
    {
        "loss": 2.4055,
        "grad_norm": 2.27427339553833,
        "learning_rate": 9.951990341526929e-05,
        "epoch": 0.01674826075753672,
        "step": 130
    },
    {
        "loss": 2.3369,
        "grad_norm": 1.2553589344024658,
        "learning_rate": 9.951145752944948e-05,
        "epoch": 0.016877093532594693,
        "step": 131
    },
    {
        "loss": 2.3145,
        "grad_norm": 2.0181338787078857,
        "learning_rate": 9.950293836441595e-05,
        "epoch": 0.01700592630765267,
        "step": 132
    },
    {
        "loss": 2.3779,
        "grad_norm": 3.3550148010253906,
        "learning_rate": 9.949434593277747e-05,
        "epoch": 0.017134759082710643,
        "step": 133
    },
    {
        "loss": 2.3946,
        "grad_norm": 2.942291498184204,
        "learning_rate": 9.948568024725121e-05,
        "epoch": 0.017263591857768615,
        "step": 134
    },
    {
        "loss": 1.5947,
        "grad_norm": 2.633727550506592,
        "learning_rate": 9.947694132066278e-05,
        "epoch": 0.01739242463282659,
        "step": 135
    },
    {
        "loss": 2.5465,
        "grad_norm": 2.493361473083496,
        "learning_rate": 9.946812916594621e-05,
        "epoch": 0.017521257407884565,
        "step": 136
    },
    {
        "loss": 2.5423,
        "grad_norm": 1.4814356565475464,
        "learning_rate": 9.945924379614389e-05,
        "epoch": 0.01765009018294254,
        "step": 137
    },
    {
        "loss": 2.4441,
        "grad_norm": 1.8408366441726685,
        "learning_rate": 9.945028522440653e-05,
        "epoch": 0.017778922958000515,
        "step": 138
    },
    {
        "loss": 2.5754,
        "grad_norm": 1.697743535041809,
        "learning_rate": 9.94412534639933e-05,
        "epoch": 0.01790775573305849,
        "step": 139
    },
    {
        "loss": 2.4948,
        "grad_norm": 2.412196159362793,
        "learning_rate": 9.943214852827152e-05,
        "epoch": 0.018036588508116465,
        "step": 140
    },
    {
        "loss": 2.1922,
        "grad_norm": 1.942596435546875,
        "learning_rate": 9.9422970430717e-05,
        "epoch": 0.01816542128317444,
        "step": 141
    },
    {
        "loss": 2.657,
        "grad_norm": 1.3706423044204712,
        "learning_rate": 9.94137191849137e-05,
        "epoch": 0.018294254058232415,
        "step": 142
    },
    {
        "loss": 2.2988,
        "grad_norm": 1.7825158834457397,
        "learning_rate": 9.940439480455386e-05,
        "epoch": 0.01842308683329039,
        "step": 143
    },
    {
        "loss": 2.5415,
        "grad_norm": 1.9558223485946655,
        "learning_rate": 9.939499730343804e-05,
        "epoch": 0.018551919608348365,
        "step": 144
    },
    {
        "loss": 2.3453,
        "grad_norm": 2.24299693107605,
        "learning_rate": 9.938552669547494e-05,
        "epoch": 0.01868075238340634,
        "step": 145
    },
    {
        "loss": 2.6034,
        "grad_norm": 1.9811615943908691,
        "learning_rate": 9.937598299468152e-05,
        "epoch": 0.018809585158464315,
        "step": 146
    },
    {
        "loss": 2.3839,
        "grad_norm": 1.882981777191162,
        "learning_rate": 9.936636621518285e-05,
        "epoch": 0.01893841793352229,
        "step": 147
    },
    {
        "loss": 2.5561,
        "grad_norm": 2.089148759841919,
        "learning_rate": 9.935667637121221e-05,
        "epoch": 0.01906725070858026,
        "step": 148
    },
    {
        "loss": 2.4906,
        "grad_norm": 1.347879409790039,
        "learning_rate": 9.934691347711106e-05,
        "epoch": 0.019196083483638236,
        "step": 149
    },
    {
        "loss": 2.5688,
        "grad_norm": 1.7030280828475952,
        "learning_rate": 9.933707754732888e-05,
        "epoch": 0.01932491625869621,
        "step": 150
    },
    {
        "loss": 2.4914,
        "grad_norm": 2.1213512420654297,
        "learning_rate": 9.93271685964233e-05,
        "epoch": 0.019453749033754186,
        "step": 151
    },
    {
        "loss": 2.8005,
        "grad_norm": 1.779271125793457,
        "learning_rate": 9.931718663906003e-05,
        "epoch": 0.01958258180881216,
        "step": 152
    },
    {
        "loss": 2.506,
        "grad_norm": 1.3064091205596924,
        "learning_rate": 9.930713169001282e-05,
        "epoch": 0.019711414583870136,
        "step": 153
    },
    {
        "loss": 2.5262,
        "grad_norm": 1.7787858247756958,
        "learning_rate": 9.929700376416344e-05,
        "epoch": 0.01984024735892811,
        "step": 154
    },
    {
        "loss": 2.4186,
        "grad_norm": 1.9676827192306519,
        "learning_rate": 9.928680287650169e-05,
        "epoch": 0.019969080133986086,
        "step": 155
    },
    {
        "loss": 2.3811,
        "grad_norm": 1.5604768991470337,
        "learning_rate": 9.927652904212536e-05,
        "epoch": 0.02009791290904406,
        "step": 156
    },
    {
        "loss": 2.562,
        "grad_norm": 1.6238664388656616,
        "learning_rate": 9.92661822762402e-05,
        "epoch": 0.020226745684102036,
        "step": 157
    },
    {
        "loss": 2.1614,
        "grad_norm": 2.6508710384368896,
        "learning_rate": 9.925576259415986e-05,
        "epoch": 0.02035557845916001,
        "step": 158
    },
    {
        "loss": 2.0413,
        "grad_norm": 2.1811225414276123,
        "learning_rate": 9.924527001130596e-05,
        "epoch": 0.020484411234217986,
        "step": 159
    },
    {
        "loss": 2.2669,
        "grad_norm": 2.224759817123413,
        "learning_rate": 9.9234704543208e-05,
        "epoch": 0.02061324400927596,
        "step": 160
    },
    {
        "loss": 2.307,
        "grad_norm": 1.812741994857788,
        "learning_rate": 9.922406620550337e-05,
        "epoch": 0.020742076784333936,
        "step": 161
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.9187731742858887,
        "learning_rate": 9.921335501393729e-05,
        "epoch": 0.020870909559391908,
        "step": 162
    },
    {
        "loss": 2.4176,
        "grad_norm": 2.2279317378997803,
        "learning_rate": 9.92025709843628e-05,
        "epoch": 0.020999742334449883,
        "step": 163
    },
    {
        "loss": 2.4911,
        "grad_norm": 2.269899845123291,
        "learning_rate": 9.919171413274076e-05,
        "epoch": 0.021128575109507858,
        "step": 164
    },
    {
        "loss": 2.5339,
        "grad_norm": 2.7103443145751953,
        "learning_rate": 9.918078447513979e-05,
        "epoch": 0.021257407884565833,
        "step": 165
    },
    {
        "loss": 2.4046,
        "grad_norm": 2.0980775356292725,
        "learning_rate": 9.916978202773631e-05,
        "epoch": 0.021386240659623808,
        "step": 166
    },
    {
        "loss": 2.185,
        "grad_norm": 2.241645097732544,
        "learning_rate": 9.915870680681443e-05,
        "epoch": 0.021515073434681783,
        "step": 167
    },
    {
        "loss": 2.2205,
        "grad_norm": 1.9108726978302002,
        "learning_rate": 9.914755882876598e-05,
        "epoch": 0.021643906209739758,
        "step": 168
    },
    {
        "loss": 2.4131,
        "grad_norm": 2.6647088527679443,
        "learning_rate": 9.913633811009048e-05,
        "epoch": 0.021772738984797733,
        "step": 169
    },
    {
        "loss": 2.2738,
        "grad_norm": 2.8225057125091553,
        "learning_rate": 9.91250446673951e-05,
        "epoch": 0.021901571759855708,
        "step": 170
    },
    {
        "loss": 2.6055,
        "grad_norm": 1.5350345373153687,
        "learning_rate": 9.911367851739467e-05,
        "epoch": 0.022030404534913683,
        "step": 171
    },
    {
        "loss": 2.1699,
        "grad_norm": 2.1272342205047607,
        "learning_rate": 9.910223967691156e-05,
        "epoch": 0.022159237309971658,
        "step": 172
    },
    {
        "loss": 2.5973,
        "grad_norm": 1.283890962600708,
        "learning_rate": 9.909072816287584e-05,
        "epoch": 0.022288070085029633,
        "step": 173
    },
    {
        "loss": 2.4549,
        "grad_norm": 2.163141965866089,
        "learning_rate": 9.907914399232503e-05,
        "epoch": 0.022416902860087608,
        "step": 174
    },
    {
        "loss": 2.5039,
        "grad_norm": 1.4051454067230225,
        "learning_rate": 9.906748718240423e-05,
        "epoch": 0.022545735635145583,
        "step": 175
    },
    {
        "loss": 2.0908,
        "grad_norm": 2.2699527740478516,
        "learning_rate": 9.905575775036608e-05,
        "epoch": 0.022674568410203554,
        "step": 176
    },
    {
        "loss": 2.2801,
        "grad_norm": 2.3350188732147217,
        "learning_rate": 9.904395571357065e-05,
        "epoch": 0.02280340118526153,
        "step": 177
    },
    {
        "loss": 1.5347,
        "grad_norm": 3.6723365783691406,
        "learning_rate": 9.903208108948552e-05,
        "epoch": 0.022932233960319504,
        "step": 178
    },
    {
        "loss": 2.6586,
        "grad_norm": 2.3261396884918213,
        "learning_rate": 9.902013389568563e-05,
        "epoch": 0.02306106673537748,
        "step": 179
    },
    {
        "loss": 2.1104,
        "grad_norm": 2.0624873638153076,
        "learning_rate": 9.900811414985341e-05,
        "epoch": 0.023189899510435454,
        "step": 180
    },
    {
        "loss": 1.9793,
        "grad_norm": 2.4915051460266113,
        "learning_rate": 9.899602186977861e-05,
        "epoch": 0.02331873228549343,
        "step": 181
    },
    {
        "loss": 2.6381,
        "grad_norm": 1.1793246269226074,
        "learning_rate": 9.898385707335838e-05,
        "epoch": 0.023447565060551404,
        "step": 182
    },
    {
        "loss": 2.5684,
        "grad_norm": 1.1546826362609863,
        "learning_rate": 9.897161977859715e-05,
        "epoch": 0.02357639783560938,
        "step": 183
    },
    {
        "loss": 2.0837,
        "grad_norm": 1.5251401662826538,
        "learning_rate": 9.895931000360666e-05,
        "epoch": 0.023705230610667354,
        "step": 184
    },
    {
        "loss": 2.5708,
        "grad_norm": 1.66607666015625,
        "learning_rate": 9.894692776660598e-05,
        "epoch": 0.02383406338572533,
        "step": 185
    },
    {
        "loss": 2.5637,
        "grad_norm": 1.6463693380355835,
        "learning_rate": 9.893447308592136e-05,
        "epoch": 0.023962896160783304,
        "step": 186
    },
    {
        "loss": 1.7571,
        "grad_norm": 2.401198148727417,
        "learning_rate": 9.89219459799863e-05,
        "epoch": 0.02409172893584128,
        "step": 187
    },
    {
        "loss": 2.2818,
        "grad_norm": 2.8055100440979004,
        "learning_rate": 9.890934646734148e-05,
        "epoch": 0.024220561710899254,
        "step": 188
    },
    {
        "loss": 2.6661,
        "grad_norm": 1.6490994691848755,
        "learning_rate": 9.889667456663477e-05,
        "epoch": 0.02434939448595723,
        "step": 189
    },
    {
        "loss": 1.7284,
        "grad_norm": 2.8300082683563232,
        "learning_rate": 9.888393029662113e-05,
        "epoch": 0.0244782272610152,
        "step": 190
    },
    {
        "loss": 2.1287,
        "grad_norm": 2.498300790786743,
        "learning_rate": 9.88711136761627e-05,
        "epoch": 0.024607060036073176,
        "step": 191
    },
    {
        "loss": 2.4108,
        "grad_norm": 1.7874544858932495,
        "learning_rate": 9.885822472422863e-05,
        "epoch": 0.02473589281113115,
        "step": 192
    },
    {
        "loss": 1.5206,
        "grad_norm": 3.338003396987915,
        "learning_rate": 9.884526345989518e-05,
        "epoch": 0.024864725586189126,
        "step": 193
    },
    {
        "loss": 2.515,
        "grad_norm": 1.6253325939178467,
        "learning_rate": 9.883222990234559e-05,
        "epoch": 0.0249935583612471,
        "step": 194
    },
    {
        "loss": 2.0704,
        "grad_norm": 2.6690545082092285,
        "learning_rate": 9.881912407087015e-05,
        "epoch": 0.025122391136305076,
        "step": 195
    },
    {
        "loss": 2.7156,
        "grad_norm": 1.2424780130386353,
        "learning_rate": 9.880594598486606e-05,
        "epoch": 0.02525122391136305,
        "step": 196
    },
    {
        "loss": 2.5369,
        "grad_norm": 1.775937795639038,
        "learning_rate": 9.879269566383749e-05,
        "epoch": 0.025380056686421026,
        "step": 197
    },
    {
        "loss": 2.3126,
        "grad_norm": 1.840282678604126,
        "learning_rate": 9.877937312739554e-05,
        "epoch": 0.025508889461479,
        "step": 198
    },
    {
        "loss": 2.2661,
        "grad_norm": 2.4005091190338135,
        "learning_rate": 9.876597839525814e-05,
        "epoch": 0.025637722236536976,
        "step": 199
    },
    {
        "loss": 2.3084,
        "grad_norm": 2.346816062927246,
        "learning_rate": 9.875251148725012e-05,
        "epoch": 0.02576655501159495,
        "step": 200
    },
    {
        "loss": 1.7541,
        "grad_norm": 1.732414960861206,
        "learning_rate": 9.873897242330312e-05,
        "epoch": 0.025895387786652926,
        "step": 201
    },
    {
        "loss": 2.0899,
        "grad_norm": 2.298980951309204,
        "learning_rate": 9.872536122345558e-05,
        "epoch": 0.0260242205617109,
        "step": 202
    },
    {
        "loss": 1.6814,
        "grad_norm": 3.092916250228882,
        "learning_rate": 9.871167790785267e-05,
        "epoch": 0.026153053336768876,
        "step": 203
    },
    {
        "loss": 2.4345,
        "grad_norm": 2.0739388465881348,
        "learning_rate": 9.869792249674637e-05,
        "epoch": 0.026281886111826847,
        "step": 204
    },
    {
        "loss": 2.7089,
        "grad_norm": 1.4846452474594116,
        "learning_rate": 9.868409501049525e-05,
        "epoch": 0.026410718886884822,
        "step": 205
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.49806547164917,
        "learning_rate": 9.867019546956466e-05,
        "epoch": 0.026539551661942797,
        "step": 206
    },
    {
        "loss": 1.6517,
        "grad_norm": 2.826106548309326,
        "learning_rate": 9.865622389452655e-05,
        "epoch": 0.026668384437000772,
        "step": 207
    },
    {
        "loss": 1.6911,
        "grad_norm": 2.3562371730804443,
        "learning_rate": 9.864218030605948e-05,
        "epoch": 0.026797217212058747,
        "step": 208
    },
    {
        "loss": 2.7773,
        "grad_norm": 1.2966783046722412,
        "learning_rate": 9.86280647249486e-05,
        "epoch": 0.026926049987116722,
        "step": 209
    },
    {
        "loss": 2.5461,
        "grad_norm": 1.5577003955841064,
        "learning_rate": 9.861387717208563e-05,
        "epoch": 0.027054882762174697,
        "step": 210
    },
    {
        "loss": 2.3389,
        "grad_norm": 2.2740402221679688,
        "learning_rate": 9.859961766846876e-05,
        "epoch": 0.027183715537232672,
        "step": 211
    },
    {
        "loss": 1.2367,
        "grad_norm": 2.9493331909179688,
        "learning_rate": 9.858528623520275e-05,
        "epoch": 0.027312548312290647,
        "step": 212
    },
    {
        "loss": 2.5948,
        "grad_norm": 1.741129755973816,
        "learning_rate": 9.857088289349876e-05,
        "epoch": 0.027441381087348622,
        "step": 213
    },
    {
        "loss": 1.7584,
        "grad_norm": 2.7739784717559814,
        "learning_rate": 9.855640766467438e-05,
        "epoch": 0.027570213862406597,
        "step": 214
    },
    {
        "loss": 2.5093,
        "grad_norm": 1.355846881866455,
        "learning_rate": 9.85418605701536e-05,
        "epoch": 0.027699046637464572,
        "step": 215
    },
    {
        "loss": 2.6451,
        "grad_norm": 1.5760631561279297,
        "learning_rate": 9.85272416314668e-05,
        "epoch": 0.027827879412522547,
        "step": 216
    },
    {
        "loss": 1.5784,
        "grad_norm": 2.6261796951293945,
        "learning_rate": 9.851255087025067e-05,
        "epoch": 0.027956712187580522,
        "step": 217
    },
    {
        "loss": 2.6478,
        "grad_norm": 2.1235506534576416,
        "learning_rate": 9.849778830824822e-05,
        "epoch": 0.028085544962638494,
        "step": 218
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.1804654598236084,
        "learning_rate": 9.84829539673087e-05,
        "epoch": 0.02821437773769647,
        "step": 219
    },
    {
        "loss": 2.5341,
        "grad_norm": 1.3374249935150146,
        "learning_rate": 9.84680478693876e-05,
        "epoch": 0.028343210512754444,
        "step": 220
    },
    {
        "loss": 2.3342,
        "grad_norm": 1.9632868766784668,
        "learning_rate": 9.845307003654664e-05,
        "epoch": 0.02847204328781242,
        "step": 221
    },
    {
        "loss": 2.7603,
        "grad_norm": 1.623720645904541,
        "learning_rate": 9.843802049095371e-05,
        "epoch": 0.028600876062870394,
        "step": 222
    },
    {
        "loss": 2.675,
        "grad_norm": 1.5666171312332153,
        "learning_rate": 9.84228992548828e-05,
        "epoch": 0.02872970883792837,
        "step": 223
    },
    {
        "loss": 1.6192,
        "grad_norm": 3.124058485031128,
        "learning_rate": 9.840770635071403e-05,
        "epoch": 0.028858541612986344,
        "step": 224
    },
    {
        "loss": 1.9062,
        "grad_norm": 1.8071651458740234,
        "learning_rate": 9.83924418009336e-05,
        "epoch": 0.02898737438804432,
        "step": 225
    },
    {
        "loss": 1.7124,
        "grad_norm": 3.6051034927368164,
        "learning_rate": 9.837710562813372e-05,
        "epoch": 0.029116207163102294,
        "step": 226
    },
    {
        "loss": 2.5395,
        "grad_norm": 1.5355483293533325,
        "learning_rate": 9.836169785501264e-05,
        "epoch": 0.02924503993816027,
        "step": 227
    },
    {
        "loss": 2.4069,
        "grad_norm": 1.7341563701629639,
        "learning_rate": 9.834621850437457e-05,
        "epoch": 0.029373872713218244,
        "step": 228
    },
    {
        "loss": 2.6208,
        "grad_norm": 1.8386459350585938,
        "learning_rate": 9.833066759912968e-05,
        "epoch": 0.02950270548827622,
        "step": 229
    },
    {
        "loss": 1.6015,
        "grad_norm": 2.2112507820129395,
        "learning_rate": 9.831504516229394e-05,
        "epoch": 0.029631538263334194,
        "step": 230
    },
    {
        "loss": 2.4328,
        "grad_norm": 1.794385313987732,
        "learning_rate": 9.829935121698936e-05,
        "epoch": 0.02976037103839217,
        "step": 231
    },
    {
        "loss": 2.2147,
        "grad_norm": 1.4962371587753296,
        "learning_rate": 9.828358578644364e-05,
        "epoch": 0.02988920381345014,
        "step": 232
    },
    {
        "loss": 1.3404,
        "grad_norm": 3.543750286102295,
        "learning_rate": 9.826774889399033e-05,
        "epoch": 0.030018036588508115,
        "step": 233
    },
    {
        "loss": 2.8619,
        "grad_norm": 1.6682931184768677,
        "learning_rate": 9.825184056306878e-05,
        "epoch": 0.03014686936356609,
        "step": 234
    },
    {
        "loss": 2.5193,
        "grad_norm": 2.423334836959839,
        "learning_rate": 9.823586081722404e-05,
        "epoch": 0.030275702138624065,
        "step": 235
    },
    {
        "loss": 2.5094,
        "grad_norm": 1.8858282566070557,
        "learning_rate": 9.821980968010686e-05,
        "epoch": 0.03040453491368204,
        "step": 236
    },
    {
        "loss": 1.91,
        "grad_norm": 2.581122398376465,
        "learning_rate": 9.820368717547364e-05,
        "epoch": 0.030533367688740015,
        "step": 237
    },
    {
        "loss": 2.8535,
        "grad_norm": 1.2071622610092163,
        "learning_rate": 9.818749332718645e-05,
        "epoch": 0.03066220046379799,
        "step": 238
    },
    {
        "loss": 2.1731,
        "grad_norm": 2.18746280670166,
        "learning_rate": 9.817122815921288e-05,
        "epoch": 0.030791033238855965,
        "step": 239
    },
    {
        "loss": 1.6558,
        "grad_norm": 3.0057694911956787,
        "learning_rate": 9.815489169562617e-05,
        "epoch": 0.03091986601391394,
        "step": 240
    },
    {
        "loss": 1.8584,
        "grad_norm": 2.5371124744415283,
        "learning_rate": 9.813848396060499e-05,
        "epoch": 0.031048698788971915,
        "step": 241
    },
    {
        "loss": 2.5326,
        "grad_norm": 1.4267857074737549,
        "learning_rate": 9.812200497843357e-05,
        "epoch": 0.03117753156402989,
        "step": 242
    },
    {
        "loss": 1.5652,
        "grad_norm": 2.927891969680786,
        "learning_rate": 9.810545477350153e-05,
        "epoch": 0.031306364339087865,
        "step": 243
    },
    {
        "loss": 2.259,
        "grad_norm": 2.539947032928467,
        "learning_rate": 9.808883337030392e-05,
        "epoch": 0.03143519711414584,
        "step": 244
    },
    {
        "loss": 1.929,
        "grad_norm": 3.0230653285980225,
        "learning_rate": 9.807214079344122e-05,
        "epoch": 0.031564029889203815,
        "step": 245
    },
    {
        "loss": 2.449,
        "grad_norm": 2.320887565612793,
        "learning_rate": 9.805537706761916e-05,
        "epoch": 0.03169286266426179,
        "step": 246
    },
    {
        "loss": 1.9938,
        "grad_norm": 2.637559175491333,
        "learning_rate": 9.803854221764883e-05,
        "epoch": 0.031821695439319765,
        "step": 247
    },
    {
        "loss": 2.4853,
        "grad_norm": 2.1494648456573486,
        "learning_rate": 9.802163626844658e-05,
        "epoch": 0.03195052821437774,
        "step": 248
    },
    {
        "loss": 2.2295,
        "grad_norm": 1.867652177810669,
        "learning_rate": 9.800465924503398e-05,
        "epoch": 0.032079360989435715,
        "step": 249
    },
    {
        "loss": 2.3859,
        "grad_norm": 1.5579535961151123,
        "learning_rate": 9.79876111725378e-05,
        "epoch": 0.03220819376449369,
        "step": 250
    },
    {
        "loss": 1.9944,
        "grad_norm": 3.3266825675964355,
        "learning_rate": 9.797049207618997e-05,
        "epoch": 0.032337026539551665,
        "step": 251
    },
    {
        "loss": 2.022,
        "grad_norm": 2.7589123249053955,
        "learning_rate": 9.795330198132753e-05,
        "epoch": 0.03246585931460964,
        "step": 252
    },
    {
        "loss": 1.9231,
        "grad_norm": 2.8094425201416016,
        "learning_rate": 9.793604091339259e-05,
        "epoch": 0.03259469208966761,
        "step": 253
    },
    {
        "loss": 2.2916,
        "grad_norm": 2.1056809425354004,
        "learning_rate": 9.791870889793235e-05,
        "epoch": 0.03272352486472559,
        "step": 254
    },
    {
        "loss": 2.3606,
        "grad_norm": 1.5380780696868896,
        "learning_rate": 9.790130596059895e-05,
        "epoch": 0.03285235763978356,
        "step": 255
    },
    {
        "loss": 2.1256,
        "grad_norm": 1.3429502248764038,
        "learning_rate": 9.788383212714953e-05,
        "epoch": 0.03298119041484154,
        "step": 256
    },
    {
        "loss": 2.0796,
        "grad_norm": 2.4963440895080566,
        "learning_rate": 9.78662874234462e-05,
        "epoch": 0.03311002318989951,
        "step": 257
    },
    {
        "loss": 2.6995,
        "grad_norm": 1.2643271684646606,
        "learning_rate": 9.784867187545588e-05,
        "epoch": 0.03323885596495749,
        "step": 258
    },
    {
        "loss": 2.1914,
        "grad_norm": 2.5876896381378174,
        "learning_rate": 9.78309855092504e-05,
        "epoch": 0.03336768874001546,
        "step": 259
    },
    {
        "loss": 2.6465,
        "grad_norm": 1.9129409790039062,
        "learning_rate": 9.781322835100638e-05,
        "epoch": 0.03349652151507344,
        "step": 260
    },
    {
        "loss": 2.225,
        "grad_norm": 2.066927671432495,
        "learning_rate": 9.779540042700523e-05,
        "epoch": 0.03362535429013141,
        "step": 261
    },
    {
        "loss": 2.342,
        "grad_norm": 2.694573163986206,
        "learning_rate": 9.77775017636331e-05,
        "epoch": 0.03375418706518939,
        "step": 262
    },
    {
        "loss": 2.2204,
        "grad_norm": 2.362710952758789,
        "learning_rate": 9.77595323873808e-05,
        "epoch": 0.03388301984024736,
        "step": 263
    },
    {
        "loss": 2.1287,
        "grad_norm": 2.6684012413024902,
        "learning_rate": 9.774149232484387e-05,
        "epoch": 0.03401185261530534,
        "step": 264
    },
    {
        "loss": 2.4128,
        "grad_norm": 1.9670354127883911,
        "learning_rate": 9.772338160272236e-05,
        "epoch": 0.03414068539036331,
        "step": 265
    },
    {
        "loss": 2.4479,
        "grad_norm": 1.5575095415115356,
        "learning_rate": 9.770520024782104e-05,
        "epoch": 0.03426951816542129,
        "step": 266
    },
    {
        "loss": 2.5535,
        "grad_norm": 1.681655764579773,
        "learning_rate": 9.768694828704908e-05,
        "epoch": 0.03439835094047926,
        "step": 267
    },
    {
        "loss": 2.5745,
        "grad_norm": 1.4342968463897705,
        "learning_rate": 9.766862574742024e-05,
        "epoch": 0.03452718371553723,
        "step": 268
    },
    {
        "loss": 2.8256,
        "grad_norm": 1.454925537109375,
        "learning_rate": 9.765023265605274e-05,
        "epoch": 0.03465601649059521,
        "step": 269
    },
    {
        "loss": 2.1103,
        "grad_norm": 2.5767786502838135,
        "learning_rate": 9.763176904016913e-05,
        "epoch": 0.03478484926565318,
        "step": 270
    },
    {
        "loss": 2.4075,
        "grad_norm": 1.6125696897506714,
        "learning_rate": 9.761323492709647e-05,
        "epoch": 0.03491368204071116,
        "step": 271
    },
    {
        "loss": 2.4045,
        "grad_norm": 1.9615153074264526,
        "learning_rate": 9.759463034426604e-05,
        "epoch": 0.03504251481576913,
        "step": 272
    },
    {
        "loss": 2.3195,
        "grad_norm": 1.6392372846603394,
        "learning_rate": 9.757595531921349e-05,
        "epoch": 0.03517134759082711,
        "step": 273
    },
    {
        "loss": 1.7326,
        "grad_norm": 1.9181932210922241,
        "learning_rate": 9.755720987957872e-05,
        "epoch": 0.03530018036588508,
        "step": 274
    },
    {
        "loss": 2.305,
        "grad_norm": 1.232383131980896,
        "learning_rate": 9.753839405310583e-05,
        "epoch": 0.03542901314094306,
        "step": 275
    },
    {
        "loss": 2.778,
        "grad_norm": 2.646930694580078,
        "learning_rate": 9.75195078676431e-05,
        "epoch": 0.03555784591600103,
        "step": 276
    },
    {
        "loss": 2.6158,
        "grad_norm": 1.897112488746643,
        "learning_rate": 9.750055135114293e-05,
        "epoch": 0.03568667869105901,
        "step": 277
    },
    {
        "loss": 2.6543,
        "grad_norm": 1.3222978115081787,
        "learning_rate": 9.748152453166184e-05,
        "epoch": 0.03581551146611698,
        "step": 278
    },
    {
        "loss": 2.4153,
        "grad_norm": 2.1292006969451904,
        "learning_rate": 9.74624274373604e-05,
        "epoch": 0.03594434424117496,
        "step": 279
    },
    {
        "loss": 2.2217,
        "grad_norm": 1.6857457160949707,
        "learning_rate": 9.744326009650317e-05,
        "epoch": 0.03607317701623293,
        "step": 280
    },
    {
        "loss": 2.6925,
        "grad_norm": 1.9095211029052734,
        "learning_rate": 9.742402253745868e-05,
        "epoch": 0.0362020097912909,
        "step": 281
    },
    {
        "loss": 2.6104,
        "grad_norm": 2.4899187088012695,
        "learning_rate": 9.740471478869938e-05,
        "epoch": 0.03633084256634888,
        "step": 282
    },
    {
        "loss": 2.1954,
        "grad_norm": 1.662325382232666,
        "learning_rate": 9.738533687880169e-05,
        "epoch": 0.03645967534140685,
        "step": 283
    },
    {
        "loss": 2.221,
        "grad_norm": 2.1843924522399902,
        "learning_rate": 9.73658888364457e-05,
        "epoch": 0.03658850811646483,
        "step": 284
    },
    {
        "loss": 2.5749,
        "grad_norm": 1.8459324836730957,
        "learning_rate": 9.734637069041548e-05,
        "epoch": 0.0367173408915228,
        "step": 285
    },
    {
        "loss": 2.1231,
        "grad_norm": 2.210395097732544,
        "learning_rate": 9.732678246959874e-05,
        "epoch": 0.03684617366658078,
        "step": 286
    },
    {
        "loss": 2.76,
        "grad_norm": 1.714654564857483,
        "learning_rate": 9.730712420298696e-05,
        "epoch": 0.03697500644163875,
        "step": 287
    },
    {
        "loss": 2.6216,
        "grad_norm": 2.7616448402404785,
        "learning_rate": 9.728739591967522e-05,
        "epoch": 0.03710383921669673,
        "step": 288
    },
    {
        "loss": 1.9029,
        "grad_norm": 2.36130428314209,
        "learning_rate": 9.726759764886233e-05,
        "epoch": 0.0372326719917547,
        "step": 289
    },
    {
        "loss": 1.6496,
        "grad_norm": 2.827995538711548,
        "learning_rate": 9.724772941985064e-05,
        "epoch": 0.03736150476681268,
        "step": 290
    },
    {
        "loss": 2.3834,
        "grad_norm": 3.0785138607025146,
        "learning_rate": 9.722779126204598e-05,
        "epoch": 0.03749033754187065,
        "step": 291
    },
    {
        "loss": 2.3192,
        "grad_norm": 2.119633436203003,
        "learning_rate": 9.720778320495776e-05,
        "epoch": 0.03761917031692863,
        "step": 292
    },
    {
        "loss": 1.9974,
        "grad_norm": 2.117812395095825,
        "learning_rate": 9.718770527819883e-05,
        "epoch": 0.0377480030919866,
        "step": 293
    },
    {
        "loss": 2.5981,
        "grad_norm": 1.4152073860168457,
        "learning_rate": 9.716755751148544e-05,
        "epoch": 0.03787683586704458,
        "step": 294
    },
    {
        "loss": 1.988,
        "grad_norm": 2.7845468521118164,
        "learning_rate": 9.714733993463716e-05,
        "epoch": 0.03800566864210255,
        "step": 295
    },
    {
        "loss": 2.2154,
        "grad_norm": 2.426948308944702,
        "learning_rate": 9.712705257757698e-05,
        "epoch": 0.03813450141716052,
        "step": 296
    },
    {
        "loss": 1.5356,
        "grad_norm": 2.8507332801818848,
        "learning_rate": 9.710669547033108e-05,
        "epoch": 0.0382633341922185,
        "step": 297
    },
    {
        "loss": 0.9817,
        "grad_norm": 2.766404390335083,
        "learning_rate": 9.708626864302891e-05,
        "epoch": 0.03839216696727647,
        "step": 298
    },
    {
        "loss": 2.4722,
        "grad_norm": 1.9841073751449585,
        "learning_rate": 9.706577212590311e-05,
        "epoch": 0.03852099974233445,
        "step": 299
    },
    {
        "loss": 1.9555,
        "grad_norm": 1.8321027755737305,
        "learning_rate": 9.704520594928946e-05,
        "epoch": 0.03864983251739242,
        "step": 300
    },
    {
        "loss": 2.0513,
        "grad_norm": 2.0059432983398438,
        "learning_rate": 9.702457014362685e-05,
        "epoch": 0.0387786652924504,
        "step": 301
    },
    {
        "loss": 1.9771,
        "grad_norm": 2.8651223182678223,
        "learning_rate": 9.700386473945718e-05,
        "epoch": 0.03890749806750837,
        "step": 302
    },
    {
        "loss": 2.6871,
        "grad_norm": 2.48543643951416,
        "learning_rate": 9.698308976742543e-05,
        "epoch": 0.03903633084256635,
        "step": 303
    },
    {
        "loss": 2.3161,
        "grad_norm": 1.8976417779922485,
        "learning_rate": 9.696224525827948e-05,
        "epoch": 0.03916516361762432,
        "step": 304
    },
    {
        "loss": 1.2804,
        "grad_norm": 3.147246837615967,
        "learning_rate": 9.694133124287017e-05,
        "epoch": 0.0392939963926823,
        "step": 305
    },
    {
        "loss": 2.1852,
        "grad_norm": 2.405331611633301,
        "learning_rate": 9.692034775215117e-05,
        "epoch": 0.03942282916774027,
        "step": 306
    },
    {
        "loss": 2.383,
        "grad_norm": 1.9618066549301147,
        "learning_rate": 9.689929481717903e-05,
        "epoch": 0.03955166194279825,
        "step": 307
    },
    {
        "loss": 2.378,
        "grad_norm": 1.814038634300232,
        "learning_rate": 9.687817246911305e-05,
        "epoch": 0.03968049471785622,
        "step": 308
    },
    {
        "loss": 2.0892,
        "grad_norm": 2.099809408187866,
        "learning_rate": 9.685698073921526e-05,
        "epoch": 0.039809327492914194,
        "step": 309
    },
    {
        "loss": 2.1638,
        "grad_norm": 2.594802141189575,
        "learning_rate": 9.683571965885037e-05,
        "epoch": 0.03993816026797217,
        "step": 310
    },
    {
        "loss": 2.2245,
        "grad_norm": 2.36421537399292,
        "learning_rate": 9.681438925948578e-05,
        "epoch": 0.040066993043030144,
        "step": 311
    },
    {
        "loss": 1.8299,
        "grad_norm": 2.6268551349639893,
        "learning_rate": 9.679298957269144e-05,
        "epoch": 0.04019582581808812,
        "step": 312
    },
    {
        "loss": 1.5723,
        "grad_norm": 2.3794503211975098,
        "learning_rate": 9.677152063013986e-05,
        "epoch": 0.040324658593146094,
        "step": 313
    },
    {
        "loss": 2.113,
        "grad_norm": 1.8759711980819702,
        "learning_rate": 9.674998246360605e-05,
        "epoch": 0.04045349136820407,
        "step": 314
    },
    {
        "loss": 2.2213,
        "grad_norm": 2.6588594913482666,
        "learning_rate": 9.672837510496749e-05,
        "epoch": 0.040582324143262044,
        "step": 315
    },
    {
        "loss": 1.449,
        "grad_norm": 2.185800075531006,
        "learning_rate": 9.670669858620403e-05,
        "epoch": 0.04071115691832002,
        "step": 316
    },
    {
        "loss": 2.2124,
        "grad_norm": 1.9183632135391235,
        "learning_rate": 9.668495293939793e-05,
        "epoch": 0.040839989693377994,
        "step": 317
    },
    {
        "loss": 2.444,
        "grad_norm": 1.9879387617111206,
        "learning_rate": 9.666313819673374e-05,
        "epoch": 0.04096882246843597,
        "step": 318
    },
    {
        "loss": 2.4006,
        "grad_norm": 2.4751498699188232,
        "learning_rate": 9.664125439049826e-05,
        "epoch": 0.041097655243493944,
        "step": 319
    },
    {
        "loss": 2.3898,
        "grad_norm": 1.9050450325012207,
        "learning_rate": 9.661930155308052e-05,
        "epoch": 0.04122648801855192,
        "step": 320
    },
    {
        "loss": 2.6452,
        "grad_norm": 1.7545140981674194,
        "learning_rate": 9.659727971697174e-05,
        "epoch": 0.041355320793609894,
        "step": 321
    },
    {
        "loss": 2.0978,
        "grad_norm": 2.3598735332489014,
        "learning_rate": 9.657518891476521e-05,
        "epoch": 0.04148415356866787,
        "step": 322
    },
    {
        "loss": 2.2247,
        "grad_norm": 2.0514423847198486,
        "learning_rate": 9.655302917915635e-05,
        "epoch": 0.041612986343725844,
        "step": 323
    },
    {
        "loss": 2.0085,
        "grad_norm": 2.4627814292907715,
        "learning_rate": 9.653080054294256e-05,
        "epoch": 0.041741819118783816,
        "step": 324
    },
    {
        "loss": 1.7519,
        "grad_norm": 2.9954006671905518,
        "learning_rate": 9.650850303902326e-05,
        "epoch": 0.041870651893841794,
        "step": 325
    },
    {
        "loss": 2.1916,
        "grad_norm": 2.4761571884155273,
        "learning_rate": 9.648613670039975e-05,
        "epoch": 0.041999484668899766,
        "step": 326
    },
    {
        "loss": 2.379,
        "grad_norm": 1.7400271892547607,
        "learning_rate": 9.646370156017523e-05,
        "epoch": 0.042128317443957744,
        "step": 327
    },
    {
        "loss": 2.354,
        "grad_norm": 2.1633825302124023,
        "learning_rate": 9.644119765155474e-05,
        "epoch": 0.042257150219015716,
        "step": 328
    },
    {
        "loss": 2.5492,
        "grad_norm": 1.5718698501586914,
        "learning_rate": 9.641862500784509e-05,
        "epoch": 0.042385982994073694,
        "step": 329
    },
    {
        "loss": 2.1313,
        "grad_norm": 1.9941308498382568,
        "learning_rate": 9.639598366245481e-05,
        "epoch": 0.042514815769131666,
        "step": 330
    },
    {
        "loss": 2.2075,
        "grad_norm": 2.2817559242248535,
        "learning_rate": 9.637327364889415e-05,
        "epoch": 0.042643648544189644,
        "step": 331
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.3595399856567383,
        "learning_rate": 9.635049500077493e-05,
        "epoch": 0.042772481319247616,
        "step": 332
    },
    {
        "loss": 2.3321,
        "grad_norm": 1.834457278251648,
        "learning_rate": 9.632764775181062e-05,
        "epoch": 0.042901314094305594,
        "step": 333
    },
    {
        "loss": 2.5945,
        "grad_norm": 1.4955490827560425,
        "learning_rate": 9.630473193581616e-05,
        "epoch": 0.043030146869363566,
        "step": 334
    },
    {
        "loss": 2.3773,
        "grad_norm": 2.4020769596099854,
        "learning_rate": 9.628174758670803e-05,
        "epoch": 0.043158979644421544,
        "step": 335
    },
    {
        "loss": 2.1665,
        "grad_norm": 2.2930755615234375,
        "learning_rate": 9.625869473850408e-05,
        "epoch": 0.043287812419479516,
        "step": 336
    },
    {
        "loss": 1.7539,
        "grad_norm": 2.5869998931884766,
        "learning_rate": 9.623557342532361e-05,
        "epoch": 0.04341664519453749,
        "step": 337
    },
    {
        "loss": 2.1113,
        "grad_norm": 2.464662551879883,
        "learning_rate": 9.621238368138719e-05,
        "epoch": 0.043545477969595466,
        "step": 338
    },
    {
        "loss": 2.6536,
        "grad_norm": 1.7798893451690674,
        "learning_rate": 9.618912554101673e-05,
        "epoch": 0.04367431074465344,
        "step": 339
    },
    {
        "loss": 2.3391,
        "grad_norm": 1.935857892036438,
        "learning_rate": 9.616579903863532e-05,
        "epoch": 0.043803143519711416,
        "step": 340
    },
    {
        "loss": 2.0501,
        "grad_norm": 2.3672187328338623,
        "learning_rate": 9.614240420876724e-05,
        "epoch": 0.04393197629476939,
        "step": 341
    },
    {
        "loss": 2.3362,
        "grad_norm": 1.35234534740448,
        "learning_rate": 9.611894108603789e-05,
        "epoch": 0.044060809069827366,
        "step": 342
    },
    {
        "loss": 2.5713,
        "grad_norm": 1.2733700275421143,
        "learning_rate": 9.60954097051738e-05,
        "epoch": 0.04418964184488534,
        "step": 343
    },
    {
        "loss": 2.4962,
        "grad_norm": 1.3482248783111572,
        "learning_rate": 9.607181010100248e-05,
        "epoch": 0.044318474619943315,
        "step": 344
    },
    {
        "loss": 2.2755,
        "grad_norm": 2.832735300064087,
        "learning_rate": 9.604814230845239e-05,
        "epoch": 0.04444730739500129,
        "step": 345
    },
    {
        "loss": 2.1209,
        "grad_norm": 2.2196929454803467,
        "learning_rate": 9.602440636255297e-05,
        "epoch": 0.044576140170059265,
        "step": 346
    },
    {
        "loss": 2.2095,
        "grad_norm": 1.3352220058441162,
        "learning_rate": 9.600060229843448e-05,
        "epoch": 0.04470497294511724,
        "step": 347
    },
    {
        "loss": 2.0133,
        "grad_norm": 2.739895820617676,
        "learning_rate": 9.597673015132805e-05,
        "epoch": 0.044833805720175215,
        "step": 348
    },
    {
        "loss": 2.3782,
        "grad_norm": 1.6473448276519775,
        "learning_rate": 9.59527899565655e-05,
        "epoch": 0.04496263849523319,
        "step": 349
    },
    {
        "loss": 2.3927,
        "grad_norm": 1.6259515285491943,
        "learning_rate": 9.592878174957944e-05,
        "epoch": 0.045091471270291165,
        "step": 350
    },
    {
        "loss": 2.6162,
        "grad_norm": 2.0452096462249756,
        "learning_rate": 9.590470556590312e-05,
        "epoch": 0.04522030404534914,
        "step": 351
    },
    {
        "loss": 2.5085,
        "grad_norm": 1.94228196144104,
        "learning_rate": 9.588056144117037e-05,
        "epoch": 0.04534913682040711,
        "step": 352
    },
    {
        "loss": 1.8865,
        "grad_norm": 2.2288870811462402,
        "learning_rate": 9.585634941111561e-05,
        "epoch": 0.04547796959546509,
        "step": 353
    },
    {
        "loss": 2.2077,
        "grad_norm": 1.7267652750015259,
        "learning_rate": 9.583206951157373e-05,
        "epoch": 0.04560680237052306,
        "step": 354
    },
    {
        "loss": 2.3319,
        "grad_norm": 1.797366976737976,
        "learning_rate": 9.580772177848008e-05,
        "epoch": 0.04573563514558104,
        "step": 355
    },
    {
        "loss": 1.7715,
        "grad_norm": 2.6134040355682373,
        "learning_rate": 9.578330624787046e-05,
        "epoch": 0.04586446792063901,
        "step": 356
    },
    {
        "loss": 1.6028,
        "grad_norm": 1.6915489435195923,
        "learning_rate": 9.575882295588092e-05,
        "epoch": 0.04599330069569699,
        "step": 357
    },
    {
        "loss": 2.6143,
        "grad_norm": 1.2526874542236328,
        "learning_rate": 9.573427193874788e-05,
        "epoch": 0.04612213347075496,
        "step": 358
    },
    {
        "loss": 2.4152,
        "grad_norm": 2.202521800994873,
        "learning_rate": 9.570965323280793e-05,
        "epoch": 0.04625096624581294,
        "step": 359
    },
    {
        "loss": 2.5261,
        "grad_norm": 1.3961424827575684,
        "learning_rate": 9.568496687449793e-05,
        "epoch": 0.04637979902087091,
        "step": 360
    },
    {
        "loss": 2.1004,
        "grad_norm": 2.4647765159606934,
        "learning_rate": 9.566021290035476e-05,
        "epoch": 0.04650863179592889,
        "step": 361
    },
    {
        "loss": 2.2238,
        "grad_norm": 2.2401275634765625,
        "learning_rate": 9.563539134701545e-05,
        "epoch": 0.04663746457098686,
        "step": 362
    },
    {
        "loss": 1.8943,
        "grad_norm": 1.3452895879745483,
        "learning_rate": 9.561050225121704e-05,
        "epoch": 0.04676629734604484,
        "step": 363
    },
    {
        "loss": 2.6966,
        "grad_norm": 1.6020323038101196,
        "learning_rate": 9.558554564979653e-05,
        "epoch": 0.04689513012110281,
        "step": 364
    },
    {
        "loss": 1.9568,
        "grad_norm": 2.520453691482544,
        "learning_rate": 9.556052157969081e-05,
        "epoch": 0.04702396289616078,
        "step": 365
    },
    {
        "loss": 2.1195,
        "grad_norm": 2.550922155380249,
        "learning_rate": 9.553543007793666e-05,
        "epoch": 0.04715279567121876,
        "step": 366
    },
    {
        "loss": 2.1369,
        "grad_norm": 1.7074614763259888,
        "learning_rate": 9.551027118167064e-05,
        "epoch": 0.04728162844627673,
        "step": 367
    },
    {
        "loss": 2.6929,
        "grad_norm": 1.2977452278137207,
        "learning_rate": 9.548504492812905e-05,
        "epoch": 0.04741046122133471,
        "step": 368
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.1961748600006104,
        "learning_rate": 9.545975135464791e-05,
        "epoch": 0.04753929399639268,
        "step": 369
    },
    {
        "loss": 1.9393,
        "grad_norm": 2.9073009490966797,
        "learning_rate": 9.543439049866285e-05,
        "epoch": 0.04766812677145066,
        "step": 370
    },
    {
        "loss": 2.5055,
        "grad_norm": 1.6481624841690063,
        "learning_rate": 9.540896239770911e-05,
        "epoch": 0.04779695954650863,
        "step": 371
    },
    {
        "loss": 1.8546,
        "grad_norm": 1.2299118041992188,
        "learning_rate": 9.538346708942143e-05,
        "epoch": 0.04792579232156661,
        "step": 372
    },
    {
        "loss": 2.2508,
        "grad_norm": 1.6186882257461548,
        "learning_rate": 9.535790461153403e-05,
        "epoch": 0.04805462509662458,
        "step": 373
    },
    {
        "loss": 2.1504,
        "grad_norm": 1.952687382698059,
        "learning_rate": 9.533227500188052e-05,
        "epoch": 0.04818345787168256,
        "step": 374
    },
    {
        "loss": 2.3711,
        "grad_norm": 1.6071956157684326,
        "learning_rate": 9.530657829839394e-05,
        "epoch": 0.04831229064674053,
        "step": 375
    },
    {
        "loss": 2.2026,
        "grad_norm": 1.7979952096939087,
        "learning_rate": 9.528081453910652e-05,
        "epoch": 0.04844112342179851,
        "step": 376
    },
    {
        "loss": 2.4801,
        "grad_norm": 1.5039759874343872,
        "learning_rate": 9.525498376214985e-05,
        "epoch": 0.04856995619685648,
        "step": 377
    },
    {
        "loss": 2.1299,
        "grad_norm": 2.018735647201538,
        "learning_rate": 9.522908600575462e-05,
        "epoch": 0.04869878897191446,
        "step": 378
    },
    {
        "loss": 1.4778,
        "grad_norm": 2.450967311859131,
        "learning_rate": 9.520312130825073e-05,
        "epoch": 0.04882762174697243,
        "step": 379
    },
    {
        "loss": 2.6561,
        "grad_norm": 2.287147045135498,
        "learning_rate": 9.517708970806708e-05,
        "epoch": 0.0489564545220304,
        "step": 380
    },
    {
        "loss": 1.9415,
        "grad_norm": 1.7861909866333008,
        "learning_rate": 9.515099124373164e-05,
        "epoch": 0.04908528729708838,
        "step": 381
    },
    {
        "loss": 2.0111,
        "grad_norm": 2.3687429428100586,
        "learning_rate": 9.512482595387132e-05,
        "epoch": 0.04921412007214635,
        "step": 382
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.9014283418655396,
        "learning_rate": 9.509859387721195e-05,
        "epoch": 0.04934295284720433,
        "step": 383
    },
    {
        "loss": 2.1123,
        "grad_norm": 2.30224347114563,
        "learning_rate": 9.50722950525782e-05,
        "epoch": 0.0494717856222623,
        "step": 384
    },
    {
        "loss": 2.0406,
        "grad_norm": 2.2940547466278076,
        "learning_rate": 9.50459295188935e-05,
        "epoch": 0.04960061839732028,
        "step": 385
    },
    {
        "loss": 2.2074,
        "grad_norm": 2.1864805221557617,
        "learning_rate": 9.501949731518009e-05,
        "epoch": 0.04972945117237825,
        "step": 386
    },
    {
        "loss": 2.0561,
        "grad_norm": 2.674921989440918,
        "learning_rate": 9.49929984805588e-05,
        "epoch": 0.04985828394743623,
        "step": 387
    },
    {
        "loss": 2.0181,
        "grad_norm": 2.349710702896118,
        "learning_rate": 9.496643305424914e-05,
        "epoch": 0.0499871167224942,
        "step": 388
    },
    {
        "loss": 1.545,
        "grad_norm": 3.196490526199341,
        "learning_rate": 9.493980107556913e-05,
        "epoch": 0.05011594949755218,
        "step": 389
    },
    {
        "loss": 2.0547,
        "grad_norm": 2.490755319595337,
        "learning_rate": 9.491310258393531e-05,
        "epoch": 0.05024478227261015,
        "step": 390
    },
    {
        "loss": 2.5405,
        "grad_norm": 1.4740618467330933,
        "learning_rate": 9.488633761886268e-05,
        "epoch": 0.05037361504766813,
        "step": 391
    },
    {
        "loss": 2.0492,
        "grad_norm": 3.0351901054382324,
        "learning_rate": 9.48595062199646e-05,
        "epoch": 0.0505024478227261,
        "step": 392
    },
    {
        "loss": 2.4482,
        "grad_norm": 1.3913936614990234,
        "learning_rate": 9.483260842695274e-05,
        "epoch": 0.05063128059778407,
        "step": 393
    },
    {
        "loss": 2.4819,
        "grad_norm": 1.858818769454956,
        "learning_rate": 9.480564427963711e-05,
        "epoch": 0.05076011337284205,
        "step": 394
    },
    {
        "loss": 2.2367,
        "grad_norm": 1.8825860023498535,
        "learning_rate": 9.477861381792586e-05,
        "epoch": 0.05088894614790002,
        "step": 395
    },
    {
        "loss": 2.6933,
        "grad_norm": 1.6352272033691406,
        "learning_rate": 9.475151708182528e-05,
        "epoch": 0.051017778922958,
        "step": 396
    },
    {
        "loss": 2.1909,
        "grad_norm": 2.0427889823913574,
        "learning_rate": 9.472435411143978e-05,
        "epoch": 0.05114661169801597,
        "step": 397
    },
    {
        "loss": 1.5698,
        "grad_norm": 2.93774676322937,
        "learning_rate": 9.469712494697183e-05,
        "epoch": 0.05127544447307395,
        "step": 398
    },
    {
        "loss": 2.1742,
        "grad_norm": 1.7940478324890137,
        "learning_rate": 9.46698296287218e-05,
        "epoch": 0.05140427724813192,
        "step": 399
    },
    {
        "loss": 2.7238,
        "grad_norm": 1.596698522567749,
        "learning_rate": 9.4642468197088e-05,
        "epoch": 0.0515331100231899,
        "step": 400
    },
    {
        "loss": 1.8992,
        "grad_norm": 2.1962547302246094,
        "learning_rate": 9.461504069256661e-05,
        "epoch": 0.05166194279824787,
        "step": 401
    },
    {
        "loss": 2.1763,
        "grad_norm": 1.5486533641815186,
        "learning_rate": 9.45875471557516e-05,
        "epoch": 0.05179077557330585,
        "step": 402
    },
    {
        "loss": 2.3665,
        "grad_norm": 1.602953553199768,
        "learning_rate": 9.455998762733465e-05,
        "epoch": 0.05191960834836382,
        "step": 403
    },
    {
        "loss": 2.069,
        "grad_norm": 2.138432741165161,
        "learning_rate": 9.45323621481051e-05,
        "epoch": 0.0520484411234218,
        "step": 404
    },
    {
        "loss": 1.755,
        "grad_norm": 2.6954030990600586,
        "learning_rate": 9.450467075894993e-05,
        "epoch": 0.05217727389847977,
        "step": 405
    },
    {
        "loss": 2.1396,
        "grad_norm": 1.552453637123108,
        "learning_rate": 9.447691350085366e-05,
        "epoch": 0.05230610667353775,
        "step": 406
    },
    {
        "loss": 1.4667,
        "grad_norm": 2.7945621013641357,
        "learning_rate": 9.444909041489831e-05,
        "epoch": 0.05243493944859572,
        "step": 407
    },
    {
        "loss": 2.5128,
        "grad_norm": 1.5099761486053467,
        "learning_rate": 9.442120154226329e-05,
        "epoch": 0.052563772223653694,
        "step": 408
    },
    {
        "loss": 2.4276,
        "grad_norm": 2.3276829719543457,
        "learning_rate": 9.439324692422543e-05,
        "epoch": 0.05269260499871167,
        "step": 409
    },
    {
        "loss": 2.2377,
        "grad_norm": 2.1472525596618652,
        "learning_rate": 9.43652266021588e-05,
        "epoch": 0.052821437773769644,
        "step": 410
    },
    {
        "loss": 2.3538,
        "grad_norm": 1.76508629322052,
        "learning_rate": 9.433714061753478e-05,
        "epoch": 0.05295027054882762,
        "step": 411
    },
    {
        "loss": 2.2144,
        "grad_norm": 1.7811897993087769,
        "learning_rate": 9.430898901192191e-05,
        "epoch": 0.053079103323885594,
        "step": 412
    },
    {
        "loss": 1.3931,
        "grad_norm": 2.5871288776397705,
        "learning_rate": 9.428077182698584e-05,
        "epoch": 0.05320793609894357,
        "step": 413
    },
    {
        "loss": 2.4222,
        "grad_norm": 2.413856029510498,
        "learning_rate": 9.42524891044893e-05,
        "epoch": 0.053336768874001544,
        "step": 414
    },
    {
        "loss": 2.0081,
        "grad_norm": 2.7436115741729736,
        "learning_rate": 9.422414088629198e-05,
        "epoch": 0.05346560164905952,
        "step": 415
    },
    {
        "loss": 2.1898,
        "grad_norm": 1.713653326034546,
        "learning_rate": 9.419572721435057e-05,
        "epoch": 0.053594434424117494,
        "step": 416
    },
    {
        "loss": 2.2573,
        "grad_norm": 2.1584150791168213,
        "learning_rate": 9.416724813071859e-05,
        "epoch": 0.05372326719917547,
        "step": 417
    },
    {
        "loss": 1.8368,
        "grad_norm": 3.3964571952819824,
        "learning_rate": 9.413870367754637e-05,
        "epoch": 0.053852099974233444,
        "step": 418
    },
    {
        "loss": 2.325,
        "grad_norm": 1.7967345714569092,
        "learning_rate": 9.4110093897081e-05,
        "epoch": 0.05398093274929142,
        "step": 419
    },
    {
        "loss": 2.339,
        "grad_norm": 2.626575469970703,
        "learning_rate": 9.408141883166628e-05,
        "epoch": 0.054109765524349394,
        "step": 420
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.91751229763031,
        "learning_rate": 9.40526785237426e-05,
        "epoch": 0.054238598299407366,
        "step": 421
    },
    {
        "loss": 2.3237,
        "grad_norm": 1.420390009880066,
        "learning_rate": 9.402387301584694e-05,
        "epoch": 0.054367431074465344,
        "step": 422
    },
    {
        "loss": 2.3671,
        "grad_norm": 1.7803512811660767,
        "learning_rate": 9.399500235061274e-05,
        "epoch": 0.054496263849523316,
        "step": 423
    },
    {
        "loss": 2.1033,
        "grad_norm": 2.424541473388672,
        "learning_rate": 9.396606657076992e-05,
        "epoch": 0.054625096624581294,
        "step": 424
    },
    {
        "loss": 2.7017,
        "grad_norm": 2.0273425579071045,
        "learning_rate": 9.393706571914473e-05,
        "epoch": 0.054753929399639266,
        "step": 425
    },
    {
        "loss": 2.4869,
        "grad_norm": 1.6429156064987183,
        "learning_rate": 9.390799983865978e-05,
        "epoch": 0.054882762174697244,
        "step": 426
    },
    {
        "loss": 1.5081,
        "grad_norm": 2.4589309692382812,
        "learning_rate": 9.387886897233387e-05,
        "epoch": 0.055011594949755216,
        "step": 427
    },
    {
        "loss": 2.2917,
        "grad_norm": 2.4460737705230713,
        "learning_rate": 9.384967316328204e-05,
        "epoch": 0.055140427724813194,
        "step": 428
    },
    {
        "loss": 2.4032,
        "grad_norm": 1.892926573753357,
        "learning_rate": 9.38204124547154e-05,
        "epoch": 0.055269260499871166,
        "step": 429
    },
    {
        "loss": 2.4628,
        "grad_norm": 1.1673970222473145,
        "learning_rate": 9.379108688994113e-05,
        "epoch": 0.055398093274929144,
        "step": 430
    },
    {
        "loss": 2.0456,
        "grad_norm": 2.08260178565979,
        "learning_rate": 9.376169651236242e-05,
        "epoch": 0.055526926049987116,
        "step": 431
    },
    {
        "loss": 2.0865,
        "grad_norm": 1.4142444133758545,
        "learning_rate": 9.373224136547833e-05,
        "epoch": 0.055655758825045094,
        "step": 432
    },
    {
        "loss": 2.2113,
        "grad_norm": 1.7332510948181152,
        "learning_rate": 9.370272149288387e-05,
        "epoch": 0.055784591600103066,
        "step": 433
    },
    {
        "loss": 2.2285,
        "grad_norm": 1.9287973642349243,
        "learning_rate": 9.367313693826977e-05,
        "epoch": 0.055913424375161044,
        "step": 434
    },
    {
        "loss": 1.8817,
        "grad_norm": 2.1347496509552,
        "learning_rate": 9.364348774542251e-05,
        "epoch": 0.056042257150219016,
        "step": 435
    },
    {
        "loss": 2.3023,
        "grad_norm": 1.4826109409332275,
        "learning_rate": 9.361377395822428e-05,
        "epoch": 0.05617108992527699,
        "step": 436
    },
    {
        "loss": 2.4229,
        "grad_norm": 1.8641473054885864,
        "learning_rate": 9.35839956206528e-05,
        "epoch": 0.056299922700334966,
        "step": 437
    },
    {
        "loss": 2.6243,
        "grad_norm": 1.7202672958374023,
        "learning_rate": 9.355415277678138e-05,
        "epoch": 0.05642875547539294,
        "step": 438
    },
    {
        "loss": 2.5672,
        "grad_norm": 1.3747495412826538,
        "learning_rate": 9.35242454707788e-05,
        "epoch": 0.056557588250450916,
        "step": 439
    },
    {
        "loss": 2.4722,
        "grad_norm": 1.370765209197998,
        "learning_rate": 9.349427374690924e-05,
        "epoch": 0.05668642102550889,
        "step": 440
    },
    {
        "loss": 2.2293,
        "grad_norm": 1.5028313398361206,
        "learning_rate": 9.34642376495322e-05,
        "epoch": 0.056815253800566866,
        "step": 441
    },
    {
        "loss": 2.5301,
        "grad_norm": 2.058635950088501,
        "learning_rate": 9.343413722310248e-05,
        "epoch": 0.05694408657562484,
        "step": 442
    },
    {
        "loss": 2.3336,
        "grad_norm": 2.19814395904541,
        "learning_rate": 9.340397251217009e-05,
        "epoch": 0.057072919350682816,
        "step": 443
    },
    {
        "loss": 1.3499,
        "grad_norm": 2.7362964153289795,
        "learning_rate": 9.337374356138015e-05,
        "epoch": 0.05720175212574079,
        "step": 444
    },
    {
        "loss": 2.3411,
        "grad_norm": 1.5135916471481323,
        "learning_rate": 9.334345041547294e-05,
        "epoch": 0.057330584900798766,
        "step": 445
    },
    {
        "loss": 2.3975,
        "grad_norm": 1.6574194431304932,
        "learning_rate": 9.331309311928365e-05,
        "epoch": 0.05745941767585674,
        "step": 446
    },
    {
        "loss": 2.7758,
        "grad_norm": 1.9025955200195312,
        "learning_rate": 9.328267171774246e-05,
        "epoch": 0.057588250450914716,
        "step": 447
    },
    {
        "loss": 2.0841,
        "grad_norm": 1.876151442527771,
        "learning_rate": 9.325218625587445e-05,
        "epoch": 0.05771708322597269,
        "step": 448
    },
    {
        "loss": 2.2919,
        "grad_norm": 1.4259675741195679,
        "learning_rate": 9.322163677879948e-05,
        "epoch": 0.05784591600103066,
        "step": 449
    },
    {
        "loss": 2.4184,
        "grad_norm": 2.045254945755005,
        "learning_rate": 9.319102333173219e-05,
        "epoch": 0.05797474877608864,
        "step": 450
    },
    {
        "loss": 2.1606,
        "grad_norm": 1.8217978477478027,
        "learning_rate": 9.316034595998185e-05,
        "epoch": 0.05810358155114661,
        "step": 451
    },
    {
        "loss": 2.6552,
        "grad_norm": 1.7312514781951904,
        "learning_rate": 9.31296047089524e-05,
        "epoch": 0.05823241432620459,
        "step": 452
    },
    {
        "loss": 2.1418,
        "grad_norm": 2.305225372314453,
        "learning_rate": 9.309879962414225e-05,
        "epoch": 0.05836124710126256,
        "step": 453
    },
    {
        "loss": 1.622,
        "grad_norm": 2.8968427181243896,
        "learning_rate": 9.306793075114436e-05,
        "epoch": 0.05849007987632054,
        "step": 454
    },
    {
        "loss": 1.8239,
        "grad_norm": 2.2477121353149414,
        "learning_rate": 9.303699813564607e-05,
        "epoch": 0.05861891265137851,
        "step": 455
    },
    {
        "loss": 2.0869,
        "grad_norm": 1.8565280437469482,
        "learning_rate": 9.300600182342905e-05,
        "epoch": 0.05874774542643649,
        "step": 456
    },
    {
        "loss": 2.4455,
        "grad_norm": 1.2011194229125977,
        "learning_rate": 9.297494186036926e-05,
        "epoch": 0.05887657820149446,
        "step": 457
    },
    {
        "loss": 2.3536,
        "grad_norm": 2.190199136734009,
        "learning_rate": 9.294381829243686e-05,
        "epoch": 0.05900541097655244,
        "step": 458
    },
    {
        "loss": 2.2223,
        "grad_norm": 2.2272307872772217,
        "learning_rate": 9.291263116569614e-05,
        "epoch": 0.05913424375161041,
        "step": 459
    },
    {
        "loss": 2.518,
        "grad_norm": 1.438349723815918,
        "learning_rate": 9.288138052630549e-05,
        "epoch": 0.05926307652666839,
        "step": 460
    },
    {
        "loss": 2.495,
        "grad_norm": 2.2215731143951416,
        "learning_rate": 9.285006642051726e-05,
        "epoch": 0.05939190930172636,
        "step": 461
    },
    {
        "loss": 2.1265,
        "grad_norm": 1.893356442451477,
        "learning_rate": 9.281868889467776e-05,
        "epoch": 0.05952074207678434,
        "step": 462
    },
    {
        "loss": 2.2115,
        "grad_norm": 2.4669036865234375,
        "learning_rate": 9.278724799522716e-05,
        "epoch": 0.05964957485184231,
        "step": 463
    },
    {
        "loss": 2.5086,
        "grad_norm": 1.9079868793487549,
        "learning_rate": 9.275574376869942e-05,
        "epoch": 0.05977840762690028,
        "step": 464
    },
    {
        "loss": 2.3835,
        "grad_norm": 1.449346899986267,
        "learning_rate": 9.272417626172223e-05,
        "epoch": 0.05990724040195826,
        "step": 465
    },
    {
        "loss": 2.0139,
        "grad_norm": 2.3097171783447266,
        "learning_rate": 9.269254552101696e-05,
        "epoch": 0.06003607317701623,
        "step": 466
    },
    {
        "loss": 1.4258,
        "grad_norm": 2.6928956508636475,
        "learning_rate": 9.266085159339852e-05,
        "epoch": 0.06016490595207421,
        "step": 467
    },
    {
        "loss": 1.8848,
        "grad_norm": 2.627403497695923,
        "learning_rate": 9.262909452577539e-05,
        "epoch": 0.06029373872713218,
        "step": 468
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.1996791362762451,
        "learning_rate": 9.259727436514945e-05,
        "epoch": 0.06042257150219016,
        "step": 469
    },
    {
        "loss": 2.2875,
        "grad_norm": 1.6015325784683228,
        "learning_rate": 9.256539115861601e-05,
        "epoch": 0.06055140427724813,
        "step": 470
    },
    {
        "loss": 2.2571,
        "grad_norm": 1.1411305665969849,
        "learning_rate": 9.253344495336367e-05,
        "epoch": 0.06068023705230611,
        "step": 471
    },
    {
        "loss": 2.4663,
        "grad_norm": 1.4640436172485352,
        "learning_rate": 9.250143579667426e-05,
        "epoch": 0.06080906982736408,
        "step": 472
    },
    {
        "loss": 2.3417,
        "grad_norm": 1.0868960618972778,
        "learning_rate": 9.24693637359228e-05,
        "epoch": 0.06093790260242206,
        "step": 473
    },
    {
        "loss": 1.7051,
        "grad_norm": 2.1133968830108643,
        "learning_rate": 9.243722881857739e-05,
        "epoch": 0.06106673537748003,
        "step": 474
    },
    {
        "loss": 2.0836,
        "grad_norm": 2.9299604892730713,
        "learning_rate": 9.240503109219918e-05,
        "epoch": 0.06119556815253801,
        "step": 475
    },
    {
        "loss": 2.4035,
        "grad_norm": 1.4488447904586792,
        "learning_rate": 9.237277060444227e-05,
        "epoch": 0.06132440092759598,
        "step": 476
    },
    {
        "loss": 1.7999,
        "grad_norm": 2.7809393405914307,
        "learning_rate": 9.234044740305364e-05,
        "epoch": 0.06145323370265395,
        "step": 477
    },
    {
        "loss": 2.2489,
        "grad_norm": 1.9266852140426636,
        "learning_rate": 9.230806153587313e-05,
        "epoch": 0.06158206647771193,
        "step": 478
    },
    {
        "loss": 1.8342,
        "grad_norm": 2.8696038722991943,
        "learning_rate": 9.227561305083328e-05,
        "epoch": 0.0617108992527699,
        "step": 479
    },
    {
        "loss": 2.2666,
        "grad_norm": 2.1940951347351074,
        "learning_rate": 9.224310199595933e-05,
        "epoch": 0.06183973202782788,
        "step": 480
    },
    {
        "loss": 2.1189,
        "grad_norm": 2.1310782432556152,
        "learning_rate": 9.221052841936912e-05,
        "epoch": 0.06196856480288585,
        "step": 481
    },
    {
        "loss": 2.7008,
        "grad_norm": 1.000833511352539,
        "learning_rate": 9.217789236927303e-05,
        "epoch": 0.06209739757794383,
        "step": 482
    },
    {
        "loss": 2.5555,
        "grad_norm": 1.9287869930267334,
        "learning_rate": 9.214519389397391e-05,
        "epoch": 0.0622262303530018,
        "step": 483
    },
    {
        "loss": 2.1396,
        "grad_norm": 1.9111608266830444,
        "learning_rate": 9.211243304186696e-05,
        "epoch": 0.06235506312805978,
        "step": 484
    },
    {
        "loss": 2.3656,
        "grad_norm": 1.4524775743484497,
        "learning_rate": 9.20796098614398e-05,
        "epoch": 0.06248389590311775,
        "step": 485
    },
    {
        "loss": 1.7293,
        "grad_norm": 2.2992727756500244,
        "learning_rate": 9.204672440127217e-05,
        "epoch": 0.06261272867817573,
        "step": 486
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.423285961151123,
        "learning_rate": 9.201377671003608e-05,
        "epoch": 0.0627415614532337,
        "step": 487
    },
    {
        "loss": 2.4398,
        "grad_norm": 1.1324236392974854,
        "learning_rate": 9.19807668364956e-05,
        "epoch": 0.06287039422829167,
        "step": 488
    },
    {
        "loss": 1.3827,
        "grad_norm": 2.647214651107788,
        "learning_rate": 9.194769482950687e-05,
        "epoch": 0.06299922700334966,
        "step": 489
    },
    {
        "loss": 2.3975,
        "grad_norm": 1.8847038745880127,
        "learning_rate": 9.191456073801794e-05,
        "epoch": 0.06312805977840763,
        "step": 490
    },
    {
        "loss": 2.1697,
        "grad_norm": 2.5744998455047607,
        "learning_rate": 9.188136461106879e-05,
        "epoch": 0.0632568925534656,
        "step": 491
    },
    {
        "loss": 2.1843,
        "grad_norm": 1.8622303009033203,
        "learning_rate": 9.18481064977912e-05,
        "epoch": 0.06338572532852357,
        "step": 492
    },
    {
        "loss": 2.129,
        "grad_norm": 1.9454222917556763,
        "learning_rate": 9.18147864474087e-05,
        "epoch": 0.06351455810358154,
        "step": 493
    },
    {
        "loss": 2.5279,
        "grad_norm": 1.5517572164535522,
        "learning_rate": 9.178140450923647e-05,
        "epoch": 0.06364339087863953,
        "step": 494
    },
    {
        "loss": 2.4253,
        "grad_norm": 1.8820908069610596,
        "learning_rate": 9.17479607326813e-05,
        "epoch": 0.0637722236536975,
        "step": 495
    },
    {
        "loss": 2.0531,
        "grad_norm": 2.424290180206299,
        "learning_rate": 9.171445516724151e-05,
        "epoch": 0.06390105642875547,
        "step": 496
    },
    {
        "loss": 2.0247,
        "grad_norm": 2.4804821014404297,
        "learning_rate": 9.168088786250688e-05,
        "epoch": 0.06402988920381344,
        "step": 497
    },
    {
        "loss": 2.5195,
        "grad_norm": 1.865530014038086,
        "learning_rate": 9.164725886815853e-05,
        "epoch": 0.06415872197887143,
        "step": 498
    },
    {
        "loss": 2.6435,
        "grad_norm": 1.1010581254959106,
        "learning_rate": 9.161356823396889e-05,
        "epoch": 0.0642875547539294,
        "step": 499
    },
    {
        "loss": 1.5085,
        "grad_norm": 1.0018200874328613,
        "learning_rate": 9.157981600980168e-05,
        "epoch": 0.06441638752898737,
        "step": 500
    },
    {
        "loss": 2.7046,
        "grad_norm": 1.268190622329712,
        "learning_rate": 9.154600224561169e-05,
        "epoch": 0.06454522030404534,
        "step": 501
    },
    {
        "loss": 2.2645,
        "grad_norm": 2.2930212020874023,
        "learning_rate": 9.151212699144484e-05,
        "epoch": 0.06467405307910333,
        "step": 502
    },
    {
        "loss": 2.4217,
        "grad_norm": 1.5119601488113403,
        "learning_rate": 9.147819029743807e-05,
        "epoch": 0.0648028858541613,
        "step": 503
    },
    {
        "loss": 2.2955,
        "grad_norm": 2.623110294342041,
        "learning_rate": 9.144419221381919e-05,
        "epoch": 0.06493171862921927,
        "step": 504
    },
    {
        "loss": 1.5123,
        "grad_norm": 2.265261650085449,
        "learning_rate": 9.141013279090698e-05,
        "epoch": 0.06506055140427724,
        "step": 505
    },
    {
        "loss": 2.4271,
        "grad_norm": 1.4962072372436523,
        "learning_rate": 9.137601207911086e-05,
        "epoch": 0.06518938417933522,
        "step": 506
    },
    {
        "loss": 2.2309,
        "grad_norm": 1.5741335153579712,
        "learning_rate": 9.134183012893112e-05,
        "epoch": 0.0653182169543932,
        "step": 507
    },
    {
        "loss": 2.6148,
        "grad_norm": 1.4301165342330933,
        "learning_rate": 9.130758699095854e-05,
        "epoch": 0.06544704972945117,
        "step": 508
    },
    {
        "loss": 2.1413,
        "grad_norm": 1.990279197692871,
        "learning_rate": 9.127328271587455e-05,
        "epoch": 0.06557588250450914,
        "step": 509
    },
    {
        "loss": 2.0953,
        "grad_norm": 2.503170967102051,
        "learning_rate": 9.123891735445104e-05,
        "epoch": 0.06570471527956712,
        "step": 510
    },
    {
        "loss": 2.3067,
        "grad_norm": 2.2002437114715576,
        "learning_rate": 9.12044909575503e-05,
        "epoch": 0.0658335480546251,
        "step": 511
    },
    {
        "loss": 1.8894,
        "grad_norm": 2.744931936264038,
        "learning_rate": 9.1170003576125e-05,
        "epoch": 0.06596238082968307,
        "step": 512
    },
    {
        "loss": 2.5219,
        "grad_norm": 1.2874419689178467,
        "learning_rate": 9.1135455261218e-05,
        "epoch": 0.06609121360474104,
        "step": 513
    },
    {
        "loss": 2.1427,
        "grad_norm": 1.9116348028182983,
        "learning_rate": 9.110084606396238e-05,
        "epoch": 0.06622004637979902,
        "step": 514
    },
    {
        "loss": 2.3281,
        "grad_norm": 1.9149599075317383,
        "learning_rate": 9.106617603558136e-05,
        "epoch": 0.066348879154857,
        "step": 515
    },
    {
        "loss": 2.5035,
        "grad_norm": 2.8765172958374023,
        "learning_rate": 9.103144522738814e-05,
        "epoch": 0.06647771192991497,
        "step": 516
    },
    {
        "loss": 2.3689,
        "grad_norm": 2.334101438522339,
        "learning_rate": 9.09966536907859e-05,
        "epoch": 0.06660654470497294,
        "step": 517
    },
    {
        "loss": 2.6068,
        "grad_norm": 1.5073039531707764,
        "learning_rate": 9.09618014772677e-05,
        "epoch": 0.06673537748003092,
        "step": 518
    },
    {
        "loss": 2.2528,
        "grad_norm": 1.2896013259887695,
        "learning_rate": 9.092688863841644e-05,
        "epoch": 0.06686421025508889,
        "step": 519
    },
    {
        "loss": 2.2436,
        "grad_norm": 1.2945235967636108,
        "learning_rate": 9.089191522590463e-05,
        "epoch": 0.06699304303014687,
        "step": 520
    },
    {
        "loss": 1.9812,
        "grad_norm": 2.4990220069885254,
        "learning_rate": 9.085688129149461e-05,
        "epoch": 0.06712187580520484,
        "step": 521
    },
    {
        "loss": 2.3641,
        "grad_norm": 1.9495246410369873,
        "learning_rate": 9.082178688703813e-05,
        "epoch": 0.06725070858026282,
        "step": 522
    },
    {
        "loss": 2.4117,
        "grad_norm": 1.1045957803726196,
        "learning_rate": 9.078663206447654e-05,
        "epoch": 0.06737954135532079,
        "step": 523
    },
    {
        "loss": 2.0004,
        "grad_norm": 3.8773698806762695,
        "learning_rate": 9.075141687584057e-05,
        "epoch": 0.06750837413037877,
        "step": 524
    },
    {
        "loss": 2.5167,
        "grad_norm": 1.6365574598312378,
        "learning_rate": 9.071614137325031e-05,
        "epoch": 0.06763720690543674,
        "step": 525
    },
    {
        "loss": 2.486,
        "grad_norm": 1.4213669300079346,
        "learning_rate": 9.068080560891511e-05,
        "epoch": 0.06776603968049472,
        "step": 526
    },
    {
        "loss": 2.2487,
        "grad_norm": 1.6543203592300415,
        "learning_rate": 9.064540963513351e-05,
        "epoch": 0.06789487245555269,
        "step": 527
    },
    {
        "loss": 2.3891,
        "grad_norm": 1.5791600942611694,
        "learning_rate": 9.060995350429317e-05,
        "epoch": 0.06802370523061067,
        "step": 528
    },
    {
        "loss": 2.0392,
        "grad_norm": 2.8354265689849854,
        "learning_rate": 9.057443726887076e-05,
        "epoch": 0.06815253800566864,
        "step": 529
    },
    {
        "loss": 2.5544,
        "grad_norm": 1.9043283462524414,
        "learning_rate": 9.053886098143195e-05,
        "epoch": 0.06828137078072662,
        "step": 530
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.5545376539230347,
        "learning_rate": 9.050322469463127e-05,
        "epoch": 0.06841020355578459,
        "step": 531
    },
    {
        "loss": 2.8355,
        "grad_norm": 1.2729324102401733,
        "learning_rate": 9.0467528461212e-05,
        "epoch": 0.06853903633084257,
        "step": 532
    },
    {
        "loss": 2.584,
        "grad_norm": 2.007239818572998,
        "learning_rate": 9.043177233400625e-05,
        "epoch": 0.06866786910590054,
        "step": 533
    },
    {
        "loss": 2.2057,
        "grad_norm": 1.956738829612732,
        "learning_rate": 9.039595636593472e-05,
        "epoch": 0.06879670188095852,
        "step": 534
    },
    {
        "loss": 2.6857,
        "grad_norm": 1.7007818222045898,
        "learning_rate": 9.036008061000662e-05,
        "epoch": 0.06892553465601649,
        "step": 535
    },
    {
        "loss": 2.4619,
        "grad_norm": 2.0354888439178467,
        "learning_rate": 9.032414511931974e-05,
        "epoch": 0.06905436743107446,
        "step": 536
    },
    {
        "loss": 2.6668,
        "grad_norm": 1.1663234233856201,
        "learning_rate": 9.028814994706025e-05,
        "epoch": 0.06918320020613244,
        "step": 537
    },
    {
        "loss": 1.4393,
        "grad_norm": 0.9854402542114258,
        "learning_rate": 9.025209514650263e-05,
        "epoch": 0.06931203298119042,
        "step": 538
    },
    {
        "loss": 2.4636,
        "grad_norm": 1.7345479726791382,
        "learning_rate": 9.021598077100966e-05,
        "epoch": 0.06944086575624839,
        "step": 539
    },
    {
        "loss": 2.3319,
        "grad_norm": 1.8924583196640015,
        "learning_rate": 9.017980687403221e-05,
        "epoch": 0.06956969853130636,
        "step": 540
    },
    {
        "loss": 2.2209,
        "grad_norm": 1.8788002729415894,
        "learning_rate": 9.014357350910932e-05,
        "epoch": 0.06969853130636434,
        "step": 541
    },
    {
        "loss": 2.0992,
        "grad_norm": 2.2097368240356445,
        "learning_rate": 9.010728072986803e-05,
        "epoch": 0.06982736408142232,
        "step": 542
    },
    {
        "loss": 2.7294,
        "grad_norm": 1.9228525161743164,
        "learning_rate": 9.00709285900233e-05,
        "epoch": 0.06995619685648029,
        "step": 543
    },
    {
        "loss": 2.2872,
        "grad_norm": 1.6595791578292847,
        "learning_rate": 9.003451714337796e-05,
        "epoch": 0.07008502963153826,
        "step": 544
    },
    {
        "loss": 1.6874,
        "grad_norm": 2.6848912239074707,
        "learning_rate": 8.99980464438226e-05,
        "epoch": 0.07021386240659624,
        "step": 545
    },
    {
        "loss": 2.0739,
        "grad_norm": 2.581678628921509,
        "learning_rate": 8.99615165453355e-05,
        "epoch": 0.07034269518165422,
        "step": 546
    },
    {
        "loss": 2.0037,
        "grad_norm": 2.0431861877441406,
        "learning_rate": 8.992492750198262e-05,
        "epoch": 0.07047152795671219,
        "step": 547
    },
    {
        "loss": 2.7143,
        "grad_norm": 1.5056147575378418,
        "learning_rate": 8.98882793679174e-05,
        "epoch": 0.07060036073177016,
        "step": 548
    },
    {
        "loss": 2.1489,
        "grad_norm": 1.9494884014129639,
        "learning_rate": 8.985157219738072e-05,
        "epoch": 0.07072919350682813,
        "step": 549
    },
    {
        "loss": 2.479,
        "grad_norm": 1.6548434495925903,
        "learning_rate": 8.981480604470089e-05,
        "epoch": 0.07085802628188612,
        "step": 550
    },
    {
        "loss": 2.2683,
        "grad_norm": 1.9074872732162476,
        "learning_rate": 8.977798096429348e-05,
        "epoch": 0.07098685905694409,
        "step": 551
    },
    {
        "loss": 1.8624,
        "grad_norm": 2.303896188735962,
        "learning_rate": 8.974109701066128e-05,
        "epoch": 0.07111569183200206,
        "step": 552
    },
    {
        "loss": 2.3738,
        "grad_norm": 1.0662094354629517,
        "learning_rate": 8.970415423839427e-05,
        "epoch": 0.07124452460706003,
        "step": 553
    },
    {
        "loss": 1.3598,
        "grad_norm": 3.216451406478882,
        "learning_rate": 8.966715270216937e-05,
        "epoch": 0.07137335738211802,
        "step": 554
    },
    {
        "loss": 1.8736,
        "grad_norm": 2.48927640914917,
        "learning_rate": 8.963009245675058e-05,
        "epoch": 0.07150219015717599,
        "step": 555
    },
    {
        "loss": 2.5974,
        "grad_norm": 1.3170630931854248,
        "learning_rate": 8.959297355698875e-05,
        "epoch": 0.07163102293223396,
        "step": 556
    },
    {
        "loss": 2.1241,
        "grad_norm": 2.7255804538726807,
        "learning_rate": 8.955579605782153e-05,
        "epoch": 0.07175985570729193,
        "step": 557
    },
    {
        "loss": 1.9976,
        "grad_norm": 2.914966344833374,
        "learning_rate": 8.951856001427332e-05,
        "epoch": 0.07188868848234992,
        "step": 558
    },
    {
        "loss": 2.2469,
        "grad_norm": 1.2917641401290894,
        "learning_rate": 8.948126548145517e-05,
        "epoch": 0.07201752125740789,
        "step": 559
    },
    {
        "loss": 2.0725,
        "grad_norm": 2.2700345516204834,
        "learning_rate": 8.944391251456467e-05,
        "epoch": 0.07214635403246586,
        "step": 560
    },
    {
        "loss": 2.387,
        "grad_norm": 1.591711163520813,
        "learning_rate": 8.940650116888594e-05,
        "epoch": 0.07227518680752383,
        "step": 561
    },
    {
        "loss": 1.738,
        "grad_norm": 2.549452304840088,
        "learning_rate": 8.936903149978943e-05,
        "epoch": 0.0724040195825818,
        "step": 562
    },
    {
        "loss": 2.068,
        "grad_norm": 2.1989095211029053,
        "learning_rate": 8.933150356273202e-05,
        "epoch": 0.07253285235763979,
        "step": 563
    },
    {
        "loss": 2.5466,
        "grad_norm": 2.2558910846710205,
        "learning_rate": 8.92939174132567e-05,
        "epoch": 0.07266168513269776,
        "step": 564
    },
    {
        "loss": 2.4443,
        "grad_norm": 2.068009853363037,
        "learning_rate": 8.925627310699275e-05,
        "epoch": 0.07279051790775573,
        "step": 565
    },
    {
        "loss": 2.1941,
        "grad_norm": 1.7159123420715332,
        "learning_rate": 8.921857069965542e-05,
        "epoch": 0.0729193506828137,
        "step": 566
    },
    {
        "loss": 2.4221,
        "grad_norm": 1.326237440109253,
        "learning_rate": 8.9180810247046e-05,
        "epoch": 0.07304818345787169,
        "step": 567
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.700084924697876,
        "learning_rate": 8.914299180505168e-05,
        "epoch": 0.07317701623292966,
        "step": 568
    },
    {
        "loss": 1.7422,
        "grad_norm": 2.7456631660461426,
        "learning_rate": 8.910511542964546e-05,
        "epoch": 0.07330584900798763,
        "step": 569
    },
    {
        "loss": 1.7269,
        "grad_norm": 2.052622079849243,
        "learning_rate": 8.906718117688615e-05,
        "epoch": 0.0734346817830456,
        "step": 570
    },
    {
        "loss": 2.6957,
        "grad_norm": 1.513914704322815,
        "learning_rate": 8.902918910291813e-05,
        "epoch": 0.07356351455810359,
        "step": 571
    },
    {
        "loss": 2.15,
        "grad_norm": 2.0959246158599854,
        "learning_rate": 8.899113926397141e-05,
        "epoch": 0.07369234733316156,
        "step": 572
    },
    {
        "loss": 2.0365,
        "grad_norm": 1.319648027420044,
        "learning_rate": 8.895303171636148e-05,
        "epoch": 0.07382118010821953,
        "step": 573
    },
    {
        "loss": 2.3366,
        "grad_norm": 1.4037539958953857,
        "learning_rate": 8.89148665164893e-05,
        "epoch": 0.0739500128832775,
        "step": 574
    },
    {
        "loss": 1.7012,
        "grad_norm": 2.594332456588745,
        "learning_rate": 8.887664372084103e-05,
        "epoch": 0.07407884565833547,
        "step": 575
    },
    {
        "loss": 2.1407,
        "grad_norm": 1.881164789199829,
        "learning_rate": 8.883836338598819e-05,
        "epoch": 0.07420767843339346,
        "step": 576
    },
    {
        "loss": 2.406,
        "grad_norm": 1.8368152379989624,
        "learning_rate": 8.880002556858741e-05,
        "epoch": 0.07433651120845143,
        "step": 577
    },
    {
        "loss": 2.2368,
        "grad_norm": 1.5675393342971802,
        "learning_rate": 8.87616303253804e-05,
        "epoch": 0.0744653439835094,
        "step": 578
    },
    {
        "loss": 1.2811,
        "grad_norm": 2.5834386348724365,
        "learning_rate": 8.872317771319389e-05,
        "epoch": 0.07459417675856737,
        "step": 579
    },
    {
        "loss": 2.1525,
        "grad_norm": 3.4902493953704834,
        "learning_rate": 8.86846677889395e-05,
        "epoch": 0.07472300953362536,
        "step": 580
    },
    {
        "loss": 2.0001,
        "grad_norm": 2.313370704650879,
        "learning_rate": 8.864610060961365e-05,
        "epoch": 0.07485184230868333,
        "step": 581
    },
    {
        "loss": 2.1481,
        "grad_norm": 2.1668307781219482,
        "learning_rate": 8.860747623229752e-05,
        "epoch": 0.0749806750837413,
        "step": 582
    },
    {
        "loss": 2.3435,
        "grad_norm": 1.2684671878814697,
        "learning_rate": 8.856879471415699e-05,
        "epoch": 0.07510950785879927,
        "step": 583
    },
    {
        "loss": 2.2829,
        "grad_norm": 1.5121670961380005,
        "learning_rate": 8.853005611244243e-05,
        "epoch": 0.07523834063385726,
        "step": 584
    },
    {
        "loss": 1.8305,
        "grad_norm": 2.574277639389038,
        "learning_rate": 8.849126048448875e-05,
        "epoch": 0.07536717340891523,
        "step": 585
    },
    {
        "loss": 2.4115,
        "grad_norm": 1.879162311553955,
        "learning_rate": 8.845240788771523e-05,
        "epoch": 0.0754960061839732,
        "step": 586
    },
    {
        "loss": 2.2933,
        "grad_norm": 1.6021041870117188,
        "learning_rate": 8.841349837962553e-05,
        "epoch": 0.07562483895903117,
        "step": 587
    },
    {
        "loss": 1.9282,
        "grad_norm": 2.322659969329834,
        "learning_rate": 8.837453201780746e-05,
        "epoch": 0.07575367173408916,
        "step": 588
    },
    {
        "loss": 2.3624,
        "grad_norm": 1.905401349067688,
        "learning_rate": 8.8335508859933e-05,
        "epoch": 0.07588250450914713,
        "step": 589
    },
    {
        "loss": 1.5509,
        "grad_norm": 2.6398491859436035,
        "learning_rate": 8.829642896375823e-05,
        "epoch": 0.0760113372842051,
        "step": 590
    },
    {
        "loss": 2.3589,
        "grad_norm": 2.3432018756866455,
        "learning_rate": 8.825729238712317e-05,
        "epoch": 0.07614017005926307,
        "step": 591
    },
    {
        "loss": 1.8684,
        "grad_norm": 2.6136038303375244,
        "learning_rate": 8.821809918795173e-05,
        "epoch": 0.07626900283432105,
        "step": 592
    },
    {
        "loss": 2.4816,
        "grad_norm": 2.1378893852233887,
        "learning_rate": 8.817884942425163e-05,
        "epoch": 0.07639783560937903,
        "step": 593
    },
    {
        "loss": 2.4912,
        "grad_norm": 1.992444396018982,
        "learning_rate": 8.813954315411433e-05,
        "epoch": 0.076526668384437,
        "step": 594
    },
    {
        "loss": 2.1411,
        "grad_norm": 2.6173815727233887,
        "learning_rate": 8.810018043571487e-05,
        "epoch": 0.07665550115949497,
        "step": 595
    },
    {
        "loss": 1.8676,
        "grad_norm": 2.145472288131714,
        "learning_rate": 8.80607613273119e-05,
        "epoch": 0.07678433393455295,
        "step": 596
    },
    {
        "loss": 2.2754,
        "grad_norm": 1.5894941091537476,
        "learning_rate": 8.802128588724746e-05,
        "epoch": 0.07691316670961093,
        "step": 597
    },
    {
        "loss": 2.3818,
        "grad_norm": 1.5114545822143555,
        "learning_rate": 8.798175417394703e-05,
        "epoch": 0.0770419994846689,
        "step": 598
    },
    {
        "loss": 1.9764,
        "grad_norm": 2.314605474472046,
        "learning_rate": 8.794216624591933e-05,
        "epoch": 0.07717083225972687,
        "step": 599
    },
    {
        "loss": 2.5258,
        "grad_norm": 1.4689358472824097,
        "learning_rate": 8.790252216175632e-05,
        "epoch": 0.07729966503478485,
        "step": 600
    },
    {
        "loss": 2.2008,
        "grad_norm": 1.4239544868469238,
        "learning_rate": 8.786282198013304e-05,
        "epoch": 0.07742849780984283,
        "step": 601
    },
    {
        "loss": 1.7067,
        "grad_norm": 2.3532309532165527,
        "learning_rate": 8.782306575980754e-05,
        "epoch": 0.0775573305849008,
        "step": 602
    },
    {
        "loss": 1.7663,
        "grad_norm": 2.698882818222046,
        "learning_rate": 8.778325355962089e-05,
        "epoch": 0.07768616335995877,
        "step": 603
    },
    {
        "loss": 2.1325,
        "grad_norm": 1.8741295337677002,
        "learning_rate": 8.774338543849692e-05,
        "epoch": 0.07781499613501675,
        "step": 604
    },
    {
        "loss": 2.6521,
        "grad_norm": 1.6834200620651245,
        "learning_rate": 8.77034614554423e-05,
        "epoch": 0.07794382891007472,
        "step": 605
    },
    {
        "loss": 2.3975,
        "grad_norm": 2.1299333572387695,
        "learning_rate": 8.76634816695463e-05,
        "epoch": 0.0780726616851327,
        "step": 606
    },
    {
        "loss": 2.4574,
        "grad_norm": 0.9834490418434143,
        "learning_rate": 8.762344613998086e-05,
        "epoch": 0.07820149446019067,
        "step": 607
    },
    {
        "loss": 2.3729,
        "grad_norm": 1.6982895135879517,
        "learning_rate": 8.758335492600038e-05,
        "epoch": 0.07833032723524865,
        "step": 608
    },
    {
        "loss": 2.0664,
        "grad_norm": 2.2705423831939697,
        "learning_rate": 8.754320808694168e-05,
        "epoch": 0.07845916001030662,
        "step": 609
    },
    {
        "loss": 2.5335,
        "grad_norm": 1.5316513776779175,
        "learning_rate": 8.750300568222392e-05,
        "epoch": 0.0785879927853646,
        "step": 610
    },
    {
        "loss": 2.2086,
        "grad_norm": 1.9824450016021729,
        "learning_rate": 8.746274777134846e-05,
        "epoch": 0.07871682556042257,
        "step": 611
    },
    {
        "loss": 1.1976,
        "grad_norm": 3.075247049331665,
        "learning_rate": 8.74224344138989e-05,
        "epoch": 0.07884565833548055,
        "step": 612
    },
    {
        "loss": 2.6584,
        "grad_norm": 1.306239128112793,
        "learning_rate": 8.738206566954081e-05,
        "epoch": 0.07897449111053852,
        "step": 613
    },
    {
        "loss": 2.2654,
        "grad_norm": 1.317970871925354,
        "learning_rate": 8.734164159802176e-05,
        "epoch": 0.0791033238855965,
        "step": 614
    },
    {
        "loss": 2.4019,
        "grad_norm": 1.8050428628921509,
        "learning_rate": 8.730116225917124e-05,
        "epoch": 0.07923215666065447,
        "step": 615
    },
    {
        "loss": 2.2061,
        "grad_norm": 1.623724341392517,
        "learning_rate": 8.726062771290051e-05,
        "epoch": 0.07936098943571245,
        "step": 616
    },
    {
        "loss": 2.013,
        "grad_norm": 2.5321478843688965,
        "learning_rate": 8.722003801920257e-05,
        "epoch": 0.07948982221077042,
        "step": 617
    },
    {
        "loss": 2.0883,
        "grad_norm": 1.178671956062317,
        "learning_rate": 8.717939323815196e-05,
        "epoch": 0.07961865498582839,
        "step": 618
    },
    {
        "loss": 2.2308,
        "grad_norm": 2.0026934146881104,
        "learning_rate": 8.713869342990486e-05,
        "epoch": 0.07974748776088637,
        "step": 619
    },
    {
        "loss": 2.3299,
        "grad_norm": 1.7732417583465576,
        "learning_rate": 8.709793865469883e-05,
        "epoch": 0.07987632053594435,
        "step": 620
    },
    {
        "loss": 2.5983,
        "grad_norm": 1.9824024438858032,
        "learning_rate": 8.705712897285277e-05,
        "epoch": 0.08000515331100232,
        "step": 621
    },
    {
        "loss": 2.3048,
        "grad_norm": 2.190011739730835,
        "learning_rate": 8.70162644447669e-05,
        "epoch": 0.08013398608606029,
        "step": 622
    },
    {
        "loss": 1.963,
        "grad_norm": 2.589475393295288,
        "learning_rate": 8.697534513092257e-05,
        "epoch": 0.08026281886111827,
        "step": 623
    },
    {
        "loss": 1.5064,
        "grad_norm": 2.005354166030884,
        "learning_rate": 8.693437109188223e-05,
        "epoch": 0.08039165163617625,
        "step": 624
    },
    {
        "loss": 1.9488,
        "grad_norm": 2.4217894077301025,
        "learning_rate": 8.689334238828932e-05,
        "epoch": 0.08052048441123422,
        "step": 625
    },
    {
        "loss": 2.6135,
        "grad_norm": 1.7669174671173096,
        "learning_rate": 8.68522590808682e-05,
        "epoch": 0.08064931718629219,
        "step": 626
    },
    {
        "loss": 2.0648,
        "grad_norm": 2.1502573490142822,
        "learning_rate": 8.681112123042403e-05,
        "epoch": 0.08077814996135017,
        "step": 627
    },
    {
        "loss": 2.5746,
        "grad_norm": 1.4279392957687378,
        "learning_rate": 8.676992889784271e-05,
        "epoch": 0.08090698273640815,
        "step": 628
    },
    {
        "loss": 2.497,
        "grad_norm": 1.8586374521255493,
        "learning_rate": 8.672868214409078e-05,
        "epoch": 0.08103581551146612,
        "step": 629
    },
    {
        "loss": 2.1823,
        "grad_norm": 1.5267003774642944,
        "learning_rate": 8.668738103021529e-05,
        "epoch": 0.08116464828652409,
        "step": 630
    },
    {
        "loss": 2.0356,
        "grad_norm": 2.4120566844940186,
        "learning_rate": 8.664602561734379e-05,
        "epoch": 0.08129348106158206,
        "step": 631
    },
    {
        "loss": 2.1183,
        "grad_norm": 1.9999443292617798,
        "learning_rate": 8.660461596668415e-05,
        "epoch": 0.08142231383664005,
        "step": 632
    },
    {
        "loss": 2.505,
        "grad_norm": 1.6785634756088257,
        "learning_rate": 8.656315213952459e-05,
        "epoch": 0.08155114661169802,
        "step": 633
    },
    {
        "loss": 1.5165,
        "grad_norm": 2.636744499206543,
        "learning_rate": 8.652163419723342e-05,
        "epoch": 0.08167997938675599,
        "step": 634
    },
    {
        "loss": 2.2982,
        "grad_norm": 2.2958154678344727,
        "learning_rate": 8.64800622012591e-05,
        "epoch": 0.08180881216181396,
        "step": 635
    },
    {
        "loss": 1.8856,
        "grad_norm": 2.2772696018218994,
        "learning_rate": 8.643843621313009e-05,
        "epoch": 0.08193764493687195,
        "step": 636
    },
    {
        "loss": 2.2801,
        "grad_norm": 1.900389313697815,
        "learning_rate": 8.639675629445472e-05,
        "epoch": 0.08206647771192992,
        "step": 637
    },
    {
        "loss": 2.2351,
        "grad_norm": 1.608398675918579,
        "learning_rate": 8.63550225069212e-05,
        "epoch": 0.08219531048698789,
        "step": 638
    },
    {
        "loss": 2.4227,
        "grad_norm": 1.353420615196228,
        "learning_rate": 8.631323491229743e-05,
        "epoch": 0.08232414326204586,
        "step": 639
    },
    {
        "loss": 2.4314,
        "grad_norm": 1.810337781906128,
        "learning_rate": 8.627139357243095e-05,
        "epoch": 0.08245297603710385,
        "step": 640
    },
    {
        "loss": 1.617,
        "grad_norm": 2.502476215362549,
        "learning_rate": 8.622949854924884e-05,
        "epoch": 0.08258180881216182,
        "step": 641
    },
    {
        "loss": 1.9441,
        "grad_norm": 1.931971788406372,
        "learning_rate": 8.618754990475765e-05,
        "epoch": 0.08271064158721979,
        "step": 642
    },
    {
        "loss": 2.4281,
        "grad_norm": 1.6332868337631226,
        "learning_rate": 8.61455477010433e-05,
        "epoch": 0.08283947436227776,
        "step": 643
    },
    {
        "loss": 2.4434,
        "grad_norm": 1.4808717966079712,
        "learning_rate": 8.610349200027097e-05,
        "epoch": 0.08296830713733575,
        "step": 644
    },
    {
        "loss": 2.2032,
        "grad_norm": 2.0374915599823,
        "learning_rate": 8.606138286468498e-05,
        "epoch": 0.08309713991239372,
        "step": 645
    },
    {
        "loss": 2.0246,
        "grad_norm": 1.4051846265792847,
        "learning_rate": 8.60192203566088e-05,
        "epoch": 0.08322597268745169,
        "step": 646
    },
    {
        "loss": 1.9891,
        "grad_norm": 1.741053819656372,
        "learning_rate": 8.597700453844486e-05,
        "epoch": 0.08335480546250966,
        "step": 647
    },
    {
        "loss": 2.1868,
        "grad_norm": 1.8209893703460693,
        "learning_rate": 8.593473547267448e-05,
        "epoch": 0.08348363823756763,
        "step": 648
    },
    {
        "loss": 2.5298,
        "grad_norm": 1.6642770767211914,
        "learning_rate": 8.589241322185781e-05,
        "epoch": 0.08361247101262562,
        "step": 649
    },
    {
        "loss": 2.386,
        "grad_norm": 1.3956074714660645,
        "learning_rate": 8.58500378486337e-05,
        "epoch": 0.08374130378768359,
        "step": 650
    },
    {
        "loss": 2.0616,
        "grad_norm": 2.75384783744812,
        "learning_rate": 8.580760941571967e-05,
        "epoch": 0.08387013656274156,
        "step": 651
    },
    {
        "loss": 2.3906,
        "grad_norm": 1.357240915298462,
        "learning_rate": 8.57651279859117e-05,
        "epoch": 0.08399896933779953,
        "step": 652
    },
    {
        "loss": 2.2253,
        "grad_norm": 1.9241070747375488,
        "learning_rate": 8.572259362208426e-05,
        "epoch": 0.08412780211285752,
        "step": 653
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.3133832216262817,
        "learning_rate": 8.568000638719014e-05,
        "epoch": 0.08425663488791549,
        "step": 654
    },
    {
        "loss": 2.3355,
        "grad_norm": 1.1007328033447266,
        "learning_rate": 8.563736634426037e-05,
        "epoch": 0.08438546766297346,
        "step": 655
    },
    {
        "loss": 1.9119,
        "grad_norm": 1.3378684520721436,
        "learning_rate": 8.55946735564042e-05,
        "epoch": 0.08451430043803143,
        "step": 656
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.5498045682907104,
        "learning_rate": 8.555192808680886e-05,
        "epoch": 0.08464313321308942,
        "step": 657
    },
    {
        "loss": 2.5453,
        "grad_norm": 1.1318167448043823,
        "learning_rate": 8.550912999873961e-05,
        "epoch": 0.08477196598814739,
        "step": 658
    },
    {
        "loss": 2.3056,
        "grad_norm": 2.373446464538574,
        "learning_rate": 8.546627935553957e-05,
        "epoch": 0.08490079876320536,
        "step": 659
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.4377998113632202,
        "learning_rate": 8.542337622062965e-05,
        "epoch": 0.08502963153826333,
        "step": 660
    },
    {
        "loss": 1.6633,
        "grad_norm": 2.3286359310150146,
        "learning_rate": 8.538042065750846e-05,
        "epoch": 0.0851584643133213,
        "step": 661
    },
    {
        "loss": 1.6718,
        "grad_norm": 2.433180332183838,
        "learning_rate": 8.533741272975215e-05,
        "epoch": 0.08528729708837929,
        "step": 662
    },
    {
        "loss": 1.9715,
        "grad_norm": 2.0486502647399902,
        "learning_rate": 8.529435250101443e-05,
        "epoch": 0.08541612986343726,
        "step": 663
    },
    {
        "loss": 2.3767,
        "grad_norm": 2.4305553436279297,
        "learning_rate": 8.525124003502644e-05,
        "epoch": 0.08554496263849523,
        "step": 664
    },
    {
        "loss": 2.548,
        "grad_norm": 1.5698373317718506,
        "learning_rate": 8.520807539559653e-05,
        "epoch": 0.0856737954135532,
        "step": 665
    },
    {
        "loss": 2.481,
        "grad_norm": 1.2980964183807373,
        "learning_rate": 8.51648586466104e-05,
        "epoch": 0.08580262818861119,
        "step": 666
    },
    {
        "loss": 2.1335,
        "grad_norm": 1.570082426071167,
        "learning_rate": 8.512158985203073e-05,
        "epoch": 0.08593146096366916,
        "step": 667
    },
    {
        "loss": 2.3073,
        "grad_norm": 1.5923906564712524,
        "learning_rate": 8.507826907589737e-05,
        "epoch": 0.08606029373872713,
        "step": 668
    },
    {
        "loss": 1.9624,
        "grad_norm": 2.2778570652008057,
        "learning_rate": 8.503489638232702e-05,
        "epoch": 0.0861891265137851,
        "step": 669
    },
    {
        "loss": 1.9004,
        "grad_norm": 1.5258203744888306,
        "learning_rate": 8.499147183551326e-05,
        "epoch": 0.08631795928884309,
        "step": 670
    },
    {
        "loss": 2.0645,
        "grad_norm": 2.2053163051605225,
        "learning_rate": 8.494799549972638e-05,
        "epoch": 0.08644679206390106,
        "step": 671
    },
    {
        "loss": 1.9614,
        "grad_norm": 2.219691753387451,
        "learning_rate": 8.490446743931334e-05,
        "epoch": 0.08657562483895903,
        "step": 672
    },
    {
        "loss": 2.5637,
        "grad_norm": 1.1553585529327393,
        "learning_rate": 8.486088771869767e-05,
        "epoch": 0.086704457614017,
        "step": 673
    },
    {
        "loss": 2.1921,
        "grad_norm": 2.0767085552215576,
        "learning_rate": 8.481725640237932e-05,
        "epoch": 0.08683329038907497,
        "step": 674
    },
    {
        "loss": 2.4693,
        "grad_norm": 0.936330258846283,
        "learning_rate": 8.477357355493462e-05,
        "epoch": 0.08696212316413296,
        "step": 675
    },
    {
        "loss": 2.4183,
        "grad_norm": 1.2387019395828247,
        "learning_rate": 8.472983924101624e-05,
        "epoch": 0.08709095593919093,
        "step": 676
    },
    {
        "loss": 2.5832,
        "grad_norm": 1.211575984954834,
        "learning_rate": 8.468605352535286e-05,
        "epoch": 0.0872197887142489,
        "step": 677
    },
    {
        "loss": 1.7329,
        "grad_norm": 2.00234055519104,
        "learning_rate": 8.464221647274941e-05,
        "epoch": 0.08734862148930687,
        "step": 678
    },
    {
        "loss": 1.7363,
        "grad_norm": 2.4818637371063232,
        "learning_rate": 8.45983281480867e-05,
        "epoch": 0.08747745426436486,
        "step": 679
    },
    {
        "loss": 2.1988,
        "grad_norm": 1.6931853294372559,
        "learning_rate": 8.455438861632146e-05,
        "epoch": 0.08760628703942283,
        "step": 680
    },
    {
        "loss": 1.8535,
        "grad_norm": 2.3814496994018555,
        "learning_rate": 8.451039794248619e-05,
        "epoch": 0.0877351198144808,
        "step": 681
    },
    {
        "loss": 2.0772,
        "grad_norm": 2.402155876159668,
        "learning_rate": 8.446635619168907e-05,
        "epoch": 0.08786395258953877,
        "step": 682
    },
    {
        "loss": 1.4159,
        "grad_norm": 2.362654685974121,
        "learning_rate": 8.442226342911394e-05,
        "epoch": 0.08799278536459676,
        "step": 683
    },
    {
        "loss": 1.7591,
        "grad_norm": 2.7361714839935303,
        "learning_rate": 8.437811972002008e-05,
        "epoch": 0.08812161813965473,
        "step": 684
    },
    {
        "loss": 2.294,
        "grad_norm": 2.664207696914673,
        "learning_rate": 8.433392512974219e-05,
        "epoch": 0.0882504509147127,
        "step": 685
    },
    {
        "loss": 2.462,
        "grad_norm": 1.7127301692962646,
        "learning_rate": 8.428967972369028e-05,
        "epoch": 0.08837928368977067,
        "step": 686
    },
    {
        "loss": 2.4127,
        "grad_norm": 2.285372495651245,
        "learning_rate": 8.424538356734957e-05,
        "epoch": 0.08850811646482865,
        "step": 687
    },
    {
        "loss": 2.3922,
        "grad_norm": 2.249559164047241,
        "learning_rate": 8.420103672628037e-05,
        "epoch": 0.08863694923988663,
        "step": 688
    },
    {
        "loss": 1.5882,
        "grad_norm": 2.940934896469116,
        "learning_rate": 8.415663926611805e-05,
        "epoch": 0.0887657820149446,
        "step": 689
    },
    {
        "loss": 2.2002,
        "grad_norm": 1.8293830156326294,
        "learning_rate": 8.411219125257288e-05,
        "epoch": 0.08889461479000257,
        "step": 690
    },
    {
        "loss": 1.8001,
        "grad_norm": 2.3840584754943848,
        "learning_rate": 8.406769275142994e-05,
        "epoch": 0.08902344756506055,
        "step": 691
    },
    {
        "loss": 2.0895,
        "grad_norm": 2.7129552364349365,
        "learning_rate": 8.402314382854901e-05,
        "epoch": 0.08915228034011853,
        "step": 692
    },
    {
        "loss": 2.2852,
        "grad_norm": 2.5593693256378174,
        "learning_rate": 8.397854454986455e-05,
        "epoch": 0.0892811131151765,
        "step": 693
    },
    {
        "loss": 1.8972,
        "grad_norm": 1.5442942380905151,
        "learning_rate": 8.393389498138554e-05,
        "epoch": 0.08940994589023447,
        "step": 694
    },
    {
        "loss": 2.2629,
        "grad_norm": 2.2807178497314453,
        "learning_rate": 8.388919518919536e-05,
        "epoch": 0.08953877866529245,
        "step": 695
    },
    {
        "loss": 2.6889,
        "grad_norm": 1.3412294387817383,
        "learning_rate": 8.384444523945174e-05,
        "epoch": 0.08966761144035043,
        "step": 696
    },
    {
        "loss": 1.7261,
        "grad_norm": 2.5848848819732666,
        "learning_rate": 8.379964519838665e-05,
        "epoch": 0.0897964442154084,
        "step": 697
    },
    {
        "loss": 1.853,
        "grad_norm": 2.841639757156372,
        "learning_rate": 8.375479513230617e-05,
        "epoch": 0.08992527699046637,
        "step": 698
    },
    {
        "loss": 1.554,
        "grad_norm": 3.0107128620147705,
        "learning_rate": 8.370989510759049e-05,
        "epoch": 0.09005410976552435,
        "step": 699
    },
    {
        "loss": 2.2076,
        "grad_norm": 1.1959099769592285,
        "learning_rate": 8.366494519069363e-05,
        "epoch": 0.09018294254058233,
        "step": 700
    },
    {
        "loss": 2.1235,
        "grad_norm": 1.7791544198989868,
        "learning_rate": 8.361994544814358e-05,
        "epoch": 0.0903117753156403,
        "step": 701
    },
    {
        "loss": 1.9829,
        "grad_norm": 1.946465253829956,
        "learning_rate": 8.357489594654196e-05,
        "epoch": 0.09044060809069827,
        "step": 702
    },
    {
        "loss": 2.3648,
        "grad_norm": 1.4645568132400513,
        "learning_rate": 8.352979675256409e-05,
        "epoch": 0.09056944086575625,
        "step": 703
    },
    {
        "loss": 1.2036,
        "grad_norm": 2.8003344535827637,
        "learning_rate": 8.348464793295888e-05,
        "epoch": 0.09069827364081422,
        "step": 704
    },
    {
        "loss": 1.3926,
        "grad_norm": 4.034090995788574,
        "learning_rate": 8.343944955454859e-05,
        "epoch": 0.0908271064158722,
        "step": 705
    },
    {
        "loss": 1.8562,
        "grad_norm": 1.8084129095077515,
        "learning_rate": 8.339420168422891e-05,
        "epoch": 0.09095593919093017,
        "step": 706
    },
    {
        "loss": 2.4185,
        "grad_norm": 1.4898550510406494,
        "learning_rate": 8.334890438896873e-05,
        "epoch": 0.09108477196598815,
        "step": 707
    },
    {
        "loss": 2.4799,
        "grad_norm": 1.9338717460632324,
        "learning_rate": 8.330355773581012e-05,
        "epoch": 0.09121360474104612,
        "step": 708
    },
    {
        "loss": 2.1327,
        "grad_norm": 2.365936756134033,
        "learning_rate": 8.325816179186819e-05,
        "epoch": 0.0913424375161041,
        "step": 709
    },
    {
        "loss": 2.2191,
        "grad_norm": 1.573874592781067,
        "learning_rate": 8.3212716624331e-05,
        "epoch": 0.09147127029116207,
        "step": 710
    },
    {
        "loss": 1.976,
        "grad_norm": 2.018833637237549,
        "learning_rate": 8.31672223004595e-05,
        "epoch": 0.09160010306622005,
        "step": 711
    },
    {
        "loss": 1.7656,
        "grad_norm": 2.470614433288574,
        "learning_rate": 8.312167888758733e-05,
        "epoch": 0.09172893584127802,
        "step": 712
    },
    {
        "loss": 2.038,
        "grad_norm": 1.6893781423568726,
        "learning_rate": 8.307608645312086e-05,
        "epoch": 0.091857768616336,
        "step": 713
    },
    {
        "loss": 1.2231,
        "grad_norm": 1.0455594062805176,
        "learning_rate": 8.30304450645389e-05,
        "epoch": 0.09198660139139397,
        "step": 714
    },
    {
        "loss": 1.9127,
        "grad_norm": 2.596696138381958,
        "learning_rate": 8.298475478939283e-05,
        "epoch": 0.09211543416645195,
        "step": 715
    },
    {
        "loss": 1.9874,
        "grad_norm": 2.109846830368042,
        "learning_rate": 8.293901569530634e-05,
        "epoch": 0.09224426694150992,
        "step": 716
    },
    {
        "loss": 2.1543,
        "grad_norm": 2.034731149673462,
        "learning_rate": 8.289322784997537e-05,
        "epoch": 0.09237309971656789,
        "step": 717
    },
    {
        "loss": 1.9822,
        "grad_norm": 1.725480318069458,
        "learning_rate": 8.284739132116802e-05,
        "epoch": 0.09250193249162587,
        "step": 718
    },
    {
        "loss": 1.2939,
        "grad_norm": 2.7416062355041504,
        "learning_rate": 8.280150617672442e-05,
        "epoch": 0.09263076526668385,
        "step": 719
    },
    {
        "loss": 1.7534,
        "grad_norm": 2.5977702140808105,
        "learning_rate": 8.27555724845567e-05,
        "epoch": 0.09275959804174182,
        "step": 720
    },
    {
        "loss": 2.8366,
        "grad_norm": 1.7245063781738281,
        "learning_rate": 8.270959031264882e-05,
        "epoch": 0.09288843081679979,
        "step": 721
    },
    {
        "loss": 2.6517,
        "grad_norm": 2.623887538909912,
        "learning_rate": 8.266355972905647e-05,
        "epoch": 0.09301726359185777,
        "step": 722
    },
    {
        "loss": 2.3704,
        "grad_norm": 1.9245812892913818,
        "learning_rate": 8.261748080190703e-05,
        "epoch": 0.09314609636691575,
        "step": 723
    },
    {
        "loss": 2.5256,
        "grad_norm": 1.362389087677002,
        "learning_rate": 8.25713535993994e-05,
        "epoch": 0.09327492914197372,
        "step": 724
    },
    {
        "loss": 2.2613,
        "grad_norm": 1.3297364711761475,
        "learning_rate": 8.252517818980394e-05,
        "epoch": 0.09340376191703169,
        "step": 725
    },
    {
        "loss": 2.4442,
        "grad_norm": 1.5617872476577759,
        "learning_rate": 8.247895464146238e-05,
        "epoch": 0.09353259469208967,
        "step": 726
    },
    {
        "loss": 1.6555,
        "grad_norm": 2.2532272338867188,
        "learning_rate": 8.243268302278767e-05,
        "epoch": 0.09366142746714765,
        "step": 727
    },
    {
        "loss": 2.3595,
        "grad_norm": 2.264089584350586,
        "learning_rate": 8.23863634022639e-05,
        "epoch": 0.09379026024220562,
        "step": 728
    },
    {
        "loss": 1.9463,
        "grad_norm": 1.9754900932312012,
        "learning_rate": 8.233999584844623e-05,
        "epoch": 0.09391909301726359,
        "step": 729
    },
    {
        "loss": 2.2884,
        "grad_norm": 1.8830653429031372,
        "learning_rate": 8.229358042996076e-05,
        "epoch": 0.09404792579232156,
        "step": 730
    },
    {
        "loss": 2.1894,
        "grad_norm": 2.0616815090179443,
        "learning_rate": 8.224711721550439e-05,
        "epoch": 0.09417675856737955,
        "step": 731
    },
    {
        "loss": 2.4587,
        "grad_norm": 1.4446715116500854,
        "learning_rate": 8.220060627384485e-05,
        "epoch": 0.09430559134243752,
        "step": 732
    },
    {
        "loss": 2.5105,
        "grad_norm": 1.3064709901809692,
        "learning_rate": 8.215404767382042e-05,
        "epoch": 0.09443442411749549,
        "step": 733
    },
    {
        "loss": 2.1361,
        "grad_norm": 2.4316608905792236,
        "learning_rate": 8.210744148433996e-05,
        "epoch": 0.09456325689255346,
        "step": 734
    },
    {
        "loss": 2.3887,
        "grad_norm": 1.404064416885376,
        "learning_rate": 8.206078777438275e-05,
        "epoch": 0.09469208966761145,
        "step": 735
    },
    {
        "loss": 1.9367,
        "grad_norm": 3.0313236713409424,
        "learning_rate": 8.20140866129984e-05,
        "epoch": 0.09482092244266942,
        "step": 736
    },
    {
        "loss": 2.1658,
        "grad_norm": 2.0792183876037598,
        "learning_rate": 8.196733806930677e-05,
        "epoch": 0.09494975521772739,
        "step": 737
    },
    {
        "loss": 2.1835,
        "grad_norm": 2.524697780609131,
        "learning_rate": 8.192054221249783e-05,
        "epoch": 0.09507858799278536,
        "step": 738
    },
    {
        "loss": 2.461,
        "grad_norm": 1.3749570846557617,
        "learning_rate": 8.187369911183158e-05,
        "epoch": 0.09520742076784335,
        "step": 739
    },
    {
        "loss": 2.0421,
        "grad_norm": 1.9884841442108154,
        "learning_rate": 8.182680883663794e-05,
        "epoch": 0.09533625354290132,
        "step": 740
    },
    {
        "loss": 2.6913,
        "grad_norm": 1.723279595375061,
        "learning_rate": 8.177987145631667e-05,
        "epoch": 0.09546508631795929,
        "step": 741
    },
    {
        "loss": 2.3523,
        "grad_norm": 1.654388189315796,
        "learning_rate": 8.173288704033722e-05,
        "epoch": 0.09559391909301726,
        "step": 742
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.261587142944336,
        "learning_rate": 8.168585565823867e-05,
        "epoch": 0.09572275186807523,
        "step": 743
    },
    {
        "loss": 2.4119,
        "grad_norm": 1.6249587535858154,
        "learning_rate": 8.163877737962959e-05,
        "epoch": 0.09585158464313322,
        "step": 744
    },
    {
        "loss": 2.6471,
        "grad_norm": 1.4854596853256226,
        "learning_rate": 8.159165227418802e-05,
        "epoch": 0.09598041741819119,
        "step": 745
    },
    {
        "loss": 2.3058,
        "grad_norm": 1.6321953535079956,
        "learning_rate": 8.154448041166122e-05,
        "epoch": 0.09610925019324916,
        "step": 746
    },
    {
        "loss": 1.7432,
        "grad_norm": 2.7210752964019775,
        "learning_rate": 8.149726186186572e-05,
        "epoch": 0.09623808296830713,
        "step": 747
    },
    {
        "loss": 1.998,
        "grad_norm": 1.278765082359314,
        "learning_rate": 8.144999669468714e-05,
        "epoch": 0.09636691574336512,
        "step": 748
    },
    {
        "loss": 2.1879,
        "grad_norm": 1.5865687131881714,
        "learning_rate": 8.140268498008004e-05,
        "epoch": 0.09649574851842309,
        "step": 749
    },
    {
        "loss": 1.8725,
        "grad_norm": 2.316175937652588,
        "learning_rate": 8.135532678806794e-05,
        "epoch": 0.09662458129348106,
        "step": 750
    },
    {
        "loss": 2.4377,
        "grad_norm": 1.2634433507919312,
        "learning_rate": 8.130792218874313e-05,
        "epoch": 0.09675341406853903,
        "step": 751
    },
    {
        "loss": 2.1098,
        "grad_norm": 1.1023741960525513,
        "learning_rate": 8.126047125226657e-05,
        "epoch": 0.09688224684359702,
        "step": 752
    },
    {
        "loss": 2.0811,
        "grad_norm": 2.097412347793579,
        "learning_rate": 8.121297404886779e-05,
        "epoch": 0.09701107961865499,
        "step": 753
    },
    {
        "loss": 2.2436,
        "grad_norm": 2.3081321716308594,
        "learning_rate": 8.116543064884486e-05,
        "epoch": 0.09713991239371296,
        "step": 754
    },
    {
        "loss": 1.8991,
        "grad_norm": 2.0518479347229004,
        "learning_rate": 8.111784112256413e-05,
        "epoch": 0.09726874516877093,
        "step": 755
    },
    {
        "loss": 2.3226,
        "grad_norm": 1.9135921001434326,
        "learning_rate": 8.107020554046028e-05,
        "epoch": 0.09739757794382892,
        "step": 756
    },
    {
        "loss": 1.7318,
        "grad_norm": 2.4879379272460938,
        "learning_rate": 8.102252397303617e-05,
        "epoch": 0.09752641071888689,
        "step": 757
    },
    {
        "loss": 2.7512,
        "grad_norm": 1.2025741338729858,
        "learning_rate": 8.097479649086267e-05,
        "epoch": 0.09765524349394486,
        "step": 758
    },
    {
        "loss": 2.1667,
        "grad_norm": 2.440416097640991,
        "learning_rate": 8.092702316457863e-05,
        "epoch": 0.09778407626900283,
        "step": 759
    },
    {
        "loss": 1.5663,
        "grad_norm": 2.816178798675537,
        "learning_rate": 8.087920406489075e-05,
        "epoch": 0.0979129090440608,
        "step": 760
    },
    {
        "loss": 2.4222,
        "grad_norm": 1.5070178508758545,
        "learning_rate": 8.083133926257349e-05,
        "epoch": 0.09804174181911879,
        "step": 761
    },
    {
        "loss": 2.4151,
        "grad_norm": 1.8894059658050537,
        "learning_rate": 8.078342882846893e-05,
        "epoch": 0.09817057459417676,
        "step": 762
    },
    {
        "loss": 2.0702,
        "grad_norm": 2.159076452255249,
        "learning_rate": 8.073547283348668e-05,
        "epoch": 0.09829940736923473,
        "step": 763
    },
    {
        "loss": 2.0697,
        "grad_norm": 2.0028791427612305,
        "learning_rate": 8.068747134860383e-05,
        "epoch": 0.0984282401442927,
        "step": 764
    },
    {
        "loss": 2.3627,
        "grad_norm": 1.1326680183410645,
        "learning_rate": 8.063942444486474e-05,
        "epoch": 0.09855707291935069,
        "step": 765
    },
    {
        "loss": 2.1362,
        "grad_norm": 1.3119239807128906,
        "learning_rate": 8.059133219338101e-05,
        "epoch": 0.09868590569440866,
        "step": 766
    },
    {
        "loss": 2.4981,
        "grad_norm": 1.8251487016677856,
        "learning_rate": 8.054319466533143e-05,
        "epoch": 0.09881473846946663,
        "step": 767
    },
    {
        "loss": 2.416,
        "grad_norm": 2.0961830615997314,
        "learning_rate": 8.049501193196165e-05,
        "epoch": 0.0989435712445246,
        "step": 768
    },
    {
        "loss": 1.1119,
        "grad_norm": 2.9781620502471924,
        "learning_rate": 8.044678406458434e-05,
        "epoch": 0.09907240401958259,
        "step": 769
    },
    {
        "loss": 2.5772,
        "grad_norm": 1.055347204208374,
        "learning_rate": 8.039851113457897e-05,
        "epoch": 0.09920123679464056,
        "step": 770
    },
    {
        "loss": 2.1818,
        "grad_norm": 1.1273003816604614,
        "learning_rate": 8.035019321339163e-05,
        "epoch": 0.09933006956969853,
        "step": 771
    },
    {
        "loss": 2.4495,
        "grad_norm": 2.274325370788574,
        "learning_rate": 8.030183037253511e-05,
        "epoch": 0.0994589023447565,
        "step": 772
    },
    {
        "loss": 1.8682,
        "grad_norm": 1.7423406839370728,
        "learning_rate": 8.025342268358855e-05,
        "epoch": 0.09958773511981447,
        "step": 773
    },
    {
        "loss": 2.261,
        "grad_norm": 1.7955474853515625,
        "learning_rate": 8.020497021819758e-05,
        "epoch": 0.09971656789487246,
        "step": 774
    },
    {
        "loss": 2.239,
        "grad_norm": 2.587434768676758,
        "learning_rate": 8.015647304807403e-05,
        "epoch": 0.09984540066993043,
        "step": 775
    },
    {
        "loss": 2.5365,
        "grad_norm": 1.8206379413604736,
        "learning_rate": 8.010793124499596e-05,
        "epoch": 0.0999742334449884,
        "step": 776
    },
    {
        "loss": 1.7303,
        "grad_norm": 2.348083019256592,
        "learning_rate": 8.00593448808074e-05,
        "epoch": 0.10010306622004637,
        "step": 777
    },
    {
        "loss": 2.4898,
        "grad_norm": 1.5004651546478271,
        "learning_rate": 8.001071402741842e-05,
        "epoch": 0.10023189899510436,
        "step": 778
    },
    {
        "loss": 1.0604,
        "grad_norm": 2.66689395904541,
        "learning_rate": 7.996203875680489e-05,
        "epoch": 0.10036073177016233,
        "step": 779
    },
    {
        "loss": 1.7981,
        "grad_norm": 1.8190263509750366,
        "learning_rate": 7.991331914100841e-05,
        "epoch": 0.1004895645452203,
        "step": 780
    },
    {
        "loss": 1.8549,
        "grad_norm": 1.4458686113357544,
        "learning_rate": 7.986455525213628e-05,
        "epoch": 0.10061839732027827,
        "step": 781
    },
    {
        "loss": 2.4249,
        "grad_norm": 1.442081332206726,
        "learning_rate": 7.981574716236122e-05,
        "epoch": 0.10074723009533626,
        "step": 782
    },
    {
        "loss": 2.3391,
        "grad_norm": 1.8971381187438965,
        "learning_rate": 7.976689494392147e-05,
        "epoch": 0.10087606287039423,
        "step": 783
    },
    {
        "loss": 1.344,
        "grad_norm": 2.0461509227752686,
        "learning_rate": 7.971799866912053e-05,
        "epoch": 0.1010048956454522,
        "step": 784
    },
    {
        "loss": 1.9832,
        "grad_norm": 2.402458667755127,
        "learning_rate": 7.96690584103271e-05,
        "epoch": 0.10113372842051017,
        "step": 785
    },
    {
        "loss": 2.2543,
        "grad_norm": 1.9493223428726196,
        "learning_rate": 7.9620074239975e-05,
        "epoch": 0.10126256119556815,
        "step": 786
    },
    {
        "loss": 2.054,
        "grad_norm": 1.8371471166610718,
        "learning_rate": 7.957104623056304e-05,
        "epoch": 0.10139139397062613,
        "step": 787
    },
    {
        "loss": 2.027,
        "grad_norm": 2.267979860305786,
        "learning_rate": 7.952197445465492e-05,
        "epoch": 0.1015202267456841,
        "step": 788
    },
    {
        "loss": 2.1963,
        "grad_norm": 1.653867483139038,
        "learning_rate": 7.947285898487909e-05,
        "epoch": 0.10164905952074207,
        "step": 789
    },
    {
        "loss": 2.2571,
        "grad_norm": 1.1320781707763672,
        "learning_rate": 7.942369989392868e-05,
        "epoch": 0.10177789229580005,
        "step": 790
    },
    {
        "loss": 2.176,
        "grad_norm": 1.974792242050171,
        "learning_rate": 7.937449725456137e-05,
        "epoch": 0.10190672507085803,
        "step": 791
    },
    {
        "loss": 2.5208,
        "grad_norm": 1.1478755474090576,
        "learning_rate": 7.932525113959935e-05,
        "epoch": 0.102035557845916,
        "step": 792
    },
    {
        "loss": 2.4662,
        "grad_norm": 2.4533514976501465,
        "learning_rate": 7.92759616219291e-05,
        "epoch": 0.10216439062097397,
        "step": 793
    },
    {
        "loss": 2.0002,
        "grad_norm": 1.9999446868896484,
        "learning_rate": 7.922662877450133e-05,
        "epoch": 0.10229322339603195,
        "step": 794
    },
    {
        "loss": 1.7307,
        "grad_norm": 2.5619609355926514,
        "learning_rate": 7.917725267033094e-05,
        "epoch": 0.10242205617108993,
        "step": 795
    },
    {
        "loss": 2.5098,
        "grad_norm": 1.3750642538070679,
        "learning_rate": 7.912783338249676e-05,
        "epoch": 0.1025508889461479,
        "step": 796
    },
    {
        "loss": 2.1068,
        "grad_norm": 1.7314834594726562,
        "learning_rate": 7.907837098414166e-05,
        "epoch": 0.10267972172120587,
        "step": 797
    },
    {
        "loss": 2.3671,
        "grad_norm": 1.2769749164581299,
        "learning_rate": 7.902886554847219e-05,
        "epoch": 0.10280855449626385,
        "step": 798
    },
    {
        "loss": 2.6385,
        "grad_norm": 1.2063419818878174,
        "learning_rate": 7.897931714875866e-05,
        "epoch": 0.10293738727132182,
        "step": 799
    },
    {
        "loss": 2.0918,
        "grad_norm": 1.0150642395019531,
        "learning_rate": 7.892972585833497e-05,
        "epoch": 0.1030662200463798,
        "step": 800
    },
    {
        "loss": 2.0209,
        "grad_norm": 1.99598228931427,
        "learning_rate": 7.88800917505985e-05,
        "epoch": 0.10319505282143777,
        "step": 801
    },
    {
        "loss": 1.762,
        "grad_norm": 2.479365348815918,
        "learning_rate": 7.883041489900997e-05,
        "epoch": 0.10332388559649575,
        "step": 802
    },
    {
        "loss": 1.7123,
        "grad_norm": 2.8005013465881348,
        "learning_rate": 7.878069537709339e-05,
        "epoch": 0.10345271837155372,
        "step": 803
    },
    {
        "loss": 2.1131,
        "grad_norm": 2.3094849586486816,
        "learning_rate": 7.873093325843592e-05,
        "epoch": 0.1035815511466117,
        "step": 804
    },
    {
        "loss": 2.2139,
        "grad_norm": 2.5120692253112793,
        "learning_rate": 7.868112861668778e-05,
        "epoch": 0.10371038392166967,
        "step": 805
    },
    {
        "loss": 2.7438,
        "grad_norm": 1.679635763168335,
        "learning_rate": 7.863128152556207e-05,
        "epoch": 0.10383921669672765,
        "step": 806
    },
    {
        "loss": 2.6689,
        "grad_norm": 1.2844096422195435,
        "learning_rate": 7.85813920588348e-05,
        "epoch": 0.10396804947178562,
        "step": 807
    },
    {
        "loss": 2.4549,
        "grad_norm": 1.380679726600647,
        "learning_rate": 7.853146029034464e-05,
        "epoch": 0.1040968822468436,
        "step": 808
    },
    {
        "loss": 2.6164,
        "grad_norm": 1.584796667098999,
        "learning_rate": 7.848148629399285e-05,
        "epoch": 0.10422571502190157,
        "step": 809
    },
    {
        "loss": 2.4014,
        "grad_norm": 1.203482747077942,
        "learning_rate": 7.84314701437433e-05,
        "epoch": 0.10435454779695955,
        "step": 810
    },
    {
        "loss": 2.3411,
        "grad_norm": 1.5248655080795288,
        "learning_rate": 7.838141191362209e-05,
        "epoch": 0.10448338057201752,
        "step": 811
    },
    {
        "loss": 2.2415,
        "grad_norm": 1.7073955535888672,
        "learning_rate": 7.833131167771771e-05,
        "epoch": 0.1046122133470755,
        "step": 812
    },
    {
        "loss": 2.305,
        "grad_norm": 2.6211278438568115,
        "learning_rate": 7.828116951018081e-05,
        "epoch": 0.10474104612213347,
        "step": 813
    },
    {
        "loss": 2.5634,
        "grad_norm": 1.3531712293624878,
        "learning_rate": 7.823098548522407e-05,
        "epoch": 0.10486987889719145,
        "step": 814
    },
    {
        "loss": 2.3879,
        "grad_norm": 1.690092921257019,
        "learning_rate": 7.818075967712214e-05,
        "epoch": 0.10499871167224942,
        "step": 815
    },
    {
        "loss": 1.952,
        "grad_norm": 2.0136778354644775,
        "learning_rate": 7.813049216021148e-05,
        "epoch": 0.10512754444730739,
        "step": 816
    },
    {
        "loss": 1.9099,
        "grad_norm": 2.834991931915283,
        "learning_rate": 7.808018300889035e-05,
        "epoch": 0.10525637722236537,
        "step": 817
    },
    {
        "loss": 2.109,
        "grad_norm": 1.9849231243133545,
        "learning_rate": 7.802983229761858e-05,
        "epoch": 0.10538520999742335,
        "step": 818
    },
    {
        "loss": 2.5306,
        "grad_norm": 1.9380171298980713,
        "learning_rate": 7.797944010091747e-05,
        "epoch": 0.10551404277248132,
        "step": 819
    },
    {
        "loss": 2.261,
        "grad_norm": 2.4744608402252197,
        "learning_rate": 7.79290064933698e-05,
        "epoch": 0.10564287554753929,
        "step": 820
    },
    {
        "loss": 2.0219,
        "grad_norm": 2.7087812423706055,
        "learning_rate": 7.787853154961962e-05,
        "epoch": 0.10577170832259727,
        "step": 821
    },
    {
        "loss": 2.2659,
        "grad_norm": 1.825737476348877,
        "learning_rate": 7.782801534437212e-05,
        "epoch": 0.10590054109765525,
        "step": 822
    },
    {
        "loss": 2.549,
        "grad_norm": 2.472362756729126,
        "learning_rate": 7.777745795239361e-05,
        "epoch": 0.10602937387271322,
        "step": 823
    },
    {
        "loss": 2.199,
        "grad_norm": 1.851259469985962,
        "learning_rate": 7.772685944851132e-05,
        "epoch": 0.10615820664777119,
        "step": 824
    },
    {
        "loss": 1.8773,
        "grad_norm": 2.1721062660217285,
        "learning_rate": 7.767621990761335e-05,
        "epoch": 0.10628703942282917,
        "step": 825
    },
    {
        "loss": 2.6011,
        "grad_norm": 2.085326910018921,
        "learning_rate": 7.762553940464851e-05,
        "epoch": 0.10641587219788715,
        "step": 826
    },
    {
        "loss": 1.6461,
        "grad_norm": 1.7280645370483398,
        "learning_rate": 7.757481801462629e-05,
        "epoch": 0.10654470497294512,
        "step": 827
    },
    {
        "loss": 2.4303,
        "grad_norm": 1.3224155902862549,
        "learning_rate": 7.752405581261664e-05,
        "epoch": 0.10667353774800309,
        "step": 828
    },
    {
        "loss": 2.1742,
        "grad_norm": 0.9670929908752441,
        "learning_rate": 7.747325287374992e-05,
        "epoch": 0.10680237052306106,
        "step": 829
    },
    {
        "loss": 2.3182,
        "grad_norm": 2.221911907196045,
        "learning_rate": 7.74224092732168e-05,
        "epoch": 0.10693120329811905,
        "step": 830
    },
    {
        "loss": 2.5584,
        "grad_norm": 1.3550095558166504,
        "learning_rate": 7.737152508626814e-05,
        "epoch": 0.10706003607317702,
        "step": 831
    },
    {
        "loss": 2.014,
        "grad_norm": 2.1663362979888916,
        "learning_rate": 7.732060038821485e-05,
        "epoch": 0.10718886884823499,
        "step": 832
    },
    {
        "loss": 2.3055,
        "grad_norm": 2.081190824508667,
        "learning_rate": 7.726963525442779e-05,
        "epoch": 0.10731770162329296,
        "step": 833
    },
    {
        "loss": 1.9436,
        "grad_norm": 2.9437859058380127,
        "learning_rate": 7.721862976033771e-05,
        "epoch": 0.10744653439835095,
        "step": 834
    },
    {
        "loss": 2.3835,
        "grad_norm": 1.56121027469635,
        "learning_rate": 7.716758398143503e-05,
        "epoch": 0.10757536717340892,
        "step": 835
    },
    {
        "loss": 2.2569,
        "grad_norm": 1.9033262729644775,
        "learning_rate": 7.711649799326986e-05,
        "epoch": 0.10770419994846689,
        "step": 836
    },
    {
        "loss": 2.0595,
        "grad_norm": 2.077369213104248,
        "learning_rate": 7.706537187145177e-05,
        "epoch": 0.10783303272352486,
        "step": 837
    },
    {
        "loss": 2.2957,
        "grad_norm": 1.181806206703186,
        "learning_rate": 7.701420569164977e-05,
        "epoch": 0.10796186549858285,
        "step": 838
    },
    {
        "loss": 1.9646,
        "grad_norm": 2.4951701164245605,
        "learning_rate": 7.696299952959211e-05,
        "epoch": 0.10809069827364082,
        "step": 839
    },
    {
        "loss": 2.1047,
        "grad_norm": 1.4058375358581543,
        "learning_rate": 7.691175346106627e-05,
        "epoch": 0.10821953104869879,
        "step": 840
    },
    {
        "loss": 1.841,
        "grad_norm": 2.1158719062805176,
        "learning_rate": 7.686046756191875e-05,
        "epoch": 0.10834836382375676,
        "step": 841
    },
    {
        "loss": 2.8831,
        "grad_norm": 1.5411652326583862,
        "learning_rate": 7.680914190805504e-05,
        "epoch": 0.10847719659881473,
        "step": 842
    },
    {
        "loss": 2.0513,
        "grad_norm": 1.140065312385559,
        "learning_rate": 7.675777657543941e-05,
        "epoch": 0.10860602937387272,
        "step": 843
    },
    {
        "loss": 2.7288,
        "grad_norm": 1.6917226314544678,
        "learning_rate": 7.670637164009494e-05,
        "epoch": 0.10873486214893069,
        "step": 844
    },
    {
        "loss": 2.1885,
        "grad_norm": 2.146171808242798,
        "learning_rate": 7.665492717810326e-05,
        "epoch": 0.10886369492398866,
        "step": 845
    },
    {
        "loss": 2.0539,
        "grad_norm": 2.7068333625793457,
        "learning_rate": 7.660344326560449e-05,
        "epoch": 0.10899252769904663,
        "step": 846
    },
    {
        "loss": 2.4552,
        "grad_norm": 1.2603007555007935,
        "learning_rate": 7.655191997879718e-05,
        "epoch": 0.10912136047410462,
        "step": 847
    },
    {
        "loss": 2.2473,
        "grad_norm": 1.672879695892334,
        "learning_rate": 7.650035739393816e-05,
        "epoch": 0.10925019324916259,
        "step": 848
    },
    {
        "loss": 2.3542,
        "grad_norm": 2.042973041534424,
        "learning_rate": 7.644875558734242e-05,
        "epoch": 0.10937902602422056,
        "step": 849
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.1956756114959717,
        "learning_rate": 7.639711463538295e-05,
        "epoch": 0.10950785879927853,
        "step": 850
    },
    {
        "loss": 1.5047,
        "grad_norm": 2.259467601776123,
        "learning_rate": 7.634543461449072e-05,
        "epoch": 0.10963669157433652,
        "step": 851
    },
    {
        "loss": 2.3528,
        "grad_norm": 1.786664366722107,
        "learning_rate": 7.629371560115453e-05,
        "epoch": 0.10976552434939449,
        "step": 852
    },
    {
        "loss": 2.0126,
        "grad_norm": 1.5640653371810913,
        "learning_rate": 7.624195767192085e-05,
        "epoch": 0.10989435712445246,
        "step": 853
    },
    {
        "loss": 2.3602,
        "grad_norm": 1.614176869392395,
        "learning_rate": 7.619016090339379e-05,
        "epoch": 0.11002318989951043,
        "step": 854
    },
    {
        "loss": 1.4599,
        "grad_norm": 2.568896532058716,
        "learning_rate": 7.613832537223496e-05,
        "epoch": 0.1101520226745684,
        "step": 855
    },
    {
        "loss": 2.2842,
        "grad_norm": 1.788185954093933,
        "learning_rate": 7.608645115516327e-05,
        "epoch": 0.11028085544962639,
        "step": 856
    },
    {
        "loss": 1.9425,
        "grad_norm": 2.8562328815460205,
        "learning_rate": 7.603453832895491e-05,
        "epoch": 0.11040968822468436,
        "step": 857
    },
    {
        "loss": 1.8671,
        "grad_norm": 3.460674285888672,
        "learning_rate": 7.598258697044327e-05,
        "epoch": 0.11053852099974233,
        "step": 858
    },
    {
        "loss": 1.1574,
        "grad_norm": 1.2763689756393433,
        "learning_rate": 7.593059715651872e-05,
        "epoch": 0.1106673537748003,
        "step": 859
    },
    {
        "loss": 2.2466,
        "grad_norm": 2.502429246902466,
        "learning_rate": 7.587856896412853e-05,
        "epoch": 0.11079618654985829,
        "step": 860
    },
    {
        "loss": 1.8611,
        "grad_norm": 1.616079330444336,
        "learning_rate": 7.582650247027684e-05,
        "epoch": 0.11092501932491626,
        "step": 861
    },
    {
        "loss": 2.3226,
        "grad_norm": 1.602262020111084,
        "learning_rate": 7.577439775202437e-05,
        "epoch": 0.11105385209997423,
        "step": 862
    },
    {
        "loss": 2.3696,
        "grad_norm": 2.1884045600891113,
        "learning_rate": 7.572225488648855e-05,
        "epoch": 0.1111826848750322,
        "step": 863
    },
    {
        "loss": 1.9804,
        "grad_norm": 2.312631845474243,
        "learning_rate": 7.567007395084313e-05,
        "epoch": 0.11131151765009019,
        "step": 864
    },
    {
        "loss": 1.9787,
        "grad_norm": 2.7023768424987793,
        "learning_rate": 7.561785502231833e-05,
        "epoch": 0.11144035042514816,
        "step": 865
    },
    {
        "loss": 2.0697,
        "grad_norm": 2.7435390949249268,
        "learning_rate": 7.556559817820051e-05,
        "epoch": 0.11156918320020613,
        "step": 866
    },
    {
        "loss": 2.3232,
        "grad_norm": 1.698425054550171,
        "learning_rate": 7.551330349583218e-05,
        "epoch": 0.1116980159752641,
        "step": 867
    },
    {
        "loss": 2.0271,
        "grad_norm": 2.4684994220733643,
        "learning_rate": 7.546097105261187e-05,
        "epoch": 0.11182684875032209,
        "step": 868
    },
    {
        "loss": 1.5094,
        "grad_norm": 2.8421976566314697,
        "learning_rate": 7.540860092599396e-05,
        "epoch": 0.11195568152538006,
        "step": 869
    },
    {
        "loss": 1.9883,
        "grad_norm": 1.4521214962005615,
        "learning_rate": 7.535619319348866e-05,
        "epoch": 0.11208451430043803,
        "step": 870
    },
    {
        "loss": 2.0219,
        "grad_norm": 1.8270602226257324,
        "learning_rate": 7.530374793266178e-05,
        "epoch": 0.112213347075496,
        "step": 871
    },
    {
        "loss": 1.85,
        "grad_norm": 1.4644253253936768,
        "learning_rate": 7.525126522113467e-05,
        "epoch": 0.11234217985055397,
        "step": 872
    },
    {
        "loss": 2.0311,
        "grad_norm": 2.8849127292633057,
        "learning_rate": 7.519874513658418e-05,
        "epoch": 0.11247101262561196,
        "step": 873
    },
    {
        "loss": 1.5933,
        "grad_norm": 2.514282703399658,
        "learning_rate": 7.514618775674242e-05,
        "epoch": 0.11259984540066993,
        "step": 874
    },
    {
        "loss": 2.3909,
        "grad_norm": 1.7938830852508545,
        "learning_rate": 7.50935931593967e-05,
        "epoch": 0.1127286781757279,
        "step": 875
    },
    {
        "loss": 2.6113,
        "grad_norm": 1.339698314666748,
        "learning_rate": 7.504096142238941e-05,
        "epoch": 0.11285751095078587,
        "step": 876
    },
    {
        "loss": 2.0672,
        "grad_norm": 2.564300298690796,
        "learning_rate": 7.498829262361793e-05,
        "epoch": 0.11298634372584386,
        "step": 877
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.0486950874328613,
        "learning_rate": 7.493558684103448e-05,
        "epoch": 0.11311517650090183,
        "step": 878
    },
    {
        "loss": 1.9701,
        "grad_norm": 2.2108614444732666,
        "learning_rate": 7.488284415264602e-05,
        "epoch": 0.1132440092759598,
        "step": 879
    },
    {
        "loss": 2.2084,
        "grad_norm": 1.2712644338607788,
        "learning_rate": 7.483006463651416e-05,
        "epoch": 0.11337284205101777,
        "step": 880
    },
    {
        "loss": 1.6932,
        "grad_norm": 2.328777551651001,
        "learning_rate": 7.477724837075494e-05,
        "epoch": 0.11350167482607576,
        "step": 881
    },
    {
        "loss": 2.4063,
        "grad_norm": 1.8046526908874512,
        "learning_rate": 7.472439543353886e-05,
        "epoch": 0.11363050760113373,
        "step": 882
    },
    {
        "loss": 2.7284,
        "grad_norm": 1.2482038736343384,
        "learning_rate": 7.467150590309069e-05,
        "epoch": 0.1137593403761917,
        "step": 883
    },
    {
        "loss": 2.2041,
        "grad_norm": 2.3479838371276855,
        "learning_rate": 7.461857985768934e-05,
        "epoch": 0.11388817315124967,
        "step": 884
    },
    {
        "loss": 2.2848,
        "grad_norm": 1.1121959686279297,
        "learning_rate": 7.456561737566778e-05,
        "epoch": 0.11401700592630765,
        "step": 885
    },
    {
        "loss": 1.8628,
        "grad_norm": 2.3710989952087402,
        "learning_rate": 7.451261853541286e-05,
        "epoch": 0.11414583870136563,
        "step": 886
    },
    {
        "loss": 2.0568,
        "grad_norm": 2.291555881500244,
        "learning_rate": 7.44595834153653e-05,
        "epoch": 0.1142746714764236,
        "step": 887
    },
    {
        "loss": 1.9586,
        "grad_norm": 2.2304086685180664,
        "learning_rate": 7.440651209401951e-05,
        "epoch": 0.11440350425148157,
        "step": 888
    },
    {
        "loss": 2.4223,
        "grad_norm": 1.1644662618637085,
        "learning_rate": 7.435340464992344e-05,
        "epoch": 0.11453233702653955,
        "step": 889
    },
    {
        "loss": 2.4274,
        "grad_norm": 2.4360668659210205,
        "learning_rate": 7.430026116167855e-05,
        "epoch": 0.11466116980159753,
        "step": 890
    },
    {
        "loss": 2.0702,
        "grad_norm": 1.9769680500030518,
        "learning_rate": 7.424708170793962e-05,
        "epoch": 0.1147900025766555,
        "step": 891
    },
    {
        "loss": 1.9647,
        "grad_norm": 2.1491541862487793,
        "learning_rate": 7.419386636741464e-05,
        "epoch": 0.11491883535171347,
        "step": 892
    },
    {
        "loss": 2.3148,
        "grad_norm": 1.2572293281555176,
        "learning_rate": 7.414061521886478e-05,
        "epoch": 0.11504766812677145,
        "step": 893
    },
    {
        "loss": 1.6228,
        "grad_norm": 2.8580448627471924,
        "learning_rate": 7.408732834110413e-05,
        "epoch": 0.11517650090182943,
        "step": 894
    },
    {
        "loss": 1.8131,
        "grad_norm": 2.103386402130127,
        "learning_rate": 7.403400581299972e-05,
        "epoch": 0.1153053336768874,
        "step": 895
    },
    {
        "loss": 2.2552,
        "grad_norm": 0.9910520911216736,
        "learning_rate": 7.398064771347131e-05,
        "epoch": 0.11543416645194537,
        "step": 896
    },
    {
        "loss": 1.6702,
        "grad_norm": 2.408137559890747,
        "learning_rate": 7.392725412149133e-05,
        "epoch": 0.11556299922700335,
        "step": 897
    },
    {
        "loss": 2.1744,
        "grad_norm": 1.6486146450042725,
        "learning_rate": 7.387382511608471e-05,
        "epoch": 0.11569183200206132,
        "step": 898
    },
    {
        "loss": 2.3057,
        "grad_norm": 1.7147501707077026,
        "learning_rate": 7.382036077632885e-05,
        "epoch": 0.1158206647771193,
        "step": 899
    },
    {
        "loss": 2.3981,
        "grad_norm": 1.4939029216766357,
        "learning_rate": 7.376686118135336e-05,
        "epoch": 0.11594949755217727,
        "step": 900
    },
    {
        "loss": 2.4848,
        "grad_norm": 1.5511833429336548,
        "learning_rate": 7.371332641034013e-05,
        "epoch": 0.11607833032723525,
        "step": 901
    },
    {
        "loss": 1.5339,
        "grad_norm": 2.490776300430298,
        "learning_rate": 7.365975654252301e-05,
        "epoch": 0.11620716310229322,
        "step": 902
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.756171703338623,
        "learning_rate": 7.360615165718789e-05,
        "epoch": 0.1163359958773512,
        "step": 903
    },
    {
        "loss": 2.3242,
        "grad_norm": 1.4842478036880493,
        "learning_rate": 7.355251183367244e-05,
        "epoch": 0.11646482865240917,
        "step": 904
    },
    {
        "loss": 2.331,
        "grad_norm": 1.7094944715499878,
        "learning_rate": 7.3498837151366e-05,
        "epoch": 0.11659366142746715,
        "step": 905
    },
    {
        "loss": 1.9106,
        "grad_norm": 2.1780853271484375,
        "learning_rate": 7.344512768970961e-05,
        "epoch": 0.11672249420252512,
        "step": 906
    },
    {
        "loss": 2.5487,
        "grad_norm": 1.6938284635543823,
        "learning_rate": 7.339138352819564e-05,
        "epoch": 0.1168513269775831,
        "step": 907
    },
    {
        "loss": 1.968,
        "grad_norm": 1.4521448612213135,
        "learning_rate": 7.333760474636797e-05,
        "epoch": 0.11698015975264107,
        "step": 908
    },
    {
        "loss": 2.1827,
        "grad_norm": 1.3005868196487427,
        "learning_rate": 7.328379142382163e-05,
        "epoch": 0.11710899252769905,
        "step": 909
    },
    {
        "loss": 2.2722,
        "grad_norm": 1.9575293064117432,
        "learning_rate": 7.322994364020279e-05,
        "epoch": 0.11723782530275702,
        "step": 910
    },
    {
        "loss": 2.2513,
        "grad_norm": 1.3977059125900269,
        "learning_rate": 7.31760614752086e-05,
        "epoch": 0.11736665807781499,
        "step": 911
    },
    {
        "loss": 2.3193,
        "grad_norm": 1.6285755634307861,
        "learning_rate": 7.312214500858714e-05,
        "epoch": 0.11749549085287297,
        "step": 912
    },
    {
        "loss": 2.244,
        "grad_norm": 2.0069169998168945,
        "learning_rate": 7.306819432013725e-05,
        "epoch": 0.11762432362793095,
        "step": 913
    },
    {
        "loss": 2.2062,
        "grad_norm": 1.9553639888763428,
        "learning_rate": 7.301420948970838e-05,
        "epoch": 0.11775315640298892,
        "step": 914
    },
    {
        "loss": 2.2738,
        "grad_norm": 1.29775869846344,
        "learning_rate": 7.296019059720054e-05,
        "epoch": 0.11788198917804689,
        "step": 915
    },
    {
        "loss": 2.4438,
        "grad_norm": 1.4616954326629639,
        "learning_rate": 7.290613772256418e-05,
        "epoch": 0.11801082195310487,
        "step": 916
    },
    {
        "loss": 1.4652,
        "grad_norm": 2.707308053970337,
        "learning_rate": 7.28520509458e-05,
        "epoch": 0.11813965472816285,
        "step": 917
    },
    {
        "loss": 2.0209,
        "grad_norm": 2.0665831565856934,
        "learning_rate": 7.279793034695891e-05,
        "epoch": 0.11826848750322082,
        "step": 918
    },
    {
        "loss": 1.8534,
        "grad_norm": 3.057313919067383,
        "learning_rate": 7.274377600614184e-05,
        "epoch": 0.11839732027827879,
        "step": 919
    },
    {
        "loss": 1.7844,
        "grad_norm": 1.9917114973068237,
        "learning_rate": 7.26895880034997e-05,
        "epoch": 0.11852615305333677,
        "step": 920
    },
    {
        "loss": 0.8593,
        "grad_norm": 3.1938178539276123,
        "learning_rate": 7.263536641923318e-05,
        "epoch": 0.11865498582839475,
        "step": 921
    },
    {
        "loss": 2.3692,
        "grad_norm": 1.2872580289840698,
        "learning_rate": 7.258111133359274e-05,
        "epoch": 0.11878381860345272,
        "step": 922
    },
    {
        "loss": 1.8747,
        "grad_norm": 1.0713701248168945,
        "learning_rate": 7.252682282687833e-05,
        "epoch": 0.11891265137851069,
        "step": 923
    },
    {
        "loss": 2.2805,
        "grad_norm": 2.215054988861084,
        "learning_rate": 7.247250097943944e-05,
        "epoch": 0.11904148415356867,
        "step": 924
    },
    {
        "loss": 2.5021,
        "grad_norm": 1.3786249160766602,
        "learning_rate": 7.241814587167488e-05,
        "epoch": 0.11917031692862665,
        "step": 925
    },
    {
        "loss": 2.1626,
        "grad_norm": 2.207657814025879,
        "learning_rate": 7.236375758403268e-05,
        "epoch": 0.11929914970368462,
        "step": 926
    },
    {
        "loss": 1.8499,
        "grad_norm": 2.0423076152801514,
        "learning_rate": 7.230933619701001e-05,
        "epoch": 0.11942798247874259,
        "step": 927
    },
    {
        "loss": 1.8975,
        "grad_norm": 2.1243317127227783,
        "learning_rate": 7.225488179115297e-05,
        "epoch": 0.11955681525380056,
        "step": 928
    },
    {
        "loss": 1.561,
        "grad_norm": 2.4347403049468994,
        "learning_rate": 7.220039444705658e-05,
        "epoch": 0.11968564802885855,
        "step": 929
    },
    {
        "loss": 2.5004,
        "grad_norm": 1.0688527822494507,
        "learning_rate": 7.214587424536456e-05,
        "epoch": 0.11981448080391652,
        "step": 930
    },
    {
        "loss": 1.9774,
        "grad_norm": 1.4084715843200684,
        "learning_rate": 7.209132126676934e-05,
        "epoch": 0.11994331357897449,
        "step": 931
    },
    {
        "loss": 2.0108,
        "grad_norm": 1.4076082706451416,
        "learning_rate": 7.20367355920118e-05,
        "epoch": 0.12007214635403246,
        "step": 932
    },
    {
        "loss": 2.2646,
        "grad_norm": 2.8476250171661377,
        "learning_rate": 7.19821173018812e-05,
        "epoch": 0.12020097912909045,
        "step": 933
    },
    {
        "loss": 2.1935,
        "grad_norm": 1.400325059890747,
        "learning_rate": 7.192746647721514e-05,
        "epoch": 0.12032981190414842,
        "step": 934
    },
    {
        "loss": 2.1584,
        "grad_norm": 1.383470892906189,
        "learning_rate": 7.187278319889931e-05,
        "epoch": 0.12045864467920639,
        "step": 935
    },
    {
        "loss": 1.7282,
        "grad_norm": 1.2642426490783691,
        "learning_rate": 7.181806754786744e-05,
        "epoch": 0.12058747745426436,
        "step": 936
    },
    {
        "loss": 1.742,
        "grad_norm": 2.780660629272461,
        "learning_rate": 7.176331960510121e-05,
        "epoch": 0.12071631022932235,
        "step": 937
    },
    {
        "loss": 2.1166,
        "grad_norm": 1.5694342851638794,
        "learning_rate": 7.170853945163008e-05,
        "epoch": 0.12084514300438032,
        "step": 938
    },
    {
        "loss": 2.3127,
        "grad_norm": 1.3191308975219727,
        "learning_rate": 7.165372716853113e-05,
        "epoch": 0.12097397577943829,
        "step": 939
    },
    {
        "loss": 2.0262,
        "grad_norm": 1.4179532527923584,
        "learning_rate": 7.159888283692908e-05,
        "epoch": 0.12110280855449626,
        "step": 940
    },
    {
        "loss": 2.483,
        "grad_norm": 2.002049684524536,
        "learning_rate": 7.154400653799602e-05,
        "epoch": 0.12123164132955423,
        "step": 941
    },
    {
        "loss": 2.3794,
        "grad_norm": 2.1015853881835938,
        "learning_rate": 7.148909835295137e-05,
        "epoch": 0.12136047410461222,
        "step": 942
    },
    {
        "loss": 1.842,
        "grad_norm": 1.740189552307129,
        "learning_rate": 7.143415836306178e-05,
        "epoch": 0.12148930687967019,
        "step": 943
    },
    {
        "loss": 1.8393,
        "grad_norm": 2.2957875728607178,
        "learning_rate": 7.137918664964089e-05,
        "epoch": 0.12161813965472816,
        "step": 944
    },
    {
        "loss": 2.4104,
        "grad_norm": 2.1646649837493896,
        "learning_rate": 7.132418329404937e-05,
        "epoch": 0.12174697242978613,
        "step": 945
    },
    {
        "loss": 2.1073,
        "grad_norm": 1.613876223564148,
        "learning_rate": 7.126914837769469e-05,
        "epoch": 0.12187580520484412,
        "step": 946
    },
    {
        "loss": 2.3919,
        "grad_norm": 1.9317255020141602,
        "learning_rate": 7.121408198203104e-05,
        "epoch": 0.12200463797990209,
        "step": 947
    },
    {
        "loss": 2.5663,
        "grad_norm": 1.4745280742645264,
        "learning_rate": 7.115898418855918e-05,
        "epoch": 0.12213347075496006,
        "step": 948
    },
    {
        "loss": 2.2055,
        "grad_norm": 1.7696666717529297,
        "learning_rate": 7.110385507882636e-05,
        "epoch": 0.12226230353001803,
        "step": 949
    },
    {
        "loss": 2.3276,
        "grad_norm": 1.7666161060333252,
        "learning_rate": 7.104869473442618e-05,
        "epoch": 0.12239113630507602,
        "step": 950
    },
    {
        "loss": 1.6024,
        "grad_norm": 2.548422336578369,
        "learning_rate": 7.099350323699846e-05,
        "epoch": 0.12251996908013399,
        "step": 951
    },
    {
        "loss": 2.0204,
        "grad_norm": 2.024231433868408,
        "learning_rate": 7.093828066822915e-05,
        "epoch": 0.12264880185519196,
        "step": 952
    },
    {
        "loss": 1.8143,
        "grad_norm": 1.7665449380874634,
        "learning_rate": 7.088302710985014e-05,
        "epoch": 0.12277763463024993,
        "step": 953
    },
    {
        "loss": 2.2817,
        "grad_norm": 2.129974126815796,
        "learning_rate": 7.082774264363923e-05,
        "epoch": 0.1229064674053079,
        "step": 954
    },
    {
        "loss": 2.1879,
        "grad_norm": 1.646865963935852,
        "learning_rate": 7.077242735141994e-05,
        "epoch": 0.12303530018036589,
        "step": 955
    },
    {
        "loss": 1.9469,
        "grad_norm": 2.098836898803711,
        "learning_rate": 7.071708131506145e-05,
        "epoch": 0.12316413295542386,
        "step": 956
    },
    {
        "loss": 2.4495,
        "grad_norm": 1.506496787071228,
        "learning_rate": 7.066170461647837e-05,
        "epoch": 0.12329296573048183,
        "step": 957
    },
    {
        "loss": 2.3367,
        "grad_norm": 1.8646677732467651,
        "learning_rate": 7.060629733763078e-05,
        "epoch": 0.1234217985055398,
        "step": 958
    },
    {
        "loss": 2.2982,
        "grad_norm": 1.8784980773925781,
        "learning_rate": 7.055085956052396e-05,
        "epoch": 0.12355063128059779,
        "step": 959
    },
    {
        "loss": 2.5299,
        "grad_norm": 1.455515742301941,
        "learning_rate": 7.049539136720834e-05,
        "epoch": 0.12367946405565576,
        "step": 960
    },
    {
        "loss": 2.2226,
        "grad_norm": 2.2481727600097656,
        "learning_rate": 7.04398928397794e-05,
        "epoch": 0.12380829683071373,
        "step": 961
    },
    {
        "loss": 1.4135,
        "grad_norm": 2.503093719482422,
        "learning_rate": 7.038436406037746e-05,
        "epoch": 0.1239371296057717,
        "step": 962
    },
    {
        "loss": 1.4025,
        "grad_norm": 3.086787700653076,
        "learning_rate": 7.032880511118766e-05,
        "epoch": 0.12406596238082969,
        "step": 963
    },
    {
        "loss": 1.6566,
        "grad_norm": 2.3562560081481934,
        "learning_rate": 7.027321607443979e-05,
        "epoch": 0.12419479515588766,
        "step": 964
    },
    {
        "loss": 2.3623,
        "grad_norm": 1.783430576324463,
        "learning_rate": 7.021759703240811e-05,
        "epoch": 0.12432362793094563,
        "step": 965
    },
    {
        "loss": 2.2747,
        "grad_norm": 1.7018674612045288,
        "learning_rate": 7.016194806741138e-05,
        "epoch": 0.1244524607060036,
        "step": 966
    },
    {
        "loss": 2.1951,
        "grad_norm": 2.155752658843994,
        "learning_rate": 7.01062692618126e-05,
        "epoch": 0.12458129348106158,
        "step": 967
    },
    {
        "loss": 1.6461,
        "grad_norm": 2.304293632507324,
        "learning_rate": 7.005056069801895e-05,
        "epoch": 0.12471012625611956,
        "step": 968
    },
    {
        "loss": 1.7703,
        "grad_norm": 1.893638253211975,
        "learning_rate": 6.999482245848162e-05,
        "epoch": 0.12483895903117753,
        "step": 969
    },
    {
        "loss": 1.8352,
        "grad_norm": 2.0951900482177734,
        "learning_rate": 6.993905462569575e-05,
        "epoch": 0.1249677918062355,
        "step": 970
    },
    {
        "loss": 2.6403,
        "grad_norm": 1.8119882345199585,
        "learning_rate": 6.988325728220026e-05,
        "epoch": 0.12509662458129348,
        "step": 971
    },
    {
        "loss": 1.5623,
        "grad_norm": 2.951704978942871,
        "learning_rate": 6.982743051057781e-05,
        "epoch": 0.12522545735635146,
        "step": 972
    },
    {
        "loss": 2.5036,
        "grad_norm": 1.1103848218917847,
        "learning_rate": 6.977157439345452e-05,
        "epoch": 0.12535429013140942,
        "step": 973
    },
    {
        "loss": 2.4209,
        "grad_norm": 1.4489792585372925,
        "learning_rate": 6.97156890135e-05,
        "epoch": 0.1254831229064674,
        "step": 974
    },
    {
        "loss": 2.5546,
        "grad_norm": 1.2532700300216675,
        "learning_rate": 6.965977445342716e-05,
        "epoch": 0.1256119556815254,
        "step": 975
    },
    {
        "loss": 2.0667,
        "grad_norm": 2.543179750442505,
        "learning_rate": 6.96038307959921e-05,
        "epoch": 0.12574078845658335,
        "step": 976
    },
    {
        "loss": 2.2538,
        "grad_norm": 2.0113377571105957,
        "learning_rate": 6.954785812399399e-05,
        "epoch": 0.12586962123164133,
        "step": 977
    },
    {
        "loss": 1.2409,
        "grad_norm": 2.4519286155700684,
        "learning_rate": 6.949185652027493e-05,
        "epoch": 0.12599845400669932,
        "step": 978
    },
    {
        "loss": 2.1558,
        "grad_norm": 2.084108591079712,
        "learning_rate": 6.943582606771985e-05,
        "epoch": 0.12612728678175728,
        "step": 979
    },
    {
        "loss": 2.2406,
        "grad_norm": 1.479924201965332,
        "learning_rate": 6.937976684925635e-05,
        "epoch": 0.12625611955681526,
        "step": 980
    },
    {
        "loss": 2.1791,
        "grad_norm": 2.4958038330078125,
        "learning_rate": 6.932367894785467e-05,
        "epoch": 0.12638495233187322,
        "step": 981
    },
    {
        "loss": 1.4804,
        "grad_norm": 1.5148247480392456,
        "learning_rate": 6.926756244652741e-05,
        "epoch": 0.1265137851069312,
        "step": 982
    },
    {
        "loss": 2.0667,
        "grad_norm": 2.4173030853271484,
        "learning_rate": 6.92114174283296e-05,
        "epoch": 0.1266426178819892,
        "step": 983
    },
    {
        "loss": 2.3566,
        "grad_norm": 1.7797833681106567,
        "learning_rate": 6.915524397635839e-05,
        "epoch": 0.12677145065704715,
        "step": 984
    },
    {
        "loss": 2.0062,
        "grad_norm": 3.1194159984588623,
        "learning_rate": 6.909904217375305e-05,
        "epoch": 0.12690028343210513,
        "step": 985
    },
    {
        "loss": 2.3567,
        "grad_norm": 2.0598881244659424,
        "learning_rate": 6.904281210369483e-05,
        "epoch": 0.1270291162071631,
        "step": 986
    },
    {
        "loss": 1.9355,
        "grad_norm": 1.9729136228561401,
        "learning_rate": 6.89865538494068e-05,
        "epoch": 0.12715794898222108,
        "step": 987
    },
    {
        "loss": 2.1943,
        "grad_norm": 2.2814626693725586,
        "learning_rate": 6.893026749415372e-05,
        "epoch": 0.12728678175727906,
        "step": 988
    },
    {
        "loss": 1.7361,
        "grad_norm": 2.514293909072876,
        "learning_rate": 6.887395312124197e-05,
        "epoch": 0.12741561453233702,
        "step": 989
    },
    {
        "loss": 2.4437,
        "grad_norm": 1.7789182662963867,
        "learning_rate": 6.881761081401939e-05,
        "epoch": 0.127544447307395,
        "step": 990
    },
    {
        "loss": 2.3535,
        "grad_norm": 1.8133339881896973,
        "learning_rate": 6.876124065587514e-05,
        "epoch": 0.127673280082453,
        "step": 991
    },
    {
        "loss": 2.284,
        "grad_norm": 1.8337059020996094,
        "learning_rate": 6.870484273023968e-05,
        "epoch": 0.12780211285751095,
        "step": 992
    },
    {
        "loss": 1.3121,
        "grad_norm": 2.3054542541503906,
        "learning_rate": 6.864841712058444e-05,
        "epoch": 0.12793094563256893,
        "step": 993
    },
    {
        "loss": 1.7944,
        "grad_norm": 2.0468292236328125,
        "learning_rate": 6.859196391042193e-05,
        "epoch": 0.1280597784076269,
        "step": 994
    },
    {
        "loss": 1.1329,
        "grad_norm": 3.360025644302368,
        "learning_rate": 6.853548318330548e-05,
        "epoch": 0.12818861118268488,
        "step": 995
    },
    {
        "loss": 2.2117,
        "grad_norm": 1.9693905115127563,
        "learning_rate": 6.847897502282913e-05,
        "epoch": 0.12831744395774286,
        "step": 996
    },
    {
        "loss": 2.2051,
        "grad_norm": 2.430142879486084,
        "learning_rate": 6.842243951262755e-05,
        "epoch": 0.12844627673280082,
        "step": 997
    },
    {
        "loss": 2.3131,
        "grad_norm": 1.8927276134490967,
        "learning_rate": 6.836587673637584e-05,
        "epoch": 0.1285751095078588,
        "step": 998
    },
    {
        "loss": 2.1119,
        "grad_norm": 1.9826476573944092,
        "learning_rate": 6.83092867777895e-05,
        "epoch": 0.12870394228291676,
        "step": 999
    },
    {
        "loss": 2.6096,
        "grad_norm": 1.6212036609649658,
        "learning_rate": 6.825266972062425e-05,
        "epoch": 0.12883277505797475,
        "step": 1000
    },
    {
        "loss": 1.5371,
        "grad_norm": 2.651197671890259,
        "learning_rate": 6.819602564867593e-05,
        "epoch": 0.12896160783303273,
        "step": 1001
    },
    {
        "loss": 2.285,
        "grad_norm": 1.517498254776001,
        "learning_rate": 6.813935464578032e-05,
        "epoch": 0.1290904406080907,
        "step": 1002
    },
    {
        "loss": 2.2833,
        "grad_norm": 1.3037840127944946,
        "learning_rate": 6.808265679581312e-05,
        "epoch": 0.12921927338314868,
        "step": 1003
    },
    {
        "loss": 2.117,
        "grad_norm": 1.9490817785263062,
        "learning_rate": 6.80259321826897e-05,
        "epoch": 0.12934810615820666,
        "step": 1004
    },
    {
        "loss": 2.2881,
        "grad_norm": 1.9463073015213013,
        "learning_rate": 6.796918089036509e-05,
        "epoch": 0.12947693893326462,
        "step": 1005
    },
    {
        "loss": 2.2471,
        "grad_norm": 2.1165823936462402,
        "learning_rate": 6.791240300283378e-05,
        "epoch": 0.1296057717083226,
        "step": 1006
    },
    {
        "loss": 2.1997,
        "grad_norm": 1.8591951131820679,
        "learning_rate": 6.785559860412965e-05,
        "epoch": 0.12973460448338056,
        "step": 1007
    },
    {
        "loss": 2.2016,
        "grad_norm": 1.7461285591125488,
        "learning_rate": 6.779876777832577e-05,
        "epoch": 0.12986343725843855,
        "step": 1008
    },
    {
        "loss": 1.5751,
        "grad_norm": 3.2876241207122803,
        "learning_rate": 6.774191060953438e-05,
        "epoch": 0.12999227003349653,
        "step": 1009
    },
    {
        "loss": 1.5956,
        "grad_norm": 2.604522228240967,
        "learning_rate": 6.768502718190667e-05,
        "epoch": 0.1301211028085545,
        "step": 1010
    },
    {
        "loss": 2.1939,
        "grad_norm": 1.7051502466201782,
        "learning_rate": 6.762811757963269e-05,
        "epoch": 0.13024993558361248,
        "step": 1011
    },
    {
        "loss": 2.1791,
        "grad_norm": 1.7376452684402466,
        "learning_rate": 6.757118188694127e-05,
        "epoch": 0.13037876835867043,
        "step": 1012
    },
    {
        "loss": 2.1442,
        "grad_norm": 1.2958364486694336,
        "learning_rate": 6.751422018809983e-05,
        "epoch": 0.13050760113372842,
        "step": 1013
    },
    {
        "loss": 1.6769,
        "grad_norm": 2.327979803085327,
        "learning_rate": 6.745723256741425e-05,
        "epoch": 0.1306364339087864,
        "step": 1014
    },
    {
        "loss": 2.1672,
        "grad_norm": 2.7799408435821533,
        "learning_rate": 6.740021910922885e-05,
        "epoch": 0.13076526668384436,
        "step": 1015
    },
    {
        "loss": 2.398,
        "grad_norm": 1.5606374740600586,
        "learning_rate": 6.734317989792611e-05,
        "epoch": 0.13089409945890235,
        "step": 1016
    },
    {
        "loss": 2.0921,
        "grad_norm": 2.175276756286621,
        "learning_rate": 6.728611501792667e-05,
        "epoch": 0.13102293223396033,
        "step": 1017
    },
    {
        "loss": 2.388,
        "grad_norm": 2.060978651046753,
        "learning_rate": 6.722902455368916e-05,
        "epoch": 0.1311517650090183,
        "step": 1018
    },
    {
        "loss": 2.3811,
        "grad_norm": 2.144451379776001,
        "learning_rate": 6.717190858971007e-05,
        "epoch": 0.13128059778407628,
        "step": 1019
    },
    {
        "loss": 1.7477,
        "grad_norm": 1.2498916387557983,
        "learning_rate": 6.711476721052364e-05,
        "epoch": 0.13140943055913423,
        "step": 1020
    },
    {
        "loss": 1.5469,
        "grad_norm": 2.325883626937866,
        "learning_rate": 6.705760050070167e-05,
        "epoch": 0.13153826333419222,
        "step": 1021
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.9820715188980103,
        "learning_rate": 6.700040854485356e-05,
        "epoch": 0.1316670961092502,
        "step": 1022
    },
    {
        "loss": 1.9905,
        "grad_norm": 2.3254005908966064,
        "learning_rate": 6.694319142762599e-05,
        "epoch": 0.13179592888430816,
        "step": 1023
    },
    {
        "loss": 2.2761,
        "grad_norm": 1.7371978759765625,
        "learning_rate": 6.688594923370286e-05,
        "epoch": 0.13192476165936615,
        "step": 1024
    },
    {
        "loss": 2.0238,
        "grad_norm": 2.0908474922180176,
        "learning_rate": 6.682868204780528e-05,
        "epoch": 0.1320535944344241,
        "step": 1025
    },
    {
        "loss": 1.9267,
        "grad_norm": 2.230146646499634,
        "learning_rate": 6.677138995469126e-05,
        "epoch": 0.1321824272094821,
        "step": 1026
    },
    {
        "loss": 2.197,
        "grad_norm": 1.3513785600662231,
        "learning_rate": 6.671407303915574e-05,
        "epoch": 0.13231125998454007,
        "step": 1027
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.0807000398635864,
        "learning_rate": 6.665673138603032e-05,
        "epoch": 0.13244009275959803,
        "step": 1028
    },
    {
        "loss": 2.3014,
        "grad_norm": 1.6694859266281128,
        "learning_rate": 6.659936508018329e-05,
        "epoch": 0.13256892553465602,
        "step": 1029
    },
    {
        "loss": 2.1204,
        "grad_norm": 2.039907217025757,
        "learning_rate": 6.654197420651942e-05,
        "epoch": 0.132697758309714,
        "step": 1030
    },
    {
        "loss": 2.0742,
        "grad_norm": 2.1816492080688477,
        "learning_rate": 6.64845588499798e-05,
        "epoch": 0.13282659108477196,
        "step": 1031
    },
    {
        "loss": 1.7694,
        "grad_norm": 2.312765598297119,
        "learning_rate": 6.642711909554174e-05,
        "epoch": 0.13295542385982995,
        "step": 1032
    },
    {
        "loss": 2.0042,
        "grad_norm": 1.6247917413711548,
        "learning_rate": 6.636965502821875e-05,
        "epoch": 0.1330842566348879,
        "step": 1033
    },
    {
        "loss": 2.0581,
        "grad_norm": 1.3027695417404175,
        "learning_rate": 6.63121667330602e-05,
        "epoch": 0.1332130894099459,
        "step": 1034
    },
    {
        "loss": 1.4575,
        "grad_norm": 1.9852581024169922,
        "learning_rate": 6.625465429515149e-05,
        "epoch": 0.13334192218500387,
        "step": 1035
    },
    {
        "loss": 2.1907,
        "grad_norm": 1.696224331855774,
        "learning_rate": 6.619711779961355e-05,
        "epoch": 0.13347075496006183,
        "step": 1036
    },
    {
        "loss": 2.4806,
        "grad_norm": 1.8755549192428589,
        "learning_rate": 6.613955733160307e-05,
        "epoch": 0.13359958773511982,
        "step": 1037
    },
    {
        "loss": 1.7378,
        "grad_norm": 1.5707613229751587,
        "learning_rate": 6.608197297631213e-05,
        "epoch": 0.13372842051017778,
        "step": 1038
    },
    {
        "loss": 2.189,
        "grad_norm": 2.3674349784851074,
        "learning_rate": 6.602436481896824e-05,
        "epoch": 0.13385725328523576,
        "step": 1039
    },
    {
        "loss": 1.8955,
        "grad_norm": 1.926051378250122,
        "learning_rate": 6.596673294483408e-05,
        "epoch": 0.13398608606029375,
        "step": 1040
    },
    {
        "loss": 1.925,
        "grad_norm": 2.7277534008026123,
        "learning_rate": 6.590907743920745e-05,
        "epoch": 0.1341149188353517,
        "step": 1041
    },
    {
        "loss": 2.4723,
        "grad_norm": 2.1623737812042236,
        "learning_rate": 6.585139838742113e-05,
        "epoch": 0.1342437516104097,
        "step": 1042
    },
    {
        "loss": 2.1064,
        "grad_norm": 1.6732889413833618,
        "learning_rate": 6.579369587484273e-05,
        "epoch": 0.13437258438546767,
        "step": 1043
    },
    {
        "loss": 2.614,
        "grad_norm": 1.8191460371017456,
        "learning_rate": 6.573596998687461e-05,
        "epoch": 0.13450141716052563,
        "step": 1044
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.466162919998169,
        "learning_rate": 6.567822080895371e-05,
        "epoch": 0.13463024993558362,
        "step": 1045
    },
    {
        "loss": 2.1597,
        "grad_norm": 1.5988380908966064,
        "learning_rate": 6.562044842655148e-05,
        "epoch": 0.13475908271064158,
        "step": 1046
    },
    {
        "loss": 2.343,
        "grad_norm": 1.813845157623291,
        "learning_rate": 6.556265292517362e-05,
        "epoch": 0.13488791548569956,
        "step": 1047
    },
    {
        "loss": 2.2508,
        "grad_norm": 1.5848933458328247,
        "learning_rate": 6.550483439036015e-05,
        "epoch": 0.13501674826075755,
        "step": 1048
    },
    {
        "loss": 2.1868,
        "grad_norm": 1.4515310525894165,
        "learning_rate": 6.544699290768512e-05,
        "epoch": 0.1351455810358155,
        "step": 1049
    },
    {
        "loss": 2.5673,
        "grad_norm": 1.8480632305145264,
        "learning_rate": 6.538912856275656e-05,
        "epoch": 0.1352744138108735,
        "step": 1050
    },
    {
        "loss": 2.3975,
        "grad_norm": 1.5254268646240234,
        "learning_rate": 6.533124144121633e-05,
        "epoch": 0.13540324658593145,
        "step": 1051
    },
    {
        "loss": 2.4007,
        "grad_norm": 1.2918800115585327,
        "learning_rate": 6.527333162874004e-05,
        "epoch": 0.13553207936098943,
        "step": 1052
    },
    {
        "loss": 1.4973,
        "grad_norm": 2.257148265838623,
        "learning_rate": 6.52153992110368e-05,
        "epoch": 0.13566091213604742,
        "step": 1053
    },
    {
        "loss": 2.1946,
        "grad_norm": 2.1849277019500732,
        "learning_rate": 6.515744427384929e-05,
        "epoch": 0.13578974491110538,
        "step": 1054
    },
    {
        "loss": 2.3717,
        "grad_norm": 2.628208637237549,
        "learning_rate": 6.509946690295341e-05,
        "epoch": 0.13591857768616336,
        "step": 1055
    },
    {
        "loss": 2.5321,
        "grad_norm": 1.5529924631118774,
        "learning_rate": 6.504146718415835e-05,
        "epoch": 0.13604741046122135,
        "step": 1056
    },
    {
        "loss": 2.2346,
        "grad_norm": 1.691457748413086,
        "learning_rate": 6.49834452033063e-05,
        "epoch": 0.1361762432362793,
        "step": 1057
    },
    {
        "loss": 2.3167,
        "grad_norm": 2.489701509475708,
        "learning_rate": 6.492540104627245e-05,
        "epoch": 0.1363050760113373,
        "step": 1058
    },
    {
        "loss": 2.3942,
        "grad_norm": 1.7514022588729858,
        "learning_rate": 6.486733479896482e-05,
        "epoch": 0.13643390878639525,
        "step": 1059
    },
    {
        "loss": 2.0644,
        "grad_norm": 2.651414632797241,
        "learning_rate": 6.480924654732406e-05,
        "epoch": 0.13656274156145323,
        "step": 1060
    },
    {
        "loss": 2.0817,
        "grad_norm": 2.317366600036621,
        "learning_rate": 6.475113637732346e-05,
        "epoch": 0.13669157433651122,
        "step": 1061
    },
    {
        "loss": 2.2815,
        "grad_norm": 2.025881052017212,
        "learning_rate": 6.469300437496872e-05,
        "epoch": 0.13682040711156918,
        "step": 1062
    },
    {
        "loss": 2.3638,
        "grad_norm": 2.730302333831787,
        "learning_rate": 6.463485062629782e-05,
        "epoch": 0.13694923988662716,
        "step": 1063
    },
    {
        "loss": 2.2351,
        "grad_norm": 1.9327582120895386,
        "learning_rate": 6.457667521738102e-05,
        "epoch": 0.13707807266168515,
        "step": 1064
    },
    {
        "loss": 2.3655,
        "grad_norm": 2.2860097885131836,
        "learning_rate": 6.451847823432053e-05,
        "epoch": 0.1372069054367431,
        "step": 1065
    },
    {
        "loss": 2.1203,
        "grad_norm": 1.239683985710144,
        "learning_rate": 6.446025976325054e-05,
        "epoch": 0.1373357382118011,
        "step": 1066
    },
    {
        "loss": 1.7407,
        "grad_norm": 2.618926525115967,
        "learning_rate": 6.440201989033706e-05,
        "epoch": 0.13746457098685905,
        "step": 1067
    },
    {
        "loss": 2.2696,
        "grad_norm": 1.6404025554656982,
        "learning_rate": 6.434375870177773e-05,
        "epoch": 0.13759340376191703,
        "step": 1068
    },
    {
        "loss": 1.9017,
        "grad_norm": 1.7018306255340576,
        "learning_rate": 6.428547628380178e-05,
        "epoch": 0.13772223653697502,
        "step": 1069
    },
    {
        "loss": 2.222,
        "grad_norm": 1.1268402338027954,
        "learning_rate": 6.422717272266985e-05,
        "epoch": 0.13785106931203298,
        "step": 1070
    },
    {
        "loss": 2.1706,
        "grad_norm": 2.485670566558838,
        "learning_rate": 6.416884810467388e-05,
        "epoch": 0.13797990208709096,
        "step": 1071
    },
    {
        "loss": 1.5813,
        "grad_norm": 2.5099904537200928,
        "learning_rate": 6.411050251613693e-05,
        "epoch": 0.13810873486214892,
        "step": 1072
    },
    {
        "loss": 1.4947,
        "grad_norm": 2.4421775341033936,
        "learning_rate": 6.405213604341312e-05,
        "epoch": 0.1382375676372069,
        "step": 1073
    },
    {
        "loss": 1.8814,
        "grad_norm": 2.7533023357391357,
        "learning_rate": 6.399374877288755e-05,
        "epoch": 0.1383664004122649,
        "step": 1074
    },
    {
        "loss": 1.8005,
        "grad_norm": 2.1687188148498535,
        "learning_rate": 6.393534079097601e-05,
        "epoch": 0.13849523318732285,
        "step": 1075
    },
    {
        "loss": 2.2105,
        "grad_norm": 1.5686500072479248,
        "learning_rate": 6.387691218412496e-05,
        "epoch": 0.13862406596238083,
        "step": 1076
    },
    {
        "loss": 2.908,
        "grad_norm": 1.8469089269638062,
        "learning_rate": 6.38184630388114e-05,
        "epoch": 0.13875289873743882,
        "step": 1077
    },
    {
        "loss": 1.9306,
        "grad_norm": 2.65110445022583,
        "learning_rate": 6.375999344154276e-05,
        "epoch": 0.13888173151249678,
        "step": 1078
    },
    {
        "loss": 2.2131,
        "grad_norm": 2.087979316711426,
        "learning_rate": 6.370150347885666e-05,
        "epoch": 0.13901056428755476,
        "step": 1079
    },
    {
        "loss": 2.8627,
        "grad_norm": 1.1446540355682373,
        "learning_rate": 6.364299323732097e-05,
        "epoch": 0.13913939706261272,
        "step": 1080
    },
    {
        "loss": 2.566,
        "grad_norm": 1.3133060932159424,
        "learning_rate": 6.35844628035335e-05,
        "epoch": 0.1392682298376707,
        "step": 1081
    },
    {
        "loss": 2.0184,
        "grad_norm": 1.6619389057159424,
        "learning_rate": 6.352591226412192e-05,
        "epoch": 0.1393970626127287,
        "step": 1082
    },
    {
        "loss": 1.9508,
        "grad_norm": 2.8028132915496826,
        "learning_rate": 6.346734170574372e-05,
        "epoch": 0.13952589538778665,
        "step": 1083
    },
    {
        "loss": 1.9744,
        "grad_norm": 2.331146001815796,
        "learning_rate": 6.340875121508601e-05,
        "epoch": 0.13965472816284463,
        "step": 1084
    },
    {
        "loss": 2.321,
        "grad_norm": 1.3999053239822388,
        "learning_rate": 6.335014087886537e-05,
        "epoch": 0.1397835609379026,
        "step": 1085
    },
    {
        "loss": 2.0099,
        "grad_norm": 1.5592682361602783,
        "learning_rate": 6.329151078382778e-05,
        "epoch": 0.13991239371296058,
        "step": 1086
    },
    {
        "loss": 2.1998,
        "grad_norm": 2.579348087310791,
        "learning_rate": 6.323286101674844e-05,
        "epoch": 0.14004122648801856,
        "step": 1087
    },
    {
        "loss": 1.5496,
        "grad_norm": 2.6991729736328125,
        "learning_rate": 6.317419166443167e-05,
        "epoch": 0.14017005926307652,
        "step": 1088
    },
    {
        "loss": 1.9729,
        "grad_norm": 2.031470775604248,
        "learning_rate": 6.311550281371083e-05,
        "epoch": 0.1402988920381345,
        "step": 1089
    },
    {
        "loss": 1.9576,
        "grad_norm": 1.6776026487350464,
        "learning_rate": 6.305679455144803e-05,
        "epoch": 0.1404277248131925,
        "step": 1090
    },
    {
        "loss": 2.0837,
        "grad_norm": 1.595860481262207,
        "learning_rate": 6.299806696453426e-05,
        "epoch": 0.14055655758825045,
        "step": 1091
    },
    {
        "loss": 2.2429,
        "grad_norm": 1.0761950016021729,
        "learning_rate": 6.293932013988892e-05,
        "epoch": 0.14068539036330843,
        "step": 1092
    },
    {
        "loss": 2.6351,
        "grad_norm": 1.565913200378418,
        "learning_rate": 6.288055416446006e-05,
        "epoch": 0.1408142231383664,
        "step": 1093
    },
    {
        "loss": 2.3504,
        "grad_norm": 1.3454642295837402,
        "learning_rate": 6.282176912522397e-05,
        "epoch": 0.14094305591342438,
        "step": 1094
    },
    {
        "loss": 2.2919,
        "grad_norm": 1.8080313205718994,
        "learning_rate": 6.276296510918523e-05,
        "epoch": 0.14107188868848236,
        "step": 1095
    },
    {
        "loss": 2.732,
        "grad_norm": 1.2892158031463623,
        "learning_rate": 6.270414220337641e-05,
        "epoch": 0.14120072146354032,
        "step": 1096
    },
    {
        "loss": 2.165,
        "grad_norm": 1.9085942506790161,
        "learning_rate": 6.264530049485813e-05,
        "epoch": 0.1413295542385983,
        "step": 1097
    },
    {
        "loss": 2.1791,
        "grad_norm": 1.6700550317764282,
        "learning_rate": 6.258644007071877e-05,
        "epoch": 0.14145838701365626,
        "step": 1098
    },
    {
        "loss": 1.4275,
        "grad_norm": 2.266389846801758,
        "learning_rate": 6.252756101807446e-05,
        "epoch": 0.14158721978871425,
        "step": 1099
    },
    {
        "loss": 2.1996,
        "grad_norm": 1.904038429260254,
        "learning_rate": 6.246866342406888e-05,
        "epoch": 0.14171605256377223,
        "step": 1100
    },
    {
        "loss": 2.469,
        "grad_norm": 2.013345241546631,
        "learning_rate": 6.240974737587318e-05,
        "epoch": 0.1418448853388302,
        "step": 1101
    },
    {
        "loss": 2.5535,
        "grad_norm": 1.5135433673858643,
        "learning_rate": 6.235081296068573e-05,
        "epoch": 0.14197371811388818,
        "step": 1102
    },
    {
        "loss": 2.0507,
        "grad_norm": 2.3517730236053467,
        "learning_rate": 6.229186026573219e-05,
        "epoch": 0.14210255088894616,
        "step": 1103
    },
    {
        "loss": 2.6399,
        "grad_norm": 1.3856744766235352,
        "learning_rate": 6.223288937826526e-05,
        "epoch": 0.14223138366400412,
        "step": 1104
    },
    {
        "loss": 2.8399,
        "grad_norm": 1.6353847980499268,
        "learning_rate": 6.217390038556449e-05,
        "epoch": 0.1423602164390621,
        "step": 1105
    },
    {
        "loss": 2.7319,
        "grad_norm": 1.7164669036865234,
        "learning_rate": 6.21148933749363e-05,
        "epoch": 0.14248904921412006,
        "step": 1106
    },
    {
        "loss": 2.1088,
        "grad_norm": 2.2392385005950928,
        "learning_rate": 6.205586843371378e-05,
        "epoch": 0.14261788198917805,
        "step": 1107
    },
    {
        "loss": 1.9194,
        "grad_norm": 2.2938082218170166,
        "learning_rate": 6.19968256492565e-05,
        "epoch": 0.14274671476423603,
        "step": 1108
    },
    {
        "loss": 2.1979,
        "grad_norm": 2.174877882003784,
        "learning_rate": 6.193776510895048e-05,
        "epoch": 0.142875547539294,
        "step": 1109
    },
    {
        "loss": 2.213,
        "grad_norm": 1.6322336196899414,
        "learning_rate": 6.187868690020803e-05,
        "epoch": 0.14300438031435198,
        "step": 1110
    },
    {
        "loss": 2.2501,
        "grad_norm": 1.6338530778884888,
        "learning_rate": 6.181959111046755e-05,
        "epoch": 0.14313321308940993,
        "step": 1111
    },
    {
        "loss": 2.5552,
        "grad_norm": 1.5722503662109375,
        "learning_rate": 6.176047782719353e-05,
        "epoch": 0.14326204586446792,
        "step": 1112
    },
    {
        "loss": 2.097,
        "grad_norm": 1.9115707874298096,
        "learning_rate": 6.170134713787634e-05,
        "epoch": 0.1433908786395259,
        "step": 1113
    },
    {
        "loss": 1.8706,
        "grad_norm": 2.1786463260650635,
        "learning_rate": 6.164219913003207e-05,
        "epoch": 0.14351971141458386,
        "step": 1114
    },
    {
        "loss": 2.0951,
        "grad_norm": 1.7893952131271362,
        "learning_rate": 6.158303389120248e-05,
        "epoch": 0.14364854418964185,
        "step": 1115
    },
    {
        "loss": 2.3234,
        "grad_norm": 1.605608344078064,
        "learning_rate": 6.15238515089548e-05,
        "epoch": 0.14377737696469983,
        "step": 1116
    },
    {
        "loss": 1.4349,
        "grad_norm": 2.6697986125946045,
        "learning_rate": 6.146465207088169e-05,
        "epoch": 0.1439062097397578,
        "step": 1117
    },
    {
        "loss": 2.2495,
        "grad_norm": 1.635959506034851,
        "learning_rate": 6.140543566460098e-05,
        "epoch": 0.14403504251481578,
        "step": 1118
    },
    {
        "loss": 2.5882,
        "grad_norm": 2.3322231769561768,
        "learning_rate": 6.134620237775567e-05,
        "epoch": 0.14416387528987373,
        "step": 1119
    },
    {
        "loss": 2.59,
        "grad_norm": 2.0060322284698486,
        "learning_rate": 6.128695229801369e-05,
        "epoch": 0.14429270806493172,
        "step": 1120
    },
    {
        "loss": 1.9415,
        "grad_norm": 1.6916033029556274,
        "learning_rate": 6.122768551306792e-05,
        "epoch": 0.1444215408399897,
        "step": 1121
    },
    {
        "loss": 2.0528,
        "grad_norm": 2.4808731079101562,
        "learning_rate": 6.116840211063585e-05,
        "epoch": 0.14455037361504766,
        "step": 1122
    },
    {
        "loss": 1.8049,
        "grad_norm": 1.240020751953125,
        "learning_rate": 6.110910217845964e-05,
        "epoch": 0.14467920639010565,
        "step": 1123
    },
    {
        "loss": 2.185,
        "grad_norm": 3.8079886436462402,
        "learning_rate": 6.104978580430588e-05,
        "epoch": 0.1448080391651636,
        "step": 1124
    },
    {
        "loss": 2.1688,
        "grad_norm": 1.3937251567840576,
        "learning_rate": 6.099045307596555e-05,
        "epoch": 0.1449368719402216,
        "step": 1125
    },
    {
        "loss": 2.4257,
        "grad_norm": 1.7887427806854248,
        "learning_rate": 6.093110408125373e-05,
        "epoch": 0.14506570471527958,
        "step": 1126
    },
    {
        "loss": 2.5706,
        "grad_norm": 1.2447458505630493,
        "learning_rate": 6.087173890800967e-05,
        "epoch": 0.14519453749033753,
        "step": 1127
    },
    {
        "loss": 2.1216,
        "grad_norm": 2.37331223487854,
        "learning_rate": 6.081235764409652e-05,
        "epoch": 0.14532337026539552,
        "step": 1128
    },
    {
        "loss": 2.2854,
        "grad_norm": 1.504016399383545,
        "learning_rate": 6.075296037740128e-05,
        "epoch": 0.1454522030404535,
        "step": 1129
    },
    {
        "loss": 1.8248,
        "grad_norm": 3.0286786556243896,
        "learning_rate": 6.069354719583458e-05,
        "epoch": 0.14558103581551146,
        "step": 1130
    },
    {
        "loss": 2.3423,
        "grad_norm": 1.8218586444854736,
        "learning_rate": 6.063411818733068e-05,
        "epoch": 0.14570986859056945,
        "step": 1131
    },
    {
        "loss": 2.1852,
        "grad_norm": 1.5563236474990845,
        "learning_rate": 6.0574673439847175e-05,
        "epoch": 0.1458387013656274,
        "step": 1132
    },
    {
        "loss": 2.1283,
        "grad_norm": 2.0397186279296875,
        "learning_rate": 6.0515213041365025e-05,
        "epoch": 0.1459675341406854,
        "step": 1133
    },
    {
        "loss": 2.249,
        "grad_norm": 2.739309549331665,
        "learning_rate": 6.045573707988833e-05,
        "epoch": 0.14609636691574338,
        "step": 1134
    },
    {
        "loss": 1.7689,
        "grad_norm": 2.5073113441467285,
        "learning_rate": 6.039624564344421e-05,
        "epoch": 0.14622519969080133,
        "step": 1135
    },
    {
        "loss": 2.506,
        "grad_norm": 1.6946783065795898,
        "learning_rate": 6.033673882008271e-05,
        "epoch": 0.14635403246585932,
        "step": 1136
    },
    {
        "loss": 2.5314,
        "grad_norm": 1.8039416074752808,
        "learning_rate": 6.027721669787663e-05,
        "epoch": 0.14648286524091728,
        "step": 1137
    },
    {
        "loss": 2.4718,
        "grad_norm": 1.6041185855865479,
        "learning_rate": 6.0217679364921466e-05,
        "epoch": 0.14661169801597526,
        "step": 1138
    },
    {
        "loss": 2.5124,
        "grad_norm": 1.1845736503601074,
        "learning_rate": 6.015812690933512e-05,
        "epoch": 0.14674053079103325,
        "step": 1139
    },
    {
        "loss": 2.2781,
        "grad_norm": 2.1471972465515137,
        "learning_rate": 6.0098559419257994e-05,
        "epoch": 0.1468693635660912,
        "step": 1140
    },
    {
        "loss": 2.0408,
        "grad_norm": 2.4530539512634277,
        "learning_rate": 6.003897698285267e-05,
        "epoch": 0.1469981963411492,
        "step": 1141
    },
    {
        "loss": 2.5491,
        "grad_norm": 1.8547598123550415,
        "learning_rate": 5.997937968830386e-05,
        "epoch": 0.14712702911620718,
        "step": 1142
    },
    {
        "loss": 2.0705,
        "grad_norm": 2.1927781105041504,
        "learning_rate": 5.9919767623818276e-05,
        "epoch": 0.14725586189126513,
        "step": 1143
    },
    {
        "loss": 2.2676,
        "grad_norm": 1.4660471677780151,
        "learning_rate": 5.986014087762453e-05,
        "epoch": 0.14738469466632312,
        "step": 1144
    },
    {
        "loss": 1.4107,
        "grad_norm": 2.2296149730682373,
        "learning_rate": 5.980049953797285e-05,
        "epoch": 0.14751352744138108,
        "step": 1145
    },
    {
        "loss": 2.2458,
        "grad_norm": 1.700600266456604,
        "learning_rate": 5.9740843693135206e-05,
        "epoch": 0.14764236021643906,
        "step": 1146
    },
    {
        "loss": 2.0552,
        "grad_norm": 2.686497449874878,
        "learning_rate": 5.968117343140493e-05,
        "epoch": 0.14777119299149705,
        "step": 1147
    },
    {
        "loss": 2.2916,
        "grad_norm": 1.5766836404800415,
        "learning_rate": 5.962148884109673e-05,
        "epoch": 0.147900025766555,
        "step": 1148
    },
    {
        "loss": 2.0971,
        "grad_norm": 2.7390835285186768,
        "learning_rate": 5.9561790010546545e-05,
        "epoch": 0.148028858541613,
        "step": 1149
    },
    {
        "loss": 1.917,
        "grad_norm": 1.5697128772735596,
        "learning_rate": 5.950207702811135e-05,
        "epoch": 0.14815769131667095,
        "step": 1150
    },
    {
        "loss": 2.5628,
        "grad_norm": 1.227728009223938,
        "learning_rate": 5.944234998216908e-05,
        "epoch": 0.14828652409172893,
        "step": 1151
    },
    {
        "loss": 2.4633,
        "grad_norm": 2.107532024383545,
        "learning_rate": 5.938260896111847e-05,
        "epoch": 0.14841535686678692,
        "step": 1152
    },
    {
        "loss": 2.0728,
        "grad_norm": 2.558300733566284,
        "learning_rate": 5.932285405337898e-05,
        "epoch": 0.14854418964184488,
        "step": 1153
    },
    {
        "loss": 2.1183,
        "grad_norm": 1.899344563484192,
        "learning_rate": 5.926308534739058e-05,
        "epoch": 0.14867302241690286,
        "step": 1154
    },
    {
        "loss": 2.2331,
        "grad_norm": 1.814420223236084,
        "learning_rate": 5.92033029316137e-05,
        "epoch": 0.14880185519196085,
        "step": 1155
    },
    {
        "loss": 1.6735,
        "grad_norm": 2.414350986480713,
        "learning_rate": 5.9143506894529e-05,
        "epoch": 0.1489306879670188,
        "step": 1156
    },
    {
        "loss": 2.3187,
        "grad_norm": 1.5399948358535767,
        "learning_rate": 5.9083697324637374e-05,
        "epoch": 0.1490595207420768,
        "step": 1157
    },
    {
        "loss": 2.1004,
        "grad_norm": 2.123225212097168,
        "learning_rate": 5.9023874310459704e-05,
        "epoch": 0.14918835351713475,
        "step": 1158
    },
    {
        "loss": 1.8872,
        "grad_norm": 3.421435832977295,
        "learning_rate": 5.896403794053679e-05,
        "epoch": 0.14931718629219273,
        "step": 1159
    },
    {
        "loss": 1.8971,
        "grad_norm": 2.6176693439483643,
        "learning_rate": 5.890418830342916e-05,
        "epoch": 0.14944601906725072,
        "step": 1160
    },
    {
        "loss": 1.9954,
        "grad_norm": 2.1091465950012207,
        "learning_rate": 5.8844325487717014e-05,
        "epoch": 0.14957485184230868,
        "step": 1161
    },
    {
        "loss": 1.9342,
        "grad_norm": 2.3539810180664062,
        "learning_rate": 5.8784449582000045e-05,
        "epoch": 0.14970368461736666,
        "step": 1162
    },
    {
        "loss": 1.4254,
        "grad_norm": 3.35196590423584,
        "learning_rate": 5.872456067489735e-05,
        "epoch": 0.14983251739242462,
        "step": 1163
    },
    {
        "loss": 2.2594,
        "grad_norm": 1.0667271614074707,
        "learning_rate": 5.866465885504721e-05,
        "epoch": 0.1499613501674826,
        "step": 1164
    },
    {
        "loss": 1.8065,
        "grad_norm": 3.4841561317443848,
        "learning_rate": 5.860474421110704e-05,
        "epoch": 0.1500901829425406,
        "step": 1165
    },
    {
        "loss": 2.3508,
        "grad_norm": 1.6299569606781006,
        "learning_rate": 5.854481683175328e-05,
        "epoch": 0.15021901571759855,
        "step": 1166
    },
    {
        "loss": 2.5123,
        "grad_norm": 2.949333429336548,
        "learning_rate": 5.848487680568115e-05,
        "epoch": 0.15034784849265653,
        "step": 1167
    },
    {
        "loss": 2.2719,
        "grad_norm": 1.8943610191345215,
        "learning_rate": 5.842492422160466e-05,
        "epoch": 0.15047668126771452,
        "step": 1168
    },
    {
        "loss": 1.9985,
        "grad_norm": 2.070478916168213,
        "learning_rate": 5.8364959168256314e-05,
        "epoch": 0.15060551404277248,
        "step": 1169
    },
    {
        "loss": 1.9145,
        "grad_norm": 2.6892569065093994,
        "learning_rate": 5.830498173438717e-05,
        "epoch": 0.15073434681783046,
        "step": 1170
    },
    {
        "loss": 2.063,
        "grad_norm": 2.3017232418060303,
        "learning_rate": 5.824499200876653e-05,
        "epoch": 0.15086317959288842,
        "step": 1171
    },
    {
        "loss": 2.5643,
        "grad_norm": 1.0537203550338745,
        "learning_rate": 5.818499008018195e-05,
        "epoch": 0.1509920123679464,
        "step": 1172
    },
    {
        "loss": 1.4081,
        "grad_norm": 2.0582058429718018,
        "learning_rate": 5.812497603743898e-05,
        "epoch": 0.1511208451430044,
        "step": 1173
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.3874163627624512,
        "learning_rate": 5.80649499693612e-05,
        "epoch": 0.15124967791806235,
        "step": 1174
    },
    {
        "loss": 2.3183,
        "grad_norm": 1.622796654701233,
        "learning_rate": 5.8004911964789874e-05,
        "epoch": 0.15137851069312033,
        "step": 1175
    },
    {
        "loss": 2.2777,
        "grad_norm": 1.8229646682739258,
        "learning_rate": 5.794486211258401e-05,
        "epoch": 0.15150734346817832,
        "step": 1176
    },
    {
        "loss": 2.5181,
        "grad_norm": 1.4332772493362427,
        "learning_rate": 5.7884800501620106e-05,
        "epoch": 0.15163617624323628,
        "step": 1177
    },
    {
        "loss": 2.2335,
        "grad_norm": 2.3774867057800293,
        "learning_rate": 5.782472722079212e-05,
        "epoch": 0.15176500901829426,
        "step": 1178
    },
    {
        "loss": 2.1708,
        "grad_norm": 1.7329002618789673,
        "learning_rate": 5.776464235901121e-05,
        "epoch": 0.15189384179335222,
        "step": 1179
    },
    {
        "loss": 1.9991,
        "grad_norm": 1.7529228925704956,
        "learning_rate": 5.7704546005205704e-05,
        "epoch": 0.1520226745684102,
        "step": 1180
    },
    {
        "loss": 1.1317,
        "grad_norm": 3.1409592628479004,
        "learning_rate": 5.7644438248320974e-05,
        "epoch": 0.1521515073434682,
        "step": 1181
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.8945958614349365,
        "learning_rate": 5.7584319177319214e-05,
        "epoch": 0.15228034011852615,
        "step": 1182
    },
    {
        "loss": 1.5667,
        "grad_norm": 1.743766188621521,
        "learning_rate": 5.752418888117943e-05,
        "epoch": 0.15240917289358413,
        "step": 1183
    },
    {
        "loss": 1.9636,
        "grad_norm": 1.9425981044769287,
        "learning_rate": 5.7464047448897165e-05,
        "epoch": 0.1525380056686421,
        "step": 1184
    },
    {
        "loss": 1.8789,
        "grad_norm": 2.392430543899536,
        "learning_rate": 5.740389496948449e-05,
        "epoch": 0.15266683844370008,
        "step": 1185
    },
    {
        "loss": 2.0116,
        "grad_norm": 2.9627978801727295,
        "learning_rate": 5.73437315319698e-05,
        "epoch": 0.15279567121875806,
        "step": 1186
    },
    {
        "loss": 1.7774,
        "grad_norm": 1.8488996028900146,
        "learning_rate": 5.7283557225397755e-05,
        "epoch": 0.15292450399381602,
        "step": 1187
    },
    {
        "loss": 1.8091,
        "grad_norm": 2.1908679008483887,
        "learning_rate": 5.722337213882905e-05,
        "epoch": 0.153053336768874,
        "step": 1188
    },
    {
        "loss": 1.2956,
        "grad_norm": 3.499124050140381,
        "learning_rate": 5.716317636134036e-05,
        "epoch": 0.153182169543932,
        "step": 1189
    },
    {
        "loss": 2.5401,
        "grad_norm": 2.133838653564453,
        "learning_rate": 5.710296998202417e-05,
        "epoch": 0.15331100231898995,
        "step": 1190
    },
    {
        "loss": 2.1127,
        "grad_norm": 2.15022349357605,
        "learning_rate": 5.7042753089988686e-05,
        "epoch": 0.15343983509404793,
        "step": 1191
    },
    {
        "loss": 2.2818,
        "grad_norm": 1.225996732711792,
        "learning_rate": 5.6982525774357656e-05,
        "epoch": 0.1535686678691059,
        "step": 1192
    },
    {
        "loss": 2.1978,
        "grad_norm": 2.086359739303589,
        "learning_rate": 5.6922288124270226e-05,
        "epoch": 0.15369750064416388,
        "step": 1193
    },
    {
        "loss": 2.3244,
        "grad_norm": 2.075517177581787,
        "learning_rate": 5.6862040228880894e-05,
        "epoch": 0.15382633341922186,
        "step": 1194
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.5777593851089478,
        "learning_rate": 5.680178217735925e-05,
        "epoch": 0.15395516619427982,
        "step": 1195
    },
    {
        "loss": 2.1135,
        "grad_norm": 2.2286877632141113,
        "learning_rate": 5.6741514058889997e-05,
        "epoch": 0.1540839989693378,
        "step": 1196
    },
    {
        "loss": 2.5476,
        "grad_norm": 1.2263743877410889,
        "learning_rate": 5.668123596267266e-05,
        "epoch": 0.15421283174439576,
        "step": 1197
    },
    {
        "loss": 1.7756,
        "grad_norm": 2.5384299755096436,
        "learning_rate": 5.6620947977921615e-05,
        "epoch": 0.15434166451945375,
        "step": 1198
    },
    {
        "loss": 2.5417,
        "grad_norm": 1.581205129623413,
        "learning_rate": 5.656065019386578e-05,
        "epoch": 0.15447049729451173,
        "step": 1199
    },
    {
        "loss": 1.9121,
        "grad_norm": 2.3309292793273926,
        "learning_rate": 5.650034269974866e-05,
        "epoch": 0.1545993300695697,
        "step": 1200
    },
    {
        "loss": 2.0086,
        "grad_norm": 1.6559768915176392,
        "learning_rate": 5.644002558482806e-05,
        "epoch": 0.15472816284462768,
        "step": 1201
    },
    {
        "loss": 1.4314,
        "grad_norm": 2.4087722301483154,
        "learning_rate": 5.63796989383761e-05,
        "epoch": 0.15485699561968566,
        "step": 1202
    },
    {
        "loss": 2.1952,
        "grad_norm": 2.0551388263702393,
        "learning_rate": 5.631936284967895e-05,
        "epoch": 0.15498582839474362,
        "step": 1203
    },
    {
        "loss": 2.2823,
        "grad_norm": 1.7055606842041016,
        "learning_rate": 5.625901740803676e-05,
        "epoch": 0.1551146611698016,
        "step": 1204
    },
    {
        "loss": 2.7487,
        "grad_norm": 1.588761806488037,
        "learning_rate": 5.619866270276353e-05,
        "epoch": 0.15524349394485956,
        "step": 1205
    },
    {
        "loss": 1.9119,
        "grad_norm": 2.026808977127075,
        "learning_rate": 5.613829882318698e-05,
        "epoch": 0.15537232671991755,
        "step": 1206
    },
    {
        "loss": 2.3751,
        "grad_norm": 2.1034319400787354,
        "learning_rate": 5.607792585864841e-05,
        "epoch": 0.15550115949497553,
        "step": 1207
    },
    {
        "loss": 2.0198,
        "grad_norm": 2.0233118534088135,
        "learning_rate": 5.601754389850258e-05,
        "epoch": 0.1556299922700335,
        "step": 1208
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.4254828691482544,
        "learning_rate": 5.5957153032117516e-05,
        "epoch": 0.15575882504509148,
        "step": 1209
    },
    {
        "loss": 2.2637,
        "grad_norm": 1.6031968593597412,
        "learning_rate": 5.589675334887444e-05,
        "epoch": 0.15588765782014943,
        "step": 1210
    },
    {
        "loss": 2.2331,
        "grad_norm": 1.6770365238189697,
        "learning_rate": 5.583634493816766e-05,
        "epoch": 0.15601649059520742,
        "step": 1211
    },
    {
        "loss": 2.2719,
        "grad_norm": 1.5460267066955566,
        "learning_rate": 5.5775927889404364e-05,
        "epoch": 0.1561453233702654,
        "step": 1212
    },
    {
        "loss": 1.1539,
        "grad_norm": 2.932879686355591,
        "learning_rate": 5.5715502292004564e-05,
        "epoch": 0.15627415614532336,
        "step": 1213
    },
    {
        "loss": 1.7593,
        "grad_norm": 1.6490325927734375,
        "learning_rate": 5.565506823540085e-05,
        "epoch": 0.15640298892038135,
        "step": 1214
    },
    {
        "loss": 2.2617,
        "grad_norm": 1.3057435750961304,
        "learning_rate": 5.559462580903842e-05,
        "epoch": 0.15653182169543933,
        "step": 1215
    },
    {
        "loss": 2.1245,
        "grad_norm": 1.965399146080017,
        "learning_rate": 5.55341751023748e-05,
        "epoch": 0.1566606544704973,
        "step": 1216
    },
    {
        "loss": 1.4883,
        "grad_norm": 3.1774179935455322,
        "learning_rate": 5.547371620487981e-05,
        "epoch": 0.15678948724555528,
        "step": 1217
    },
    {
        "loss": 1.8307,
        "grad_norm": 1.8688162565231323,
        "learning_rate": 5.5413249206035364e-05,
        "epoch": 0.15691832002061323,
        "step": 1218
    },
    {
        "loss": 2.112,
        "grad_norm": 2.450284242630005,
        "learning_rate": 5.535277419533538e-05,
        "epoch": 0.15704715279567122,
        "step": 1219
    },
    {
        "loss": 2.1873,
        "grad_norm": 2.608607530593872,
        "learning_rate": 5.52922912622856e-05,
        "epoch": 0.1571759855707292,
        "step": 1220
    },
    {
        "loss": 1.3647,
        "grad_norm": 2.238645553588867,
        "learning_rate": 5.523180049640357e-05,
        "epoch": 0.15730481834578716,
        "step": 1221
    },
    {
        "loss": 1.3613,
        "grad_norm": 3.062736749649048,
        "learning_rate": 5.517130198721834e-05,
        "epoch": 0.15743365112084515,
        "step": 1222
    },
    {
        "loss": 2.5498,
        "grad_norm": 1.4427064657211304,
        "learning_rate": 5.51107958242705e-05,
        "epoch": 0.1575624838959031,
        "step": 1223
    },
    {
        "loss": 2.0144,
        "grad_norm": 3.1385231018066406,
        "learning_rate": 5.5050282097111895e-05,
        "epoch": 0.1576913166709611,
        "step": 1224
    },
    {
        "loss": 2.0944,
        "grad_norm": 2.2854878902435303,
        "learning_rate": 5.498976089530561e-05,
        "epoch": 0.15782014944601908,
        "step": 1225
    },
    {
        "loss": 2.5262,
        "grad_norm": 2.1671385765075684,
        "learning_rate": 5.492923230842578e-05,
        "epoch": 0.15794898222107703,
        "step": 1226
    },
    {
        "loss": 2.1177,
        "grad_norm": 2.4361071586608887,
        "learning_rate": 5.486869642605748e-05,
        "epoch": 0.15807781499613502,
        "step": 1227
    },
    {
        "loss": 2.3337,
        "grad_norm": 2.875293254852295,
        "learning_rate": 5.4808153337796576e-05,
        "epoch": 0.158206647771193,
        "step": 1228
    },
    {
        "loss": 2.2706,
        "grad_norm": 1.6506779193878174,
        "learning_rate": 5.474760313324958e-05,
        "epoch": 0.15833548054625096,
        "step": 1229
    },
    {
        "loss": 2.3858,
        "grad_norm": 1.1287903785705566,
        "learning_rate": 5.468704590203357e-05,
        "epoch": 0.15846431332130895,
        "step": 1230
    },
    {
        "loss": 2.3467,
        "grad_norm": 1.9680570363998413,
        "learning_rate": 5.462648173377599e-05,
        "epoch": 0.1585931460963669,
        "step": 1231
    },
    {
        "loss": 2.0752,
        "grad_norm": 1.727077603340149,
        "learning_rate": 5.456591071811461e-05,
        "epoch": 0.1587219788714249,
        "step": 1232
    },
    {
        "loss": 0.8693,
        "grad_norm": 2.753727674484253,
        "learning_rate": 5.450533294469725e-05,
        "epoch": 0.15885081164648288,
        "step": 1233
    },
    {
        "loss": 1.5095,
        "grad_norm": 2.0164058208465576,
        "learning_rate": 5.444474850318181e-05,
        "epoch": 0.15897964442154083,
        "step": 1234
    },
    {
        "loss": 2.6215,
        "grad_norm": 1.7749322652816772,
        "learning_rate": 5.4384157483236e-05,
        "epoch": 0.15910847719659882,
        "step": 1235
    },
    {
        "loss": 2.0465,
        "grad_norm": 1.787980556488037,
        "learning_rate": 5.432355997453729e-05,
        "epoch": 0.15923730997165678,
        "step": 1236
    },
    {
        "loss": 2.1827,
        "grad_norm": 1.1340197324752808,
        "learning_rate": 5.426295606677277e-05,
        "epoch": 0.15936614274671476,
        "step": 1237
    },
    {
        "loss": 1.6054,
        "grad_norm": 2.6315665245056152,
        "learning_rate": 5.4202345849638994e-05,
        "epoch": 0.15949497552177275,
        "step": 1238
    },
    {
        "loss": 2.1907,
        "grad_norm": 1.728164553642273,
        "learning_rate": 5.4141729412841815e-05,
        "epoch": 0.1596238082968307,
        "step": 1239
    },
    {
        "loss": 2.1143,
        "grad_norm": 1.8547016382217407,
        "learning_rate": 5.408110684609633e-05,
        "epoch": 0.1597526410718887,
        "step": 1240
    },
    {
        "loss": 1.9759,
        "grad_norm": 1.6815860271453857,
        "learning_rate": 5.4020478239126734e-05,
        "epoch": 0.15988147384694668,
        "step": 1241
    },
    {
        "loss": 1.0636,
        "grad_norm": 2.940113067626953,
        "learning_rate": 5.395984368166608e-05,
        "epoch": 0.16001030662200463,
        "step": 1242
    },
    {
        "loss": 2.1144,
        "grad_norm": 1.790217638015747,
        "learning_rate": 5.389920326345632e-05,
        "epoch": 0.16013913939706262,
        "step": 1243
    },
    {
        "loss": 1.9719,
        "grad_norm": 2.5920259952545166,
        "learning_rate": 5.383855707424801e-05,
        "epoch": 0.16026797217212058,
        "step": 1244
    },
    {
        "loss": 2.2901,
        "grad_norm": 2.275218963623047,
        "learning_rate": 5.3777905203800273e-05,
        "epoch": 0.16039680494717856,
        "step": 1245
    },
    {
        "loss": 2.6131,
        "grad_norm": 2.833747148513794,
        "learning_rate": 5.371724774188066e-05,
        "epoch": 0.16052563772223655,
        "step": 1246
    },
    {
        "loss": 1.8488,
        "grad_norm": 2.667755365371704,
        "learning_rate": 5.365658477826497e-05,
        "epoch": 0.1606544704972945,
        "step": 1247
    },
    {
        "loss": 1.9175,
        "grad_norm": 3.1200172901153564,
        "learning_rate": 5.359591640273713e-05,
        "epoch": 0.1607833032723525,
        "step": 1248
    },
    {
        "loss": 1.4217,
        "grad_norm": 2.722137689590454,
        "learning_rate": 5.353524270508915e-05,
        "epoch": 0.16091213604741045,
        "step": 1249
    },
    {
        "loss": 2.2884,
        "grad_norm": 1.8090384006500244,
        "learning_rate": 5.3474563775120836e-05,
        "epoch": 0.16104096882246843,
        "step": 1250
    },
    {
        "loss": 2.4909,
        "grad_norm": 2.0406880378723145,
        "learning_rate": 5.3413879702639765e-05,
        "epoch": 0.16116980159752642,
        "step": 1251
    },
    {
        "loss": 2.3746,
        "grad_norm": 1.641472578048706,
        "learning_rate": 5.3353190577461156e-05,
        "epoch": 0.16129863437258438,
        "step": 1252
    },
    {
        "loss": 2.2562,
        "grad_norm": 1.9490100145339966,
        "learning_rate": 5.329249648940766e-05,
        "epoch": 0.16142746714764236,
        "step": 1253
    },
    {
        "loss": 2.148,
        "grad_norm": 2.0610761642456055,
        "learning_rate": 5.32317975283093e-05,
        "epoch": 0.16155629992270035,
        "step": 1254
    },
    {
        "loss": 2.5491,
        "grad_norm": 1.5379582643508911,
        "learning_rate": 5.31710937840033e-05,
        "epoch": 0.1616851326977583,
        "step": 1255
    },
    {
        "loss": 1.8196,
        "grad_norm": 2.981889009475708,
        "learning_rate": 5.311038534633398e-05,
        "epoch": 0.1618139654728163,
        "step": 1256
    },
    {
        "loss": 1.9171,
        "grad_norm": 2.7007741928100586,
        "learning_rate": 5.304967230515256e-05,
        "epoch": 0.16194279824787425,
        "step": 1257
    },
    {
        "loss": 2.2239,
        "grad_norm": 1.6005494594573975,
        "learning_rate": 5.2988954750317165e-05,
        "epoch": 0.16207163102293223,
        "step": 1258
    },
    {
        "loss": 2.5154,
        "grad_norm": 1.521515130996704,
        "learning_rate": 5.292823277169249e-05,
        "epoch": 0.16220046379799022,
        "step": 1259
    },
    {
        "loss": 2.4159,
        "grad_norm": 1.2630425691604614,
        "learning_rate": 5.286750645914986e-05,
        "epoch": 0.16232929657304818,
        "step": 1260
    },
    {
        "loss": 1.6734,
        "grad_norm": 1.9594213962554932,
        "learning_rate": 5.280677590256697e-05,
        "epoch": 0.16245812934810616,
        "step": 1261
    },
    {
        "loss": 1.644,
        "grad_norm": 2.6145105361938477,
        "learning_rate": 5.2746041191827855e-05,
        "epoch": 0.16258696212316412,
        "step": 1262
    },
    {
        "loss": 2.1497,
        "grad_norm": 1.8183655738830566,
        "learning_rate": 5.268530241682259e-05,
        "epoch": 0.1627157948982221,
        "step": 1263
    },
    {
        "loss": 2.3005,
        "grad_norm": 1.7515614032745361,
        "learning_rate": 5.262455966744737e-05,
        "epoch": 0.1628446276732801,
        "step": 1264
    },
    {
        "loss": 2.0974,
        "grad_norm": 2.2000441551208496,
        "learning_rate": 5.2563813033604205e-05,
        "epoch": 0.16297346044833805,
        "step": 1265
    },
    {
        "loss": 1.7842,
        "grad_norm": 3.4436779022216797,
        "learning_rate": 5.2503062605200926e-05,
        "epoch": 0.16310229322339603,
        "step": 1266
    },
    {
        "loss": 1.4654,
        "grad_norm": 2.827515125274658,
        "learning_rate": 5.244230847215089e-05,
        "epoch": 0.16323112599845402,
        "step": 1267
    },
    {
        "loss": 2.5047,
        "grad_norm": 1.6633824110031128,
        "learning_rate": 5.238155072437303e-05,
        "epoch": 0.16335995877351198,
        "step": 1268
    },
    {
        "loss": 2.1905,
        "grad_norm": 1.3331183195114136,
        "learning_rate": 5.232078945179153e-05,
        "epoch": 0.16348879154856996,
        "step": 1269
    },
    {
        "loss": 1.9046,
        "grad_norm": 2.6364667415618896,
        "learning_rate": 5.226002474433588e-05,
        "epoch": 0.16361762432362792,
        "step": 1270
    },
    {
        "loss": 1.4429,
        "grad_norm": 3.312948703765869,
        "learning_rate": 5.2199256691940605e-05,
        "epoch": 0.1637464570986859,
        "step": 1271
    },
    {
        "loss": 2.556,
        "grad_norm": 1.9810737371444702,
        "learning_rate": 5.213848538454519e-05,
        "epoch": 0.1638752898737439,
        "step": 1272
    },
    {
        "loss": 1.7449,
        "grad_norm": 2.950157880783081,
        "learning_rate": 5.2077710912093926e-05,
        "epoch": 0.16400412264880185,
        "step": 1273
    },
    {
        "loss": 2.2395,
        "grad_norm": 2.401578187942505,
        "learning_rate": 5.201693336453581e-05,
        "epoch": 0.16413295542385983,
        "step": 1274
    },
    {
        "loss": 2.0012,
        "grad_norm": 1.9750006198883057,
        "learning_rate": 5.195615283182442e-05,
        "epoch": 0.1642617881989178,
        "step": 1275
    },
    {
        "loss": 2.4094,
        "grad_norm": 2.004737615585327,
        "learning_rate": 5.189536940391766e-05,
        "epoch": 0.16439062097397578,
        "step": 1276
    },
    {
        "loss": 1.886,
        "grad_norm": 1.7323317527770996,
        "learning_rate": 5.1834583170777814e-05,
        "epoch": 0.16451945374903376,
        "step": 1277
    },
    {
        "loss": 2.673,
        "grad_norm": 1.987788200378418,
        "learning_rate": 5.1773794222371254e-05,
        "epoch": 0.16464828652409172,
        "step": 1278
    },
    {
        "loss": 2.3831,
        "grad_norm": 2.3916029930114746,
        "learning_rate": 5.1713002648668405e-05,
        "epoch": 0.1647771192991497,
        "step": 1279
    },
    {
        "loss": 1.8882,
        "grad_norm": 1.4782694578170776,
        "learning_rate": 5.1652208539643554e-05,
        "epoch": 0.1649059520742077,
        "step": 1280
    },
    {
        "loss": 2.1643,
        "grad_norm": 1.8745801448822021,
        "learning_rate": 5.1591411985274774e-05,
        "epoch": 0.16503478484926565,
        "step": 1281
    },
    {
        "loss": 1.9767,
        "grad_norm": 2.529700994491577,
        "learning_rate": 5.153061307554371e-05,
        "epoch": 0.16516361762432363,
        "step": 1282
    },
    {
        "loss": 2.5901,
        "grad_norm": 1.3308898210525513,
        "learning_rate": 5.146981190043555e-05,
        "epoch": 0.1652924503993816,
        "step": 1283
    },
    {
        "loss": 2.012,
        "grad_norm": 2.3784940242767334,
        "learning_rate": 5.140900854993877e-05,
        "epoch": 0.16542128317443958,
        "step": 1284
    },
    {
        "loss": 2.2172,
        "grad_norm": 1.267655372619629,
        "learning_rate": 5.1348203114045125e-05,
        "epoch": 0.16555011594949756,
        "step": 1285
    },
    {
        "loss": 2.3498,
        "grad_norm": 1.96956205368042,
        "learning_rate": 5.128739568274944e-05,
        "epoch": 0.16567894872455552,
        "step": 1286
    },
    {
        "loss": 1.936,
        "grad_norm": 2.1891696453094482,
        "learning_rate": 5.122658634604945e-05,
        "epoch": 0.1658077814996135,
        "step": 1287
    },
    {
        "loss": 2.0123,
        "grad_norm": 2.312100410461426,
        "learning_rate": 5.1165775193945766e-05,
        "epoch": 0.1659366142746715,
        "step": 1288
    },
    {
        "loss": 2.0448,
        "grad_norm": 2.4179563522338867,
        "learning_rate": 5.110496231644167e-05,
        "epoch": 0.16606544704972945,
        "step": 1289
    },
    {
        "loss": 2.7541,
        "grad_norm": 1.1596734523773193,
        "learning_rate": 5.1044147803543e-05,
        "epoch": 0.16619427982478743,
        "step": 1290
    },
    {
        "loss": 2.6088,
        "grad_norm": 2.28377628326416,
        "learning_rate": 5.0983331745257956e-05,
        "epoch": 0.1663231125998454,
        "step": 1291
    },
    {
        "loss": 2.4768,
        "grad_norm": 1.2123515605926514,
        "learning_rate": 5.092251423159714e-05,
        "epoch": 0.16645194537490338,
        "step": 1292
    },
    {
        "loss": 1.9957,
        "grad_norm": 1.940441370010376,
        "learning_rate": 5.086169535257319e-05,
        "epoch": 0.16658077814996136,
        "step": 1293
    },
    {
        "loss": 1.6579,
        "grad_norm": 2.4368321895599365,
        "learning_rate": 5.080087519820086e-05,
        "epoch": 0.16670961092501932,
        "step": 1294
    },
    {
        "loss": 1.552,
        "grad_norm": 2.7016677856445312,
        "learning_rate": 5.0740053858496736e-05,
        "epoch": 0.1668384437000773,
        "step": 1295
    },
    {
        "loss": 2.4104,
        "grad_norm": 1.7072482109069824,
        "learning_rate": 5.0679231423479176e-05,
        "epoch": 0.16696727647513526,
        "step": 1296
    },
    {
        "loss": 2.2691,
        "grad_norm": 2.022085189819336,
        "learning_rate": 5.0618407983168146e-05,
        "epoch": 0.16709610925019325,
        "step": 1297
    },
    {
        "loss": 1.9802,
        "grad_norm": 1.3817195892333984,
        "learning_rate": 5.055758362758513e-05,
        "epoch": 0.16722494202525123,
        "step": 1298
    },
    {
        "loss": 2.5441,
        "grad_norm": 2.2551088333129883,
        "learning_rate": 5.0496758446752904e-05,
        "epoch": 0.1673537748003092,
        "step": 1299
    },
    {
        "loss": 2.236,
        "grad_norm": 1.289688229560852,
        "learning_rate": 5.043593253069556e-05,
        "epoch": 0.16748260757536718,
        "step": 1300
    },
    {
        "loss": 2.4858,
        "grad_norm": 1.5886032581329346,
        "learning_rate": 5.0375105969438206e-05,
        "epoch": 0.16761144035042516,
        "step": 1301
    },
    {
        "loss": 2.1005,
        "grad_norm": 3.605422019958496,
        "learning_rate": 5.03142788530069e-05,
        "epoch": 0.16774027312548312,
        "step": 1302
    },
    {
        "loss": 1.4099,
        "grad_norm": 2.63881516456604,
        "learning_rate": 5.025345127142857e-05,
        "epoch": 0.1678691059005411,
        "step": 1303
    },
    {
        "loss": 2.0933,
        "grad_norm": 2.368985176086426,
        "learning_rate": 5.019262331473081e-05,
        "epoch": 0.16799793867559906,
        "step": 1304
    },
    {
        "loss": 1.5419,
        "grad_norm": 2.7521607875823975,
        "learning_rate": 5.013179507294179e-05,
        "epoch": 0.16812677145065705,
        "step": 1305
    },
    {
        "loss": 2.3323,
        "grad_norm": 1.0911999940872192,
        "learning_rate": 5.007096663609003e-05,
        "epoch": 0.16825560422571503,
        "step": 1306
    },
    {
        "loss": 2.1252,
        "grad_norm": 1.3213039636611938,
        "learning_rate": 5.001013809420443e-05,
        "epoch": 0.168384437000773,
        "step": 1307
    },
    {
        "loss": 2.0591,
        "grad_norm": 1.892317771911621,
        "learning_rate": 4.9949309537313965e-05,
        "epoch": 0.16851326977583098,
        "step": 1308
    },
    {
        "loss": 2.2816,
        "grad_norm": 1.5555375814437866,
        "learning_rate": 4.988848105544771e-05,
        "epoch": 0.16864210255088893,
        "step": 1309
    },
    {
        "loss": 2.8371,
        "grad_norm": 1.3904268741607666,
        "learning_rate": 4.982765273863456e-05,
        "epoch": 0.16877093532594692,
        "step": 1310
    },
    {
        "loss": 1.6583,
        "grad_norm": 3.5380284786224365,
        "learning_rate": 4.9766824676903234e-05,
        "epoch": 0.1688997681010049,
        "step": 1311
    },
    {
        "loss": 1.6861,
        "grad_norm": 2.944126605987549,
        "learning_rate": 4.9705996960282015e-05,
        "epoch": 0.16902860087606286,
        "step": 1312
    },
    {
        "loss": 2.0757,
        "grad_norm": 2.1929404735565186,
        "learning_rate": 4.964516967879868e-05,
        "epoch": 0.16915743365112085,
        "step": 1313
    },
    {
        "loss": 2.5249,
        "grad_norm": 1.3894526958465576,
        "learning_rate": 4.958434292248039e-05,
        "epoch": 0.16928626642617883,
        "step": 1314
    },
    {
        "loss": 2.3943,
        "grad_norm": 1.4433950185775757,
        "learning_rate": 4.9523516781353536e-05,
        "epoch": 0.1694150992012368,
        "step": 1315
    },
    {
        "loss": 1.9167,
        "grad_norm": 1.8325704336166382,
        "learning_rate": 4.9462691345443555e-05,
        "epoch": 0.16954393197629478,
        "step": 1316
    },
    {
        "loss": 2.5183,
        "grad_norm": 1.6745834350585938,
        "learning_rate": 4.940186670477486e-05,
        "epoch": 0.16967276475135273,
        "step": 1317
    },
    {
        "loss": 2.1515,
        "grad_norm": 1.5933374166488647,
        "learning_rate": 4.934104294937071e-05,
        "epoch": 0.16980159752641072,
        "step": 1318
    },
    {
        "loss": 1.7252,
        "grad_norm": 1.457022786140442,
        "learning_rate": 4.9280220169253017e-05,
        "epoch": 0.1699304303014687,
        "step": 1319
    },
    {
        "loss": 2.1338,
        "grad_norm": 1.7552448511123657,
        "learning_rate": 4.9219398454442303e-05,
        "epoch": 0.17005926307652666,
        "step": 1320
    },
    {
        "loss": 1.909,
        "grad_norm": 2.7244067192077637,
        "learning_rate": 4.915857789495745e-05,
        "epoch": 0.17018809585158465,
        "step": 1321
    },
    {
        "loss": 1.5363,
        "grad_norm": 2.445042133331299,
        "learning_rate": 4.9097758580815636e-05,
        "epoch": 0.1703169286266426,
        "step": 1322
    },
    {
        "loss": 1.9544,
        "grad_norm": 1.3594331741333008,
        "learning_rate": 4.9036940602032266e-05,
        "epoch": 0.1704457614017006,
        "step": 1323
    },
    {
        "loss": 2.2655,
        "grad_norm": 1.5087482929229736,
        "learning_rate": 4.89761240486207e-05,
        "epoch": 0.17057459417675858,
        "step": 1324
    },
    {
        "loss": 2.3112,
        "grad_norm": 2.280545711517334,
        "learning_rate": 4.8915309010592195e-05,
        "epoch": 0.17070342695181653,
        "step": 1325
    },
    {
        "loss": 1.9662,
        "grad_norm": 2.274578332901001,
        "learning_rate": 4.88544955779558e-05,
        "epoch": 0.17083225972687452,
        "step": 1326
    },
    {
        "loss": 2.1386,
        "grad_norm": 1.3487770557403564,
        "learning_rate": 4.879368384071815e-05,
        "epoch": 0.1709610925019325,
        "step": 1327
    },
    {
        "loss": 2.3936,
        "grad_norm": 2.63284969329834,
        "learning_rate": 4.8732873888883404e-05,
        "epoch": 0.17108992527699046,
        "step": 1328
    },
    {
        "loss": 2.459,
        "grad_norm": 1.7177281379699707,
        "learning_rate": 4.867206581245304e-05,
        "epoch": 0.17121875805204845,
        "step": 1329
    },
    {
        "loss": 2.0896,
        "grad_norm": 1.9146474599838257,
        "learning_rate": 4.861125970142582e-05,
        "epoch": 0.1713475908271064,
        "step": 1330
    },
    {
        "loss": 1.8734,
        "grad_norm": 2.3226287364959717,
        "learning_rate": 4.8550455645797515e-05,
        "epoch": 0.1714764236021644,
        "step": 1331
    },
    {
        "loss": 1.6089,
        "grad_norm": 2.3110835552215576,
        "learning_rate": 4.8489653735560934e-05,
        "epoch": 0.17160525637722238,
        "step": 1332
    },
    {
        "loss": 1.2834,
        "grad_norm": 2.9140117168426514,
        "learning_rate": 4.842885406070566e-05,
        "epoch": 0.17173408915228033,
        "step": 1333
    },
    {
        "loss": 2.5179,
        "grad_norm": 1.4578458070755005,
        "learning_rate": 4.8368056711217965e-05,
        "epoch": 0.17186292192733832,
        "step": 1334
    },
    {
        "loss": 1.1698,
        "grad_norm": 2.518099784851074,
        "learning_rate": 4.8307261777080735e-05,
        "epoch": 0.17199175470239628,
        "step": 1335
    },
    {
        "loss": 2.1111,
        "grad_norm": 2.0157387256622314,
        "learning_rate": 4.8246469348273204e-05,
        "epoch": 0.17212058747745426,
        "step": 1336
    },
    {
        "loss": 2.0758,
        "grad_norm": 1.7383875846862793,
        "learning_rate": 4.818567951477099e-05,
        "epoch": 0.17224942025251225,
        "step": 1337
    },
    {
        "loss": 2.3717,
        "grad_norm": 1.7334253787994385,
        "learning_rate": 4.812489236654575e-05,
        "epoch": 0.1723782530275702,
        "step": 1338
    },
    {
        "loss": 2.5897,
        "grad_norm": 1.7871623039245605,
        "learning_rate": 4.806410799356531e-05,
        "epoch": 0.1725070858026282,
        "step": 1339
    },
    {
        "loss": 1.7556,
        "grad_norm": 2.2494735717773438,
        "learning_rate": 4.800332648579323e-05,
        "epoch": 0.17263591857768618,
        "step": 1340
    },
    {
        "loss": 2.0994,
        "grad_norm": 1.3572089672088623,
        "learning_rate": 4.794254793318898e-05,
        "epoch": 0.17276475135274413,
        "step": 1341
    },
    {
        "loss": 1.4614,
        "grad_norm": 2.830658197402954,
        "learning_rate": 4.788177242570754e-05,
        "epoch": 0.17289358412780212,
        "step": 1342
    },
    {
        "loss": 2.1002,
        "grad_norm": 2.392993927001953,
        "learning_rate": 4.782100005329946e-05,
        "epoch": 0.17302241690286008,
        "step": 1343
    },
    {
        "loss": 2.0751,
        "grad_norm": 1.8670010566711426,
        "learning_rate": 4.7760230905910606e-05,
        "epoch": 0.17315124967791806,
        "step": 1344
    },
    {
        "loss": 2.1725,
        "grad_norm": 1.8571866750717163,
        "learning_rate": 4.769946507348211e-05,
        "epoch": 0.17328008245297605,
        "step": 1345
    },
    {
        "loss": 1.7925,
        "grad_norm": 2.4050493240356445,
        "learning_rate": 4.763870264595016e-05,
        "epoch": 0.173408915228034,
        "step": 1346
    },
    {
        "loss": 2.1788,
        "grad_norm": 1.680963158607483,
        "learning_rate": 4.757794371324589e-05,
        "epoch": 0.173537748003092,
        "step": 1347
    },
    {
        "loss": 2.1784,
        "grad_norm": 2.174827814102173,
        "learning_rate": 4.751718836529535e-05,
        "epoch": 0.17366658077814995,
        "step": 1348
    },
    {
        "loss": 2.2752,
        "grad_norm": 2.4801886081695557,
        "learning_rate": 4.745643669201915e-05,
        "epoch": 0.17379541355320793,
        "step": 1349
    },
    {
        "loss": 2.4715,
        "grad_norm": 2.0907301902770996,
        "learning_rate": 4.73956887833326e-05,
        "epoch": 0.17392424632826592,
        "step": 1350
    },
    {
        "loss": 2.3864,
        "grad_norm": 1.7047911882400513,
        "learning_rate": 4.733494472914532e-05,
        "epoch": 0.17405307910332388,
        "step": 1351
    },
    {
        "loss": 2.1044,
        "grad_norm": 1.4686119556427002,
        "learning_rate": 4.727420461936132e-05,
        "epoch": 0.17418191187838186,
        "step": 1352
    },
    {
        "loss": 1.8711,
        "grad_norm": 2.493544578552246,
        "learning_rate": 4.7213468543878714e-05,
        "epoch": 0.17431074465343985,
        "step": 1353
    },
    {
        "loss": 2.6235,
        "grad_norm": 1.3493880033493042,
        "learning_rate": 4.715273659258966e-05,
        "epoch": 0.1744395774284978,
        "step": 1354
    },
    {
        "loss": 1.5177,
        "grad_norm": 3.44272780418396,
        "learning_rate": 4.709200885538024e-05,
        "epoch": 0.1745684102035558,
        "step": 1355
    },
    {
        "loss": 2.6357,
        "grad_norm": 1.7058039903640747,
        "learning_rate": 4.703128542213022e-05,
        "epoch": 0.17469724297861375,
        "step": 1356
    },
    {
        "loss": 2.4235,
        "grad_norm": 1.975159764289856,
        "learning_rate": 4.69705663827131e-05,
        "epoch": 0.17482607575367173,
        "step": 1357
    },
    {
        "loss": 1.8317,
        "grad_norm": 2.7949917316436768,
        "learning_rate": 4.69098518269958e-05,
        "epoch": 0.17495490852872972,
        "step": 1358
    },
    {
        "loss": 2.168,
        "grad_norm": 1.5541918277740479,
        "learning_rate": 4.684914184483863e-05,
        "epoch": 0.17508374130378768,
        "step": 1359
    },
    {
        "loss": 2.2657,
        "grad_norm": 1.3807573318481445,
        "learning_rate": 4.6788436526095144e-05,
        "epoch": 0.17521257407884566,
        "step": 1360
    },
    {
        "loss": 2.276,
        "grad_norm": 1.214454174041748,
        "learning_rate": 4.672773596061198e-05,
        "epoch": 0.17534140685390362,
        "step": 1361
    },
    {
        "loss": 2.4304,
        "grad_norm": 1.6583279371261597,
        "learning_rate": 4.666704023822871e-05,
        "epoch": 0.1754702396289616,
        "step": 1362
    },
    {
        "loss": 1.7478,
        "grad_norm": 2.419063091278076,
        "learning_rate": 4.6606349448777804e-05,
        "epoch": 0.1755990724040196,
        "step": 1363
    },
    {
        "loss": 1.4663,
        "grad_norm": 2.8695151805877686,
        "learning_rate": 4.6545663682084397e-05,
        "epoch": 0.17572790517907755,
        "step": 1364
    },
    {
        "loss": 2.3184,
        "grad_norm": 1.600486397743225,
        "learning_rate": 4.648498302796615e-05,
        "epoch": 0.17585673795413553,
        "step": 1365
    },
    {
        "loss": 1.7944,
        "grad_norm": 2.150763988494873,
        "learning_rate": 4.642430757623325e-05,
        "epoch": 0.17598557072919352,
        "step": 1366
    },
    {
        "loss": 1.7241,
        "grad_norm": 2.224119186401367,
        "learning_rate": 4.63636374166881e-05,
        "epoch": 0.17611440350425148,
        "step": 1367
    },
    {
        "loss": 2.3244,
        "grad_norm": 1.5247609615325928,
        "learning_rate": 4.630297263912529e-05,
        "epoch": 0.17624323627930946,
        "step": 1368
    },
    {
        "loss": 1.7103,
        "grad_norm": 2.8369243144989014,
        "learning_rate": 4.624231333333149e-05,
        "epoch": 0.17637206905436742,
        "step": 1369
    },
    {
        "loss": 2.4763,
        "grad_norm": 1.1892060041427612,
        "learning_rate": 4.618165958908521e-05,
        "epoch": 0.1765009018294254,
        "step": 1370
    },
    {
        "loss": 2.1006,
        "grad_norm": 1.514617919921875,
        "learning_rate": 4.612101149615678e-05,
        "epoch": 0.1766297346044834,
        "step": 1371
    },
    {
        "loss": 2.4694,
        "grad_norm": 1.9754540920257568,
        "learning_rate": 4.606036914430811e-05,
        "epoch": 0.17675856737954135,
        "step": 1372
    },
    {
        "loss": 2.5717,
        "grad_norm": 2.1525304317474365,
        "learning_rate": 4.599973262329269e-05,
        "epoch": 0.17688740015459933,
        "step": 1373
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.9742622375488281,
        "learning_rate": 4.5939102022855274e-05,
        "epoch": 0.1770162329296573,
        "step": 1374
    },
    {
        "loss": 2.4859,
        "grad_norm": 1.4201152324676514,
        "learning_rate": 4.587847743273197e-05,
        "epoch": 0.17714506570471528,
        "step": 1375
    },
    {
        "loss": 1.8781,
        "grad_norm": 2.756742000579834,
        "learning_rate": 4.58178589426499e-05,
        "epoch": 0.17727389847977326,
        "step": 1376
    },
    {
        "loss": 1.6827,
        "grad_norm": 2.6733272075653076,
        "learning_rate": 4.57572466423272e-05,
        "epoch": 0.17740273125483122,
        "step": 1377
    },
    {
        "loss": 1.4565,
        "grad_norm": 3.271249771118164,
        "learning_rate": 4.5696640621472846e-05,
        "epoch": 0.1775315640298892,
        "step": 1378
    },
    {
        "loss": 2.2024,
        "grad_norm": 2.332286834716797,
        "learning_rate": 4.56360409697865e-05,
        "epoch": 0.1776603968049472,
        "step": 1379
    },
    {
        "loss": 2.4938,
        "grad_norm": 1.223151683807373,
        "learning_rate": 4.5575447776958426e-05,
        "epoch": 0.17778922958000515,
        "step": 1380
    },
    {
        "loss": 1.9876,
        "grad_norm": 2.2630245685577393,
        "learning_rate": 4.551486113266927e-05,
        "epoch": 0.17791806235506313,
        "step": 1381
    },
    {
        "loss": 2.6795,
        "grad_norm": 1.993476390838623,
        "learning_rate": 4.54542811265901e-05,
        "epoch": 0.1780468951301211,
        "step": 1382
    },
    {
        "loss": 2.2585,
        "grad_norm": 1.4772369861602783,
        "learning_rate": 4.539370784838201e-05,
        "epoch": 0.17817572790517908,
        "step": 1383
    },
    {
        "loss": 2.2311,
        "grad_norm": 1.8183319568634033,
        "learning_rate": 4.533314138769626e-05,
        "epoch": 0.17830456068023706,
        "step": 1384
    },
    {
        "loss": 1.4604,
        "grad_norm": 3.098294973373413,
        "learning_rate": 4.5272581834173936e-05,
        "epoch": 0.17843339345529502,
        "step": 1385
    },
    {
        "loss": 2.1809,
        "grad_norm": 2.391144037246704,
        "learning_rate": 4.521202927744598e-05,
        "epoch": 0.178562226230353,
        "step": 1386
    },
    {
        "loss": 2.0872,
        "grad_norm": 1.7152098417282104,
        "learning_rate": 4.5151483807132894e-05,
        "epoch": 0.178691059005411,
        "step": 1387
    },
    {
        "loss": 2.409,
        "grad_norm": 1.7263375520706177,
        "learning_rate": 4.509094551284476e-05,
        "epoch": 0.17881989178046895,
        "step": 1388
    },
    {
        "loss": 2.4512,
        "grad_norm": 2.121905565261841,
        "learning_rate": 4.503041448418102e-05,
        "epoch": 0.17894872455552693,
        "step": 1389
    },
    {
        "loss": 2.2183,
        "grad_norm": 1.8790483474731445,
        "learning_rate": 4.496989081073032e-05,
        "epoch": 0.1790775573305849,
        "step": 1390
    },
    {
        "loss": 2.2817,
        "grad_norm": 1.738028645515442,
        "learning_rate": 4.4909374582070494e-05,
        "epoch": 0.17920639010564288,
        "step": 1391
    },
    {
        "loss": 2.3478,
        "grad_norm": 1.350115418434143,
        "learning_rate": 4.4848865887768294e-05,
        "epoch": 0.17933522288070086,
        "step": 1392
    },
    {
        "loss": 2.459,
        "grad_norm": 1.7157782316207886,
        "learning_rate": 4.4788364817379355e-05,
        "epoch": 0.17946405565575882,
        "step": 1393
    },
    {
        "loss": 2.0697,
        "grad_norm": 1.867631435394287,
        "learning_rate": 4.4727871460448e-05,
        "epoch": 0.1795928884308168,
        "step": 1394
    },
    {
        "loss": 2.0289,
        "grad_norm": 2.377255916595459,
        "learning_rate": 4.4667385906507194e-05,
        "epoch": 0.17972172120587476,
        "step": 1395
    },
    {
        "loss": 2.5911,
        "grad_norm": 1.6993831396102905,
        "learning_rate": 4.460690824507826e-05,
        "epoch": 0.17985055398093275,
        "step": 1396
    },
    {
        "loss": 2.1676,
        "grad_norm": 2.0275826454162598,
        "learning_rate": 4.454643856567094e-05,
        "epoch": 0.17997938675599073,
        "step": 1397
    },
    {
        "loss": 2.4188,
        "grad_norm": 2.0678281784057617,
        "learning_rate": 4.448597695778311e-05,
        "epoch": 0.1801082195310487,
        "step": 1398
    },
    {
        "loss": 2.3894,
        "grad_norm": 1.5238364934921265,
        "learning_rate": 4.442552351090067e-05,
        "epoch": 0.18023705230610668,
        "step": 1399
    },
    {
        "loss": 1.0846,
        "grad_norm": 3.7129909992218018,
        "learning_rate": 4.436507831449752e-05,
        "epoch": 0.18036588508116466,
        "step": 1400
    },
    {
        "loss": 2.2409,
        "grad_norm": 1.4221078157424927,
        "learning_rate": 4.430464145803528e-05,
        "epoch": 0.18049471785622262,
        "step": 1401
    },
    {
        "loss": 2.5234,
        "grad_norm": 1.766870141029358,
        "learning_rate": 4.4244213030963236e-05,
        "epoch": 0.1806235506312806,
        "step": 1402
    },
    {
        "loss": 2.2879,
        "grad_norm": 1.6002254486083984,
        "learning_rate": 4.418379312271827e-05,
        "epoch": 0.18075238340633856,
        "step": 1403
    },
    {
        "loss": 1.8218,
        "grad_norm": 2.2780792713165283,
        "learning_rate": 4.412338182272454e-05,
        "epoch": 0.18088121618139655,
        "step": 1404
    },
    {
        "loss": 1.941,
        "grad_norm": 1.6393646001815796,
        "learning_rate": 4.406297922039357e-05,
        "epoch": 0.18101004895645453,
        "step": 1405
    },
    {
        "loss": 2.1371,
        "grad_norm": 1.320488452911377,
        "learning_rate": 4.400258540512392e-05,
        "epoch": 0.1811388817315125,
        "step": 1406
    },
    {
        "loss": 2.3336,
        "grad_norm": 1.2349555492401123,
        "learning_rate": 4.3942200466301254e-05,
        "epoch": 0.18126771450657048,
        "step": 1407
    },
    {
        "loss": 2.1392,
        "grad_norm": 2.2546234130859375,
        "learning_rate": 4.388182449329796e-05,
        "epoch": 0.18139654728162843,
        "step": 1408
    },
    {
        "loss": 2.5096,
        "grad_norm": 1.3774824142456055,
        "learning_rate": 4.382145757547327e-05,
        "epoch": 0.18152538005668642,
        "step": 1409
    },
    {
        "loss": 2.2292,
        "grad_norm": 2.36458158493042,
        "learning_rate": 4.3761099802172965e-05,
        "epoch": 0.1816542128317444,
        "step": 1410
    },
    {
        "loss": 2.0106,
        "grad_norm": 2.5170071125030518,
        "learning_rate": 4.3700751262729304e-05,
        "epoch": 0.18178304560680236,
        "step": 1411
    },
    {
        "loss": 2.2254,
        "grad_norm": 1.8247530460357666,
        "learning_rate": 4.364041204646087e-05,
        "epoch": 0.18191187838186035,
        "step": 1412
    },
    {
        "loss": 2.095,
        "grad_norm": 2.332303285598755,
        "learning_rate": 4.358008224267245e-05,
        "epoch": 0.18204071115691833,
        "step": 1413
    },
    {
        "loss": 1.8612,
        "grad_norm": 3.0603699684143066,
        "learning_rate": 4.3519761940654926e-05,
        "epoch": 0.1821695439319763,
        "step": 1414
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.658663034439087,
        "learning_rate": 4.345945122968506e-05,
        "epoch": 0.18229837670703428,
        "step": 1415
    },
    {
        "loss": 1.8733,
        "grad_norm": 2.1503453254699707,
        "learning_rate": 4.33991501990255e-05,
        "epoch": 0.18242720948209223,
        "step": 1416
    },
    {
        "loss": 2.4733,
        "grad_norm": 1.1840671300888062,
        "learning_rate": 4.333885893792446e-05,
        "epoch": 0.18255604225715022,
        "step": 1417
    },
    {
        "loss": 1.0659,
        "grad_norm": 2.81217885017395,
        "learning_rate": 4.32785775356158e-05,
        "epoch": 0.1826848750322082,
        "step": 1418
    },
    {
        "loss": 2.1311,
        "grad_norm": 1.7115209102630615,
        "learning_rate": 4.3218306081318716e-05,
        "epoch": 0.18281370780726616,
        "step": 1419
    },
    {
        "loss": 2.3037,
        "grad_norm": 1.4207600355148315,
        "learning_rate": 4.315804466423774e-05,
        "epoch": 0.18294254058232415,
        "step": 1420
    },
    {
        "loss": 1.8566,
        "grad_norm": 2.1946349143981934,
        "learning_rate": 4.309779337356247e-05,
        "epoch": 0.1830713733573821,
        "step": 1421
    },
    {
        "loss": 1.8296,
        "grad_norm": 1.9413697719573975,
        "learning_rate": 4.3037552298467595e-05,
        "epoch": 0.1832002061324401,
        "step": 1422
    },
    {
        "loss": 2.4038,
        "grad_norm": 1.5890586376190186,
        "learning_rate": 4.2977321528112666e-05,
        "epoch": 0.18332903890749808,
        "step": 1423
    },
    {
        "loss": 2.5319,
        "grad_norm": 1.7667045593261719,
        "learning_rate": 4.2917101151641915e-05,
        "epoch": 0.18345787168255603,
        "step": 1424
    },
    {
        "loss": 1.8738,
        "grad_norm": 2.772925615310669,
        "learning_rate": 4.2856891258184304e-05,
        "epoch": 0.18358670445761402,
        "step": 1425
    },
    {
        "loss": 2.1437,
        "grad_norm": 1.667438268661499,
        "learning_rate": 4.2796691936853166e-05,
        "epoch": 0.183715537232672,
        "step": 1426
    },
    {
        "loss": 2.2827,
        "grad_norm": 1.8348098993301392,
        "learning_rate": 4.2736503276746285e-05,
        "epoch": 0.18384437000772996,
        "step": 1427
    },
    {
        "loss": 2.0883,
        "grad_norm": 1.7453234195709229,
        "learning_rate": 4.267632536694558e-05,
        "epoch": 0.18397320278278795,
        "step": 1428
    },
    {
        "loss": 2.0569,
        "grad_norm": 1.1747695207595825,
        "learning_rate": 4.2616158296517136e-05,
        "epoch": 0.1841020355578459,
        "step": 1429
    },
    {
        "loss": 1.6121,
        "grad_norm": 2.3868370056152344,
        "learning_rate": 4.2556002154510935e-05,
        "epoch": 0.1842308683329039,
        "step": 1430
    },
    {
        "loss": 2.2618,
        "grad_norm": 1.5640119314193726,
        "learning_rate": 4.249585702996082e-05,
        "epoch": 0.18435970110796188,
        "step": 1431
    },
    {
        "loss": 1.84,
        "grad_norm": 1.689373254776001,
        "learning_rate": 4.243572301188433e-05,
        "epoch": 0.18448853388301983,
        "step": 1432
    },
    {
        "loss": 2.3586,
        "grad_norm": 2.388686180114746,
        "learning_rate": 4.237560018928251e-05,
        "epoch": 0.18461736665807782,
        "step": 1433
    },
    {
        "loss": 1.3908,
        "grad_norm": 2.8522286415100098,
        "learning_rate": 4.2315488651139935e-05,
        "epoch": 0.18474619943313578,
        "step": 1434
    },
    {
        "loss": 1.9425,
        "grad_norm": 2.062613010406494,
        "learning_rate": 4.225538848642437e-05,
        "epoch": 0.18487503220819376,
        "step": 1435
    },
    {
        "loss": 2.4253,
        "grad_norm": 1.2124967575073242,
        "learning_rate": 4.219529978408683e-05,
        "epoch": 0.18500386498325175,
        "step": 1436
    },
    {
        "loss": 2.4111,
        "grad_norm": 1.5429421663284302,
        "learning_rate": 4.2135222633061315e-05,
        "epoch": 0.1851326977583097,
        "step": 1437
    },
    {
        "loss": 1.7459,
        "grad_norm": 2.1229407787323,
        "learning_rate": 4.207515712226477e-05,
        "epoch": 0.1852615305333677,
        "step": 1438
    },
    {
        "loss": 1.8557,
        "grad_norm": 2.722754955291748,
        "learning_rate": 4.201510334059685e-05,
        "epoch": 0.18539036330842568,
        "step": 1439
    },
    {
        "loss": 2.0586,
        "grad_norm": 1.5873515605926514,
        "learning_rate": 4.195506137693993e-05,
        "epoch": 0.18551919608348363,
        "step": 1440
    },
    {
        "loss": 1.9036,
        "grad_norm": 2.237480640411377,
        "learning_rate": 4.1895031320158826e-05,
        "epoch": 0.18564802885854162,
        "step": 1441
    },
    {
        "loss": 2.3925,
        "grad_norm": 2.262336492538452,
        "learning_rate": 4.1835013259100755e-05,
        "epoch": 0.18577686163359958,
        "step": 1442
    },
    {
        "loss": 1.7007,
        "grad_norm": 2.5293068885803223,
        "learning_rate": 4.1775007282595214e-05,
        "epoch": 0.18590569440865756,
        "step": 1443
    },
    {
        "loss": 1.829,
        "grad_norm": 2.543043613433838,
        "learning_rate": 4.171501347945377e-05,
        "epoch": 0.18603452718371555,
        "step": 1444
    },
    {
        "loss": 2.2377,
        "grad_norm": 1.4008890390396118,
        "learning_rate": 4.165503193846995e-05,
        "epoch": 0.1861633599587735,
        "step": 1445
    },
    {
        "loss": 2.2462,
        "grad_norm": 1.5924241542816162,
        "learning_rate": 4.159506274841921e-05,
        "epoch": 0.1862921927338315,
        "step": 1446
    },
    {
        "loss": 1.9613,
        "grad_norm": 2.676769256591797,
        "learning_rate": 4.153510599805866e-05,
        "epoch": 0.18642102550888945,
        "step": 1447
    },
    {
        "loss": 2.1024,
        "grad_norm": 2.438974380493164,
        "learning_rate": 4.147516177612706e-05,
        "epoch": 0.18654985828394743,
        "step": 1448
    },
    {
        "loss": 1.6466,
        "grad_norm": 2.788975954055786,
        "learning_rate": 4.1415230171344545e-05,
        "epoch": 0.18667869105900542,
        "step": 1449
    },
    {
        "loss": 1.8237,
        "grad_norm": 2.725022077560425,
        "learning_rate": 4.1355311272412664e-05,
        "epoch": 0.18680752383406338,
        "step": 1450
    },
    {
        "loss": 2.0704,
        "grad_norm": 2.4260690212249756,
        "learning_rate": 4.129540516801408e-05,
        "epoch": 0.18693635660912136,
        "step": 1451
    },
    {
        "loss": 1.957,
        "grad_norm": 2.648709535598755,
        "learning_rate": 4.123551194681259e-05,
        "epoch": 0.18706518938417935,
        "step": 1452
    },
    {
        "loss": 2.5499,
        "grad_norm": 2.322956085205078,
        "learning_rate": 4.117563169745287e-05,
        "epoch": 0.1871940221592373,
        "step": 1453
    },
    {
        "loss": 1.296,
        "grad_norm": 2.876580238342285,
        "learning_rate": 4.11157645085604e-05,
        "epoch": 0.1873228549342953,
        "step": 1454
    },
    {
        "loss": 2.4654,
        "grad_norm": 1.9126667976379395,
        "learning_rate": 4.1055910468741394e-05,
        "epoch": 0.18745168770935325,
        "step": 1455
    },
    {
        "loss": 2.3219,
        "grad_norm": 1.8397048711776733,
        "learning_rate": 4.099606966658252e-05,
        "epoch": 0.18758052048441123,
        "step": 1456
    },
    {
        "loss": 1.6775,
        "grad_norm": 1.2841178178787231,
        "learning_rate": 4.093624219065093e-05,
        "epoch": 0.18770935325946922,
        "step": 1457
    },
    {
        "loss": 1.8339,
        "grad_norm": 2.3714945316314697,
        "learning_rate": 4.0876428129493974e-05,
        "epoch": 0.18783818603452718,
        "step": 1458
    },
    {
        "loss": 2.5441,
        "grad_norm": 1.2783312797546387,
        "learning_rate": 4.0816627571639245e-05,
        "epoch": 0.18796701880958516,
        "step": 1459
    },
    {
        "loss": 2.0535,
        "grad_norm": 2.4931344985961914,
        "learning_rate": 4.075684060559422e-05,
        "epoch": 0.18809585158464312,
        "step": 1460
    },
    {
        "loss": 2.338,
        "grad_norm": 2.353642702102661,
        "learning_rate": 4.0697067319846394e-05,
        "epoch": 0.1882246843597011,
        "step": 1461
    },
    {
        "loss": 1.8739,
        "grad_norm": 1.7477370500564575,
        "learning_rate": 4.063730780286292e-05,
        "epoch": 0.1883535171347591,
        "step": 1462
    },
    {
        "loss": 1.9915,
        "grad_norm": 2.0181710720062256,
        "learning_rate": 4.057756214309063e-05,
        "epoch": 0.18848234990981705,
        "step": 1463
    },
    {
        "loss": 2.1163,
        "grad_norm": 2.2510478496551514,
        "learning_rate": 4.0517830428955796e-05,
        "epoch": 0.18861118268487503,
        "step": 1464
    },
    {
        "loss": 1.6105,
        "grad_norm": 2.6330909729003906,
        "learning_rate": 4.045811274886411e-05,
        "epoch": 0.18874001545993302,
        "step": 1465
    },
    {
        "loss": 2.3662,
        "grad_norm": 2.915682077407837,
        "learning_rate": 4.039840919120043e-05,
        "epoch": 0.18886884823499098,
        "step": 1466
    },
    {
        "loss": 2.3339,
        "grad_norm": 1.7268189191818237,
        "learning_rate": 4.0338719844328756e-05,
        "epoch": 0.18899768101004896,
        "step": 1467
    },
    {
        "loss": 1.9339,
        "grad_norm": 2.631775140762329,
        "learning_rate": 4.0279044796592066e-05,
        "epoch": 0.18912651378510692,
        "step": 1468
    },
    {
        "loss": 2.2375,
        "grad_norm": 1.2912825345993042,
        "learning_rate": 4.021938413631211e-05,
        "epoch": 0.1892553465601649,
        "step": 1469
    },
    {
        "loss": 2.0209,
        "grad_norm": 2.6200098991394043,
        "learning_rate": 4.01597379517894e-05,
        "epoch": 0.1893841793352229,
        "step": 1470
    },
    {
        "loss": 2.2391,
        "grad_norm": 2.1316890716552734,
        "learning_rate": 4.0100106331302994e-05,
        "epoch": 0.18951301211028085,
        "step": 1471
    },
    {
        "loss": 1.8477,
        "grad_norm": 2.176581621170044,
        "learning_rate": 4.004048936311045e-05,
        "epoch": 0.18964184488533883,
        "step": 1472
    },
    {
        "loss": 2.2635,
        "grad_norm": 1.3360533714294434,
        "learning_rate": 3.998088713544754e-05,
        "epoch": 0.1897706776603968,
        "step": 1473
    },
    {
        "loss": 2.3325,
        "grad_norm": 1.9072036743164062,
        "learning_rate": 3.992129973652834e-05,
        "epoch": 0.18989951043545478,
        "step": 1474
    },
    {
        "loss": 2.3696,
        "grad_norm": 1.4552452564239502,
        "learning_rate": 3.9861727254544865e-05,
        "epoch": 0.19002834321051276,
        "step": 1475
    },
    {
        "loss": 1.3789,
        "grad_norm": 3.1266095638275146,
        "learning_rate": 3.980216977766712e-05,
        "epoch": 0.19015717598557072,
        "step": 1476
    },
    {
        "loss": 2.3998,
        "grad_norm": 1.7010021209716797,
        "learning_rate": 3.974262739404291e-05,
        "epoch": 0.1902860087606287,
        "step": 1477
    },
    {
        "loss": 2.2013,
        "grad_norm": 2.100048303604126,
        "learning_rate": 3.968310019179765e-05,
        "epoch": 0.1904148415356867,
        "step": 1478
    },
    {
        "loss": 2.2063,
        "grad_norm": 1.883663296699524,
        "learning_rate": 3.9623588259034295e-05,
        "epoch": 0.19054367431074465,
        "step": 1479
    },
    {
        "loss": 2.1566,
        "grad_norm": 2.270559549331665,
        "learning_rate": 3.956409168383325e-05,
        "epoch": 0.19067250708580263,
        "step": 1480
    },
    {
        "loss": 1.6608,
        "grad_norm": 2.182412624359131,
        "learning_rate": 3.9504610554252135e-05,
        "epoch": 0.1908013398608606,
        "step": 1481
    },
    {
        "loss": 1.9223,
        "grad_norm": 2.7896270751953125,
        "learning_rate": 3.944514495832577e-05,
        "epoch": 0.19093017263591858,
        "step": 1482
    },
    {
        "loss": 2.2386,
        "grad_norm": 1.571203351020813,
        "learning_rate": 3.9385694984065926e-05,
        "epoch": 0.19105900541097656,
        "step": 1483
    },
    {
        "loss": 2.3969,
        "grad_norm": 1.3516710996627808,
        "learning_rate": 3.932626071946125e-05,
        "epoch": 0.19118783818603452,
        "step": 1484
    },
    {
        "loss": 2.4166,
        "grad_norm": 1.7019665241241455,
        "learning_rate": 3.926684225247718e-05,
        "epoch": 0.1913166709610925,
        "step": 1485
    },
    {
        "loss": 2.3194,
        "grad_norm": 1.4499236345291138,
        "learning_rate": 3.920743967105578e-05,
        "epoch": 0.19144550373615046,
        "step": 1486
    },
    {
        "loss": 2.053,
        "grad_norm": 1.7500836849212646,
        "learning_rate": 3.914805306311556e-05,
        "epoch": 0.19157433651120845,
        "step": 1487
    },
    {
        "loss": 2.4661,
        "grad_norm": 1.6647143363952637,
        "learning_rate": 3.9088682516551376e-05,
        "epoch": 0.19170316928626643,
        "step": 1488
    },
    {
        "loss": 1.9617,
        "grad_norm": 1.3615303039550781,
        "learning_rate": 3.902932811923438e-05,
        "epoch": 0.1918320020613244,
        "step": 1489
    },
    {
        "loss": 1.6695,
        "grad_norm": 2.916530132293701,
        "learning_rate": 3.896998995901178e-05,
        "epoch": 0.19196083483638238,
        "step": 1490
    },
    {
        "loss": 2.4735,
        "grad_norm": 1.7479212284088135,
        "learning_rate": 3.891066812370675e-05,
        "epoch": 0.19208966761144036,
        "step": 1491
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.3061134815216064,
        "learning_rate": 3.8851362701118296e-05,
        "epoch": 0.19221850038649832,
        "step": 1492
    },
    {
        "loss": 1.6558,
        "grad_norm": 2.0895767211914062,
        "learning_rate": 3.8792073779021206e-05,
        "epoch": 0.1923473331615563,
        "step": 1493
    },
    {
        "loss": 2.123,
        "grad_norm": 2.1731464862823486,
        "learning_rate": 3.873280144516571e-05,
        "epoch": 0.19247616593661426,
        "step": 1494
    },
    {
        "loss": 2.5199,
        "grad_norm": 2.1255555152893066,
        "learning_rate": 3.867354578727761e-05,
        "epoch": 0.19260499871167225,
        "step": 1495
    },
    {
        "loss": 1.9046,
        "grad_norm": 2.527858257293701,
        "learning_rate": 3.861430689305795e-05,
        "epoch": 0.19273383148673023,
        "step": 1496
    },
    {
        "loss": 2.4028,
        "grad_norm": 1.5276899337768555,
        "learning_rate": 3.855508485018303e-05,
        "epoch": 0.1928626642617882,
        "step": 1497
    },
    {
        "loss": 1.6558,
        "grad_norm": 2.933567523956299,
        "learning_rate": 3.849587974630414e-05,
        "epoch": 0.19299149703684618,
        "step": 1498
    },
    {
        "loss": 2.1255,
        "grad_norm": 1.8248974084854126,
        "learning_rate": 3.843669166904757e-05,
        "epoch": 0.19312032981190416,
        "step": 1499
    },
    {
        "loss": 1.9446,
        "grad_norm": 3.0232107639312744,
        "learning_rate": 3.8377520706014326e-05,
        "epoch": 0.19324916258696212,
        "step": 1500
    },
    {
        "loss": 2.371,
        "grad_norm": 2.3064281940460205,
        "learning_rate": 3.8318366944780145e-05,
        "epoch": 0.1933779953620201,
        "step": 1501
    },
    {
        "loss": 2.363,
        "grad_norm": 1.140008568763733,
        "learning_rate": 3.825923047289533e-05,
        "epoch": 0.19350682813707806,
        "step": 1502
    },
    {
        "loss": 1.9553,
        "grad_norm": 1.5089600086212158,
        "learning_rate": 3.8200111377884495e-05,
        "epoch": 0.19363566091213605,
        "step": 1503
    },
    {
        "loss": 1.839,
        "grad_norm": 1.8363795280456543,
        "learning_rate": 3.814100974724662e-05,
        "epoch": 0.19376449368719403,
        "step": 1504
    },
    {
        "loss": 2.3218,
        "grad_norm": 1.7213209867477417,
        "learning_rate": 3.808192566845481e-05,
        "epoch": 0.193893326462252,
        "step": 1505
    },
    {
        "loss": 2.4806,
        "grad_norm": 2.0915462970733643,
        "learning_rate": 3.802285922895622e-05,
        "epoch": 0.19402215923730998,
        "step": 1506
    },
    {
        "loss": 1.9841,
        "grad_norm": 1.6770676374435425,
        "learning_rate": 3.796381051617183e-05,
        "epoch": 0.19415099201236793,
        "step": 1507
    },
    {
        "loss": 1.8609,
        "grad_norm": 2.4392142295837402,
        "learning_rate": 3.790477961749646e-05,
        "epoch": 0.19427982478742592,
        "step": 1508
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.4736318588256836,
        "learning_rate": 3.784576662029851e-05,
        "epoch": 0.1944086575624839,
        "step": 1509
    },
    {
        "loss": 1.7977,
        "grad_norm": 3.192556142807007,
        "learning_rate": 3.77867716119199e-05,
        "epoch": 0.19453749033754186,
        "step": 1510
    },
    {
        "loss": 2.1199,
        "grad_norm": 2.985813856124878,
        "learning_rate": 3.7727794679675966e-05,
        "epoch": 0.19466632311259985,
        "step": 1511
    },
    {
        "loss": 2.2264,
        "grad_norm": 1.6245890855789185,
        "learning_rate": 3.766883591085523e-05,
        "epoch": 0.19479515588765783,
        "step": 1512
    },
    {
        "loss": 2.1718,
        "grad_norm": 1.661419153213501,
        "learning_rate": 3.760989539271934e-05,
        "epoch": 0.1949239886627158,
        "step": 1513
    },
    {
        "loss": 2.1412,
        "grad_norm": 1.8922138214111328,
        "learning_rate": 3.755097321250299e-05,
        "epoch": 0.19505282143777378,
        "step": 1514
    },
    {
        "loss": 2.1547,
        "grad_norm": 2.089712619781494,
        "learning_rate": 3.749206945741367e-05,
        "epoch": 0.19518165421283173,
        "step": 1515
    },
    {
        "loss": 2.5936,
        "grad_norm": 1.3145015239715576,
        "learning_rate": 3.7433184214631605e-05,
        "epoch": 0.19531048698788972,
        "step": 1516
    },
    {
        "loss": 2.3563,
        "grad_norm": 1.5628021955490112,
        "learning_rate": 3.737431757130968e-05,
        "epoch": 0.1954393197629477,
        "step": 1517
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.7824310064315796,
        "learning_rate": 3.731546961457319e-05,
        "epoch": 0.19556815253800566,
        "step": 1518
    },
    {
        "loss": 2.1468,
        "grad_norm": 1.714318037033081,
        "learning_rate": 3.7256640431519764e-05,
        "epoch": 0.19569698531306365,
        "step": 1519
    },
    {
        "loss": 2.0188,
        "grad_norm": 2.4694340229034424,
        "learning_rate": 3.719783010921931e-05,
        "epoch": 0.1958258180881216,
        "step": 1520
    },
    {
        "loss": 2.6532,
        "grad_norm": 2.078153610229492,
        "learning_rate": 3.713903873471379e-05,
        "epoch": 0.1959546508631796,
        "step": 1521
    },
    {
        "loss": 1.9709,
        "grad_norm": 3.8563766479492188,
        "learning_rate": 3.708026639501708e-05,
        "epoch": 0.19608348363823758,
        "step": 1522
    },
    {
        "loss": 2.1062,
        "grad_norm": 2.0915157794952393,
        "learning_rate": 3.702151317711497e-05,
        "epoch": 0.19621231641329553,
        "step": 1523
    },
    {
        "loss": 1.7765,
        "grad_norm": 2.2685439586639404,
        "learning_rate": 3.696277916796485e-05,
        "epoch": 0.19634114918835352,
        "step": 1524
    },
    {
        "loss": 2.5049,
        "grad_norm": 2.262948751449585,
        "learning_rate": 3.690406445449577e-05,
        "epoch": 0.1964699819634115,
        "step": 1525
    },
    {
        "loss": 2.4125,
        "grad_norm": 1.9023284912109375,
        "learning_rate": 3.6845369123608155e-05,
        "epoch": 0.19659881473846946,
        "step": 1526
    },
    {
        "loss": 2.0547,
        "grad_norm": 2.0941526889801025,
        "learning_rate": 3.678669326217382e-05,
        "epoch": 0.19672764751352745,
        "step": 1527
    },
    {
        "loss": 2.1394,
        "grad_norm": 1.6218562126159668,
        "learning_rate": 3.6728036957035635e-05,
        "epoch": 0.1968564802885854,
        "step": 1528
    },
    {
        "loss": 1.9468,
        "grad_norm": 2.8947393894195557,
        "learning_rate": 3.666940029500766e-05,
        "epoch": 0.1969853130636434,
        "step": 1529
    },
    {
        "loss": 2.4025,
        "grad_norm": 2.2500905990600586,
        "learning_rate": 3.661078336287482e-05,
        "epoch": 0.19711414583870138,
        "step": 1530
    },
    {
        "loss": 2.3432,
        "grad_norm": 1.9239692687988281,
        "learning_rate": 3.6552186247392825e-05,
        "epoch": 0.19724297861375933,
        "step": 1531
    },
    {
        "loss": 1.801,
        "grad_norm": 1.9974802732467651,
        "learning_rate": 3.6493609035288115e-05,
        "epoch": 0.19737181138881732,
        "step": 1532
    },
    {
        "loss": 1.9245,
        "grad_norm": 2.710378646850586,
        "learning_rate": 3.643505181325758e-05,
        "epoch": 0.19750064416387528,
        "step": 1533
    },
    {
        "loss": 1.7168,
        "grad_norm": 2.4952404499053955,
        "learning_rate": 3.637651466796862e-05,
        "epoch": 0.19762947693893326,
        "step": 1534
    },
    {
        "loss": 2.1595,
        "grad_norm": 2.39992094039917,
        "learning_rate": 3.631799768605886e-05,
        "epoch": 0.19775830971399125,
        "step": 1535
    },
    {
        "loss": 2.1757,
        "grad_norm": 2.0096042156219482,
        "learning_rate": 3.625950095413614e-05,
        "epoch": 0.1978871424890492,
        "step": 1536
    },
    {
        "loss": 2.1477,
        "grad_norm": 1.8179851770401,
        "learning_rate": 3.6201024558778216e-05,
        "epoch": 0.1980159752641072,
        "step": 1537
    },
    {
        "loss": 1.3468,
        "grad_norm": 2.1011526584625244,
        "learning_rate": 3.614256858653287e-05,
        "epoch": 0.19814480803916518,
        "step": 1538
    },
    {
        "loss": 2.4436,
        "grad_norm": 2.482348680496216,
        "learning_rate": 3.6084133123917586e-05,
        "epoch": 0.19827364081422313,
        "step": 1539
    },
    {
        "loss": 2.0298,
        "grad_norm": 1.503664493560791,
        "learning_rate": 3.602571825741953e-05,
        "epoch": 0.19840247358928112,
        "step": 1540
    },
    {
        "loss": 2.1302,
        "grad_norm": 1.7863446474075317,
        "learning_rate": 3.5967324073495344e-05,
        "epoch": 0.19853130636433908,
        "step": 1541
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.9845741987228394,
        "learning_rate": 3.5908950658571106e-05,
        "epoch": 0.19866013913939706,
        "step": 1542
    },
    {
        "loss": 2.4488,
        "grad_norm": 1.8587065935134888,
        "learning_rate": 3.585059809904212e-05,
        "epoch": 0.19878897191445505,
        "step": 1543
    },
    {
        "loss": 1.546,
        "grad_norm": 2.719815731048584,
        "learning_rate": 3.579226648127281e-05,
        "epoch": 0.198917804689513,
        "step": 1544
    },
    {
        "loss": 2.1432,
        "grad_norm": 1.80784273147583,
        "learning_rate": 3.573395589159668e-05,
        "epoch": 0.199046637464571,
        "step": 1545
    },
    {
        "loss": 1.8307,
        "grad_norm": 2.2204346656799316,
        "learning_rate": 3.5675666416316004e-05,
        "epoch": 0.19917547023962895,
        "step": 1546
    },
    {
        "loss": 2.2129,
        "grad_norm": 2.2584524154663086,
        "learning_rate": 3.561739814170191e-05,
        "epoch": 0.19930430301468693,
        "step": 1547
    },
    {
        "loss": 2.5368,
        "grad_norm": 2.1385440826416016,
        "learning_rate": 3.5559151153994055e-05,
        "epoch": 0.19943313578974492,
        "step": 1548
    },
    {
        "loss": 2.63,
        "grad_norm": 1.530228853225708,
        "learning_rate": 3.550092553940066e-05,
        "epoch": 0.19956196856480288,
        "step": 1549
    },
    {
        "loss": 2.5686,
        "grad_norm": 2.3952908515930176,
        "learning_rate": 3.544272138409829e-05,
        "epoch": 0.19969080133986086,
        "step": 1550
    },
    {
        "loss": 2.2303,
        "grad_norm": 1.564391016960144,
        "learning_rate": 3.538453877423176e-05,
        "epoch": 0.19981963411491885,
        "step": 1551
    },
    {
        "loss": 2.2356,
        "grad_norm": 2.301589250564575,
        "learning_rate": 3.5326377795913965e-05,
        "epoch": 0.1999484668899768,
        "step": 1552
    },
    {
        "loss": 1.7374,
        "grad_norm": 1.7338430881500244,
        "learning_rate": 3.5268238535225794e-05,
        "epoch": 0.2000772996650348,
        "step": 1553
    },
    {
        "loss": 1.4011,
        "grad_norm": 3.5987508296966553,
        "learning_rate": 3.5210121078216026e-05,
        "epoch": 0.20020613244009275,
        "step": 1554
    },
    {
        "loss": 1.616,
        "grad_norm": 3.3493852615356445,
        "learning_rate": 3.515202551090114e-05,
        "epoch": 0.20033496521515073,
        "step": 1555
    },
    {
        "loss": 2.0419,
        "grad_norm": 1.6011791229248047,
        "learning_rate": 3.509395191926521e-05,
        "epoch": 0.20046379799020872,
        "step": 1556
    },
    {
        "loss": 2.2521,
        "grad_norm": 2.1760380268096924,
        "learning_rate": 3.5035900389259826e-05,
        "epoch": 0.20059263076526668,
        "step": 1557
    },
    {
        "loss": 1.434,
        "grad_norm": 2.703110694885254,
        "learning_rate": 3.497787100680386e-05,
        "epoch": 0.20072146354032466,
        "step": 1558
    },
    {
        "loss": 2.6783,
        "grad_norm": 1.405478596687317,
        "learning_rate": 3.491986385778347e-05,
        "epoch": 0.20085029631538262,
        "step": 1559
    },
    {
        "loss": 2.0219,
        "grad_norm": 1.868947148323059,
        "learning_rate": 3.486187902805189e-05,
        "epoch": 0.2009791290904406,
        "step": 1560
    },
    {
        "loss": 2.3978,
        "grad_norm": 1.5096298456192017,
        "learning_rate": 3.480391660342929e-05,
        "epoch": 0.2011079618654986,
        "step": 1561
    },
    {
        "loss": 1.8507,
        "grad_norm": 2.251033306121826,
        "learning_rate": 3.4745976669702687e-05,
        "epoch": 0.20123679464055655,
        "step": 1562
    },
    {
        "loss": 1.9152,
        "grad_norm": 2.559312105178833,
        "learning_rate": 3.468805931262585e-05,
        "epoch": 0.20136562741561453,
        "step": 1563
    },
    {
        "loss": 1.7546,
        "grad_norm": 2.9224302768707275,
        "learning_rate": 3.4630164617919115e-05,
        "epoch": 0.20149446019067252,
        "step": 1564
    },
    {
        "loss": 1.195,
        "grad_norm": 3.363267421722412,
        "learning_rate": 3.4572292671269226e-05,
        "epoch": 0.20162329296573048,
        "step": 1565
    },
    {
        "loss": 2.394,
        "grad_norm": 1.1517809629440308,
        "learning_rate": 3.4514443558329343e-05,
        "epoch": 0.20175212574078846,
        "step": 1566
    },
    {
        "loss": 2.6284,
        "grad_norm": 1.1365312337875366,
        "learning_rate": 3.445661736471877e-05,
        "epoch": 0.20188095851584642,
        "step": 1567
    },
    {
        "loss": 1.6605,
        "grad_norm": 2.6935455799102783,
        "learning_rate": 3.439881417602292e-05,
        "epoch": 0.2020097912909044,
        "step": 1568
    },
    {
        "loss": 1.6007,
        "grad_norm": 2.173443078994751,
        "learning_rate": 3.434103407779313e-05,
        "epoch": 0.2021386240659624,
        "step": 1569
    },
    {
        "loss": 2.4409,
        "grad_norm": 1.2678567171096802,
        "learning_rate": 3.428327715554661e-05,
        "epoch": 0.20226745684102035,
        "step": 1570
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.5159112215042114,
        "learning_rate": 3.42255434947662e-05,
        "epoch": 0.20239628961607833,
        "step": 1571
    },
    {
        "loss": 2.2886,
        "grad_norm": 1.2464503049850464,
        "learning_rate": 3.416783318090038e-05,
        "epoch": 0.2025251223911363,
        "step": 1572
    },
    {
        "loss": 2.2355,
        "grad_norm": 2.602900505065918,
        "learning_rate": 3.4110146299363e-05,
        "epoch": 0.20265395516619428,
        "step": 1573
    },
    {
        "loss": 2.5162,
        "grad_norm": 1.2846091985702515,
        "learning_rate": 3.4052482935533325e-05,
        "epoch": 0.20278278794125226,
        "step": 1574
    },
    {
        "loss": 1.4357,
        "grad_norm": 2.870852470397949,
        "learning_rate": 3.399484317475575e-05,
        "epoch": 0.20291162071631022,
        "step": 1575
    },
    {
        "loss": 2.0504,
        "grad_norm": 1.6984806060791016,
        "learning_rate": 3.393722710233972e-05,
        "epoch": 0.2030404534913682,
        "step": 1576
    },
    {
        "loss": 1.7619,
        "grad_norm": 2.631350040435791,
        "learning_rate": 3.387963480355969e-05,
        "epoch": 0.2031692862664262,
        "step": 1577
    },
    {
        "loss": 1.8759,
        "grad_norm": 2.189826250076294,
        "learning_rate": 3.3822066363654835e-05,
        "epoch": 0.20329811904148415,
        "step": 1578
    },
    {
        "loss": 2.4401,
        "grad_norm": 1.7026948928833008,
        "learning_rate": 3.376452186782914e-05,
        "epoch": 0.20342695181654213,
        "step": 1579
    },
    {
        "loss": 2.1406,
        "grad_norm": 2.6191792488098145,
        "learning_rate": 3.370700140125101e-05,
        "epoch": 0.2035557845916001,
        "step": 1580
    },
    {
        "loss": 2.4185,
        "grad_norm": 1.9779514074325562,
        "learning_rate": 3.36495050490534e-05,
        "epoch": 0.20368461736665808,
        "step": 1581
    },
    {
        "loss": 1.5825,
        "grad_norm": 2.869657516479492,
        "learning_rate": 3.3592032896333504e-05,
        "epoch": 0.20381345014171606,
        "step": 1582
    },
    {
        "loss": 1.871,
        "grad_norm": 2.3702149391174316,
        "learning_rate": 3.3534585028152754e-05,
        "epoch": 0.20394228291677402,
        "step": 1583
    },
    {
        "loss": 2.101,
        "grad_norm": 1.6864852905273438,
        "learning_rate": 3.347716152953659e-05,
        "epoch": 0.204071115691832,
        "step": 1584
    },
    {
        "loss": 2.2287,
        "grad_norm": 1.8475528955459595,
        "learning_rate": 3.341976248547445e-05,
        "epoch": 0.20419994846688996,
        "step": 1585
    },
    {
        "loss": 1.5947,
        "grad_norm": 2.3077921867370605,
        "learning_rate": 3.336238798091949e-05,
        "epoch": 0.20432878124194795,
        "step": 1586
    },
    {
        "loss": 2.0623,
        "grad_norm": 2.8252553939819336,
        "learning_rate": 3.330503810078859e-05,
        "epoch": 0.20445761401700593,
        "step": 1587
    },
    {
        "loss": 2.2794,
        "grad_norm": 1.5784434080123901,
        "learning_rate": 3.324771292996222e-05,
        "epoch": 0.2045864467920639,
        "step": 1588
    },
    {
        "loss": 2.0926,
        "grad_norm": 2.4104366302490234,
        "learning_rate": 3.319041255328423e-05,
        "epoch": 0.20471527956712188,
        "step": 1589
    },
    {
        "loss": 1.5403,
        "grad_norm": 2.1641266345977783,
        "learning_rate": 3.3133137055561794e-05,
        "epoch": 0.20484411234217986,
        "step": 1590
    },
    {
        "loss": 1.3408,
        "grad_norm": 2.8760671615600586,
        "learning_rate": 3.307588652156523e-05,
        "epoch": 0.20497294511723782,
        "step": 1591
    },
    {
        "loss": 2.2322,
        "grad_norm": 1.984313726425171,
        "learning_rate": 3.3018661036027967e-05,
        "epoch": 0.2051017778922958,
        "step": 1592
    },
    {
        "loss": 2.547,
        "grad_norm": 1.7593194246292114,
        "learning_rate": 3.2961460683646326e-05,
        "epoch": 0.20523061066735376,
        "step": 1593
    },
    {
        "loss": 2.2255,
        "grad_norm": 1.4120757579803467,
        "learning_rate": 3.290428554907944e-05,
        "epoch": 0.20535944344241175,
        "step": 1594
    },
    {
        "loss": 2.0555,
        "grad_norm": 1.701546549797058,
        "learning_rate": 3.284713571694911e-05,
        "epoch": 0.20548827621746973,
        "step": 1595
    },
    {
        "loss": 2.3195,
        "grad_norm": 1.883451223373413,
        "learning_rate": 3.279001127183968e-05,
        "epoch": 0.2056171089925277,
        "step": 1596
    },
    {
        "loss": 2.503,
        "grad_norm": 1.8538440465927124,
        "learning_rate": 3.273291229829795e-05,
        "epoch": 0.20574594176758568,
        "step": 1597
    },
    {
        "loss": 2.4272,
        "grad_norm": 1.0775657892227173,
        "learning_rate": 3.2675838880833e-05,
        "epoch": 0.20587477454264363,
        "step": 1598
    },
    {
        "loss": 2.5984,
        "grad_norm": 1.6549715995788574,
        "learning_rate": 3.261879110391607e-05,
        "epoch": 0.20600360731770162,
        "step": 1599
    },
    {
        "loss": 1.6421,
        "grad_norm": 2.9550185203552246,
        "learning_rate": 3.256176905198049e-05,
        "epoch": 0.2061324400927596,
        "step": 1600
    },
    {
        "loss": 1.7607,
        "grad_norm": 3.167188882827759,
        "learning_rate": 3.250477280942149e-05,
        "epoch": 0.20626127286781756,
        "step": 1601
    },
    {
        "loss": 2.4476,
        "grad_norm": 2.2861506938934326,
        "learning_rate": 3.2447802460596124e-05,
        "epoch": 0.20639010564287555,
        "step": 1602
    },
    {
        "loss": 2.2757,
        "grad_norm": 1.8514550924301147,
        "learning_rate": 3.239085808982307e-05,
        "epoch": 0.20651893841793353,
        "step": 1603
    },
    {
        "loss": 2.3534,
        "grad_norm": 2.2380263805389404,
        "learning_rate": 3.233393978138264e-05,
        "epoch": 0.2066477711929915,
        "step": 1604
    },
    {
        "loss": 2.422,
        "grad_norm": 1.89385986328125,
        "learning_rate": 3.227704761951645e-05,
        "epoch": 0.20677660396804948,
        "step": 1605
    },
    {
        "loss": 1.8018,
        "grad_norm": 2.603630781173706,
        "learning_rate": 3.2220181688427575e-05,
        "epoch": 0.20690543674310743,
        "step": 1606
    },
    {
        "loss": 2.1061,
        "grad_norm": 2.0286662578582764,
        "learning_rate": 3.216334207228013e-05,
        "epoch": 0.20703426951816542,
        "step": 1607
    },
    {
        "loss": 2.1657,
        "grad_norm": 2.0136053562164307,
        "learning_rate": 3.2106528855199346e-05,
        "epoch": 0.2071631022932234,
        "step": 1608
    },
    {
        "loss": 2.2935,
        "grad_norm": 1.7165184020996094,
        "learning_rate": 3.204974212127141e-05,
        "epoch": 0.20729193506828136,
        "step": 1609
    },
    {
        "loss": 2.0042,
        "grad_norm": 1.8456268310546875,
        "learning_rate": 3.199298195454322e-05,
        "epoch": 0.20742076784333935,
        "step": 1610
    },
    {
        "loss": 1.9176,
        "grad_norm": 2.2504069805145264,
        "learning_rate": 3.193624843902246e-05,
        "epoch": 0.20754960061839733,
        "step": 1611
    },
    {
        "loss": 1.9063,
        "grad_norm": 2.6978378295898438,
        "learning_rate": 3.187954165867727e-05,
        "epoch": 0.2076784333934553,
        "step": 1612
    },
    {
        "loss": 2.3473,
        "grad_norm": 1.9820164442062378,
        "learning_rate": 3.182286169743634e-05,
        "epoch": 0.20780726616851328,
        "step": 1613
    },
    {
        "loss": 2.5559,
        "grad_norm": 1.7750463485717773,
        "learning_rate": 3.1766208639188514e-05,
        "epoch": 0.20793609894357123,
        "step": 1614
    },
    {
        "loss": 2.5063,
        "grad_norm": 1.083397626876831,
        "learning_rate": 3.1709582567782965e-05,
        "epoch": 0.20806493171862922,
        "step": 1615
    },
    {
        "loss": 2.1466,
        "grad_norm": 1.2656697034835815,
        "learning_rate": 3.165298356702881e-05,
        "epoch": 0.2081937644936872,
        "step": 1616
    },
    {
        "loss": 2.0892,
        "grad_norm": 2.1784417629241943,
        "learning_rate": 3.159641172069518e-05,
        "epoch": 0.20832259726874516,
        "step": 1617
    },
    {
        "loss": 1.9851,
        "grad_norm": 2.306819200515747,
        "learning_rate": 3.153986711251098e-05,
        "epoch": 0.20845143004380315,
        "step": 1618
    },
    {
        "loss": 2.6175,
        "grad_norm": 1.9625002145767212,
        "learning_rate": 3.148334982616481e-05,
        "epoch": 0.2085802628188611,
        "step": 1619
    },
    {
        "loss": 2.3285,
        "grad_norm": 2.628065586090088,
        "learning_rate": 3.142685994530483e-05,
        "epoch": 0.2087090955939191,
        "step": 1620
    },
    {
        "loss": 2.089,
        "grad_norm": 2.23221492767334,
        "learning_rate": 3.1370397553538625e-05,
        "epoch": 0.20883792836897708,
        "step": 1621
    },
    {
        "loss": 2.1202,
        "grad_norm": 2.6339666843414307,
        "learning_rate": 3.131396273443314e-05,
        "epoch": 0.20896676114403503,
        "step": 1622
    },
    {
        "loss": 2.3579,
        "grad_norm": 1.8521499633789062,
        "learning_rate": 3.125755557151443e-05,
        "epoch": 0.20909559391909302,
        "step": 1623
    },
    {
        "loss": 1.9654,
        "grad_norm": 2.209484577178955,
        "learning_rate": 3.1201176148267706e-05,
        "epoch": 0.209224426694151,
        "step": 1624
    },
    {
        "loss": 2.2542,
        "grad_norm": 1.4030284881591797,
        "learning_rate": 3.1144824548137056e-05,
        "epoch": 0.20935325946920896,
        "step": 1625
    },
    {
        "loss": 2.4353,
        "grad_norm": 1.8401551246643066,
        "learning_rate": 3.1088500854525454e-05,
        "epoch": 0.20948209224426695,
        "step": 1626
    },
    {
        "loss": 1.6644,
        "grad_norm": 2.4677462577819824,
        "learning_rate": 3.103220515079449e-05,
        "epoch": 0.2096109250193249,
        "step": 1627
    },
    {
        "loss": 2.0677,
        "grad_norm": 3.580589532852173,
        "learning_rate": 3.097593752026441e-05,
        "epoch": 0.2097397577943829,
        "step": 1628
    },
    {
        "loss": 2.1666,
        "grad_norm": 2.6718430519104004,
        "learning_rate": 3.091969804621385e-05,
        "epoch": 0.20986859056944088,
        "step": 1629
    },
    {
        "loss": 2.3991,
        "grad_norm": 1.897290587425232,
        "learning_rate": 3.0863486811879794e-05,
        "epoch": 0.20999742334449883,
        "step": 1630
    },
    {
        "loss": 1.5745,
        "grad_norm": 2.939990282058716,
        "learning_rate": 3.080730390045745e-05,
        "epoch": 0.21012625611955682,
        "step": 1631
    },
    {
        "loss": 2.3845,
        "grad_norm": 1.3424644470214844,
        "learning_rate": 3.075114939510007e-05,
        "epoch": 0.21025508889461478,
        "step": 1632
    },
    {
        "loss": 2.5196,
        "grad_norm": 1.8035286664962769,
        "learning_rate": 3.069502337891886e-05,
        "epoch": 0.21038392166967276,
        "step": 1633
    },
    {
        "loss": 1.8243,
        "grad_norm": 1.7846674919128418,
        "learning_rate": 3.0638925934982944e-05,
        "epoch": 0.21051275444473075,
        "step": 1634
    },
    {
        "loss": 2.4509,
        "grad_norm": 2.1657509803771973,
        "learning_rate": 3.058285714631903e-05,
        "epoch": 0.2106415872197887,
        "step": 1635
    },
    {
        "loss": 2.2106,
        "grad_norm": 1.916843056678772,
        "learning_rate": 3.052681709591153e-05,
        "epoch": 0.2107704199948467,
        "step": 1636
    },
    {
        "loss": 2.3634,
        "grad_norm": 1.729792833328247,
        "learning_rate": 3.0470805866702245e-05,
        "epoch": 0.21089925276990468,
        "step": 1637
    },
    {
        "loss": 2.5122,
        "grad_norm": 1.6887547969818115,
        "learning_rate": 3.0414823541590364e-05,
        "epoch": 0.21102808554496263,
        "step": 1638
    },
    {
        "loss": 2.0762,
        "grad_norm": 1.193764328956604,
        "learning_rate": 3.0358870203432245e-05,
        "epoch": 0.21115691832002062,
        "step": 1639
    },
    {
        "loss": 2.0094,
        "grad_norm": 1.1374330520629883,
        "learning_rate": 3.0302945935041426e-05,
        "epoch": 0.21128575109507858,
        "step": 1640
    },
    {
        "loss": 1.8495,
        "grad_norm": 1.9550917148590088,
        "learning_rate": 3.0247050819188345e-05,
        "epoch": 0.21141458387013656,
        "step": 1641
    },
    {
        "loss": 2.2062,
        "grad_norm": 1.8858569860458374,
        "learning_rate": 3.0191184938600315e-05,
        "epoch": 0.21154341664519455,
        "step": 1642
    },
    {
        "loss": 2.4079,
        "grad_norm": 1.2188245058059692,
        "learning_rate": 3.0135348375961415e-05,
        "epoch": 0.2116722494202525,
        "step": 1643
    },
    {
        "loss": 2.521,
        "grad_norm": 1.040236473083496,
        "learning_rate": 3.0079541213912266e-05,
        "epoch": 0.2118010821953105,
        "step": 1644
    },
    {
        "loss": 2.1782,
        "grad_norm": 2.008807897567749,
        "learning_rate": 3.0023763535050053e-05,
        "epoch": 0.21192991497036845,
        "step": 1645
    },
    {
        "loss": 2.523,
        "grad_norm": 1.288956880569458,
        "learning_rate": 2.9968015421928232e-05,
        "epoch": 0.21205874774542643,
        "step": 1646
    },
    {
        "loss": 2.2444,
        "grad_norm": 2.046193838119507,
        "learning_rate": 2.991229695705662e-05,
        "epoch": 0.21218758052048442,
        "step": 1647
    },
    {
        "loss": 1.9208,
        "grad_norm": 2.017516613006592,
        "learning_rate": 2.9856608222901006e-05,
        "epoch": 0.21231641329554238,
        "step": 1648
    },
    {
        "loss": 1.6524,
        "grad_norm": 2.6895039081573486,
        "learning_rate": 2.9800949301883307e-05,
        "epoch": 0.21244524607060036,
        "step": 1649
    },
    {
        "loss": 1.788,
        "grad_norm": 1.9630433320999146,
        "learning_rate": 2.974532027638124e-05,
        "epoch": 0.21257407884565835,
        "step": 1650
    },
    {
        "loss": 2.3247,
        "grad_norm": 2.340754985809326,
        "learning_rate": 2.9689721228728317e-05,
        "epoch": 0.2127029116207163,
        "step": 1651
    },
    {
        "loss": 2.3009,
        "grad_norm": 2.4867289066314697,
        "learning_rate": 2.9634152241213652e-05,
        "epoch": 0.2128317443957743,
        "step": 1652
    },
    {
        "loss": 2.2478,
        "grad_norm": 1.7365350723266602,
        "learning_rate": 2.957861339608186e-05,
        "epoch": 0.21296057717083225,
        "step": 1653
    },
    {
        "loss": 2.4238,
        "grad_norm": 1.570802092552185,
        "learning_rate": 2.9523104775533005e-05,
        "epoch": 0.21308940994589023,
        "step": 1654
    },
    {
        "loss": 1.6707,
        "grad_norm": 2.4723050594329834,
        "learning_rate": 2.9467626461722332e-05,
        "epoch": 0.21321824272094822,
        "step": 1655
    },
    {
        "loss": 1.5668,
        "grad_norm": 2.3823788166046143,
        "learning_rate": 2.9412178536760326e-05,
        "epoch": 0.21334707549600618,
        "step": 1656
    },
    {
        "loss": 2.057,
        "grad_norm": 2.335751533508301,
        "learning_rate": 2.935676108271238e-05,
        "epoch": 0.21347590827106416,
        "step": 1657
    },
    {
        "loss": 2.7342,
        "grad_norm": 1.3794591426849365,
        "learning_rate": 2.9301374181598906e-05,
        "epoch": 0.21360474104612212,
        "step": 1658
    },
    {
        "loss": 2.1762,
        "grad_norm": 1.623279094696045,
        "learning_rate": 2.9246017915394995e-05,
        "epoch": 0.2137335738211801,
        "step": 1659
    },
    {
        "loss": 2.3965,
        "grad_norm": 2.3881003856658936,
        "learning_rate": 2.919069236603048e-05,
        "epoch": 0.2138624065962381,
        "step": 1660
    },
    {
        "loss": 2.0931,
        "grad_norm": 1.950352668762207,
        "learning_rate": 2.9135397615389674e-05,
        "epoch": 0.21399123937129605,
        "step": 1661
    },
    {
        "loss": 2.4022,
        "grad_norm": 1.6584322452545166,
        "learning_rate": 2.9080133745311354e-05,
        "epoch": 0.21412007214635403,
        "step": 1662
    },
    {
        "loss": 1.8006,
        "grad_norm": 2.442964792251587,
        "learning_rate": 2.9024900837588558e-05,
        "epoch": 0.21424890492141202,
        "step": 1663
    },
    {
        "loss": 2.1934,
        "grad_norm": 1.573364019393921,
        "learning_rate": 2.8969698973968474e-05,
        "epoch": 0.21437773769646998,
        "step": 1664
    },
    {
        "loss": 2.3055,
        "grad_norm": 1.5796844959259033,
        "learning_rate": 2.8914528236152422e-05,
        "epoch": 0.21450657047152796,
        "step": 1665
    },
    {
        "loss": 2.1009,
        "grad_norm": 1.7443915605545044,
        "learning_rate": 2.8859388705795588e-05,
        "epoch": 0.21463540324658592,
        "step": 1666
    },
    {
        "loss": 2.0682,
        "grad_norm": 2.05414080619812,
        "learning_rate": 2.880428046450699e-05,
        "epoch": 0.2147642360216439,
        "step": 1667
    },
    {
        "loss": 2.1948,
        "grad_norm": 2.2150421142578125,
        "learning_rate": 2.8749203593849318e-05,
        "epoch": 0.2148930687967019,
        "step": 1668
    },
    {
        "loss": 2.3827,
        "grad_norm": 2.02166485786438,
        "learning_rate": 2.8694158175338865e-05,
        "epoch": 0.21502190157175985,
        "step": 1669
    },
    {
        "loss": 2.356,
        "grad_norm": 2.1088740825653076,
        "learning_rate": 2.8639144290445363e-05,
        "epoch": 0.21515073434681783,
        "step": 1670
    },
    {
        "loss": 2.1574,
        "grad_norm": 1.7952712774276733,
        "learning_rate": 2.8584162020591892e-05,
        "epoch": 0.2152795671218758,
        "step": 1671
    },
    {
        "loss": 2.3851,
        "grad_norm": 1.6001051664352417,
        "learning_rate": 2.8529211447154698e-05,
        "epoch": 0.21540839989693378,
        "step": 1672
    },
    {
        "loss": 2.1367,
        "grad_norm": 2.9888715744018555,
        "learning_rate": 2.8474292651463107e-05,
        "epoch": 0.21553723267199176,
        "step": 1673
    },
    {
        "loss": 1.9483,
        "grad_norm": 2.0010313987731934,
        "learning_rate": 2.841940571479948e-05,
        "epoch": 0.21566606544704972,
        "step": 1674
    },
    {
        "loss": 2.4952,
        "grad_norm": 1.6119985580444336,
        "learning_rate": 2.8364550718398976e-05,
        "epoch": 0.2157948982221077,
        "step": 1675
    },
    {
        "loss": 2.603,
        "grad_norm": 1.8126108646392822,
        "learning_rate": 2.8309727743449456e-05,
        "epoch": 0.2159237309971657,
        "step": 1676
    },
    {
        "loss": 2.3589,
        "grad_norm": 1.5911223888397217,
        "learning_rate": 2.8254936871091463e-05,
        "epoch": 0.21605256377222365,
        "step": 1677
    },
    {
        "loss": 1.5608,
        "grad_norm": 2.5407371520996094,
        "learning_rate": 2.820017818241796e-05,
        "epoch": 0.21618139654728163,
        "step": 1678
    },
    {
        "loss": 2.2201,
        "grad_norm": 2.267186164855957,
        "learning_rate": 2.8145451758474307e-05,
        "epoch": 0.2163102293223396,
        "step": 1679
    },
    {
        "loss": 2.2704,
        "grad_norm": 1.325444221496582,
        "learning_rate": 2.8090757680258117e-05,
        "epoch": 0.21643906209739758,
        "step": 1680
    },
    {
        "loss": 2.3629,
        "grad_norm": 1.4845011234283447,
        "learning_rate": 2.8036096028719143e-05,
        "epoch": 0.21656789487245556,
        "step": 1681
    },
    {
        "loss": 2.3942,
        "grad_norm": 1.6732770204544067,
        "learning_rate": 2.7981466884759056e-05,
        "epoch": 0.21669672764751352,
        "step": 1682
    },
    {
        "loss": 2.2306,
        "grad_norm": 2.5405046939849854,
        "learning_rate": 2.7926870329231532e-05,
        "epoch": 0.2168255604225715,
        "step": 1683
    },
    {
        "loss": 2.6617,
        "grad_norm": 1.7214716672897339,
        "learning_rate": 2.787230644294194e-05,
        "epoch": 0.21695439319762946,
        "step": 1684
    },
    {
        "loss": 2.2407,
        "grad_norm": 1.7577123641967773,
        "learning_rate": 2.7817775306647297e-05,
        "epoch": 0.21708322597268745,
        "step": 1685
    },
    {
        "loss": 2.1556,
        "grad_norm": 1.3765056133270264,
        "learning_rate": 2.776327700105622e-05,
        "epoch": 0.21721205874774543,
        "step": 1686
    },
    {
        "loss": 1.9429,
        "grad_norm": 1.8303868770599365,
        "learning_rate": 2.770881160682864e-05,
        "epoch": 0.2173408915228034,
        "step": 1687
    },
    {
        "loss": 2.2869,
        "grad_norm": 1.6216288805007935,
        "learning_rate": 2.7654379204575855e-05,
        "epoch": 0.21746972429786138,
        "step": 1688
    },
    {
        "loss": 2.2113,
        "grad_norm": 1.3508529663085938,
        "learning_rate": 2.7599979874860273e-05,
        "epoch": 0.21759855707291936,
        "step": 1689
    },
    {
        "loss": 1.473,
        "grad_norm": 2.9417505264282227,
        "learning_rate": 2.7545613698195416e-05,
        "epoch": 0.21772738984797732,
        "step": 1690
    },
    {
        "loss": 1.6767,
        "grad_norm": 3.22602915763855,
        "learning_rate": 2.7491280755045658e-05,
        "epoch": 0.2178562226230353,
        "step": 1691
    },
    {
        "loss": 2.3379,
        "grad_norm": 1.8444780111312866,
        "learning_rate": 2.7436981125826276e-05,
        "epoch": 0.21798505539809326,
        "step": 1692
    },
    {
        "loss": 2.5572,
        "grad_norm": 1.643507957458496,
        "learning_rate": 2.7382714890903165e-05,
        "epoch": 0.21811388817315125,
        "step": 1693
    },
    {
        "loss": 1.6127,
        "grad_norm": 1.6085792779922485,
        "learning_rate": 2.7328482130592858e-05,
        "epoch": 0.21824272094820923,
        "step": 1694
    },
    {
        "loss": 2.0545,
        "grad_norm": 2.3908185958862305,
        "learning_rate": 2.7274282925162265e-05,
        "epoch": 0.2183715537232672,
        "step": 1695
    },
    {
        "loss": 2.1166,
        "grad_norm": 1.8174537420272827,
        "learning_rate": 2.722011735482873e-05,
        "epoch": 0.21850038649832518,
        "step": 1696
    },
    {
        "loss": 1.893,
        "grad_norm": 3.0946576595306396,
        "learning_rate": 2.7165985499759738e-05,
        "epoch": 0.21862921927338314,
        "step": 1697
    },
    {
        "loss": 2.1863,
        "grad_norm": 2.2001566886901855,
        "learning_rate": 2.7111887440072893e-05,
        "epoch": 0.21875805204844112,
        "step": 1698
    },
    {
        "loss": 2.1122,
        "grad_norm": 3.180572509765625,
        "learning_rate": 2.7057823255835822e-05,
        "epoch": 0.2188868848234991,
        "step": 1699
    },
    {
        "loss": 2.4473,
        "grad_norm": 1.588890552520752,
        "learning_rate": 2.7003793027065936e-05,
        "epoch": 0.21901571759855706,
        "step": 1700
    },
    {
        "loss": 1.8656,
        "grad_norm": 2.8154830932617188,
        "learning_rate": 2.694979683373049e-05,
        "epoch": 0.21914455037361505,
        "step": 1701
    },
    {
        "loss": 1.6406,
        "grad_norm": 2.082871198654175,
        "learning_rate": 2.689583475574626e-05,
        "epoch": 0.21927338314867303,
        "step": 1702
    },
    {
        "loss": 1.5328,
        "grad_norm": 2.3855130672454834,
        "learning_rate": 2.6841906872979617e-05,
        "epoch": 0.219402215923731,
        "step": 1703
    },
    {
        "loss": 2.1624,
        "grad_norm": 2.080291748046875,
        "learning_rate": 2.678801326524626e-05,
        "epoch": 0.21953104869878898,
        "step": 1704
    },
    {
        "loss": 1.4732,
        "grad_norm": 2.618537425994873,
        "learning_rate": 2.673415401231122e-05,
        "epoch": 0.21965988147384694,
        "step": 1705
    },
    {
        "loss": 1.904,
        "grad_norm": 1.8307093381881714,
        "learning_rate": 2.6680329193888615e-05,
        "epoch": 0.21978871424890492,
        "step": 1706
    },
    {
        "loss": 1.6354,
        "grad_norm": 2.840265989303589,
        "learning_rate": 2.6626538889641622e-05,
        "epoch": 0.2199175470239629,
        "step": 1707
    },
    {
        "loss": 2.3769,
        "grad_norm": 1.5565235614776611,
        "learning_rate": 2.657278317918236e-05,
        "epoch": 0.22004637979902086,
        "step": 1708
    },
    {
        "loss": 2.0941,
        "grad_norm": 2.698079824447632,
        "learning_rate": 2.651906214207175e-05,
        "epoch": 0.22017521257407885,
        "step": 1709
    },
    {
        "loss": 2.2846,
        "grad_norm": 2.0412304401397705,
        "learning_rate": 2.646537585781933e-05,
        "epoch": 0.2203040453491368,
        "step": 1710
    },
    {
        "loss": 2.1094,
        "grad_norm": 2.9273056983947754,
        "learning_rate": 2.6411724405883293e-05,
        "epoch": 0.2204328781241948,
        "step": 1711
    },
    {
        "loss": 1.7467,
        "grad_norm": 2.988163471221924,
        "learning_rate": 2.635810786567019e-05,
        "epoch": 0.22056171089925278,
        "step": 1712
    },
    {
        "loss": 2.0253,
        "grad_norm": 2.547722816467285,
        "learning_rate": 2.6304526316534996e-05,
        "epoch": 0.22069054367431074,
        "step": 1713
    },
    {
        "loss": 2.3601,
        "grad_norm": 1.5970350503921509,
        "learning_rate": 2.6250979837780802e-05,
        "epoch": 0.22081937644936872,
        "step": 1714
    },
    {
        "loss": 2.1933,
        "grad_norm": 2.6234700679779053,
        "learning_rate": 2.6197468508658864e-05,
        "epoch": 0.2209482092244267,
        "step": 1715
    },
    {
        "loss": 2.1861,
        "grad_norm": 1.7664189338684082,
        "learning_rate": 2.6143992408368344e-05,
        "epoch": 0.22107704199948466,
        "step": 1716
    },
    {
        "loss": 2.662,
        "grad_norm": 1.6689364910125732,
        "learning_rate": 2.609055161605637e-05,
        "epoch": 0.22120587477454265,
        "step": 1717
    },
    {
        "loss": 2.555,
        "grad_norm": 1.2088721990585327,
        "learning_rate": 2.6037146210817686e-05,
        "epoch": 0.2213347075496006,
        "step": 1718
    },
    {
        "loss": 2.2492,
        "grad_norm": 2.386441707611084,
        "learning_rate": 2.598377627169477e-05,
        "epoch": 0.2214635403246586,
        "step": 1719
    },
    {
        "loss": 2.2819,
        "grad_norm": 1.6677935123443604,
        "learning_rate": 2.593044187767757e-05,
        "epoch": 0.22159237309971658,
        "step": 1720
    },
    {
        "loss": 2.0368,
        "grad_norm": 2.193596839904785,
        "learning_rate": 2.5877143107703384e-05,
        "epoch": 0.22172120587477454,
        "step": 1721
    },
    {
        "loss": 2.7488,
        "grad_norm": 1.5389304161071777,
        "learning_rate": 2.5823880040656857e-05,
        "epoch": 0.22185003864983252,
        "step": 1722
    },
    {
        "loss": 2.5835,
        "grad_norm": 1.4975016117095947,
        "learning_rate": 2.5770652755369717e-05,
        "epoch": 0.2219788714248905,
        "step": 1723
    },
    {
        "loss": 1.3018,
        "grad_norm": 4.59869384765625,
        "learning_rate": 2.5717461330620834e-05,
        "epoch": 0.22210770419994846,
        "step": 1724
    },
    {
        "loss": 1.6155,
        "grad_norm": 2.8083906173706055,
        "learning_rate": 2.5664305845135862e-05,
        "epoch": 0.22223653697500645,
        "step": 1725
    },
    {
        "loss": 1.9632,
        "grad_norm": 1.8837908506393433,
        "learning_rate": 2.5611186377587404e-05,
        "epoch": 0.2223653697500644,
        "step": 1726
    },
    {
        "loss": 2.5116,
        "grad_norm": 1.771047592163086,
        "learning_rate": 2.5558103006594646e-05,
        "epoch": 0.2224942025251224,
        "step": 1727
    },
    {
        "loss": 2.4148,
        "grad_norm": 1.7901519536972046,
        "learning_rate": 2.550505581072342e-05,
        "epoch": 0.22262303530018038,
        "step": 1728
    },
    {
        "loss": 2.2902,
        "grad_norm": 1.7035642862319946,
        "learning_rate": 2.545204486848602e-05,
        "epoch": 0.22275186807523834,
        "step": 1729
    },
    {
        "loss": 1.8906,
        "grad_norm": 1.625496745109558,
        "learning_rate": 2.5399070258341007e-05,
        "epoch": 0.22288070085029632,
        "step": 1730
    },
    {
        "loss": 1.5056,
        "grad_norm": 2.9181158542633057,
        "learning_rate": 2.5346132058693263e-05,
        "epoch": 0.22300953362535428,
        "step": 1731
    },
    {
        "loss": 2.2859,
        "grad_norm": 1.8547099828720093,
        "learning_rate": 2.5293230347893693e-05,
        "epoch": 0.22313836640041226,
        "step": 1732
    },
    {
        "loss": 2.3591,
        "grad_norm": 2.0477073192596436,
        "learning_rate": 2.5240365204239302e-05,
        "epoch": 0.22326719917547025,
        "step": 1733
    },
    {
        "loss": 2.705,
        "grad_norm": 2.49522066116333,
        "learning_rate": 2.518753670597283e-05,
        "epoch": 0.2233960319505282,
        "step": 1734
    },
    {
        "loss": 1.8389,
        "grad_norm": 2.3060784339904785,
        "learning_rate": 2.513474493128293e-05,
        "epoch": 0.2235248647255862,
        "step": 1735
    },
    {
        "loss": 1.9755,
        "grad_norm": 1.5924627780914307,
        "learning_rate": 2.5081989958303787e-05,
        "epoch": 0.22365369750064418,
        "step": 1736
    },
    {
        "loss": 1.9493,
        "grad_norm": 3.0600123405456543,
        "learning_rate": 2.5029271865115185e-05,
        "epoch": 0.22378253027570214,
        "step": 1737
    },
    {
        "loss": 2.108,
        "grad_norm": 1.9153432846069336,
        "learning_rate": 2.4976590729742327e-05,
        "epoch": 0.22391136305076012,
        "step": 1738
    },
    {
        "loss": 2.1591,
        "grad_norm": 2.3391318321228027,
        "learning_rate": 2.492394663015566e-05,
        "epoch": 0.22404019582581808,
        "step": 1739
    },
    {
        "loss": 1.8276,
        "grad_norm": 2.0665791034698486,
        "learning_rate": 2.487133964427088e-05,
        "epoch": 0.22416902860087606,
        "step": 1740
    },
    {
        "loss": 2.3162,
        "grad_norm": 2.423131227493286,
        "learning_rate": 2.4818769849948688e-05,
        "epoch": 0.22429786137593405,
        "step": 1741
    },
    {
        "loss": 2.4302,
        "grad_norm": 1.7837369441986084,
        "learning_rate": 2.4766237324994817e-05,
        "epoch": 0.224426694150992,
        "step": 1742
    },
    {
        "loss": 1.9972,
        "grad_norm": 2.134855270385742,
        "learning_rate": 2.471374214715978e-05,
        "epoch": 0.22455552692605,
        "step": 1743
    },
    {
        "loss": 2.1785,
        "grad_norm": 1.9006755352020264,
        "learning_rate": 2.4661284394138833e-05,
        "epoch": 0.22468435970110795,
        "step": 1744
    },
    {
        "loss": 2.01,
        "grad_norm": 2.2804043292999268,
        "learning_rate": 2.4608864143571812e-05,
        "epoch": 0.22481319247616594,
        "step": 1745
    },
    {
        "loss": 2.1275,
        "grad_norm": 2.846123695373535,
        "learning_rate": 2.455648147304313e-05,
        "epoch": 0.22494202525122392,
        "step": 1746
    },
    {
        "loss": 2.4651,
        "grad_norm": 1.3413892984390259,
        "learning_rate": 2.450413646008148e-05,
        "epoch": 0.22507085802628188,
        "step": 1747
    },
    {
        "loss": 2.4249,
        "grad_norm": 1.6241395473480225,
        "learning_rate": 2.4451829182159885e-05,
        "epoch": 0.22519969080133986,
        "step": 1748
    },
    {
        "loss": 1.8618,
        "grad_norm": 3.1085124015808105,
        "learning_rate": 2.4399559716695526e-05,
        "epoch": 0.22532852357639785,
        "step": 1749
    },
    {
        "loss": 2.1995,
        "grad_norm": 1.3104066848754883,
        "learning_rate": 2.4347328141049546e-05,
        "epoch": 0.2254573563514558,
        "step": 1750
    },
    {
        "loss": 1.6707,
        "grad_norm": 2.7325870990753174,
        "learning_rate": 2.4295134532527107e-05,
        "epoch": 0.2255861891265138,
        "step": 1751
    },
    {
        "loss": 1.9306,
        "grad_norm": 2.380781650543213,
        "learning_rate": 2.42429789683771e-05,
        "epoch": 0.22571502190157175,
        "step": 1752
    },
    {
        "loss": 2.1989,
        "grad_norm": 1.3518882989883423,
        "learning_rate": 2.4190861525792123e-05,
        "epoch": 0.22584385467662974,
        "step": 1753
    },
    {
        "loss": 2.2201,
        "grad_norm": 1.9391255378723145,
        "learning_rate": 2.4138782281908405e-05,
        "epoch": 0.22597268745168772,
        "step": 1754
    },
    {
        "loss": 2.1574,
        "grad_norm": 1.120025634765625,
        "learning_rate": 2.408674131380556e-05,
        "epoch": 0.22610152022674568,
        "step": 1755
    },
    {
        "loss": 2.1118,
        "grad_norm": 2.2697229385375977,
        "learning_rate": 2.403473869850663e-05,
        "epoch": 0.22623035300180366,
        "step": 1756
    },
    {
        "loss": 1.6508,
        "grad_norm": 2.5013298988342285,
        "learning_rate": 2.398277451297782e-05,
        "epoch": 0.22635918577686162,
        "step": 1757
    },
    {
        "loss": 1.9428,
        "grad_norm": 1.83488929271698,
        "learning_rate": 2.3930848834128532e-05,
        "epoch": 0.2264880185519196,
        "step": 1758
    },
    {
        "loss": 2.1571,
        "grad_norm": 1.4021353721618652,
        "learning_rate": 2.3878961738811095e-05,
        "epoch": 0.2266168513269776,
        "step": 1759
    },
    {
        "loss": 2.3434,
        "grad_norm": 1.973894715309143,
        "learning_rate": 2.3827113303820818e-05,
        "epoch": 0.22674568410203555,
        "step": 1760
    },
    {
        "loss": 2.1092,
        "grad_norm": 2.066802978515625,
        "learning_rate": 2.3775303605895722e-05,
        "epoch": 0.22687451687709353,
        "step": 1761
    },
    {
        "loss": 1.3813,
        "grad_norm": 2.6040287017822266,
        "learning_rate": 2.372353272171651e-05,
        "epoch": 0.22700334965215152,
        "step": 1762
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.6266285181045532,
        "learning_rate": 2.3671800727906484e-05,
        "epoch": 0.22713218242720948,
        "step": 1763
    },
    {
        "loss": 2.0639,
        "grad_norm": 2.3221020698547363,
        "learning_rate": 2.362010770103131e-05,
        "epoch": 0.22726101520226746,
        "step": 1764
    },
    {
        "loss": 1.6066,
        "grad_norm": 2.5809571743011475,
        "learning_rate": 2.3568453717599077e-05,
        "epoch": 0.22738984797732542,
        "step": 1765
    },
    {
        "loss": 2.4649,
        "grad_norm": 1.5605274438858032,
        "learning_rate": 2.351683885405998e-05,
        "epoch": 0.2275186807523834,
        "step": 1766
    },
    {
        "loss": 2.2482,
        "grad_norm": 2.668799638748169,
        "learning_rate": 2.3465263186806417e-05,
        "epoch": 0.2276475135274414,
        "step": 1767
    },
    {
        "loss": 2.1813,
        "grad_norm": 1.8054405450820923,
        "learning_rate": 2.3413726792172687e-05,
        "epoch": 0.22777634630249935,
        "step": 1768
    },
    {
        "loss": 1.9997,
        "grad_norm": 2.4449400901794434,
        "learning_rate": 2.336222974643504e-05,
        "epoch": 0.22790517907755733,
        "step": 1769
    },
    {
        "loss": 2.1604,
        "grad_norm": 2.441497564315796,
        "learning_rate": 2.3310772125811403e-05,
        "epoch": 0.2280340118526153,
        "step": 1770
    },
    {
        "loss": 2.0558,
        "grad_norm": 2.2794382572174072,
        "learning_rate": 2.3259354006461452e-05,
        "epoch": 0.22816284462767328,
        "step": 1771
    },
    {
        "loss": 1.7253,
        "grad_norm": 1.7187645435333252,
        "learning_rate": 2.320797546448631e-05,
        "epoch": 0.22829167740273126,
        "step": 1772
    },
    {
        "loss": 2.3794,
        "grad_norm": 1.5444573163986206,
        "learning_rate": 2.3156636575928587e-05,
        "epoch": 0.22842051017778922,
        "step": 1773
    },
    {
        "loss": 1.8199,
        "grad_norm": 2.6103363037109375,
        "learning_rate": 2.3105337416772165e-05,
        "epoch": 0.2285493429528472,
        "step": 1774
    },
    {
        "loss": 1.859,
        "grad_norm": 2.5495731830596924,
        "learning_rate": 2.305407806294212e-05,
        "epoch": 0.2286781757279052,
        "step": 1775
    },
    {
        "loss": 2.3831,
        "grad_norm": 1.3606261014938354,
        "learning_rate": 2.3002858590304665e-05,
        "epoch": 0.22880700850296315,
        "step": 1776
    },
    {
        "loss": 2.2475,
        "grad_norm": 1.635087013244629,
        "learning_rate": 2.295167907466692e-05,
        "epoch": 0.22893584127802113,
        "step": 1777
    },
    {
        "loss": 2.3885,
        "grad_norm": 1.500576376914978,
        "learning_rate": 2.2900539591776938e-05,
        "epoch": 0.2290646740530791,
        "step": 1778
    },
    {
        "loss": 1.4268,
        "grad_norm": 2.683701515197754,
        "learning_rate": 2.2849440217323448e-05,
        "epoch": 0.22919350682813708,
        "step": 1779
    },
    {
        "loss": 2.0116,
        "grad_norm": 1.8708893060684204,
        "learning_rate": 2.2798381026935888e-05,
        "epoch": 0.22932233960319506,
        "step": 1780
    },
    {
        "loss": 1.9333,
        "grad_norm": 2.3758201599121094,
        "learning_rate": 2.274736209618415e-05,
        "epoch": 0.22945117237825302,
        "step": 1781
    },
    {
        "loss": 2.3124,
        "grad_norm": 1.9311022758483887,
        "learning_rate": 2.2696383500578615e-05,
        "epoch": 0.229580005153311,
        "step": 1782
    },
    {
        "loss": 1.9008,
        "grad_norm": 2.2249655723571777,
        "learning_rate": 2.2645445315569912e-05,
        "epoch": 0.22970883792836896,
        "step": 1783
    },
    {
        "loss": 2.2913,
        "grad_norm": 1.3086495399475098,
        "learning_rate": 2.259454761654885e-05,
        "epoch": 0.22983767070342695,
        "step": 1784
    },
    {
        "loss": 2.0988,
        "grad_norm": 2.3970389366149902,
        "learning_rate": 2.2543690478846392e-05,
        "epoch": 0.22996650347848493,
        "step": 1785
    },
    {
        "loss": 2.6054,
        "grad_norm": 1.2405204772949219,
        "learning_rate": 2.249287397773337e-05,
        "epoch": 0.2300953362535429,
        "step": 1786
    },
    {
        "loss": 2.3036,
        "grad_norm": 1.3681604862213135,
        "learning_rate": 2.2442098188420546e-05,
        "epoch": 0.23022416902860088,
        "step": 1787
    },
    {
        "loss": 1.6717,
        "grad_norm": 2.360167980194092,
        "learning_rate": 2.2391363186058423e-05,
        "epoch": 0.23035300180365886,
        "step": 1788
    },
    {
        "loss": 2.0907,
        "grad_norm": 2.524676561355591,
        "learning_rate": 2.23406690457371e-05,
        "epoch": 0.23048183457871682,
        "step": 1789
    },
    {
        "loss": 1.6354,
        "grad_norm": 2.9597184658050537,
        "learning_rate": 2.229001584248619e-05,
        "epoch": 0.2306106673537748,
        "step": 1790
    },
    {
        "loss": 2.2428,
        "grad_norm": 2.0908522605895996,
        "learning_rate": 2.223940365127478e-05,
        "epoch": 0.23073950012883276,
        "step": 1791
    },
    {
        "loss": 2.4691,
        "grad_norm": 2.01204776763916,
        "learning_rate": 2.218883254701121e-05,
        "epoch": 0.23086833290389075,
        "step": 1792
    },
    {
        "loss": 1.8843,
        "grad_norm": 1.9784903526306152,
        "learning_rate": 2.2138302604542986e-05,
        "epoch": 0.23099716567894873,
        "step": 1793
    },
    {
        "loss": 1.782,
        "grad_norm": 1.7118691205978394,
        "learning_rate": 2.2087813898656774e-05,
        "epoch": 0.2311259984540067,
        "step": 1794
    },
    {
        "loss": 2.1946,
        "grad_norm": 2.018019199371338,
        "learning_rate": 2.2037366504078123e-05,
        "epoch": 0.23125483122906468,
        "step": 1795
    },
    {
        "loss": 2.0473,
        "grad_norm": 1.6987745761871338,
        "learning_rate": 2.1986960495471508e-05,
        "epoch": 0.23138366400412264,
        "step": 1796
    },
    {
        "loss": 2.2292,
        "grad_norm": 1.8202816247940063,
        "learning_rate": 2.1936595947440086e-05,
        "epoch": 0.23151249677918062,
        "step": 1797
    },
    {
        "loss": 2.1848,
        "grad_norm": 1.2838021516799927,
        "learning_rate": 2.18862729345257e-05,
        "epoch": 0.2316413295542386,
        "step": 1798
    },
    {
        "loss": 1.9649,
        "grad_norm": 2.629162073135376,
        "learning_rate": 2.1835991531208715e-05,
        "epoch": 0.23177016232929656,
        "step": 1799
    },
    {
        "loss": 2.3904,
        "grad_norm": 1.9652098417282104,
        "learning_rate": 2.178575181190788e-05,
        "epoch": 0.23189899510435455,
        "step": 1800
    },
    {
        "loss": 2.4849,
        "grad_norm": 2.1198039054870605,
        "learning_rate": 2.1735553850980304e-05,
        "epoch": 0.23202782787941253,
        "step": 1801
    },
    {
        "loss": 1.7506,
        "grad_norm": 1.7304401397705078,
        "learning_rate": 2.1685397722721202e-05,
        "epoch": 0.2321566606544705,
        "step": 1802
    },
    {
        "loss": 1.8371,
        "grad_norm": 3.0449440479278564,
        "learning_rate": 2.1635283501363968e-05,
        "epoch": 0.23228549342952848,
        "step": 1803
    },
    {
        "loss": 2.0573,
        "grad_norm": 2.398540496826172,
        "learning_rate": 2.158521126107993e-05,
        "epoch": 0.23241432620458644,
        "step": 1804
    },
    {
        "loss": 2.5223,
        "grad_norm": 1.5806132555007935,
        "learning_rate": 2.1535181075978257e-05,
        "epoch": 0.23254315897964442,
        "step": 1805
    },
    {
        "loss": 2.2194,
        "grad_norm": 1.2215372323989868,
        "learning_rate": 2.1485193020105926e-05,
        "epoch": 0.2326719917547024,
        "step": 1806
    },
    {
        "loss": 2.1948,
        "grad_norm": 2.593205690383911,
        "learning_rate": 2.1435247167447537e-05,
        "epoch": 0.23280082452976036,
        "step": 1807
    },
    {
        "loss": 2.2704,
        "grad_norm": 1.8872696161270142,
        "learning_rate": 2.1385343591925246e-05,
        "epoch": 0.23292965730481835,
        "step": 1808
    },
    {
        "loss": 1.3363,
        "grad_norm": 2.3755247592926025,
        "learning_rate": 2.1335482367398586e-05,
        "epoch": 0.2330584900798763,
        "step": 1809
    },
    {
        "loss": 2.3303,
        "grad_norm": 1.901013970375061,
        "learning_rate": 2.1285663567664494e-05,
        "epoch": 0.2331873228549343,
        "step": 1810
    },
    {
        "loss": 2.4614,
        "grad_norm": 2.3255455493927,
        "learning_rate": 2.1235887266456987e-05,
        "epoch": 0.23331615562999228,
        "step": 1811
    },
    {
        "loss": 2.208,
        "grad_norm": 1.33414626121521,
        "learning_rate": 2.1186153537447323e-05,
        "epoch": 0.23344498840505024,
        "step": 1812
    },
    {
        "loss": 2.2539,
        "grad_norm": 1.3459248542785645,
        "learning_rate": 2.1136462454243643e-05,
        "epoch": 0.23357382118010822,
        "step": 1813
    },
    {
        "loss": 2.2766,
        "grad_norm": 2.2693231105804443,
        "learning_rate": 2.108681409039106e-05,
        "epoch": 0.2337026539551662,
        "step": 1814
    },
    {
        "loss": 2.0892,
        "grad_norm": 2.4453938007354736,
        "learning_rate": 2.103720851937137e-05,
        "epoch": 0.23383148673022416,
        "step": 1815
    },
    {
        "loss": 1.8875,
        "grad_norm": 3.1256706714630127,
        "learning_rate": 2.0987645814603104e-05,
        "epoch": 0.23396031950528215,
        "step": 1816
    },
    {
        "loss": 2.1228,
        "grad_norm": 2.6122055053710938,
        "learning_rate": 2.0938126049441337e-05,
        "epoch": 0.2340891522803401,
        "step": 1817
    },
    {
        "loss": 1.9187,
        "grad_norm": 2.294386386871338,
        "learning_rate": 2.088864929717754e-05,
        "epoch": 0.2342179850553981,
        "step": 1818
    },
    {
        "loss": 1.6417,
        "grad_norm": 2.779418468475342,
        "learning_rate": 2.083921563103961e-05,
        "epoch": 0.23434681783045608,
        "step": 1819
    },
    {
        "loss": 2.5719,
        "grad_norm": 1.4202319383621216,
        "learning_rate": 2.0789825124191592e-05,
        "epoch": 0.23447565060551404,
        "step": 1820
    },
    {
        "loss": 1.9682,
        "grad_norm": 1.7290034294128418,
        "learning_rate": 2.07404778497337e-05,
        "epoch": 0.23460448338057202,
        "step": 1821
    },
    {
        "loss": 1.5284,
        "grad_norm": 2.451119899749756,
        "learning_rate": 2.0691173880702126e-05,
        "epoch": 0.23473331615562998,
        "step": 1822
    },
    {
        "loss": 0.9062,
        "grad_norm": 2.6361193656921387,
        "learning_rate": 2.0641913290069027e-05,
        "epoch": 0.23486214893068796,
        "step": 1823
    },
    {
        "loss": 2.2727,
        "grad_norm": 1.6965733766555786,
        "learning_rate": 2.0592696150742285e-05,
        "epoch": 0.23499098170574595,
        "step": 1824
    },
    {
        "loss": 1.3334,
        "grad_norm": 3.504319190979004,
        "learning_rate": 2.0543522535565547e-05,
        "epoch": 0.2351198144808039,
        "step": 1825
    },
    {
        "loss": 1.9204,
        "grad_norm": 1.6610850095748901,
        "learning_rate": 2.049439251731797e-05,
        "epoch": 0.2352486472558619,
        "step": 1826
    },
    {
        "loss": 2.3577,
        "grad_norm": 1.6906578540802002,
        "learning_rate": 2.0445306168714234e-05,
        "epoch": 0.23537748003091988,
        "step": 1827
    },
    {
        "loss": 2.2775,
        "grad_norm": 2.0977044105529785,
        "learning_rate": 2.03962635624044e-05,
        "epoch": 0.23550631280597784,
        "step": 1828
    },
    {
        "loss": 1.5191,
        "grad_norm": 1.2247234582901,
        "learning_rate": 2.0347264770973728e-05,
        "epoch": 0.23563514558103582,
        "step": 1829
    },
    {
        "loss": 2.1589,
        "grad_norm": 1.978567361831665,
        "learning_rate": 2.0298309866942656e-05,
        "epoch": 0.23576397835609378,
        "step": 1830
    },
    {
        "loss": 1.8745,
        "grad_norm": 1.910929799079895,
        "learning_rate": 2.0249398922766706e-05,
        "epoch": 0.23589281113115176,
        "step": 1831
    },
    {
        "loss": 1.8283,
        "grad_norm": 2.1402041912078857,
        "learning_rate": 2.0200532010836264e-05,
        "epoch": 0.23602164390620975,
        "step": 1832
    },
    {
        "loss": 2.4844,
        "grad_norm": 1.7235528230667114,
        "learning_rate": 2.0151709203476626e-05,
        "epoch": 0.2361504766812677,
        "step": 1833
    },
    {
        "loss": 1.5112,
        "grad_norm": 2.9137918949127197,
        "learning_rate": 2.010293057294774e-05,
        "epoch": 0.2362793094563257,
        "step": 1834
    },
    {
        "loss": 1.6288,
        "grad_norm": 2.892253875732422,
        "learning_rate": 2.0054196191444236e-05,
        "epoch": 0.23640814223138368,
        "step": 1835
    },
    {
        "loss": 1.9306,
        "grad_norm": 2.364358425140381,
        "learning_rate": 2.000550613109518e-05,
        "epoch": 0.23653697500644164,
        "step": 1836
    },
    {
        "loss": 2.0394,
        "grad_norm": 2.2618796825408936,
        "learning_rate": 1.9956860463964122e-05,
        "epoch": 0.23666580778149962,
        "step": 1837
    },
    {
        "loss": 2.1085,
        "grad_norm": 2.666105270385742,
        "learning_rate": 1.9908259262048844e-05,
        "epoch": 0.23679464055655758,
        "step": 1838
    },
    {
        "loss": 2.4984,
        "grad_norm": 1.257541537284851,
        "learning_rate": 1.9859702597281327e-05,
        "epoch": 0.23692347333161556,
        "step": 1839
    },
    {
        "loss": 2.1408,
        "grad_norm": 2.1324498653411865,
        "learning_rate": 1.9811190541527676e-05,
        "epoch": 0.23705230610667355,
        "step": 1840
    },
    {
        "loss": 1.2405,
        "grad_norm": 2.877952814102173,
        "learning_rate": 1.9762723166587914e-05,
        "epoch": 0.2371811388817315,
        "step": 1841
    },
    {
        "loss": 2.13,
        "grad_norm": 1.3836761713027954,
        "learning_rate": 1.9714300544196e-05,
        "epoch": 0.2373099716567895,
        "step": 1842
    },
    {
        "loss": 1.7484,
        "grad_norm": 2.6562464237213135,
        "learning_rate": 1.9665922746019587e-05,
        "epoch": 0.23743880443184745,
        "step": 1843
    },
    {
        "loss": 2.4181,
        "grad_norm": 1.5912443399429321,
        "learning_rate": 1.9617589843660055e-05,
        "epoch": 0.23756763720690544,
        "step": 1844
    },
    {
        "loss": 2.1394,
        "grad_norm": 2.256338596343994,
        "learning_rate": 1.9569301908652272e-05,
        "epoch": 0.23769646998196342,
        "step": 1845
    },
    {
        "loss": 2.2997,
        "grad_norm": 1.7058554887771606,
        "learning_rate": 1.952105901246461e-05,
        "epoch": 0.23782530275702138,
        "step": 1846
    },
    {
        "loss": 2.3244,
        "grad_norm": 2.296105146408081,
        "learning_rate": 1.9472861226498722e-05,
        "epoch": 0.23795413553207936,
        "step": 1847
    },
    {
        "loss": 1.5628,
        "grad_norm": 3.197955846786499,
        "learning_rate": 1.9424708622089564e-05,
        "epoch": 0.23808296830713735,
        "step": 1848
    },
    {
        "loss": 1.7312,
        "grad_norm": 2.236668348312378,
        "learning_rate": 1.937660127050516e-05,
        "epoch": 0.2382118010821953,
        "step": 1849
    },
    {
        "loss": 2.3655,
        "grad_norm": 2.068091869354248,
        "learning_rate": 1.9328539242946596e-05,
        "epoch": 0.2383406338572533,
        "step": 1850
    },
    {
        "loss": 1.8502,
        "grad_norm": 2.5892021656036377,
        "learning_rate": 1.9280522610547857e-05,
        "epoch": 0.23846946663231125,
        "step": 1851
    },
    {
        "loss": 2.2115,
        "grad_norm": 1.9810566902160645,
        "learning_rate": 1.923255144437574e-05,
        "epoch": 0.23859829940736924,
        "step": 1852
    },
    {
        "loss": 1.6998,
        "grad_norm": 2.640413284301758,
        "learning_rate": 1.918462581542979e-05,
        "epoch": 0.23872713218242722,
        "step": 1853
    },
    {
        "loss": 1.9495,
        "grad_norm": 2.895786762237549,
        "learning_rate": 1.9136745794642068e-05,
        "epoch": 0.23885596495748518,
        "step": 1854
    },
    {
        "loss": 2.5756,
        "grad_norm": 1.5453789234161377,
        "learning_rate": 1.9088911452877224e-05,
        "epoch": 0.23898479773254316,
        "step": 1855
    },
    {
        "loss": 2.1024,
        "grad_norm": 1.5685930252075195,
        "learning_rate": 1.9041122860932246e-05,
        "epoch": 0.23911363050760112,
        "step": 1856
    },
    {
        "loss": 1.805,
        "grad_norm": 2.3936831951141357,
        "learning_rate": 1.8993380089536457e-05,
        "epoch": 0.2392424632826591,
        "step": 1857
    },
    {
        "loss": 1.3875,
        "grad_norm": 2.5300629138946533,
        "learning_rate": 1.8945683209351302e-05,
        "epoch": 0.2393712960577171,
        "step": 1858
    },
    {
        "loss": 2.546,
        "grad_norm": 1.149303913116455,
        "learning_rate": 1.8898032290970368e-05,
        "epoch": 0.23950012883277505,
        "step": 1859
    },
    {
        "loss": 2.2818,
        "grad_norm": 1.3477721214294434,
        "learning_rate": 1.885042740491917e-05,
        "epoch": 0.23962896160783304,
        "step": 1860
    },
    {
        "loss": 2.2695,
        "grad_norm": 1.2174216508865356,
        "learning_rate": 1.880286862165509e-05,
        "epoch": 0.23975779438289102,
        "step": 1861
    },
    {
        "loss": 1.9234,
        "grad_norm": 2.141061782836914,
        "learning_rate": 1.8755356011567332e-05,
        "epoch": 0.23988662715794898,
        "step": 1862
    },
    {
        "loss": 1.6372,
        "grad_norm": 2.3252151012420654,
        "learning_rate": 1.8707889644976705e-05,
        "epoch": 0.24001545993300696,
        "step": 1863
    },
    {
        "loss": 1.6691,
        "grad_norm": 1.6898672580718994,
        "learning_rate": 1.866046959213558e-05,
        "epoch": 0.24014429270806492,
        "step": 1864
    },
    {
        "loss": 1.5559,
        "grad_norm": 2.4676568508148193,
        "learning_rate": 1.8613095923227812e-05,
        "epoch": 0.2402731254831229,
        "step": 1865
    },
    {
        "loss": 2.245,
        "grad_norm": 2.5465309619903564,
        "learning_rate": 1.856576870836861e-05,
        "epoch": 0.2404019582581809,
        "step": 1866
    },
    {
        "loss": 2.4906,
        "grad_norm": 1.6989209651947021,
        "learning_rate": 1.851848801760437e-05,
        "epoch": 0.24053079103323885,
        "step": 1867
    },
    {
        "loss": 2.0217,
        "grad_norm": 2.1884398460388184,
        "learning_rate": 1.8471253920912708e-05,
        "epoch": 0.24065962380829684,
        "step": 1868
    },
    {
        "loss": 1.8636,
        "grad_norm": 2.43804931640625,
        "learning_rate": 1.8424066488202217e-05,
        "epoch": 0.2407884565833548,
        "step": 1869
    },
    {
        "loss": 2.2987,
        "grad_norm": 1.4028995037078857,
        "learning_rate": 1.837692578931243e-05,
        "epoch": 0.24091728935841278,
        "step": 1870
    },
    {
        "loss": 2.3158,
        "grad_norm": 1.4362313747406006,
        "learning_rate": 1.8329831894013782e-05,
        "epoch": 0.24104612213347076,
        "step": 1871
    },
    {
        "loss": 2.2579,
        "grad_norm": 1.1816197633743286,
        "learning_rate": 1.8282784872007348e-05,
        "epoch": 0.24117495490852872,
        "step": 1872
    },
    {
        "loss": 1.4012,
        "grad_norm": 3.127779006958008,
        "learning_rate": 1.8235784792924847e-05,
        "epoch": 0.2413037876835867,
        "step": 1873
    },
    {
        "loss": 1.8993,
        "grad_norm": 2.729365587234497,
        "learning_rate": 1.818883172632857e-05,
        "epoch": 0.2414326204586447,
        "step": 1874
    },
    {
        "loss": 1.9354,
        "grad_norm": 2.7445120811462402,
        "learning_rate": 1.8141925741711202e-05,
        "epoch": 0.24156145323370265,
        "step": 1875
    },
    {
        "loss": 1.4298,
        "grad_norm": 2.5653979778289795,
        "learning_rate": 1.8095066908495707e-05,
        "epoch": 0.24169028600876064,
        "step": 1876
    },
    {
        "loss": 2.0284,
        "grad_norm": 1.8443554639816284,
        "learning_rate": 1.804825529603531e-05,
        "epoch": 0.2418191187838186,
        "step": 1877
    },
    {
        "loss": 1.396,
        "grad_norm": 1.350549340248108,
        "learning_rate": 1.800149097361336e-05,
        "epoch": 0.24194795155887658,
        "step": 1878
    },
    {
        "loss": 2.3249,
        "grad_norm": 2.0514140129089355,
        "learning_rate": 1.7954774010443132e-05,
        "epoch": 0.24207678433393456,
        "step": 1879
    },
    {
        "loss": 2.3928,
        "grad_norm": 2.256671190261841,
        "learning_rate": 1.79081044756679e-05,
        "epoch": 0.24220561710899252,
        "step": 1880
    },
    {
        "loss": 2.1499,
        "grad_norm": 1.8503390550613403,
        "learning_rate": 1.786148243836068e-05,
        "epoch": 0.2423344498840505,
        "step": 1881
    },
    {
        "loss": 2.4852,
        "grad_norm": 2.122462749481201,
        "learning_rate": 1.7814907967524214e-05,
        "epoch": 0.24246328265910846,
        "step": 1882
    },
    {
        "loss": 2.0359,
        "grad_norm": 1.3633431196212769,
        "learning_rate": 1.776838113209086e-05,
        "epoch": 0.24259211543416645,
        "step": 1883
    },
    {
        "loss": 2.1331,
        "grad_norm": 2.5883333683013916,
        "learning_rate": 1.7721902000922425e-05,
        "epoch": 0.24272094820922444,
        "step": 1884
    },
    {
        "loss": 2.8532,
        "grad_norm": 1.5980359315872192,
        "learning_rate": 1.767547064281016e-05,
        "epoch": 0.2428497809842824,
        "step": 1885
    },
    {
        "loss": 2.3209,
        "grad_norm": 1.8167997598648071,
        "learning_rate": 1.762908712647459e-05,
        "epoch": 0.24297861375934038,
        "step": 1886
    },
    {
        "loss": 2.3109,
        "grad_norm": 1.9951695203781128,
        "learning_rate": 1.758275152056547e-05,
        "epoch": 0.24310744653439836,
        "step": 1887
    },
    {
        "loss": 2.1914,
        "grad_norm": 1.2660678625106812,
        "learning_rate": 1.7536463893661538e-05,
        "epoch": 0.24323627930945632,
        "step": 1888
    },
    {
        "loss": 2.4706,
        "grad_norm": 1.3948606252670288,
        "learning_rate": 1.7490224314270643e-05,
        "epoch": 0.2433651120845143,
        "step": 1889
    },
    {
        "loss": 1.9187,
        "grad_norm": 2.5221893787384033,
        "learning_rate": 1.744403285082943e-05,
        "epoch": 0.24349394485957226,
        "step": 1890
    },
    {
        "loss": 1.4767,
        "grad_norm": 2.8156349658966064,
        "learning_rate": 1.7397889571703412e-05,
        "epoch": 0.24362277763463025,
        "step": 1891
    },
    {
        "loss": 1.9607,
        "grad_norm": 2.2733850479125977,
        "learning_rate": 1.735179454518671e-05,
        "epoch": 0.24375161040968824,
        "step": 1892
    },
    {
        "loss": 1.985,
        "grad_norm": 2.2330193519592285,
        "learning_rate": 1.73057478395021e-05,
        "epoch": 0.2438804431847462,
        "step": 1893
    },
    {
        "loss": 2.1915,
        "grad_norm": 2.456965923309326,
        "learning_rate": 1.7259749522800767e-05,
        "epoch": 0.24400927595980418,
        "step": 1894
    },
    {
        "loss": 1.5336,
        "grad_norm": 2.803501605987549,
        "learning_rate": 1.7213799663162333e-05,
        "epoch": 0.24413810873486214,
        "step": 1895
    },
    {
        "loss": 2.1015,
        "grad_norm": 2.6493871212005615,
        "learning_rate": 1.7167898328594716e-05,
        "epoch": 0.24426694150992012,
        "step": 1896
    },
    {
        "loss": 2.407,
        "grad_norm": 1.6524765491485596,
        "learning_rate": 1.7122045587033924e-05,
        "epoch": 0.2443957742849781,
        "step": 1897
    },
    {
        "loss": 2.174,
        "grad_norm": 1.0765239000320435,
        "learning_rate": 1.707624150634415e-05,
        "epoch": 0.24452460706003606,
        "step": 1898
    },
    {
        "loss": 2.2878,
        "grad_norm": 1.427568793296814,
        "learning_rate": 1.703048615431748e-05,
        "epoch": 0.24465343983509405,
        "step": 1899
    },
    {
        "loss": 2.3246,
        "grad_norm": 2.4448401927948,
        "learning_rate": 1.6984779598673973e-05,
        "epoch": 0.24478227261015204,
        "step": 1900
    },
    {
        "loss": 2.577,
        "grad_norm": 1.5640888214111328,
        "learning_rate": 1.6939121907061362e-05,
        "epoch": 0.24491110538521,
        "step": 1901
    },
    {
        "loss": 2.4767,
        "grad_norm": 2.143709659576416,
        "learning_rate": 1.689351314705515e-05,
        "epoch": 0.24503993816026798,
        "step": 1902
    },
    {
        "loss": 2.1556,
        "grad_norm": 1.4522258043289185,
        "learning_rate": 1.684795338615837e-05,
        "epoch": 0.24516877093532594,
        "step": 1903
    },
    {
        "loss": 2.5422,
        "grad_norm": 1.124739408493042,
        "learning_rate": 1.6802442691801527e-05,
        "epoch": 0.24529760371038392,
        "step": 1904
    },
    {
        "loss": 2.3183,
        "grad_norm": 1.704469919204712,
        "learning_rate": 1.6756981131342535e-05,
        "epoch": 0.2454264364854419,
        "step": 1905
    },
    {
        "loss": 2.4202,
        "grad_norm": 2.3423311710357666,
        "learning_rate": 1.6711568772066595e-05,
        "epoch": 0.24555526926049986,
        "step": 1906
    },
    {
        "loss": 1.2195,
        "grad_norm": 3.2887916564941406,
        "learning_rate": 1.666620568118603e-05,
        "epoch": 0.24568410203555785,
        "step": 1907
    },
    {
        "loss": 1.6649,
        "grad_norm": 2.0664727687835693,
        "learning_rate": 1.662089192584032e-05,
        "epoch": 0.2458129348106158,
        "step": 1908
    },
    {
        "loss": 2.2908,
        "grad_norm": 1.8360482454299927,
        "learning_rate": 1.657562757309586e-05,
        "epoch": 0.2459417675856738,
        "step": 1909
    },
    {
        "loss": 2.2741,
        "grad_norm": 1.7879595756530762,
        "learning_rate": 1.6530412689945974e-05,
        "epoch": 0.24607060036073178,
        "step": 1910
    },
    {
        "loss": 1.7305,
        "grad_norm": 2.741666555404663,
        "learning_rate": 1.6485247343310746e-05,
        "epoch": 0.24619943313578974,
        "step": 1911
    },
    {
        "loss": 2.0288,
        "grad_norm": 1.4926549196243286,
        "learning_rate": 1.644013160003694e-05,
        "epoch": 0.24632826591084772,
        "step": 1912
    },
    {
        "loss": 2.6198,
        "grad_norm": 1.5282095670700073,
        "learning_rate": 1.63950655268979e-05,
        "epoch": 0.2464570986859057,
        "step": 1913
    },
    {
        "loss": 2.0898,
        "grad_norm": 1.8811962604522705,
        "learning_rate": 1.6350049190593487e-05,
        "epoch": 0.24658593146096366,
        "step": 1914
    },
    {
        "loss": 1.6532,
        "grad_norm": 2.0565760135650635,
        "learning_rate": 1.6305082657749942e-05,
        "epoch": 0.24671476423602165,
        "step": 1915
    },
    {
        "loss": 1.595,
        "grad_norm": 2.6914703845977783,
        "learning_rate": 1.626016599491975e-05,
        "epoch": 0.2468435970110796,
        "step": 1916
    },
    {
        "loss": 1.9657,
        "grad_norm": 1.7092876434326172,
        "learning_rate": 1.6215299268581657e-05,
        "epoch": 0.2469724297861376,
        "step": 1917
    },
    {
        "loss": 2.349,
        "grad_norm": 1.5786476135253906,
        "learning_rate": 1.6170482545140425e-05,
        "epoch": 0.24710126256119558,
        "step": 1918
    },
    {
        "loss": 2.2618,
        "grad_norm": 1.5128862857818604,
        "learning_rate": 1.612571589092688e-05,
        "epoch": 0.24723009533625354,
        "step": 1919
    },
    {
        "loss": 1.8031,
        "grad_norm": 2.3642373085021973,
        "learning_rate": 1.608099937219768e-05,
        "epoch": 0.24735892811131152,
        "step": 1920
    },
    {
        "loss": 1.974,
        "grad_norm": 2.637603521347046,
        "learning_rate": 1.603633305513536e-05,
        "epoch": 0.24748776088636948,
        "step": 1921
    },
    {
        "loss": 1.8574,
        "grad_norm": 2.70937180519104,
        "learning_rate": 1.5991717005848045e-05,
        "epoch": 0.24761659366142746,
        "step": 1922
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.4365299940109253,
        "learning_rate": 1.594715129036954e-05,
        "epoch": 0.24774542643648545,
        "step": 1923
    },
    {
        "loss": 1.5398,
        "grad_norm": 2.240901470184326,
        "learning_rate": 1.5902635974659164e-05,
        "epoch": 0.2478742592115434,
        "step": 1924
    },
    {
        "loss": 2.6541,
        "grad_norm": 1.639977216720581,
        "learning_rate": 1.5858171124601573e-05,
        "epoch": 0.2480030919866014,
        "step": 1925
    },
    {
        "loss": 2.103,
        "grad_norm": 2.1634511947631836,
        "learning_rate": 1.5813756806006792e-05,
        "epoch": 0.24813192476165938,
        "step": 1926
    },
    {
        "loss": 1.9999,
        "grad_norm": 2.326427459716797,
        "learning_rate": 1.5769393084610053e-05,
        "epoch": 0.24826075753671734,
        "step": 1927
    },
    {
        "loss": 2.3025,
        "grad_norm": 1.6144764423370361,
        "learning_rate": 1.5725080026071666e-05,
        "epoch": 0.24838959031177532,
        "step": 1928
    },
    {
        "loss": 1.8183,
        "grad_norm": 2.629093647003174,
        "learning_rate": 1.568081769597696e-05,
        "epoch": 0.24851842308683328,
        "step": 1929
    },
    {
        "loss": 1.9258,
        "grad_norm": 2.895249605178833,
        "learning_rate": 1.5636606159836264e-05,
        "epoch": 0.24864725586189126,
        "step": 1930
    },
    {
        "loss": 1.8661,
        "grad_norm": 2.7875616550445557,
        "learning_rate": 1.5592445483084584e-05,
        "epoch": 0.24877608863694925,
        "step": 1931
    },
    {
        "loss": 2.0806,
        "grad_norm": 2.157442569732666,
        "learning_rate": 1.5548335731081788e-05,
        "epoch": 0.2489049214120072,
        "step": 1932
    },
    {
        "loss": 1.9039,
        "grad_norm": 2.1651761531829834,
        "learning_rate": 1.550427696911229e-05,
        "epoch": 0.2490337541870652,
        "step": 1933
    },
    {
        "loss": 2.1568,
        "grad_norm": 2.4622085094451904,
        "learning_rate": 1.546026926238508e-05,
        "epoch": 0.24916258696212315,
        "step": 1934
    },
    {
        "loss": 2.3664,
        "grad_norm": 1.6908892393112183,
        "learning_rate": 1.5416312676033573e-05,
        "epoch": 0.24929141973718114,
        "step": 1935
    },
    {
        "loss": 2.2695,
        "grad_norm": 2.1698405742645264,
        "learning_rate": 1.537240727511553e-05,
        "epoch": 0.24942025251223912,
        "step": 1936
    },
    {
        "loss": 2.2819,
        "grad_norm": 2.3810884952545166,
        "learning_rate": 1.532855312461293e-05,
        "epoch": 0.24954908528729708,
        "step": 1937
    },
    {
        "loss": 1.9639,
        "grad_norm": 2.7371022701263428,
        "learning_rate": 1.528475028943191e-05,
        "epoch": 0.24967791806235506,
        "step": 1938
    },
    {
        "loss": 2.2498,
        "grad_norm": 1.4908854961395264,
        "learning_rate": 1.5240998834402681e-05,
        "epoch": 0.24980675083741305,
        "step": 1939
    },
    {
        "loss": 2.082,
        "grad_norm": 2.410872459411621,
        "learning_rate": 1.5197298824279376e-05,
        "epoch": 0.249935583612471,
        "step": 1940
    },
    {
        "loss": 1.7799,
        "grad_norm": 2.4320695400238037,
        "learning_rate": 1.5153650323739992e-05,
        "epoch": 0.25006441638752896,
        "step": 1941
    },
    {
        "loss": 1.8619,
        "grad_norm": 2.359513521194458,
        "learning_rate": 1.5110053397386332e-05,
        "epoch": 0.25019324916258695,
        "step": 1942
    },
    {
        "loss": 1.9856,
        "grad_norm": 1.990638256072998,
        "learning_rate": 1.5066508109743793e-05,
        "epoch": 0.25032208193764494,
        "step": 1943
    },
    {
        "loss": 2.348,
        "grad_norm": 1.3180512189865112,
        "learning_rate": 1.5023014525261404e-05,
        "epoch": 0.2504509147127029,
        "step": 1944
    },
    {
        "loss": 2.1438,
        "grad_norm": 2.3222460746765137,
        "learning_rate": 1.4979572708311673e-05,
        "epoch": 0.2505797474877609,
        "step": 1945
    },
    {
        "loss": 2.3802,
        "grad_norm": 1.9057178497314453,
        "learning_rate": 1.493618272319045e-05,
        "epoch": 0.25070858026281884,
        "step": 1946
    },
    {
        "loss": 2.1188,
        "grad_norm": 2.2921059131622314,
        "learning_rate": 1.4892844634116882e-05,
        "epoch": 0.2508374130378768,
        "step": 1947
    },
    {
        "loss": 2.7157,
        "grad_norm": 2.2991459369659424,
        "learning_rate": 1.4849558505233335e-05,
        "epoch": 0.2509662458129348,
        "step": 1948
    },
    {
        "loss": 2.4479,
        "grad_norm": 1.8914953470230103,
        "learning_rate": 1.4806324400605242e-05,
        "epoch": 0.2510950785879928,
        "step": 1949
    },
    {
        "loss": 2.4387,
        "grad_norm": 1.888666033744812,
        "learning_rate": 1.4763142384221036e-05,
        "epoch": 0.2512239113630508,
        "step": 1950
    },
    {
        "loss": 2.2098,
        "grad_norm": 1.934699535369873,
        "learning_rate": 1.4720012519992099e-05,
        "epoch": 0.2513527441381087,
        "step": 1951
    },
    {
        "loss": 2.2342,
        "grad_norm": 1.6224262714385986,
        "learning_rate": 1.4676934871752557e-05,
        "epoch": 0.2514815769131667,
        "step": 1952
    },
    {
        "loss": 2.1579,
        "grad_norm": 2.7016913890838623,
        "learning_rate": 1.463390950325934e-05,
        "epoch": 0.2516104096882247,
        "step": 1953
    },
    {
        "loss": 2.5886,
        "grad_norm": 1.3877569437026978,
        "learning_rate": 1.4590936478191908e-05,
        "epoch": 0.25173924246328266,
        "step": 1954
    },
    {
        "loss": 2.2482,
        "grad_norm": 2.5398805141448975,
        "learning_rate": 1.4548015860152337e-05,
        "epoch": 0.25186807523834065,
        "step": 1955
    },
    {
        "loss": 2.4929,
        "grad_norm": 1.9552394151687622,
        "learning_rate": 1.4505147712665058e-05,
        "epoch": 0.25199690801339863,
        "step": 1956
    },
    {
        "loss": 2.2512,
        "grad_norm": 1.2770215272903442,
        "learning_rate": 1.446233209917694e-05,
        "epoch": 0.25212574078845656,
        "step": 1957
    },
    {
        "loss": 2.0452,
        "grad_norm": 1.8176109790802002,
        "learning_rate": 1.4419569083056961e-05,
        "epoch": 0.25225457356351455,
        "step": 1958
    },
    {
        "loss": 2.2246,
        "grad_norm": 2.113147020339966,
        "learning_rate": 1.4376858727596404e-05,
        "epoch": 0.25238340633857254,
        "step": 1959
    },
    {
        "loss": 2.2382,
        "grad_norm": 2.2520432472229004,
        "learning_rate": 1.433420109600852e-05,
        "epoch": 0.2525122391136305,
        "step": 1960
    },
    {
        "loss": 2.3959,
        "grad_norm": 1.479546308517456,
        "learning_rate": 1.4291596251428547e-05,
        "epoch": 0.2526410718886885,
        "step": 1961
    },
    {
        "loss": 2.29,
        "grad_norm": 2.054669141769409,
        "learning_rate": 1.4249044256913579e-05,
        "epoch": 0.25276990466374644,
        "step": 1962
    },
    {
        "loss": 2.4152,
        "grad_norm": 2.31333589553833,
        "learning_rate": 1.4206545175442521e-05,
        "epoch": 0.2528987374388044,
        "step": 1963
    },
    {
        "loss": 2.7764,
        "grad_norm": 1.594774603843689,
        "learning_rate": 1.416409906991597e-05,
        "epoch": 0.2530275702138624,
        "step": 1964
    },
    {
        "loss": 2.3952,
        "grad_norm": 1.4755126237869263,
        "learning_rate": 1.412170600315606e-05,
        "epoch": 0.2531564029889204,
        "step": 1965
    },
    {
        "loss": 1.5009,
        "grad_norm": 2.4555792808532715,
        "learning_rate": 1.4079366037906483e-05,
        "epoch": 0.2532852357639784,
        "step": 1966
    },
    {
        "loss": 1.4592,
        "grad_norm": 2.898580551147461,
        "learning_rate": 1.4037079236832329e-05,
        "epoch": 0.2534140685390363,
        "step": 1967
    },
    {
        "loss": 1.657,
        "grad_norm": 2.7290656566619873,
        "learning_rate": 1.3994845662519984e-05,
        "epoch": 0.2535429013140943,
        "step": 1968
    },
    {
        "loss": 1.8028,
        "grad_norm": 3.1605727672576904,
        "learning_rate": 1.3952665377477048e-05,
        "epoch": 0.2536717340891523,
        "step": 1969
    },
    {
        "loss": 2.7434,
        "grad_norm": 2.3020644187927246,
        "learning_rate": 1.3910538444132248e-05,
        "epoch": 0.25380056686421026,
        "step": 1970
    },
    {
        "loss": 2.225,
        "grad_norm": 2.3540446758270264,
        "learning_rate": 1.3868464924835433e-05,
        "epoch": 0.25392939963926825,
        "step": 1971
    },
    {
        "loss": 2.4169,
        "grad_norm": 2.22823166847229,
        "learning_rate": 1.3826444881857259e-05,
        "epoch": 0.2540582324143262,
        "step": 1972
    },
    {
        "loss": 1.8592,
        "grad_norm": 2.6835131645202637,
        "learning_rate": 1.3784478377389348e-05,
        "epoch": 0.25418706518938416,
        "step": 1973
    },
    {
        "loss": 2.1605,
        "grad_norm": 1.9008253812789917,
        "learning_rate": 1.3742565473544005e-05,
        "epoch": 0.25431589796444215,
        "step": 1974
    },
    {
        "loss": 1.5238,
        "grad_norm": 3.454899787902832,
        "learning_rate": 1.370070623235426e-05,
        "epoch": 0.25444473073950014,
        "step": 1975
    },
    {
        "loss": 1.716,
        "grad_norm": 1.4285645484924316,
        "learning_rate": 1.3658900715773709e-05,
        "epoch": 0.2545735635145581,
        "step": 1976
    },
    {
        "loss": 2.2285,
        "grad_norm": 1.879995346069336,
        "learning_rate": 1.361714898567641e-05,
        "epoch": 0.25470239628961605,
        "step": 1977
    },
    {
        "loss": 2.1364,
        "grad_norm": 2.933605670928955,
        "learning_rate": 1.3575451103856824e-05,
        "epoch": 0.25483122906467404,
        "step": 1978
    },
    {
        "loss": 1.9192,
        "grad_norm": 2.568312644958496,
        "learning_rate": 1.3533807132029685e-05,
        "epoch": 0.254960061839732,
        "step": 1979
    },
    {
        "loss": 1.347,
        "grad_norm": 2.8591392040252686,
        "learning_rate": 1.3492217131830042e-05,
        "epoch": 0.25508889461479,
        "step": 1980
    },
    {
        "loss": 2.036,
        "grad_norm": 1.5587637424468994,
        "learning_rate": 1.3450681164812928e-05,
        "epoch": 0.255217727389848,
        "step": 1981
    },
    {
        "loss": 1.9932,
        "grad_norm": 2.0271291732788086,
        "learning_rate": 1.34091992924535e-05,
        "epoch": 0.255346560164906,
        "step": 1982
    },
    {
        "loss": 1.2255,
        "grad_norm": 3.261486530303955,
        "learning_rate": 1.3367771576146793e-05,
        "epoch": 0.2554753929399639,
        "step": 1983
    },
    {
        "loss": 2.1745,
        "grad_norm": 2.0438196659088135,
        "learning_rate": 1.332639807720773e-05,
        "epoch": 0.2556042257150219,
        "step": 1984
    },
    {
        "loss": 2.1292,
        "grad_norm": 1.7247614860534668,
        "learning_rate": 1.3285078856870992e-05,
        "epoch": 0.2557330584900799,
        "step": 1985
    },
    {
        "loss": 2.2716,
        "grad_norm": 2.333235740661621,
        "learning_rate": 1.3243813976290892e-05,
        "epoch": 0.25586189126513786,
        "step": 1986
    },
    {
        "loss": 1.6667,
        "grad_norm": 2.9704902172088623,
        "learning_rate": 1.3202603496541321e-05,
        "epoch": 0.25599072404019585,
        "step": 1987
    },
    {
        "loss": 2.3057,
        "grad_norm": 2.2085039615631104,
        "learning_rate": 1.316144747861568e-05,
        "epoch": 0.2561195568152538,
        "step": 1988
    },
    {
        "loss": 1.4836,
        "grad_norm": 3.1771278381347656,
        "learning_rate": 1.312034598342678e-05,
        "epoch": 0.25624838959031176,
        "step": 1989
    },
    {
        "loss": 2.1431,
        "grad_norm": 2.063004970550537,
        "learning_rate": 1.307929907180665e-05,
        "epoch": 0.25637722236536975,
        "step": 1990
    },
    {
        "loss": 2.2691,
        "grad_norm": 1.8342325687408447,
        "learning_rate": 1.3038306804506633e-05,
        "epoch": 0.25650605514042774,
        "step": 1991
    },
    {
        "loss": 2.0821,
        "grad_norm": 2.149176597595215,
        "learning_rate": 1.2997369242197127e-05,
        "epoch": 0.2566348879154857,
        "step": 1992
    },
    {
        "loss": 1.8119,
        "grad_norm": 2.199352264404297,
        "learning_rate": 1.2956486445467597e-05,
        "epoch": 0.25676372069054365,
        "step": 1993
    },
    {
        "loss": 2.4638,
        "grad_norm": 1.7557662725448608,
        "learning_rate": 1.291565847482647e-05,
        "epoch": 0.25689255346560164,
        "step": 1994
    },
    {
        "loss": 2.1904,
        "grad_norm": 2.4555108547210693,
        "learning_rate": 1.2874885390700986e-05,
        "epoch": 0.2570213862406596,
        "step": 1995
    },
    {
        "loss": 1.8057,
        "grad_norm": 1.027550220489502,
        "learning_rate": 1.2834167253437146e-05,
        "epoch": 0.2571502190157176,
        "step": 1996
    },
    {
        "loss": 2.1022,
        "grad_norm": 2.134542465209961,
        "learning_rate": 1.2793504123299665e-05,
        "epoch": 0.2572790517907756,
        "step": 1997
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.812063217163086,
        "learning_rate": 1.275289606047187e-05,
        "epoch": 0.2574078845658335,
        "step": 1998
    },
    {
        "loss": 1.7829,
        "grad_norm": 3.2541937828063965,
        "learning_rate": 1.2712343125055476e-05,
        "epoch": 0.2575367173408915,
        "step": 1999
    },
    {
        "loss": 1.835,
        "grad_norm": 1.686785340309143,
        "learning_rate": 1.2671845377070702e-05,
        "epoch": 0.2576655501159495,
        "step": 2000
    },
    {
        "loss": 2.1616,
        "grad_norm": 1.563197135925293,
        "learning_rate": 1.2631402876456083e-05,
        "epoch": 0.2577943828910075,
        "step": 2001
    },
    {
        "loss": 2.1531,
        "grad_norm": 2.0721933841705322,
        "learning_rate": 1.259101568306833e-05,
        "epoch": 0.25792321566606546,
        "step": 2002
    },
    {
        "loss": 1.7519,
        "grad_norm": 3.646177291870117,
        "learning_rate": 1.2550683856682365e-05,
        "epoch": 0.25805204844112345,
        "step": 2003
    },
    {
        "loss": 2.4847,
        "grad_norm": 1.246654987335205,
        "learning_rate": 1.2510407456991108e-05,
        "epoch": 0.2581808812161814,
        "step": 2004
    },
    {
        "loss": 2.2663,
        "grad_norm": 2.2916054725646973,
        "learning_rate": 1.247018654360545e-05,
        "epoch": 0.25830971399123936,
        "step": 2005
    },
    {
        "loss": 1.5368,
        "grad_norm": 3.2431271076202393,
        "learning_rate": 1.2430021176054196e-05,
        "epoch": 0.25843854676629735,
        "step": 2006
    },
    {
        "loss": 2.1033,
        "grad_norm": 1.449327826499939,
        "learning_rate": 1.238991141378395e-05,
        "epoch": 0.25856737954135534,
        "step": 2007
    },
    {
        "loss": 2.4829,
        "grad_norm": 1.7619506120681763,
        "learning_rate": 1.2349857316158924e-05,
        "epoch": 0.2586962123164133,
        "step": 2008
    },
    {
        "loss": 2.1842,
        "grad_norm": 1.5673518180847168,
        "learning_rate": 1.2309858942461045e-05,
        "epoch": 0.25882504509147125,
        "step": 2009
    },
    {
        "loss": 1.0533,
        "grad_norm": 3.2250900268554688,
        "learning_rate": 1.2269916351889738e-05,
        "epoch": 0.25895387786652924,
        "step": 2010
    },
    {
        "loss": 1.8725,
        "grad_norm": 3.204533815383911,
        "learning_rate": 1.2230029603561843e-05,
        "epoch": 0.2590827106415872,
        "step": 2011
    },
    {
        "loss": 1.6564,
        "grad_norm": 2.643460273742676,
        "learning_rate": 1.2190198756511556e-05,
        "epoch": 0.2592115434166452,
        "step": 2012
    },
    {
        "loss": 2.4752,
        "grad_norm": 2.364469528198242,
        "learning_rate": 1.2150423869690353e-05,
        "epoch": 0.2593403761917032,
        "step": 2013
    },
    {
        "loss": 2.1756,
        "grad_norm": 1.9500855207443237,
        "learning_rate": 1.2110705001966898e-05,
        "epoch": 0.2594692089667611,
        "step": 2014
    },
    {
        "loss": 1.9061,
        "grad_norm": 2.135930299758911,
        "learning_rate": 1.2071042212126898e-05,
        "epoch": 0.2595980417418191,
        "step": 2015
    },
    {
        "loss": 1.4667,
        "grad_norm": 2.874194383621216,
        "learning_rate": 1.2031435558873134e-05,
        "epoch": 0.2597268745168771,
        "step": 2016
    },
    {
        "loss": 2.5085,
        "grad_norm": 1.4053484201431274,
        "learning_rate": 1.1991885100825178e-05,
        "epoch": 0.2598557072919351,
        "step": 2017
    },
    {
        "loss": 2.4487,
        "grad_norm": 2.2388834953308105,
        "learning_rate": 1.1952390896519589e-05,
        "epoch": 0.25998454006699306,
        "step": 2018
    },
    {
        "loss": 2.1158,
        "grad_norm": 1.3444751501083374,
        "learning_rate": 1.1912953004409561e-05,
        "epoch": 0.260113372842051,
        "step": 2019
    },
    {
        "loss": 2.3925,
        "grad_norm": 2.174009084701538,
        "learning_rate": 1.1873571482864965e-05,
        "epoch": 0.260242205617109,
        "step": 2020
    },
    {
        "loss": 2.4019,
        "grad_norm": 2.084012508392334,
        "learning_rate": 1.1834246390172243e-05,
        "epoch": 0.26037103839216696,
        "step": 2021
    },
    {
        "loss": 0.9568,
        "grad_norm": 1.378798007965088,
        "learning_rate": 1.1794977784534333e-05,
        "epoch": 0.26049987116722495,
        "step": 2022
    },
    {
        "loss": 2.5582,
        "grad_norm": 1.8496651649475098,
        "learning_rate": 1.1755765724070578e-05,
        "epoch": 0.26062870394228294,
        "step": 2023
    },
    {
        "loss": 2.2023,
        "grad_norm": 1.2815073728561401,
        "learning_rate": 1.1716610266816586e-05,
        "epoch": 0.26075753671734087,
        "step": 2024
    },
    {
        "loss": 0.7349,
        "grad_norm": 2.277693510055542,
        "learning_rate": 1.1677511470724255e-05,
        "epoch": 0.26088636949239885,
        "step": 2025
    },
    {
        "loss": 2.0933,
        "grad_norm": 2.1630825996398926,
        "learning_rate": 1.1638469393661522e-05,
        "epoch": 0.26101520226745684,
        "step": 2026
    },
    {
        "loss": 1.614,
        "grad_norm": 3.176645517349243,
        "learning_rate": 1.159948409341251e-05,
        "epoch": 0.2611440350425148,
        "step": 2027
    },
    {
        "loss": 1.8728,
        "grad_norm": 2.496567964553833,
        "learning_rate": 1.1560555627677205e-05,
        "epoch": 0.2612728678175728,
        "step": 2028
    },
    {
        "loss": 2.0499,
        "grad_norm": 1.9958840608596802,
        "learning_rate": 1.1521684054071524e-05,
        "epoch": 0.2614017005926308,
        "step": 2029
    },
    {
        "loss": 2.1211,
        "grad_norm": 2.4279727935791016,
        "learning_rate": 1.1482869430127135e-05,
        "epoch": 0.2615305333676887,
        "step": 2030
    },
    {
        "loss": 1.6712,
        "grad_norm": 2.217541456222534,
        "learning_rate": 1.144411181329148e-05,
        "epoch": 0.2616593661427467,
        "step": 2031
    },
    {
        "loss": 2.575,
        "grad_norm": 1.4006494283676147,
        "learning_rate": 1.1405411260927613e-05,
        "epoch": 0.2617881989178047,
        "step": 2032
    },
    {
        "loss": 2.161,
        "grad_norm": 2.0337908267974854,
        "learning_rate": 1.1366767830314073e-05,
        "epoch": 0.2619170316928627,
        "step": 2033
    },
    {
        "loss": 2.1862,
        "grad_norm": 1.6247040033340454,
        "learning_rate": 1.1328181578644953e-05,
        "epoch": 0.26204586446792066,
        "step": 2034
    },
    {
        "loss": 1.9914,
        "grad_norm": 1.9748599529266357,
        "learning_rate": 1.1289652563029585e-05,
        "epoch": 0.2621746972429786,
        "step": 2035
    },
    {
        "loss": 1.8665,
        "grad_norm": 2.3138463497161865,
        "learning_rate": 1.1251180840492747e-05,
        "epoch": 0.2623035300180366,
        "step": 2036
    },
    {
        "loss": 1.1204,
        "grad_norm": 2.786668539047241,
        "learning_rate": 1.1212766467974312e-05,
        "epoch": 0.26243236279309456,
        "step": 2037
    },
    {
        "loss": 2.3413,
        "grad_norm": 1.5620840787887573,
        "learning_rate": 1.1174409502329297e-05,
        "epoch": 0.26256119556815255,
        "step": 2038
    },
    {
        "loss": 2.5065,
        "grad_norm": 1.762030839920044,
        "learning_rate": 1.1136110000327748e-05,
        "epoch": 0.26269002834321054,
        "step": 2039
    },
    {
        "loss": 2.0043,
        "grad_norm": 1.984954595565796,
        "learning_rate": 1.1097868018654695e-05,
        "epoch": 0.26281886111826847,
        "step": 2040
    },
    {
        "loss": 2.5931,
        "grad_norm": 1.5390604734420776,
        "learning_rate": 1.105968361391001e-05,
        "epoch": 0.26294769389332645,
        "step": 2041
    },
    {
        "loss": 2.2463,
        "grad_norm": 1.9295183420181274,
        "learning_rate": 1.1021556842608345e-05,
        "epoch": 0.26307652666838444,
        "step": 2042
    },
    {
        "loss": 2.1998,
        "grad_norm": 2.853355646133423,
        "learning_rate": 1.0983487761179057e-05,
        "epoch": 0.2632053594434424,
        "step": 2043
    },
    {
        "loss": 1.8159,
        "grad_norm": 2.242607355117798,
        "learning_rate": 1.0945476425966145e-05,
        "epoch": 0.2633341922185004,
        "step": 2044
    },
    {
        "loss": 2.2992,
        "grad_norm": 1.8606141805648804,
        "learning_rate": 1.09075228932281e-05,
        "epoch": 0.26346302499355834,
        "step": 2045
    },
    {
        "loss": 1.2797,
        "grad_norm": 2.889528274536133,
        "learning_rate": 1.0869627219137878e-05,
        "epoch": 0.2635918577686163,
        "step": 2046
    },
    {
        "loss": 2.3134,
        "grad_norm": 2.0623269081115723,
        "learning_rate": 1.0831789459782782e-05,
        "epoch": 0.2637206905436743,
        "step": 2047
    },
    {
        "loss": 2.1921,
        "grad_norm": 1.3290151357650757,
        "learning_rate": 1.0794009671164484e-05,
        "epoch": 0.2638495233187323,
        "step": 2048
    },
    {
        "loss": 1.9599,
        "grad_norm": 2.262803316116333,
        "learning_rate": 1.0756287909198732e-05,
        "epoch": 0.2639783560937903,
        "step": 2049
    },
    {
        "loss": 2.4224,
        "grad_norm": 2.0653891563415527,
        "learning_rate": 1.0718624229715484e-05,
        "epoch": 0.2641071888688482,
        "step": 2050
    },
    {
        "loss": 2.2902,
        "grad_norm": 1.797008752822876,
        "learning_rate": 1.0681018688458684e-05,
        "epoch": 0.2642360216439062,
        "step": 2051
    },
    {
        "loss": 2.294,
        "grad_norm": 2.45632266998291,
        "learning_rate": 1.0643471341086247e-05,
        "epoch": 0.2643648544189642,
        "step": 2052
    },
    {
        "loss": 1.9119,
        "grad_norm": 2.7515604496002197,
        "learning_rate": 1.0605982243169987e-05,
        "epoch": 0.26449368719402216,
        "step": 2053
    },
    {
        "loss": 1.8852,
        "grad_norm": 2.24953293800354,
        "learning_rate": 1.0568551450195457e-05,
        "epoch": 0.26462251996908015,
        "step": 2054
    },
    {
        "loss": 2.4378,
        "grad_norm": 1.775506615638733,
        "learning_rate": 1.0531179017561943e-05,
        "epoch": 0.26475135274413814,
        "step": 2055
    },
    {
        "loss": 1.9074,
        "grad_norm": 2.3572285175323486,
        "learning_rate": 1.0493865000582309e-05,
        "epoch": 0.26488018551919607,
        "step": 2056
    },
    {
        "loss": 2.2564,
        "grad_norm": 2.662273645401001,
        "learning_rate": 1.0456609454483079e-05,
        "epoch": 0.26500901829425405,
        "step": 2057
    },
    {
        "loss": 2.5191,
        "grad_norm": 2.1885688304901123,
        "learning_rate": 1.0419412434404091e-05,
        "epoch": 0.26513785106931204,
        "step": 2058
    },
    {
        "loss": 1.9197,
        "grad_norm": 2.0850722789764404,
        "learning_rate": 1.038227399539866e-05,
        "epoch": 0.26526668384437,
        "step": 2059
    },
    {
        "loss": 2.1519,
        "grad_norm": 1.4911305904388428,
        "learning_rate": 1.0345194192433355e-05,
        "epoch": 0.265395516619428,
        "step": 2060
    },
    {
        "loss": 1.8129,
        "grad_norm": 2.330580234527588,
        "learning_rate": 1.0308173080387996e-05,
        "epoch": 0.26552434939448594,
        "step": 2061
    },
    {
        "loss": 2.0899,
        "grad_norm": 1.635873556137085,
        "learning_rate": 1.0271210714055485e-05,
        "epoch": 0.2656531821695439,
        "step": 2062
    },
    {
        "loss": 2.2612,
        "grad_norm": 1.5672284364700317,
        "learning_rate": 1.0234307148141841e-05,
        "epoch": 0.2657820149446019,
        "step": 2063
    },
    {
        "loss": 2.4528,
        "grad_norm": 1.9299147129058838,
        "learning_rate": 1.0197462437266008e-05,
        "epoch": 0.2659108477196599,
        "step": 2064
    },
    {
        "loss": 1.9749,
        "grad_norm": 2.0638322830200195,
        "learning_rate": 1.0160676635959815e-05,
        "epoch": 0.2660396804947179,
        "step": 2065
    },
    {
        "loss": 2.1436,
        "grad_norm": 2.0833046436309814,
        "learning_rate": 1.0123949798667986e-05,
        "epoch": 0.2661685132697758,
        "step": 2066
    },
    {
        "loss": 1.4191,
        "grad_norm": 6.721426010131836,
        "learning_rate": 1.0087281979747849e-05,
        "epoch": 0.2662973460448338,
        "step": 2067
    },
    {
        "loss": 2.3217,
        "grad_norm": 2.0813820362091064,
        "learning_rate": 1.0050673233469498e-05,
        "epoch": 0.2664261788198918,
        "step": 2068
    },
    {
        "loss": 2.242,
        "grad_norm": 1.8590677976608276,
        "learning_rate": 1.001412361401548e-05,
        "epoch": 0.26655501159494976,
        "step": 2069
    },
    {
        "loss": 2.383,
        "grad_norm": 2.397092342376709,
        "learning_rate": 9.977633175480972e-06,
        "epoch": 0.26668384437000775,
        "step": 2070
    },
    {
        "loss": 2.2723,
        "grad_norm": 2.144900321960449,
        "learning_rate": 9.941201971873432e-06,
        "epoch": 0.2668126771450657,
        "step": 2071
    },
    {
        "loss": 2.7476,
        "grad_norm": 1.0216394662857056,
        "learning_rate": 9.90483005711274e-06,
        "epoch": 0.26694150992012367,
        "step": 2072
    },
    {
        "loss": 2.1131,
        "grad_norm": 2.39622163772583,
        "learning_rate": 9.86851748503096e-06,
        "epoch": 0.26707034269518165,
        "step": 2073
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.7737128734588623,
        "learning_rate": 9.832264309372364e-06,
        "epoch": 0.26719917547023964,
        "step": 2074
    },
    {
        "loss": 2.3018,
        "grad_norm": 1.2596054077148438,
        "learning_rate": 9.796070583793326e-06,
        "epoch": 0.2673280082452976,
        "step": 2075
    },
    {
        "loss": 2.6481,
        "grad_norm": 1.2219820022583008,
        "learning_rate": 9.759936361862172e-06,
        "epoch": 0.26745684102035555,
        "step": 2076
    },
    {
        "loss": 2.4166,
        "grad_norm": 1.6820026636123657,
        "learning_rate": 9.723861697059206e-06,
        "epoch": 0.26758567379541354,
        "step": 2077
    },
    {
        "loss": 2.5857,
        "grad_norm": 1.2054201364517212,
        "learning_rate": 9.687846642776593e-06,
        "epoch": 0.2677145065704715,
        "step": 2078
    },
    {
        "loss": 2.3422,
        "grad_norm": 1.9401710033416748,
        "learning_rate": 9.651891252318252e-06,
        "epoch": 0.2678433393455295,
        "step": 2079
    },
    {
        "loss": 2.2721,
        "grad_norm": 1.3761818408966064,
        "learning_rate": 9.615995578899777e-06,
        "epoch": 0.2679721721205875,
        "step": 2080
    },
    {
        "loss": 2.3176,
        "grad_norm": 2.66056489944458,
        "learning_rate": 9.58015967564843e-06,
        "epoch": 0.2681010048956455,
        "step": 2081
    },
    {
        "loss": 2.0922,
        "grad_norm": 2.851875066757202,
        "learning_rate": 9.544383595602957e-06,
        "epoch": 0.2682298376707034,
        "step": 2082
    },
    {
        "loss": 2.4236,
        "grad_norm": 1.4394358396530151,
        "learning_rate": 9.508667391713599e-06,
        "epoch": 0.2683586704457614,
        "step": 2083
    },
    {
        "loss": 1.8848,
        "grad_norm": 1.8527019023895264,
        "learning_rate": 9.473011116841984e-06,
        "epoch": 0.2684875032208194,
        "step": 2084
    },
    {
        "loss": 2.2856,
        "grad_norm": 1.85495126247406,
        "learning_rate": 9.437414823760982e-06,
        "epoch": 0.26861633599587736,
        "step": 2085
    },
    {
        "loss": 2.3094,
        "grad_norm": 1.3865143060684204,
        "learning_rate": 9.401878565154754e-06,
        "epoch": 0.26874516877093535,
        "step": 2086
    },
    {
        "loss": 2.3733,
        "grad_norm": 2.0181825160980225,
        "learning_rate": 9.36640239361859e-06,
        "epoch": 0.2688740015459933,
        "step": 2087
    },
    {
        "loss": 2.381,
        "grad_norm": 1.1514945030212402,
        "learning_rate": 9.330986361658827e-06,
        "epoch": 0.26900283432105127,
        "step": 2088
    },
    {
        "loss": 1.5527,
        "grad_norm": 2.5807528495788574,
        "learning_rate": 9.2956305216928e-06,
        "epoch": 0.26913166709610925,
        "step": 2089
    },
    {
        "loss": 2.635,
        "grad_norm": 1.1156177520751953,
        "learning_rate": 9.260334926048775e-06,
        "epoch": 0.26926049987116724,
        "step": 2090
    },
    {
        "loss": 1.9805,
        "grad_norm": 1.8890700340270996,
        "learning_rate": 9.225099626965849e-06,
        "epoch": 0.2693893326462252,
        "step": 2091
    },
    {
        "loss": 2.6197,
        "grad_norm": 1.920149326324463,
        "learning_rate": 9.189924676593852e-06,
        "epoch": 0.26951816542128315,
        "step": 2092
    },
    {
        "loss": 1.9855,
        "grad_norm": 2.5136380195617676,
        "learning_rate": 9.154810126993352e-06,
        "epoch": 0.26964699819634114,
        "step": 2093
    },
    {
        "loss": 1.9311,
        "grad_norm": 2.1262757778167725,
        "learning_rate": 9.119756030135413e-06,
        "epoch": 0.2697758309713991,
        "step": 2094
    },
    {
        "loss": 2.4518,
        "grad_norm": 1.775896430015564,
        "learning_rate": 9.084762437901767e-06,
        "epoch": 0.2699046637464571,
        "step": 2095
    },
    {
        "loss": 1.956,
        "grad_norm": 2.0016181468963623,
        "learning_rate": 9.049829402084492e-06,
        "epoch": 0.2700334965215151,
        "step": 2096
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.9247127771377563,
        "learning_rate": 9.01495697438608e-06,
        "epoch": 0.270162329296573,
        "step": 2097
    },
    {
        "loss": 2.1126,
        "grad_norm": 1.8333584070205688,
        "learning_rate": 8.98014520641931e-06,
        "epoch": 0.270291162071631,
        "step": 2098
    },
    {
        "loss": 2.4071,
        "grad_norm": 2.2732646465301514,
        "learning_rate": 8.945394149707159e-06,
        "epoch": 0.270419994846689,
        "step": 2099
    },
    {
        "loss": 2.3635,
        "grad_norm": 1.4649828672409058,
        "learning_rate": 8.910703855682828e-06,
        "epoch": 0.270548827621747,
        "step": 2100
    },
    {
        "loss": 2.2498,
        "grad_norm": 1.4223438501358032,
        "learning_rate": 8.876074375689497e-06,
        "epoch": 0.27067766039680496,
        "step": 2101
    },
    {
        "loss": 1.5162,
        "grad_norm": 2.9547512531280518,
        "learning_rate": 8.841505760980406e-06,
        "epoch": 0.2708064931718629,
        "step": 2102
    },
    {
        "loss": 1.8629,
        "grad_norm": 2.941481113433838,
        "learning_rate": 8.806998062718624e-06,
        "epoch": 0.2709353259469209,
        "step": 2103
    },
    {
        "loss": 1.9197,
        "grad_norm": 2.837631940841675,
        "learning_rate": 8.772551331977175e-06,
        "epoch": 0.27106415872197887,
        "step": 2104
    },
    {
        "loss": 1.9848,
        "grad_norm": 2.2200253009796143,
        "learning_rate": 8.738165619738769e-06,
        "epoch": 0.27119299149703685,
        "step": 2105
    },
    {
        "loss": 1.7991,
        "grad_norm": 2.856851816177368,
        "learning_rate": 8.703840976895828e-06,
        "epoch": 0.27132182427209484,
        "step": 2106
    },
    {
        "loss": 1.9321,
        "grad_norm": 2.441075086593628,
        "learning_rate": 8.669577454250366e-06,
        "epoch": 0.2714506570471528,
        "step": 2107
    },
    {
        "loss": 2.3837,
        "grad_norm": 1.9075533151626587,
        "learning_rate": 8.635375102513982e-06,
        "epoch": 0.27157948982221075,
        "step": 2108
    },
    {
        "loss": 1.7316,
        "grad_norm": 5.304205894470215,
        "learning_rate": 8.601233972307726e-06,
        "epoch": 0.27170832259726874,
        "step": 2109
    },
    {
        "loss": 1.6815,
        "grad_norm": 2.5692596435546875,
        "learning_rate": 8.567154114161995e-06,
        "epoch": 0.2718371553723267,
        "step": 2110
    },
    {
        "loss": 2.2121,
        "grad_norm": 2.595733642578125,
        "learning_rate": 8.533135578516576e-06,
        "epoch": 0.2719659881473847,
        "step": 2111
    },
    {
        "loss": 2.4955,
        "grad_norm": 1.8922767639160156,
        "learning_rate": 8.499178415720387e-06,
        "epoch": 0.2720948209224427,
        "step": 2112
    },
    {
        "loss": 2.0462,
        "grad_norm": 1.2412428855895996,
        "learning_rate": 8.465282676031645e-06,
        "epoch": 0.2722236536975006,
        "step": 2113
    },
    {
        "loss": 2.296,
        "grad_norm": 1.3427437543869019,
        "learning_rate": 8.431448409617554e-06,
        "epoch": 0.2723524864725586,
        "step": 2114
    },
    {
        "loss": 2.4026,
        "grad_norm": 2.066599130630493,
        "learning_rate": 8.397675666554378e-06,
        "epoch": 0.2724813192476166,
        "step": 2115
    },
    {
        "loss": 2.2972,
        "grad_norm": 2.4115889072418213,
        "learning_rate": 8.363964496827299e-06,
        "epoch": 0.2726101520226746,
        "step": 2116
    },
    {
        "loss": 2.4068,
        "grad_norm": 1.9244059324264526,
        "learning_rate": 8.330314950330403e-06,
        "epoch": 0.27273898479773256,
        "step": 2117
    },
    {
        "loss": 2.2303,
        "grad_norm": 1.8422825336456299,
        "learning_rate": 8.296727076866573e-06,
        "epoch": 0.2728678175727905,
        "step": 2118
    },
    {
        "loss": 1.9039,
        "grad_norm": 2.0421335697174072,
        "learning_rate": 8.263200926147341e-06,
        "epoch": 0.2729966503478485,
        "step": 2119
    },
    {
        "loss": 2.0921,
        "grad_norm": 2.382993698120117,
        "learning_rate": 8.229736547792965e-06,
        "epoch": 0.27312548312290647,
        "step": 2120
    },
    {
        "loss": 1.9693,
        "grad_norm": 1.175775408744812,
        "learning_rate": 8.196333991332255e-06,
        "epoch": 0.27325431589796445,
        "step": 2121
    },
    {
        "loss": 2.6427,
        "grad_norm": 1.7963842153549194,
        "learning_rate": 8.162993306202526e-06,
        "epoch": 0.27338314867302244,
        "step": 2122
    },
    {
        "loss": 2.0281,
        "grad_norm": 2.434117078781128,
        "learning_rate": 8.129714541749494e-06,
        "epoch": 0.27351198144808037,
        "step": 2123
    },
    {
        "loss": 1.6876,
        "grad_norm": 2.439406156539917,
        "learning_rate": 8.096497747227233e-06,
        "epoch": 0.27364081422313835,
        "step": 2124
    },
    {
        "loss": 2.4579,
        "grad_norm": 1.5802115201950073,
        "learning_rate": 8.06334297179816e-06,
        "epoch": 0.27376964699819634,
        "step": 2125
    },
    {
        "loss": 2.6174,
        "grad_norm": 1.766526699066162,
        "learning_rate": 8.030250264532807e-06,
        "epoch": 0.2738984797732543,
        "step": 2126
    },
    {
        "loss": 2.2011,
        "grad_norm": 1.1119955778121948,
        "learning_rate": 7.997219674409929e-06,
        "epoch": 0.2740273125483123,
        "step": 2127
    },
    {
        "loss": 2.2628,
        "grad_norm": 3.296135663986206,
        "learning_rate": 7.964251250316258e-06,
        "epoch": 0.2741561453233703,
        "step": 2128
    },
    {
        "loss": 1.6379,
        "grad_norm": 2.3509318828582764,
        "learning_rate": 7.93134504104659e-06,
        "epoch": 0.2742849780984282,
        "step": 2129
    },
    {
        "loss": 1.8317,
        "grad_norm": 2.636446475982666,
        "learning_rate": 7.898501095303612e-06,
        "epoch": 0.2744138108734862,
        "step": 2130
    },
    {
        "loss": 2.5071,
        "grad_norm": 1.780455470085144,
        "learning_rate": 7.865719461697873e-06,
        "epoch": 0.2745426436485442,
        "step": 2131
    },
    {
        "loss": 1.7692,
        "grad_norm": 2.6697893142700195,
        "learning_rate": 7.83300018874768e-06,
        "epoch": 0.2746714764236022,
        "step": 2132
    },
    {
        "loss": 2.7201,
        "grad_norm": 1.5303723812103271,
        "learning_rate": 7.80034332487901e-06,
        "epoch": 0.27480030919866016,
        "step": 2133
    },
    {
        "loss": 2.1349,
        "grad_norm": 2.156628131866455,
        "learning_rate": 7.767748918425583e-06,
        "epoch": 0.2749291419737181,
        "step": 2134
    },
    {
        "loss": 1.7942,
        "grad_norm": 2.458038806915283,
        "learning_rate": 7.735217017628543e-06,
        "epoch": 0.2750579747487761,
        "step": 2135
    },
    {
        "loss": 1.6405,
        "grad_norm": 2.3121871948242188,
        "learning_rate": 7.702747670636629e-06,
        "epoch": 0.27518680752383406,
        "step": 2136
    },
    {
        "loss": 1.171,
        "grad_norm": 3.364658832550049,
        "learning_rate": 7.670340925505897e-06,
        "epoch": 0.27531564029889205,
        "step": 2137
    },
    {
        "loss": 1.0551,
        "grad_norm": 2.8699004650115967,
        "learning_rate": 7.637996830199868e-06,
        "epoch": 0.27544447307395004,
        "step": 2138
    },
    {
        "loss": 1.3654,
        "grad_norm": 1.8432098627090454,
        "learning_rate": 7.605715432589255e-06,
        "epoch": 0.27557330584900797,
        "step": 2139
    },
    {
        "loss": 2.3062,
        "grad_norm": 1.7220250368118286,
        "learning_rate": 7.573496780451978e-06,
        "epoch": 0.27570213862406595,
        "step": 2140
    },
    {
        "loss": 2.2966,
        "grad_norm": 2.021589517593384,
        "learning_rate": 7.541340921473145e-06,
        "epoch": 0.27583097139912394,
        "step": 2141
    },
    {
        "loss": 1.6187,
        "grad_norm": 2.212416887283325,
        "learning_rate": 7.509247903244848e-06,
        "epoch": 0.2759598041741819,
        "step": 2142
    },
    {
        "loss": 2.2777,
        "grad_norm": 1.472559928894043,
        "learning_rate": 7.477217773266282e-06,
        "epoch": 0.2760886369492399,
        "step": 2143
    },
    {
        "loss": 2.3678,
        "grad_norm": 1.2870515584945679,
        "learning_rate": 7.445250578943436e-06,
        "epoch": 0.27621746972429784,
        "step": 2144
    },
    {
        "loss": 2.2587,
        "grad_norm": 1.8390296697616577,
        "learning_rate": 7.413346367589263e-06,
        "epoch": 0.2763463024993558,
        "step": 2145
    },
    {
        "loss": 2.0435,
        "grad_norm": 2.1208126544952393,
        "learning_rate": 7.381505186423393e-06,
        "epoch": 0.2764751352744138,
        "step": 2146
    },
    {
        "loss": 1.9749,
        "grad_norm": 2.305962562561035,
        "learning_rate": 7.349727082572283e-06,
        "epoch": 0.2766039680494718,
        "step": 2147
    },
    {
        "loss": 2.0446,
        "grad_norm": 2.023430585861206,
        "learning_rate": 7.318012103068955e-06,
        "epoch": 0.2767328008245298,
        "step": 2148
    },
    {
        "loss": 2.2236,
        "grad_norm": 2.1555488109588623,
        "learning_rate": 7.286360294853001e-06,
        "epoch": 0.2768616335995877,
        "step": 2149
    },
    {
        "loss": 1.2034,
        "grad_norm": 3.0933640003204346,
        "learning_rate": 7.254771704770563e-06,
        "epoch": 0.2769904663746457,
        "step": 2150
    },
    {
        "loss": 2.2173,
        "grad_norm": 2.1791718006134033,
        "learning_rate": 7.223246379574195e-06,
        "epoch": 0.2771192991497037,
        "step": 2151
    },
    {
        "loss": 1.4265,
        "grad_norm": 1.759881854057312,
        "learning_rate": 7.1917843659228266e-06,
        "epoch": 0.27724813192476166,
        "step": 2152
    },
    {
        "loss": 2.3135,
        "grad_norm": 1.469355821609497,
        "learning_rate": 7.160385710381634e-06,
        "epoch": 0.27737696469981965,
        "step": 2153
    },
    {
        "loss": 1.9114,
        "grad_norm": 2.4975147247314453,
        "learning_rate": 7.129050459422082e-06,
        "epoch": 0.27750579747487764,
        "step": 2154
    },
    {
        "loss": 1.3495,
        "grad_norm": 2.4042341709136963,
        "learning_rate": 7.097778659421777e-06,
        "epoch": 0.27763463024993557,
        "step": 2155
    },
    {
        "loss": 2.2685,
        "grad_norm": 1.7017751932144165,
        "learning_rate": 7.066570356664404e-06,
        "epoch": 0.27776346302499355,
        "step": 2156
    },
    {
        "loss": 2.2907,
        "grad_norm": 2.121406316757202,
        "learning_rate": 7.035425597339668e-06,
        "epoch": 0.27789229580005154,
        "step": 2157
    },
    {
        "loss": 1.685,
        "grad_norm": 2.084911584854126,
        "learning_rate": 7.004344427543219e-06,
        "epoch": 0.2780211285751095,
        "step": 2158
    },
    {
        "loss": 2.2886,
        "grad_norm": 2.2165961265563965,
        "learning_rate": 6.973326893276621e-06,
        "epoch": 0.2781499613501675,
        "step": 2159
    },
    {
        "loss": 2.4481,
        "grad_norm": 1.3332005739212036,
        "learning_rate": 6.942373040447236e-06,
        "epoch": 0.27827879412522544,
        "step": 2160
    },
    {
        "loss": 2.0446,
        "grad_norm": 2.10451602935791,
        "learning_rate": 6.911482914868201e-06,
        "epoch": 0.2784076269002834,
        "step": 2161
    },
    {
        "loss": 2.2083,
        "grad_norm": 1.7558423280715942,
        "learning_rate": 6.8806565622582585e-06,
        "epoch": 0.2785364596753414,
        "step": 2162
    },
    {
        "loss": 2.4215,
        "grad_norm": 1.4929633140563965,
        "learning_rate": 6.849894028241849e-06,
        "epoch": 0.2786652924503994,
        "step": 2163
    },
    {
        "loss": 2.155,
        "grad_norm": 1.7568196058273315,
        "learning_rate": 6.819195358348929e-06,
        "epoch": 0.2787941252254574,
        "step": 2164
    },
    {
        "loss": 2.1222,
        "grad_norm": 2.441697597503662,
        "learning_rate": 6.788560598014921e-06,
        "epoch": 0.2789229580005153,
        "step": 2165
    },
    {
        "loss": 1.9341,
        "grad_norm": 2.6839563846588135,
        "learning_rate": 6.75798979258066e-06,
        "epoch": 0.2790517907755733,
        "step": 2166
    },
    {
        "loss": 2.4916,
        "grad_norm": 1.3041226863861084,
        "learning_rate": 6.727482987292316e-06,
        "epoch": 0.2791806235506313,
        "step": 2167
    },
    {
        "loss": 1.3882,
        "grad_norm": 2.6169137954711914,
        "learning_rate": 6.697040227301405e-06,
        "epoch": 0.27930945632568926,
        "step": 2168
    },
    {
        "loss": 1.7349,
        "grad_norm": 2.492230176925659,
        "learning_rate": 6.666661557664544e-06,
        "epoch": 0.27943828910074725,
        "step": 2169
    },
    {
        "loss": 2.1648,
        "grad_norm": 1.8872289657592773,
        "learning_rate": 6.636347023343581e-06,
        "epoch": 0.2795671218758052,
        "step": 2170
    },
    {
        "loss": 2.5901,
        "grad_norm": 1.8963629007339478,
        "learning_rate": 6.606096669205381e-06,
        "epoch": 0.27969595465086317,
        "step": 2171
    },
    {
        "loss": 1.9048,
        "grad_norm": 2.7475855350494385,
        "learning_rate": 6.5759105400218676e-06,
        "epoch": 0.27982478742592115,
        "step": 2172
    },
    {
        "loss": 1.9895,
        "grad_norm": 2.430654764175415,
        "learning_rate": 6.545788680469895e-06,
        "epoch": 0.27995362020097914,
        "step": 2173
    },
    {
        "loss": 1.8751,
        "grad_norm": 2.7752468585968018,
        "learning_rate": 6.515731135131176e-06,
        "epoch": 0.2800824529760371,
        "step": 2174
    },
    {
        "loss": 2.2417,
        "grad_norm": 1.6261261701583862,
        "learning_rate": 6.4857379484922486e-06,
        "epoch": 0.28021128575109505,
        "step": 2175
    },
    {
        "loss": 2.2016,
        "grad_norm": 2.1225204467773438,
        "learning_rate": 6.455809164944376e-06,
        "epoch": 0.28034011852615304,
        "step": 2176
    },
    {
        "loss": 1.9771,
        "grad_norm": 2.3372480869293213,
        "learning_rate": 6.42594482878357e-06,
        "epoch": 0.280468951301211,
        "step": 2177
    },
    {
        "loss": 1.862,
        "grad_norm": 2.932938575744629,
        "learning_rate": 6.396144984210362e-06,
        "epoch": 0.280597784076269,
        "step": 2178
    },
    {
        "loss": 2.3806,
        "grad_norm": 1.5249245166778564,
        "learning_rate": 6.366409675329909e-06,
        "epoch": 0.280726616851327,
        "step": 2179
    },
    {
        "loss": 1.9835,
        "grad_norm": 2.311321258544922,
        "learning_rate": 6.336738946151794e-06,
        "epoch": 0.280855449626385,
        "step": 2180
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.7487256526947021,
        "learning_rate": 6.307132840590091e-06,
        "epoch": 0.2809842824014429,
        "step": 2181
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.2767133712768555,
        "learning_rate": 6.2775914024631565e-06,
        "epoch": 0.2811131151765009,
        "step": 2182
    },
    {
        "loss": 2.0363,
        "grad_norm": 1.9948021173477173,
        "learning_rate": 6.248114675493677e-06,
        "epoch": 0.2812419479515589,
        "step": 2183
    },
    {
        "loss": 1.9552,
        "grad_norm": 1.8399121761322021,
        "learning_rate": 6.218702703308532e-06,
        "epoch": 0.28137078072661686,
        "step": 2184
    },
    {
        "loss": 2.3422,
        "grad_norm": 2.036480665206909,
        "learning_rate": 6.189355529438795e-06,
        "epoch": 0.28149961350167485,
        "step": 2185
    },
    {
        "loss": 2.0979,
        "grad_norm": 2.1495068073272705,
        "learning_rate": 6.160073197319644e-06,
        "epoch": 0.2816284462767328,
        "step": 2186
    },
    {
        "loss": 1.6108,
        "grad_norm": 2.24111008644104,
        "learning_rate": 6.130855750290215e-06,
        "epoch": 0.28175727905179077,
        "step": 2187
    },
    {
        "loss": 1.6408,
        "grad_norm": 3.023946523666382,
        "learning_rate": 6.101703231593703e-06,
        "epoch": 0.28188611182684875,
        "step": 2188
    },
    {
        "loss": 2.2441,
        "grad_norm": 1.5190225839614868,
        "learning_rate": 6.072615684377142e-06,
        "epoch": 0.28201494460190674,
        "step": 2189
    },
    {
        "loss": 1.8198,
        "grad_norm": 1.8137505054473877,
        "learning_rate": 6.043593151691435e-06,
        "epoch": 0.2821437773769647,
        "step": 2190
    },
    {
        "loss": 2.4914,
        "grad_norm": 1.9117882251739502,
        "learning_rate": 6.014635676491259e-06,
        "epoch": 0.28227261015202265,
        "step": 2191
    },
    {
        "loss": 2.2744,
        "grad_norm": 1.5692641735076904,
        "learning_rate": 5.985743301634994e-06,
        "epoch": 0.28240144292708064,
        "step": 2192
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.3444817066192627,
        "learning_rate": 5.95691606988466e-06,
        "epoch": 0.2825302757021386,
        "step": 2193
    },
    {
        "loss": 2.1285,
        "grad_norm": 2.104483127593994,
        "learning_rate": 5.92815402390588e-06,
        "epoch": 0.2826591084771966,
        "step": 2194
    },
    {
        "loss": 1.519,
        "grad_norm": 2.7436559200286865,
        "learning_rate": 5.8994572062678166e-06,
        "epoch": 0.2827879412522546,
        "step": 2195
    },
    {
        "loss": 1.2518,
        "grad_norm": 2.9747695922851562,
        "learning_rate": 5.87082565944303e-06,
        "epoch": 0.2829167740273125,
        "step": 2196
    },
    {
        "loss": 2.1942,
        "grad_norm": 2.9415152072906494,
        "learning_rate": 5.842259425807528e-06,
        "epoch": 0.2830456068023705,
        "step": 2197
    },
    {
        "loss": 1.7006,
        "grad_norm": 2.222229242324829,
        "learning_rate": 5.813758547640652e-06,
        "epoch": 0.2831744395774285,
        "step": 2198
    },
    {
        "loss": 2.1358,
        "grad_norm": 1.963236927986145,
        "learning_rate": 5.785323067124982e-06,
        "epoch": 0.2833032723524865,
        "step": 2199
    },
    {
        "loss": 1.4969,
        "grad_norm": 2.467163562774658,
        "learning_rate": 5.756953026346346e-06,
        "epoch": 0.28343210512754446,
        "step": 2200
    },
    {
        "loss": 1.9304,
        "grad_norm": 2.411557674407959,
        "learning_rate": 5.728648467293662e-06,
        "epoch": 0.2835609379026024,
        "step": 2201
    },
    {
        "loss": 2.316,
        "grad_norm": 2.241886854171753,
        "learning_rate": 5.7004094318590215e-06,
        "epoch": 0.2836897706776604,
        "step": 2202
    },
    {
        "loss": 2.127,
        "grad_norm": 1.9068046808242798,
        "learning_rate": 5.67223596183743e-06,
        "epoch": 0.28381860345271837,
        "step": 2203
    },
    {
        "loss": 1.6279,
        "grad_norm": 3.2963433265686035,
        "learning_rate": 5.644128098926954e-06,
        "epoch": 0.28394743622777635,
        "step": 2204
    },
    {
        "loss": 1.4372,
        "grad_norm": 3.5324878692626953,
        "learning_rate": 5.616085884728467e-06,
        "epoch": 0.28407626900283434,
        "step": 2205
    },
    {
        "loss": 2.0415,
        "grad_norm": 2.5039453506469727,
        "learning_rate": 5.588109360745741e-06,
        "epoch": 0.2842051017778923,
        "step": 2206
    },
    {
        "loss": 1.04,
        "grad_norm": 2.7899277210235596,
        "learning_rate": 5.560198568385322e-06,
        "epoch": 0.28433393455295025,
        "step": 2207
    },
    {
        "loss": 2.076,
        "grad_norm": 1.938624620437622,
        "learning_rate": 5.532353548956437e-06,
        "epoch": 0.28446276732800824,
        "step": 2208
    },
    {
        "loss": 2.0704,
        "grad_norm": 3.130783796310425,
        "learning_rate": 5.504574343670999e-06,
        "epoch": 0.2845916001030662,
        "step": 2209
    },
    {
        "loss": 2.4873,
        "grad_norm": 1.4824210405349731,
        "learning_rate": 5.4768609936434734e-06,
        "epoch": 0.2847204328781242,
        "step": 2210
    },
    {
        "loss": 2.378,
        "grad_norm": 1.6086229085922241,
        "learning_rate": 5.449213539890913e-06,
        "epoch": 0.2848492656531822,
        "step": 2211
    },
    {
        "loss": 1.649,
        "grad_norm": 2.029228925704956,
        "learning_rate": 5.421632023332779e-06,
        "epoch": 0.2849780984282401,
        "step": 2212
    },
    {
        "loss": 2.3227,
        "grad_norm": 1.8952996730804443,
        "learning_rate": 5.394116484791012e-06,
        "epoch": 0.2851069312032981,
        "step": 2213
    },
    {
        "loss": 1.6919,
        "grad_norm": 2.062105894088745,
        "learning_rate": 5.366666964989803e-06,
        "epoch": 0.2852357639783561,
        "step": 2214
    },
    {
        "loss": 2.092,
        "grad_norm": 2.4493823051452637,
        "learning_rate": 5.339283504555753e-06,
        "epoch": 0.2853645967534141,
        "step": 2215
    },
    {
        "loss": 1.9109,
        "grad_norm": 2.1315419673919678,
        "learning_rate": 5.3119661440176e-06,
        "epoch": 0.28549342952847206,
        "step": 2216
    },
    {
        "loss": 2.2709,
        "grad_norm": 1.6160808801651,
        "learning_rate": 5.2847149238062835e-06,
        "epoch": 0.28562226230353,
        "step": 2217
    },
    {
        "loss": 2.202,
        "grad_norm": 1.6461645364761353,
        "learning_rate": 5.257529884254869e-06,
        "epoch": 0.285751095078588,
        "step": 2218
    },
    {
        "loss": 1.0781,
        "grad_norm": 2.6979472637176514,
        "learning_rate": 5.2304110655984285e-06,
        "epoch": 0.28587992785364597,
        "step": 2219
    },
    {
        "loss": 1.6981,
        "grad_norm": 3.4360313415527344,
        "learning_rate": 5.203358507974071e-06,
        "epoch": 0.28600876062870395,
        "step": 2220
    },
    {
        "loss": 1.8526,
        "grad_norm": 2.195979118347168,
        "learning_rate": 5.176372251420797e-06,
        "epoch": 0.28613759340376194,
        "step": 2221
    },
    {
        "loss": 2.5519,
        "grad_norm": 1.512359857559204,
        "learning_rate": 5.1494523358795285e-06,
        "epoch": 0.28626642617881987,
        "step": 2222
    },
    {
        "loss": 1.7466,
        "grad_norm": 1.9980061054229736,
        "learning_rate": 5.122598801192919e-06,
        "epoch": 0.28639525895387785,
        "step": 2223
    },
    {
        "loss": 1.8493,
        "grad_norm": 2.3509163856506348,
        "learning_rate": 5.095811687105462e-06,
        "epoch": 0.28652409172893584,
        "step": 2224
    },
    {
        "loss": 2.063,
        "grad_norm": 2.684061288833618,
        "learning_rate": 5.069091033263295e-06,
        "epoch": 0.2866529245039938,
        "step": 2225
    },
    {
        "loss": 2.545,
        "grad_norm": 1.1229443550109863,
        "learning_rate": 5.042436879214208e-06,
        "epoch": 0.2867817572790518,
        "step": 2226
    },
    {
        "loss": 2.3433,
        "grad_norm": 1.5022495985031128,
        "learning_rate": 5.015849264407546e-06,
        "epoch": 0.2869105900541098,
        "step": 2227
    },
    {
        "loss": 2.4595,
        "grad_norm": 1.6437053680419922,
        "learning_rate": 4.989328228194201e-06,
        "epoch": 0.2870394228291677,
        "step": 2228
    },
    {
        "loss": 2.1698,
        "grad_norm": 1.4821735620498657,
        "learning_rate": 4.9628738098265345e-06,
        "epoch": 0.2871682556042257,
        "step": 2229
    },
    {
        "loss": 1.3938,
        "grad_norm": 2.911353588104248,
        "learning_rate": 4.936486048458261e-06,
        "epoch": 0.2872970883792837,
        "step": 2230
    },
    {
        "loss": 2.1017,
        "grad_norm": 2.740715503692627,
        "learning_rate": 4.910164983144483e-06,
        "epoch": 0.2874259211543417,
        "step": 2231
    },
    {
        "loss": 1.4098,
        "grad_norm": 1.6575952768325806,
        "learning_rate": 4.883910652841594e-06,
        "epoch": 0.28755475392939966,
        "step": 2232
    },
    {
        "loss": 1.9357,
        "grad_norm": 1.5078006982803345,
        "learning_rate": 4.857723096407174e-06,
        "epoch": 0.2876835867044576,
        "step": 2233
    },
    {
        "loss": 1.8845,
        "grad_norm": 2.5156197547912598,
        "learning_rate": 4.8316023526000175e-06,
        "epoch": 0.2878124194795156,
        "step": 2234
    },
    {
        "loss": 2.4565,
        "grad_norm": 1.3557441234588623,
        "learning_rate": 4.805548460080001e-06,
        "epoch": 0.28794125225457357,
        "step": 2235
    },
    {
        "loss": 2.2397,
        "grad_norm": 1.8773938417434692,
        "learning_rate": 4.779561457408077e-06,
        "epoch": 0.28807008502963155,
        "step": 2236
    },
    {
        "loss": 2.0773,
        "grad_norm": 2.659273624420166,
        "learning_rate": 4.7536413830461925e-06,
        "epoch": 0.28819891780468954,
        "step": 2237
    },
    {
        "loss": 2.3207,
        "grad_norm": 2.3810575008392334,
        "learning_rate": 4.727788275357248e-06,
        "epoch": 0.28832775057974747,
        "step": 2238
    },
    {
        "loss": 2.306,
        "grad_norm": 1.7026312351226807,
        "learning_rate": 4.7020021726050135e-06,
        "epoch": 0.28845658335480545,
        "step": 2239
    },
    {
        "loss": 2.2706,
        "grad_norm": 1.4391615390777588,
        "learning_rate": 4.676283112954094e-06,
        "epoch": 0.28858541612986344,
        "step": 2240
    },
    {
        "loss": 1.9442,
        "grad_norm": 2.910775661468506,
        "learning_rate": 4.650631134469885e-06,
        "epoch": 0.2887142489049214,
        "step": 2241
    },
    {
        "loss": 2.577,
        "grad_norm": 1.9031230211257935,
        "learning_rate": 4.625046275118478e-06,
        "epoch": 0.2888430816799794,
        "step": 2242
    },
    {
        "loss": 1.9085,
        "grad_norm": 2.9705259799957275,
        "learning_rate": 4.59952857276662e-06,
        "epoch": 0.28897191445503734,
        "step": 2243
    },
    {
        "loss": 2.5988,
        "grad_norm": 1.3202170133590698,
        "learning_rate": 4.574078065181664e-06,
        "epoch": 0.2891007472300953,
        "step": 2244
    },
    {
        "loss": 1.0553,
        "grad_norm": 2.8824334144592285,
        "learning_rate": 4.548694790031566e-06,
        "epoch": 0.2892295800051533,
        "step": 2245
    },
    {
        "loss": 2.159,
        "grad_norm": 2.006631851196289,
        "learning_rate": 4.52337878488468e-06,
        "epoch": 0.2893584127802113,
        "step": 2246
    },
    {
        "loss": 1.9614,
        "grad_norm": 2.334136486053467,
        "learning_rate": 4.498130087209878e-06,
        "epoch": 0.2894872455552693,
        "step": 2247
    },
    {
        "loss": 2.1817,
        "grad_norm": 1.6769077777862549,
        "learning_rate": 4.47294873437637e-06,
        "epoch": 0.2896160783303272,
        "step": 2248
    },
    {
        "loss": 2.2667,
        "grad_norm": 1.631624460220337,
        "learning_rate": 4.447834763653702e-06,
        "epoch": 0.2897449111053852,
        "step": 2249
    },
    {
        "loss": 2.3068,
        "grad_norm": 3.065906047821045,
        "learning_rate": 4.42278821221172e-06,
        "epoch": 0.2898737438804432,
        "step": 2250
    },
    {
        "loss": 2.3399,
        "grad_norm": 2.328228235244751,
        "learning_rate": 4.397809117120438e-06,
        "epoch": 0.29000257665550117,
        "step": 2251
    },
    {
        "loss": 2.5699,
        "grad_norm": 1.3507893085479736,
        "learning_rate": 4.3728975153500665e-06,
        "epoch": 0.29013140943055915,
        "step": 2252
    },
    {
        "loss": 1.9534,
        "grad_norm": 2.3060126304626465,
        "learning_rate": 4.348053443770889e-06,
        "epoch": 0.29026024220561714,
        "step": 2253
    },
    {
        "loss": 2.0565,
        "grad_norm": 2.413722515106201,
        "learning_rate": 4.323276939153304e-06,
        "epoch": 0.29038907498067507,
        "step": 2254
    },
    {
        "loss": 1.6694,
        "grad_norm": 2.6327927112579346,
        "learning_rate": 4.298568038167633e-06,
        "epoch": 0.29051790775573305,
        "step": 2255
    },
    {
        "loss": 2.1636,
        "grad_norm": 1.9727022647857666,
        "learning_rate": 4.273926777384196e-06,
        "epoch": 0.29064674053079104,
        "step": 2256
    },
    {
        "loss": 2.6952,
        "grad_norm": 1.3897502422332764,
        "learning_rate": 4.249353193273159e-06,
        "epoch": 0.290775573305849,
        "step": 2257
    },
    {
        "loss": 2.6235,
        "grad_norm": 1.5019582509994507,
        "learning_rate": 4.224847322204567e-06,
        "epoch": 0.290904406080907,
        "step": 2258
    },
    {
        "loss": 2.0405,
        "grad_norm": 2.5375332832336426,
        "learning_rate": 4.2004092004482295e-06,
        "epoch": 0.29103323885596494,
        "step": 2259
    },
    {
        "loss": 2.436,
        "grad_norm": 1.6402299404144287,
        "learning_rate": 4.17603886417367e-06,
        "epoch": 0.2911620716310229,
        "step": 2260
    },
    {
        "loss": 2.3396,
        "grad_norm": 1.8972080945968628,
        "learning_rate": 4.151736349450091e-06,
        "epoch": 0.2912909044060809,
        "step": 2261
    },
    {
        "loss": 2.193,
        "grad_norm": 1.7970162630081177,
        "learning_rate": 4.127501692246327e-06,
        "epoch": 0.2914197371811389,
        "step": 2262
    },
    {
        "loss": 1.6133,
        "grad_norm": 3.212608814239502,
        "learning_rate": 4.1033349284307965e-06,
        "epoch": 0.2915485699561969,
        "step": 2263
    },
    {
        "loss": 2.523,
        "grad_norm": 1.8074088096618652,
        "learning_rate": 4.079236093771377e-06,
        "epoch": 0.2916774027312548,
        "step": 2264
    },
    {
        "loss": 1.494,
        "grad_norm": 3.8176333904266357,
        "learning_rate": 4.0552052239354595e-06,
        "epoch": 0.2918062355063128,
        "step": 2265
    },
    {
        "loss": 2.0407,
        "grad_norm": 2.116581916809082,
        "learning_rate": 4.031242354489823e-06,
        "epoch": 0.2919350682813708,
        "step": 2266
    },
    {
        "loss": 1.8897,
        "grad_norm": 2.0410633087158203,
        "learning_rate": 4.007347520900606e-06,
        "epoch": 0.29206390105642877,
        "step": 2267
    },
    {
        "loss": 2.2904,
        "grad_norm": 1.7758967876434326,
        "learning_rate": 3.98352075853326e-06,
        "epoch": 0.29219273383148675,
        "step": 2268
    },
    {
        "loss": 0.9173,
        "grad_norm": 1.6016314029693604,
        "learning_rate": 3.959762102652476e-06,
        "epoch": 0.2923215666065447,
        "step": 2269
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.5941095352172852,
        "learning_rate": 3.936071588422135e-06,
        "epoch": 0.29245039938160267,
        "step": 2270
    },
    {
        "loss": 1.9366,
        "grad_norm": 2.6076276302337646,
        "learning_rate": 3.9124492509052814e-06,
        "epoch": 0.29257923215666065,
        "step": 2271
    },
    {
        "loss": 2.2151,
        "grad_norm": 2.204706907272339,
        "learning_rate": 3.888895125064079e-06,
        "epoch": 0.29270806493171864,
        "step": 2272
    },
    {
        "loss": 2.2414,
        "grad_norm": 2.0021169185638428,
        "learning_rate": 3.8654092457596715e-06,
        "epoch": 0.2928368977067766,
        "step": 2273
    },
    {
        "loss": 1.6271,
        "grad_norm": 3.2952356338500977,
        "learning_rate": 3.841991647752241e-06,
        "epoch": 0.29296573048183455,
        "step": 2274
    },
    {
        "loss": 2.0758,
        "grad_norm": 1.9672869443893433,
        "learning_rate": 3.818642365700908e-06,
        "epoch": 0.29309456325689254,
        "step": 2275
    },
    {
        "loss": 1.6666,
        "grad_norm": 3.790945529937744,
        "learning_rate": 3.7953614341636745e-06,
        "epoch": 0.2932233960319505,
        "step": 2276
    },
    {
        "loss": 2.0456,
        "grad_norm": 1.2039140462875366,
        "learning_rate": 3.7721488875973656e-06,
        "epoch": 0.2933522288070085,
        "step": 2277
    },
    {
        "loss": 2.004,
        "grad_norm": 2.7798476219177246,
        "learning_rate": 3.7490047603576274e-06,
        "epoch": 0.2934810615820665,
        "step": 2278
    },
    {
        "loss": 2.0289,
        "grad_norm": 3.292703866958618,
        "learning_rate": 3.725929086698804e-06,
        "epoch": 0.2936098943571245,
        "step": 2279
    },
    {
        "loss": 1.8153,
        "grad_norm": 2.521000623703003,
        "learning_rate": 3.7029219007739535e-06,
        "epoch": 0.2937387271321824,
        "step": 2280
    },
    {
        "loss": 1.8198,
        "grad_norm": 1.9732693433761597,
        "learning_rate": 3.6799832366347763e-06,
        "epoch": 0.2938675599072404,
        "step": 2281
    },
    {
        "loss": 2.1184,
        "grad_norm": 2.6823110580444336,
        "learning_rate": 3.6571131282315096e-06,
        "epoch": 0.2939963926822984,
        "step": 2282
    },
    {
        "loss": 1.6995,
        "grad_norm": 2.5677950382232666,
        "learning_rate": 3.6343116094129715e-06,
        "epoch": 0.29412522545735637,
        "step": 2283
    },
    {
        "loss": 2.5057,
        "grad_norm": 2.7326345443725586,
        "learning_rate": 3.6115787139264555e-06,
        "epoch": 0.29425405823241435,
        "step": 2284
    },
    {
        "loss": 1.1038,
        "grad_norm": 3.06026291847229,
        "learning_rate": 3.5889144754176763e-06,
        "epoch": 0.2943828910074723,
        "step": 2285
    },
    {
        "loss": 1.0191,
        "grad_norm": 3.2111778259277344,
        "learning_rate": 3.5663189274307283e-06,
        "epoch": 0.29451172378253027,
        "step": 2286
    },
    {
        "loss": 1.3835,
        "grad_norm": 3.7875497341156006,
        "learning_rate": 3.5437921034080667e-06,
        "epoch": 0.29464055655758825,
        "step": 2287
    },
    {
        "loss": 2.7009,
        "grad_norm": 2.332714557647705,
        "learning_rate": 3.521334036690427e-06,
        "epoch": 0.29476938933264624,
        "step": 2288
    },
    {
        "loss": 1.8859,
        "grad_norm": 2.669264554977417,
        "learning_rate": 3.4989447605167424e-06,
        "epoch": 0.2948982221077042,
        "step": 2289
    },
    {
        "loss": 1.3219,
        "grad_norm": 3.2454333305358887,
        "learning_rate": 3.476624308024201e-06,
        "epoch": 0.29502705488276215,
        "step": 2290
    },
    {
        "loss": 1.4128,
        "grad_norm": 2.991457462310791,
        "learning_rate": 3.4543727122480384e-06,
        "epoch": 0.29515588765782014,
        "step": 2291
    },
    {
        "loss": 1.798,
        "grad_norm": 2.0392916202545166,
        "learning_rate": 3.4321900061216825e-06,
        "epoch": 0.2952847204328781,
        "step": 2292
    },
    {
        "loss": 2.4158,
        "grad_norm": 1.683077096939087,
        "learning_rate": 3.4100762224765214e-06,
        "epoch": 0.2954135532079361,
        "step": 2293
    },
    {
        "loss": 2.2811,
        "grad_norm": 2.2393484115600586,
        "learning_rate": 3.3880313940419685e-06,
        "epoch": 0.2955423859829941,
        "step": 2294
    },
    {
        "loss": 2.4093,
        "grad_norm": 1.4696645736694336,
        "learning_rate": 3.3660555534453685e-06,
        "epoch": 0.295671218758052,
        "step": 2295
    },
    {
        "loss": 2.0254,
        "grad_norm": 1.251219630241394,
        "learning_rate": 3.3441487332119712e-06,
        "epoch": 0.29580005153311,
        "step": 2296
    },
    {
        "loss": 0.8833,
        "grad_norm": 2.6849539279937744,
        "learning_rate": 3.32231096576488e-06,
        "epoch": 0.295928884308168,
        "step": 2297
    },
    {
        "loss": 1.606,
        "grad_norm": 2.920987129211426,
        "learning_rate": 3.3005422834249676e-06,
        "epoch": 0.296057717083226,
        "step": 2298
    },
    {
        "loss": 1.8645,
        "grad_norm": 1.6634442806243896,
        "learning_rate": 3.2788427184109016e-06,
        "epoch": 0.29618654985828397,
        "step": 2299
    },
    {
        "loss": 2.1826,
        "grad_norm": 1.9682682752609253,
        "learning_rate": 3.2572123028389856e-06,
        "epoch": 0.2963153826333419,
        "step": 2300
    },
    {
        "loss": 1.9703,
        "grad_norm": 1.8846068382263184,
        "learning_rate": 3.2356510687232613e-06,
        "epoch": 0.2964442154083999,
        "step": 2301
    },
    {
        "loss": 2.1525,
        "grad_norm": 2.1360511779785156,
        "learning_rate": 3.2141590479753236e-06,
        "epoch": 0.29657304818345787,
        "step": 2302
    },
    {
        "loss": 1.79,
        "grad_norm": 2.6108510494232178,
        "learning_rate": 3.19273627240434e-06,
        "epoch": 0.29670188095851585,
        "step": 2303
    },
    {
        "loss": 1.9266,
        "grad_norm": 1.5984338521957397,
        "learning_rate": 3.1713827737169853e-06,
        "epoch": 0.29683071373357384,
        "step": 2304
    },
    {
        "loss": 2.4863,
        "grad_norm": 1.4405465126037598,
        "learning_rate": 3.1500985835174247e-06,
        "epoch": 0.2969595465086318,
        "step": 2305
    },
    {
        "loss": 2.5077,
        "grad_norm": 2.46726393699646,
        "learning_rate": 3.1288837333072307e-06,
        "epoch": 0.29708837928368975,
        "step": 2306
    },
    {
        "loss": 2.4457,
        "grad_norm": 1.530234932899475,
        "learning_rate": 3.107738254485337e-06,
        "epoch": 0.29721721205874774,
        "step": 2307
    },
    {
        "loss": 1.7639,
        "grad_norm": 2.6081597805023193,
        "learning_rate": 3.0866621783480266e-06,
        "epoch": 0.2973460448338057,
        "step": 2308
    },
    {
        "loss": 2.4787,
        "grad_norm": 2.1819794178009033,
        "learning_rate": 3.0656555360888574e-06,
        "epoch": 0.2974748776088637,
        "step": 2309
    },
    {
        "loss": 1.6079,
        "grad_norm": 2.4461796283721924,
        "learning_rate": 3.0447183587986095e-06,
        "epoch": 0.2976037103839217,
        "step": 2310
    },
    {
        "loss": 2.269,
        "grad_norm": 1.448618769645691,
        "learning_rate": 3.0238506774652562e-06,
        "epoch": 0.2977325431589796,
        "step": 2311
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.3401321172714233,
        "learning_rate": 3.00305252297392e-06,
        "epoch": 0.2978613759340376,
        "step": 2312
    },
    {
        "loss": 2.2967,
        "grad_norm": 1.40111243724823,
        "learning_rate": 2.9823239261068113e-06,
        "epoch": 0.2979902087090956,
        "step": 2313
    },
    {
        "loss": 1.5968,
        "grad_norm": 2.6106443405151367,
        "learning_rate": 2.9616649175432053e-06,
        "epoch": 0.2981190414841536,
        "step": 2314
    },
    {
        "loss": 1.985,
        "grad_norm": 1.4512066841125488,
        "learning_rate": 2.9410755278593826e-06,
        "epoch": 0.29824787425921157,
        "step": 2315
    },
    {
        "loss": 2.3572,
        "grad_norm": 1.3455058336257935,
        "learning_rate": 2.9205557875285615e-06,
        "epoch": 0.2983767070342695,
        "step": 2316
    },
    {
        "loss": 2.2388,
        "grad_norm": 1.412711501121521,
        "learning_rate": 2.900105726920904e-06,
        "epoch": 0.2985055398093275,
        "step": 2317
    },
    {
        "loss": 2.1712,
        "grad_norm": 1.6247923374176025,
        "learning_rate": 2.879725376303438e-06,
        "epoch": 0.29863437258438547,
        "step": 2318
    },
    {
        "loss": 2.2683,
        "grad_norm": 2.3361024856567383,
        "learning_rate": 2.8594147658400074e-06,
        "epoch": 0.29876320535944345,
        "step": 2319
    },
    {
        "loss": 2.4494,
        "grad_norm": 1.643919825553894,
        "learning_rate": 2.839173925591232e-06,
        "epoch": 0.29889203813450144,
        "step": 2320
    },
    {
        "loss": 2.1592,
        "grad_norm": 1.9004180431365967,
        "learning_rate": 2.819002885514471e-06,
        "epoch": 0.29902087090955937,
        "step": 2321
    },
    {
        "loss": 2.2201,
        "grad_norm": 2.320718288421631,
        "learning_rate": 2.7989016754638164e-06,
        "epoch": 0.29914970368461735,
        "step": 2322
    },
    {
        "loss": 2.1675,
        "grad_norm": 2.1307413578033447,
        "learning_rate": 2.7788703251899416e-06,
        "epoch": 0.29927853645967534,
        "step": 2323
    },
    {
        "loss": 2.0058,
        "grad_norm": 1.8546432256698608,
        "learning_rate": 2.758908864340176e-06,
        "epoch": 0.2994073692347333,
        "step": 2324
    },
    {
        "loss": 1.9901,
        "grad_norm": 2.197418689727783,
        "learning_rate": 2.7390173224583815e-06,
        "epoch": 0.2995362020097913,
        "step": 2325
    },
    {
        "loss": 2.2802,
        "grad_norm": 1.759748935699463,
        "learning_rate": 2.7191957289849578e-06,
        "epoch": 0.29966503478484924,
        "step": 2326
    },
    {
        "loss": 2.7358,
        "grad_norm": 1.5879343748092651,
        "learning_rate": 2.699444113256755e-06,
        "epoch": 0.2997938675599072,
        "step": 2327
    },
    {
        "loss": 1.8781,
        "grad_norm": 3.2355775833129883,
        "learning_rate": 2.6797625045070717e-06,
        "epoch": 0.2999227003349652,
        "step": 2328
    },
    {
        "loss": 2.3223,
        "grad_norm": 1.9325840473175049,
        "learning_rate": 2.6601509318655905e-06,
        "epoch": 0.3000515331100232,
        "step": 2329
    },
    {
        "loss": 2.4522,
        "grad_norm": 1.265579342842102,
        "learning_rate": 2.640609424358309e-06,
        "epoch": 0.3001803658850812,
        "step": 2330
    },
    {
        "loss": 1.2281,
        "grad_norm": 2.962238073348999,
        "learning_rate": 2.6211380109075867e-06,
        "epoch": 0.30030919866013916,
        "step": 2331
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.5206899642944336,
        "learning_rate": 2.6017367203319656e-06,
        "epoch": 0.3004380314351971,
        "step": 2332
    },
    {
        "loss": 1.7723,
        "grad_norm": 2.539158344268799,
        "learning_rate": 2.5824055813462754e-06,
        "epoch": 0.3005668642102551,
        "step": 2333
    },
    {
        "loss": 1.853,
        "grad_norm": 1.7984598875045776,
        "learning_rate": 2.5631446225614418e-06,
        "epoch": 0.30069569698531307,
        "step": 2334
    },
    {
        "loss": 2.1322,
        "grad_norm": 2.1386427879333496,
        "learning_rate": 2.5439538724846003e-06,
        "epoch": 0.30082452976037105,
        "step": 2335
    },
    {
        "loss": 1.7263,
        "grad_norm": 2.566180944442749,
        "learning_rate": 2.5248333595189076e-06,
        "epoch": 0.30095336253542904,
        "step": 2336
    },
    {
        "loss": 2.2863,
        "grad_norm": 2.265157699584961,
        "learning_rate": 2.5057831119636165e-06,
        "epoch": 0.30108219531048697,
        "step": 2337
    },
    {
        "loss": 1.4161,
        "grad_norm": 2.8182764053344727,
        "learning_rate": 2.486803158013945e-06,
        "epoch": 0.30121102808554495,
        "step": 2338
    },
    {
        "loss": 2.2412,
        "grad_norm": 1.676203966140747,
        "learning_rate": 2.4678935257611004e-06,
        "epoch": 0.30133986086060294,
        "step": 2339
    },
    {
        "loss": 2.554,
        "grad_norm": 1.2595186233520508,
        "learning_rate": 2.4490542431922004e-06,
        "epoch": 0.3014686936356609,
        "step": 2340
    },
    {
        "loss": 2.0204,
        "grad_norm": 3.2456893920898438,
        "learning_rate": 2.4302853381902303e-06,
        "epoch": 0.3015975264107189,
        "step": 2341
    },
    {
        "loss": 1.5733,
        "grad_norm": 3.289339303970337,
        "learning_rate": 2.411586838534047e-06,
        "epoch": 0.30172635918577684,
        "step": 2342
    },
    {
        "loss": 2.4857,
        "grad_norm": 1.5677090883255005,
        "learning_rate": 2.3929587718982406e-06,
        "epoch": 0.3018551919608348,
        "step": 2343
    },
    {
        "loss": 2.2262,
        "grad_norm": 1.5996816158294678,
        "learning_rate": 2.3744011658532463e-06,
        "epoch": 0.3019840247358928,
        "step": 2344
    },
    {
        "loss": 1.9754,
        "grad_norm": 2.252411127090454,
        "learning_rate": 2.3559140478651374e-06,
        "epoch": 0.3021128575109508,
        "step": 2345
    },
    {
        "loss": 1.7953,
        "grad_norm": 2.99420166015625,
        "learning_rate": 2.3374974452957056e-06,
        "epoch": 0.3022416902860088,
        "step": 2346
    },
    {
        "loss": 2.2142,
        "grad_norm": 1.4363317489624023,
        "learning_rate": 2.319151385402346e-06,
        "epoch": 0.3023705230610667,
        "step": 2347
    },
    {
        "loss": 1.9129,
        "grad_norm": 2.9927778244018555,
        "learning_rate": 2.300875895338078e-06,
        "epoch": 0.3024993558361247,
        "step": 2348
    },
    {
        "loss": 2.2285,
        "grad_norm": 1.6594185829162598,
        "learning_rate": 2.282671002151471e-06,
        "epoch": 0.3026281886111827,
        "step": 2349
    },
    {
        "loss": 2.2694,
        "grad_norm": 2.001426935195923,
        "learning_rate": 2.2645367327865653e-06,
        "epoch": 0.30275702138624067,
        "step": 2350
    },
    {
        "loss": 2.3367,
        "grad_norm": 2.197152614593506,
        "learning_rate": 2.24647311408292e-06,
        "epoch": 0.30288585416129865,
        "step": 2351
    },
    {
        "loss": 2.0086,
        "grad_norm": 1.856697678565979,
        "learning_rate": 2.228480172775527e-06,
        "epoch": 0.30301468693635664,
        "step": 2352
    },
    {
        "loss": 2.186,
        "grad_norm": 1.9762603044509888,
        "learning_rate": 2.2105579354947448e-06,
        "epoch": 0.30314351971141457,
        "step": 2353
    },
    {
        "loss": 2.2283,
        "grad_norm": 2.0840697288513184,
        "learning_rate": 2.192706428766306e-06,
        "epoch": 0.30327235248647255,
        "step": 2354
    },
    {
        "loss": 1.7014,
        "grad_norm": 2.219481945037842,
        "learning_rate": 2.174925679011247e-06,
        "epoch": 0.30340118526153054,
        "step": 2355
    },
    {
        "loss": 1.8839,
        "grad_norm": 2.067058801651001,
        "learning_rate": 2.157215712545896e-06,
        "epoch": 0.3035300180365885,
        "step": 2356
    },
    {
        "loss": 1.7846,
        "grad_norm": 2.170823097229004,
        "learning_rate": 2.139576555581807e-06,
        "epoch": 0.3036588508116465,
        "step": 2357
    },
    {
        "loss": 2.294,
        "grad_norm": 2.480184555053711,
        "learning_rate": 2.122008234225753e-06,
        "epoch": 0.30378768358670444,
        "step": 2358
    },
    {
        "loss": 2.4266,
        "grad_norm": 2.5777392387390137,
        "learning_rate": 2.1045107744796233e-06,
        "epoch": 0.3039165163617624,
        "step": 2359
    },
    {
        "loss": 2.0113,
        "grad_norm": 2.824899673461914,
        "learning_rate": 2.0870842022404637e-06,
        "epoch": 0.3040453491368204,
        "step": 2360
    },
    {
        "loss": 1.918,
        "grad_norm": 2.0524649620056152,
        "learning_rate": 2.069728543300409e-06,
        "epoch": 0.3041741819118784,
        "step": 2361
    },
    {
        "loss": 2.1764,
        "grad_norm": 2.0583157539367676,
        "learning_rate": 2.0524438233466113e-06,
        "epoch": 0.3043030146869364,
        "step": 2362
    },
    {
        "loss": 1.6905,
        "grad_norm": 2.701343297958374,
        "learning_rate": 2.0352300679612523e-06,
        "epoch": 0.3044318474619943,
        "step": 2363
    },
    {
        "loss": 2.5973,
        "grad_norm": 1.4073419570922852,
        "learning_rate": 2.0180873026214576e-06,
        "epoch": 0.3045606802370523,
        "step": 2364
    },
    {
        "loss": 1.3121,
        "grad_norm": 3.1727142333984375,
        "learning_rate": 2.0010155526993257e-06,
        "epoch": 0.3046895130121103,
        "step": 2365
    },
    {
        "loss": 2.374,
        "grad_norm": 1.5878831148147583,
        "learning_rate": 1.9840148434618067e-06,
        "epoch": 0.30481834578716827,
        "step": 2366
    },
    {
        "loss": 2.1764,
        "grad_norm": 2.0700700283050537,
        "learning_rate": 1.9670852000707453e-06,
        "epoch": 0.30494717856222625,
        "step": 2367
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.3156485557556152,
        "learning_rate": 1.9502266475827533e-06,
        "epoch": 0.3050760113372842,
        "step": 2368
    },
    {
        "loss": 1.3774,
        "grad_norm": 2.5107831954956055,
        "learning_rate": 1.9334392109492894e-06,
        "epoch": 0.30520484411234217,
        "step": 2369
    },
    {
        "loss": 2.0028,
        "grad_norm": 2.300157308578491,
        "learning_rate": 1.9167229150165057e-06,
        "epoch": 0.30533367688740015,
        "step": 2370
    },
    {
        "loss": 1.9548,
        "grad_norm": 2.2930591106414795,
        "learning_rate": 1.9000777845252893e-06,
        "epoch": 0.30546250966245814,
        "step": 2371
    },
    {
        "loss": 2.1023,
        "grad_norm": 2.2833187580108643,
        "learning_rate": 1.8835038441111896e-06,
        "epoch": 0.3055913424375161,
        "step": 2372
    },
    {
        "loss": 2.487,
        "grad_norm": 2.202075958251953,
        "learning_rate": 1.8670011183043789e-06,
        "epoch": 0.30572017521257405,
        "step": 2373
    },
    {
        "loss": 2.0836,
        "grad_norm": 1.9517054557800293,
        "learning_rate": 1.850569631529675e-06,
        "epoch": 0.30584900798763204,
        "step": 2374
    },
    {
        "loss": 1.9189,
        "grad_norm": 1.8669204711914062,
        "learning_rate": 1.8342094081064021e-06,
        "epoch": 0.30597784076269,
        "step": 2375
    },
    {
        "loss": 2.1864,
        "grad_norm": 2.341066360473633,
        "learning_rate": 1.8179204722484578e-06,
        "epoch": 0.306106673537748,
        "step": 2376
    },
    {
        "loss": 2.2509,
        "grad_norm": 1.8819549083709717,
        "learning_rate": 1.8017028480641795e-06,
        "epoch": 0.306235506312806,
        "step": 2377
    },
    {
        "loss": 2.2369,
        "grad_norm": 1.5902737379074097,
        "learning_rate": 1.7855565595564172e-06,
        "epoch": 0.306364339087864,
        "step": 2378
    },
    {
        "loss": 2.5338,
        "grad_norm": 2.263158082962036,
        "learning_rate": 1.769481630622405e-06,
        "epoch": 0.3064931718629219,
        "step": 2379
    },
    {
        "loss": 1.5481,
        "grad_norm": 1.8868030309677124,
        "learning_rate": 1.7534780850537724e-06,
        "epoch": 0.3066220046379799,
        "step": 2380
    },
    {
        "loss": 2.2544,
        "grad_norm": 1.7971676588058472,
        "learning_rate": 1.737545946536484e-06,
        "epoch": 0.3067508374130379,
        "step": 2381
    },
    {
        "loss": 1.8375,
        "grad_norm": 2.0922157764434814,
        "learning_rate": 1.7216852386508387e-06,
        "epoch": 0.30687967018809587,
        "step": 2382
    },
    {
        "loss": 2.2147,
        "grad_norm": 2.117068290710449,
        "learning_rate": 1.705895984871425e-06,
        "epoch": 0.30700850296315385,
        "step": 2383
    },
    {
        "loss": 2.0337,
        "grad_norm": 2.6414337158203125,
        "learning_rate": 1.690178208567028e-06,
        "epoch": 0.3071373357382118,
        "step": 2384
    },
    {
        "loss": 1.3426,
        "grad_norm": 3.030212163925171,
        "learning_rate": 1.6745319330006892e-06,
        "epoch": 0.30726616851326977,
        "step": 2385
    },
    {
        "loss": 2.0363,
        "grad_norm": 1.8426252603530884,
        "learning_rate": 1.6589571813296012e-06,
        "epoch": 0.30739500128832775,
        "step": 2386
    },
    {
        "loss": 2.1991,
        "grad_norm": 1.2755024433135986,
        "learning_rate": 1.6434539766051194e-06,
        "epoch": 0.30752383406338574,
        "step": 2387
    },
    {
        "loss": 1.912,
        "grad_norm": 2.240354537963867,
        "learning_rate": 1.6280223417726892e-06,
        "epoch": 0.3076526668384437,
        "step": 2388
    },
    {
        "loss": 2.0092,
        "grad_norm": 2.0611937046051025,
        "learning_rate": 1.6126622996718244e-06,
        "epoch": 0.30778149961350165,
        "step": 2389
    },
    {
        "loss": 1.8215,
        "grad_norm": 2.0276217460632324,
        "learning_rate": 1.5973738730360954e-06,
        "epoch": 0.30791033238855964,
        "step": 2390
    },
    {
        "loss": 1.7096,
        "grad_norm": 2.2766029834747314,
        "learning_rate": 1.5821570844930634e-06,
        "epoch": 0.3080391651636176,
        "step": 2391
    },
    {
        "loss": 2.3469,
        "grad_norm": 1.5555336475372314,
        "learning_rate": 1.5670119565642848e-06,
        "epoch": 0.3081679979386756,
        "step": 2392
    },
    {
        "loss": 1.5946,
        "grad_norm": 3.117227792739868,
        "learning_rate": 1.5519385116652186e-06,
        "epoch": 0.3082968307137336,
        "step": 2393
    },
    {
        "loss": 1.976,
        "grad_norm": 2.2407033443450928,
        "learning_rate": 1.536936772105263e-06,
        "epoch": 0.3084256634887915,
        "step": 2394
    },
    {
        "loss": 1.6908,
        "grad_norm": 2.689805030822754,
        "learning_rate": 1.5220067600876687e-06,
        "epoch": 0.3085544962638495,
        "step": 2395
    },
    {
        "loss": 2.2563,
        "grad_norm": 1.1871087551116943,
        "learning_rate": 1.5071484977095485e-06,
        "epoch": 0.3086833290389075,
        "step": 2396
    },
    {
        "loss": 1.9296,
        "grad_norm": 2.3457179069519043,
        "learning_rate": 1.492362006961795e-06,
        "epoch": 0.3088121618139655,
        "step": 2397
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.6541320085525513,
        "learning_rate": 1.47764730972908e-06,
        "epoch": 0.30894099458902347,
        "step": 2398
    },
    {
        "loss": 2.2984,
        "grad_norm": 1.7994416952133179,
        "learning_rate": 1.4630044277898547e-06,
        "epoch": 0.3090698273640814,
        "step": 2399
    },
    {
        "loss": 2.148,
        "grad_norm": 3.1738526821136475,
        "learning_rate": 1.4484333828162223e-06,
        "epoch": 0.3091986601391394,
        "step": 2400
    },
    {
        "loss": 1.6559,
        "grad_norm": 1.771142840385437,
        "learning_rate": 1.433934196374015e-06,
        "epoch": 0.30932749291419737,
        "step": 2401
    },
    {
        "loss": 2.0676,
        "grad_norm": 1.9225788116455078,
        "learning_rate": 1.4195068899226726e-06,
        "epoch": 0.30945632568925535,
        "step": 2402
    },
    {
        "loss": 2.202,
        "grad_norm": 1.1797438859939575,
        "learning_rate": 1.4051514848152702e-06,
        "epoch": 0.30958515846431334,
        "step": 2403
    },
    {
        "loss": 2.1827,
        "grad_norm": 2.410686492919922,
        "learning_rate": 1.3908680022984733e-06,
        "epoch": 0.3097139912393713,
        "step": 2404
    },
    {
        "loss": 0.9711,
        "grad_norm": 2.6991264820098877,
        "learning_rate": 1.3766564635124768e-06,
        "epoch": 0.30984282401442925,
        "step": 2405
    },
    {
        "loss": 1.7566,
        "grad_norm": 3.0139243602752686,
        "learning_rate": 1.3625168894910113e-06,
        "epoch": 0.30997165678948724,
        "step": 2406
    },
    {
        "loss": 2.5591,
        "grad_norm": 1.790990948677063,
        "learning_rate": 1.3484493011612754e-06,
        "epoch": 0.3101004895645452,
        "step": 2407
    },
    {
        "loss": 2.26,
        "grad_norm": 1.6689194440841675,
        "learning_rate": 1.3344537193439644e-06,
        "epoch": 0.3102293223396032,
        "step": 2408
    },
    {
        "loss": 2.5641,
        "grad_norm": 1.8470968008041382,
        "learning_rate": 1.3205301647531642e-06,
        "epoch": 0.3103581551146612,
        "step": 2409
    },
    {
        "loss": 1.4168,
        "grad_norm": 3.3555543422698975,
        "learning_rate": 1.306678657996374e-06,
        "epoch": 0.3104869878897191,
        "step": 2410
    },
    {
        "loss": 2.2576,
        "grad_norm": 1.6468571424484253,
        "learning_rate": 1.292899219574445e-06,
        "epoch": 0.3106158206647771,
        "step": 2411
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.6233190298080444,
        "learning_rate": 1.2791918698815853e-06,
        "epoch": 0.3107446534398351,
        "step": 2412
    },
    {
        "loss": 2.1828,
        "grad_norm": 1.6532025337219238,
        "learning_rate": 1.2655566292053002e-06,
        "epoch": 0.3108734862148931,
        "step": 2413
    },
    {
        "loss": 2.2288,
        "grad_norm": 1.480706810951233,
        "learning_rate": 1.251993517726352e-06,
        "epoch": 0.31100231898995107,
        "step": 2414
    },
    {
        "loss": 2.046,
        "grad_norm": 2.138892412185669,
        "learning_rate": 1.238502555518778e-06,
        "epoch": 0.311131151765009,
        "step": 2415
    },
    {
        "loss": 2.0862,
        "grad_norm": 2.2765612602233887,
        "learning_rate": 1.2250837625498e-06,
        "epoch": 0.311259984540067,
        "step": 2416
    },
    {
        "loss": 2.5043,
        "grad_norm": 2.3132004737854004,
        "learning_rate": 1.2117371586798764e-06,
        "epoch": 0.31138881731512497,
        "step": 2417
    },
    {
        "loss": 2.2477,
        "grad_norm": 1.6555383205413818,
        "learning_rate": 1.1984627636625557e-06,
        "epoch": 0.31151765009018295,
        "step": 2418
    },
    {
        "loss": 2.6214,
        "grad_norm": 1.8168972730636597,
        "learning_rate": 1.1852605971445663e-06,
        "epoch": 0.31164648286524094,
        "step": 2419
    },
    {
        "loss": 2.3885,
        "grad_norm": 1.6938564777374268,
        "learning_rate": 1.1721306786657004e-06,
        "epoch": 0.31177531564029887,
        "step": 2420
    },
    {
        "loss": 2.2037,
        "grad_norm": 1.5932506322860718,
        "learning_rate": 1.1590730276588524e-06,
        "epoch": 0.31190414841535685,
        "step": 2421
    },
    {
        "loss": 1.2763,
        "grad_norm": 2.639737129211426,
        "learning_rate": 1.1460876634499297e-06,
        "epoch": 0.31203298119041484,
        "step": 2422
    },
    {
        "loss": 2.0676,
        "grad_norm": 2.2335703372955322,
        "learning_rate": 1.1331746052578695e-06,
        "epoch": 0.3121618139654728,
        "step": 2423
    },
    {
        "loss": 2.4226,
        "grad_norm": 2.0428247451782227,
        "learning_rate": 1.1203338721945789e-06,
        "epoch": 0.3122906467405308,
        "step": 2424
    },
    {
        "loss": 2.2944,
        "grad_norm": 2.154689073562622,
        "learning_rate": 1.1075654832649441e-06,
        "epoch": 0.31241947951558874,
        "step": 2425
    },
    {
        "loss": 1.9717,
        "grad_norm": 1.9054499864578247,
        "learning_rate": 1.0948694573667485e-06,
        "epoch": 0.3125483122906467,
        "step": 2426
    },
    {
        "loss": 2.3714,
        "grad_norm": 1.3373836278915405,
        "learning_rate": 1.0822458132906888e-06,
        "epoch": 0.3126771450657047,
        "step": 2427
    },
    {
        "loss": 1.2725,
        "grad_norm": 2.6478796005249023,
        "learning_rate": 1.0696945697203365e-06,
        "epoch": 0.3128059778407627,
        "step": 2428
    },
    {
        "loss": 2.2875,
        "grad_norm": 1.524193525314331,
        "learning_rate": 1.0572157452321097e-06,
        "epoch": 0.3129348106158207,
        "step": 2429
    },
    {
        "loss": 2.0509,
        "grad_norm": 1.5837260484695435,
        "learning_rate": 1.0448093582952289e-06,
        "epoch": 0.31306364339087867,
        "step": 2430
    },
    {
        "loss": 2.399,
        "grad_norm": 1.5641403198242188,
        "learning_rate": 1.032475427271723e-06,
        "epoch": 0.3131924761659366,
        "step": 2431
    },
    {
        "loss": 2.321,
        "grad_norm": 1.9879405498504639,
        "learning_rate": 1.020213970416356e-06,
        "epoch": 0.3133213089409946,
        "step": 2432
    },
    {
        "loss": 2.4831,
        "grad_norm": 1.400029182434082,
        "learning_rate": 1.0080250058766561e-06,
        "epoch": 0.31345014171605257,
        "step": 2433
    },
    {
        "loss": 2.1125,
        "grad_norm": 1.3683087825775146,
        "learning_rate": 9.959085516928368e-07,
        "epoch": 0.31357897449111055,
        "step": 2434
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.0207290649414062,
        "learning_rate": 9.838646257978145e-07,
        "epoch": 0.31370780726616854,
        "step": 2435
    },
    {
        "loss": 2.3162,
        "grad_norm": 1.4636132717132568,
        "learning_rate": 9.718932460171414e-07,
        "epoch": 0.31383664004122647,
        "step": 2436
    },
    {
        "loss": 2.5447,
        "grad_norm": 1.8982574939727783,
        "learning_rate": 9.59994430069e-07,
        "epoch": 0.31396547281628445,
        "step": 2437
    },
    {
        "loss": 1.7089,
        "grad_norm": 2.5752177238464355,
        "learning_rate": 9.481681955641919e-07,
        "epoch": 0.31409430559134244,
        "step": 2438
    },
    {
        "loss": 1.79,
        "grad_norm": 2.708524465560913,
        "learning_rate": 9.36414560006077e-07,
        "epoch": 0.3142231383664004,
        "step": 2439
    },
    {
        "loss": 2.6707,
        "grad_norm": 1.467467188835144,
        "learning_rate": 9.247335407905733e-07,
        "epoch": 0.3143519711414584,
        "step": 2440
    },
    {
        "loss": 1.9799,
        "grad_norm": 2.9376187324523926,
        "learning_rate": 9.131251552061071e-07,
        "epoch": 0.31448080391651634,
        "step": 2441
    },
    {
        "loss": 1.8422,
        "grad_norm": 2.9618682861328125,
        "learning_rate": 9.015894204336462e-07,
        "epoch": 0.3146096366915743,
        "step": 2442
    },
    {
        "loss": 2.0577,
        "grad_norm": 2.2098846435546875,
        "learning_rate": 8.901263535465831e-07,
        "epoch": 0.3147384694666323,
        "step": 2443
    },
    {
        "loss": 2.2153,
        "grad_norm": 1.8922308683395386,
        "learning_rate": 8.787359715107856e-07,
        "epoch": 0.3148673022416903,
        "step": 2444
    },
    {
        "loss": 2.1517,
        "grad_norm": 1.5723148584365845,
        "learning_rate": 8.674182911845296e-07,
        "epoch": 0.3149961350167483,
        "step": 2445
    },
    {
        "loss": 1.2217,
        "grad_norm": 2.811828136444092,
        "learning_rate": 8.561733293185159e-07,
        "epoch": 0.3151249677918062,
        "step": 2446
    },
    {
        "loss": 1.8841,
        "grad_norm": 2.2419981956481934,
        "learning_rate": 8.45001102555787e-07,
        "epoch": 0.3152538005668642,
        "step": 2447
    },
    {
        "loss": 1.851,
        "grad_norm": 2.5988993644714355,
        "learning_rate": 8.339016274317546e-07,
        "epoch": 0.3153826333419222,
        "step": 2448
    },
    {
        "loss": 2.2409,
        "grad_norm": 1.6276551485061646,
        "learning_rate": 8.228749203741448e-07,
        "epoch": 0.31551146611698017,
        "step": 2449
    },
    {
        "loss": 1.7287,
        "grad_norm": 2.306164264678955,
        "learning_rate": 8.119209977029751e-07,
        "epoch": 0.31564029889203815,
        "step": 2450
    },
    {
        "loss": 2.0804,
        "grad_norm": 1.7428512573242188,
        "learning_rate": 8.010398756305715e-07,
        "epoch": 0.31576913166709614,
        "step": 2451
    },
    {
        "loss": 1.2155,
        "grad_norm": 2.9228484630584717,
        "learning_rate": 7.902315702614738e-07,
        "epoch": 0.31589796444215407,
        "step": 2452
    },
    {
        "loss": 2.0623,
        "grad_norm": 2.310091257095337,
        "learning_rate": 7.794960975924803e-07,
        "epoch": 0.31602679721721205,
        "step": 2453
    },
    {
        "loss": 2.4459,
        "grad_norm": 1.5985397100448608,
        "learning_rate": 7.688334735125647e-07,
        "epoch": 0.31615562999227004,
        "step": 2454
    },
    {
        "loss": 2.13,
        "grad_norm": 1.778580904006958,
        "learning_rate": 7.582437138029086e-07,
        "epoch": 0.316284462767328,
        "step": 2455
    },
    {
        "loss": 2.1024,
        "grad_norm": 2.383382797241211,
        "learning_rate": 7.477268341368359e-07,
        "epoch": 0.316413295542386,
        "step": 2456
    },
    {
        "loss": 1.9982,
        "grad_norm": 2.294701099395752,
        "learning_rate": 7.372828500798068e-07,
        "epoch": 0.31654212831744394,
        "step": 2457
    },
    {
        "loss": 1.675,
        "grad_norm": 2.421135902404785,
        "learning_rate": 7.269117770893896e-07,
        "epoch": 0.3166709610925019,
        "step": 2458
    },
    {
        "loss": 1.3534,
        "grad_norm": 2.286825656890869,
        "learning_rate": 7.166136305152504e-07,
        "epoch": 0.3167997938675599,
        "step": 2459
    },
    {
        "loss": 2.0642,
        "grad_norm": 1.8285783529281616,
        "learning_rate": 7.063884255991193e-07,
        "epoch": 0.3169286266426179,
        "step": 2460
    },
    {
        "loss": 1.9803,
        "grad_norm": 3.155747890472412,
        "learning_rate": 6.962361774747573e-07,
        "epoch": 0.3170574594176759,
        "step": 2461
    },
    {
        "loss": 2.1895,
        "grad_norm": 1.18184232711792,
        "learning_rate": 6.861569011679614e-07,
        "epoch": 0.3171862921927338,
        "step": 2462
    },
    {
        "loss": 1.2841,
        "grad_norm": 2.6064817905426025,
        "learning_rate": 6.761506115965099e-07,
        "epoch": 0.3173151249677918,
        "step": 2463
    },
    {
        "loss": 2.0757,
        "grad_norm": 1.9176498651504517,
        "learning_rate": 6.662173235701729e-07,
        "epoch": 0.3174439577428498,
        "step": 2464
    },
    {
        "loss": 1.6959,
        "grad_norm": 3.490565299987793,
        "learning_rate": 6.563570517906736e-07,
        "epoch": 0.31757279051790777,
        "step": 2465
    },
    {
        "loss": 1.605,
        "grad_norm": 2.844282627105713,
        "learning_rate": 6.465698108516604e-07,
        "epoch": 0.31770162329296575,
        "step": 2466
    },
    {
        "loss": 1.3378,
        "grad_norm": 3.162681818008423,
        "learning_rate": 6.368556152386962e-07,
        "epoch": 0.3178304560680237,
        "step": 2467
    },
    {
        "loss": 1.2564,
        "grad_norm": 3.665722370147705,
        "learning_rate": 6.272144793292301e-07,
        "epoch": 0.31795928884308167,
        "step": 2468
    },
    {
        "loss": 2.0562,
        "grad_norm": 1.2291654348373413,
        "learning_rate": 6.176464173925922e-07,
        "epoch": 0.31808812161813965,
        "step": 2469
    },
    {
        "loss": 2.1713,
        "grad_norm": 1.0865992307662964,
        "learning_rate": 6.081514435899382e-07,
        "epoch": 0.31821695439319764,
        "step": 2470
    },
    {
        "loss": 2.2284,
        "grad_norm": 2.2316713333129883,
        "learning_rate": 5.987295719742603e-07,
        "epoch": 0.3183457871682556,
        "step": 2471
    },
    {
        "loss": 2.1112,
        "grad_norm": 2.293886423110962,
        "learning_rate": 5.893808164903757e-07,
        "epoch": 0.31847461994331355,
        "step": 2472
    },
    {
        "loss": 2.4736,
        "grad_norm": 2.6577064990997314,
        "learning_rate": 5.801051909748501e-07,
        "epoch": 0.31860345271837154,
        "step": 2473
    },
    {
        "loss": 2.345,
        "grad_norm": 1.4970759153366089,
        "learning_rate": 5.709027091560459e-07,
        "epoch": 0.3187322854934295,
        "step": 2474
    },
    {
        "loss": 2.0492,
        "grad_norm": 2.297424077987671,
        "learning_rate": 5.617733846540463e-07,
        "epoch": 0.3188611182684875,
        "step": 2475
    },
    {
        "loss": 1.8776,
        "grad_norm": 2.09562349319458,
        "learning_rate": 5.527172309806706e-07,
        "epoch": 0.3189899510435455,
        "step": 2476
    },
    {
        "loss": 1.7065,
        "grad_norm": 2.3312363624572754,
        "learning_rate": 5.437342615394414e-07,
        "epoch": 0.3191187838186035,
        "step": 2477
    },
    {
        "loss": 1.8916,
        "grad_norm": 3.0189480781555176,
        "learning_rate": 5.348244896255628e-07,
        "epoch": 0.3192476165936614,
        "step": 2478
    },
    {
        "loss": 2.1702,
        "grad_norm": 1.8697415590286255,
        "learning_rate": 5.259879284258972e-07,
        "epoch": 0.3193764493687194,
        "step": 2479
    },
    {
        "loss": 1.9424,
        "grad_norm": 1.866991400718689,
        "learning_rate": 5.17224591018961e-07,
        "epoch": 0.3195052821437774,
        "step": 2480
    },
    {
        "loss": 1.3481,
        "grad_norm": 2.75687313079834,
        "learning_rate": 5.085344903748957e-07,
        "epoch": 0.31963411491883537,
        "step": 2481
    },
    {
        "loss": 2.2677,
        "grad_norm": 1.882243037223816,
        "learning_rate": 4.999176393554405e-07,
        "epoch": 0.31976294769389335,
        "step": 2482
    },
    {
        "loss": 2.2896,
        "grad_norm": 2.2599329948425293,
        "learning_rate": 4.913740507139386e-07,
        "epoch": 0.3198917804689513,
        "step": 2483
    },
    {
        "loss": 2.2699,
        "grad_norm": 1.3148387670516968,
        "learning_rate": 4.82903737095275e-07,
        "epoch": 0.32002061324400927,
        "step": 2484
    },
    {
        "loss": 2.4093,
        "grad_norm": 1.9234989881515503,
        "learning_rate": 4.745067110359158e-07,
        "epoch": 0.32014944601906725,
        "step": 2485
    },
    {
        "loss": 2.1792,
        "grad_norm": 2.639707088470459,
        "learning_rate": 4.6618298496383104e-07,
        "epoch": 0.32027827879412524,
        "step": 2486
    },
    {
        "loss": 1.7202,
        "grad_norm": 2.7015655040740967,
        "learning_rate": 4.579325711985272e-07,
        "epoch": 0.3204071115691832,
        "step": 2487
    },
    {
        "loss": 1.9048,
        "grad_norm": 2.8561887741088867,
        "learning_rate": 4.497554819509697e-07,
        "epoch": 0.32053594434424115,
        "step": 2488
    },
    {
        "loss": 2.0325,
        "grad_norm": 2.3943517208099365,
        "learning_rate": 4.416517293236444e-07,
        "epoch": 0.32066477711929914,
        "step": 2489
    },
    {
        "loss": 2.5197,
        "grad_norm": 1.6911211013793945,
        "learning_rate": 4.3362132531046815e-07,
        "epoch": 0.3207936098943571,
        "step": 2490
    },
    {
        "loss": 2.5625,
        "grad_norm": 2.7504091262817383,
        "learning_rate": 4.2566428179680594e-07,
        "epoch": 0.3209224426694151,
        "step": 2491
    },
    {
        "loss": 2.1527,
        "grad_norm": 2.7638776302337646,
        "learning_rate": 4.1778061055944283e-07,
        "epoch": 0.3210512754444731,
        "step": 2492
    },
    {
        "loss": 1.7774,
        "grad_norm": 2.3786604404449463,
        "learning_rate": 4.099703232665675e-07,
        "epoch": 0.321180108219531,
        "step": 2493
    },
    {
        "loss": 2.9066,
        "grad_norm": 1.8597171306610107,
        "learning_rate": 4.0223343147777203e-07,
        "epoch": 0.321308940994589,
        "step": 2494
    },
    {
        "loss": 2.0818,
        "grad_norm": 1.717081069946289,
        "learning_rate": 3.9456994664400225e-07,
        "epoch": 0.321437773769647,
        "step": 2495
    },
    {
        "loss": 0.9371,
        "grad_norm": 2.975588321685791,
        "learning_rate": 3.8697988010756305e-07,
        "epoch": 0.321566606544705,
        "step": 2496
    },
    {
        "loss": 2.1899,
        "grad_norm": 1.4019169807434082,
        "learning_rate": 3.7946324310210167e-07,
        "epoch": 0.32169543931976297,
        "step": 2497
    },
    {
        "loss": 2.2192,
        "grad_norm": 1.2445976734161377,
        "learning_rate": 3.720200467525803e-07,
        "epoch": 0.3218242720948209,
        "step": 2498
    },
    {
        "loss": 2.1393,
        "grad_norm": 2.2181222438812256,
        "learning_rate": 3.646503020752756e-07,
        "epoch": 0.3219531048698789,
        "step": 2499
    },
    {
        "loss": 2.2232,
        "grad_norm": 2.7995057106018066,
        "learning_rate": 3.573540199777403e-07,
        "epoch": 0.32208193764493687,
        "step": 2500
    },
    {
        "loss": 1.7355,
        "grad_norm": 2.646285057067871,
        "learning_rate": 3.501312112588029e-07,
        "epoch": 0.32221077041999485,
        "step": 2501
    },
    {
        "loss": 1.9345,
        "grad_norm": 1.9542360305786133,
        "learning_rate": 3.4298188660855657e-07,
        "epoch": 0.32233960319505284,
        "step": 2502
    },
    {
        "loss": 2.0525,
        "grad_norm": 2.6026511192321777,
        "learning_rate": 3.359060566083205e-07,
        "epoch": 0.3224684359701108,
        "step": 2503
    },
    {
        "loss": 1.5348,
        "grad_norm": 2.121040105819702,
        "learning_rate": 3.2890373173064535e-07,
        "epoch": 0.32259726874516875,
        "step": 2504
    },
    {
        "loss": 1.3499,
        "grad_norm": 3.2537155151367188,
        "learning_rate": 3.219749223392965e-07,
        "epoch": 0.32272610152022674,
        "step": 2505
    },
    {
        "loss": 2.2962,
        "grad_norm": 2.3576626777648926,
        "learning_rate": 3.1511963868922636e-07,
        "epoch": 0.3228549342952847,
        "step": 2506
    },
    {
        "loss": 2.4112,
        "grad_norm": 2.014965295791626,
        "learning_rate": 3.0833789092656885e-07,
        "epoch": 0.3229837670703427,
        "step": 2507
    },
    {
        "loss": 1.7137,
        "grad_norm": 2.346597194671631,
        "learning_rate": 3.016296890886117e-07,
        "epoch": 0.3231125998454007,
        "step": 2508
    },
    {
        "loss": 1.742,
        "grad_norm": 2.3014075756073,
        "learning_rate": 2.9499504310380733e-07,
        "epoch": 0.3232414326204586,
        "step": 2509
    },
    {
        "loss": 2.1605,
        "grad_norm": 2.0865602493286133,
        "learning_rate": 2.8843396279172876e-07,
        "epoch": 0.3233702653955166,
        "step": 2510
    },
    {
        "loss": 1.8108,
        "grad_norm": 2.71919846534729,
        "learning_rate": 2.819464578630804e-07,
        "epoch": 0.3234990981705746,
        "step": 2511
    },
    {
        "loss": 1.8247,
        "grad_norm": 2.622122049331665,
        "learning_rate": 2.755325379196594e-07,
        "epoch": 0.3236279309456326,
        "step": 2512
    },
    {
        "loss": 1.8161,
        "grad_norm": 2.5641486644744873,
        "learning_rate": 2.6919221245435553e-07,
        "epoch": 0.32375676372069057,
        "step": 2513
    },
    {
        "loss": 1.9442,
        "grad_norm": 2.0531280040740967,
        "learning_rate": 2.6292549085115136e-07,
        "epoch": 0.3238855964957485,
        "step": 2514
    },
    {
        "loss": 1.8736,
        "grad_norm": 2.4235246181488037,
        "learning_rate": 2.567323823850776e-07,
        "epoch": 0.3240144292708065,
        "step": 2515
    },
    {
        "loss": 2.0784,
        "grad_norm": 2.258362054824829,
        "learning_rate": 2.506128962222076e-07,
        "epoch": 0.32414326204586447,
        "step": 2516
    },
    {
        "loss": 2.5281,
        "grad_norm": 1.7088663578033447,
        "learning_rate": 2.4456704141967437e-07,
        "epoch": 0.32427209482092245,
        "step": 2517
    },
    {
        "loss": 2.2189,
        "grad_norm": 2.51774263381958,
        "learning_rate": 2.385948269256033e-07,
        "epoch": 0.32440092759598044,
        "step": 2518
    },
    {
        "loss": 2.2581,
        "grad_norm": 1.3839011192321777,
        "learning_rate": 2.3269626157915169e-07,
        "epoch": 0.32452976037103837,
        "step": 2519
    },
    {
        "loss": 1.7896,
        "grad_norm": 2.3895769119262695,
        "learning_rate": 2.2687135411046944e-07,
        "epoch": 0.32465859314609635,
        "step": 2520
    },
    {
        "loss": 1.6631,
        "grad_norm": 2.871640682220459,
        "learning_rate": 2.2112011314067706e-07,
        "epoch": 0.32478742592115434,
        "step": 2521
    },
    {
        "loss": 1.667,
        "grad_norm": 2.5435407161712646,
        "learning_rate": 2.1544254718187108e-07,
        "epoch": 0.3249162586962123,
        "step": 2522
    },
    {
        "loss": 2.4839,
        "grad_norm": 2.0360970497131348,
        "learning_rate": 2.0983866463711865e-07,
        "epoch": 0.3250450914712703,
        "step": 2523
    },
    {
        "loss": 2.5296,
        "grad_norm": 1.7308990955352783,
        "learning_rate": 2.043084738004186e-07,
        "epoch": 0.32517392424632824,
        "step": 2524
    },
    {
        "loss": 1.669,
        "grad_norm": 2.6101837158203125,
        "learning_rate": 1.9885198285670148e-07,
        "epoch": 0.3253027570213862,
        "step": 2525
    },
    {
        "loss": 2.2513,
        "grad_norm": 1.431391954421997,
        "learning_rate": 1.9346919988181832e-07,
        "epoch": 0.3254315897964442,
        "step": 2526
    },
    {
        "loss": 2.474,
        "grad_norm": 1.7509928941726685,
        "learning_rate": 1.8816013284254085e-07,
        "epoch": 0.3255604225715022,
        "step": 2527
    },
    {
        "loss": 2.0343,
        "grad_norm": 2.7775580883026123,
        "learning_rate": 1.8292478959653357e-07,
        "epoch": 0.3256892553465602,
        "step": 2528
    },
    {
        "loss": 1.987,
        "grad_norm": 2.7041311264038086,
        "learning_rate": 1.7776317789233167e-07,
        "epoch": 0.32581808812161817,
        "step": 2529
    },
    {
        "loss": 1.9453,
        "grad_norm": 2.690747022628784,
        "learning_rate": 1.726753053693686e-07,
        "epoch": 0.3259469208966761,
        "step": 2530
    },
    {
        "loss": 2.4644,
        "grad_norm": 2.0688581466674805,
        "learning_rate": 1.6766117955791527e-07,
        "epoch": 0.3260757536717341,
        "step": 2531
    },
    {
        "loss": 2.2069,
        "grad_norm": 2.6963090896606445,
        "learning_rate": 1.6272080787911314e-07,
        "epoch": 0.32620458644679207,
        "step": 2532
    },
    {
        "loss": 1.9329,
        "grad_norm": 2.4273834228515625,
        "learning_rate": 1.5785419764493547e-07,
        "epoch": 0.32633341922185005,
        "step": 2533
    },
    {
        "loss": 2.0174,
        "grad_norm": 2.0824952125549316,
        "learning_rate": 1.530613560581873e-07,
        "epoch": 0.32646225199690804,
        "step": 2534
    },
    {
        "loss": 2.2316,
        "grad_norm": 1.8928736448287964,
        "learning_rate": 1.4834229021249424e-07,
        "epoch": 0.32659108477196597,
        "step": 2535
    },
    {
        "loss": 1.7362,
        "grad_norm": 3.0147058963775635,
        "learning_rate": 1.4369700709228606e-07,
        "epoch": 0.32671991754702395,
        "step": 2536
    },
    {
        "loss": 1.51,
        "grad_norm": 2.5138020515441895,
        "learning_rate": 1.39125513572802e-07,
        "epoch": 0.32684875032208194,
        "step": 2537
    },
    {
        "loss": 1.9812,
        "grad_norm": 1.7604496479034424,
        "learning_rate": 1.3462781642004652e-07,
        "epoch": 0.3269775830971399,
        "step": 2538
    },
    {
        "loss": 1.6215,
        "grad_norm": 2.952423334121704,
        "learning_rate": 1.30203922290828e-07,
        "epoch": 0.3271064158721979,
        "step": 2539
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.262709379196167,
        "learning_rate": 1.2585383773270343e-07,
        "epoch": 0.32723524864725584,
        "step": 2540
    },
    {
        "loss": 2.2561,
        "grad_norm": 2.3843090534210205,
        "learning_rate": 1.2157756918400597e-07,
        "epoch": 0.3273640814223138,
        "step": 2541
    },
    {
        "loss": 2.1658,
        "grad_norm": 2.1670455932617188,
        "learning_rate": 1.1737512297380072e-07,
        "epoch": 0.3274929141973718,
        "step": 2542
    },
    {
        "loss": 2.69,
        "grad_norm": 1.709717869758606,
        "learning_rate": 1.1324650532189563e-07,
        "epoch": 0.3276217469724298,
        "step": 2543
    },
    {
        "loss": 1.8861,
        "grad_norm": 2.680389165878296,
        "learning_rate": 1.0919172233884167e-07,
        "epoch": 0.3277505797474878,
        "step": 2544
    },
    {
        "loss": 2.7581,
        "grad_norm": 1.422823190689087,
        "learning_rate": 1.0521078002589946e-07,
        "epoch": 0.3278794125225457,
        "step": 2545
    },
    {
        "loss": 2.3991,
        "grad_norm": 1.3609660863876343,
        "learning_rate": 1.0130368427505032e-07,
        "epoch": 0.3280082452976037,
        "step": 2546
    },
    {
        "loss": 2.427,
        "grad_norm": 1.8619256019592285,
        "learning_rate": 9.747044086895751e-08,
        "epoch": 0.3281370780726617,
        "step": 2547
    },
    {
        "loss": 2.5769,
        "grad_norm": 1.5340800285339355,
        "learning_rate": 9.371105548101611e-08,
        "epoch": 0.32826591084771967,
        "step": 2548
    },
    {
        "loss": 2.2535,
        "grad_norm": 2.200622320175171,
        "learning_rate": 9.002553367527533e-08,
        "epoch": 0.32839474362277765,
        "step": 2549
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.8604609966278076,
        "learning_rate": 8.64138809064774e-08,
        "epoch": 0.3285235763978356,
        "step": 2550
    },
    {
        "loss": 1.2409,
        "grad_norm": 3.2305259704589844,
        "learning_rate": 8.28761025200353e-08,
        "epoch": 0.32865240917289357,
        "step": 2551
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.5957578420639038,
        "learning_rate": 7.941220375202174e-08,
        "epoch": 0.32878124194795155,
        "step": 2552
    },
    {
        "loss": 2.4336,
        "grad_norm": 1.8534636497497559,
        "learning_rate": 7.602218972916352e-08,
        "epoch": 0.32891007472300954,
        "step": 2553
    },
    {
        "loss": 2.1824,
        "grad_norm": 2.3832342624664307,
        "learning_rate": 7.270606546883607e-08,
        "epoch": 0.3290389074980675,
        "step": 2554
    },
    {
        "loss": 1.7037,
        "grad_norm": 1.745356559753418,
        "learning_rate": 6.946383587906336e-08,
        "epoch": 0.3291677402731255,
        "step": 2555
    },
    {
        "loss": 2.549,
        "grad_norm": 1.7187378406524658,
        "learning_rate": 6.629550575847354e-08,
        "epoch": 0.32929657304818344,
        "step": 2556
    },
    {
        "loss": 2.5862,
        "grad_norm": 1.3074368238449097,
        "learning_rate": 6.320107979635448e-08,
        "epoch": 0.3294254058232414,
        "step": 2557
    },
    {
        "loss": 2.1072,
        "grad_norm": 2.1495771408081055,
        "learning_rate": 6.018056257259819e-08,
        "epoch": 0.3295542385982994,
        "step": 2558
    },
    {
        "loss": 1.5544,
        "grad_norm": 3.221351385116577,
        "learning_rate": 5.723395855770086e-08,
        "epoch": 0.3296830713733574,
        "step": 2559
    },
    {
        "loss": 2.099,
        "grad_norm": 2.3380415439605713,
        "learning_rate": 5.4361272112773973e-08,
        "epoch": 0.3298119041484154,
        "step": 2560
    },
    {
        "loss": 1.46,
        "grad_norm": 2.9851670265197754,
        "learning_rate": 5.1562507489522073e-08,
        "epoch": 0.3299407369234733,
        "step": 2561
    },
    {
        "loss": 2.4015,
        "grad_norm": 1.2495157718658447,
        "learning_rate": 4.8837668830242766e-08,
        "epoch": 0.3300695696985313,
        "step": 2562
    },
    {
        "loss": 2.0979,
        "grad_norm": 1.4346309900283813,
        "learning_rate": 4.61867601678323e-08,
        "epoch": 0.3301984024735893,
        "step": 2563
    },
    {
        "loss": 2.3172,
        "grad_norm": 1.8694097995758057,
        "learning_rate": 4.360978542574668e-08,
        "epoch": 0.33032723524864727,
        "step": 2564
    },
    {
        "loss": 2.2129,
        "grad_norm": 1.3335866928100586,
        "learning_rate": 4.1106748418023866e-08,
        "epoch": 0.33045606802370525,
        "step": 2565
    },
    {
        "loss": 1.5856,
        "grad_norm": 2.0687270164489746,
        "learning_rate": 3.867765284928382e-08,
        "epoch": 0.3305849007987632,
        "step": 2566
    },
    {
        "loss": 2.1835,
        "grad_norm": 2.1193315982818604,
        "learning_rate": 3.632250231468959e-08,
        "epoch": 0.33071373357382117,
        "step": 2567
    },
    {
        "loss": 2.216,
        "grad_norm": 1.9622223377227783,
        "learning_rate": 3.404130029996955e-08,
        "epoch": 0.33084256634887915,
        "step": 2568
    },
    {
        "loss": 2.0446,
        "grad_norm": 1.5105887651443481,
        "learning_rate": 3.1834050181411834e-08,
        "epoch": 0.33097139912393714,
        "step": 2569
    },
    {
        "loss": 2.1319,
        "grad_norm": 1.7453793287277222,
        "learning_rate": 2.9700755225847697e-08,
        "epoch": 0.3311002318989951,
        "step": 2570
    },
    {
        "loss": 2.0294,
        "grad_norm": 2.9188168048858643,
        "learning_rate": 2.7641418590651503e-08,
        "epoch": 0.33122906467405305,
        "step": 2571
    },
    {
        "loss": 2.335,
        "grad_norm": 2.073528528213501,
        "learning_rate": 2.565604332372962e-08,
        "epoch": 0.33135789744911104,
        "step": 2572
    },
    {
        "loss": 1.9646,
        "grad_norm": 1.923977017402649,
        "learning_rate": 2.374463236353708e-08,
        "epoch": 0.331486730224169,
        "step": 2573
    },
    {
        "loss": 2.3402,
        "grad_norm": 2.228008985519409,
        "learning_rate": 2.1907188539038724e-08,
        "epoch": 0.331615562999227,
        "step": 2574
    },
    {
        "loss": 2.1333,
        "grad_norm": 2.09307599067688,
        "learning_rate": 2.014371456973696e-08,
        "epoch": 0.331744395774285,
        "step": 2575
    },
    {
        "loss": 1.9548,
        "grad_norm": 2.5643434524536133,
        "learning_rate": 1.845421306566064e-08,
        "epoch": 0.331873228549343,
        "step": 2576
    },
    {
        "loss": 1.6629,
        "grad_norm": 3.0652785301208496,
        "learning_rate": 1.6838686527348434e-08,
        "epoch": 0.3320020613244009,
        "step": 2577
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.998990535736084,
        "learning_rate": 1.529713734584326e-08,
        "epoch": 0.3321308940994589,
        "step": 2578
    },
    {
        "loss": 2.0632,
        "grad_norm": 2.6354732513427734,
        "learning_rate": 1.3829567802720044e-08,
        "epoch": 0.3322597268745169,
        "step": 2579
    },
    {
        "loss": 2.2736,
        "grad_norm": 2.431586742401123,
        "learning_rate": 1.243598007004132e-08,
        "epoch": 0.33238855964957487,
        "step": 2580
    },
    {
        "loss": 1.7687,
        "grad_norm": 2.9347755908966064,
        "learning_rate": 1.111637621037942e-08,
        "epoch": 0.33251739242463285,
        "step": 2581
    },
    {
        "loss": 2.0618,
        "grad_norm": 2.583451509475708,
        "learning_rate": 9.870758176816485e-09,
        "epoch": 0.3326462251996908,
        "step": 2582
    },
    {
        "loss": 2.2048,
        "grad_norm": 1.9657577276229858,
        "learning_rate": 8.699127812916708e-09,
        "epoch": 0.33277505797474877,
        "step": 2583
    },
    {
        "loss": 2.0173,
        "grad_norm": 1.4930899143218994,
        "learning_rate": 7.601486852742979e-09,
        "epoch": 0.33290389074980675,
        "step": 2584
    },
    {
        "loss": 1.8326,
        "grad_norm": 2.601677894592285,
        "learning_rate": 6.577836920867997e-09,
        "epoch": 0.33303272352486474,
        "step": 2585
    },
    {
        "loss": 1.7811,
        "grad_norm": 2.6717689037323,
        "learning_rate": 5.628179532324307e-09,
        "epoch": 0.3331615562999227,
        "step": 2586
    },
    {
        "loss": 1.7902,
        "grad_norm": 2.439863681793213,
        "learning_rate": 4.7525160926598086e-09,
        "epoch": 0.33329038907498065,
        "step": 2587
    },
    {
        "loss": 2.1868,
        "grad_norm": 1.6325960159301758,
        "learning_rate": 3.950847897893351e-09,
        "epoch": 0.33341922185003864,
        "step": 2588
    },
    {
        "loss": 2.4089,
        "grad_norm": 1.7047110795974731,
        "learning_rate": 3.2231761345258293e-09,
        "epoch": 0.3335480546250966,
        "step": 2589
    },
    {
        "loss": 0.8537,
        "grad_norm": 2.6225059032440186,
        "learning_rate": 2.569501879551295e-09,
        "epoch": 0.3336768874001546,
        "step": 2590
    },
    {
        "loss": 2.2443,
        "grad_norm": 2.4750499725341797,
        "learning_rate": 1.989826100429193e-09,
        "epoch": 0.3338057201752126,
        "step": 2591
    },
    {
        "loss": 1.8249,
        "grad_norm": 2.212399959564209,
        "learning_rate": 1.4841496551176725e-09,
        "epoch": 0.3339345529502705,
        "step": 2592
    },
    {
        "loss": 2.1203,
        "grad_norm": 1.6391369104385376,
        "learning_rate": 1.052473292029177e-09,
        "epoch": 0.3340633857253285,
        "step": 2593
    },
    {
        "loss": 1.7416,
        "grad_norm": 2.826645612716675,
        "learning_rate": 6.94797650069301e-10,
        "epoch": 0.3341922185003865,
        "step": 2594
    },
    {
        "loss": 2.0722,
        "grad_norm": 1.3613660335540771,
        "learning_rate": 4.1112325861458747e-10,
        "epoch": 0.3343210512754445,
        "step": 2595
    },
    {
        "loss": 1.7752,
        "grad_norm": 2.7259628772735596,
        "learning_rate": 2.0145053751807752e-10,
        "epoch": 0.33444988405050247,
        "step": 2596
    },
    {
        "loss": 2.2718,
        "grad_norm": 1.572731375694275,
        "learning_rate": 6.577979709820881e-11,
        "epoch": 0.3345787168255604,
        "step": 2597
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.4413248300552368,
        "learning_rate": 4.111238166570886e-12,
        "epoch": 0.3347075496006184,
        "step": 2598
    },
    {
        "loss": 2.0307,
        "grad_norm": 2.7627763748168945,
        "learning_rate": 9.999998355504802e-05,
        "epoch": 0.33483638237567637,
        "step": 2599
    },
    {
        "loss": 1.9465,
        "grad_norm": 2.223378896713257,
        "learning_rate": 9.999989721907971e-05,
        "epoch": 0.33496521515073435,
        "step": 2600
    },
    {
        "loss": 1.8609,
        "grad_norm": 2.394049882888794,
        "learning_rate": 9.999973688098467e-05,
        "epoch": 0.33509404792579234,
        "step": 2601
    },
    {
        "loss": 1.638,
        "grad_norm": 2.594107151031494,
        "learning_rate": 9.999950254100024e-05,
        "epoch": 0.3352228807008503,
        "step": 2602
    },
    {
        "loss": 1.839,
        "grad_norm": 1.6474696397781372,
        "learning_rate": 9.999919419947324e-05,
        "epoch": 0.33535171347590825,
        "step": 2603
    },
    {
        "loss": 2.1158,
        "grad_norm": 1.6721245050430298,
        "learning_rate": 9.999881185685999e-05,
        "epoch": 0.33548054625096624,
        "step": 2604
    },
    {
        "loss": 1.196,
        "grad_norm": 3.1069774627685547,
        "learning_rate": 9.999835551372644e-05,
        "epoch": 0.3356093790260242,
        "step": 2605
    },
    {
        "loss": 2.3512,
        "grad_norm": 2.5168254375457764,
        "learning_rate": 9.999782517074796e-05,
        "epoch": 0.3357382118010822,
        "step": 2606
    },
    {
        "loss": 2.3324,
        "grad_norm": 2.4996540546417236,
        "learning_rate": 9.999722082870948e-05,
        "epoch": 0.3358670445761402,
        "step": 2607
    },
    {
        "loss": 2.3935,
        "grad_norm": 1.670381784439087,
        "learning_rate": 9.999654248850548e-05,
        "epoch": 0.3359958773511981,
        "step": 2608
    },
    {
        "loss": 1.7959,
        "grad_norm": 3.0346872806549072,
        "learning_rate": 9.999579015113989e-05,
        "epoch": 0.3361247101262561,
        "step": 2609
    },
    {
        "loss": 2.2191,
        "grad_norm": 1.674281358718872,
        "learning_rate": 9.999496381772625e-05,
        "epoch": 0.3362535429013141,
        "step": 2610
    },
    {
        "loss": 2.3003,
        "grad_norm": 2.514194965362549,
        "learning_rate": 9.999406348948754e-05,
        "epoch": 0.3363823756763721,
        "step": 2611
    },
    {
        "loss": 2.0684,
        "grad_norm": 2.656994581222534,
        "learning_rate": 9.999308916775631e-05,
        "epoch": 0.33651120845143007,
        "step": 2612
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.6316006183624268,
        "learning_rate": 9.99920408539746e-05,
        "epoch": 0.336640041226488,
        "step": 2613
    },
    {
        "loss": 2.1814,
        "grad_norm": 2.169579267501831,
        "learning_rate": 9.999091854969393e-05,
        "epoch": 0.336768874001546,
        "step": 2614
    },
    {
        "loss": 2.2048,
        "grad_norm": 1.3390642404556274,
        "learning_rate": 9.99897222565754e-05,
        "epoch": 0.33689770677660397,
        "step": 2615
    },
    {
        "loss": 1.7343,
        "grad_norm": 2.443319320678711,
        "learning_rate": 9.998845197638955e-05,
        "epoch": 0.33702653955166195,
        "step": 2616
    },
    {
        "loss": 2.3743,
        "grad_norm": 1.590423583984375,
        "learning_rate": 9.998710771101647e-05,
        "epoch": 0.33715537232671994,
        "step": 2617
    },
    {
        "loss": 2.1242,
        "grad_norm": 1.8161259889602661,
        "learning_rate": 9.998568946244573e-05,
        "epoch": 0.33728420510177787,
        "step": 2618
    },
    {
        "loss": 1.8275,
        "grad_norm": 1.7858456373214722,
        "learning_rate": 9.998419723277639e-05,
        "epoch": 0.33741303787683585,
        "step": 2619
    },
    {
        "loss": 1.6524,
        "grad_norm": 2.300227403640747,
        "learning_rate": 9.998263102421704e-05,
        "epoch": 0.33754187065189384,
        "step": 2620
    },
    {
        "loss": 2.4187,
        "grad_norm": 1.3504623174667358,
        "learning_rate": 9.998099083908572e-05,
        "epoch": 0.3376707034269518,
        "step": 2621
    },
    {
        "loss": 2.0195,
        "grad_norm": 2.3805227279663086,
        "learning_rate": 9.997927667981e-05,
        "epoch": 0.3377995362020098,
        "step": 2622
    },
    {
        "loss": 2.8124,
        "grad_norm": 1.6610383987426758,
        "learning_rate": 9.99774885489269e-05,
        "epoch": 0.33792836897706774,
        "step": 2623
    },
    {
        "loss": 1.821,
        "grad_norm": 2.6039862632751465,
        "learning_rate": 9.997562644908292e-05,
        "epoch": 0.3380572017521257,
        "step": 2624
    },
    {
        "loss": 2.4549,
        "grad_norm": 1.5198063850402832,
        "learning_rate": 9.997369038303408e-05,
        "epoch": 0.3381860345271837,
        "step": 2625
    },
    {
        "loss": 1.6661,
        "grad_norm": 2.180532932281494,
        "learning_rate": 9.997168035364584e-05,
        "epoch": 0.3383148673022417,
        "step": 2626
    },
    {
        "loss": 2.2692,
        "grad_norm": 1.4952954053878784,
        "learning_rate": 9.996959636389311e-05,
        "epoch": 0.3384437000772997,
        "step": 2627
    },
    {
        "loss": 2.3213,
        "grad_norm": 1.8107846975326538,
        "learning_rate": 9.996743841686034e-05,
        "epoch": 0.33857253285235767,
        "step": 2628
    },
    {
        "loss": 2.0417,
        "grad_norm": 1.8894652128219604,
        "learning_rate": 9.996520651574134e-05,
        "epoch": 0.3387013656274156,
        "step": 2629
    },
    {
        "loss": 2.1947,
        "grad_norm": 1.940087080001831,
        "learning_rate": 9.996290066383944e-05,
        "epoch": 0.3388301984024736,
        "step": 2630
    },
    {
        "loss": 2.3498,
        "grad_norm": 2.044649362564087,
        "learning_rate": 9.996052086456742e-05,
        "epoch": 0.33895903117753157,
        "step": 2631
    },
    {
        "loss": 1.5678,
        "grad_norm": 3.522871732711792,
        "learning_rate": 9.995806712144749e-05,
        "epoch": 0.33908786395258955,
        "step": 2632
    },
    {
        "loss": 1.2375,
        "grad_norm": 2.4142842292785645,
        "learning_rate": 9.995553943811129e-05,
        "epoch": 0.33921669672764754,
        "step": 2633
    },
    {
        "loss": 2.0536,
        "grad_norm": 2.041395902633667,
        "learning_rate": 9.995293781829991e-05,
        "epoch": 0.33934552950270547,
        "step": 2634
    },
    {
        "loss": 2.0951,
        "grad_norm": 1.6804704666137695,
        "learning_rate": 9.995026226586386e-05,
        "epoch": 0.33947436227776345,
        "step": 2635
    },
    {
        "loss": 2.2717,
        "grad_norm": 2.1240673065185547,
        "learning_rate": 9.994751278476308e-05,
        "epoch": 0.33960319505282144,
        "step": 2636
    },
    {
        "loss": 1.8382,
        "grad_norm": 1.9263646602630615,
        "learning_rate": 9.994468937906695e-05,
        "epoch": 0.3397320278278794,
        "step": 2637
    },
    {
        "loss": 1.1222,
        "grad_norm": 3.9288747310638428,
        "learning_rate": 9.994179205295421e-05,
        "epoch": 0.3398608606029374,
        "step": 2638
    },
    {
        "loss": 2.4149,
        "grad_norm": 1.7849291563034058,
        "learning_rate": 9.993882081071306e-05,
        "epoch": 0.33998969337799534,
        "step": 2639
    },
    {
        "loss": 2.0366,
        "grad_norm": 3.0334954261779785,
        "learning_rate": 9.993577565674106e-05,
        "epoch": 0.3401185261530533,
        "step": 2640
    },
    {
        "loss": 2.343,
        "grad_norm": 1.4301278591156006,
        "learning_rate": 9.993265659554518e-05,
        "epoch": 0.3402473589281113,
        "step": 2641
    },
    {
        "loss": 2.09,
        "grad_norm": 2.163670539855957,
        "learning_rate": 9.992946363174176e-05,
        "epoch": 0.3403761917031693,
        "step": 2642
    },
    {
        "loss": 2.2411,
        "grad_norm": 1.8116811513900757,
        "learning_rate": 9.992619677005655e-05,
        "epoch": 0.3405050244782273,
        "step": 2643
    },
    {
        "loss": 1.6265,
        "grad_norm": 2.6848199367523193,
        "learning_rate": 9.992285601532465e-05,
        "epoch": 0.3406338572532852,
        "step": 2644
    },
    {
        "loss": 2.1932,
        "grad_norm": 1.6945964097976685,
        "learning_rate": 9.991944137249052e-05,
        "epoch": 0.3407626900283432,
        "step": 2645
    },
    {
        "loss": 2.2527,
        "grad_norm": 1.535652995109558,
        "learning_rate": 9.991595284660799e-05,
        "epoch": 0.3408915228034012,
        "step": 2646
    },
    {
        "loss": 2.4093,
        "grad_norm": 1.5711114406585693,
        "learning_rate": 9.991239044284024e-05,
        "epoch": 0.34102035557845917,
        "step": 2647
    },
    {
        "loss": 2.1321,
        "grad_norm": 1.9694125652313232,
        "learning_rate": 9.99087541664598e-05,
        "epoch": 0.34114918835351715,
        "step": 2648
    },
    {
        "loss": 1.5598,
        "grad_norm": 2.660637855529785,
        "learning_rate": 9.99050440228485e-05,
        "epoch": 0.3412780211285751,
        "step": 2649
    },
    {
        "loss": 2.3647,
        "grad_norm": 2.995025396347046,
        "learning_rate": 9.990126001749756e-05,
        "epoch": 0.34140685390363307,
        "step": 2650
    },
    {
        "loss": 2.0382,
        "grad_norm": 2.386911153793335,
        "learning_rate": 9.989740215600745e-05,
        "epoch": 0.34153568667869105,
        "step": 2651
    },
    {
        "loss": 2.3447,
        "grad_norm": 1.4564884901046753,
        "learning_rate": 9.989347044408798e-05,
        "epoch": 0.34166451945374904,
        "step": 2652
    },
    {
        "loss": 2.1099,
        "grad_norm": 1.6050105094909668,
        "learning_rate": 9.988946488755826e-05,
        "epoch": 0.341793352228807,
        "step": 2653
    },
    {
        "loss": 2.3216,
        "grad_norm": 1.8580052852630615,
        "learning_rate": 9.988538549234673e-05,
        "epoch": 0.341922185003865,
        "step": 2654
    },
    {
        "loss": 1.7705,
        "grad_norm": 4.414575099945068,
        "learning_rate": 9.988123226449105e-05,
        "epoch": 0.34205101777892294,
        "step": 2655
    },
    {
        "loss": 1.8959,
        "grad_norm": 1.8155070543289185,
        "learning_rate": 9.987700521013818e-05,
        "epoch": 0.3421798505539809,
        "step": 2656
    },
    {
        "loss": 1.4681,
        "grad_norm": 2.378112554550171,
        "learning_rate": 9.987270433554439e-05,
        "epoch": 0.3423086833290389,
        "step": 2657
    },
    {
        "loss": 2.2505,
        "grad_norm": 1.8086248636245728,
        "learning_rate": 9.986832964707513e-05,
        "epoch": 0.3424375161040969,
        "step": 2658
    },
    {
        "loss": 2.2615,
        "grad_norm": 1.7724623680114746,
        "learning_rate": 9.986388115120517e-05,
        "epoch": 0.3425663488791549,
        "step": 2659
    },
    {
        "loss": 2.2117,
        "grad_norm": 2.326137065887451,
        "learning_rate": 9.985935885451847e-05,
        "epoch": 0.3426951816542128,
        "step": 2660
    },
    {
        "loss": 2.4154,
        "grad_norm": 2.0389645099639893,
        "learning_rate": 9.985476276370824e-05,
        "epoch": 0.3428240144292708,
        "step": 2661
    },
    {
        "loss": 2.1465,
        "grad_norm": 2.6288864612579346,
        "learning_rate": 9.985009288557689e-05,
        "epoch": 0.3429528472043288,
        "step": 2662
    },
    {
        "loss": 1.8702,
        "grad_norm": 2.554262161254883,
        "learning_rate": 9.984534922703607e-05,
        "epoch": 0.34308167997938677,
        "step": 2663
    },
    {
        "loss": 2.4721,
        "grad_norm": 1.6640002727508545,
        "learning_rate": 9.984053179510661e-05,
        "epoch": 0.34321051275444475,
        "step": 2664
    },
    {
        "loss": 1.8825,
        "grad_norm": 2.3380677700042725,
        "learning_rate": 9.983564059691852e-05,
        "epoch": 0.3433393455295027,
        "step": 2665
    },
    {
        "loss": 2.3637,
        "grad_norm": 1.4983175992965698,
        "learning_rate": 9.9830675639711e-05,
        "epoch": 0.34346817830456067,
        "step": 2666
    },
    {
        "loss": 1.3232,
        "grad_norm": 3.4338502883911133,
        "learning_rate": 9.982563693083242e-05,
        "epoch": 0.34359701107961865,
        "step": 2667
    },
    {
        "loss": 1.5192,
        "grad_norm": 2.8930506706237793,
        "learning_rate": 9.982052447774028e-05,
        "epoch": 0.34372584385467664,
        "step": 2668
    },
    {
        "loss": 2.212,
        "grad_norm": 1.7922693490982056,
        "learning_rate": 9.981533828800127e-05,
        "epoch": 0.3438546766297346,
        "step": 2669
    },
    {
        "loss": 1.4619,
        "grad_norm": 2.7356951236724854,
        "learning_rate": 9.981007836929117e-05,
        "epoch": 0.34398350940479255,
        "step": 2670
    },
    {
        "loss": 2.0871,
        "grad_norm": 2.1279854774475098,
        "learning_rate": 9.98047447293949e-05,
        "epoch": 0.34411234217985054,
        "step": 2671
    },
    {
        "loss": 2.1496,
        "grad_norm": 2.4685022830963135,
        "learning_rate": 9.979933737620648e-05,
        "epoch": 0.3442411749549085,
        "step": 2672
    },
    {
        "loss": 2.1057,
        "grad_norm": 1.8527157306671143,
        "learning_rate": 9.979385631772906e-05,
        "epoch": 0.3443700077299665,
        "step": 2673
    },
    {
        "loss": 2.3044,
        "grad_norm": 1.976413607597351,
        "learning_rate": 9.978830156207486e-05,
        "epoch": 0.3444988405050245,
        "step": 2674
    },
    {
        "loss": 1.4347,
        "grad_norm": 2.813803195953369,
        "learning_rate": 9.978267311746514e-05,
        "epoch": 0.3446276732800825,
        "step": 2675
    },
    {
        "loss": 2.0271,
        "grad_norm": 2.8201634883880615,
        "learning_rate": 9.977697099223029e-05,
        "epoch": 0.3447565060551404,
        "step": 2676
    },
    {
        "loss": 2.4314,
        "grad_norm": 2.548779249191284,
        "learning_rate": 9.97711951948097e-05,
        "epoch": 0.3448853388301984,
        "step": 2677
    },
    {
        "loss": 2.4727,
        "grad_norm": 1.2666431665420532,
        "learning_rate": 9.97653457337518e-05,
        "epoch": 0.3450141716052564,
        "step": 2678
    },
    {
        "loss": 2.1561,
        "grad_norm": 2.5507516860961914,
        "learning_rate": 9.975942261771408e-05,
        "epoch": 0.34514300438031437,
        "step": 2679
    },
    {
        "loss": 2.3859,
        "grad_norm": 1.4092342853546143,
        "learning_rate": 9.975342585546301e-05,
        "epoch": 0.34527183715537235,
        "step": 2680
    },
    {
        "loss": 1.6084,
        "grad_norm": 3.0239765644073486,
        "learning_rate": 9.974735545587408e-05,
        "epoch": 0.3454006699304303,
        "step": 2681
    },
    {
        "loss": 2.2723,
        "grad_norm": 1.3538248538970947,
        "learning_rate": 9.974121142793174e-05,
        "epoch": 0.34552950270548827,
        "step": 2682
    },
    {
        "loss": 2.1656,
        "grad_norm": 1.443223476409912,
        "learning_rate": 9.973499378072945e-05,
        "epoch": 0.34565833548054625,
        "step": 2683
    },
    {
        "loss": 1.7778,
        "grad_norm": 2.6643662452697754,
        "learning_rate": 9.972870252346962e-05,
        "epoch": 0.34578716825560424,
        "step": 2684
    },
    {
        "loss": 2.4351,
        "grad_norm": 2.0986554622650146,
        "learning_rate": 9.972233766546357e-05,
        "epoch": 0.3459160010306622,
        "step": 2685
    },
    {
        "loss": 2.2988,
        "grad_norm": 1.8401455879211426,
        "learning_rate": 9.971589921613159e-05,
        "epoch": 0.34604483380572015,
        "step": 2686
    },
    {
        "loss": 2.5038,
        "grad_norm": 1.40688955783844,
        "learning_rate": 9.97093871850029e-05,
        "epoch": 0.34617366658077814,
        "step": 2687
    },
    {
        "loss": 1.0747,
        "grad_norm": 2.3578028678894043,
        "learning_rate": 9.970280158171557e-05,
        "epoch": 0.3463024993558361,
        "step": 2688
    },
    {
        "loss": 1.6683,
        "grad_norm": 2.9708974361419678,
        "learning_rate": 9.969614241601659e-05,
        "epoch": 0.3464313321308941,
        "step": 2689
    },
    {
        "loss": 1.4905,
        "grad_norm": 2.058086395263672,
        "learning_rate": 9.968940969776185e-05,
        "epoch": 0.3465601649059521,
        "step": 2690
    },
    {
        "loss": 2.1922,
        "grad_norm": 1.7474638223648071,
        "learning_rate": 9.968260343691612e-05,
        "epoch": 0.34668899768101,
        "step": 2691
    },
    {
        "loss": 2.0065,
        "grad_norm": 2.68937611579895,
        "learning_rate": 9.967572364355289e-05,
        "epoch": 0.346817830456068,
        "step": 2692
    },
    {
        "loss": 1.9624,
        "grad_norm": 1.517551064491272,
        "learning_rate": 9.966877032785463e-05,
        "epoch": 0.346946663231126,
        "step": 2693
    },
    {
        "loss": 2.5195,
        "grad_norm": 1.6354002952575684,
        "learning_rate": 9.966174350011253e-05,
        "epoch": 0.347075496006184,
        "step": 2694
    },
    {
        "loss": 2.3877,
        "grad_norm": 2.146320104598999,
        "learning_rate": 9.965464317072663e-05,
        "epoch": 0.34720432878124197,
        "step": 2695
    },
    {
        "loss": 2.7585,
        "grad_norm": 1.8156894445419312,
        "learning_rate": 9.964746935020574e-05,
        "epoch": 0.3473331615562999,
        "step": 2696
    },
    {
        "loss": 2.1227,
        "grad_norm": 1.522035002708435,
        "learning_rate": 9.964022204916745e-05,
        "epoch": 0.3474619943313579,
        "step": 2697
    },
    {
        "loss": 1.7073,
        "grad_norm": 1.9387397766113281,
        "learning_rate": 9.963290127833809e-05,
        "epoch": 0.34759082710641587,
        "step": 2698
    },
    {
        "loss": 2.3528,
        "grad_norm": 2.142955780029297,
        "learning_rate": 9.962550704855272e-05,
        "epoch": 0.34771965988147385,
        "step": 2699
    },
    {
        "loss": 2.2431,
        "grad_norm": 1.4397997856140137,
        "learning_rate": 9.961803937075516e-05,
        "epoch": 0.34784849265653184,
        "step": 2700
    },
    {
        "loss": 2.3542,
        "grad_norm": 1.3586790561676025,
        "learning_rate": 9.961049825599789e-05,
        "epoch": 0.3479773254315898,
        "step": 2701
    },
    {
        "loss": 2.1345,
        "grad_norm": 1.9670733213424683,
        "learning_rate": 9.960288371544213e-05,
        "epoch": 0.34810615820664775,
        "step": 2702
    },
    {
        "loss": 2.3113,
        "grad_norm": 1.35684335231781,
        "learning_rate": 9.959519576035772e-05,
        "epoch": 0.34823499098170574,
        "step": 2703
    },
    {
        "loss": 2.1709,
        "grad_norm": 1.6510300636291504,
        "learning_rate": 9.958743440212321e-05,
        "epoch": 0.3483638237567637,
        "step": 2704
    },
    {
        "loss": 2.1791,
        "grad_norm": 1.5664684772491455,
        "learning_rate": 9.957959965222575e-05,
        "epoch": 0.3484926565318217,
        "step": 2705
    },
    {
        "loss": 1.7405,
        "grad_norm": 2.386204719543457,
        "learning_rate": 9.957169152226112e-05,
        "epoch": 0.3486214893068797,
        "step": 2706
    },
    {
        "loss": 2.2992,
        "grad_norm": 2.6621289253234863,
        "learning_rate": 9.956371002393373e-05,
        "epoch": 0.3487503220819376,
        "step": 2707
    },
    {
        "loss": 2.3079,
        "grad_norm": 1.9960615634918213,
        "learning_rate": 9.955565516905654e-05,
        "epoch": 0.3488791548569956,
        "step": 2708
    },
    {
        "loss": 1.7121,
        "grad_norm": 2.3085012435913086,
        "learning_rate": 9.954752696955111e-05,
        "epoch": 0.3490079876320536,
        "step": 2709
    },
    {
        "loss": 2.0927,
        "grad_norm": 2.546104669570923,
        "learning_rate": 9.953932543744758e-05,
        "epoch": 0.3491368204071116,
        "step": 2710
    },
    {
        "loss": 1.677,
        "grad_norm": 2.25264573097229,
        "learning_rate": 9.953105058488454e-05,
        "epoch": 0.34926565318216957,
        "step": 2711
    },
    {
        "loss": 1.9799,
        "grad_norm": 2.2636024951934814,
        "learning_rate": 9.952270242410917e-05,
        "epoch": 0.3493944859572275,
        "step": 2712
    },
    {
        "loss": 1.9281,
        "grad_norm": 1.2103174924850464,
        "learning_rate": 9.951428096747713e-05,
        "epoch": 0.3495233187322855,
        "step": 2713
    },
    {
        "loss": 0.7948,
        "grad_norm": 3.5295891761779785,
        "learning_rate": 9.950578622745257e-05,
        "epoch": 0.34965215150734347,
        "step": 2714
    },
    {
        "loss": 2.0633,
        "grad_norm": 2.535047769546509,
        "learning_rate": 9.949721821660807e-05,
        "epoch": 0.34978098428240145,
        "step": 2715
    },
    {
        "loss": 2.1751,
        "grad_norm": 3.346503496170044,
        "learning_rate": 9.948857694762468e-05,
        "epoch": 0.34990981705745944,
        "step": 2716
    },
    {
        "loss": 2.2444,
        "grad_norm": 2.8567330837249756,
        "learning_rate": 9.94798624332919e-05,
        "epoch": 0.35003864983251737,
        "step": 2717
    },
    {
        "loss": 1.7422,
        "grad_norm": 2.493464946746826,
        "learning_rate": 9.947107468650757e-05,
        "epoch": 0.35016748260757535,
        "step": 2718
    },
    {
        "loss": 2.5128,
        "grad_norm": 2.480088949203491,
        "learning_rate": 9.946221372027797e-05,
        "epoch": 0.35029631538263334,
        "step": 2719
    },
    {
        "loss": 2.4597,
        "grad_norm": 1.2517415285110474,
        "learning_rate": 9.945327954771775e-05,
        "epoch": 0.3504251481576913,
        "step": 2720
    },
    {
        "loss": 2.114,
        "grad_norm": 1.4745579957962036,
        "learning_rate": 9.944427218204988e-05,
        "epoch": 0.3505539809327493,
        "step": 2721
    },
    {
        "loss": 2.389,
        "grad_norm": 1.6737812757492065,
        "learning_rate": 9.943519163660567e-05,
        "epoch": 0.35068281370780724,
        "step": 2722
    },
    {
        "loss": 2.3704,
        "grad_norm": 1.7299655675888062,
        "learning_rate": 9.942603792482475e-05,
        "epoch": 0.3508116464828652,
        "step": 2723
    },
    {
        "loss": 1.8868,
        "grad_norm": 2.8118529319763184,
        "learning_rate": 9.941681106025499e-05,
        "epoch": 0.3509404792579232,
        "step": 2724
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.2898551225662231,
        "learning_rate": 9.940751105655265e-05,
        "epoch": 0.3510693120329812,
        "step": 2725
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.7895586490631104,
        "learning_rate": 9.93981379274821e-05,
        "epoch": 0.3511981448080392,
        "step": 2726
    },
    {
        "loss": 2.2275,
        "grad_norm": 1.7901175022125244,
        "learning_rate": 9.9388691686916e-05,
        "epoch": 0.35132697758309717,
        "step": 2727
    },
    {
        "loss": 1.7983,
        "grad_norm": 2.5766761302948,
        "learning_rate": 9.937917234883522e-05,
        "epoch": 0.3514558103581551,
        "step": 2728
    },
    {
        "loss": 2.5842,
        "grad_norm": 1.3471766710281372,
        "learning_rate": 9.936957992732883e-05,
        "epoch": 0.3515846431332131,
        "step": 2729
    },
    {
        "loss": 2.573,
        "grad_norm": 2.2431788444519043,
        "learning_rate": 9.935991443659405e-05,
        "epoch": 0.35171347590827107,
        "step": 2730
    },
    {
        "loss": 2.3251,
        "grad_norm": 1.6002388000488281,
        "learning_rate": 9.935017589093623e-05,
        "epoch": 0.35184230868332905,
        "step": 2731
    },
    {
        "loss": 2.4973,
        "grad_norm": 1.391733169555664,
        "learning_rate": 9.934036430476887e-05,
        "epoch": 0.35197114145838704,
        "step": 2732
    },
    {
        "loss": 1.7487,
        "grad_norm": 2.4195637702941895,
        "learning_rate": 9.933047969261358e-05,
        "epoch": 0.35209997423344497,
        "step": 2733
    },
    {
        "loss": 2.1388,
        "grad_norm": 2.8646326065063477,
        "learning_rate": 9.93205220691e-05,
        "epoch": 0.35222880700850295,
        "step": 2734
    },
    {
        "loss": 1.5926,
        "grad_norm": 2.5199761390686035,
        "learning_rate": 9.931049144896591e-05,
        "epoch": 0.35235763978356094,
        "step": 2735
    },
    {
        "loss": 2.1635,
        "grad_norm": 1.6050431728363037,
        "learning_rate": 9.930038784705706e-05,
        "epoch": 0.3524864725586189,
        "step": 2736
    },
    {
        "loss": 2.2041,
        "grad_norm": 1.4241427183151245,
        "learning_rate": 9.929021127832725e-05,
        "epoch": 0.3526153053336769,
        "step": 2737
    },
    {
        "loss": 2.668,
        "grad_norm": 1.6671299934387207,
        "learning_rate": 9.927996175783824e-05,
        "epoch": 0.35274413810873484,
        "step": 2738
    },
    {
        "loss": 0.9656,
        "grad_norm": 2.760538339614868,
        "learning_rate": 9.926963930075983e-05,
        "epoch": 0.3528729708837928,
        "step": 2739
    },
    {
        "loss": 2.3474,
        "grad_norm": 1.9594786167144775,
        "learning_rate": 9.92592439223697e-05,
        "epoch": 0.3530018036588508,
        "step": 2740
    },
    {
        "loss": 2.2403,
        "grad_norm": 1.2987264394760132,
        "learning_rate": 9.924877563805348e-05,
        "epoch": 0.3531306364339088,
        "step": 2741
    },
    {
        "loss": 1.7182,
        "grad_norm": 1.5388907194137573,
        "learning_rate": 9.923823446330473e-05,
        "epoch": 0.3532594692089668,
        "step": 2742
    },
    {
        "loss": 2.124,
        "grad_norm": 2.3361947536468506,
        "learning_rate": 9.922762041372485e-05,
        "epoch": 0.3533883019840247,
        "step": 2743
    },
    {
        "loss": 2.0818,
        "grad_norm": 3.1028454303741455,
        "learning_rate": 9.921693350502311e-05,
        "epoch": 0.3535171347590827,
        "step": 2744
    },
    {
        "loss": 2.052,
        "grad_norm": 1.9317827224731445,
        "learning_rate": 9.920617375301665e-05,
        "epoch": 0.3536459675341407,
        "step": 2745
    },
    {
        "loss": 1.9368,
        "grad_norm": 2.585820198059082,
        "learning_rate": 9.919534117363038e-05,
        "epoch": 0.35377480030919867,
        "step": 2746
    },
    {
        "loss": 2.175,
        "grad_norm": 2.1765012741088867,
        "learning_rate": 9.9184435782897e-05,
        "epoch": 0.35390363308425665,
        "step": 2747
    },
    {
        "loss": 2.202,
        "grad_norm": 2.028674840927124,
        "learning_rate": 9.9173457596957e-05,
        "epoch": 0.3540324658593146,
        "step": 2748
    },
    {
        "loss": 2.1331,
        "grad_norm": 2.3586819171905518,
        "learning_rate": 9.916240663205857e-05,
        "epoch": 0.35416129863437257,
        "step": 2749
    },
    {
        "loss": 1.8244,
        "grad_norm": 2.7217984199523926,
        "learning_rate": 9.91512829045577e-05,
        "epoch": 0.35429013140943055,
        "step": 2750
    },
    {
        "loss": 2.2751,
        "grad_norm": 1.8928604125976562,
        "learning_rate": 9.914008643091797e-05,
        "epoch": 0.35441896418448854,
        "step": 2751
    },
    {
        "loss": 2.0889,
        "grad_norm": 2.1286845207214355,
        "learning_rate": 9.912881722771065e-05,
        "epoch": 0.3545477969595465,
        "step": 2752
    },
    {
        "loss": 1.9587,
        "grad_norm": 1.6181920766830444,
        "learning_rate": 9.911747531161473e-05,
        "epoch": 0.3546766297346045,
        "step": 2753
    },
    {
        "loss": 2.3115,
        "grad_norm": 1.9752944707870483,
        "learning_rate": 9.910606069941671e-05,
        "epoch": 0.35480546250966244,
        "step": 2754
    },
    {
        "loss": 2.0737,
        "grad_norm": 2.163874626159668,
        "learning_rate": 9.909457340801078e-05,
        "epoch": 0.3549342952847204,
        "step": 2755
    },
    {
        "loss": 1.9224,
        "grad_norm": 1.4802961349487305,
        "learning_rate": 9.908301345439862e-05,
        "epoch": 0.3550631280597784,
        "step": 2756
    },
    {
        "loss": 2.4541,
        "grad_norm": 1.7893339395523071,
        "learning_rate": 9.907138085568952e-05,
        "epoch": 0.3551919608348364,
        "step": 2757
    },
    {
        "loss": 2.1542,
        "grad_norm": 1.684634804725647,
        "learning_rate": 9.905967562910021e-05,
        "epoch": 0.3553207936098944,
        "step": 2758
    },
    {
        "loss": 2.2411,
        "grad_norm": 2.409226894378662,
        "learning_rate": 9.904789779195503e-05,
        "epoch": 0.3554496263849523,
        "step": 2759
    },
    {
        "loss": 2.3437,
        "grad_norm": 1.7117609977722168,
        "learning_rate": 9.903604736168565e-05,
        "epoch": 0.3555784591600103,
        "step": 2760
    },
    {
        "loss": 1.8391,
        "grad_norm": 1.8596678972244263,
        "learning_rate": 9.902412435583129e-05,
        "epoch": 0.3557072919350683,
        "step": 2761
    },
    {
        "loss": 2.0979,
        "grad_norm": 1.9578992128372192,
        "learning_rate": 9.90121287920385e-05,
        "epoch": 0.35583612471012627,
        "step": 2762
    },
    {
        "loss": 2.0754,
        "grad_norm": 2.2483818531036377,
        "learning_rate": 9.900006068806129e-05,
        "epoch": 0.35596495748518425,
        "step": 2763
    },
    {
        "loss": 2.0943,
        "grad_norm": 1.6518034934997559,
        "learning_rate": 9.898792006176098e-05,
        "epoch": 0.3560937902602422,
        "step": 2764
    },
    {
        "loss": 2.1828,
        "grad_norm": 1.8771662712097168,
        "learning_rate": 9.89757069311063e-05,
        "epoch": 0.35622262303530017,
        "step": 2765
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.9568588733673096,
        "learning_rate": 9.89634213141732e-05,
        "epoch": 0.35635145581035815,
        "step": 2766
    },
    {
        "loss": 1.8021,
        "grad_norm": 2.650362491607666,
        "learning_rate": 9.895106322914496e-05,
        "epoch": 0.35648028858541614,
        "step": 2767
    },
    {
        "loss": 1.9541,
        "grad_norm": 2.10746431350708,
        "learning_rate": 9.893863269431212e-05,
        "epoch": 0.3566091213604741,
        "step": 2768
    },
    {
        "loss": 1.3483,
        "grad_norm": 2.768953800201416,
        "learning_rate": 9.892612972807241e-05,
        "epoch": 0.35673795413553205,
        "step": 2769
    },
    {
        "loss": 1.0901,
        "grad_norm": 3.2200233936309814,
        "learning_rate": 9.891355434893083e-05,
        "epoch": 0.35686678691059004,
        "step": 2770
    },
    {
        "loss": 1.8294,
        "grad_norm": 2.5033717155456543,
        "learning_rate": 9.890090657549948e-05,
        "epoch": 0.356995619685648,
        "step": 2771
    },
    {
        "loss": 2.3672,
        "grad_norm": 1.498490810394287,
        "learning_rate": 9.888818642649766e-05,
        "epoch": 0.357124452460706,
        "step": 2772
    },
    {
        "loss": 2.397,
        "grad_norm": 1.798477292060852,
        "learning_rate": 9.887539392075176e-05,
        "epoch": 0.357253285235764,
        "step": 2773
    },
    {
        "loss": 2.1297,
        "grad_norm": 2.627206563949585,
        "learning_rate": 9.886252907719528e-05,
        "epoch": 0.357382118010822,
        "step": 2774
    },
    {
        "loss": 1.4018,
        "grad_norm": 3.764799118041992,
        "learning_rate": 9.884959191486875e-05,
        "epoch": 0.3575109507858799,
        "step": 2775
    },
    {
        "loss": 2.3199,
        "grad_norm": 1.7506588697433472,
        "learning_rate": 9.88365824529198e-05,
        "epoch": 0.3576397835609379,
        "step": 2776
    },
    {
        "loss": 1.5967,
        "grad_norm": 1.7962465286254883,
        "learning_rate": 9.882350071060302e-05,
        "epoch": 0.3577686163359959,
        "step": 2777
    },
    {
        "loss": 2.3648,
        "grad_norm": 1.5789995193481445,
        "learning_rate": 9.881034670727995e-05,
        "epoch": 0.35789744911105387,
        "step": 2778
    },
    {
        "loss": 2.1046,
        "grad_norm": 3.2524099349975586,
        "learning_rate": 9.879712046241914e-05,
        "epoch": 0.35802628188611185,
        "step": 2779
    },
    {
        "loss": 2.3774,
        "grad_norm": 1.7207176685333252,
        "learning_rate": 9.878382199559605e-05,
        "epoch": 0.3581551146611698,
        "step": 2780
    },
    {
        "loss": 1.63,
        "grad_norm": 2.098118305206299,
        "learning_rate": 9.877045132649299e-05,
        "epoch": 0.35828394743622777,
        "step": 2781
    },
    {
        "loss": 1.8482,
        "grad_norm": 2.629457473754883,
        "learning_rate": 9.875700847489914e-05,
        "epoch": 0.35841278021128575,
        "step": 2782
    },
    {
        "loss": 2.3459,
        "grad_norm": 1.8694512844085693,
        "learning_rate": 9.874349346071061e-05,
        "epoch": 0.35854161298634374,
        "step": 2783
    },
    {
        "loss": 1.9338,
        "grad_norm": 2.0595476627349854,
        "learning_rate": 9.872990630393015e-05,
        "epoch": 0.3586704457614017,
        "step": 2784
    },
    {
        "loss": 2.2944,
        "grad_norm": 1.541319489479065,
        "learning_rate": 9.871624702466743e-05,
        "epoch": 0.35879927853645965,
        "step": 2785
    },
    {
        "loss": 1.8442,
        "grad_norm": 2.6943678855895996,
        "learning_rate": 9.870251564313879e-05,
        "epoch": 0.35892811131151764,
        "step": 2786
    },
    {
        "loss": 2.4851,
        "grad_norm": 2.2085041999816895,
        "learning_rate": 9.868871217966728e-05,
        "epoch": 0.3590569440865756,
        "step": 2787
    },
    {
        "loss": 1.9854,
        "grad_norm": 2.674402952194214,
        "learning_rate": 9.867483665468267e-05,
        "epoch": 0.3591857768616336,
        "step": 2788
    },
    {
        "loss": 2.4059,
        "grad_norm": 1.741632103919983,
        "learning_rate": 9.866088908872138e-05,
        "epoch": 0.3593146096366916,
        "step": 2789
    },
    {
        "loss": 1.5681,
        "grad_norm": 2.516456365585327,
        "learning_rate": 9.864686950242643e-05,
        "epoch": 0.3594434424117495,
        "step": 2790
    },
    {
        "loss": 2.2275,
        "grad_norm": 2.4492948055267334,
        "learning_rate": 9.863277791654746e-05,
        "epoch": 0.3595722751868075,
        "step": 2791
    },
    {
        "loss": 2.3143,
        "grad_norm": 1.913993239402771,
        "learning_rate": 9.861861435194063e-05,
        "epoch": 0.3597011079618655,
        "step": 2792
    },
    {
        "loss": 2.2991,
        "grad_norm": 1.786195993423462,
        "learning_rate": 9.86043788295687e-05,
        "epoch": 0.3598299407369235,
        "step": 2793
    },
    {
        "loss": 2.0508,
        "grad_norm": 2.3189942836761475,
        "learning_rate": 9.859007137050086e-05,
        "epoch": 0.35995877351198147,
        "step": 2794
    },
    {
        "loss": 2.1611,
        "grad_norm": 1.9357514381408691,
        "learning_rate": 9.857569199591284e-05,
        "epoch": 0.3600876062870394,
        "step": 2795
    },
    {
        "loss": 2.6124,
        "grad_norm": 1.0493322610855103,
        "learning_rate": 9.85612407270867e-05,
        "epoch": 0.3602164390620974,
        "step": 2796
    },
    {
        "loss": 2.1238,
        "grad_norm": 1.8741716146469116,
        "learning_rate": 9.854671758541106e-05,
        "epoch": 0.36034527183715537,
        "step": 2797
    },
    {
        "loss": 2.4923,
        "grad_norm": 2.5657782554626465,
        "learning_rate": 9.853212259238077e-05,
        "epoch": 0.36047410461221335,
        "step": 2798
    },
    {
        "loss": 2.1258,
        "grad_norm": 2.9247074127197266,
        "learning_rate": 9.851745576959712e-05,
        "epoch": 0.36060293738727134,
        "step": 2799
    },
    {
        "loss": 2.0921,
        "grad_norm": 1.8790338039398193,
        "learning_rate": 9.850271713876765e-05,
        "epoch": 0.3607317701623293,
        "step": 2800
    },
    {
        "loss": 2.386,
        "grad_norm": 2.185642957687378,
        "learning_rate": 9.848790672170623e-05,
        "epoch": 0.36086060293738725,
        "step": 2801
    },
    {
        "loss": 2.1198,
        "grad_norm": 2.142404079437256,
        "learning_rate": 9.847302454033293e-05,
        "epoch": 0.36098943571244524,
        "step": 2802
    },
    {
        "loss": 2.1538,
        "grad_norm": 1.8220292329788208,
        "learning_rate": 9.845807061667409e-05,
        "epoch": 0.3611182684875032,
        "step": 2803
    },
    {
        "loss": 1.5272,
        "grad_norm": 3.071749448776245,
        "learning_rate": 9.844304497286216e-05,
        "epoch": 0.3612471012625612,
        "step": 2804
    },
    {
        "loss": 1.6478,
        "grad_norm": 2.456571102142334,
        "learning_rate": 9.842794763113579e-05,
        "epoch": 0.3613759340376192,
        "step": 2805
    },
    {
        "loss": 2.3161,
        "grad_norm": 1.7347655296325684,
        "learning_rate": 9.841277861383974e-05,
        "epoch": 0.3615047668126771,
        "step": 2806
    },
    {
        "loss": 2.4258,
        "grad_norm": 1.4964159727096558,
        "learning_rate": 9.839753794342486e-05,
        "epoch": 0.3616335995877351,
        "step": 2807
    },
    {
        "loss": 2.1476,
        "grad_norm": 2.588643789291382,
        "learning_rate": 9.8382225642448e-05,
        "epoch": 0.3617624323627931,
        "step": 2808
    },
    {
        "loss": 1.8093,
        "grad_norm": 2.218425989151001,
        "learning_rate": 9.836684173357208e-05,
        "epoch": 0.3618912651378511,
        "step": 2809
    },
    {
        "loss": 2.2455,
        "grad_norm": 1.564611554145813,
        "learning_rate": 9.835138623956601e-05,
        "epoch": 0.36202009791290907,
        "step": 2810
    },
    {
        "loss": 2.0766,
        "grad_norm": 1.8354113101959229,
        "learning_rate": 9.833585918330461e-05,
        "epoch": 0.362148930687967,
        "step": 2811
    },
    {
        "loss": 2.1542,
        "grad_norm": 2.551480531692505,
        "learning_rate": 9.832026058776862e-05,
        "epoch": 0.362277763463025,
        "step": 2812
    },
    {
        "loss": 1.3607,
        "grad_norm": 2.8059909343719482,
        "learning_rate": 9.830459047604469e-05,
        "epoch": 0.36240659623808297,
        "step": 2813
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.6626701354980469,
        "learning_rate": 9.828884887132527e-05,
        "epoch": 0.36253542901314095,
        "step": 2814
    },
    {
        "loss": 2.5349,
        "grad_norm": 1.3164939880371094,
        "learning_rate": 9.827303579690869e-05,
        "epoch": 0.36266426178819894,
        "step": 2815
    },
    {
        "loss": 2.1408,
        "grad_norm": 1.3454408645629883,
        "learning_rate": 9.8257151276199e-05,
        "epoch": 0.36279309456325687,
        "step": 2816
    },
    {
        "loss": 1.8328,
        "grad_norm": 2.2854275703430176,
        "learning_rate": 9.824119533270602e-05,
        "epoch": 0.36292192733831485,
        "step": 2817
    },
    {
        "loss": 1.7502,
        "grad_norm": 2.6953728199005127,
        "learning_rate": 9.822516799004524e-05,
        "epoch": 0.36305076011337284,
        "step": 2818
    },
    {
        "loss": 1.8198,
        "grad_norm": 2.1661882400512695,
        "learning_rate": 9.820906927193791e-05,
        "epoch": 0.3631795928884308,
        "step": 2819
    },
    {
        "loss": 2.1649,
        "grad_norm": 1.725887656211853,
        "learning_rate": 9.819289920221086e-05,
        "epoch": 0.3633084256634888,
        "step": 2820
    },
    {
        "loss": 2.4577,
        "grad_norm": 1.9249317646026611,
        "learning_rate": 9.81766578047965e-05,
        "epoch": 0.36343725843854674,
        "step": 2821
    },
    {
        "loss": 2.3495,
        "grad_norm": 1.611014485359192,
        "learning_rate": 9.816034510373286e-05,
        "epoch": 0.3635660912136047,
        "step": 2822
    },
    {
        "loss": 1.8928,
        "grad_norm": 1.8724470138549805,
        "learning_rate": 9.814396112316348e-05,
        "epoch": 0.3636949239886627,
        "step": 2823
    },
    {
        "loss": 2.0587,
        "grad_norm": 2.664947032928467,
        "learning_rate": 9.812750588733737e-05,
        "epoch": 0.3638237567637207,
        "step": 2824
    },
    {
        "loss": 1.6768,
        "grad_norm": 1.9353407621383667,
        "learning_rate": 9.811097942060908e-05,
        "epoch": 0.3639525895387787,
        "step": 2825
    },
    {
        "loss": 1.8654,
        "grad_norm": 1.7682411670684814,
        "learning_rate": 9.809438174743848e-05,
        "epoch": 0.36408142231383667,
        "step": 2826
    },
    {
        "loss": 2.3628,
        "grad_norm": 2.1425445079803467,
        "learning_rate": 9.807771289239092e-05,
        "epoch": 0.3642102550888946,
        "step": 2827
    },
    {
        "loss": 1.835,
        "grad_norm": 1.541895866394043,
        "learning_rate": 9.806097288013705e-05,
        "epoch": 0.3643390878639526,
        "step": 2828
    },
    {
        "loss": 1.3626,
        "grad_norm": 3.377312660217285,
        "learning_rate": 9.804416173545287e-05,
        "epoch": 0.36446792063901057,
        "step": 2829
    },
    {
        "loss": 1.8268,
        "grad_norm": 3.5065457820892334,
        "learning_rate": 9.802727948321961e-05,
        "epoch": 0.36459675341406855,
        "step": 2830
    },
    {
        "loss": 1.8583,
        "grad_norm": 2.044921398162842,
        "learning_rate": 9.801032614842378e-05,
        "epoch": 0.36472558618912654,
        "step": 2831
    },
    {
        "loss": 1.9724,
        "grad_norm": 1.436211109161377,
        "learning_rate": 9.799330175615711e-05,
        "epoch": 0.36485441896418447,
        "step": 2832
    },
    {
        "loss": 1.7572,
        "grad_norm": 4.2092061042785645,
        "learning_rate": 9.797620633161646e-05,
        "epoch": 0.36498325173924245,
        "step": 2833
    },
    {
        "loss": 2.1371,
        "grad_norm": 1.8478901386260986,
        "learning_rate": 9.795903990010385e-05,
        "epoch": 0.36511208451430044,
        "step": 2834
    },
    {
        "loss": 2.1036,
        "grad_norm": 2.856921911239624,
        "learning_rate": 9.794180248702636e-05,
        "epoch": 0.3652409172893584,
        "step": 2835
    },
    {
        "loss": 2.276,
        "grad_norm": 1.5796940326690674,
        "learning_rate": 9.792449411789616e-05,
        "epoch": 0.3653697500644164,
        "step": 2836
    },
    {
        "loss": 1.3114,
        "grad_norm": 3.40042781829834,
        "learning_rate": 9.790711481833042e-05,
        "epoch": 0.36549858283947434,
        "step": 2837
    },
    {
        "loss": 2.5903,
        "grad_norm": 1.9228070974349976,
        "learning_rate": 9.78896646140513e-05,
        "epoch": 0.3656274156145323,
        "step": 2838
    },
    {
        "loss": 2.1845,
        "grad_norm": 1.4344701766967773,
        "learning_rate": 9.78721435308859e-05,
        "epoch": 0.3657562483895903,
        "step": 2839
    },
    {
        "loss": 1.7995,
        "grad_norm": 2.0607080459594727,
        "learning_rate": 9.78545515947662e-05,
        "epoch": 0.3658850811646483,
        "step": 2840
    },
    {
        "loss": 2.0569,
        "grad_norm": 2.1521220207214355,
        "learning_rate": 9.783688883172913e-05,
        "epoch": 0.3660139139397063,
        "step": 2841
    },
    {
        "loss": 1.6382,
        "grad_norm": 3.0225632190704346,
        "learning_rate": 9.78191552679163e-05,
        "epoch": 0.3661427467147642,
        "step": 2842
    },
    {
        "loss": 2.1686,
        "grad_norm": 1.3745129108428955,
        "learning_rate": 9.780135092957423e-05,
        "epoch": 0.3662715794898222,
        "step": 2843
    },
    {
        "loss": 2.0571,
        "grad_norm": 2.7294862270355225,
        "learning_rate": 9.778347584305416e-05,
        "epoch": 0.3664004122648802,
        "step": 2844
    },
    {
        "loss": 2.0358,
        "grad_norm": 1.9039344787597656,
        "learning_rate": 9.776553003481204e-05,
        "epoch": 0.36652924503993817,
        "step": 2845
    },
    {
        "loss": 2.4017,
        "grad_norm": 2.030505657196045,
        "learning_rate": 9.774751353140848e-05,
        "epoch": 0.36665807781499615,
        "step": 2846
    },
    {
        "loss": 1.5002,
        "grad_norm": 3.025675058364868,
        "learning_rate": 9.77294263595087e-05,
        "epoch": 0.3667869105900541,
        "step": 2847
    },
    {
        "loss": 1.6478,
        "grad_norm": 2.47932505607605,
        "learning_rate": 9.771126854588255e-05,
        "epoch": 0.36691574336511207,
        "step": 2848
    },
    {
        "loss": 2.07,
        "grad_norm": 1.7772531509399414,
        "learning_rate": 9.769304011740444e-05,
        "epoch": 0.36704457614017005,
        "step": 2849
    },
    {
        "loss": 2.4266,
        "grad_norm": 1.84674870967865,
        "learning_rate": 9.767474110105325e-05,
        "epoch": 0.36717340891522804,
        "step": 2850
    },
    {
        "loss": 2.4523,
        "grad_norm": 1.8016853332519531,
        "learning_rate": 9.765637152391238e-05,
        "epoch": 0.367302241690286,
        "step": 2851
    },
    {
        "loss": 2.3966,
        "grad_norm": 1.6203802824020386,
        "learning_rate": 9.763793141316964e-05,
        "epoch": 0.367431074465344,
        "step": 2852
    },
    {
        "loss": 2.392,
        "grad_norm": 1.671492099761963,
        "learning_rate": 9.761942079611722e-05,
        "epoch": 0.36755990724040194,
        "step": 2853
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.547855257987976,
        "learning_rate": 9.760083970015168e-05,
        "epoch": 0.3676887400154599,
        "step": 2854
    },
    {
        "loss": 2.2007,
        "grad_norm": 2.8967573642730713,
        "learning_rate": 9.75821881527739e-05,
        "epoch": 0.3678175727905179,
        "step": 2855
    },
    {
        "loss": 1.715,
        "grad_norm": 2.486436128616333,
        "learning_rate": 9.7563466181589e-05,
        "epoch": 0.3679464055655759,
        "step": 2856
    },
    {
        "loss": 2.439,
        "grad_norm": 1.2919836044311523,
        "learning_rate": 9.754467381430637e-05,
        "epoch": 0.3680752383406339,
        "step": 2857
    },
    {
        "loss": 2.0241,
        "grad_norm": 2.0237491130828857,
        "learning_rate": 9.752581107873957e-05,
        "epoch": 0.3682040711156918,
        "step": 2858
    },
    {
        "loss": 2.2106,
        "grad_norm": 2.0088424682617188,
        "learning_rate": 9.75068780028063e-05,
        "epoch": 0.3683329038907498,
        "step": 2859
    },
    {
        "loss": 1.6647,
        "grad_norm": 2.319840431213379,
        "learning_rate": 9.748787461452837e-05,
        "epoch": 0.3684617366658078,
        "step": 2860
    },
    {
        "loss": 2.5199,
        "grad_norm": 1.8950083255767822,
        "learning_rate": 9.746880094203167e-05,
        "epoch": 0.36859056944086577,
        "step": 2861
    },
    {
        "loss": 2.2362,
        "grad_norm": 2.0822200775146484,
        "learning_rate": 9.744965701354609e-05,
        "epoch": 0.36871940221592375,
        "step": 2862
    },
    {
        "loss": 2.0305,
        "grad_norm": 2.7362523078918457,
        "learning_rate": 9.743044285740555e-05,
        "epoch": 0.3688482349909817,
        "step": 2863
    },
    {
        "loss": 2.0265,
        "grad_norm": 1.8519484996795654,
        "learning_rate": 9.741115850204785e-05,
        "epoch": 0.36897706776603967,
        "step": 2864
    },
    {
        "loss": 1.4862,
        "grad_norm": 2.141270875930786,
        "learning_rate": 9.739180397601471e-05,
        "epoch": 0.36910590054109765,
        "step": 2865
    },
    {
        "loss": 2.3822,
        "grad_norm": 1.786085605621338,
        "learning_rate": 9.737237930795175e-05,
        "epoch": 0.36923473331615564,
        "step": 2866
    },
    {
        "loss": 2.1848,
        "grad_norm": 1.9614017009735107,
        "learning_rate": 9.735288452660832e-05,
        "epoch": 0.3693635660912136,
        "step": 2867
    },
    {
        "loss": 2.0109,
        "grad_norm": 1.8193018436431885,
        "learning_rate": 9.733331966083758e-05,
        "epoch": 0.36949239886627155,
        "step": 2868
    },
    {
        "loss": 2.3256,
        "grad_norm": 2.2502450942993164,
        "learning_rate": 9.731368473959645e-05,
        "epoch": 0.36962123164132954,
        "step": 2869
    },
    {
        "loss": 2.2877,
        "grad_norm": 1.4128962755203247,
        "learning_rate": 9.729397979194554e-05,
        "epoch": 0.3697500644163875,
        "step": 2870
    },
    {
        "loss": 1.7568,
        "grad_norm": 2.4489738941192627,
        "learning_rate": 9.727420484704898e-05,
        "epoch": 0.3698788971914455,
        "step": 2871
    },
    {
        "loss": 1.9138,
        "grad_norm": 3.864619493484497,
        "learning_rate": 9.725435993417468e-05,
        "epoch": 0.3700077299665035,
        "step": 2872
    },
    {
        "loss": 1.7366,
        "grad_norm": 1.9977606534957886,
        "learning_rate": 9.723444508269396e-05,
        "epoch": 0.3701365627415614,
        "step": 2873
    },
    {
        "loss": 1.7206,
        "grad_norm": 2.414245843887329,
        "learning_rate": 9.721446032208173e-05,
        "epoch": 0.3702653955166194,
        "step": 2874
    },
    {
        "loss": 2.3162,
        "grad_norm": 1.5368674993515015,
        "learning_rate": 9.719440568191633e-05,
        "epoch": 0.3703942282916774,
        "step": 2875
    },
    {
        "loss": 2.4329,
        "grad_norm": 1.2309578657150269,
        "learning_rate": 9.71742811918796e-05,
        "epoch": 0.3705230610667354,
        "step": 2876
    },
    {
        "loss": 1.0221,
        "grad_norm": 2.35929012298584,
        "learning_rate": 9.715408688175662e-05,
        "epoch": 0.37065189384179337,
        "step": 2877
    },
    {
        "loss": 2.0543,
        "grad_norm": 2.928220272064209,
        "learning_rate": 9.71338227814359e-05,
        "epoch": 0.37078072661685135,
        "step": 2878
    },
    {
        "loss": 2.0789,
        "grad_norm": 2.3946645259857178,
        "learning_rate": 9.71134889209093e-05,
        "epoch": 0.3709095593919093,
        "step": 2879
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.27640438079834,
        "learning_rate": 9.709308533027179e-05,
        "epoch": 0.37103839216696727,
        "step": 2880
    },
    {
        "loss": 2.3415,
        "grad_norm": 2.033597469329834,
        "learning_rate": 9.707261203972166e-05,
        "epoch": 0.37116722494202525,
        "step": 2881
    },
    {
        "loss": 2.5354,
        "grad_norm": 1.49038827419281,
        "learning_rate": 9.705206907956029e-05,
        "epoch": 0.37129605771708324,
        "step": 2882
    },
    {
        "loss": 2.3317,
        "grad_norm": 2.277731418609619,
        "learning_rate": 9.70314564801922e-05,
        "epoch": 0.3714248904921412,
        "step": 2883
    },
    {
        "loss": 1.8413,
        "grad_norm": 2.161328077316284,
        "learning_rate": 9.701077427212498e-05,
        "epoch": 0.37155372326719915,
        "step": 2884
    },
    {
        "loss": 2.2071,
        "grad_norm": 1.9542968273162842,
        "learning_rate": 9.699002248596924e-05,
        "epoch": 0.37168255604225714,
        "step": 2885
    },
    {
        "loss": 2.0426,
        "grad_norm": 2.114170551300049,
        "learning_rate": 9.696920115243856e-05,
        "epoch": 0.3718113888173151,
        "step": 2886
    },
    {
        "loss": 2.3267,
        "grad_norm": 2.2828540802001953,
        "learning_rate": 9.694831030234947e-05,
        "epoch": 0.3719402215923731,
        "step": 2887
    },
    {
        "loss": 1.7772,
        "grad_norm": 2.217285633087158,
        "learning_rate": 9.69273499666214e-05,
        "epoch": 0.3720690543674311,
        "step": 2888
    },
    {
        "loss": 1.1584,
        "grad_norm": 2.8990118503570557,
        "learning_rate": 9.690632017627656e-05,
        "epoch": 0.372197887142489,
        "step": 2889
    },
    {
        "loss": 1.9081,
        "grad_norm": 5.296224594116211,
        "learning_rate": 9.688522096244005e-05,
        "epoch": 0.372326719917547,
        "step": 2890
    },
    {
        "loss": 2.3188,
        "grad_norm": 1.4867700338363647,
        "learning_rate": 9.686405235633963e-05,
        "epoch": 0.372455552692605,
        "step": 2891
    },
    {
        "loss": 0.9989,
        "grad_norm": 3.359527826309204,
        "learning_rate": 9.684281438930581e-05,
        "epoch": 0.372584385467663,
        "step": 2892
    },
    {
        "loss": 2.3931,
        "grad_norm": 1.3432350158691406,
        "learning_rate": 9.682150709277173e-05,
        "epoch": 0.37271321824272097,
        "step": 2893
    },
    {
        "loss": 2.084,
        "grad_norm": 3.5702743530273438,
        "learning_rate": 9.68001304982732e-05,
        "epoch": 0.3728420510177789,
        "step": 2894
    },
    {
        "loss": 2.1319,
        "grad_norm": 2.80908465385437,
        "learning_rate": 9.677868463744854e-05,
        "epoch": 0.3729708837928369,
        "step": 2895
    },
    {
        "loss": 2.0277,
        "grad_norm": 2.566617012023926,
        "learning_rate": 9.67571695420386e-05,
        "epoch": 0.37309971656789487,
        "step": 2896
    },
    {
        "loss": 2.7424,
        "grad_norm": 1.8085771799087524,
        "learning_rate": 9.673558524388668e-05,
        "epoch": 0.37322854934295285,
        "step": 2897
    },
    {
        "loss": 1.7529,
        "grad_norm": 2.128852128982544,
        "learning_rate": 9.671393177493854e-05,
        "epoch": 0.37335738211801084,
        "step": 2898
    },
    {
        "loss": 2.1834,
        "grad_norm": 1.2593506574630737,
        "learning_rate": 9.669220916724233e-05,
        "epoch": 0.3734862148930688,
        "step": 2899
    },
    {
        "loss": 1.8272,
        "grad_norm": 2.3143749237060547,
        "learning_rate": 9.667041745294845e-05,
        "epoch": 0.37361504766812675,
        "step": 2900
    },
    {
        "loss": 1.9949,
        "grad_norm": 3.7697887420654297,
        "learning_rate": 9.664855666430967e-05,
        "epoch": 0.37374388044318474,
        "step": 2901
    },
    {
        "loss": 2.2049,
        "grad_norm": 1.4987554550170898,
        "learning_rate": 9.662662683368094e-05,
        "epoch": 0.3738727132182427,
        "step": 2902
    },
    {
        "loss": 2.2123,
        "grad_norm": 3.0078160762786865,
        "learning_rate": 9.66046279935194e-05,
        "epoch": 0.3740015459933007,
        "step": 2903
    },
    {
        "loss": 1.9858,
        "grad_norm": 2.096078872680664,
        "learning_rate": 9.658256017638434e-05,
        "epoch": 0.3741303787683587,
        "step": 2904
    },
    {
        "loss": 1.6382,
        "grad_norm": 2.6245317459106445,
        "learning_rate": 9.656042341493711e-05,
        "epoch": 0.3742592115434166,
        "step": 2905
    },
    {
        "loss": 2.1547,
        "grad_norm": 1.8389382362365723,
        "learning_rate": 9.653821774194121e-05,
        "epoch": 0.3743880443184746,
        "step": 2906
    },
    {
        "loss": 1.6776,
        "grad_norm": 3.7914023399353027,
        "learning_rate": 9.651594319026196e-05,
        "epoch": 0.3745168770935326,
        "step": 2907
    },
    {
        "loss": 2.1173,
        "grad_norm": 2.1779232025146484,
        "learning_rate": 9.649359979286674e-05,
        "epoch": 0.3746457098685906,
        "step": 2908
    },
    {
        "loss": 1.2718,
        "grad_norm": 3.616908073425293,
        "learning_rate": 9.647118758282481e-05,
        "epoch": 0.37477454264364857,
        "step": 2909
    },
    {
        "loss": 1.5026,
        "grad_norm": 2.649522066116333,
        "learning_rate": 9.644870659330724e-05,
        "epoch": 0.3749033754187065,
        "step": 2910
    },
    {
        "loss": 2.2149,
        "grad_norm": 1.5221376419067383,
        "learning_rate": 9.642615685758694e-05,
        "epoch": 0.3750322081937645,
        "step": 2911
    },
    {
        "loss": 2.1381,
        "grad_norm": 1.7562719583511353,
        "learning_rate": 9.640353840903854e-05,
        "epoch": 0.37516104096882247,
        "step": 2912
    },
    {
        "loss": 2.3296,
        "grad_norm": 1.5229215621948242,
        "learning_rate": 9.638085128113836e-05,
        "epoch": 0.37528987374388045,
        "step": 2913
    },
    {
        "loss": 2.3075,
        "grad_norm": 1.7956963777542114,
        "learning_rate": 9.63580955074644e-05,
        "epoch": 0.37541870651893844,
        "step": 2914
    },
    {
        "loss": 1.5942,
        "grad_norm": 2.4759559631347656,
        "learning_rate": 9.633527112169627e-05,
        "epoch": 0.37554753929399637,
        "step": 2915
    },
    {
        "loss": 1.7951,
        "grad_norm": 1.6067981719970703,
        "learning_rate": 9.631237815761503e-05,
        "epoch": 0.37567637206905435,
        "step": 2916
    },
    {
        "loss": 2.3721,
        "grad_norm": 1.6454652547836304,
        "learning_rate": 9.628941664910336e-05,
        "epoch": 0.37580520484411234,
        "step": 2917
    },
    {
        "loss": 2.1943,
        "grad_norm": 2.4937710762023926,
        "learning_rate": 9.626638663014533e-05,
        "epoch": 0.3759340376191703,
        "step": 2918
    },
    {
        "loss": 1.9753,
        "grad_norm": 2.9296722412109375,
        "learning_rate": 9.624328813482642e-05,
        "epoch": 0.3760628703942283,
        "step": 2919
    },
    {
        "loss": 1.9784,
        "grad_norm": 1.7485827207565308,
        "learning_rate": 9.622012119733346e-05,
        "epoch": 0.37619170316928624,
        "step": 2920
    },
    {
        "loss": 2.2532,
        "grad_norm": 2.1160831451416016,
        "learning_rate": 9.619688585195453e-05,
        "epoch": 0.3763205359443442,
        "step": 2921
    },
    {
        "loss": 1.9232,
        "grad_norm": 2.433340072631836,
        "learning_rate": 9.617358213307908e-05,
        "epoch": 0.3764493687194022,
        "step": 2922
    },
    {
        "loss": 2.1243,
        "grad_norm": 2.5727009773254395,
        "learning_rate": 9.61502100751976e-05,
        "epoch": 0.3765782014944602,
        "step": 2923
    },
    {
        "loss": 1.6603,
        "grad_norm": 3.8006432056427,
        "learning_rate": 9.612676971290185e-05,
        "epoch": 0.3767070342695182,
        "step": 2924
    },
    {
        "loss": 1.8335,
        "grad_norm": 2.5179312229156494,
        "learning_rate": 9.610326108088458e-05,
        "epoch": 0.37683586704457617,
        "step": 2925
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.976564884185791,
        "learning_rate": 9.607968421393971e-05,
        "epoch": 0.3769646998196341,
        "step": 2926
    },
    {
        "loss": 2.1824,
        "grad_norm": 1.4026618003845215,
        "learning_rate": 9.605603914696206e-05,
        "epoch": 0.3770935325946921,
        "step": 2927
    },
    {
        "loss": 1.9783,
        "grad_norm": 1.8788198232650757,
        "learning_rate": 9.603232591494735e-05,
        "epoch": 0.37722236536975007,
        "step": 2928
    },
    {
        "loss": 2.3752,
        "grad_norm": 1.328927993774414,
        "learning_rate": 9.600854455299229e-05,
        "epoch": 0.37735119814480805,
        "step": 2929
    },
    {
        "loss": 2.0768,
        "grad_norm": 2.8692269325256348,
        "learning_rate": 9.598469509629437e-05,
        "epoch": 0.37748003091986604,
        "step": 2930
    },
    {
        "loss": 1.1405,
        "grad_norm": 3.869535446166992,
        "learning_rate": 9.596077758015189e-05,
        "epoch": 0.37760886369492397,
        "step": 2931
    },
    {
        "loss": 2.2911,
        "grad_norm": 2.223479747772217,
        "learning_rate": 9.593679203996385e-05,
        "epoch": 0.37773769646998195,
        "step": 2932
    },
    {
        "loss": 2.1685,
        "grad_norm": 2.0872528553009033,
        "learning_rate": 9.591273851122992e-05,
        "epoch": 0.37786652924503994,
        "step": 2933
    },
    {
        "loss": 1.549,
        "grad_norm": 4.2048444747924805,
        "learning_rate": 9.588861702955047e-05,
        "epoch": 0.3779953620200979,
        "step": 2934
    },
    {
        "loss": 1.6798,
        "grad_norm": 2.1792759895324707,
        "learning_rate": 9.586442763062635e-05,
        "epoch": 0.3781241947951559,
        "step": 2935
    },
    {
        "loss": 1.9285,
        "grad_norm": 1.9895886182785034,
        "learning_rate": 9.584017035025897e-05,
        "epoch": 0.37825302757021384,
        "step": 2936
    },
    {
        "loss": 1.226,
        "grad_norm": 3.3126204013824463,
        "learning_rate": 9.581584522435026e-05,
        "epoch": 0.3783818603452718,
        "step": 2937
    },
    {
        "loss": 2.0081,
        "grad_norm": 1.969040870666504,
        "learning_rate": 9.579145228890243e-05,
        "epoch": 0.3785106931203298,
        "step": 2938
    },
    {
        "loss": 1.721,
        "grad_norm": 1.8265306949615479,
        "learning_rate": 9.576699158001823e-05,
        "epoch": 0.3786395258953878,
        "step": 2939
    },
    {
        "loss": 2.2058,
        "grad_norm": 1.985399603843689,
        "learning_rate": 9.574246313390057e-05,
        "epoch": 0.3787683586704458,
        "step": 2940
    },
    {
        "loss": 2.6355,
        "grad_norm": 2.0447592735290527,
        "learning_rate": 9.571786698685266e-05,
        "epoch": 0.3788971914455037,
        "step": 2941
    },
    {
        "loss": 1.942,
        "grad_norm": 3.0748002529144287,
        "learning_rate": 9.569320317527796e-05,
        "epoch": 0.3790260242205617,
        "step": 2942
    },
    {
        "loss": 2.1166,
        "grad_norm": 2.6445720195770264,
        "learning_rate": 9.566847173568e-05,
        "epoch": 0.3791548569956197,
        "step": 2943
    },
    {
        "loss": 1.6042,
        "grad_norm": 3.277482509613037,
        "learning_rate": 9.564367270466247e-05,
        "epoch": 0.37928368977067767,
        "step": 2944
    },
    {
        "loss": 2.0474,
        "grad_norm": 2.267296075820923,
        "learning_rate": 9.561880611892903e-05,
        "epoch": 0.37941252254573565,
        "step": 2945
    },
    {
        "loss": 2.3696,
        "grad_norm": 1.6686251163482666,
        "learning_rate": 9.559387201528338e-05,
        "epoch": 0.3795413553207936,
        "step": 2946
    },
    {
        "loss": 1.829,
        "grad_norm": 2.2520461082458496,
        "learning_rate": 9.556887043062915e-05,
        "epoch": 0.37967018809585157,
        "step": 2947
    },
    {
        "loss": 2.4054,
        "grad_norm": 2.4940459728240967,
        "learning_rate": 9.554380140196977e-05,
        "epoch": 0.37979902087090955,
        "step": 2948
    },
    {
        "loss": 2.1825,
        "grad_norm": 2.3226816654205322,
        "learning_rate": 9.55186649664086e-05,
        "epoch": 0.37992785364596754,
        "step": 2949
    },
    {
        "loss": 2.3601,
        "grad_norm": 2.188419818878174,
        "learning_rate": 9.549346116114867e-05,
        "epoch": 0.3800566864210255,
        "step": 2950
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.9946154356002808,
        "learning_rate": 9.546819002349278e-05,
        "epoch": 0.3801855191960835,
        "step": 2951
    },
    {
        "loss": 1.5914,
        "grad_norm": 3.2521135807037354,
        "learning_rate": 9.544285159084337e-05,
        "epoch": 0.38031435197114144,
        "step": 2952
    },
    {
        "loss": 2.2634,
        "grad_norm": 1.9292895793914795,
        "learning_rate": 9.541744590070247e-05,
        "epoch": 0.3804431847461994,
        "step": 2953
    },
    {
        "loss": 2.8116,
        "grad_norm": 1.7326173782348633,
        "learning_rate": 9.539197299067164e-05,
        "epoch": 0.3805720175212574,
        "step": 2954
    },
    {
        "loss": 1.5911,
        "grad_norm": 2.850855827331543,
        "learning_rate": 9.536643289845197e-05,
        "epoch": 0.3807008502963154,
        "step": 2955
    },
    {
        "loss": 2.332,
        "grad_norm": 1.610771656036377,
        "learning_rate": 9.534082566184398e-05,
        "epoch": 0.3808296830713734,
        "step": 2956
    },
    {
        "loss": 2.7051,
        "grad_norm": 1.1908918619155884,
        "learning_rate": 9.531515131874751e-05,
        "epoch": 0.3809585158464313,
        "step": 2957
    },
    {
        "loss": 2.4609,
        "grad_norm": 1.631453037261963,
        "learning_rate": 9.528940990716178e-05,
        "epoch": 0.3810873486214893,
        "step": 2958
    },
    {
        "loss": 2.3706,
        "grad_norm": 1.429008960723877,
        "learning_rate": 9.526360146518523e-05,
        "epoch": 0.3812161813965473,
        "step": 2959
    },
    {
        "loss": 1.7324,
        "grad_norm": 2.43003249168396,
        "learning_rate": 9.523772603101552e-05,
        "epoch": 0.38134501417160527,
        "step": 2960
    },
    {
        "loss": 2.1745,
        "grad_norm": 1.2943164110183716,
        "learning_rate": 9.521178364294954e-05,
        "epoch": 0.38147384694666325,
        "step": 2961
    },
    {
        "loss": 2.3001,
        "grad_norm": 1.6335790157318115,
        "learning_rate": 9.518577433938315e-05,
        "epoch": 0.3816026797217212,
        "step": 2962
    },
    {
        "loss": 1.8842,
        "grad_norm": 2.617337942123413,
        "learning_rate": 9.515969815881131e-05,
        "epoch": 0.38173151249677917,
        "step": 2963
    },
    {
        "loss": 1.6155,
        "grad_norm": 3.2516930103302,
        "learning_rate": 9.513355513982796e-05,
        "epoch": 0.38186034527183715,
        "step": 2964
    },
    {
        "loss": 1.8134,
        "grad_norm": 2.5505611896514893,
        "learning_rate": 9.510734532112598e-05,
        "epoch": 0.38198917804689514,
        "step": 2965
    },
    {
        "loss": 1.9084,
        "grad_norm": 2.147465705871582,
        "learning_rate": 9.508106874149705e-05,
        "epoch": 0.3821180108219531,
        "step": 2966
    },
    {
        "loss": 2.1209,
        "grad_norm": 1.7593145370483398,
        "learning_rate": 9.505472543983177e-05,
        "epoch": 0.38224684359701105,
        "step": 2967
    },
    {
        "loss": 2.4011,
        "grad_norm": 1.7493314743041992,
        "learning_rate": 9.502831545511935e-05,
        "epoch": 0.38237567637206904,
        "step": 2968
    },
    {
        "loss": 2.1198,
        "grad_norm": 1.454314112663269,
        "learning_rate": 9.500183882644784e-05,
        "epoch": 0.382504509147127,
        "step": 2969
    },
    {
        "loss": 2.0118,
        "grad_norm": 1.9973194599151611,
        "learning_rate": 9.497529559300384e-05,
        "epoch": 0.382633341922185,
        "step": 2970
    },
    {
        "loss": 2.3161,
        "grad_norm": 2.4754300117492676,
        "learning_rate": 9.494868579407252e-05,
        "epoch": 0.382762174697243,
        "step": 2971
    },
    {
        "loss": 2.1698,
        "grad_norm": 1.4119699001312256,
        "learning_rate": 9.49220094690376e-05,
        "epoch": 0.3828910074723009,
        "step": 2972
    },
    {
        "loss": 1.8275,
        "grad_norm": 1.4087532758712769,
        "learning_rate": 9.489526665738128e-05,
        "epoch": 0.3830198402473589,
        "step": 2973
    },
    {
        "loss": 2.4218,
        "grad_norm": 2.7134459018707275,
        "learning_rate": 9.486845739868413e-05,
        "epoch": 0.3831486730224169,
        "step": 2974
    },
    {
        "loss": 2.1794,
        "grad_norm": 1.5647526979446411,
        "learning_rate": 9.484158173262507e-05,
        "epoch": 0.3832775057974749,
        "step": 2975
    },
    {
        "loss": 2.3018,
        "grad_norm": 1.6384625434875488,
        "learning_rate": 9.48146396989813e-05,
        "epoch": 0.38340633857253287,
        "step": 2976
    },
    {
        "loss": 2.6019,
        "grad_norm": 1.612249732017517,
        "learning_rate": 9.47876313376283e-05,
        "epoch": 0.38353517134759085,
        "step": 2977
    },
    {
        "loss": 2.224,
        "grad_norm": 1.2670953273773193,
        "learning_rate": 9.47605566885396e-05,
        "epoch": 0.3836640041226488,
        "step": 2978
    },
    {
        "loss": 2.1216,
        "grad_norm": 1.476135492324829,
        "learning_rate": 9.473341579178699e-05,
        "epoch": 0.38379283689770677,
        "step": 2979
    },
    {
        "loss": 2.2638,
        "grad_norm": 3.4884166717529297,
        "learning_rate": 9.470620868754019e-05,
        "epoch": 0.38392166967276475,
        "step": 2980
    },
    {
        "loss": 2.4422,
        "grad_norm": 1.5313340425491333,
        "learning_rate": 9.467893541606698e-05,
        "epoch": 0.38405050244782274,
        "step": 2981
    },
    {
        "loss": 1.9775,
        "grad_norm": 1.9826545715332031,
        "learning_rate": 9.465159601773304e-05,
        "epoch": 0.3841793352228807,
        "step": 2982
    },
    {
        "loss": 1.897,
        "grad_norm": 2.9360923767089844,
        "learning_rate": 9.462419053300191e-05,
        "epoch": 0.38430816799793865,
        "step": 2983
    },
    {
        "loss": 2.0496,
        "grad_norm": 1.788330078125,
        "learning_rate": 9.459671900243497e-05,
        "epoch": 0.38443700077299664,
        "step": 2984
    },
    {
        "loss": 1.5109,
        "grad_norm": 2.7118656635284424,
        "learning_rate": 9.456918146669132e-05,
        "epoch": 0.3845658335480546,
        "step": 2985
    },
    {
        "loss": 1.998,
        "grad_norm": 1.9246878623962402,
        "learning_rate": 9.454157796652779e-05,
        "epoch": 0.3846946663231126,
        "step": 2986
    },
    {
        "loss": 2.1419,
        "grad_norm": 2.2251482009887695,
        "learning_rate": 9.451390854279882e-05,
        "epoch": 0.3848234990981706,
        "step": 2987
    },
    {
        "loss": 1.8966,
        "grad_norm": 1.599966287612915,
        "learning_rate": 9.448617323645642e-05,
        "epoch": 0.3849523318732285,
        "step": 2988
    },
    {
        "loss": 1.244,
        "grad_norm": 2.2894725799560547,
        "learning_rate": 9.445837208855006e-05,
        "epoch": 0.3850811646482865,
        "step": 2989
    },
    {
        "loss": 2.2845,
        "grad_norm": 2.219960927963257,
        "learning_rate": 9.443050514022676e-05,
        "epoch": 0.3852099974233445,
        "step": 2990
    },
    {
        "loss": 2.2729,
        "grad_norm": 2.150033712387085,
        "learning_rate": 9.440257243273085e-05,
        "epoch": 0.3853388301984025,
        "step": 2991
    },
    {
        "loss": 2.1522,
        "grad_norm": 3.025315999984741,
        "learning_rate": 9.437457400740401e-05,
        "epoch": 0.38546766297346047,
        "step": 2992
    },
    {
        "loss": 2.4845,
        "grad_norm": 1.554244875907898,
        "learning_rate": 9.434650990568519e-05,
        "epoch": 0.3855964957485184,
        "step": 2993
    },
    {
        "loss": 1.7215,
        "grad_norm": 2.827277898788452,
        "learning_rate": 9.431838016911055e-05,
        "epoch": 0.3857253285235764,
        "step": 2994
    },
    {
        "loss": 2.3927,
        "grad_norm": 1.7302743196487427,
        "learning_rate": 9.429018483931338e-05,
        "epoch": 0.38585416129863437,
        "step": 2995
    },
    {
        "loss": 1.891,
        "grad_norm": 2.863879680633545,
        "learning_rate": 9.426192395802403e-05,
        "epoch": 0.38598299407369235,
        "step": 2996
    },
    {
        "loss": 1.7781,
        "grad_norm": 2.6212146282196045,
        "learning_rate": 9.423359756706991e-05,
        "epoch": 0.38611182684875034,
        "step": 2997
    },
    {
        "loss": 2.276,
        "grad_norm": 2.1262385845184326,
        "learning_rate": 9.420520570837537e-05,
        "epoch": 0.3862406596238083,
        "step": 2998
    },
    {
        "loss": 1.6056,
        "grad_norm": 2.536719560623169,
        "learning_rate": 9.417674842396165e-05,
        "epoch": 0.38636949239886625,
        "step": 2999
    },
    {
        "loss": 2.1446,
        "grad_norm": 2.2311184406280518,
        "learning_rate": 9.414822575594684e-05,
        "epoch": 0.38649832517392424,
        "step": 3000
    },
    {
        "loss": 2.4769,
        "grad_norm": 1.8194165229797363,
        "learning_rate": 9.411963774654577e-05,
        "epoch": 0.3866271579489822,
        "step": 3001
    },
    {
        "loss": 2.3461,
        "grad_norm": 2.025026321411133,
        "learning_rate": 9.409098443807e-05,
        "epoch": 0.3867559907240402,
        "step": 3002
    },
    {
        "loss": 2.3387,
        "grad_norm": 1.6486492156982422,
        "learning_rate": 9.406226587292774e-05,
        "epoch": 0.3868848234990982,
        "step": 3003
    },
    {
        "loss": 1.8485,
        "grad_norm": 1.6909509897232056,
        "learning_rate": 9.403348209362377e-05,
        "epoch": 0.3870136562741561,
        "step": 3004
    },
    {
        "loss": 2.308,
        "grad_norm": 1.2652360200881958,
        "learning_rate": 9.400463314275943e-05,
        "epoch": 0.3871424890492141,
        "step": 3005
    },
    {
        "loss": 1.8874,
        "grad_norm": 2.734375476837158,
        "learning_rate": 9.397571906303242e-05,
        "epoch": 0.3872713218242721,
        "step": 3006
    },
    {
        "loss": 2.2552,
        "grad_norm": 2.033644676208496,
        "learning_rate": 9.394673989723693e-05,
        "epoch": 0.3874001545993301,
        "step": 3007
    },
    {
        "loss": 1.6959,
        "grad_norm": 1.8692195415496826,
        "learning_rate": 9.391769568826346e-05,
        "epoch": 0.38752898737438807,
        "step": 3008
    },
    {
        "loss": 1.7091,
        "grad_norm": 2.278928756713867,
        "learning_rate": 9.388858647909873e-05,
        "epoch": 0.387657820149446,
        "step": 3009
    },
    {
        "loss": 2.5459,
        "grad_norm": 1.606055736541748,
        "learning_rate": 9.38594123128257e-05,
        "epoch": 0.387786652924504,
        "step": 3010
    },
    {
        "loss": 2.4008,
        "grad_norm": 1.3694653511047363,
        "learning_rate": 9.383017323262349e-05,
        "epoch": 0.38791548569956197,
        "step": 3011
    },
    {
        "loss": 1.7565,
        "grad_norm": 1.8237508535385132,
        "learning_rate": 9.380086928176727e-05,
        "epoch": 0.38804431847461995,
        "step": 3012
    },
    {
        "loss": 2.2946,
        "grad_norm": 2.0458314418792725,
        "learning_rate": 9.377150050362821e-05,
        "epoch": 0.38817315124967794,
        "step": 3013
    },
    {
        "loss": 2.4897,
        "grad_norm": 2.4591879844665527,
        "learning_rate": 9.374206694167345e-05,
        "epoch": 0.38830198402473587,
        "step": 3014
    },
    {
        "loss": 2.5225,
        "grad_norm": 1.8623400926589966,
        "learning_rate": 9.371256863946597e-05,
        "epoch": 0.38843081679979385,
        "step": 3015
    },
    {
        "loss": 2.4494,
        "grad_norm": 2.499591588973999,
        "learning_rate": 9.368300564066463e-05,
        "epoch": 0.38855964957485184,
        "step": 3016
    },
    {
        "loss": 1.9673,
        "grad_norm": 3.0866057872772217,
        "learning_rate": 9.365337798902404e-05,
        "epoch": 0.3886884823499098,
        "step": 3017
    },
    {
        "loss": 2.0872,
        "grad_norm": 2.091299295425415,
        "learning_rate": 9.362368572839442e-05,
        "epoch": 0.3888173151249678,
        "step": 3018
    },
    {
        "loss": 2.0665,
        "grad_norm": 2.13573956489563,
        "learning_rate": 9.359392890272169e-05,
        "epoch": 0.38894614790002574,
        "step": 3019
    },
    {
        "loss": 1.1095,
        "grad_norm": 2.791579246520996,
        "learning_rate": 9.356410755604735e-05,
        "epoch": 0.3890749806750837,
        "step": 3020
    },
    {
        "loss": 2.3311,
        "grad_norm": 2.4295427799224854,
        "learning_rate": 9.353422173250829e-05,
        "epoch": 0.3892038134501417,
        "step": 3021
    },
    {
        "loss": 2.2649,
        "grad_norm": 1.550891399383545,
        "learning_rate": 9.350427147633693e-05,
        "epoch": 0.3893326462251997,
        "step": 3022
    },
    {
        "loss": 2.2644,
        "grad_norm": 1.6479134559631348,
        "learning_rate": 9.347425683186099e-05,
        "epoch": 0.3894614790002577,
        "step": 3023
    },
    {
        "loss": 2.2546,
        "grad_norm": 1.2188804149627686,
        "learning_rate": 9.344417784350355e-05,
        "epoch": 0.38959031177531567,
        "step": 3024
    },
    {
        "loss": 1.5075,
        "grad_norm": 2.0547397136688232,
        "learning_rate": 9.341403455578282e-05,
        "epoch": 0.3897191445503736,
        "step": 3025
    },
    {
        "loss": 2.1227,
        "grad_norm": 2.381054639816284,
        "learning_rate": 9.338382701331231e-05,
        "epoch": 0.3898479773254316,
        "step": 3026
    },
    {
        "loss": 1.8794,
        "grad_norm": 2.3590900897979736,
        "learning_rate": 9.335355526080051e-05,
        "epoch": 0.38997681010048957,
        "step": 3027
    },
    {
        "loss": 2.2662,
        "grad_norm": 2.8684206008911133,
        "learning_rate": 9.3323219343051e-05,
        "epoch": 0.39010564287554755,
        "step": 3028
    },
    {
        "loss": 1.9528,
        "grad_norm": 2.283350944519043,
        "learning_rate": 9.329281930496235e-05,
        "epoch": 0.39023447565060554,
        "step": 3029
    },
    {
        "loss": 2.2556,
        "grad_norm": 1.7296147346496582,
        "learning_rate": 9.326235519152797e-05,
        "epoch": 0.39036330842566347,
        "step": 3030
    },
    {
        "loss": 2.0381,
        "grad_norm": 2.452331304550171,
        "learning_rate": 9.323182704783617e-05,
        "epoch": 0.39049214120072145,
        "step": 3031
    },
    {
        "loss": 1.5081,
        "grad_norm": 2.43733549118042,
        "learning_rate": 9.320123491906995e-05,
        "epoch": 0.39062097397577944,
        "step": 3032
    },
    {
        "loss": 1.2024,
        "grad_norm": 2.3032734394073486,
        "learning_rate": 9.31705788505071e-05,
        "epoch": 0.3907498067508374,
        "step": 3033
    },
    {
        "loss": 2.4321,
        "grad_norm": 1.4347916841506958,
        "learning_rate": 9.313985888751995e-05,
        "epoch": 0.3908786395258954,
        "step": 3034
    },
    {
        "loss": 1.8848,
        "grad_norm": 1.413153052330017,
        "learning_rate": 9.31090750755755e-05,
        "epoch": 0.39100747230095334,
        "step": 3035
    },
    {
        "loss": 2.2442,
        "grad_norm": 1.4411976337432861,
        "learning_rate": 9.307822746023513e-05,
        "epoch": 0.3911363050760113,
        "step": 3036
    },
    {
        "loss": 2.169,
        "grad_norm": 2.7278945446014404,
        "learning_rate": 9.304731608715479e-05,
        "epoch": 0.3912651378510693,
        "step": 3037
    },
    {
        "loss": 2.4436,
        "grad_norm": 1.2069767713546753,
        "learning_rate": 9.301634100208469e-05,
        "epoch": 0.3913939706261273,
        "step": 3038
    },
    {
        "loss": 2.4238,
        "grad_norm": 1.5531176328659058,
        "learning_rate": 9.298530225086933e-05,
        "epoch": 0.3915228034011853,
        "step": 3039
    },
    {
        "loss": 1.7132,
        "grad_norm": 2.6980857849121094,
        "learning_rate": 9.295419987944751e-05,
        "epoch": 0.3916516361762432,
        "step": 3040
    },
    {
        "loss": 1.8403,
        "grad_norm": 2.7143092155456543,
        "learning_rate": 9.292303393385218e-05,
        "epoch": 0.3917804689513012,
        "step": 3041
    },
    {
        "loss": 1.9706,
        "grad_norm": 2.5422000885009766,
        "learning_rate": 9.289180446021034e-05,
        "epoch": 0.3919093017263592,
        "step": 3042
    },
    {
        "loss": 2.3853,
        "grad_norm": 1.9695711135864258,
        "learning_rate": 9.2860511504743e-05,
        "epoch": 0.39203813450141717,
        "step": 3043
    },
    {
        "loss": 2.3036,
        "grad_norm": 1.3039231300354004,
        "learning_rate": 9.282915511376522e-05,
        "epoch": 0.39216696727647515,
        "step": 3044
    },
    {
        "loss": 2.426,
        "grad_norm": 2.1085522174835205,
        "learning_rate": 9.279773533368583e-05,
        "epoch": 0.3922958000515331,
        "step": 3045
    },
    {
        "loss": 1.9312,
        "grad_norm": 1.682949423789978,
        "learning_rate": 9.276625221100757e-05,
        "epoch": 0.39242463282659107,
        "step": 3046
    },
    {
        "loss": 2.3746,
        "grad_norm": 2.0972483158111572,
        "learning_rate": 9.27347057923269e-05,
        "epoch": 0.39255346560164905,
        "step": 3047
    },
    {
        "loss": 1.9926,
        "grad_norm": 3.011054039001465,
        "learning_rate": 9.270309612433395e-05,
        "epoch": 0.39268229837670704,
        "step": 3048
    },
    {
        "loss": 2.3797,
        "grad_norm": 1.6178226470947266,
        "learning_rate": 9.267142325381246e-05,
        "epoch": 0.392811131151765,
        "step": 3049
    },
    {
        "loss": 2.3602,
        "grad_norm": 1.8359993696212769,
        "learning_rate": 9.263968722763974e-05,
        "epoch": 0.392939963926823,
        "step": 3050
    },
    {
        "loss": 1.8157,
        "grad_norm": 2.1370513439178467,
        "learning_rate": 9.260788809278656e-05,
        "epoch": 0.39306879670188094,
        "step": 3051
    },
    {
        "loss": 2.0739,
        "grad_norm": 2.5497891902923584,
        "learning_rate": 9.257602589631705e-05,
        "epoch": 0.3931976294769389,
        "step": 3052
    },
    {
        "loss": 2.3054,
        "grad_norm": 1.7312878370285034,
        "learning_rate": 9.254410068538872e-05,
        "epoch": 0.3933264622519969,
        "step": 3053
    },
    {
        "loss": 1.0678,
        "grad_norm": 3.195897340774536,
        "learning_rate": 9.251211250725237e-05,
        "epoch": 0.3934552950270549,
        "step": 3054
    },
    {
        "loss": 2.1915,
        "grad_norm": 1.7426973581314087,
        "learning_rate": 9.248006140925196e-05,
        "epoch": 0.3935841278021129,
        "step": 3055
    },
    {
        "loss": 2.5879,
        "grad_norm": 1.5200339555740356,
        "learning_rate": 9.244794743882457e-05,
        "epoch": 0.3937129605771708,
        "step": 3056
    },
    {
        "loss": 2.109,
        "grad_norm": 1.881212592124939,
        "learning_rate": 9.241577064350031e-05,
        "epoch": 0.3938417933522288,
        "step": 3057
    },
    {
        "loss": 2.3763,
        "grad_norm": 1.8303353786468506,
        "learning_rate": 9.238353107090231e-05,
        "epoch": 0.3939706261272868,
        "step": 3058
    },
    {
        "loss": 2.279,
        "grad_norm": 1.6765607595443726,
        "learning_rate": 9.235122876874665e-05,
        "epoch": 0.39409945890234477,
        "step": 3059
    },
    {
        "loss": 2.2718,
        "grad_norm": 2.24737811088562,
        "learning_rate": 9.231886378484218e-05,
        "epoch": 0.39422829167740275,
        "step": 3060
    },
    {
        "loss": 2.0784,
        "grad_norm": 2.4053421020507812,
        "learning_rate": 9.228643616709054e-05,
        "epoch": 0.3943571244524607,
        "step": 3061
    },
    {
        "loss": 2.506,
        "grad_norm": 2.319725275039673,
        "learning_rate": 9.225394596348609e-05,
        "epoch": 0.39448595722751867,
        "step": 3062
    },
    {
        "loss": 1.9497,
        "grad_norm": 1.839837908744812,
        "learning_rate": 9.222139322211585e-05,
        "epoch": 0.39461479000257665,
        "step": 3063
    },
    {
        "loss": 2.1969,
        "grad_norm": 2.4712421894073486,
        "learning_rate": 9.21887779911593e-05,
        "epoch": 0.39474362277763464,
        "step": 3064
    },
    {
        "loss": 1.8232,
        "grad_norm": 2.4080235958099365,
        "learning_rate": 9.215610031888848e-05,
        "epoch": 0.3948724555526926,
        "step": 3065
    },
    {
        "loss": 2.2032,
        "grad_norm": 1.3081846237182617,
        "learning_rate": 9.212336025366787e-05,
        "epoch": 0.39500128832775055,
        "step": 3066
    },
    {
        "loss": 1.8825,
        "grad_norm": 2.1904571056365967,
        "learning_rate": 9.209055784395424e-05,
        "epoch": 0.39513012110280854,
        "step": 3067
    },
    {
        "loss": 2.3458,
        "grad_norm": 1.3978115320205688,
        "learning_rate": 9.205769313829665e-05,
        "epoch": 0.3952589538778665,
        "step": 3068
    },
    {
        "loss": 2.3734,
        "grad_norm": 1.979137659072876,
        "learning_rate": 9.202476618533638e-05,
        "epoch": 0.3953877866529245,
        "step": 3069
    },
    {
        "loss": 1.909,
        "grad_norm": 3.033790349960327,
        "learning_rate": 9.199177703380677e-05,
        "epoch": 0.3955166194279825,
        "step": 3070
    },
    {
        "loss": 2.4305,
        "grad_norm": 1.832076072692871,
        "learning_rate": 9.195872573253333e-05,
        "epoch": 0.3956454522030404,
        "step": 3071
    },
    {
        "loss": 2.3626,
        "grad_norm": 1.6786167621612549,
        "learning_rate": 9.192561233043343e-05,
        "epoch": 0.3957742849780984,
        "step": 3072
    },
    {
        "loss": 2.2125,
        "grad_norm": 1.7598119974136353,
        "learning_rate": 9.189243687651648e-05,
        "epoch": 0.3959031177531564,
        "step": 3073
    },
    {
        "loss": 2.3007,
        "grad_norm": 1.4793497323989868,
        "learning_rate": 9.185919941988363e-05,
        "epoch": 0.3960319505282144,
        "step": 3074
    },
    {
        "loss": 2.1672,
        "grad_norm": 1.8437100648880005,
        "learning_rate": 9.18259000097278e-05,
        "epoch": 0.39616078330327237,
        "step": 3075
    },
    {
        "loss": 2.242,
        "grad_norm": 1.7507964372634888,
        "learning_rate": 9.17925386953337e-05,
        "epoch": 0.39628961607833035,
        "step": 3076
    },
    {
        "loss": 2.1037,
        "grad_norm": 2.079413414001465,
        "learning_rate": 9.175911552607757e-05,
        "epoch": 0.3964184488533883,
        "step": 3077
    },
    {
        "loss": 1.3938,
        "grad_norm": 3.008617401123047,
        "learning_rate": 9.17256305514272e-05,
        "epoch": 0.39654728162844627,
        "step": 3078
    },
    {
        "loss": 2.2787,
        "grad_norm": 2.045196056365967,
        "learning_rate": 9.169208382094188e-05,
        "epoch": 0.39667611440350425,
        "step": 3079
    },
    {
        "loss": 2.3427,
        "grad_norm": 1.9926506280899048,
        "learning_rate": 9.165847538427235e-05,
        "epoch": 0.39680494717856224,
        "step": 3080
    },
    {
        "loss": 2.4666,
        "grad_norm": 1.9700738191604614,
        "learning_rate": 9.162480529116058e-05,
        "epoch": 0.3969337799536202,
        "step": 3081
    },
    {
        "loss": 1.0628,
        "grad_norm": 3.0777742862701416,
        "learning_rate": 9.159107359143985e-05,
        "epoch": 0.39706261272867815,
        "step": 3082
    },
    {
        "loss": 2.4235,
        "grad_norm": 2.6570398807525635,
        "learning_rate": 9.155728033503461e-05,
        "epoch": 0.39719144550373614,
        "step": 3083
    },
    {
        "loss": 1.8264,
        "grad_norm": 2.3182480335235596,
        "learning_rate": 9.152342557196043e-05,
        "epoch": 0.3973202782787941,
        "step": 3084
    },
    {
        "loss": 1.5011,
        "grad_norm": 2.427318811416626,
        "learning_rate": 9.148950935232393e-05,
        "epoch": 0.3974491110538521,
        "step": 3085
    },
    {
        "loss": 2.4407,
        "grad_norm": 1.5137869119644165,
        "learning_rate": 9.14555317263226e-05,
        "epoch": 0.3975779438289101,
        "step": 3086
    },
    {
        "loss": 1.5229,
        "grad_norm": 3.117227077484131,
        "learning_rate": 9.142149274424492e-05,
        "epoch": 0.397706776603968,
        "step": 3087
    },
    {
        "loss": 1.1976,
        "grad_norm": 3.095362901687622,
        "learning_rate": 9.138739245647012e-05,
        "epoch": 0.397835609379026,
        "step": 3088
    },
    {
        "loss": 1.8538,
        "grad_norm": 2.455704927444458,
        "learning_rate": 9.135323091346819e-05,
        "epoch": 0.397964442154084,
        "step": 3089
    },
    {
        "loss": 2.1876,
        "grad_norm": 2.317776679992676,
        "learning_rate": 9.131900816579974e-05,
        "epoch": 0.398093274929142,
        "step": 3090
    },
    {
        "loss": 1.9418,
        "grad_norm": 2.730116128921509,
        "learning_rate": 9.128472426411605e-05,
        "epoch": 0.39822210770419997,
        "step": 3091
    },
    {
        "loss": 2.0707,
        "grad_norm": 1.8936684131622314,
        "learning_rate": 9.12503792591588e-05,
        "epoch": 0.3983509404792579,
        "step": 3092
    },
    {
        "loss": 1.9609,
        "grad_norm": 2.2811050415039062,
        "learning_rate": 9.12159732017602e-05,
        "epoch": 0.3984797732543159,
        "step": 3093
    },
    {
        "loss": 1.2452,
        "grad_norm": 2.7888739109039307,
        "learning_rate": 9.118150614284277e-05,
        "epoch": 0.39860860602937387,
        "step": 3094
    },
    {
        "loss": 1.7791,
        "grad_norm": 5.075735092163086,
        "learning_rate": 9.114697813341934e-05,
        "epoch": 0.39873743880443185,
        "step": 3095
    },
    {
        "loss": 1.8804,
        "grad_norm": 2.2322611808776855,
        "learning_rate": 9.111238922459291e-05,
        "epoch": 0.39886627157948984,
        "step": 3096
    },
    {
        "loss": 2.0822,
        "grad_norm": 2.078695058822632,
        "learning_rate": 9.107773946755665e-05,
        "epoch": 0.39899510435454777,
        "step": 3097
    },
    {
        "loss": 2.1445,
        "grad_norm": 2.9693210124969482,
        "learning_rate": 9.104302891359383e-05,
        "epoch": 0.39912393712960575,
        "step": 3098
    },
    {
        "loss": 2.3172,
        "grad_norm": 2.2139129638671875,
        "learning_rate": 9.100825761407759e-05,
        "epoch": 0.39925276990466374,
        "step": 3099
    },
    {
        "loss": 2.8037,
        "grad_norm": 1.4585694074630737,
        "learning_rate": 9.097342562047103e-05,
        "epoch": 0.3993816026797217,
        "step": 3100
    },
    {
        "loss": 2.4296,
        "grad_norm": 1.5897700786590576,
        "learning_rate": 9.093853298432715e-05,
        "epoch": 0.3995104354547797,
        "step": 3101
    },
    {
        "loss": 2.4515,
        "grad_norm": 1.3578550815582275,
        "learning_rate": 9.09035797572886e-05,
        "epoch": 0.3996392682298377,
        "step": 3102
    },
    {
        "loss": 1.6465,
        "grad_norm": 2.008790969848633,
        "learning_rate": 9.086856599108778e-05,
        "epoch": 0.3997681010048956,
        "step": 3103
    },
    {
        "loss": 2.2683,
        "grad_norm": 2.7628815174102783,
        "learning_rate": 9.083349173754659e-05,
        "epoch": 0.3998969337799536,
        "step": 3104
    },
    {
        "loss": 2.0004,
        "grad_norm": 3.1335580348968506,
        "learning_rate": 9.079835704857658e-05,
        "epoch": 0.4000257665550116,
        "step": 3105
    },
    {
        "loss": 1.8105,
        "grad_norm": 1.7776671648025513,
        "learning_rate": 9.076316197617871e-05,
        "epoch": 0.4001545993300696,
        "step": 3106
    },
    {
        "loss": 2.19,
        "grad_norm": 1.6102856397628784,
        "learning_rate": 9.072790657244326e-05,
        "epoch": 0.40028343210512757,
        "step": 3107
    },
    {
        "loss": 1.7411,
        "grad_norm": 2.0324196815490723,
        "learning_rate": 9.06925908895498e-05,
        "epoch": 0.4004122648801855,
        "step": 3108
    },
    {
        "loss": 1.8956,
        "grad_norm": 2.159194231033325,
        "learning_rate": 9.065721497976722e-05,
        "epoch": 0.4005410976552435,
        "step": 3109
    },
    {
        "loss": 1.7948,
        "grad_norm": 2.0003013610839844,
        "learning_rate": 9.062177889545345e-05,
        "epoch": 0.40066993043030147,
        "step": 3110
    },
    {
        "loss": 2.1029,
        "grad_norm": 2.0323760509490967,
        "learning_rate": 9.058628268905549e-05,
        "epoch": 0.40079876320535945,
        "step": 3111
    },
    {
        "loss": 1.8746,
        "grad_norm": 1.4230380058288574,
        "learning_rate": 9.055072641310937e-05,
        "epoch": 0.40092759598041744,
        "step": 3112
    },
    {
        "loss": 2.2331,
        "grad_norm": 2.0112478733062744,
        "learning_rate": 9.051511012023999e-05,
        "epoch": 0.40105642875547537,
        "step": 3113
    },
    {
        "loss": 2.3806,
        "grad_norm": 1.4072377681732178,
        "learning_rate": 9.047943386316107e-05,
        "epoch": 0.40118526153053335,
        "step": 3114
    },
    {
        "loss": 2.1789,
        "grad_norm": 2.0303115844726562,
        "learning_rate": 9.04436976946751e-05,
        "epoch": 0.40131409430559134,
        "step": 3115
    },
    {
        "loss": 1.1916,
        "grad_norm": 2.910390853881836,
        "learning_rate": 9.04079016676733e-05,
        "epoch": 0.4014429270806493,
        "step": 3116
    },
    {
        "loss": 2.1659,
        "grad_norm": 2.138018846511841,
        "learning_rate": 9.037204583513534e-05,
        "epoch": 0.4015717598557073,
        "step": 3117
    },
    {
        "loss": 1.6828,
        "grad_norm": 1.524069905281067,
        "learning_rate": 9.03361302501295e-05,
        "epoch": 0.40170059263076524,
        "step": 3118
    },
    {
        "loss": 1.981,
        "grad_norm": 1.4340753555297852,
        "learning_rate": 9.030015496581255e-05,
        "epoch": 0.4018294254058232,
        "step": 3119
    },
    {
        "loss": 1.9774,
        "grad_norm": 2.9871315956115723,
        "learning_rate": 9.026412003542946e-05,
        "epoch": 0.4019582581808812,
        "step": 3120
    },
    {
        "loss": 2.0852,
        "grad_norm": 1.622794270515442,
        "learning_rate": 9.022802551231363e-05,
        "epoch": 0.4020870909559392,
        "step": 3121
    },
    {
        "loss": 2.2925,
        "grad_norm": 1.9498710632324219,
        "learning_rate": 9.019187144988654e-05,
        "epoch": 0.4022159237309972,
        "step": 3122
    },
    {
        "loss": 2.2456,
        "grad_norm": 1.6224664449691772,
        "learning_rate": 9.015565790165794e-05,
        "epoch": 0.40234475650605517,
        "step": 3123
    },
    {
        "loss": 1.7284,
        "grad_norm": 2.14007306098938,
        "learning_rate": 9.011938492122546e-05,
        "epoch": 0.4024735892811131,
        "step": 3124
    },
    {
        "loss": 1.9546,
        "grad_norm": 1.913917899131775,
        "learning_rate": 9.00830525622748e-05,
        "epoch": 0.4026024220561711,
        "step": 3125
    },
    {
        "loss": 2.3173,
        "grad_norm": 2.1306569576263428,
        "learning_rate": 9.004666087857947e-05,
        "epoch": 0.40273125483122907,
        "step": 3126
    },
    {
        "loss": 1.6281,
        "grad_norm": 2.7637815475463867,
        "learning_rate": 9.001020992400084e-05,
        "epoch": 0.40286008760628705,
        "step": 3127
    },
    {
        "loss": 2.0495,
        "grad_norm": 2.511476755142212,
        "learning_rate": 8.997369975248802e-05,
        "epoch": 0.40298892038134504,
        "step": 3128
    },
    {
        "loss": 1.6929,
        "grad_norm": 2.9629504680633545,
        "learning_rate": 8.993713041807768e-05,
        "epoch": 0.40311775315640297,
        "step": 3129
    },
    {
        "loss": 1.9238,
        "grad_norm": 2.2896482944488525,
        "learning_rate": 8.990050197489411e-05,
        "epoch": 0.40324658593146095,
        "step": 3130
    },
    {
        "loss": 2.2389,
        "grad_norm": 2.1394169330596924,
        "learning_rate": 8.986381447714909e-05,
        "epoch": 0.40337541870651894,
        "step": 3131
    },
    {
        "loss": 2.2909,
        "grad_norm": 1.403862714767456,
        "learning_rate": 8.982706797914177e-05,
        "epoch": 0.4035042514815769,
        "step": 3132
    },
    {
        "loss": 2.2468,
        "grad_norm": 1.4514082670211792,
        "learning_rate": 8.979026253525866e-05,
        "epoch": 0.4036330842566349,
        "step": 3133
    },
    {
        "loss": 1.8847,
        "grad_norm": 1.889875054359436,
        "learning_rate": 8.97533981999735e-05,
        "epoch": 0.40376191703169284,
        "step": 3134
    },
    {
        "loss": 2.3764,
        "grad_norm": 1.5363261699676514,
        "learning_rate": 8.971647502784714e-05,
        "epoch": 0.4038907498067508,
        "step": 3135
    },
    {
        "loss": 2.1344,
        "grad_norm": 2.4182703495025635,
        "learning_rate": 8.967949307352761e-05,
        "epoch": 0.4040195825818088,
        "step": 3136
    },
    {
        "loss": 1.6372,
        "grad_norm": 1.895364761352539,
        "learning_rate": 8.964245239174988e-05,
        "epoch": 0.4041484153568668,
        "step": 3137
    },
    {
        "loss": 1.8974,
        "grad_norm": 2.1611242294311523,
        "learning_rate": 8.960535303733584e-05,
        "epoch": 0.4042772481319248,
        "step": 3138
    },
    {
        "loss": 2.2821,
        "grad_norm": 2.2553396224975586,
        "learning_rate": 8.956819506519419e-05,
        "epoch": 0.4044060809069827,
        "step": 3139
    },
    {
        "loss": 1.857,
        "grad_norm": 2.858745813369751,
        "learning_rate": 8.953097853032046e-05,
        "epoch": 0.4045349136820407,
        "step": 3140
    },
    {
        "loss": 2.1475,
        "grad_norm": 1.6801897287368774,
        "learning_rate": 8.949370348779684e-05,
        "epoch": 0.4046637464570987,
        "step": 3141
    },
    {
        "loss": 2.5277,
        "grad_norm": 1.661765694618225,
        "learning_rate": 8.945636999279208e-05,
        "epoch": 0.40479257923215667,
        "step": 3142
    },
    {
        "loss": 1.5844,
        "grad_norm": 3.38236141204834,
        "learning_rate": 8.941897810056136e-05,
        "epoch": 0.40492141200721465,
        "step": 3143
    },
    {
        "loss": 2.3842,
        "grad_norm": 2.0512285232543945,
        "learning_rate": 8.938152786644653e-05,
        "epoch": 0.4050502447822726,
        "step": 3144
    },
    {
        "loss": 2.1252,
        "grad_norm": 1.244071364402771,
        "learning_rate": 8.934401934587555e-05,
        "epoch": 0.40517907755733057,
        "step": 3145
    },
    {
        "loss": 2.172,
        "grad_norm": 1.2900922298431396,
        "learning_rate": 8.930645259436279e-05,
        "epoch": 0.40530791033238855,
        "step": 3146
    },
    {
        "loss": 2.1105,
        "grad_norm": 1.6363056898117065,
        "learning_rate": 8.926882766750868e-05,
        "epoch": 0.40543674310744654,
        "step": 3147
    },
    {
        "loss": 2.2297,
        "grad_norm": 2.041290283203125,
        "learning_rate": 8.923114462099994e-05,
        "epoch": 0.4055655758825045,
        "step": 3148
    },
    {
        "loss": 2.2067,
        "grad_norm": 1.4914599657058716,
        "learning_rate": 8.919340351060909e-05,
        "epoch": 0.4056944086575625,
        "step": 3149
    },
    {
        "loss": 2.5472,
        "grad_norm": 1.6148478984832764,
        "learning_rate": 8.915560439219475e-05,
        "epoch": 0.40582324143262044,
        "step": 3150
    },
    {
        "loss": 1.8531,
        "grad_norm": 2.4558193683624268,
        "learning_rate": 8.911774732170132e-05,
        "epoch": 0.4059520742076784,
        "step": 3151
    },
    {
        "loss": 1.7499,
        "grad_norm": 3.0237767696380615,
        "learning_rate": 8.907983235515898e-05,
        "epoch": 0.4060809069827364,
        "step": 3152
    },
    {
        "loss": 2.3742,
        "grad_norm": 2.1755528450012207,
        "learning_rate": 8.904185954868366e-05,
        "epoch": 0.4062097397577944,
        "step": 3153
    },
    {
        "loss": 1.8497,
        "grad_norm": 2.723006010055542,
        "learning_rate": 8.900382895847677e-05,
        "epoch": 0.4063385725328524,
        "step": 3154
    },
    {
        "loss": 2.2713,
        "grad_norm": 1.9558488130569458,
        "learning_rate": 8.896574064082539e-05,
        "epoch": 0.4064674053079103,
        "step": 3155
    },
    {
        "loss": 2.099,
        "grad_norm": 2.637713670730591,
        "learning_rate": 8.89275946521019e-05,
        "epoch": 0.4065962380829683,
        "step": 3156
    },
    {
        "loss": 1.803,
        "grad_norm": 2.165663480758667,
        "learning_rate": 8.888939104876415e-05,
        "epoch": 0.4067250708580263,
        "step": 3157
    },
    {
        "loss": 1.7554,
        "grad_norm": 2.497663736343384,
        "learning_rate": 8.885112988735518e-05,
        "epoch": 0.40685390363308427,
        "step": 3158
    },
    {
        "loss": 2.1502,
        "grad_norm": 2.0039079189300537,
        "learning_rate": 8.881281122450329e-05,
        "epoch": 0.40698273640814225,
        "step": 3159
    },
    {
        "loss": 1.91,
        "grad_norm": 1.868671178817749,
        "learning_rate": 8.877443511692181e-05,
        "epoch": 0.4071115691832002,
        "step": 3160
    },
    {
        "loss": 1.9908,
        "grad_norm": 2.039523124694824,
        "learning_rate": 8.873600162140914e-05,
        "epoch": 0.40724040195825817,
        "step": 3161
    },
    {
        "loss": 2.1151,
        "grad_norm": 1.9901478290557861,
        "learning_rate": 8.869751079484863e-05,
        "epoch": 0.40736923473331615,
        "step": 3162
    },
    {
        "loss": 2.176,
        "grad_norm": 2.42232084274292,
        "learning_rate": 8.865896269420841e-05,
        "epoch": 0.40749806750837414,
        "step": 3163
    },
    {
        "loss": 1.591,
        "grad_norm": 2.3478245735168457,
        "learning_rate": 8.862035737654149e-05,
        "epoch": 0.4076269002834321,
        "step": 3164
    },
    {
        "loss": 2.1147,
        "grad_norm": 2.4811956882476807,
        "learning_rate": 8.85816948989854e-05,
        "epoch": 0.40775573305849006,
        "step": 3165
    },
    {
        "loss": 2.3181,
        "grad_norm": 2.2029924392700195,
        "learning_rate": 8.85429753187625e-05,
        "epoch": 0.40788456583354804,
        "step": 3166
    },
    {
        "loss": 2.0973,
        "grad_norm": 2.1328675746917725,
        "learning_rate": 8.850419869317945e-05,
        "epoch": 0.408013398608606,
        "step": 3167
    },
    {
        "loss": 1.1056,
        "grad_norm": 3.187959909439087,
        "learning_rate": 8.846536507962742e-05,
        "epoch": 0.408142231383664,
        "step": 3168
    },
    {
        "loss": 1.4176,
        "grad_norm": 2.292100667953491,
        "learning_rate": 8.842647453558194e-05,
        "epoch": 0.408271064158722,
        "step": 3169
    },
    {
        "loss": 2.2425,
        "grad_norm": 1.3640507459640503,
        "learning_rate": 8.83875271186028e-05,
        "epoch": 0.4083998969337799,
        "step": 3170
    },
    {
        "loss": 1.9149,
        "grad_norm": 1.5272893905639648,
        "learning_rate": 8.834852288633396e-05,
        "epoch": 0.4085287297088379,
        "step": 3171
    },
    {
        "loss": 2.3543,
        "grad_norm": 1.1063176393508911,
        "learning_rate": 8.830946189650344e-05,
        "epoch": 0.4086575624838959,
        "step": 3172
    },
    {
        "loss": 2.5079,
        "grad_norm": 1.8257825374603271,
        "learning_rate": 8.827034420692328e-05,
        "epoch": 0.4087863952589539,
        "step": 3173
    },
    {
        "loss": 2.3573,
        "grad_norm": 1.698286533355713,
        "learning_rate": 8.823116987548948e-05,
        "epoch": 0.40891522803401187,
        "step": 3174
    },
    {
        "loss": 2.3387,
        "grad_norm": 1.6784740686416626,
        "learning_rate": 8.819193896018178e-05,
        "epoch": 0.40904406080906985,
        "step": 3175
    },
    {
        "loss": 1.1758,
        "grad_norm": 3.511460542678833,
        "learning_rate": 8.815265151906378e-05,
        "epoch": 0.4091728935841278,
        "step": 3176
    },
    {
        "loss": 2.159,
        "grad_norm": 2.615739107131958,
        "learning_rate": 8.811330761028265e-05,
        "epoch": 0.40930172635918577,
        "step": 3177
    },
    {
        "loss": 2.1814,
        "grad_norm": 1.7339003086090088,
        "learning_rate": 8.807390729206919e-05,
        "epoch": 0.40943055913424375,
        "step": 3178
    },
    {
        "loss": 2.0219,
        "grad_norm": 2.5428688526153564,
        "learning_rate": 8.803445062273763e-05,
        "epoch": 0.40955939190930174,
        "step": 3179
    },
    {
        "loss": 2.2556,
        "grad_norm": 2.4765050411224365,
        "learning_rate": 8.79949376606857e-05,
        "epoch": 0.4096882246843597,
        "step": 3180
    },
    {
        "loss": 1.6315,
        "grad_norm": 2.2320094108581543,
        "learning_rate": 8.795536846439431e-05,
        "epoch": 0.40981705745941766,
        "step": 3181
    },
    {
        "loss": 2.4661,
        "grad_norm": 1.3992764949798584,
        "learning_rate": 8.791574309242772e-05,
        "epoch": 0.40994589023447564,
        "step": 3182
    },
    {
        "loss": 2.2585,
        "grad_norm": 2.2337632179260254,
        "learning_rate": 8.787606160343326e-05,
        "epoch": 0.4100747230095336,
        "step": 3183
    },
    {
        "loss": 1.8044,
        "grad_norm": 2.586242914199829,
        "learning_rate": 8.78363240561414e-05,
        "epoch": 0.4102035557845916,
        "step": 3184
    },
    {
        "loss": 2.1323,
        "grad_norm": 2.0582377910614014,
        "learning_rate": 8.779653050936547e-05,
        "epoch": 0.4103323885596496,
        "step": 3185
    },
    {
        "loss": 2.0766,
        "grad_norm": 1.6935614347457886,
        "learning_rate": 8.77566810220017e-05,
        "epoch": 0.4104612213347075,
        "step": 3186
    },
    {
        "loss": 1.6567,
        "grad_norm": 2.362990617752075,
        "learning_rate": 8.771677565302923e-05,
        "epoch": 0.4105900541097655,
        "step": 3187
    },
    {
        "loss": 1.8677,
        "grad_norm": 3.0549919605255127,
        "learning_rate": 8.767681446150977e-05,
        "epoch": 0.4107188868848235,
        "step": 3188
    },
    {
        "loss": 2.0742,
        "grad_norm": 1.5108342170715332,
        "learning_rate": 8.763679750658776e-05,
        "epoch": 0.4108477196598815,
        "step": 3189
    },
    {
        "loss": 2.0563,
        "grad_norm": 2.4188590049743652,
        "learning_rate": 8.759672484749e-05,
        "epoch": 0.41097655243493947,
        "step": 3190
    },
    {
        "loss": 2.4156,
        "grad_norm": 1.825107216835022,
        "learning_rate": 8.755659654352599e-05,
        "epoch": 0.4111053852099974,
        "step": 3191
    },
    {
        "loss": 1.2203,
        "grad_norm": 2.448381185531616,
        "learning_rate": 8.75164126540874e-05,
        "epoch": 0.4112342179850554,
        "step": 3192
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.6711335182189941,
        "learning_rate": 8.747617323864816e-05,
        "epoch": 0.41136305076011337,
        "step": 3193
    },
    {
        "loss": 2.4151,
        "grad_norm": 2.0437517166137695,
        "learning_rate": 8.74358783567645e-05,
        "epoch": 0.41149188353517135,
        "step": 3194
    },
    {
        "loss": 2.1018,
        "grad_norm": 2.5545907020568848,
        "learning_rate": 8.739552806807467e-05,
        "epoch": 0.41162071631022934,
        "step": 3195
    },
    {
        "loss": 2.5459,
        "grad_norm": 1.3705716133117676,
        "learning_rate": 8.735512243229895e-05,
        "epoch": 0.41174954908528727,
        "step": 3196
    },
    {
        "loss": 1.0544,
        "grad_norm": 2.277928590774536,
        "learning_rate": 8.731466150923949e-05,
        "epoch": 0.41187838186034526,
        "step": 3197
    },
    {
        "loss": 1.8107,
        "grad_norm": 2.709214448928833,
        "learning_rate": 8.727414535878033e-05,
        "epoch": 0.41200721463540324,
        "step": 3198
    },
    {
        "loss": 2.356,
        "grad_norm": 1.738216519355774,
        "learning_rate": 8.723357404088717e-05,
        "epoch": 0.4121360474104612,
        "step": 3199
    },
    {
        "loss": 0.5693,
        "grad_norm": 2.2995221614837646,
        "learning_rate": 8.719294761560746e-05,
        "epoch": 0.4122648801855192,
        "step": 3200
    },
    {
        "loss": 2.0977,
        "grad_norm": 1.9362231492996216,
        "learning_rate": 8.715226614307016e-05,
        "epoch": 0.4123937129605772,
        "step": 3201
    },
    {
        "loss": 2.6915,
        "grad_norm": 1.588912844657898,
        "learning_rate": 8.711152968348568e-05,
        "epoch": 0.4125225457356351,
        "step": 3202
    },
    {
        "loss": 2.4151,
        "grad_norm": 2.15048885345459,
        "learning_rate": 8.707073829714584e-05,
        "epoch": 0.4126513785106931,
        "step": 3203
    },
    {
        "loss": 2.3486,
        "grad_norm": 1.5449838638305664,
        "learning_rate": 8.702989204442376e-05,
        "epoch": 0.4127802112857511,
        "step": 3204
    },
    {
        "loss": 2.2947,
        "grad_norm": 1.6070542335510254,
        "learning_rate": 8.698899098577375e-05,
        "epoch": 0.4129090440608091,
        "step": 3205
    },
    {
        "loss": 1.6313,
        "grad_norm": 2.1078028678894043,
        "learning_rate": 8.694803518173125e-05,
        "epoch": 0.41303787683586707,
        "step": 3206
    },
    {
        "loss": 2.2088,
        "grad_norm": 1.7540417909622192,
        "learning_rate": 8.69070246929127e-05,
        "epoch": 0.413166709610925,
        "step": 3207
    },
    {
        "loss": 2.5385,
        "grad_norm": 1.6370997428894043,
        "learning_rate": 8.68659595800155e-05,
        "epoch": 0.413295542385983,
        "step": 3208
    },
    {
        "loss": 2.4933,
        "grad_norm": 1.383278250694275,
        "learning_rate": 8.682483990381792e-05,
        "epoch": 0.41342437516104097,
        "step": 3209
    },
    {
        "loss": 2.3883,
        "grad_norm": 1.5602046251296997,
        "learning_rate": 8.678366572517892e-05,
        "epoch": 0.41355320793609895,
        "step": 3210
    },
    {
        "loss": 2.1327,
        "grad_norm": 2.4074137210845947,
        "learning_rate": 8.674243710503815e-05,
        "epoch": 0.41368204071115694,
        "step": 3211
    },
    {
        "loss": 2.4803,
        "grad_norm": 2.028339385986328,
        "learning_rate": 8.670115410441586e-05,
        "epoch": 0.41381087348621487,
        "step": 3212
    },
    {
        "loss": 2.2545,
        "grad_norm": 2.281200647354126,
        "learning_rate": 8.665981678441277e-05,
        "epoch": 0.41393970626127286,
        "step": 3213
    },
    {
        "loss": 2.6563,
        "grad_norm": 1.6088448762893677,
        "learning_rate": 8.661842520621003e-05,
        "epoch": 0.41406853903633084,
        "step": 3214
    },
    {
        "loss": 2.4071,
        "grad_norm": 2.166062116622925,
        "learning_rate": 8.657697943106902e-05,
        "epoch": 0.4141973718113888,
        "step": 3215
    },
    {
        "loss": 1.4157,
        "grad_norm": 3.1830506324768066,
        "learning_rate": 8.65354795203314e-05,
        "epoch": 0.4143262045864468,
        "step": 3216
    },
    {
        "loss": 2.255,
        "grad_norm": 2.8099851608276367,
        "learning_rate": 8.649392553541894e-05,
        "epoch": 0.41445503736150474,
        "step": 3217
    },
    {
        "loss": 1.8105,
        "grad_norm": 1.707899570465088,
        "learning_rate": 8.645231753783338e-05,
        "epoch": 0.4145838701365627,
        "step": 3218
    },
    {
        "loss": 2.0381,
        "grad_norm": 1.9176161289215088,
        "learning_rate": 8.641065558915648e-05,
        "epoch": 0.4147127029116207,
        "step": 3219
    },
    {
        "loss": 1.9982,
        "grad_norm": 1.4610108137130737,
        "learning_rate": 8.636893975104984e-05,
        "epoch": 0.4148415356866787,
        "step": 3220
    },
    {
        "loss": 1.717,
        "grad_norm": 1.813746690750122,
        "learning_rate": 8.632717008525482e-05,
        "epoch": 0.4149703684617367,
        "step": 3221
    },
    {
        "loss": 2.0429,
        "grad_norm": 1.8877681493759155,
        "learning_rate": 8.628534665359238e-05,
        "epoch": 0.41509920123679467,
        "step": 3222
    },
    {
        "loss": 1.4106,
        "grad_norm": 3.0811026096343994,
        "learning_rate": 8.624346951796314e-05,
        "epoch": 0.4152280340118526,
        "step": 3223
    },
    {
        "loss": 1.9513,
        "grad_norm": 2.44523024559021,
        "learning_rate": 8.620153874034712e-05,
        "epoch": 0.4153568667869106,
        "step": 3224
    },
    {
        "loss": 1.9158,
        "grad_norm": 1.7782297134399414,
        "learning_rate": 8.615955438280382e-05,
        "epoch": 0.41548569956196857,
        "step": 3225
    },
    {
        "loss": 2.0768,
        "grad_norm": 2.016761541366577,
        "learning_rate": 8.611751650747201e-05,
        "epoch": 0.41561453233702655,
        "step": 3226
    },
    {
        "loss": 2.2801,
        "grad_norm": 1.9132198095321655,
        "learning_rate": 8.607542517656966e-05,
        "epoch": 0.41574336511208454,
        "step": 3227
    },
    {
        "loss": 2.1914,
        "grad_norm": 2.930907964706421,
        "learning_rate": 8.603328045239386e-05,
        "epoch": 0.41587219788714247,
        "step": 3228
    },
    {
        "loss": 1.6598,
        "grad_norm": 2.6062934398651123,
        "learning_rate": 8.599108239732065e-05,
        "epoch": 0.41600103066220046,
        "step": 3229
    },
    {
        "loss": 2.0619,
        "grad_norm": 1.6421246528625488,
        "learning_rate": 8.59488310738052e-05,
        "epoch": 0.41612986343725844,
        "step": 3230
    },
    {
        "loss": 2.0913,
        "grad_norm": 1.9420101642608643,
        "learning_rate": 8.59065265443813e-05,
        "epoch": 0.4162586962123164,
        "step": 3231
    },
    {
        "loss": 1.8766,
        "grad_norm": 2.376236915588379,
        "learning_rate": 8.586416887166164e-05,
        "epoch": 0.4163875289873744,
        "step": 3232
    },
    {
        "loss": 1.3427,
        "grad_norm": 2.667830467224121,
        "learning_rate": 8.582175811833741e-05,
        "epoch": 0.41651636176243234,
        "step": 3233
    },
    {
        "loss": 2.2003,
        "grad_norm": 1.9466016292572021,
        "learning_rate": 8.57792943471786e-05,
        "epoch": 0.4166451945374903,
        "step": 3234
    },
    {
        "loss": 1.6329,
        "grad_norm": 2.592834234237671,
        "learning_rate": 8.573677762103345e-05,
        "epoch": 0.4167740273125483,
        "step": 3235
    },
    {
        "loss": 1.6643,
        "grad_norm": 1.9136755466461182,
        "learning_rate": 8.569420800282861e-05,
        "epoch": 0.4169028600876063,
        "step": 3236
    },
    {
        "loss": 1.8494,
        "grad_norm": 1.9412411451339722,
        "learning_rate": 8.565158555556912e-05,
        "epoch": 0.4170316928626643,
        "step": 3237
    },
    {
        "loss": 2.4491,
        "grad_norm": 1.7407170534133911,
        "learning_rate": 8.560891034233812e-05,
        "epoch": 0.4171605256377222,
        "step": 3238
    },
    {
        "loss": 2.0641,
        "grad_norm": 2.3193345069885254,
        "learning_rate": 8.556618242629689e-05,
        "epoch": 0.4172893584127802,
        "step": 3239
    },
    {
        "loss": 1.9524,
        "grad_norm": 2.1795434951782227,
        "learning_rate": 8.552340187068467e-05,
        "epoch": 0.4174181911878382,
        "step": 3240
    },
    {
        "loss": 2.48,
        "grad_norm": 1.4924415349960327,
        "learning_rate": 8.548056873881864e-05,
        "epoch": 0.41754702396289617,
        "step": 3241
    },
    {
        "loss": 2.5219,
        "grad_norm": 2.461674690246582,
        "learning_rate": 8.543768309409378e-05,
        "epoch": 0.41767585673795415,
        "step": 3242
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.2042980194091797,
        "learning_rate": 8.539474499998281e-05,
        "epoch": 0.4178046895130121,
        "step": 3243
    },
    {
        "loss": 2.1748,
        "grad_norm": 2.0169525146484375,
        "learning_rate": 8.535175452003605e-05,
        "epoch": 0.41793352228807007,
        "step": 3244
    },
    {
        "loss": 2.3799,
        "grad_norm": 2.4098269939422607,
        "learning_rate": 8.530871171788141e-05,
        "epoch": 0.41806235506312805,
        "step": 3245
    },
    {
        "loss": 2.2529,
        "grad_norm": 2.589984178543091,
        "learning_rate": 8.526561665722416e-05,
        "epoch": 0.41819118783818604,
        "step": 3246
    },
    {
        "loss": 1.9739,
        "grad_norm": 2.102306365966797,
        "learning_rate": 8.522246940184695e-05,
        "epoch": 0.418320020613244,
        "step": 3247
    },
    {
        "loss": 2.1293,
        "grad_norm": 1.908264398574829,
        "learning_rate": 8.517927001560973e-05,
        "epoch": 0.418448853388302,
        "step": 3248
    },
    {
        "loss": 1.416,
        "grad_norm": 2.6500189304351807,
        "learning_rate": 8.51360185624495e-05,
        "epoch": 0.41857768616335994,
        "step": 3249
    },
    {
        "loss": 2.2028,
        "grad_norm": 1.2064368724822998,
        "learning_rate": 8.509271510638043e-05,
        "epoch": 0.4187065189384179,
        "step": 3250
    },
    {
        "loss": 2.1031,
        "grad_norm": 1.2914315462112427,
        "learning_rate": 8.504935971149359e-05,
        "epoch": 0.4188353517134759,
        "step": 3251
    },
    {
        "loss": 2.0232,
        "grad_norm": 2.4126060009002686,
        "learning_rate": 8.500595244195694e-05,
        "epoch": 0.4189641844885339,
        "step": 3252
    },
    {
        "loss": 1.9689,
        "grad_norm": 1.7513870000839233,
        "learning_rate": 8.496249336201525e-05,
        "epoch": 0.4190930172635919,
        "step": 3253
    },
    {
        "loss": 1.8743,
        "grad_norm": 1.9294750690460205,
        "learning_rate": 8.491898253598984e-05,
        "epoch": 0.4192218500386498,
        "step": 3254
    },
    {
        "loss": 2.5599,
        "grad_norm": 1.9596772193908691,
        "learning_rate": 8.487542002827884e-05,
        "epoch": 0.4193506828137078,
        "step": 3255
    },
    {
        "loss": 1.3272,
        "grad_norm": 2.503563404083252,
        "learning_rate": 8.483180590335665e-05,
        "epoch": 0.4194795155887658,
        "step": 3256
    },
    {
        "loss": 2.09,
        "grad_norm": 1.8681461811065674,
        "learning_rate": 8.478814022577423e-05,
        "epoch": 0.41960834836382377,
        "step": 3257
    },
    {
        "loss": 2.4339,
        "grad_norm": 1.5926071405410767,
        "learning_rate": 8.474442306015874e-05,
        "epoch": 0.41973718113888175,
        "step": 3258
    },
    {
        "loss": 2.1544,
        "grad_norm": 2.270900011062622,
        "learning_rate": 8.470065447121358e-05,
        "epoch": 0.4198660139139397,
        "step": 3259
    },
    {
        "loss": 2.5228,
        "grad_norm": 1.7048561573028564,
        "learning_rate": 8.465683452371826e-05,
        "epoch": 0.41999484668899767,
        "step": 3260
    },
    {
        "loss": 2.404,
        "grad_norm": 1.5629962682724,
        "learning_rate": 8.46129632825283e-05,
        "epoch": 0.42012367946405565,
        "step": 3261
    },
    {
        "loss": 2.4049,
        "grad_norm": 1.3018338680267334,
        "learning_rate": 8.456904081257513e-05,
        "epoch": 0.42025251223911364,
        "step": 3262
    },
    {
        "loss": 2.156,
        "grad_norm": 2.3029096126556396,
        "learning_rate": 8.452506717886601e-05,
        "epoch": 0.4203813450141716,
        "step": 3263
    },
    {
        "loss": 2.1849,
        "grad_norm": 2.717088460922241,
        "learning_rate": 8.448104244648396e-05,
        "epoch": 0.42051017778922956,
        "step": 3264
    },
    {
        "loss": 2.1833,
        "grad_norm": 1.7087852954864502,
        "learning_rate": 8.443696668058751e-05,
        "epoch": 0.42063901056428754,
        "step": 3265
    },
    {
        "loss": 2.2895,
        "grad_norm": 1.3610682487487793,
        "learning_rate": 8.43928399464109e-05,
        "epoch": 0.4207678433393455,
        "step": 3266
    },
    {
        "loss": 1.4856,
        "grad_norm": 2.0702545642852783,
        "learning_rate": 8.434866230926361e-05,
        "epoch": 0.4208966761144035,
        "step": 3267
    },
    {
        "loss": 1.9959,
        "grad_norm": 2.5794222354888916,
        "learning_rate": 8.430443383453061e-05,
        "epoch": 0.4210255088894615,
        "step": 3268
    },
    {
        "loss": 2.0124,
        "grad_norm": 1.9089657068252563,
        "learning_rate": 8.426015458767205e-05,
        "epoch": 0.4211543416645194,
        "step": 3269
    },
    {
        "loss": 2.4349,
        "grad_norm": 1.4314723014831543,
        "learning_rate": 8.421582463422324e-05,
        "epoch": 0.4212831744395774,
        "step": 3270
    },
    {
        "loss": 2.0925,
        "grad_norm": 1.7962077856063843,
        "learning_rate": 8.417144403979456e-05,
        "epoch": 0.4214120072146354,
        "step": 3271
    },
    {
        "loss": 2.1001,
        "grad_norm": 2.1918561458587646,
        "learning_rate": 8.412701287007121e-05,
        "epoch": 0.4215408399896934,
        "step": 3272
    },
    {
        "loss": 1.8775,
        "grad_norm": 1.4324064254760742,
        "learning_rate": 8.408253119081346e-05,
        "epoch": 0.42166967276475137,
        "step": 3273
    },
    {
        "loss": 2.1935,
        "grad_norm": 1.7603232860565186,
        "learning_rate": 8.403799906785615e-05,
        "epoch": 0.42179850553980935,
        "step": 3274
    },
    {
        "loss": 2.2011,
        "grad_norm": 3.0365326404571533,
        "learning_rate": 8.399341656710891e-05,
        "epoch": 0.4219273383148673,
        "step": 3275
    },
    {
        "loss": 2.324,
        "grad_norm": 1.679235577583313,
        "learning_rate": 8.394878375455579e-05,
        "epoch": 0.42205617108992527,
        "step": 3276
    },
    {
        "loss": 2.2336,
        "grad_norm": 1.459628701210022,
        "learning_rate": 8.39041006962555e-05,
        "epoch": 0.42218500386498325,
        "step": 3277
    },
    {
        "loss": 2.2616,
        "grad_norm": 1.958008885383606,
        "learning_rate": 8.385936745834092e-05,
        "epoch": 0.42231383664004124,
        "step": 3278
    },
    {
        "loss": 2.0642,
        "grad_norm": 1.9857925176620483,
        "learning_rate": 8.38145841070193e-05,
        "epoch": 0.4224426694150992,
        "step": 3279
    },
    {
        "loss": 1.6544,
        "grad_norm": 2.7114815711975098,
        "learning_rate": 8.376975070857202e-05,
        "epoch": 0.42257150219015716,
        "step": 3280
    },
    {
        "loss": 1.6477,
        "grad_norm": 2.455803394317627,
        "learning_rate": 8.372486732935458e-05,
        "epoch": 0.42270033496521514,
        "step": 3281
    },
    {
        "loss": 1.8596,
        "grad_norm": 2.409266710281372,
        "learning_rate": 8.367993403579646e-05,
        "epoch": 0.4228291677402731,
        "step": 3282
    },
    {
        "loss": 2.1717,
        "grad_norm": 1.9158077239990234,
        "learning_rate": 8.363495089440092e-05,
        "epoch": 0.4229580005153311,
        "step": 3283
    },
    {
        "loss": 2.2029,
        "grad_norm": 1.7029699087142944,
        "learning_rate": 8.358991797174507e-05,
        "epoch": 0.4230868332903891,
        "step": 3284
    },
    {
        "loss": 1.9509,
        "grad_norm": 2.460731267929077,
        "learning_rate": 8.354483533447974e-05,
        "epoch": 0.423215666065447,
        "step": 3285
    },
    {
        "loss": 2.1698,
        "grad_norm": 1.727060079574585,
        "learning_rate": 8.349970304932922e-05,
        "epoch": 0.423344498840505,
        "step": 3286
    },
    {
        "loss": 1.9336,
        "grad_norm": 3.2302660942077637,
        "learning_rate": 8.345452118309139e-05,
        "epoch": 0.423473331615563,
        "step": 3287
    },
    {
        "loss": 2.2185,
        "grad_norm": 1.676284670829773,
        "learning_rate": 8.340928980263748e-05,
        "epoch": 0.423602164390621,
        "step": 3288
    },
    {
        "loss": 2.2012,
        "grad_norm": 1.5122076272964478,
        "learning_rate": 8.336400897491194e-05,
        "epoch": 0.42373099716567897,
        "step": 3289
    },
    {
        "loss": 1.7292,
        "grad_norm": 2.813021659851074,
        "learning_rate": 8.331867876693252e-05,
        "epoch": 0.4238598299407369,
        "step": 3290
    },
    {
        "loss": 1.4987,
        "grad_norm": 2.540250062942505,
        "learning_rate": 8.327329924578997e-05,
        "epoch": 0.4239886627157949,
        "step": 3291
    },
    {
        "loss": 2.1723,
        "grad_norm": 1.9641966819763184,
        "learning_rate": 8.322787047864803e-05,
        "epoch": 0.42411749549085287,
        "step": 3292
    },
    {
        "loss": 2.1749,
        "grad_norm": 1.977035641670227,
        "learning_rate": 8.318239253274336e-05,
        "epoch": 0.42424632826591085,
        "step": 3293
    },
    {
        "loss": 1.9001,
        "grad_norm": 1.7704181671142578,
        "learning_rate": 8.31368654753854e-05,
        "epoch": 0.42437516104096884,
        "step": 3294
    },
    {
        "loss": 2.2394,
        "grad_norm": 1.807760238647461,
        "learning_rate": 8.30912893739563e-05,
        "epoch": 0.42450399381602677,
        "step": 3295
    },
    {
        "loss": 2.4364,
        "grad_norm": 1.7287521362304688,
        "learning_rate": 8.304566429591072e-05,
        "epoch": 0.42463282659108476,
        "step": 3296
    },
    {
        "loss": 1.7746,
        "grad_norm": 3.1328577995300293,
        "learning_rate": 8.299999030877582e-05,
        "epoch": 0.42476165936614274,
        "step": 3297
    },
    {
        "loss": 2.382,
        "grad_norm": 1.8494598865509033,
        "learning_rate": 8.295426748015127e-05,
        "epoch": 0.4248904921412007,
        "step": 3298
    },
    {
        "loss": 2.0223,
        "grad_norm": 2.9346349239349365,
        "learning_rate": 8.29084958777089e-05,
        "epoch": 0.4250193249162587,
        "step": 3299
    },
    {
        "loss": 1.9137,
        "grad_norm": 2.0450544357299805,
        "learning_rate": 8.286267556919278e-05,
        "epoch": 0.4251481576913167,
        "step": 3300
    },
    {
        "loss": 1.8297,
        "grad_norm": 2.3500707149505615,
        "learning_rate": 8.281680662241899e-05,
        "epoch": 0.4252769904663746,
        "step": 3301
    },
    {
        "loss": 2.0765,
        "grad_norm": 2.601562023162842,
        "learning_rate": 8.277088910527577e-05,
        "epoch": 0.4254058232414326,
        "step": 3302
    },
    {
        "loss": 1.8524,
        "grad_norm": 1.5368469953536987,
        "learning_rate": 8.27249230857231e-05,
        "epoch": 0.4255346560164906,
        "step": 3303
    },
    {
        "loss": 1.9976,
        "grad_norm": 1.8402262926101685,
        "learning_rate": 8.267890863179273e-05,
        "epoch": 0.4256634887915486,
        "step": 3304
    },
    {
        "loss": 1.9832,
        "grad_norm": 1.9139294624328613,
        "learning_rate": 8.263284581158817e-05,
        "epoch": 0.42579232156660657,
        "step": 3305
    },
    {
        "loss": 2.3753,
        "grad_norm": 1.7668206691741943,
        "learning_rate": 8.258673469328453e-05,
        "epoch": 0.4259211543416645,
        "step": 3306
    },
    {
        "loss": 2.2624,
        "grad_norm": 2.367743730545044,
        "learning_rate": 8.254057534512838e-05,
        "epoch": 0.4260499871167225,
        "step": 3307
    },
    {
        "loss": 1.893,
        "grad_norm": 1.839760661125183,
        "learning_rate": 8.249436783543758e-05,
        "epoch": 0.42617881989178047,
        "step": 3308
    },
    {
        "loss": 1.9945,
        "grad_norm": 2.1272997856140137,
        "learning_rate": 8.244811223260142e-05,
        "epoch": 0.42630765266683845,
        "step": 3309
    },
    {
        "loss": 2.2136,
        "grad_norm": 1.3167517185211182,
        "learning_rate": 8.240180860508027e-05,
        "epoch": 0.42643648544189644,
        "step": 3310
    },
    {
        "loss": 1.9602,
        "grad_norm": 2.2580113410949707,
        "learning_rate": 8.235545702140562e-05,
        "epoch": 0.42656531821695437,
        "step": 3311
    },
    {
        "loss": 2.03,
        "grad_norm": 1.7268435955047607,
        "learning_rate": 8.230905755017989e-05,
        "epoch": 0.42669415099201236,
        "step": 3312
    },
    {
        "loss": 2.1929,
        "grad_norm": 1.6656872034072876,
        "learning_rate": 8.226261026007649e-05,
        "epoch": 0.42682298376707034,
        "step": 3313
    },
    {
        "loss": 1.7201,
        "grad_norm": 1.7761473655700684,
        "learning_rate": 8.221611521983945e-05,
        "epoch": 0.4269518165421283,
        "step": 3314
    },
    {
        "loss": 2.2575,
        "grad_norm": 2.682983636856079,
        "learning_rate": 8.216957249828358e-05,
        "epoch": 0.4270806493171863,
        "step": 3315
    },
    {
        "loss": 2.3755,
        "grad_norm": 2.3829469680786133,
        "learning_rate": 8.212298216429425e-05,
        "epoch": 0.42720948209224424,
        "step": 3316
    },
    {
        "loss": 2.5521,
        "grad_norm": 1.645466923713684,
        "learning_rate": 8.207634428682724e-05,
        "epoch": 0.4273383148673022,
        "step": 3317
    },
    {
        "loss": 1.8064,
        "grad_norm": 3.3386924266815186,
        "learning_rate": 8.202965893490878e-05,
        "epoch": 0.4274671476423602,
        "step": 3318
    },
    {
        "loss": 1.5455,
        "grad_norm": 3.9132909774780273,
        "learning_rate": 8.198292617763524e-05,
        "epoch": 0.4275959804174182,
        "step": 3319
    },
    {
        "loss": 2.4842,
        "grad_norm": 2.1244564056396484,
        "learning_rate": 8.19361460841733e-05,
        "epoch": 0.4277248131924762,
        "step": 3320
    },
    {
        "loss": 2.2706,
        "grad_norm": 1.8597729206085205,
        "learning_rate": 8.188931872375964e-05,
        "epoch": 0.4278536459675341,
        "step": 3321
    },
    {
        "loss": 2.4266,
        "grad_norm": 2.2114083766937256,
        "learning_rate": 8.184244416570082e-05,
        "epoch": 0.4279824787425921,
        "step": 3322
    },
    {
        "loss": 1.9811,
        "grad_norm": 3.9325249195098877,
        "learning_rate": 8.179552247937336e-05,
        "epoch": 0.4281113115176501,
        "step": 3323
    },
    {
        "loss": 2.1068,
        "grad_norm": 1.443664789199829,
        "learning_rate": 8.17485537342235e-05,
        "epoch": 0.42824014429270807,
        "step": 3324
    },
    {
        "loss": 2.0917,
        "grad_norm": 1.7792969942092896,
        "learning_rate": 8.170153799976714e-05,
        "epoch": 0.42836897706776605,
        "step": 3325
    },
    {
        "loss": 2.1467,
        "grad_norm": 2.016380786895752,
        "learning_rate": 8.165447534558966e-05,
        "epoch": 0.42849780984282404,
        "step": 3326
    },
    {
        "loss": 2.6741,
        "grad_norm": 1.509814739227295,
        "learning_rate": 8.160736584134597e-05,
        "epoch": 0.42862664261788197,
        "step": 3327
    },
    {
        "loss": 1.5483,
        "grad_norm": 2.71343731880188,
        "learning_rate": 8.156020955676028e-05,
        "epoch": 0.42875547539293996,
        "step": 3328
    },
    {
        "loss": 2.0733,
        "grad_norm": 1.792333960533142,
        "learning_rate": 8.151300656162602e-05,
        "epoch": 0.42888430816799794,
        "step": 3329
    },
    {
        "loss": 1.9372,
        "grad_norm": 1.6633820533752441,
        "learning_rate": 8.146575692580579e-05,
        "epoch": 0.4290131409430559,
        "step": 3330
    },
    {
        "loss": 1.6724,
        "grad_norm": 1.9106537103652954,
        "learning_rate": 8.141846071923121e-05,
        "epoch": 0.4291419737181139,
        "step": 3331
    },
    {
        "loss": 2.2347,
        "grad_norm": 1.4004361629486084,
        "learning_rate": 8.137111801190283e-05,
        "epoch": 0.42927080649317184,
        "step": 3332
    },
    {
        "loss": 1.9674,
        "grad_norm": 1.7994962930679321,
        "learning_rate": 8.132372887388998e-05,
        "epoch": 0.4293996392682298,
        "step": 3333
    },
    {
        "loss": 1.9006,
        "grad_norm": 2.8193440437316895,
        "learning_rate": 8.127629337533077e-05,
        "epoch": 0.4295284720432878,
        "step": 3334
    },
    {
        "loss": 2.0445,
        "grad_norm": 2.5136497020721436,
        "learning_rate": 8.122881158643188e-05,
        "epoch": 0.4296573048183458,
        "step": 3335
    },
    {
        "loss": 1.6198,
        "grad_norm": 1.9804022312164307,
        "learning_rate": 8.118128357746854e-05,
        "epoch": 0.4297861375934038,
        "step": 3336
    },
    {
        "loss": 2.5453,
        "grad_norm": 1.5144548416137695,
        "learning_rate": 8.113370941878435e-05,
        "epoch": 0.4299149703684617,
        "step": 3337
    },
    {
        "loss": 1.078,
        "grad_norm": 2.961089849472046,
        "learning_rate": 8.10860891807913e-05,
        "epoch": 0.4300438031435197,
        "step": 3338
    },
    {
        "loss": 2.3609,
        "grad_norm": 1.554162621498108,
        "learning_rate": 8.103842293396944e-05,
        "epoch": 0.4301726359185777,
        "step": 3339
    },
    {
        "loss": 2.0772,
        "grad_norm": 2.723717451095581,
        "learning_rate": 8.099071074886696e-05,
        "epoch": 0.43030146869363567,
        "step": 3340
    },
    {
        "loss": 1.9234,
        "grad_norm": 2.551971673965454,
        "learning_rate": 8.094295269610015e-05,
        "epoch": 0.43043030146869365,
        "step": 3341
    },
    {
        "loss": 2.0285,
        "grad_norm": 2.1254825592041016,
        "learning_rate": 8.089514884635308e-05,
        "epoch": 0.4305591342437516,
        "step": 3342
    },
    {
        "loss": 2.1216,
        "grad_norm": 1.8749827146530151,
        "learning_rate": 8.084729927037762e-05,
        "epoch": 0.43068796701880957,
        "step": 3343
    },
    {
        "loss": 1.9884,
        "grad_norm": 1.5625921487808228,
        "learning_rate": 8.079940403899326e-05,
        "epoch": 0.43081679979386756,
        "step": 3344
    },
    {
        "loss": 2.139,
        "grad_norm": 1.9030267000198364,
        "learning_rate": 8.075146322308725e-05,
        "epoch": 0.43094563256892554,
        "step": 3345
    },
    {
        "loss": 1.7111,
        "grad_norm": 3.7395853996276855,
        "learning_rate": 8.070347689361413e-05,
        "epoch": 0.4310744653439835,
        "step": 3346
    },
    {
        "loss": 1.9367,
        "grad_norm": 2.036032199859619,
        "learning_rate": 8.06554451215958e-05,
        "epoch": 0.4312032981190415,
        "step": 3347
    },
    {
        "loss": 2.4353,
        "grad_norm": 2.161342144012451,
        "learning_rate": 8.060736797812152e-05,
        "epoch": 0.43133213089409944,
        "step": 3348
    },
    {
        "loss": 2.3684,
        "grad_norm": 1.7943469285964966,
        "learning_rate": 8.055924553434764e-05,
        "epoch": 0.4314609636691574,
        "step": 3349
    },
    {
        "loss": 1.9812,
        "grad_norm": 2.6349334716796875,
        "learning_rate": 8.051107786149762e-05,
        "epoch": 0.4315897964442154,
        "step": 3350
    },
    {
        "loss": 2.6823,
        "grad_norm": 2.5239603519439697,
        "learning_rate": 8.046286503086174e-05,
        "epoch": 0.4317186292192734,
        "step": 3351
    },
    {
        "loss": 1.1651,
        "grad_norm": 2.9048118591308594,
        "learning_rate": 8.041460711379723e-05,
        "epoch": 0.4318474619943314,
        "step": 3352
    },
    {
        "loss": 2.1799,
        "grad_norm": 1.6143219470977783,
        "learning_rate": 8.036630418172798e-05,
        "epoch": 0.4319762947693893,
        "step": 3353
    },
    {
        "loss": 1.8787,
        "grad_norm": 2.682438611984253,
        "learning_rate": 8.031795630614454e-05,
        "epoch": 0.4321051275444473,
        "step": 3354
    },
    {
        "loss": 1.6867,
        "grad_norm": 2.8869056701660156,
        "learning_rate": 8.026956355860397e-05,
        "epoch": 0.4322339603195053,
        "step": 3355
    },
    {
        "loss": 2.2802,
        "grad_norm": 1.9838788509368896,
        "learning_rate": 8.022112601072978e-05,
        "epoch": 0.43236279309456327,
        "step": 3356
    },
    {
        "loss": 1.7819,
        "grad_norm": 2.493990182876587,
        "learning_rate": 8.017264373421168e-05,
        "epoch": 0.43249162586962125,
        "step": 3357
    },
    {
        "loss": 2.6165,
        "grad_norm": 1.292471170425415,
        "learning_rate": 8.012411680080567e-05,
        "epoch": 0.4326204586446792,
        "step": 3358
    },
    {
        "loss": 1.5925,
        "grad_norm": 2.6054282188415527,
        "learning_rate": 8.007554528233389e-05,
        "epoch": 0.43274929141973717,
        "step": 3359
    },
    {
        "loss": 1.5396,
        "grad_norm": 2.2117598056793213,
        "learning_rate": 8.00269292506843e-05,
        "epoch": 0.43287812419479516,
        "step": 3360
    },
    {
        "loss": 2.117,
        "grad_norm": 2.245757579803467,
        "learning_rate": 7.997826877781089e-05,
        "epoch": 0.43300695696985314,
        "step": 3361
    },
    {
        "loss": 2.3355,
        "grad_norm": 2.0014641284942627,
        "learning_rate": 7.992956393573338e-05,
        "epoch": 0.4331357897449111,
        "step": 3362
    },
    {
        "loss": 1.9243,
        "grad_norm": 2.7088463306427,
        "learning_rate": 7.98808147965372e-05,
        "epoch": 0.43326462251996906,
        "step": 3363
    },
    {
        "loss": 2.135,
        "grad_norm": 2.3251051902770996,
        "learning_rate": 7.983202143237323e-05,
        "epoch": 0.43339345529502704,
        "step": 3364
    },
    {
        "loss": 2.474,
        "grad_norm": 1.5303971767425537,
        "learning_rate": 7.978318391545786e-05,
        "epoch": 0.433522288070085,
        "step": 3365
    },
    {
        "loss": 1.8171,
        "grad_norm": 1.8035513162612915,
        "learning_rate": 7.973430231807288e-05,
        "epoch": 0.433651120845143,
        "step": 3366
    },
    {
        "loss": 2.0057,
        "grad_norm": 1.9683207273483276,
        "learning_rate": 7.968537671256528e-05,
        "epoch": 0.433779953620201,
        "step": 3367
    },
    {
        "loss": 2.1288,
        "grad_norm": 1.75079345703125,
        "learning_rate": 7.963640717134721e-05,
        "epoch": 0.4339087863952589,
        "step": 3368
    },
    {
        "loss": 2.0451,
        "grad_norm": 2.1796536445617676,
        "learning_rate": 7.958739376689577e-05,
        "epoch": 0.4340376191703169,
        "step": 3369
    },
    {
        "loss": 1.3425,
        "grad_norm": 2.2917699813842773,
        "learning_rate": 7.953833657175308e-05,
        "epoch": 0.4341664519453749,
        "step": 3370
    },
    {
        "loss": 2.4995,
        "grad_norm": 1.5881158113479614,
        "learning_rate": 7.9489235658526e-05,
        "epoch": 0.4342952847204329,
        "step": 3371
    },
    {
        "loss": 2.0649,
        "grad_norm": 2.081731081008911,
        "learning_rate": 7.94400910998861e-05,
        "epoch": 0.43442411749549087,
        "step": 3372
    },
    {
        "loss": 1.67,
        "grad_norm": 1.7383522987365723,
        "learning_rate": 7.939090296856958e-05,
        "epoch": 0.43455295027054885,
        "step": 3373
    },
    {
        "loss": 2.0447,
        "grad_norm": 2.281183958053589,
        "learning_rate": 7.934167133737713e-05,
        "epoch": 0.4346817830456068,
        "step": 3374
    },
    {
        "loss": 2.2416,
        "grad_norm": 2.137263059616089,
        "learning_rate": 7.929239627917384e-05,
        "epoch": 0.43481061582066477,
        "step": 3375
    },
    {
        "loss": 2.1894,
        "grad_norm": 2.7465832233428955,
        "learning_rate": 7.924307786688898e-05,
        "epoch": 0.43493944859572276,
        "step": 3376
    },
    {
        "loss": 1.6505,
        "grad_norm": 2.267982244491577,
        "learning_rate": 7.919371617351609e-05,
        "epoch": 0.43506828137078074,
        "step": 3377
    },
    {
        "loss": 1.8308,
        "grad_norm": 1.6282927989959717,
        "learning_rate": 7.914431127211269e-05,
        "epoch": 0.4351971141458387,
        "step": 3378
    },
    {
        "loss": 2.2199,
        "grad_norm": 2.670588731765747,
        "learning_rate": 7.909486323580029e-05,
        "epoch": 0.43532594692089666,
        "step": 3379
    },
    {
        "loss": 2.139,
        "grad_norm": 2.5868356227874756,
        "learning_rate": 7.904537213776426e-05,
        "epoch": 0.43545477969595464,
        "step": 3380
    },
    {
        "loss": 1.892,
        "grad_norm": 2.8724982738494873,
        "learning_rate": 7.89958380512537e-05,
        "epoch": 0.4355836124710126,
        "step": 3381
    },
    {
        "loss": 1.7484,
        "grad_norm": 4.2071213722229,
        "learning_rate": 7.89462610495813e-05,
        "epoch": 0.4357124452460706,
        "step": 3382
    },
    {
        "loss": 2.0613,
        "grad_norm": 1.486334204673767,
        "learning_rate": 7.889664120612319e-05,
        "epoch": 0.4358412780211286,
        "step": 3383
    },
    {
        "loss": 1.9037,
        "grad_norm": 2.3462467193603516,
        "learning_rate": 7.884697859431916e-05,
        "epoch": 0.4359701107961865,
        "step": 3384
    },
    {
        "loss": 1.8439,
        "grad_norm": 1.9186584949493408,
        "learning_rate": 7.879727328767202e-05,
        "epoch": 0.4360989435712445,
        "step": 3385
    },
    {
        "loss": 2.6204,
        "grad_norm": 1.6659311056137085,
        "learning_rate": 7.874752535974796e-05,
        "epoch": 0.4362277763463025,
        "step": 3386
    },
    {
        "loss": 1.8632,
        "grad_norm": 1.8786174058914185,
        "learning_rate": 7.869773488417609e-05,
        "epoch": 0.4363566091213605,
        "step": 3387
    },
    {
        "loss": 2.036,
        "grad_norm": 1.4844849109649658,
        "learning_rate": 7.864790193464869e-05,
        "epoch": 0.43648544189641847,
        "step": 3388
    },
    {
        "loss": 1.966,
        "grad_norm": 2.199009656906128,
        "learning_rate": 7.859802658492076e-05,
        "epoch": 0.4366142746714764,
        "step": 3389
    },
    {
        "loss": 1.4844,
        "grad_norm": 2.6176950931549072,
        "learning_rate": 7.854810890881005e-05,
        "epoch": 0.4367431074465344,
        "step": 3390
    },
    {
        "loss": 1.9395,
        "grad_norm": 2.593837022781372,
        "learning_rate": 7.849814898019703e-05,
        "epoch": 0.43687194022159237,
        "step": 3391
    },
    {
        "loss": 2.3046,
        "grad_norm": 1.7334359884262085,
        "learning_rate": 7.844814687302465e-05,
        "epoch": 0.43700077299665036,
        "step": 3392
    },
    {
        "loss": 2.193,
        "grad_norm": 2.033085346221924,
        "learning_rate": 7.839810266129837e-05,
        "epoch": 0.43712960577170834,
        "step": 3393
    },
    {
        "loss": 2.2231,
        "grad_norm": 1.9190564155578613,
        "learning_rate": 7.834801641908582e-05,
        "epoch": 0.43725843854676627,
        "step": 3394
    },
    {
        "loss": 1.675,
        "grad_norm": 3.0278568267822266,
        "learning_rate": 7.829788822051699e-05,
        "epoch": 0.43738727132182426,
        "step": 3395
    },
    {
        "loss": 2.2773,
        "grad_norm": 1.5297924280166626,
        "learning_rate": 7.824771813978384e-05,
        "epoch": 0.43751610409688224,
        "step": 3396
    },
    {
        "loss": 1.6414,
        "grad_norm": 2.016906261444092,
        "learning_rate": 7.81975062511404e-05,
        "epoch": 0.4376449368719402,
        "step": 3397
    },
    {
        "loss": 1.8822,
        "grad_norm": 3.1958718299865723,
        "learning_rate": 7.814725262890256e-05,
        "epoch": 0.4377737696469982,
        "step": 3398
    },
    {
        "loss": 2.1917,
        "grad_norm": 2.4985527992248535,
        "learning_rate": 7.809695734744801e-05,
        "epoch": 0.4379026024220562,
        "step": 3399
    },
    {
        "loss": 1.0308,
        "grad_norm": 4.384984493255615,
        "learning_rate": 7.804662048121599e-05,
        "epoch": 0.4380314351971141,
        "step": 3400
    },
    {
        "loss": 1.738,
        "grad_norm": 2.2879223823547363,
        "learning_rate": 7.799624210470735e-05,
        "epoch": 0.4381602679721721,
        "step": 3401
    },
    {
        "loss": 1.4435,
        "grad_norm": 3.00480580329895,
        "learning_rate": 7.794582229248448e-05,
        "epoch": 0.4382891007472301,
        "step": 3402
    },
    {
        "loss": 2.1761,
        "grad_norm": 1.6733149290084839,
        "learning_rate": 7.789536111917089e-05,
        "epoch": 0.4384179335222881,
        "step": 3403
    },
    {
        "loss": 2.4056,
        "grad_norm": 1.6814855337142944,
        "learning_rate": 7.784485865945148e-05,
        "epoch": 0.43854676629734607,
        "step": 3404
    },
    {
        "loss": 2.3382,
        "grad_norm": 2.4268243312835693,
        "learning_rate": 7.779431498807217e-05,
        "epoch": 0.438675599072404,
        "step": 3405
    },
    {
        "loss": 1.7952,
        "grad_norm": 2.1223011016845703,
        "learning_rate": 7.774373017983992e-05,
        "epoch": 0.438804431847462,
        "step": 3406
    },
    {
        "loss": 2.0136,
        "grad_norm": 1.5264716148376465,
        "learning_rate": 7.769310430962256e-05,
        "epoch": 0.43893326462251997,
        "step": 3407
    },
    {
        "loss": 1.8788,
        "grad_norm": 2.396017551422119,
        "learning_rate": 7.764243745234858e-05,
        "epoch": 0.43906209739757796,
        "step": 3408
    },
    {
        "loss": 2.3443,
        "grad_norm": 2.1255593299865723,
        "learning_rate": 7.759172968300739e-05,
        "epoch": 0.43919093017263594,
        "step": 3409
    },
    {
        "loss": 1.9096,
        "grad_norm": 1.4666966199874878,
        "learning_rate": 7.754098107664869e-05,
        "epoch": 0.43931976294769387,
        "step": 3410
    },
    {
        "loss": 1.5621,
        "grad_norm": 2.1223042011260986,
        "learning_rate": 7.749019170838279e-05,
        "epoch": 0.43944859572275186,
        "step": 3411
    },
    {
        "loss": 2.1355,
        "grad_norm": 2.1522672176361084,
        "learning_rate": 7.743936165338023e-05,
        "epoch": 0.43957742849780984,
        "step": 3412
    },
    {
        "loss": 2.0193,
        "grad_norm": 2.063072681427002,
        "learning_rate": 7.738849098687183e-05,
        "epoch": 0.4397062612728678,
        "step": 3413
    },
    {
        "loss": 2.1965,
        "grad_norm": 1.430513858795166,
        "learning_rate": 7.733757978414852e-05,
        "epoch": 0.4398350940479258,
        "step": 3414
    },
    {
        "loss": 2.2481,
        "grad_norm": 1.7610719203948975,
        "learning_rate": 7.728662812056113e-05,
        "epoch": 0.43996392682298374,
        "step": 3415
    },
    {
        "loss": 1.2443,
        "grad_norm": 2.556324005126953,
        "learning_rate": 7.72356360715205e-05,
        "epoch": 0.4400927595980417,
        "step": 3416
    },
    {
        "loss": 1.5561,
        "grad_norm": 2.777949333190918,
        "learning_rate": 7.718460371249716e-05,
        "epoch": 0.4402215923730997,
        "step": 3417
    },
    {
        "loss": 2.2033,
        "grad_norm": 2.768348455429077,
        "learning_rate": 7.713353111902137e-05,
        "epoch": 0.4403504251481577,
        "step": 3418
    },
    {
        "loss": 1.9143,
        "grad_norm": 3.5799381732940674,
        "learning_rate": 7.708241836668285e-05,
        "epoch": 0.4404792579232157,
        "step": 3419
    },
    {
        "loss": 2.6641,
        "grad_norm": 1.8635238409042358,
        "learning_rate": 7.703126553113084e-05,
        "epoch": 0.4406080906982736,
        "step": 3420
    },
    {
        "loss": 1.9887,
        "grad_norm": 1.837812066078186,
        "learning_rate": 7.698007268807382e-05,
        "epoch": 0.4407369234733316,
        "step": 3421
    },
    {
        "loss": 2.3438,
        "grad_norm": 1.0732280015945435,
        "learning_rate": 7.692883991327954e-05,
        "epoch": 0.4408657562483896,
        "step": 3422
    },
    {
        "loss": 1.6164,
        "grad_norm": 2.1699347496032715,
        "learning_rate": 7.687756728257491e-05,
        "epoch": 0.44099458902344757,
        "step": 3423
    },
    {
        "loss": 1.4452,
        "grad_norm": 2.7182719707489014,
        "learning_rate": 7.682625487184569e-05,
        "epoch": 0.44112342179850556,
        "step": 3424
    },
    {
        "loss": 2.4448,
        "grad_norm": 1.5695782899856567,
        "learning_rate": 7.677490275703663e-05,
        "epoch": 0.44125225457356354,
        "step": 3425
    },
    {
        "loss": 1.6305,
        "grad_norm": 3.0937790870666504,
        "learning_rate": 7.67235110141511e-05,
        "epoch": 0.44138108734862147,
        "step": 3426
    },
    {
        "loss": 2.5463,
        "grad_norm": 2.00944447517395,
        "learning_rate": 7.667207971925137e-05,
        "epoch": 0.44150992012367946,
        "step": 3427
    },
    {
        "loss": 1.8296,
        "grad_norm": 3.0001513957977295,
        "learning_rate": 7.662060894845799e-05,
        "epoch": 0.44163875289873744,
        "step": 3428
    },
    {
        "loss": 1.9767,
        "grad_norm": 2.236690044403076,
        "learning_rate": 7.656909877795013e-05,
        "epoch": 0.4417675856737954,
        "step": 3429
    },
    {
        "loss": 2.2683,
        "grad_norm": 1.5641098022460938,
        "learning_rate": 7.651754928396507e-05,
        "epoch": 0.4418964184488534,
        "step": 3430
    },
    {
        "loss": 1.818,
        "grad_norm": 2.119610071182251,
        "learning_rate": 7.646596054279856e-05,
        "epoch": 0.44202525122391134,
        "step": 3431
    },
    {
        "loss": 1.9234,
        "grad_norm": 1.3339647054672241,
        "learning_rate": 7.641433263080421e-05,
        "epoch": 0.4421540839989693,
        "step": 3432
    },
    {
        "loss": 2.0259,
        "grad_norm": 2.8587682247161865,
        "learning_rate": 7.636266562439364e-05,
        "epoch": 0.4422829167740273,
        "step": 3433
    },
    {
        "loss": 1.6232,
        "grad_norm": 2.5750935077667236,
        "learning_rate": 7.631095960003644e-05,
        "epoch": 0.4424117495490853,
        "step": 3434
    },
    {
        "loss": 2.3181,
        "grad_norm": 1.9084745645523071,
        "learning_rate": 7.625921463425987e-05,
        "epoch": 0.4425405823241433,
        "step": 3435
    },
    {
        "loss": 2.1719,
        "grad_norm": 1.150921106338501,
        "learning_rate": 7.620743080364886e-05,
        "epoch": 0.4426694150992012,
        "step": 3436
    },
    {
        "loss": 1.5826,
        "grad_norm": 3.485584259033203,
        "learning_rate": 7.615560818484577e-05,
        "epoch": 0.4427982478742592,
        "step": 3437
    },
    {
        "loss": 2.0588,
        "grad_norm": 2.3041880130767822,
        "learning_rate": 7.610374685455047e-05,
        "epoch": 0.4429270806493172,
        "step": 3438
    },
    {
        "loss": 2.4207,
        "grad_norm": 1.8969613313674927,
        "learning_rate": 7.605184688952015e-05,
        "epoch": 0.44305591342437517,
        "step": 3439
    },
    {
        "loss": 2.6561,
        "grad_norm": 1.6954867839813232,
        "learning_rate": 7.599990836656905e-05,
        "epoch": 0.44318474619943315,
        "step": 3440
    },
    {
        "loss": 2.214,
        "grad_norm": 1.3617916107177734,
        "learning_rate": 7.594793136256855e-05,
        "epoch": 0.4433135789744911,
        "step": 3441
    },
    {
        "loss": 1.5622,
        "grad_norm": 2.5815250873565674,
        "learning_rate": 7.589591595444706e-05,
        "epoch": 0.44344241174954907,
        "step": 3442
    },
    {
        "loss": 2.2041,
        "grad_norm": 1.124411940574646,
        "learning_rate": 7.584386221918968e-05,
        "epoch": 0.44357124452460706,
        "step": 3443
    },
    {
        "loss": 1.4869,
        "grad_norm": 2.256427526473999,
        "learning_rate": 7.579177023383832e-05,
        "epoch": 0.44370007729966504,
        "step": 3444
    },
    {
        "loss": 2.153,
        "grad_norm": 2.0718255043029785,
        "learning_rate": 7.573964007549155e-05,
        "epoch": 0.443828910074723,
        "step": 3445
    },
    {
        "loss": 2.4018,
        "grad_norm": 1.859031081199646,
        "learning_rate": 7.568747182130431e-05,
        "epoch": 0.443957742849781,
        "step": 3446
    },
    {
        "loss": 2.3607,
        "grad_norm": 2.23213267326355,
        "learning_rate": 7.563526554848801e-05,
        "epoch": 0.44408657562483894,
        "step": 3447
    },
    {
        "loss": 1.9065,
        "grad_norm": 1.7782739400863647,
        "learning_rate": 7.558302133431035e-05,
        "epoch": 0.4442154083998969,
        "step": 3448
    },
    {
        "loss": 2.1451,
        "grad_norm": 1.1676225662231445,
        "learning_rate": 7.553073925609514e-05,
        "epoch": 0.4443442411749549,
        "step": 3449
    },
    {
        "loss": 2.171,
        "grad_norm": 2.5172111988067627,
        "learning_rate": 7.547841939122222e-05,
        "epoch": 0.4444730739500129,
        "step": 3450
    },
    {
        "loss": 2.3244,
        "grad_norm": 1.8022788763046265,
        "learning_rate": 7.542606181712732e-05,
        "epoch": 0.4446019067250709,
        "step": 3451
    },
    {
        "loss": 2.2301,
        "grad_norm": 1.9209402799606323,
        "learning_rate": 7.537366661130217e-05,
        "epoch": 0.4447307395001288,
        "step": 3452
    },
    {
        "loss": 1.488,
        "grad_norm": 2.63907790184021,
        "learning_rate": 7.532123385129393e-05,
        "epoch": 0.4448595722751868,
        "step": 3453
    },
    {
        "loss": 1.3476,
        "grad_norm": 2.1066670417785645,
        "learning_rate": 7.526876361470558e-05,
        "epoch": 0.4449884050502448,
        "step": 3454
    },
    {
        "loss": 1.7493,
        "grad_norm": 1.8779451847076416,
        "learning_rate": 7.521625597919537e-05,
        "epoch": 0.44511723782530277,
        "step": 3455
    },
    {
        "loss": 2.082,
        "grad_norm": 2.4417574405670166,
        "learning_rate": 7.516371102247705e-05,
        "epoch": 0.44524607060036075,
        "step": 3456
    },
    {
        "loss": 2.1074,
        "grad_norm": 2.9555230140686035,
        "learning_rate": 7.511112882231956e-05,
        "epoch": 0.4453749033754187,
        "step": 3457
    },
    {
        "loss": 2.42,
        "grad_norm": 1.5867815017700195,
        "learning_rate": 7.50585094565469e-05,
        "epoch": 0.44550373615047667,
        "step": 3458
    },
    {
        "loss": 2.3135,
        "grad_norm": 2.1513726711273193,
        "learning_rate": 7.500585300303815e-05,
        "epoch": 0.44563256892553466,
        "step": 3459
    },
    {
        "loss": 1.7739,
        "grad_norm": 2.0490336418151855,
        "learning_rate": 7.495315953972727e-05,
        "epoch": 0.44576140170059264,
        "step": 3460
    },
    {
        "loss": 2.1373,
        "grad_norm": 1.7057963609695435,
        "learning_rate": 7.490042914460302e-05,
        "epoch": 0.4458902344756506,
        "step": 3461
    },
    {
        "loss": 1.804,
        "grad_norm": 1.8149806261062622,
        "learning_rate": 7.484766189570873e-05,
        "epoch": 0.44601906725070856,
        "step": 3462
    },
    {
        "loss": 2.4133,
        "grad_norm": 1.650868535041809,
        "learning_rate": 7.47948578711424e-05,
        "epoch": 0.44614790002576654,
        "step": 3463
    },
    {
        "loss": 1.8933,
        "grad_norm": 1.818161129951477,
        "learning_rate": 7.474201714905631e-05,
        "epoch": 0.4462767328008245,
        "step": 3464
    },
    {
        "loss": 1.9965,
        "grad_norm": 2.2248194217681885,
        "learning_rate": 7.468913980765717e-05,
        "epoch": 0.4464055655758825,
        "step": 3465
    },
    {
        "loss": 1.8479,
        "grad_norm": 3.6777408123016357,
        "learning_rate": 7.463622592520588e-05,
        "epoch": 0.4465343983509405,
        "step": 3466
    },
    {
        "loss": 1.0771,
        "grad_norm": 2.616340398788452,
        "learning_rate": 7.458327558001741e-05,
        "epoch": 0.44666323112599843,
        "step": 3467
    },
    {
        "loss": 1.5851,
        "grad_norm": 3.226442575454712,
        "learning_rate": 7.45302888504606e-05,
        "epoch": 0.4467920639010564,
        "step": 3468
    },
    {
        "loss": 2.4091,
        "grad_norm": 2.019932270050049,
        "learning_rate": 7.447726581495831e-05,
        "epoch": 0.4469208966761144,
        "step": 3469
    },
    {
        "loss": 2.0297,
        "grad_norm": 2.7034108638763428,
        "learning_rate": 7.442420655198704e-05,
        "epoch": 0.4470497294511724,
        "step": 3470
    },
    {
        "loss": 2.4197,
        "grad_norm": 2.2504172325134277,
        "learning_rate": 7.437111114007686e-05,
        "epoch": 0.44717856222623037,
        "step": 3471
    },
    {
        "loss": 2.2377,
        "grad_norm": 1.1907496452331543,
        "learning_rate": 7.431797965781148e-05,
        "epoch": 0.44730739500128835,
        "step": 3472
    },
    {
        "loss": 2.4276,
        "grad_norm": 1.174985647201538,
        "learning_rate": 7.426481218382782e-05,
        "epoch": 0.4474362277763463,
        "step": 3473
    },
    {
        "loss": 2.1388,
        "grad_norm": 1.5167189836502075,
        "learning_rate": 7.421160879681626e-05,
        "epoch": 0.44756506055140427,
        "step": 3474
    },
    {
        "loss": 2.5289,
        "grad_norm": 1.9571807384490967,
        "learning_rate": 7.41583695755202e-05,
        "epoch": 0.44769389332646226,
        "step": 3475
    },
    {
        "loss": 1.7614,
        "grad_norm": 2.4219090938568115,
        "learning_rate": 7.410509459873608e-05,
        "epoch": 0.44782272610152024,
        "step": 3476
    },
    {
        "loss": 2.0344,
        "grad_norm": 1.519473671913147,
        "learning_rate": 7.405178394531331e-05,
        "epoch": 0.4479515588765782,
        "step": 3477
    },
    {
        "loss": 1.9722,
        "grad_norm": 2.6542444229125977,
        "learning_rate": 7.39984376941541e-05,
        "epoch": 0.44808039165163616,
        "step": 3478
    },
    {
        "loss": 2.2318,
        "grad_norm": 1.7988781929016113,
        "learning_rate": 7.394505592421338e-05,
        "epoch": 0.44820922442669414,
        "step": 3479
    },
    {
        "loss": 2.5794,
        "grad_norm": 1.4383548498153687,
        "learning_rate": 7.389163871449853e-05,
        "epoch": 0.4483380572017521,
        "step": 3480
    },
    {
        "loss": 2.0377,
        "grad_norm": 1.8374940156936646,
        "learning_rate": 7.383818614406946e-05,
        "epoch": 0.4484668899768101,
        "step": 3481
    },
    {
        "loss": 1.7328,
        "grad_norm": 2.7210607528686523,
        "learning_rate": 7.378469829203847e-05,
        "epoch": 0.4485957227518681,
        "step": 3482
    },
    {
        "loss": 1.9375,
        "grad_norm": 1.3134520053863525,
        "learning_rate": 7.373117523756994e-05,
        "epoch": 0.44872455552692603,
        "step": 3483
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.0942888259887695,
        "learning_rate": 7.367761705988047e-05,
        "epoch": 0.448853388301984,
        "step": 3484
    },
    {
        "loss": 2.1581,
        "grad_norm": 1.9397246837615967,
        "learning_rate": 7.362402383823863e-05,
        "epoch": 0.448982221077042,
        "step": 3485
    },
    {
        "loss": 1.4608,
        "grad_norm": 2.442718505859375,
        "learning_rate": 7.357039565196478e-05,
        "epoch": 0.4491110538521,
        "step": 3486
    },
    {
        "loss": 2.1381,
        "grad_norm": 1.7121083736419678,
        "learning_rate": 7.35167325804311e-05,
        "epoch": 0.44923988662715797,
        "step": 3487
    },
    {
        "loss": 1.8795,
        "grad_norm": 2.824758291244507,
        "learning_rate": 7.346303470306141e-05,
        "epoch": 0.4493687194022159,
        "step": 3488
    },
    {
        "loss": 1.2486,
        "grad_norm": 5.315161228179932,
        "learning_rate": 7.340930209933099e-05,
        "epoch": 0.4494975521772739,
        "step": 3489
    },
    {
        "loss": 2.1884,
        "grad_norm": 1.2458792924880981,
        "learning_rate": 7.33555348487665e-05,
        "epoch": 0.44962638495233187,
        "step": 3490
    },
    {
        "loss": 2.3006,
        "grad_norm": 1.451444149017334,
        "learning_rate": 7.330173303094598e-05,
        "epoch": 0.44975521772738986,
        "step": 3491
    },
    {
        "loss": 1.5859,
        "grad_norm": 2.8479974269866943,
        "learning_rate": 7.32478967254986e-05,
        "epoch": 0.44988405050244784,
        "step": 3492
    },
    {
        "loss": 2.0237,
        "grad_norm": 2.5980887413024902,
        "learning_rate": 7.319402601210449e-05,
        "epoch": 0.45001288327750577,
        "step": 3493
    },
    {
        "loss": 1.935,
        "grad_norm": 1.7726911306381226,
        "learning_rate": 7.314012097049472e-05,
        "epoch": 0.45014171605256376,
        "step": 3494
    },
    {
        "loss": 2.1082,
        "grad_norm": 2.0569040775299072,
        "learning_rate": 7.308618168045132e-05,
        "epoch": 0.45027054882762174,
        "step": 3495
    },
    {
        "loss": 2.2893,
        "grad_norm": 2.180630683898926,
        "learning_rate": 7.303220822180681e-05,
        "epoch": 0.4503993816026797,
        "step": 3496
    },
    {
        "loss": 2.1148,
        "grad_norm": 1.8597397804260254,
        "learning_rate": 7.297820067444445e-05,
        "epoch": 0.4505282143777377,
        "step": 3497
    },
    {
        "loss": 1.9796,
        "grad_norm": 1.3584119081497192,
        "learning_rate": 7.292415911829773e-05,
        "epoch": 0.4506570471527957,
        "step": 3498
    },
    {
        "loss": 2.2062,
        "grad_norm": 1.2842035293579102,
        "learning_rate": 7.287008363335077e-05,
        "epoch": 0.4507858799278536,
        "step": 3499
    },
    {
        "loss": 2.5517,
        "grad_norm": 1.4147374629974365,
        "learning_rate": 7.281597429963769e-05,
        "epoch": 0.4509147127029116,
        "step": 3500
    },
    {
        "loss": 2.1537,
        "grad_norm": 1.5982686281204224,
        "learning_rate": 7.276183119724275e-05,
        "epoch": 0.4510435454779696,
        "step": 3501
    },
    {
        "loss": 2.3304,
        "grad_norm": 1.8294459581375122,
        "learning_rate": 7.270765440630022e-05,
        "epoch": 0.4511723782530276,
        "step": 3502
    },
    {
        "loss": 1.7132,
        "grad_norm": 2.2956740856170654,
        "learning_rate": 7.265344400699423e-05,
        "epoch": 0.45130121102808557,
        "step": 3503
    },
    {
        "loss": 2.0769,
        "grad_norm": 3.339562177658081,
        "learning_rate": 7.259920007955866e-05,
        "epoch": 0.4514300438031435,
        "step": 3504
    },
    {
        "loss": 1.9214,
        "grad_norm": 2.421482801437378,
        "learning_rate": 7.254492270427696e-05,
        "epoch": 0.4515588765782015,
        "step": 3505
    },
    {
        "loss": 2.1431,
        "grad_norm": 1.7164227962493896,
        "learning_rate": 7.249061196148216e-05,
        "epoch": 0.45168770935325947,
        "step": 3506
    },
    {
        "loss": 1.347,
        "grad_norm": 4.428704738616943,
        "learning_rate": 7.243626793155658e-05,
        "epoch": 0.45181654212831746,
        "step": 3507
    },
    {
        "loss": 1.9041,
        "grad_norm": 3.508202314376831,
        "learning_rate": 7.238189069493187e-05,
        "epoch": 0.45194537490337544,
        "step": 3508
    },
    {
        "loss": 1.6718,
        "grad_norm": 1.975854754447937,
        "learning_rate": 7.232748033208886e-05,
        "epoch": 0.45207420767843337,
        "step": 3509
    },
    {
        "loss": 2.1946,
        "grad_norm": 2.2566328048706055,
        "learning_rate": 7.227303692355736e-05,
        "epoch": 0.45220304045349136,
        "step": 3510
    },
    {
        "loss": 2.0022,
        "grad_norm": 2.254107713699341,
        "learning_rate": 7.221856054991606e-05,
        "epoch": 0.45233187322854934,
        "step": 3511
    },
    {
        "loss": 2.3091,
        "grad_norm": 1.6587311029434204,
        "learning_rate": 7.216405129179249e-05,
        "epoch": 0.4524607060036073,
        "step": 3512
    },
    {
        "loss": 2.1652,
        "grad_norm": 2.0579416751861572,
        "learning_rate": 7.210950922986289e-05,
        "epoch": 0.4525895387786653,
        "step": 3513
    },
    {
        "loss": 2.3857,
        "grad_norm": 1.4630619287490845,
        "learning_rate": 7.205493444485191e-05,
        "epoch": 0.45271837155372324,
        "step": 3514
    },
    {
        "loss": 1.6091,
        "grad_norm": 2.5167770385742188,
        "learning_rate": 7.200032701753275e-05,
        "epoch": 0.4528472043287812,
        "step": 3515
    },
    {
        "loss": 2.0308,
        "grad_norm": 1.9140421152114868,
        "learning_rate": 7.194568702872691e-05,
        "epoch": 0.4529760371038392,
        "step": 3516
    },
    {
        "loss": 2.0822,
        "grad_norm": 2.797093629837036,
        "learning_rate": 7.189101455930407e-05,
        "epoch": 0.4531048698788972,
        "step": 3517
    },
    {
        "loss": 2.2384,
        "grad_norm": 1.7901239395141602,
        "learning_rate": 7.183630969018198e-05,
        "epoch": 0.4532337026539552,
        "step": 3518
    },
    {
        "loss": 0.7644,
        "grad_norm": 4.451694488525391,
        "learning_rate": 7.178157250232629e-05,
        "epoch": 0.4533625354290131,
        "step": 3519
    },
    {
        "loss": 2.2619,
        "grad_norm": 1.204492211341858,
        "learning_rate": 7.172680307675055e-05,
        "epoch": 0.4534913682040711,
        "step": 3520
    },
    {
        "loss": 2.2031,
        "grad_norm": 1.3522967100143433,
        "learning_rate": 7.167200149451603e-05,
        "epoch": 0.4536202009791291,
        "step": 3521
    },
    {
        "loss": 2.2732,
        "grad_norm": 1.5959937572479248,
        "learning_rate": 7.161716783673159e-05,
        "epoch": 0.45374903375418707,
        "step": 3522
    },
    {
        "loss": 1.9568,
        "grad_norm": 2.398787021636963,
        "learning_rate": 7.156230218455348e-05,
        "epoch": 0.45387786652924506,
        "step": 3523
    },
    {
        "loss": 0.9411,
        "grad_norm": 2.673741340637207,
        "learning_rate": 7.15074046191854e-05,
        "epoch": 0.45400669930430304,
        "step": 3524
    },
    {
        "loss": 1.7784,
        "grad_norm": 2.974018096923828,
        "learning_rate": 7.145247522187825e-05,
        "epoch": 0.45413553207936097,
        "step": 3525
    },
    {
        "loss": 1.8677,
        "grad_norm": 2.0558114051818848,
        "learning_rate": 7.139751407393003e-05,
        "epoch": 0.45426436485441896,
        "step": 3526
    },
    {
        "loss": 1.3971,
        "grad_norm": 2.1131479740142822,
        "learning_rate": 7.134252125668575e-05,
        "epoch": 0.45439319762947694,
        "step": 3527
    },
    {
        "loss": 1.2604,
        "grad_norm": 2.903949499130249,
        "learning_rate": 7.128749685153729e-05,
        "epoch": 0.4545220304045349,
        "step": 3528
    },
    {
        "loss": 2.4581,
        "grad_norm": 1.5009297132492065,
        "learning_rate": 7.12324409399233e-05,
        "epoch": 0.4546508631795929,
        "step": 3529
    },
    {
        "loss": 1.9811,
        "grad_norm": 1.253826379776001,
        "learning_rate": 7.1177353603329e-05,
        "epoch": 0.45477969595465084,
        "step": 3530
    },
    {
        "loss": 2.056,
        "grad_norm": 1.7754054069519043,
        "learning_rate": 7.112223492328617e-05,
        "epoch": 0.4549085287297088,
        "step": 3531
    },
    {
        "loss": 0.8228,
        "grad_norm": 3.1542694568634033,
        "learning_rate": 7.106708498137296e-05,
        "epoch": 0.4550373615047668,
        "step": 3532
    },
    {
        "loss": 1.8444,
        "grad_norm": 1.928189754486084,
        "learning_rate": 7.101190385921381e-05,
        "epoch": 0.4551661942798248,
        "step": 3533
    },
    {
        "loss": 2.3477,
        "grad_norm": 2.4279401302337646,
        "learning_rate": 7.09566916384793e-05,
        "epoch": 0.4552950270548828,
        "step": 3534
    },
    {
        "loss": 2.0663,
        "grad_norm": 1.4892266988754272,
        "learning_rate": 7.090144840088603e-05,
        "epoch": 0.4554238598299407,
        "step": 3535
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.5994988679885864,
        "learning_rate": 7.084617422819653e-05,
        "epoch": 0.4555526926049987,
        "step": 3536
    },
    {
        "loss": 1.8272,
        "grad_norm": 1.4226969480514526,
        "learning_rate": 7.079086920221901e-05,
        "epoch": 0.4556815253800567,
        "step": 3537
    },
    {
        "loss": 2.0227,
        "grad_norm": 2.610459566116333,
        "learning_rate": 7.073553340480757e-05,
        "epoch": 0.45581035815511467,
        "step": 3538
    },
    {
        "loss": 1.7668,
        "grad_norm": 1.5646451711654663,
        "learning_rate": 7.068016691786157e-05,
        "epoch": 0.45593919093017266,
        "step": 3539
    },
    {
        "loss": 1.9337,
        "grad_norm": 2.2521235942840576,
        "learning_rate": 7.062476982332604e-05,
        "epoch": 0.4560680237052306,
        "step": 3540
    },
    {
        "loss": 2.3869,
        "grad_norm": 1.7290515899658203,
        "learning_rate": 7.056934220319109e-05,
        "epoch": 0.45619685648028857,
        "step": 3541
    },
    {
        "loss": 2.105,
        "grad_norm": 2.60341215133667,
        "learning_rate": 7.051388413949227e-05,
        "epoch": 0.45632568925534656,
        "step": 3542
    },
    {
        "loss": 2.1135,
        "grad_norm": 2.1471359729766846,
        "learning_rate": 7.045839571430996e-05,
        "epoch": 0.45645452203040454,
        "step": 3543
    },
    {
        "loss": 2.3107,
        "grad_norm": 1.5109113454818726,
        "learning_rate": 7.040287700976953e-05,
        "epoch": 0.4565833548054625,
        "step": 3544
    },
    {
        "loss": 1.904,
        "grad_norm": 2.8909974098205566,
        "learning_rate": 7.034732810804125e-05,
        "epoch": 0.45671218758052046,
        "step": 3545
    },
    {
        "loss": 2.0128,
        "grad_norm": 2.175903797149658,
        "learning_rate": 7.029174909133998e-05,
        "epoch": 0.45684102035557844,
        "step": 3546
    },
    {
        "loss": 1.8523,
        "grad_norm": 3.0041122436523438,
        "learning_rate": 7.023614004192528e-05,
        "epoch": 0.4569698531306364,
        "step": 3547
    },
    {
        "loss": 1.6003,
        "grad_norm": 2.802973508834839,
        "learning_rate": 7.018050104210097e-05,
        "epoch": 0.4570986859056944,
        "step": 3548
    },
    {
        "loss": 1.9465,
        "grad_norm": 1.7896705865859985,
        "learning_rate": 7.01248321742154e-05,
        "epoch": 0.4572275186807524,
        "step": 3549
    },
    {
        "loss": 2.0556,
        "grad_norm": 1.8437474966049194,
        "learning_rate": 7.006913352066094e-05,
        "epoch": 0.4573563514558104,
        "step": 3550
    },
    {
        "loss": 1.8678,
        "grad_norm": 3.066889524459839,
        "learning_rate": 7.00134051638742e-05,
        "epoch": 0.4574851842308683,
        "step": 3551
    },
    {
        "loss": 2.3242,
        "grad_norm": 2.7214694023132324,
        "learning_rate": 6.995764718633566e-05,
        "epoch": 0.4576140170059263,
        "step": 3552
    },
    {
        "loss": 1.9724,
        "grad_norm": 1.3333548307418823,
        "learning_rate": 6.990185967056971e-05,
        "epoch": 0.4577428497809843,
        "step": 3553
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.9990965127944946,
        "learning_rate": 6.984604269914436e-05,
        "epoch": 0.45787168255604227,
        "step": 3554
    },
    {
        "loss": 2.1074,
        "grad_norm": 1.4109199047088623,
        "learning_rate": 6.97901963546713e-05,
        "epoch": 0.45800051533110026,
        "step": 3555
    },
    {
        "loss": 1.0722,
        "grad_norm": 2.52771258354187,
        "learning_rate": 6.97343207198057e-05,
        "epoch": 0.4581293481061582,
        "step": 3556
    },
    {
        "loss": 1.7897,
        "grad_norm": 2.4624924659729004,
        "learning_rate": 6.967841587724596e-05,
        "epoch": 0.45825818088121617,
        "step": 3557
    },
    {
        "loss": 2.0034,
        "grad_norm": 2.272261619567871,
        "learning_rate": 6.962248190973384e-05,
        "epoch": 0.45838701365627416,
        "step": 3558
    },
    {
        "loss": 2.0266,
        "grad_norm": 1.6430456638336182,
        "learning_rate": 6.95665189000542e-05,
        "epoch": 0.45851584643133214,
        "step": 3559
    },
    {
        "loss": 2.265,
        "grad_norm": 1.6354995965957642,
        "learning_rate": 6.95105269310348e-05,
        "epoch": 0.4586446792063901,
        "step": 3560
    },
    {
        "loss": 2.3235,
        "grad_norm": 1.7972244024276733,
        "learning_rate": 6.945450608554633e-05,
        "epoch": 0.45877351198144806,
        "step": 3561
    },
    {
        "loss": 2.0286,
        "grad_norm": 2.050445079803467,
        "learning_rate": 6.939845644650215e-05,
        "epoch": 0.45890234475650604,
        "step": 3562
    },
    {
        "loss": 2.1875,
        "grad_norm": 1.3752315044403076,
        "learning_rate": 6.934237809685833e-05,
        "epoch": 0.459031177531564,
        "step": 3563
    },
    {
        "loss": 1.8606,
        "grad_norm": 2.8245949745178223,
        "learning_rate": 6.928627111961335e-05,
        "epoch": 0.459160010306622,
        "step": 3564
    },
    {
        "loss": 1.9581,
        "grad_norm": 1.9910593032836914,
        "learning_rate": 6.923013559780813e-05,
        "epoch": 0.45928884308168,
        "step": 3565
    },
    {
        "loss": 1.0418,
        "grad_norm": 4.420327663421631,
        "learning_rate": 6.917397161452576e-05,
        "epoch": 0.45941767585673793,
        "step": 3566
    },
    {
        "loss": 2.3846,
        "grad_norm": 1.4784328937530518,
        "learning_rate": 6.91177792528915e-05,
        "epoch": 0.4595465086317959,
        "step": 3567
    },
    {
        "loss": 2.1488,
        "grad_norm": 1.9842143058776855,
        "learning_rate": 6.906155859607266e-05,
        "epoch": 0.4596753414068539,
        "step": 3568
    },
    {
        "loss": 1.164,
        "grad_norm": 2.4036269187927246,
        "learning_rate": 6.90053097272783e-05,
        "epoch": 0.4598041741819119,
        "step": 3569
    },
    {
        "loss": 2.4837,
        "grad_norm": 1.8110721111297607,
        "learning_rate": 6.894903272975934e-05,
        "epoch": 0.45993300695696987,
        "step": 3570
    },
    {
        "loss": 2.3473,
        "grad_norm": 2.6632344722747803,
        "learning_rate": 6.88927276868083e-05,
        "epoch": 0.46006183973202786,
        "step": 3571
    },
    {
        "loss": 2.1699,
        "grad_norm": 1.2829452753067017,
        "learning_rate": 6.883639468175927e-05,
        "epoch": 0.4601906725070858,
        "step": 3572
    },
    {
        "loss": 2.0615,
        "grad_norm": 3.2543814182281494,
        "learning_rate": 6.878003379798756e-05,
        "epoch": 0.46031950528214377,
        "step": 3573
    },
    {
        "loss": 2.0103,
        "grad_norm": 3.703887939453125,
        "learning_rate": 6.872364511890992e-05,
        "epoch": 0.46044833805720176,
        "step": 3574
    },
    {
        "loss": 1.9759,
        "grad_norm": 2.3326027393341064,
        "learning_rate": 6.866722872798413e-05,
        "epoch": 0.46057717083225974,
        "step": 3575
    },
    {
        "loss": 2.2553,
        "grad_norm": 1.3487516641616821,
        "learning_rate": 6.861078470870902e-05,
        "epoch": 0.4607060036073177,
        "step": 3576
    },
    {
        "loss": 2.118,
        "grad_norm": 1.840415596961975,
        "learning_rate": 6.855431314462432e-05,
        "epoch": 0.46083483638237566,
        "step": 3577
    },
    {
        "loss": 1.8589,
        "grad_norm": 2.7460415363311768,
        "learning_rate": 6.849781411931056e-05,
        "epoch": 0.46096366915743364,
        "step": 3578
    },
    {
        "loss": 1.801,
        "grad_norm": 3.715364694595337,
        "learning_rate": 6.844128771638883e-05,
        "epoch": 0.4610925019324916,
        "step": 3579
    },
    {
        "loss": 2.1695,
        "grad_norm": 1.4851750135421753,
        "learning_rate": 6.838473401952073e-05,
        "epoch": 0.4612213347075496,
        "step": 3580
    },
    {
        "loss": 1.9819,
        "grad_norm": 3.425670623779297,
        "learning_rate": 6.832815311240841e-05,
        "epoch": 0.4613501674826076,
        "step": 3581
    },
    {
        "loss": 2.034,
        "grad_norm": 3.199610710144043,
        "learning_rate": 6.827154507879412e-05,
        "epoch": 0.46147900025766553,
        "step": 3582
    },
    {
        "loss": 2.0792,
        "grad_norm": 2.315337657928467,
        "learning_rate": 6.821491000246038e-05,
        "epoch": 0.4616078330327235,
        "step": 3583
    },
    {
        "loss": 2.4731,
        "grad_norm": 1.4368963241577148,
        "learning_rate": 6.815824796722962e-05,
        "epoch": 0.4617366658077815,
        "step": 3584
    },
    {
        "loss": 1.9555,
        "grad_norm": 2.5093538761138916,
        "learning_rate": 6.810155905696433e-05,
        "epoch": 0.4618654985828395,
        "step": 3585
    },
    {
        "loss": 2.5117,
        "grad_norm": 2.536055564880371,
        "learning_rate": 6.804484335556662e-05,
        "epoch": 0.46199433135789747,
        "step": 3586
    },
    {
        "loss": 1.9456,
        "grad_norm": 2.383369207382202,
        "learning_rate": 6.798810094697829e-05,
        "epoch": 0.4621231641329554,
        "step": 3587
    },
    {
        "loss": 1.9031,
        "grad_norm": 1.8633240461349487,
        "learning_rate": 6.793133191518072e-05,
        "epoch": 0.4622519969080134,
        "step": 3588
    },
    {
        "loss": 1.5296,
        "grad_norm": 2.424635648727417,
        "learning_rate": 6.787453634419466e-05,
        "epoch": 0.46238082968307137,
        "step": 3589
    },
    {
        "loss": 1.8612,
        "grad_norm": 2.2665841579437256,
        "learning_rate": 6.781771431808018e-05,
        "epoch": 0.46250966245812936,
        "step": 3590
    },
    {
        "loss": 1.6164,
        "grad_norm": 2.701140880584717,
        "learning_rate": 6.77608659209364e-05,
        "epoch": 0.46263849523318734,
        "step": 3591
    },
    {
        "loss": 2.1696,
        "grad_norm": 2.751380443572998,
        "learning_rate": 6.770399123690161e-05,
        "epoch": 0.46276732800824527,
        "step": 3592
    },
    {
        "loss": 2.1894,
        "grad_norm": 2.557598114013672,
        "learning_rate": 6.764709035015288e-05,
        "epoch": 0.46289616078330326,
        "step": 3593
    },
    {
        "loss": 2.2054,
        "grad_norm": 2.0013482570648193,
        "learning_rate": 6.75901633449061e-05,
        "epoch": 0.46302499355836124,
        "step": 3594
    },
    {
        "loss": 1.7371,
        "grad_norm": 1.7086817026138306,
        "learning_rate": 6.753321030541591e-05,
        "epoch": 0.4631538263334192,
        "step": 3595
    },
    {
        "loss": 2.6025,
        "grad_norm": 1.5449955463409424,
        "learning_rate": 6.747623131597538e-05,
        "epoch": 0.4632826591084772,
        "step": 3596
    },
    {
        "loss": 1.7807,
        "grad_norm": 1.6868393421173096,
        "learning_rate": 6.741922646091595e-05,
        "epoch": 0.4634114918835352,
        "step": 3597
    },
    {
        "loss": 2.0146,
        "grad_norm": 2.2692744731903076,
        "learning_rate": 6.736219582460745e-05,
        "epoch": 0.46354032465859313,
        "step": 3598
    },
    {
        "loss": 1.9297,
        "grad_norm": 2.4216485023498535,
        "learning_rate": 6.730513949145785e-05,
        "epoch": 0.4636691574336511,
        "step": 3599
    },
    {
        "loss": 1.9375,
        "grad_norm": 1.7081663608551025,
        "learning_rate": 6.724805754591305e-05,
        "epoch": 0.4637979902087091,
        "step": 3600
    },
    {
        "loss": 2.0374,
        "grad_norm": 1.8759537935256958,
        "learning_rate": 6.7190950072457e-05,
        "epoch": 0.4639268229837671,
        "step": 3601
    },
    {
        "loss": 2.2591,
        "grad_norm": 1.64015793800354,
        "learning_rate": 6.71338171556113e-05,
        "epoch": 0.46405565575882507,
        "step": 3602
    },
    {
        "loss": 1.4012,
        "grad_norm": 2.602681875228882,
        "learning_rate": 6.707665887993536e-05,
        "epoch": 0.464184488533883,
        "step": 3603
    },
    {
        "loss": 2.0841,
        "grad_norm": 1.9620447158813477,
        "learning_rate": 6.701947533002598e-05,
        "epoch": 0.464313321308941,
        "step": 3604
    },
    {
        "loss": 1.9946,
        "grad_norm": 2.472726345062256,
        "learning_rate": 6.696226659051735e-05,
        "epoch": 0.46444215408399897,
        "step": 3605
    },
    {
        "loss": 1.3276,
        "grad_norm": 3.3418214321136475,
        "learning_rate": 6.690503274608114e-05,
        "epoch": 0.46457098685905696,
        "step": 3606
    },
    {
        "loss": 2.5509,
        "grad_norm": 2.1912925243377686,
        "learning_rate": 6.684777388142597e-05,
        "epoch": 0.46469981963411494,
        "step": 3607
    },
    {
        "loss": 2.2888,
        "grad_norm": 2.8340682983398438,
        "learning_rate": 6.679049008129761e-05,
        "epoch": 0.46482865240917287,
        "step": 3608
    },
    {
        "loss": 2.3265,
        "grad_norm": 1.3764240741729736,
        "learning_rate": 6.673318143047865e-05,
        "epoch": 0.46495748518423086,
        "step": 3609
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.6984493732452393,
        "learning_rate": 6.667584801378851e-05,
        "epoch": 0.46508631795928884,
        "step": 3610
    },
    {
        "loss": 1.922,
        "grad_norm": 2.395747661590576,
        "learning_rate": 6.661848991608333e-05,
        "epoch": 0.4652151507343468,
        "step": 3611
    },
    {
        "loss": 2.1713,
        "grad_norm": 1.5809122323989868,
        "learning_rate": 6.656110722225561e-05,
        "epoch": 0.4653439835094048,
        "step": 3612
    },
    {
        "loss": 2.0004,
        "grad_norm": 2.1183979511260986,
        "learning_rate": 6.650370001723443e-05,
        "epoch": 0.46547281628446274,
        "step": 3613
    },
    {
        "loss": 2.0039,
        "grad_norm": 2.2431747913360596,
        "learning_rate": 6.6446268385985e-05,
        "epoch": 0.46560164905952073,
        "step": 3614
    },
    {
        "loss": 2.2376,
        "grad_norm": 2.249699354171753,
        "learning_rate": 6.638881241350885e-05,
        "epoch": 0.4657304818345787,
        "step": 3615
    },
    {
        "loss": 1.8357,
        "grad_norm": 2.4835596084594727,
        "learning_rate": 6.633133218484334e-05,
        "epoch": 0.4658593146096367,
        "step": 3616
    },
    {
        "loss": 2.2028,
        "grad_norm": 2.125486135482788,
        "learning_rate": 6.62738277850619e-05,
        "epoch": 0.4659881473846947,
        "step": 3617
    },
    {
        "loss": 1.7036,
        "grad_norm": 2.9475350379943848,
        "learning_rate": 6.62162992992736e-05,
        "epoch": 0.4661169801597526,
        "step": 3618
    },
    {
        "loss": 2.0031,
        "grad_norm": 2.1870198249816895,
        "learning_rate": 6.615874681262324e-05,
        "epoch": 0.4662458129348106,
        "step": 3619
    },
    {
        "loss": 1.809,
        "grad_norm": 2.5344886779785156,
        "learning_rate": 6.610117041029114e-05,
        "epoch": 0.4663746457098686,
        "step": 3620
    },
    {
        "loss": 2.2907,
        "grad_norm": 1.3253010511398315,
        "learning_rate": 6.604357017749305e-05,
        "epoch": 0.46650347848492657,
        "step": 3621
    },
    {
        "loss": 1.5782,
        "grad_norm": 3.076906442642212,
        "learning_rate": 6.598594619947982e-05,
        "epoch": 0.46663231125998456,
        "step": 3622
    },
    {
        "loss": 2.1743,
        "grad_norm": 2.117948055267334,
        "learning_rate": 6.592829856153764e-05,
        "epoch": 0.46676114403504254,
        "step": 3623
    },
    {
        "loss": 2.3011,
        "grad_norm": 2.0765340328216553,
        "learning_rate": 6.587062734898765e-05,
        "epoch": 0.46688997681010047,
        "step": 3624
    },
    {
        "loss": 1.6899,
        "grad_norm": 1.6635736227035522,
        "learning_rate": 6.581293264718583e-05,
        "epoch": 0.46701880958515846,
        "step": 3625
    },
    {
        "loss": 2.4767,
        "grad_norm": 1.4854201078414917,
        "learning_rate": 6.575521454152301e-05,
        "epoch": 0.46714764236021644,
        "step": 3626
    },
    {
        "loss": 2.1359,
        "grad_norm": 2.618429660797119,
        "learning_rate": 6.569747311742454e-05,
        "epoch": 0.4672764751352744,
        "step": 3627
    },
    {
        "loss": 2.3879,
        "grad_norm": 1.8973274230957031,
        "learning_rate": 6.56397084603505e-05,
        "epoch": 0.4674053079103324,
        "step": 3628
    },
    {
        "loss": 2.4432,
        "grad_norm": 2.379237651824951,
        "learning_rate": 6.558192065579511e-05,
        "epoch": 0.46753414068539034,
        "step": 3629
    },
    {
        "loss": 2.0218,
        "grad_norm": 1.7402915954589844,
        "learning_rate": 6.552410978928696e-05,
        "epoch": 0.46766297346044833,
        "step": 3630
    },
    {
        "loss": 2.0257,
        "grad_norm": 1.7912797927856445,
        "learning_rate": 6.546627594638877e-05,
        "epoch": 0.4677918062355063,
        "step": 3631
    },
    {
        "loss": 1.8484,
        "grad_norm": 2.173846483230591,
        "learning_rate": 6.540841921269728e-05,
        "epoch": 0.4679206390105643,
        "step": 3632
    },
    {
        "loss": 1.7048,
        "grad_norm": 2.483342170715332,
        "learning_rate": 6.535053967384312e-05,
        "epoch": 0.4680494717856223,
        "step": 3633
    },
    {
        "loss": 1.8322,
        "grad_norm": 2.7705438137054443,
        "learning_rate": 6.529263741549059e-05,
        "epoch": 0.4681783045606802,
        "step": 3634
    },
    {
        "loss": 2.1848,
        "grad_norm": 1.6807278394699097,
        "learning_rate": 6.523471252333769e-05,
        "epoch": 0.4683071373357382,
        "step": 3635
    },
    {
        "loss": 2.1262,
        "grad_norm": 1.3581196069717407,
        "learning_rate": 6.517676508311593e-05,
        "epoch": 0.4684359701107962,
        "step": 3636
    },
    {
        "loss": 2.37,
        "grad_norm": 2.074326515197754,
        "learning_rate": 6.511879518059013e-05,
        "epoch": 0.46856480288585417,
        "step": 3637
    },
    {
        "loss": 2.271,
        "grad_norm": 1.6102101802825928,
        "learning_rate": 6.506080290155838e-05,
        "epoch": 0.46869363566091216,
        "step": 3638
    },
    {
        "loss": 1.5942,
        "grad_norm": 2.4057493209838867,
        "learning_rate": 6.500278833185196e-05,
        "epoch": 0.4688224684359701,
        "step": 3639
    },
    {
        "loss": 1.9352,
        "grad_norm": 2.6615593433380127,
        "learning_rate": 6.494475155733498e-05,
        "epoch": 0.46895130121102807,
        "step": 3640
    },
    {
        "loss": 1.6715,
        "grad_norm": 2.201209545135498,
        "learning_rate": 6.488669266390457e-05,
        "epoch": 0.46908013398608606,
        "step": 3641
    },
    {
        "loss": 2.0852,
        "grad_norm": 1.6322767734527588,
        "learning_rate": 6.482861173749056e-05,
        "epoch": 0.46920896676114404,
        "step": 3642
    },
    {
        "loss": 2.2345,
        "grad_norm": 2.2545504570007324,
        "learning_rate": 6.477050886405531e-05,
        "epoch": 0.469337799536202,
        "step": 3643
    },
    {
        "loss": 1.9713,
        "grad_norm": 1.908706784248352,
        "learning_rate": 6.471238412959375e-05,
        "epoch": 0.46946663231125996,
        "step": 3644
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.3482013940811157,
        "learning_rate": 6.465423762013315e-05,
        "epoch": 0.46959546508631794,
        "step": 3645
    },
    {
        "loss": 2.4467,
        "grad_norm": 1.5632189512252808,
        "learning_rate": 6.4596069421733e-05,
        "epoch": 0.46972429786137593,
        "step": 3646
    },
    {
        "loss": 2.3994,
        "grad_norm": 1.750768780708313,
        "learning_rate": 6.453787962048486e-05,
        "epoch": 0.4698531306364339,
        "step": 3647
    },
    {
        "loss": 1.9779,
        "grad_norm": 2.2573792934417725,
        "learning_rate": 6.447966830251222e-05,
        "epoch": 0.4699819634114919,
        "step": 3648
    },
    {
        "loss": 1.8107,
        "grad_norm": 2.8244640827178955,
        "learning_rate": 6.442143555397063e-05,
        "epoch": 0.4701107961865499,
        "step": 3649
    },
    {
        "loss": 2.328,
        "grad_norm": 1.5124539136886597,
        "learning_rate": 6.436318146104712e-05,
        "epoch": 0.4702396289616078,
        "step": 3650
    },
    {
        "loss": 1.4815,
        "grad_norm": 2.405870199203491,
        "learning_rate": 6.430490610996043e-05,
        "epoch": 0.4703684617366658,
        "step": 3651
    },
    {
        "loss": 1.3543,
        "grad_norm": 2.7818028926849365,
        "learning_rate": 6.424660958696067e-05,
        "epoch": 0.4704972945117238,
        "step": 3652
    },
    {
        "loss": 2.4309,
        "grad_norm": 1.3869423866271973,
        "learning_rate": 6.418829197832947e-05,
        "epoch": 0.47062612728678177,
        "step": 3653
    },
    {
        "loss": 1.6781,
        "grad_norm": 2.2368736267089844,
        "learning_rate": 6.41299533703795e-05,
        "epoch": 0.47075496006183976,
        "step": 3654
    },
    {
        "loss": 1.6899,
        "grad_norm": 1.5892328023910522,
        "learning_rate": 6.40715938494545e-05,
        "epoch": 0.4708837928368977,
        "step": 3655
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.2824040651321411,
        "learning_rate": 6.401321350192926e-05,
        "epoch": 0.47101262561195567,
        "step": 3656
    },
    {
        "loss": 1.7141,
        "grad_norm": 1.4878593683242798,
        "learning_rate": 6.395481241420937e-05,
        "epoch": 0.47114145838701366,
        "step": 3657
    },
    {
        "loss": 2.1228,
        "grad_norm": 1.861554503440857,
        "learning_rate": 6.389639067273112e-05,
        "epoch": 0.47127029116207164,
        "step": 3658
    },
    {
        "loss": 2.4037,
        "grad_norm": 2.5600531101226807,
        "learning_rate": 6.383794836396129e-05,
        "epoch": 0.4713991239371296,
        "step": 3659
    },
    {
        "loss": 2.3409,
        "grad_norm": 2.1649985313415527,
        "learning_rate": 6.377948557439722e-05,
        "epoch": 0.47152795671218756,
        "step": 3660
    },
    {
        "loss": 1.9363,
        "grad_norm": 1.782582402229309,
        "learning_rate": 6.372100239056646e-05,
        "epoch": 0.47165678948724554,
        "step": 3661
    },
    {
        "loss": 2.2886,
        "grad_norm": 1.973374605178833,
        "learning_rate": 6.366249889902679e-05,
        "epoch": 0.47178562226230353,
        "step": 3662
    },
    {
        "loss": 2.6172,
        "grad_norm": 1.879711389541626,
        "learning_rate": 6.360397518636607e-05,
        "epoch": 0.4719144550373615,
        "step": 3663
    },
    {
        "loss": 2.1957,
        "grad_norm": 2.4853501319885254,
        "learning_rate": 6.354543133920207e-05,
        "epoch": 0.4720432878124195,
        "step": 3664
    },
    {
        "loss": 2.0626,
        "grad_norm": 3.1929867267608643,
        "learning_rate": 6.34868674441823e-05,
        "epoch": 0.47217212058747743,
        "step": 3665
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.9104423522949219,
        "learning_rate": 6.342828358798403e-05,
        "epoch": 0.4723009533625354,
        "step": 3666
    },
    {
        "loss": 1.7983,
        "grad_norm": 2.0972955226898193,
        "learning_rate": 6.336967985731406e-05,
        "epoch": 0.4724297861375934,
        "step": 3667
    },
    {
        "loss": 1.4971,
        "grad_norm": 2.801105260848999,
        "learning_rate": 6.331105633890852e-05,
        "epoch": 0.4725586189126514,
        "step": 3668
    },
    {
        "loss": 2.1034,
        "grad_norm": 2.446882486343384,
        "learning_rate": 6.325241311953296e-05,
        "epoch": 0.47268745168770937,
        "step": 3669
    },
    {
        "loss": 1.8502,
        "grad_norm": 1.4830788373947144,
        "learning_rate": 6.31937502859819e-05,
        "epoch": 0.47281628446276736,
        "step": 3670
    },
    {
        "loss": 2.2292,
        "grad_norm": 2.3492207527160645,
        "learning_rate": 6.31350679250792e-05,
        "epoch": 0.4729451172378253,
        "step": 3671
    },
    {
        "loss": 2.2475,
        "grad_norm": 2.2869865894317627,
        "learning_rate": 6.307636612367728e-05,
        "epoch": 0.47307395001288327,
        "step": 3672
    },
    {
        "loss": 2.2187,
        "grad_norm": 2.04712176322937,
        "learning_rate": 6.301764496865748e-05,
        "epoch": 0.47320278278794126,
        "step": 3673
    },
    {
        "loss": 1.6334,
        "grad_norm": 2.695706367492676,
        "learning_rate": 6.295890454692985e-05,
        "epoch": 0.47333161556299924,
        "step": 3674
    },
    {
        "loss": 2.0916,
        "grad_norm": 2.566040277481079,
        "learning_rate": 6.290014494543285e-05,
        "epoch": 0.4734604483380572,
        "step": 3675
    },
    {
        "loss": 1.6035,
        "grad_norm": 3.2407286167144775,
        "learning_rate": 6.284136625113342e-05,
        "epoch": 0.47358928111311516,
        "step": 3676
    },
    {
        "loss": 2.44,
        "grad_norm": 2.3024373054504395,
        "learning_rate": 6.278256855102663e-05,
        "epoch": 0.47371811388817314,
        "step": 3677
    },
    {
        "loss": 1.8626,
        "grad_norm": 2.2114720344543457,
        "learning_rate": 6.27237519321358e-05,
        "epoch": 0.47384694666323113,
        "step": 3678
    },
    {
        "loss": 2.3642,
        "grad_norm": 1.3425170183181763,
        "learning_rate": 6.266491648151226e-05,
        "epoch": 0.4739757794382891,
        "step": 3679
    },
    {
        "loss": 2.3459,
        "grad_norm": 1.0179862976074219,
        "learning_rate": 6.260606228623505e-05,
        "epoch": 0.4741046122133471,
        "step": 3680
    },
    {
        "loss": 2.7008,
        "grad_norm": 1.8054276704788208,
        "learning_rate": 6.25471894334111e-05,
        "epoch": 0.47423344498840503,
        "step": 3681
    },
    {
        "loss": 2.0546,
        "grad_norm": 2.201533079147339,
        "learning_rate": 6.248829801017496e-05,
        "epoch": 0.474362277763463,
        "step": 3682
    },
    {
        "loss": 2.3546,
        "grad_norm": 1.2931853532791138,
        "learning_rate": 6.242938810368859e-05,
        "epoch": 0.474491110538521,
        "step": 3683
    },
    {
        "loss": 2.5209,
        "grad_norm": 1.9720523357391357,
        "learning_rate": 6.237045980114131e-05,
        "epoch": 0.474619943313579,
        "step": 3684
    },
    {
        "loss": 2.3619,
        "grad_norm": 1.7161365747451782,
        "learning_rate": 6.231151318974976e-05,
        "epoch": 0.47474877608863697,
        "step": 3685
    },
    {
        "loss": 2.3049,
        "grad_norm": 1.494504690170288,
        "learning_rate": 6.225254835675753e-05,
        "epoch": 0.4748776088636949,
        "step": 3686
    },
    {
        "loss": 2.2595,
        "grad_norm": 2.393160581588745,
        "learning_rate": 6.219356538943532e-05,
        "epoch": 0.4750064416387529,
        "step": 3687
    },
    {
        "loss": 1.3071,
        "grad_norm": 2.9504683017730713,
        "learning_rate": 6.213456437508058e-05,
        "epoch": 0.47513527441381087,
        "step": 3688
    },
    {
        "loss": 2.2255,
        "grad_norm": 1.6221030950546265,
        "learning_rate": 6.207554540101753e-05,
        "epoch": 0.47526410718886886,
        "step": 3689
    },
    {
        "loss": 1.9038,
        "grad_norm": 2.9630329608917236,
        "learning_rate": 6.201650855459694e-05,
        "epoch": 0.47539293996392684,
        "step": 3690
    },
    {
        "loss": 1.7651,
        "grad_norm": 2.2039825916290283,
        "learning_rate": 6.195745392319594e-05,
        "epoch": 0.47552177273898477,
        "step": 3691
    },
    {
        "loss": 1.6972,
        "grad_norm": 2.2708017826080322,
        "learning_rate": 6.189838159421822e-05,
        "epoch": 0.47565060551404276,
        "step": 3692
    },
    {
        "loss": 2.2383,
        "grad_norm": 2.0322797298431396,
        "learning_rate": 6.183929165509342e-05,
        "epoch": 0.47577943828910074,
        "step": 3693
    },
    {
        "loss": 1.5443,
        "grad_norm": 2.8827266693115234,
        "learning_rate": 6.17801841932774e-05,
        "epoch": 0.4759082710641587,
        "step": 3694
    },
    {
        "loss": 1.7873,
        "grad_norm": 1.9846994876861572,
        "learning_rate": 6.172105929625182e-05,
        "epoch": 0.4760371038392167,
        "step": 3695
    },
    {
        "loss": 2.246,
        "grad_norm": 2.084421157836914,
        "learning_rate": 6.166191705152434e-05,
        "epoch": 0.4761659366142747,
        "step": 3696
    },
    {
        "loss": 1.9153,
        "grad_norm": 3.4031729698181152,
        "learning_rate": 6.16027575466281e-05,
        "epoch": 0.47629476938933263,
        "step": 3697
    },
    {
        "loss": 2.0253,
        "grad_norm": 2.683936357498169,
        "learning_rate": 6.154358086912184e-05,
        "epoch": 0.4764236021643906,
        "step": 3698
    },
    {
        "loss": 2.0381,
        "grad_norm": 2.0201210975646973,
        "learning_rate": 6.148438710658978e-05,
        "epoch": 0.4765524349394486,
        "step": 3699
    },
    {
        "loss": 2.1768,
        "grad_norm": 1.6246874332427979,
        "learning_rate": 6.142517634664138e-05,
        "epoch": 0.4766812677145066,
        "step": 3700
    },
    {
        "loss": 2.4544,
        "grad_norm": 1.7171884775161743,
        "learning_rate": 6.13659486769113e-05,
        "epoch": 0.47681010048956457,
        "step": 3701
    },
    {
        "loss": 2.0375,
        "grad_norm": 2.218806505203247,
        "learning_rate": 6.130670418505911e-05,
        "epoch": 0.4769389332646225,
        "step": 3702
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.8559036254882812,
        "learning_rate": 6.124744295876947e-05,
        "epoch": 0.4770677660396805,
        "step": 3703
    },
    {
        "loss": 1.7847,
        "grad_norm": 2.716517686843872,
        "learning_rate": 6.118816508575157e-05,
        "epoch": 0.47719659881473847,
        "step": 3704
    },
    {
        "loss": 2.1346,
        "grad_norm": 1.85365629196167,
        "learning_rate": 6.112887065373943e-05,
        "epoch": 0.47732543158979646,
        "step": 3705
    },
    {
        "loss": 2.1493,
        "grad_norm": 1.962293028831482,
        "learning_rate": 6.106955975049149e-05,
        "epoch": 0.47745426436485444,
        "step": 3706
    },
    {
        "loss": 2.4289,
        "grad_norm": 1.554352879524231,
        "learning_rate": 6.101023246379065e-05,
        "epoch": 0.47758309713991237,
        "step": 3707
    },
    {
        "loss": 2.1827,
        "grad_norm": 2.735252618789673,
        "learning_rate": 6.0950888881443925e-05,
        "epoch": 0.47771192991497036,
        "step": 3708
    },
    {
        "loss": 2.4094,
        "grad_norm": 1.68500816822052,
        "learning_rate": 6.089152909128254e-05,
        "epoch": 0.47784076269002834,
        "step": 3709
    },
    {
        "loss": 2.4089,
        "grad_norm": 1.8408513069152832,
        "learning_rate": 6.0832153181161735e-05,
        "epoch": 0.4779695954650863,
        "step": 3710
    },
    {
        "loss": 1.6035,
        "grad_norm": 2.5985753536224365,
        "learning_rate": 6.07727612389605e-05,
        "epoch": 0.4780984282401443,
        "step": 3711
    },
    {
        "loss": 2.0852,
        "grad_norm": 2.0216052532196045,
        "learning_rate": 6.071335335258165e-05,
        "epoch": 0.47822726101520224,
        "step": 3712
    },
    {
        "loss": 2.2444,
        "grad_norm": 1.9285345077514648,
        "learning_rate": 6.065392960995154e-05,
        "epoch": 0.47835609379026023,
        "step": 3713
    },
    {
        "loss": 2.256,
        "grad_norm": 2.0420756340026855,
        "learning_rate": 6.059449009902007e-05,
        "epoch": 0.4784849265653182,
        "step": 3714
    },
    {
        "loss": 2.2593,
        "grad_norm": 2.515939712524414,
        "learning_rate": 6.05350349077604e-05,
        "epoch": 0.4786137593403762,
        "step": 3715
    },
    {
        "loss": 1.8024,
        "grad_norm": 1.5920644998550415,
        "learning_rate": 6.047556412416888e-05,
        "epoch": 0.4787425921154342,
        "step": 3716
    },
    {
        "loss": 1.682,
        "grad_norm": 2.1457762718200684,
        "learning_rate": 6.041607783626502e-05,
        "epoch": 0.4788714248904921,
        "step": 3717
    },
    {
        "loss": 2.3165,
        "grad_norm": 1.7260783910751343,
        "learning_rate": 6.035657613209122e-05,
        "epoch": 0.4790002576655501,
        "step": 3718
    },
    {
        "loss": 1.9727,
        "grad_norm": 1.762300729751587,
        "learning_rate": 6.029705909971276e-05,
        "epoch": 0.4791290904406081,
        "step": 3719
    },
    {
        "loss": 1.9414,
        "grad_norm": 2.6667733192443848,
        "learning_rate": 6.0237526827217504e-05,
        "epoch": 0.47925792321566607,
        "step": 3720
    },
    {
        "loss": 2.0379,
        "grad_norm": 2.3993492126464844,
        "learning_rate": 6.017797940271595e-05,
        "epoch": 0.47938675599072406,
        "step": 3721
    },
    {
        "loss": 1.9843,
        "grad_norm": 2.408604383468628,
        "learning_rate": 6.0118416914341025e-05,
        "epoch": 0.47951558876578204,
        "step": 3722
    },
    {
        "loss": 2.1257,
        "grad_norm": 2.2301948070526123,
        "learning_rate": 6.005883945024786e-05,
        "epoch": 0.47964442154083997,
        "step": 3723
    },
    {
        "loss": 1.8485,
        "grad_norm": 3.080983877182007,
        "learning_rate": 5.9999247098613875e-05,
        "epoch": 0.47977325431589796,
        "step": 3724
    },
    {
        "loss": 2.3911,
        "grad_norm": 1.5256301164627075,
        "learning_rate": 5.993963994763845e-05,
        "epoch": 0.47990208709095594,
        "step": 3725
    },
    {
        "loss": 1.9695,
        "grad_norm": 1.3641544580459595,
        "learning_rate": 5.988001808554291e-05,
        "epoch": 0.4800309198660139,
        "step": 3726
    },
    {
        "loss": 2.3626,
        "grad_norm": 1.3467369079589844,
        "learning_rate": 5.9820381600570275e-05,
        "epoch": 0.4801597526410719,
        "step": 3727
    },
    {
        "loss": 2.4393,
        "grad_norm": 1.7290349006652832,
        "learning_rate": 5.9760730580985334e-05,
        "epoch": 0.48028858541612984,
        "step": 3728
    },
    {
        "loss": 2.2552,
        "grad_norm": 1.5877432823181152,
        "learning_rate": 5.970106511507423e-05,
        "epoch": 0.48041741819118783,
        "step": 3729
    },
    {
        "loss": 2.1704,
        "grad_norm": 1.4232397079467773,
        "learning_rate": 5.964138529114461e-05,
        "epoch": 0.4805462509662458,
        "step": 3730
    },
    {
        "loss": 2.1465,
        "grad_norm": 2.2900471687316895,
        "learning_rate": 5.958169119752533e-05,
        "epoch": 0.4806750837413038,
        "step": 3731
    },
    {
        "loss": 2.1408,
        "grad_norm": 1.9398490190505981,
        "learning_rate": 5.952198292256642e-05,
        "epoch": 0.4808039165163618,
        "step": 3732
    },
    {
        "loss": 2.0269,
        "grad_norm": 1.805474042892456,
        "learning_rate": 5.9462260554638806e-05,
        "epoch": 0.4809327492914197,
        "step": 3733
    },
    {
        "loss": 1.5246,
        "grad_norm": 2.7864229679107666,
        "learning_rate": 5.9402524182134233e-05,
        "epoch": 0.4810615820664777,
        "step": 3734
    },
    {
        "loss": 1.8631,
        "grad_norm": 2.338181972503662,
        "learning_rate": 5.9342773893465406e-05,
        "epoch": 0.4811904148415357,
        "step": 3735
    },
    {
        "loss": 2.2899,
        "grad_norm": 1.833617091178894,
        "learning_rate": 5.928300977706538e-05,
        "epoch": 0.48131924761659367,
        "step": 3736
    },
    {
        "loss": 1.8985,
        "grad_norm": 2.2713723182678223,
        "learning_rate": 5.922323192138782e-05,
        "epoch": 0.48144808039165166,
        "step": 3737
    },
    {
        "loss": 1.8541,
        "grad_norm": 1.8805932998657227,
        "learning_rate": 5.916344041490659e-05,
        "epoch": 0.4815769131667096,
        "step": 3738
    },
    {
        "loss": 1.9809,
        "grad_norm": 2.7272441387176514,
        "learning_rate": 5.910363534611596e-05,
        "epoch": 0.48170574594176757,
        "step": 3739
    },
    {
        "loss": 2.4363,
        "grad_norm": 1.8276135921478271,
        "learning_rate": 5.90438168035301e-05,
        "epoch": 0.48183457871682556,
        "step": 3740
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.120021104812622,
        "learning_rate": 5.898398487568317e-05,
        "epoch": 0.48196341149188354,
        "step": 3741
    },
    {
        "loss": 2.0689,
        "grad_norm": 2.3566269874572754,
        "learning_rate": 5.892413965112915e-05,
        "epoch": 0.4820922442669415,
        "step": 3742
    },
    {
        "loss": 2.0829,
        "grad_norm": 1.483012318611145,
        "learning_rate": 5.886428121844172e-05,
        "epoch": 0.48222107704199946,
        "step": 3743
    },
    {
        "loss": 1.7353,
        "grad_norm": 3.020347833633423,
        "learning_rate": 5.8804409666214114e-05,
        "epoch": 0.48234990981705744,
        "step": 3744
    },
    {
        "loss": 2.2358,
        "grad_norm": 2.7783446311950684,
        "learning_rate": 5.87445250830589e-05,
        "epoch": 0.48247874259211543,
        "step": 3745
    },
    {
        "loss": 2.1094,
        "grad_norm": 1.5771148204803467,
        "learning_rate": 5.868462755760803e-05,
        "epoch": 0.4826075753671734,
        "step": 3746
    },
    {
        "loss": 1.9594,
        "grad_norm": 2.5010406970977783,
        "learning_rate": 5.8624717178512534e-05,
        "epoch": 0.4827364081422314,
        "step": 3747
    },
    {
        "loss": 1.7686,
        "grad_norm": 2.6693947315216064,
        "learning_rate": 5.856479403444253e-05,
        "epoch": 0.4828652409172894,
        "step": 3748
    },
    {
        "loss": 1.4181,
        "grad_norm": 3.3009400367736816,
        "learning_rate": 5.8504858214087e-05,
        "epoch": 0.4829940736923473,
        "step": 3749
    },
    {
        "loss": 1.516,
        "grad_norm": 1.9548914432525635,
        "learning_rate": 5.844490980615371e-05,
        "epoch": 0.4831229064674053,
        "step": 3750
    },
    {
        "loss": 1.9009,
        "grad_norm": 2.702047109603882,
        "learning_rate": 5.8384948899368994e-05,
        "epoch": 0.4832517392424633,
        "step": 3751
    },
    {
        "loss": 2.0634,
        "grad_norm": 2.1648108959198,
        "learning_rate": 5.832497558247775e-05,
        "epoch": 0.48338057201752127,
        "step": 3752
    },
    {
        "loss": 1.3965,
        "grad_norm": 2.6732451915740967,
        "learning_rate": 5.8264989944243255e-05,
        "epoch": 0.48350940479257926,
        "step": 3753
    },
    {
        "loss": 2.3119,
        "grad_norm": 1.7849321365356445,
        "learning_rate": 5.8204992073446905e-05,
        "epoch": 0.4836382375676372,
        "step": 3754
    },
    {
        "loss": 1.1798,
        "grad_norm": 2.5040817260742188,
        "learning_rate": 5.814498205888834e-05,
        "epoch": 0.48376707034269517,
        "step": 3755
    },
    {
        "loss": 2.111,
        "grad_norm": 1.5933524370193481,
        "learning_rate": 5.80849599893851e-05,
        "epoch": 0.48389590311775316,
        "step": 3756
    },
    {
        "loss": 2.0842,
        "grad_norm": 4.268185615539551,
        "learning_rate": 5.8024925953772627e-05,
        "epoch": 0.48402473589281114,
        "step": 3757
    },
    {
        "loss": 1.8792,
        "grad_norm": 3.030289888381958,
        "learning_rate": 5.7964880040903976e-05,
        "epoch": 0.4841535686678691,
        "step": 3758
    },
    {
        "loss": 1.7979,
        "grad_norm": 4.304355144500732,
        "learning_rate": 5.79048223396498e-05,
        "epoch": 0.48428240144292706,
        "step": 3759
    },
    {
        "loss": 1.9338,
        "grad_norm": 3.0788798332214355,
        "learning_rate": 5.784475293889835e-05,
        "epoch": 0.48441123421798504,
        "step": 3760
    },
    {
        "loss": 1.9057,
        "grad_norm": 1.5628918409347534,
        "learning_rate": 5.778467192755498e-05,
        "epoch": 0.48454006699304303,
        "step": 3761
    },
    {
        "loss": 2.265,
        "grad_norm": 1.732493281364441,
        "learning_rate": 5.77245793945424e-05,
        "epoch": 0.484668899768101,
        "step": 3762
    },
    {
        "loss": 2.7592,
        "grad_norm": 1.5591105222702026,
        "learning_rate": 5.766447542880021e-05,
        "epoch": 0.484797732543159,
        "step": 3763
    },
    {
        "loss": 2.0518,
        "grad_norm": 2.6493823528289795,
        "learning_rate": 5.760436011928506e-05,
        "epoch": 0.48492656531821693,
        "step": 3764
    },
    {
        "loss": 2.128,
        "grad_norm": 2.353400468826294,
        "learning_rate": 5.75442335549704e-05,
        "epoch": 0.4850553980932749,
        "step": 3765
    },
    {
        "loss": 1.8569,
        "grad_norm": 2.9069643020629883,
        "learning_rate": 5.748409582484619e-05,
        "epoch": 0.4851842308683329,
        "step": 3766
    },
    {
        "loss": 2.2803,
        "grad_norm": 2.58111572265625,
        "learning_rate": 5.7423947017919056e-05,
        "epoch": 0.4853130636433909,
        "step": 3767
    },
    {
        "loss": 1.7788,
        "grad_norm": 2.1330668926239014,
        "learning_rate": 5.7363787223211985e-05,
        "epoch": 0.48544189641844887,
        "step": 3768
    },
    {
        "loss": 2.3466,
        "grad_norm": 2.5026371479034424,
        "learning_rate": 5.730361652976422e-05,
        "epoch": 0.4855707291935068,
        "step": 3769
    },
    {
        "loss": 2.0583,
        "grad_norm": 1.709687352180481,
        "learning_rate": 5.724343502663112e-05,
        "epoch": 0.4856995619685648,
        "step": 3770
    },
    {
        "loss": 2.1327,
        "grad_norm": 2.41196870803833,
        "learning_rate": 5.718324280288405e-05,
        "epoch": 0.48582839474362277,
        "step": 3771
    },
    {
        "loss": 1.1612,
        "grad_norm": 2.8527638912200928,
        "learning_rate": 5.7123039947610224e-05,
        "epoch": 0.48595722751868076,
        "step": 3772
    },
    {
        "loss": 1.8286,
        "grad_norm": 2.6215884685516357,
        "learning_rate": 5.706282654991263e-05,
        "epoch": 0.48608606029373874,
        "step": 3773
    },
    {
        "loss": 1.1671,
        "grad_norm": 2.9005751609802246,
        "learning_rate": 5.700260269890984e-05,
        "epoch": 0.4862148930687967,
        "step": 3774
    },
    {
        "loss": 1.9657,
        "grad_norm": 2.1141319274902344,
        "learning_rate": 5.694236848373593e-05,
        "epoch": 0.48634372584385466,
        "step": 3775
    },
    {
        "loss": 2.2025,
        "grad_norm": 1.874868392944336,
        "learning_rate": 5.688212399354025e-05,
        "epoch": 0.48647255861891264,
        "step": 3776
    },
    {
        "loss": 2.2821,
        "grad_norm": 2.157042980194092,
        "learning_rate": 5.682186931748733e-05,
        "epoch": 0.48660139139397063,
        "step": 3777
    },
    {
        "loss": 1.7327,
        "grad_norm": 3.162428379058838,
        "learning_rate": 5.676160454475699e-05,
        "epoch": 0.4867302241690286,
        "step": 3778
    },
    {
        "loss": 2.084,
        "grad_norm": 2.001598358154297,
        "learning_rate": 5.67013297645437e-05,
        "epoch": 0.4868590569440866,
        "step": 3779
    },
    {
        "loss": 2.3049,
        "grad_norm": 1.9944186210632324,
        "learning_rate": 5.664104506605696e-05,
        "epoch": 0.48698788971914453,
        "step": 3780
    },
    {
        "loss": 2.0772,
        "grad_norm": 1.4213300943374634,
        "learning_rate": 5.658075053852079e-05,
        "epoch": 0.4871167224942025,
        "step": 3781
    },
    {
        "loss": 1.8961,
        "grad_norm": 2.2208638191223145,
        "learning_rate": 5.652044627117395e-05,
        "epoch": 0.4872455552692605,
        "step": 3782
    },
    {
        "loss": 1.6615,
        "grad_norm": 2.683833122253418,
        "learning_rate": 5.646013235326945e-05,
        "epoch": 0.4873743880443185,
        "step": 3783
    },
    {
        "loss": 1.7719,
        "grad_norm": 1.876272201538086,
        "learning_rate": 5.63998088740746e-05,
        "epoch": 0.48750322081937647,
        "step": 3784
    },
    {
        "loss": 2.0862,
        "grad_norm": 2.001725912094116,
        "learning_rate": 5.633947592287094e-05,
        "epoch": 0.4876320535944344,
        "step": 3785
    },
    {
        "loss": 2.0231,
        "grad_norm": 1.9415128231048584,
        "learning_rate": 5.627913358895398e-05,
        "epoch": 0.4877608863694924,
        "step": 3786
    },
    {
        "loss": 2.2355,
        "grad_norm": 2.7927236557006836,
        "learning_rate": 5.621878196163317e-05,
        "epoch": 0.48788971914455037,
        "step": 3787
    },
    {
        "loss": 2.2832,
        "grad_norm": 2.0829155445098877,
        "learning_rate": 5.6158421130231585e-05,
        "epoch": 0.48801855191960836,
        "step": 3788
    },
    {
        "loss": 1.5066,
        "grad_norm": 1.906742811203003,
        "learning_rate": 5.609805118408604e-05,
        "epoch": 0.48814738469466634,
        "step": 3789
    },
    {
        "loss": 2.1543,
        "grad_norm": 2.3692564964294434,
        "learning_rate": 5.6037672212546856e-05,
        "epoch": 0.48827621746972427,
        "step": 3790
    },
    {
        "loss": 2.0952,
        "grad_norm": 2.3384556770324707,
        "learning_rate": 5.5977284304977573e-05,
        "epoch": 0.48840505024478226,
        "step": 3791
    },
    {
        "loss": 2.507,
        "grad_norm": 1.2695177793502808,
        "learning_rate": 5.5916887550755104e-05,
        "epoch": 0.48853388301984024,
        "step": 3792
    },
    {
        "loss": 2.0141,
        "grad_norm": 1.882641077041626,
        "learning_rate": 5.585648203926941e-05,
        "epoch": 0.48866271579489823,
        "step": 3793
    },
    {
        "loss": 2.0962,
        "grad_norm": 2.464833974838257,
        "learning_rate": 5.579606785992335e-05,
        "epoch": 0.4887915485699562,
        "step": 3794
    },
    {
        "loss": 1.7715,
        "grad_norm": 2.3041725158691406,
        "learning_rate": 5.573564510213272e-05,
        "epoch": 0.4889203813450142,
        "step": 3795
    },
    {
        "loss": 1.9065,
        "grad_norm": 1.4715417623519897,
        "learning_rate": 5.567521385532592e-05,
        "epoch": 0.48904921412007213,
        "step": 3796
    },
    {
        "loss": 1.8977,
        "grad_norm": 2.9370601177215576,
        "learning_rate": 5.561477420894394e-05,
        "epoch": 0.4891780468951301,
        "step": 3797
    },
    {
        "loss": 2.0015,
        "grad_norm": 2.190695285797119,
        "learning_rate": 5.555432625244023e-05,
        "epoch": 0.4893068796701881,
        "step": 3798
    },
    {
        "loss": 1.7074,
        "grad_norm": 2.649393081665039,
        "learning_rate": 5.5493870075280506e-05,
        "epoch": 0.4894357124452461,
        "step": 3799
    },
    {
        "loss": 1.8065,
        "grad_norm": 2.1349339485168457,
        "learning_rate": 5.543340576694271e-05,
        "epoch": 0.48956454522030407,
        "step": 3800
    },
    {
        "loss": 1.8874,
        "grad_norm": 2.5228567123413086,
        "learning_rate": 5.537293341691676e-05,
        "epoch": 0.489693377995362,
        "step": 3801
    },
    {
        "loss": 2.2368,
        "grad_norm": 1.5706586837768555,
        "learning_rate": 5.5312453114704396e-05,
        "epoch": 0.48982221077042,
        "step": 3802
    },
    {
        "loss": 1.2151,
        "grad_norm": 2.6231184005737305,
        "learning_rate": 5.5251964949819366e-05,
        "epoch": 0.48995104354547797,
        "step": 3803
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.6902406215667725,
        "learning_rate": 5.519146901178681e-05,
        "epoch": 0.49007987632053596,
        "step": 3804
    },
    {
        "loss": 2.209,
        "grad_norm": 1.5122978687286377,
        "learning_rate": 5.513096539014354e-05,
        "epoch": 0.49020870909559394,
        "step": 3805
    },
    {
        "loss": 2.0145,
        "grad_norm": 1.5723201036453247,
        "learning_rate": 5.50704541744376e-05,
        "epoch": 0.49033754187065187,
        "step": 3806
    },
    {
        "loss": 1.3168,
        "grad_norm": 2.424614906311035,
        "learning_rate": 5.50099354542284e-05,
        "epoch": 0.49046637464570986,
        "step": 3807
    },
    {
        "loss": 2.3093,
        "grad_norm": 2.628154754638672,
        "learning_rate": 5.494940931908641e-05,
        "epoch": 0.49059520742076784,
        "step": 3808
    },
    {
        "loss": 1.6596,
        "grad_norm": 3.6824398040771484,
        "learning_rate": 5.488887585859302e-05,
        "epoch": 0.49072404019582583,
        "step": 3809
    },
    {
        "loss": 2.2728,
        "grad_norm": 1.1049078702926636,
        "learning_rate": 5.482833516234054e-05,
        "epoch": 0.4908528729708838,
        "step": 3810
    },
    {
        "loss": 2.1348,
        "grad_norm": 2.1095693111419678,
        "learning_rate": 5.476778731993196e-05,
        "epoch": 0.49098170574594174,
        "step": 3811
    },
    {
        "loss": 1.8727,
        "grad_norm": 2.147043466567993,
        "learning_rate": 5.47072324209809e-05,
        "epoch": 0.49111053852099973,
        "step": 3812
    },
    {
        "loss": 2.0769,
        "grad_norm": 2.2068700790405273,
        "learning_rate": 5.4646670555111266e-05,
        "epoch": 0.4912393712960577,
        "step": 3813
    },
    {
        "loss": 2.1851,
        "grad_norm": 3.0106213092803955,
        "learning_rate": 5.458610181195746e-05,
        "epoch": 0.4913682040711157,
        "step": 3814
    },
    {
        "loss": 2.3756,
        "grad_norm": 1.8856722116470337,
        "learning_rate": 5.4525526281163916e-05,
        "epoch": 0.4914970368461737,
        "step": 3815
    },
    {
        "loss": 2.6186,
        "grad_norm": 1.4376364946365356,
        "learning_rate": 5.446494405238523e-05,
        "epoch": 0.4916258696212316,
        "step": 3816
    },
    {
        "loss": 2.1303,
        "grad_norm": 2.402820110321045,
        "learning_rate": 5.440435521528584e-05,
        "epoch": 0.4917547023962896,
        "step": 3817
    },
    {
        "loss": 2.2303,
        "grad_norm": 2.2416110038757324,
        "learning_rate": 5.434375985954003e-05,
        "epoch": 0.4918835351713476,
        "step": 3818
    },
    {
        "loss": 2.4839,
        "grad_norm": 2.0042567253112793,
        "learning_rate": 5.428315807483162e-05,
        "epoch": 0.49201236794640557,
        "step": 3819
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.216884136199951,
        "learning_rate": 5.4222549950854066e-05,
        "epoch": 0.49214120072146356,
        "step": 3820
    },
    {
        "loss": 1.8796,
        "grad_norm": 2.4902255535125732,
        "learning_rate": 5.416193557731014e-05,
        "epoch": 0.49227003349652154,
        "step": 3821
    },
    {
        "loss": 1.8402,
        "grad_norm": 3.069356679916382,
        "learning_rate": 5.4101315043911846e-05,
        "epoch": 0.49239886627157947,
        "step": 3822
    },
    {
        "loss": 2.2485,
        "grad_norm": 1.6126691102981567,
        "learning_rate": 5.4040688440380405e-05,
        "epoch": 0.49252769904663746,
        "step": 3823
    },
    {
        "loss": 1.7814,
        "grad_norm": 2.3831467628479004,
        "learning_rate": 5.398005585644583e-05,
        "epoch": 0.49265653182169544,
        "step": 3824
    },
    {
        "loss": 2.2208,
        "grad_norm": 2.246699571609497,
        "learning_rate": 5.391941738184727e-05,
        "epoch": 0.49278536459675343,
        "step": 3825
    },
    {
        "loss": 2.0303,
        "grad_norm": 1.9278602600097656,
        "learning_rate": 5.385877310633234e-05,
        "epoch": 0.4929141973718114,
        "step": 3826
    },
    {
        "loss": 2.2645,
        "grad_norm": 1.6098833084106445,
        "learning_rate": 5.379812311965732e-05,
        "epoch": 0.49304303014686934,
        "step": 3827
    },
    {
        "loss": 1.3368,
        "grad_norm": 1.720844030380249,
        "learning_rate": 5.373746751158697e-05,
        "epoch": 0.49317186292192733,
        "step": 3828
    },
    {
        "loss": 1.6055,
        "grad_norm": 2.658051013946533,
        "learning_rate": 5.367680637189436e-05,
        "epoch": 0.4933006956969853,
        "step": 3829
    },
    {
        "loss": 2.0211,
        "grad_norm": 2.0269293785095215,
        "learning_rate": 5.3616139790360775e-05,
        "epoch": 0.4934295284720433,
        "step": 3830
    },
    {
        "loss": 2.5712,
        "grad_norm": 1.8096531629562378,
        "learning_rate": 5.355546785677547e-05,
        "epoch": 0.4935583612471013,
        "step": 3831
    },
    {
        "loss": 2.2079,
        "grad_norm": 2.0728349685668945,
        "learning_rate": 5.349479066093567e-05,
        "epoch": 0.4936871940221592,
        "step": 3832
    },
    {
        "loss": 1.8,
        "grad_norm": 2.8878700733184814,
        "learning_rate": 5.3434108292646454e-05,
        "epoch": 0.4938160267972172,
        "step": 3833
    },
    {
        "loss": 1.951,
        "grad_norm": 1.9609990119934082,
        "learning_rate": 5.337342084172041e-05,
        "epoch": 0.4939448595722752,
        "step": 3834
    },
    {
        "loss": 1.7786,
        "grad_norm": 2.029851198196411,
        "learning_rate": 5.331272839797777e-05,
        "epoch": 0.49407369234733317,
        "step": 3835
    },
    {
        "loss": 2.1989,
        "grad_norm": 1.7103008031845093,
        "learning_rate": 5.325203105124612e-05,
        "epoch": 0.49420252512239116,
        "step": 3836
    },
    {
        "loss": 2.2416,
        "grad_norm": 2.3410027027130127,
        "learning_rate": 5.3191328891360346e-05,
        "epoch": 0.4943313578974491,
        "step": 3837
    },
    {
        "loss": 1.813,
        "grad_norm": 2.3164103031158447,
        "learning_rate": 5.3130622008162324e-05,
        "epoch": 0.49446019067250707,
        "step": 3838
    },
    {
        "loss": 1.4577,
        "grad_norm": 3.117619037628174,
        "learning_rate": 5.3069910491501065e-05,
        "epoch": 0.49458902344756506,
        "step": 3839
    },
    {
        "loss": 1.8793,
        "grad_norm": 2.25545597076416,
        "learning_rate": 5.300919443123233e-05,
        "epoch": 0.49471785622262304,
        "step": 3840
    },
    {
        "loss": 1.4833,
        "grad_norm": 2.970717191696167,
        "learning_rate": 5.2948473917218696e-05,
        "epoch": 0.49484668899768103,
        "step": 3841
    },
    {
        "loss": 1.9716,
        "grad_norm": 2.1490163803100586,
        "learning_rate": 5.2887749039329284e-05,
        "epoch": 0.49497552177273896,
        "step": 3842
    },
    {
        "loss": 2.4973,
        "grad_norm": 1.5337046384811401,
        "learning_rate": 5.28270198874397e-05,
        "epoch": 0.49510435454779694,
        "step": 3843
    },
    {
        "loss": 1.775,
        "grad_norm": 3.192756175994873,
        "learning_rate": 5.276628655143186e-05,
        "epoch": 0.49523318732285493,
        "step": 3844
    },
    {
        "loss": 1.9358,
        "grad_norm": 2.067643642425537,
        "learning_rate": 5.2705549121193765e-05,
        "epoch": 0.4953620200979129,
        "step": 3845
    },
    {
        "loss": 2.0908,
        "grad_norm": 2.7587032318115234,
        "learning_rate": 5.264480768661973e-05,
        "epoch": 0.4954908528729709,
        "step": 3846
    },
    {
        "loss": 2.4539,
        "grad_norm": 1.695594072341919,
        "learning_rate": 5.258406233760973e-05,
        "epoch": 0.4956196856480289,
        "step": 3847
    },
    {
        "loss": 1.4189,
        "grad_norm": 2.9877004623413086,
        "learning_rate": 5.252331316406974e-05,
        "epoch": 0.4957485184230868,
        "step": 3848
    },
    {
        "loss": 1.8728,
        "grad_norm": 1.4778424501419067,
        "learning_rate": 5.24625602559112e-05,
        "epoch": 0.4958773511981448,
        "step": 3849
    },
    {
        "loss": 1.6144,
        "grad_norm": 2.47826886177063,
        "learning_rate": 5.24018037030513e-05,
        "epoch": 0.4960061839732028,
        "step": 3850
    },
    {
        "loss": 1.5691,
        "grad_norm": 2.615769386291504,
        "learning_rate": 5.234104359541246e-05,
        "epoch": 0.49613501674826077,
        "step": 3851
    },
    {
        "loss": 1.8254,
        "grad_norm": 1.6278222799301147,
        "learning_rate": 5.2280280022922365e-05,
        "epoch": 0.49626384952331876,
        "step": 3852
    },
    {
        "loss": 2.0831,
        "grad_norm": 2.3300347328186035,
        "learning_rate": 5.221951307551393e-05,
        "epoch": 0.4963926822983767,
        "step": 3853
    },
    {
        "loss": 2.4327,
        "grad_norm": 3.1377193927764893,
        "learning_rate": 5.215874284312496e-05,
        "epoch": 0.49652151507343467,
        "step": 3854
    },
    {
        "loss": 1.7717,
        "grad_norm": 1.8755980730056763,
        "learning_rate": 5.209796941569825e-05,
        "epoch": 0.49665034784849266,
        "step": 3855
    },
    {
        "loss": 2.1183,
        "grad_norm": 1.8954269886016846,
        "learning_rate": 5.203719288318113e-05,
        "epoch": 0.49677918062355064,
        "step": 3856
    },
    {
        "loss": 2.3317,
        "grad_norm": 2.093309164047241,
        "learning_rate": 5.197641333552573e-05,
        "epoch": 0.49690801339860863,
        "step": 3857
    },
    {
        "loss": 1.5701,
        "grad_norm": 3.831740140914917,
        "learning_rate": 5.191563086268849e-05,
        "epoch": 0.49703684617366656,
        "step": 3858
    },
    {
        "loss": 2.2495,
        "grad_norm": 1.2889519929885864,
        "learning_rate": 5.185484555463026e-05,
        "epoch": 0.49716567894872454,
        "step": 3859
    },
    {
        "loss": 2.1957,
        "grad_norm": 2.458322286605835,
        "learning_rate": 5.179405750131607e-05,
        "epoch": 0.49729451172378253,
        "step": 3860
    },
    {
        "loss": 2.0371,
        "grad_norm": 2.2279770374298096,
        "learning_rate": 5.1733266792715064e-05,
        "epoch": 0.4974233444988405,
        "step": 3861
    },
    {
        "loss": 2.0332,
        "grad_norm": 2.49416184425354,
        "learning_rate": 5.1672473518800155e-05,
        "epoch": 0.4975521772738985,
        "step": 3862
    },
    {
        "loss": 2.0623,
        "grad_norm": 1.7647180557250977,
        "learning_rate": 5.161167776954822e-05,
        "epoch": 0.49768101004895643,
        "step": 3863
    },
    {
        "loss": 2.4489,
        "grad_norm": 1.1408838033676147,
        "learning_rate": 5.1550879634939764e-05,
        "epoch": 0.4978098428240144,
        "step": 3864
    },
    {
        "loss": 2.0472,
        "grad_norm": 1.9490010738372803,
        "learning_rate": 5.1490079204958717e-05,
        "epoch": 0.4979386755990724,
        "step": 3865
    },
    {
        "loss": 2.0504,
        "grad_norm": 2.3647654056549072,
        "learning_rate": 5.1429276569592543e-05,
        "epoch": 0.4980675083741304,
        "step": 3866
    },
    {
        "loss": 1.9182,
        "grad_norm": 2.258826732635498,
        "learning_rate": 5.136847181883191e-05,
        "epoch": 0.49819634114918837,
        "step": 3867
    },
    {
        "loss": 1.7345,
        "grad_norm": 3.011556386947632,
        "learning_rate": 5.130766504267064e-05,
        "epoch": 0.4983251739242463,
        "step": 3868
    },
    {
        "loss": 2.3066,
        "grad_norm": 1.6823267936706543,
        "learning_rate": 5.1246856331105516e-05,
        "epoch": 0.4984540066993043,
        "step": 3869
    },
    {
        "loss": 1.6816,
        "grad_norm": 2.8913118839263916,
        "learning_rate": 5.118604577413616e-05,
        "epoch": 0.49858283947436227,
        "step": 3870
    },
    {
        "loss": 2.0826,
        "grad_norm": 1.3041818141937256,
        "learning_rate": 5.1125233461765e-05,
        "epoch": 0.49871167224942026,
        "step": 3871
    },
    {
        "loss": 1.496,
        "grad_norm": 2.6656839847564697,
        "learning_rate": 5.106441948399703e-05,
        "epoch": 0.49884050502447824,
        "step": 3872
    },
    {
        "loss": 1.6561,
        "grad_norm": 3.932216167449951,
        "learning_rate": 5.100360393083974e-05,
        "epoch": 0.49896933779953623,
        "step": 3873
    },
    {
        "loss": 2.0967,
        "grad_norm": 1.5549554824829102,
        "learning_rate": 5.0942786892302865e-05,
        "epoch": 0.49909817057459416,
        "step": 3874
    },
    {
        "loss": 1.3705,
        "grad_norm": 2.636826753616333,
        "learning_rate": 5.088196845839842e-05,
        "epoch": 0.49922700334965214,
        "step": 3875
    },
    {
        "loss": 2.382,
        "grad_norm": 1.683668613433838,
        "learning_rate": 5.082114871914049e-05,
        "epoch": 0.49935583612471013,
        "step": 3876
    },
    {
        "loss": 1.8418,
        "grad_norm": 3.804363965988159,
        "learning_rate": 5.076032776454499e-05,
        "epoch": 0.4994846688997681,
        "step": 3877
    },
    {
        "loss": 2.0389,
        "grad_norm": 1.8747795820236206,
        "learning_rate": 5.069950568462977e-05,
        "epoch": 0.4996135016748261,
        "step": 3878
    },
    {
        "loss": 2.0118,
        "grad_norm": 1.9163936376571655,
        "learning_rate": 5.063868256941423e-05,
        "epoch": 0.49974233444988403,
        "step": 3879
    },
    {
        "loss": 2.0225,
        "grad_norm": 2.4624011516571045,
        "learning_rate": 5.057785850891942e-05,
        "epoch": 0.499871167224942,
        "step": 3880
    },
    {
        "loss": 1.8205,
        "grad_norm": 2.9386203289031982,
        "learning_rate": 5.0517033593167654e-05,
        "epoch": 0.5,
        "step": 3881
    },
    {
        "loss": 1.6901,
        "grad_norm": 3.602553606033325,
        "learning_rate": 5.045620791218262e-05,
        "epoch": 0.5001288327750579,
        "step": 3882
    },
    {
        "loss": 1.6046,
        "grad_norm": 4.074693202972412,
        "learning_rate": 5.039538155598906e-05,
        "epoch": 0.500257665550116,
        "step": 3883
    },
    {
        "loss": 1.5171,
        "grad_norm": 3.133073329925537,
        "learning_rate": 5.033455461461278e-05,
        "epoch": 0.5003864983251739,
        "step": 3884
    },
    {
        "loss": 2.1744,
        "grad_norm": 2.710858106613159,
        "learning_rate": 5.0273727178080424e-05,
        "epoch": 0.5005153311002319,
        "step": 3885
    },
    {
        "loss": 1.6229,
        "grad_norm": 2.7788901329040527,
        "learning_rate": 5.021289933641941e-05,
        "epoch": 0.5006441638752899,
        "step": 3886
    },
    {
        "loss": 2.5265,
        "grad_norm": 2.0562760829925537,
        "learning_rate": 5.015207117965766e-05,
        "epoch": 0.5007729966503478,
        "step": 3887
    },
    {
        "loss": 1.3444,
        "grad_norm": 2.8018229007720947,
        "learning_rate": 5.0091242797823566e-05,
        "epoch": 0.5009018294254058,
        "step": 3888
    },
    {
        "loss": 1.9138,
        "grad_norm": 2.245954990386963,
        "learning_rate": 5.003041428094605e-05,
        "epoch": 0.5010306622004638,
        "step": 3889
    },
    {
        "loss": 2.406,
        "grad_norm": 1.2510820627212524,
        "learning_rate": 4.996958571905397e-05,
        "epoch": 0.5011594949755218,
        "step": 3890
    },
    {
        "loss": 2.3644,
        "grad_norm": 1.3429690599441528,
        "learning_rate": 4.990875720217641e-05,
        "epoch": 0.5012883277505797,
        "step": 3891
    },
    {
        "loss": 1.5883,
        "grad_norm": 3.2567169666290283,
        "learning_rate": 4.9847928820342354e-05,
        "epoch": 0.5014171605256377,
        "step": 3892
    },
    {
        "loss": 1.9257,
        "grad_norm": 2.2387800216674805,
        "learning_rate": 4.97871006635806e-05,
        "epoch": 0.5015459933006957,
        "step": 3893
    },
    {
        "loss": 1.6635,
        "grad_norm": 2.0858328342437744,
        "learning_rate": 4.972627282191959e-05,
        "epoch": 0.5016748260757536,
        "step": 3894
    },
    {
        "loss": 2.356,
        "grad_norm": 1.7846890687942505,
        "learning_rate": 4.9665445385387235e-05,
        "epoch": 0.5018036588508117,
        "step": 3895
    },
    {
        "loss": 2.0098,
        "grad_norm": 2.126286268234253,
        "learning_rate": 4.960461844401095e-05,
        "epoch": 0.5019324916258696,
        "step": 3896
    },
    {
        "loss": 1.7539,
        "grad_norm": 2.3120241165161133,
        "learning_rate": 4.954379208781742e-05,
        "epoch": 0.5020613244009275,
        "step": 3897
    },
    {
        "loss": 2.0869,
        "grad_norm": 1.8770067691802979,
        "learning_rate": 4.94829664068324e-05,
        "epoch": 0.5021901571759856,
        "step": 3898
    },
    {
        "loss": 1.2354,
        "grad_norm": 2.1980741024017334,
        "learning_rate": 4.942214149108055e-05,
        "epoch": 0.5023189899510435,
        "step": 3899
    },
    {
        "loss": 1.8221,
        "grad_norm": 2.060271978378296,
        "learning_rate": 4.936131743058575e-05,
        "epoch": 0.5024478227261016,
        "step": 3900
    },
    {
        "loss": 2.3157,
        "grad_norm": 2.513242244720459,
        "learning_rate": 4.9300494315370245e-05,
        "epoch": 0.5025766555011595,
        "step": 3901
    },
    {
        "loss": 2.1596,
        "grad_norm": 2.8828439712524414,
        "learning_rate": 4.923967223545502e-05,
        "epoch": 0.5027054882762174,
        "step": 3902
    },
    {
        "loss": 1.6525,
        "grad_norm": 2.604341506958008,
        "learning_rate": 4.917885128085952e-05,
        "epoch": 0.5028343210512755,
        "step": 3903
    },
    {
        "loss": 1.7354,
        "grad_norm": 2.4715073108673096,
        "learning_rate": 4.911803154160159e-05,
        "epoch": 0.5029631538263334,
        "step": 3904
    },
    {
        "loss": 2.013,
        "grad_norm": 2.3191633224487305,
        "learning_rate": 4.905721310769718e-05,
        "epoch": 0.5030919866013914,
        "step": 3905
    },
    {
        "loss": 2.2341,
        "grad_norm": 2.0678186416625977,
        "learning_rate": 4.89963960691603e-05,
        "epoch": 0.5032208193764494,
        "step": 3906
    },
    {
        "loss": 1.9223,
        "grad_norm": 2.0086967945098877,
        "learning_rate": 4.893558051600295e-05,
        "epoch": 0.5033496521515074,
        "step": 3907
    },
    {
        "loss": 1.4834,
        "grad_norm": 3.0821382999420166,
        "learning_rate": 4.887476653823501e-05,
        "epoch": 0.5034784849265653,
        "step": 3908
    },
    {
        "loss": 1.9253,
        "grad_norm": 2.6570112705230713,
        "learning_rate": 4.881395422586385e-05,
        "epoch": 0.5036073177016233,
        "step": 3909
    },
    {
        "loss": 2.1092,
        "grad_norm": 1.7528702020645142,
        "learning_rate": 4.875314366889449e-05,
        "epoch": 0.5037361504766813,
        "step": 3910
    },
    {
        "loss": 2.253,
        "grad_norm": 2.1849303245544434,
        "learning_rate": 4.8692334957329364e-05,
        "epoch": 0.5038649832517392,
        "step": 3911
    },
    {
        "loss": 1.9029,
        "grad_norm": 3.0569355487823486,
        "learning_rate": 4.86315281811681e-05,
        "epoch": 0.5039938160267973,
        "step": 3912
    },
    {
        "loss": 2.2373,
        "grad_norm": 2.0548789501190186,
        "learning_rate": 4.857072343040746e-05,
        "epoch": 0.5041226488018552,
        "step": 3913
    },
    {
        "loss": 2.2577,
        "grad_norm": 2.542470932006836,
        "learning_rate": 4.850992079504129e-05,
        "epoch": 0.5042514815769131,
        "step": 3914
    },
    {
        "loss": 2.3363,
        "grad_norm": 1.7324894666671753,
        "learning_rate": 4.844912036506029e-05,
        "epoch": 0.5043803143519712,
        "step": 3915
    },
    {
        "loss": 2.0471,
        "grad_norm": 2.507138729095459,
        "learning_rate": 4.838832223045176e-05,
        "epoch": 0.5045091471270291,
        "step": 3916
    },
    {
        "loss": 2.114,
        "grad_norm": 1.9638895988464355,
        "learning_rate": 4.832752648119982e-05,
        "epoch": 0.5046379799020871,
        "step": 3917
    },
    {
        "loss": 2.1824,
        "grad_norm": 2.1649293899536133,
        "learning_rate": 4.826673320728495e-05,
        "epoch": 0.5047668126771451,
        "step": 3918
    },
    {
        "loss": 2.0712,
        "grad_norm": 2.601888656616211,
        "learning_rate": 4.820594249868393e-05,
        "epoch": 0.504895645452203,
        "step": 3919
    },
    {
        "loss": 1.6496,
        "grad_norm": 2.6220364570617676,
        "learning_rate": 4.814515444536974e-05,
        "epoch": 0.505024478227261,
        "step": 3920
    },
    {
        "loss": 2.2496,
        "grad_norm": 1.7523868083953857,
        "learning_rate": 4.808436913731152e-05,
        "epoch": 0.505153311002319,
        "step": 3921
    },
    {
        "loss": 2.4235,
        "grad_norm": 1.2427312135696411,
        "learning_rate": 4.802358666447431e-05,
        "epoch": 0.505282143777377,
        "step": 3922
    },
    {
        "loss": 2.1159,
        "grad_norm": 2.336327314376831,
        "learning_rate": 4.7962807116818915e-05,
        "epoch": 0.5054109765524349,
        "step": 3923
    },
    {
        "loss": 2.026,
        "grad_norm": 2.0230207443237305,
        "learning_rate": 4.790203058430173e-05,
        "epoch": 0.5055398093274929,
        "step": 3924
    },
    {
        "loss": 2.1298,
        "grad_norm": 2.3010709285736084,
        "learning_rate": 4.784125715687501e-05,
        "epoch": 0.5056686421025509,
        "step": 3925
    },
    {
        "loss": 1.1303,
        "grad_norm": 3.0790786743164062,
        "learning_rate": 4.7780486924486076e-05,
        "epoch": 0.5057974748776088,
        "step": 3926
    },
    {
        "loss": 2.1433,
        "grad_norm": 1.8550405502319336,
        "learning_rate": 4.771971997707764e-05,
        "epoch": 0.5059263076526669,
        "step": 3927
    },
    {
        "loss": 1.6922,
        "grad_norm": 1.8486299514770508,
        "learning_rate": 4.765895640458755e-05,
        "epoch": 0.5060551404277248,
        "step": 3928
    },
    {
        "loss": 2.0481,
        "grad_norm": 2.3523621559143066,
        "learning_rate": 4.759819629694871e-05,
        "epoch": 0.5061839732027827,
        "step": 3929
    },
    {
        "loss": 2.2164,
        "grad_norm": 1.7935057878494263,
        "learning_rate": 4.753743974408882e-05,
        "epoch": 0.5063128059778408,
        "step": 3930
    },
    {
        "loss": 1.6084,
        "grad_norm": 1.6847344636917114,
        "learning_rate": 4.7476686835930304e-05,
        "epoch": 0.5064416387528987,
        "step": 3931
    },
    {
        "loss": 2.2638,
        "grad_norm": 1.8452914953231812,
        "learning_rate": 4.7415937662390284e-05,
        "epoch": 0.5065704715279568,
        "step": 3932
    },
    {
        "loss": 1.9954,
        "grad_norm": 1.9190340042114258,
        "learning_rate": 4.735519231338029e-05,
        "epoch": 0.5066993043030147,
        "step": 3933
    },
    {
        "loss": 2.4385,
        "grad_norm": 2.4539594650268555,
        "learning_rate": 4.729445087880622e-05,
        "epoch": 0.5068281370780726,
        "step": 3934
    },
    {
        "loss": 2.1255,
        "grad_norm": 1.7070211172103882,
        "learning_rate": 4.723371344856815e-05,
        "epoch": 0.5069569698531307,
        "step": 3935
    },
    {
        "loss": 1.9889,
        "grad_norm": 2.4777729511260986,
        "learning_rate": 4.717298011256031e-05,
        "epoch": 0.5070858026281886,
        "step": 3936
    },
    {
        "loss": 2.2771,
        "grad_norm": 2.1773805618286133,
        "learning_rate": 4.7112250960670734e-05,
        "epoch": 0.5072146354032466,
        "step": 3937
    },
    {
        "loss": 1.9555,
        "grad_norm": 1.955891489982605,
        "learning_rate": 4.7051526082781315e-05,
        "epoch": 0.5073434681783046,
        "step": 3938
    },
    {
        "loss": 1.9844,
        "grad_norm": 1.9455938339233398,
        "learning_rate": 4.699080556876767e-05,
        "epoch": 0.5074723009533625,
        "step": 3939
    },
    {
        "loss": 2.1099,
        "grad_norm": 2.242260456085205,
        "learning_rate": 4.693008950849898e-05,
        "epoch": 0.5076011337284205,
        "step": 3940
    },
    {
        "loss": 2.3694,
        "grad_norm": 1.4157739877700806,
        "learning_rate": 4.686937799183766e-05,
        "epoch": 0.5077299665034785,
        "step": 3941
    },
    {
        "loss": 2.1882,
        "grad_norm": 2.3785369396209717,
        "learning_rate": 4.680867110863963e-05,
        "epoch": 0.5078587992785365,
        "step": 3942
    },
    {
        "loss": 2.4647,
        "grad_norm": 2.2542426586151123,
        "learning_rate": 4.674796894875384e-05,
        "epoch": 0.5079876320535944,
        "step": 3943
    },
    {
        "loss": 0.6797,
        "grad_norm": 2.423884868621826,
        "learning_rate": 4.668727160202224e-05,
        "epoch": 0.5081164648286524,
        "step": 3944
    },
    {
        "loss": 2.101,
        "grad_norm": 1.988094687461853,
        "learning_rate": 4.66265791582796e-05,
        "epoch": 0.5082452976037104,
        "step": 3945
    },
    {
        "loss": 1.5953,
        "grad_norm": 2.6168925762176514,
        "learning_rate": 4.656589170735356e-05,
        "epoch": 0.5083741303787683,
        "step": 3946
    },
    {
        "loss": 1.9296,
        "grad_norm": 2.4013946056365967,
        "learning_rate": 4.650520933906434e-05,
        "epoch": 0.5085029631538264,
        "step": 3947
    },
    {
        "loss": 1.6982,
        "grad_norm": 2.7619469165802,
        "learning_rate": 4.6444532143224584e-05,
        "epoch": 0.5086317959288843,
        "step": 3948
    },
    {
        "loss": 2.0517,
        "grad_norm": 2.292884111404419,
        "learning_rate": 4.6383860209639264e-05,
        "epoch": 0.5087606287039422,
        "step": 3949
    },
    {
        "loss": 2.2965,
        "grad_norm": 2.050672769546509,
        "learning_rate": 4.632319362810562e-05,
        "epoch": 0.5088894614790003,
        "step": 3950
    },
    {
        "loss": 2.4925,
        "grad_norm": 2.24775767326355,
        "learning_rate": 4.6262532488413035e-05,
        "epoch": 0.5090182942540582,
        "step": 3951
    },
    {
        "loss": 2.3106,
        "grad_norm": 1.297286033630371,
        "learning_rate": 4.62018768803427e-05,
        "epoch": 0.5091471270291162,
        "step": 3952
    },
    {
        "loss": 2.0149,
        "grad_norm": 1.5939984321594238,
        "learning_rate": 4.614122689366767e-05,
        "epoch": 0.5092759598041742,
        "step": 3953
    },
    {
        "loss": 1.7153,
        "grad_norm": 2.658700466156006,
        "learning_rate": 4.6080582618152744e-05,
        "epoch": 0.5094047925792321,
        "step": 3954
    },
    {
        "loss": 2.3449,
        "grad_norm": 1.2640156745910645,
        "learning_rate": 4.601994414355418e-05,
        "epoch": 0.5095336253542901,
        "step": 3955
    },
    {
        "loss": 1.6757,
        "grad_norm": 2.4322164058685303,
        "learning_rate": 4.595931155961965e-05,
        "epoch": 0.5096624581293481,
        "step": 3956
    },
    {
        "loss": 2.1664,
        "grad_norm": 1.6345775127410889,
        "learning_rate": 4.589868495608816e-05,
        "epoch": 0.5097912909044061,
        "step": 3957
    },
    {
        "loss": 2.5304,
        "grad_norm": 2.0496621131896973,
        "learning_rate": 4.583806442268987e-05,
        "epoch": 0.509920123679464,
        "step": 3958
    },
    {
        "loss": 1.9406,
        "grad_norm": 2.5164973735809326,
        "learning_rate": 4.577745004914592e-05,
        "epoch": 0.5100489564545221,
        "step": 3959
    },
    {
        "loss": 2.2848,
        "grad_norm": 2.555992841720581,
        "learning_rate": 4.571684192516835e-05,
        "epoch": 0.51017778922958,
        "step": 3960
    },
    {
        "loss": 2.1744,
        "grad_norm": 2.1323702335357666,
        "learning_rate": 4.5656240140459985e-05,
        "epoch": 0.5103066220046379,
        "step": 3961
    },
    {
        "loss": 1.6821,
        "grad_norm": 2.059999942779541,
        "learning_rate": 4.559564478471417e-05,
        "epoch": 0.510435454779696,
        "step": 3962
    },
    {
        "loss": 2.0401,
        "grad_norm": 2.07981014251709,
        "learning_rate": 4.553505594761478e-05,
        "epoch": 0.5105642875547539,
        "step": 3963
    },
    {
        "loss": 2.6542,
        "grad_norm": 2.3062584400177,
        "learning_rate": 4.547447371883609e-05,
        "epoch": 0.510693120329812,
        "step": 3964
    },
    {
        "loss": 1.7078,
        "grad_norm": 1.8408589363098145,
        "learning_rate": 4.5413898188042585e-05,
        "epoch": 0.5108219531048699,
        "step": 3965
    },
    {
        "loss": 0.9778,
        "grad_norm": 3.271343231201172,
        "learning_rate": 4.535332944488878e-05,
        "epoch": 0.5109507858799278,
        "step": 3966
    },
    {
        "loss": 1.9501,
        "grad_norm": 3.1051933765411377,
        "learning_rate": 4.529276757901908e-05,
        "epoch": 0.5110796186549859,
        "step": 3967
    },
    {
        "loss": 1.0145,
        "grad_norm": 3.2689075469970703,
        "learning_rate": 4.5232212680068e-05,
        "epoch": 0.5112084514300438,
        "step": 3968
    },
    {
        "loss": 1.4746,
        "grad_norm": 2.1076815128326416,
        "learning_rate": 4.5171664837659465e-05,
        "epoch": 0.5113372842051018,
        "step": 3969
    },
    {
        "loss": 1.6957,
        "grad_norm": 2.273083448410034,
        "learning_rate": 4.511112414140699e-05,
        "epoch": 0.5114661169801598,
        "step": 3970
    },
    {
        "loss": 2.4156,
        "grad_norm": 2.0014874935150146,
        "learning_rate": 4.50505906809136e-05,
        "epoch": 0.5115949497552177,
        "step": 3971
    },
    {
        "loss": 2.2557,
        "grad_norm": 1.3110613822937012,
        "learning_rate": 4.4990064545771615e-05,
        "epoch": 0.5117237825302757,
        "step": 3972
    },
    {
        "loss": 1.9489,
        "grad_norm": 3.266209363937378,
        "learning_rate": 4.4929545825562445e-05,
        "epoch": 0.5118526153053337,
        "step": 3973
    },
    {
        "loss": 1.5659,
        "grad_norm": 2.766456365585327,
        "learning_rate": 4.4869034609856505e-05,
        "epoch": 0.5119814480803917,
        "step": 3974
    },
    {
        "loss": 1.9635,
        "grad_norm": 2.1714863777160645,
        "learning_rate": 4.48085309882132e-05,
        "epoch": 0.5121102808554496,
        "step": 3975
    },
    {
        "loss": 2.6295,
        "grad_norm": 1.5958205461502075,
        "learning_rate": 4.474803505018064e-05,
        "epoch": 0.5122391136305076,
        "step": 3976
    },
    {
        "loss": 2.5064,
        "grad_norm": 1.2535923719406128,
        "learning_rate": 4.468754688529558e-05,
        "epoch": 0.5123679464055656,
        "step": 3977
    },
    {
        "loss": 1.3643,
        "grad_norm": 2.721503257751465,
        "learning_rate": 4.4627066583083256e-05,
        "epoch": 0.5124967791806235,
        "step": 3978
    },
    {
        "loss": 1.847,
        "grad_norm": 2.408034324645996,
        "learning_rate": 4.456659423305729e-05,
        "epoch": 0.5126256119556816,
        "step": 3979
    },
    {
        "loss": 1.915,
        "grad_norm": 2.4604384899139404,
        "learning_rate": 4.4506129924719506e-05,
        "epoch": 0.5127544447307395,
        "step": 3980
    },
    {
        "loss": 2.0385,
        "grad_norm": 1.9960874319076538,
        "learning_rate": 4.444567374755978e-05,
        "epoch": 0.5128832775057974,
        "step": 3981
    },
    {
        "loss": 2.0024,
        "grad_norm": 1.9029368162155151,
        "learning_rate": 4.4385225791056075e-05,
        "epoch": 0.5130121102808555,
        "step": 3982
    },
    {
        "loss": 1.7381,
        "grad_norm": 3.7200121879577637,
        "learning_rate": 4.4324786144674116e-05,
        "epoch": 0.5131409430559134,
        "step": 3983
    },
    {
        "loss": 2.4426,
        "grad_norm": 1.5951614379882812,
        "learning_rate": 4.426435489786727e-05,
        "epoch": 0.5132697758309714,
        "step": 3984
    },
    {
        "loss": 2.4521,
        "grad_norm": 1.930224061012268,
        "learning_rate": 4.420393214007661e-05,
        "epoch": 0.5133986086060294,
        "step": 3985
    },
    {
        "loss": 2.6118,
        "grad_norm": 1.3901010751724243,
        "learning_rate": 4.4143517960730595e-05,
        "epoch": 0.5135274413810873,
        "step": 3986
    },
    {
        "loss": 1.5056,
        "grad_norm": 2.561243772506714,
        "learning_rate": 4.4083112449244914e-05,
        "epoch": 0.5136562741561453,
        "step": 3987
    },
    {
        "loss": 2.0372,
        "grad_norm": 2.5390477180480957,
        "learning_rate": 4.402271569502244e-05,
        "epoch": 0.5137851069312033,
        "step": 3988
    },
    {
        "loss": 1.9312,
        "grad_norm": 1.8637133836746216,
        "learning_rate": 4.396232778745316e-05,
        "epoch": 0.5139139397062613,
        "step": 3989
    },
    {
        "loss": 2.0667,
        "grad_norm": 1.5631312131881714,
        "learning_rate": 4.3901948815913965e-05,
        "epoch": 0.5140427724813192,
        "step": 3990
    },
    {
        "loss": 2.3129,
        "grad_norm": 2.065582275390625,
        "learning_rate": 4.384157886976846e-05,
        "epoch": 0.5141716052563772,
        "step": 3991
    },
    {
        "loss": 2.2984,
        "grad_norm": 1.4738327264785767,
        "learning_rate": 4.3781218038366875e-05,
        "epoch": 0.5143004380314352,
        "step": 3992
    },
    {
        "loss": 1.747,
        "grad_norm": 2.758735418319702,
        "learning_rate": 4.3720866411045984e-05,
        "epoch": 0.5144292708064931,
        "step": 3993
    },
    {
        "loss": 1.6543,
        "grad_norm": 2.5092999935150146,
        "learning_rate": 4.3660524077129064e-05,
        "epoch": 0.5145581035815512,
        "step": 3994
    },
    {
        "loss": 2.2181,
        "grad_norm": 1.4677634239196777,
        "learning_rate": 4.3600191125925405e-05,
        "epoch": 0.5146869363566091,
        "step": 3995
    },
    {
        "loss": 2.2685,
        "grad_norm": 1.8879995346069336,
        "learning_rate": 4.3539867646730556e-05,
        "epoch": 0.514815769131667,
        "step": 3996
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.8091204166412354,
        "learning_rate": 4.347955372882606e-05,
        "epoch": 0.5149446019067251,
        "step": 3997
    },
    {
        "loss": 1.9984,
        "grad_norm": 2.269840717315674,
        "learning_rate": 4.341924946147923e-05,
        "epoch": 0.515073434681783,
        "step": 3998
    },
    {
        "loss": 2.2012,
        "grad_norm": 1.7738826274871826,
        "learning_rate": 4.3358954933943085e-05,
        "epoch": 0.5152022674568411,
        "step": 3999
    },
    {
        "loss": 2.116,
        "grad_norm": 2.578413248062134,
        "learning_rate": 4.329867023545632e-05,
        "epoch": 0.515331100231899,
        "step": 4000
    },
    {
        "loss": 1.7717,
        "grad_norm": 1.2516615390777588,
        "learning_rate": 4.323839545524303e-05,
        "epoch": 0.5154599330069569,
        "step": 4001
    },
    {
        "loss": 1.7736,
        "grad_norm": 2.1112592220306396,
        "learning_rate": 4.317813068251265e-05,
        "epoch": 0.515588765782015,
        "step": 4002
    },
    {
        "loss": 1.3213,
        "grad_norm": 2.083022356033325,
        "learning_rate": 4.311787600645976e-05,
        "epoch": 0.5157175985570729,
        "step": 4003
    },
    {
        "loss": 2.2357,
        "grad_norm": 2.284348964691162,
        "learning_rate": 4.3057631516264085e-05,
        "epoch": 0.5158464313321309,
        "step": 4004
    },
    {
        "loss": 2.4329,
        "grad_norm": 2.604156970977783,
        "learning_rate": 4.2997397301090175e-05,
        "epoch": 0.5159752641071889,
        "step": 4005
    },
    {
        "loss": 1.5472,
        "grad_norm": 2.9027950763702393,
        "learning_rate": 4.293717345008738e-05,
        "epoch": 0.5161040968822469,
        "step": 4006
    },
    {
        "loss": 2.6189,
        "grad_norm": 1.4635202884674072,
        "learning_rate": 4.287696005238978e-05,
        "epoch": 0.5162329296573048,
        "step": 4007
    },
    {
        "loss": 1.4524,
        "grad_norm": 2.8020308017730713,
        "learning_rate": 4.281675719711599e-05,
        "epoch": 0.5163617624323628,
        "step": 4008
    },
    {
        "loss": 2.2615,
        "grad_norm": 1.9254621267318726,
        "learning_rate": 4.275656497336894e-05,
        "epoch": 0.5164905952074208,
        "step": 4009
    },
    {
        "loss": 1.8965,
        "grad_norm": 2.271852731704712,
        "learning_rate": 4.2696383470235735e-05,
        "epoch": 0.5166194279824787,
        "step": 4010
    },
    {
        "loss": 2.5532,
        "grad_norm": 2.8087284564971924,
        "learning_rate": 4.2636212776788e-05,
        "epoch": 0.5167482607575368,
        "step": 4011
    },
    {
        "loss": 2.4383,
        "grad_norm": 1.7865639925003052,
        "learning_rate": 4.2576052982080956e-05,
        "epoch": 0.5168770935325947,
        "step": 4012
    },
    {
        "loss": 2.0588,
        "grad_norm": 2.05720853805542,
        "learning_rate": 4.2515904175153825e-05,
        "epoch": 0.5170059263076526,
        "step": 4013
    },
    {
        "loss": 0.7747,
        "grad_norm": 2.965341567993164,
        "learning_rate": 4.245576644502962e-05,
        "epoch": 0.5171347590827107,
        "step": 4014
    },
    {
        "loss": 2.1944,
        "grad_norm": 2.0882160663604736,
        "learning_rate": 4.2395639880714946e-05,
        "epoch": 0.5172635918577686,
        "step": 4015
    },
    {
        "loss": 2.1894,
        "grad_norm": 1.2865476608276367,
        "learning_rate": 4.233552457119983e-05,
        "epoch": 0.5173924246328266,
        "step": 4016
    },
    {
        "loss": 2.5637,
        "grad_norm": 1.8722230195999146,
        "learning_rate": 4.2275420605457646e-05,
        "epoch": 0.5175212574078846,
        "step": 4017
    },
    {
        "loss": 1.9403,
        "grad_norm": 1.711669921875,
        "learning_rate": 4.2215328072444985e-05,
        "epoch": 0.5176500901829425,
        "step": 4018
    },
    {
        "loss": 2.2213,
        "grad_norm": 1.7003370523452759,
        "learning_rate": 4.215524706110166e-05,
        "epoch": 0.5177789229580005,
        "step": 4019
    },
    {
        "loss": 1.7173,
        "grad_norm": 2.289374351501465,
        "learning_rate": 4.209517766035017e-05,
        "epoch": 0.5179077557330585,
        "step": 4020
    },
    {
        "loss": 2.1428,
        "grad_norm": 1.8667099475860596,
        "learning_rate": 4.203511995909603e-05,
        "epoch": 0.5180365885081165,
        "step": 4021
    },
    {
        "loss": 1.7624,
        "grad_norm": 2.275927782058716,
        "learning_rate": 4.197507404622739e-05,
        "epoch": 0.5181654212831744,
        "step": 4022
    },
    {
        "loss": 2.0574,
        "grad_norm": 1.3635886907577515,
        "learning_rate": 4.191504001061491e-05,
        "epoch": 0.5182942540582324,
        "step": 4023
    },
    {
        "loss": 2.3383,
        "grad_norm": 1.5455739498138428,
        "learning_rate": 4.1855017941111675e-05,
        "epoch": 0.5184230868332904,
        "step": 4024
    },
    {
        "loss": 1.9981,
        "grad_norm": 2.321652412414551,
        "learning_rate": 4.1795007926553106e-05,
        "epoch": 0.5185519196083483,
        "step": 4025
    },
    {
        "loss": 2.4906,
        "grad_norm": 1.181174397468567,
        "learning_rate": 4.1735010055756804e-05,
        "epoch": 0.5186807523834064,
        "step": 4026
    },
    {
        "loss": 1.6908,
        "grad_norm": 2.111985206604004,
        "learning_rate": 4.1675024417522234e-05,
        "epoch": 0.5188095851584643,
        "step": 4027
    },
    {
        "loss": 2.0366,
        "grad_norm": 1.809510588645935,
        "learning_rate": 4.1615051100630984e-05,
        "epoch": 0.5189384179335222,
        "step": 4028
    },
    {
        "loss": 1.8449,
        "grad_norm": 2.188400983810425,
        "learning_rate": 4.15550901938463e-05,
        "epoch": 0.5190672507085803,
        "step": 4029
    },
    {
        "loss": 2.2665,
        "grad_norm": 1.4022172689437866,
        "learning_rate": 4.1495141785913016e-05,
        "epoch": 0.5191960834836382,
        "step": 4030
    },
    {
        "loss": 2.2322,
        "grad_norm": 1.1695623397827148,
        "learning_rate": 4.143520596555748e-05,
        "epoch": 0.5193249162586963,
        "step": 4031
    },
    {
        "loss": 2.2162,
        "grad_norm": 2.6142218112945557,
        "learning_rate": 4.137528282148747e-05,
        "epoch": 0.5194537490337542,
        "step": 4032
    },
    {
        "loss": 2.3909,
        "grad_norm": 2.1038177013397217,
        "learning_rate": 4.131537244239201e-05,
        "epoch": 0.5195825818088121,
        "step": 4033
    },
    {
        "loss": 2.1034,
        "grad_norm": 2.395606517791748,
        "learning_rate": 4.125547491694116e-05,
        "epoch": 0.5197114145838702,
        "step": 4034
    },
    {
        "loss": 1.6992,
        "grad_norm": 1.447884202003479,
        "learning_rate": 4.119559033378593e-05,
        "epoch": 0.5198402473589281,
        "step": 4035
    },
    {
        "loss": 2.3802,
        "grad_norm": 2.7951982021331787,
        "learning_rate": 4.113571878155826e-05,
        "epoch": 0.5199690801339861,
        "step": 4036
    },
    {
        "loss": 2.5237,
        "grad_norm": 1.4409565925598145,
        "learning_rate": 4.1075860348870856e-05,
        "epoch": 0.5200979129090441,
        "step": 4037
    },
    {
        "loss": 2.2064,
        "grad_norm": 1.4487968683242798,
        "learning_rate": 4.101601512431684e-05,
        "epoch": 0.520226745684102,
        "step": 4038
    },
    {
        "loss": 1.6206,
        "grad_norm": 1.5340490341186523,
        "learning_rate": 4.09561831964699e-05,
        "epoch": 0.52035557845916,
        "step": 4039
    },
    {
        "loss": 2.0081,
        "grad_norm": 2.6248269081115723,
        "learning_rate": 4.089636465388405e-05,
        "epoch": 0.520484411234218,
        "step": 4040
    },
    {
        "loss": 2.0822,
        "grad_norm": 1.7799720764160156,
        "learning_rate": 4.0836559585093425e-05,
        "epoch": 0.520613244009276,
        "step": 4041
    },
    {
        "loss": 2.1999,
        "grad_norm": 2.276581287384033,
        "learning_rate": 4.077676807861222e-05,
        "epoch": 0.5207420767843339,
        "step": 4042
    },
    {
        "loss": 1.8998,
        "grad_norm": 2.013678550720215,
        "learning_rate": 4.0716990222934636e-05,
        "epoch": 0.5208709095593919,
        "step": 4043
    },
    {
        "loss": 2.2098,
        "grad_norm": 1.3435598611831665,
        "learning_rate": 4.06572261065346e-05,
        "epoch": 0.5209997423344499,
        "step": 4044
    },
    {
        "loss": 1.8365,
        "grad_norm": 2.166440725326538,
        "learning_rate": 4.0597475817865744e-05,
        "epoch": 0.5211285751095078,
        "step": 4045
    },
    {
        "loss": 2.2274,
        "grad_norm": 1.9442330598831177,
        "learning_rate": 4.053773944536121e-05,
        "epoch": 0.5212574078845659,
        "step": 4046
    },
    {
        "loss": 1.4161,
        "grad_norm": 1.9391658306121826,
        "learning_rate": 4.047801707743359e-05,
        "epoch": 0.5213862406596238,
        "step": 4047
    },
    {
        "loss": 2.3987,
        "grad_norm": 1.7508831024169922,
        "learning_rate": 4.041830880247467e-05,
        "epoch": 0.5215150734346817,
        "step": 4048
    },
    {
        "loss": 2.2471,
        "grad_norm": 1.5075002908706665,
        "learning_rate": 4.0358614708855394e-05,
        "epoch": 0.5216439062097398,
        "step": 4049
    },
    {
        "loss": 2.2825,
        "grad_norm": 1.3103885650634766,
        "learning_rate": 4.0298934884925774e-05,
        "epoch": 0.5217727389847977,
        "step": 4050
    },
    {
        "loss": 2.0398,
        "grad_norm": 1.9774889945983887,
        "learning_rate": 4.023926941901472e-05,
        "epoch": 0.5219015717598557,
        "step": 4051
    },
    {
        "loss": 2.1557,
        "grad_norm": 1.9728771448135376,
        "learning_rate": 4.0179618399429764e-05,
        "epoch": 0.5220304045349137,
        "step": 4052
    },
    {
        "loss": 1.696,
        "grad_norm": 3.380225658416748,
        "learning_rate": 4.0119981914457065e-05,
        "epoch": 0.5221592373099716,
        "step": 4053
    },
    {
        "loss": 2.1277,
        "grad_norm": 2.261669397354126,
        "learning_rate": 4.0060360052361535e-05,
        "epoch": 0.5222880700850296,
        "step": 4054
    },
    {
        "loss": 1.2998,
        "grad_norm": 3.0271058082580566,
        "learning_rate": 4.000075290138614e-05,
        "epoch": 0.5224169028600876,
        "step": 4055
    },
    {
        "loss": 1.1613,
        "grad_norm": 2.8989474773406982,
        "learning_rate": 3.9941160549752145e-05,
        "epoch": 0.5225457356351456,
        "step": 4056
    },
    {
        "loss": 2.1944,
        "grad_norm": 2.6248393058776855,
        "learning_rate": 3.988158308565899e-05,
        "epoch": 0.5226745684102035,
        "step": 4057
    },
    {
        "loss": 1.9875,
        "grad_norm": 1.7020243406295776,
        "learning_rate": 3.9822020597284064e-05,
        "epoch": 0.5228034011852616,
        "step": 4058
    },
    {
        "loss": 2.1349,
        "grad_norm": 1.5906885862350464,
        "learning_rate": 3.976247317278254e-05,
        "epoch": 0.5229322339603195,
        "step": 4059
    },
    {
        "loss": 2.4495,
        "grad_norm": 2.8554773330688477,
        "learning_rate": 3.970294090028728e-05,
        "epoch": 0.5230610667353774,
        "step": 4060
    },
    {
        "loss": 1.533,
        "grad_norm": 3.6092147827148438,
        "learning_rate": 3.9643423867908755e-05,
        "epoch": 0.5231898995104355,
        "step": 4061
    },
    {
        "loss": 2.2289,
        "grad_norm": 1.9061189889907837,
        "learning_rate": 3.9583922163735e-05,
        "epoch": 0.5233187322854934,
        "step": 4062
    },
    {
        "loss": 1.7286,
        "grad_norm": 2.519507884979248,
        "learning_rate": 3.9524435875831125e-05,
        "epoch": 0.5234475650605515,
        "step": 4063
    },
    {
        "loss": 1.9559,
        "grad_norm": 2.3181633949279785,
        "learning_rate": 3.946496509223961e-05,
        "epoch": 0.5235763978356094,
        "step": 4064
    },
    {
        "loss": 0.607,
        "grad_norm": 2.826206922531128,
        "learning_rate": 3.940550990097993e-05,
        "epoch": 0.5237052306106673,
        "step": 4065
    },
    {
        "loss": 2.35,
        "grad_norm": 1.6455963850021362,
        "learning_rate": 3.934607039004847e-05,
        "epoch": 0.5238340633857254,
        "step": 4066
    },
    {
        "loss": 1.2942,
        "grad_norm": 3.0686161518096924,
        "learning_rate": 3.928664664741837e-05,
        "epoch": 0.5239628961607833,
        "step": 4067
    },
    {
        "loss": 1.0753,
        "grad_norm": 2.7546772956848145,
        "learning_rate": 3.922723876103952e-05,
        "epoch": 0.5240917289358413,
        "step": 4068
    },
    {
        "loss": 0.9842,
        "grad_norm": 2.596801519393921,
        "learning_rate": 3.916784681883831e-05,
        "epoch": 0.5242205617108993,
        "step": 4069
    },
    {
        "loss": 1.3459,
        "grad_norm": 2.1525771617889404,
        "learning_rate": 3.910847090871744e-05,
        "epoch": 0.5243493944859572,
        "step": 4070
    },
    {
        "loss": 1.2905,
        "grad_norm": 1.696529746055603,
        "learning_rate": 3.904911111855606e-05,
        "epoch": 0.5244782272610152,
        "step": 4071
    },
    {
        "loss": 2.1272,
        "grad_norm": 1.7393739223480225,
        "learning_rate": 3.898976753620936e-05,
        "epoch": 0.5246070600360732,
        "step": 4072
    },
    {
        "loss": 1.9612,
        "grad_norm": 1.2983025312423706,
        "learning_rate": 3.8930440249508515e-05,
        "epoch": 0.5247358928111312,
        "step": 4073
    },
    {
        "loss": 1.8085,
        "grad_norm": 3.1443428993225098,
        "learning_rate": 3.8871129346260585e-05,
        "epoch": 0.5248647255861891,
        "step": 4074
    },
    {
        "loss": 2.301,
        "grad_norm": 2.241914749145508,
        "learning_rate": 3.881183491424845e-05,
        "epoch": 0.5249935583612471,
        "step": 4075
    },
    {
        "loss": 2.0524,
        "grad_norm": 1.9448444843292236,
        "learning_rate": 3.875255704123059e-05,
        "epoch": 0.5251223911363051,
        "step": 4076
    },
    {
        "loss": 2.4618,
        "grad_norm": 2.3369975090026855,
        "learning_rate": 3.8693295814940926e-05,
        "epoch": 0.525251223911363,
        "step": 4077
    },
    {
        "loss": 2.5166,
        "grad_norm": 1.5127427577972412,
        "learning_rate": 3.8634051323088686e-05,
        "epoch": 0.5253800566864211,
        "step": 4078
    },
    {
        "loss": 2.1322,
        "grad_norm": 1.5426599979400635,
        "learning_rate": 3.85748236533586e-05,
        "epoch": 0.525508889461479,
        "step": 4079
    },
    {
        "loss": 2.4536,
        "grad_norm": 1.81293523311615,
        "learning_rate": 3.851561289341023e-05,
        "epoch": 0.5256377222365369,
        "step": 4080
    },
    {
        "loss": 2.4117,
        "grad_norm": 1.686880111694336,
        "learning_rate": 3.8456419130878176e-05,
        "epoch": 0.525766555011595,
        "step": 4081
    },
    {
        "loss": 2.2094,
        "grad_norm": 2.124614953994751,
        "learning_rate": 3.839724245337192e-05,
        "epoch": 0.5258953877866529,
        "step": 4082
    },
    {
        "loss": 1.965,
        "grad_norm": 2.145695209503174,
        "learning_rate": 3.833808294847568e-05,
        "epoch": 0.5260242205617109,
        "step": 4083
    },
    {
        "loss": 1.9582,
        "grad_norm": 1.9969468116760254,
        "learning_rate": 3.82789407037482e-05,
        "epoch": 0.5261530533367689,
        "step": 4084
    },
    {
        "loss": 2.5281,
        "grad_norm": 1.8484030961990356,
        "learning_rate": 3.821981580672264e-05,
        "epoch": 0.5262818861118268,
        "step": 4085
    },
    {
        "loss": 2.1734,
        "grad_norm": 1.660556435585022,
        "learning_rate": 3.8160708344906596e-05,
        "epoch": 0.5264107188868848,
        "step": 4086
    },
    {
        "loss": 1.8124,
        "grad_norm": 1.2628138065338135,
        "learning_rate": 3.810161840578179e-05,
        "epoch": 0.5265395516619428,
        "step": 4087
    },
    {
        "loss": 1.7336,
        "grad_norm": 2.7555160522460938,
        "learning_rate": 3.804254607680404e-05,
        "epoch": 0.5266683844370008,
        "step": 4088
    },
    {
        "loss": 1.4299,
        "grad_norm": 2.8862216472625732,
        "learning_rate": 3.7983491445403075e-05,
        "epoch": 0.5267972172120587,
        "step": 4089
    },
    {
        "loss": 1.8607,
        "grad_norm": 1.738625407218933,
        "learning_rate": 3.792445459898247e-05,
        "epoch": 0.5269260499871167,
        "step": 4090
    },
    {
        "loss": 1.4259,
        "grad_norm": 2.906538724899292,
        "learning_rate": 3.7865435624919435e-05,
        "epoch": 0.5270548827621747,
        "step": 4091
    },
    {
        "loss": 2.2984,
        "grad_norm": 1.635148286819458,
        "learning_rate": 3.780643461056469e-05,
        "epoch": 0.5271837155372326,
        "step": 4092
    },
    {
        "loss": 2.1731,
        "grad_norm": 2.3359365463256836,
        "learning_rate": 3.7747451643242475e-05,
        "epoch": 0.5273125483122907,
        "step": 4093
    },
    {
        "loss": 1.3011,
        "grad_norm": 2.003901958465576,
        "learning_rate": 3.768848681025028e-05,
        "epoch": 0.5274413810873486,
        "step": 4094
    },
    {
        "loss": 1.0764,
        "grad_norm": 4.145840644836426,
        "learning_rate": 3.762954019885866e-05,
        "epoch": 0.5275702138624065,
        "step": 4095
    },
    {
        "loss": 2.2518,
        "grad_norm": 2.4786083698272705,
        "learning_rate": 3.757061189631139e-05,
        "epoch": 0.5276990466374646,
        "step": 4096
    },
    {
        "loss": 1.5401,
        "grad_norm": 2.704484224319458,
        "learning_rate": 3.7511701989825034e-05,
        "epoch": 0.5278278794125225,
        "step": 4097
    },
    {
        "loss": 2.2543,
        "grad_norm": 1.728656530380249,
        "learning_rate": 3.7452810566588906e-05,
        "epoch": 0.5279567121875806,
        "step": 4098
    },
    {
        "loss": 1.5286,
        "grad_norm": 5.295302391052246,
        "learning_rate": 3.7393937713764974e-05,
        "epoch": 0.5280855449626385,
        "step": 4099
    },
    {
        "loss": 2.4353,
        "grad_norm": 1.9321420192718506,
        "learning_rate": 3.7335083518487754e-05,
        "epoch": 0.5282143777376964,
        "step": 4100
    },
    {
        "loss": 2.5291,
        "grad_norm": 1.9412484169006348,
        "learning_rate": 3.7276248067864204e-05,
        "epoch": 0.5283432105127545,
        "step": 4101
    },
    {
        "loss": 1.2473,
        "grad_norm": 4.358029365539551,
        "learning_rate": 3.721743144897341e-05,
        "epoch": 0.5284720432878124,
        "step": 4102
    },
    {
        "loss": 2.9108,
        "grad_norm": 1.4853788614273071,
        "learning_rate": 3.715863374886663e-05,
        "epoch": 0.5286008760628704,
        "step": 4103
    },
    {
        "loss": 1.6286,
        "grad_norm": 3.7341761589050293,
        "learning_rate": 3.7099855054567134e-05,
        "epoch": 0.5287297088379284,
        "step": 4104
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.9470804929733276,
        "learning_rate": 3.704109545307017e-05,
        "epoch": 0.5288585416129864,
        "step": 4105
    },
    {
        "loss": 2.2315,
        "grad_norm": 2.367295265197754,
        "learning_rate": 3.698235503134253e-05,
        "epoch": 0.5289873743880443,
        "step": 4106
    },
    {
        "loss": 2.7926,
        "grad_norm": 1.404915690422058,
        "learning_rate": 3.692363387632274e-05,
        "epoch": 0.5291162071631023,
        "step": 4107
    },
    {
        "loss": 1.4559,
        "grad_norm": 2.765763998031616,
        "learning_rate": 3.6864932074920824e-05,
        "epoch": 0.5292450399381603,
        "step": 4108
    },
    {
        "loss": 1.5732,
        "grad_norm": 2.097285270690918,
        "learning_rate": 3.6806249714018106e-05,
        "epoch": 0.5293738727132182,
        "step": 4109
    },
    {
        "loss": 2.1914,
        "grad_norm": 1.8132343292236328,
        "learning_rate": 3.674758688046709e-05,
        "epoch": 0.5295027054882763,
        "step": 4110
    },
    {
        "loss": 2.4022,
        "grad_norm": 2.225350856781006,
        "learning_rate": 3.668894366109149e-05,
        "epoch": 0.5296315382633342,
        "step": 4111
    },
    {
        "loss": 2.0181,
        "grad_norm": 2.118616819381714,
        "learning_rate": 3.6630320142685995e-05,
        "epoch": 0.5297603710383921,
        "step": 4112
    },
    {
        "loss": 1.7567,
        "grad_norm": 1.3919243812561035,
        "learning_rate": 3.657171641201595e-05,
        "epoch": 0.5298892038134502,
        "step": 4113
    },
    {
        "loss": 2.2543,
        "grad_norm": 2.8265511989593506,
        "learning_rate": 3.6513132555817676e-05,
        "epoch": 0.5300180365885081,
        "step": 4114
    },
    {
        "loss": 2.7479,
        "grad_norm": 1.6994588375091553,
        "learning_rate": 3.645456866079794e-05,
        "epoch": 0.5301468693635661,
        "step": 4115
    },
    {
        "loss": 2.289,
        "grad_norm": 1.5285762548446655,
        "learning_rate": 3.6396024813633944e-05,
        "epoch": 0.5302757021386241,
        "step": 4116
    },
    {
        "loss": 2.0446,
        "grad_norm": 4.2181267738342285,
        "learning_rate": 3.6337501100973224e-05,
        "epoch": 0.530404534913682,
        "step": 4117
    },
    {
        "loss": 2.017,
        "grad_norm": 2.7140257358551025,
        "learning_rate": 3.627899760943355e-05,
        "epoch": 0.53053336768874,
        "step": 4118
    },
    {
        "loss": 1.4544,
        "grad_norm": 2.9655094146728516,
        "learning_rate": 3.622051442560281e-05,
        "epoch": 0.530662200463798,
        "step": 4119
    },
    {
        "loss": 1.9814,
        "grad_norm": 2.598254442214966,
        "learning_rate": 3.616205163603875e-05,
        "epoch": 0.530791033238856,
        "step": 4120
    },
    {
        "loss": 2.2124,
        "grad_norm": 1.741950273513794,
        "learning_rate": 3.6103609327268854e-05,
        "epoch": 0.5309198660139139,
        "step": 4121
    },
    {
        "loss": 2.4788,
        "grad_norm": 1.6651875972747803,
        "learning_rate": 3.604518758579061e-05,
        "epoch": 0.5310486987889719,
        "step": 4122
    },
    {
        "loss": 2.0641,
        "grad_norm": 1.9559069871902466,
        "learning_rate": 3.598678649807075e-05,
        "epoch": 0.5311775315640299,
        "step": 4123
    },
    {
        "loss": 1.7824,
        "grad_norm": 2.3584346771240234,
        "learning_rate": 3.592840615054551e-05,
        "epoch": 0.5313063643390878,
        "step": 4124
    },
    {
        "loss": 1.8094,
        "grad_norm": 3.00943922996521,
        "learning_rate": 3.5870046629620514e-05,
        "epoch": 0.5314351971141459,
        "step": 4125
    },
    {
        "loss": 2.17,
        "grad_norm": 1.9416327476501465,
        "learning_rate": 3.581170802167054e-05,
        "epoch": 0.5315640298892038,
        "step": 4126
    },
    {
        "loss": 1.9287,
        "grad_norm": 1.8405214548110962,
        "learning_rate": 3.575339041303934e-05,
        "epoch": 0.5316928626642617,
        "step": 4127
    },
    {
        "loss": 1.6824,
        "grad_norm": 3.7092533111572266,
        "learning_rate": 3.569509389003961e-05,
        "epoch": 0.5318216954393198,
        "step": 4128
    },
    {
        "loss": 1.7999,
        "grad_norm": 2.603449583053589,
        "learning_rate": 3.56368185389529e-05,
        "epoch": 0.5319505282143777,
        "step": 4129
    },
    {
        "loss": 1.2885,
        "grad_norm": 2.751842975616455,
        "learning_rate": 3.557856444602937e-05,
        "epoch": 0.5320793609894358,
        "step": 4130
    },
    {
        "loss": 2.0493,
        "grad_norm": 2.218398332595825,
        "learning_rate": 3.552033169748775e-05,
        "epoch": 0.5322081937644937,
        "step": 4131
    },
    {
        "loss": 2.1621,
        "grad_norm": 2.1337716579437256,
        "learning_rate": 3.5462120379515154e-05,
        "epoch": 0.5323370265395516,
        "step": 4132
    },
    {
        "loss": 1.7496,
        "grad_norm": 2.102982521057129,
        "learning_rate": 3.540393057826701e-05,
        "epoch": 0.5324658593146097,
        "step": 4133
    },
    {
        "loss": 1.8911,
        "grad_norm": 2.214252471923828,
        "learning_rate": 3.534576237986687e-05,
        "epoch": 0.5325946920896676,
        "step": 4134
    },
    {
        "loss": 2.2502,
        "grad_norm": 1.3212250471115112,
        "learning_rate": 3.528761587040626e-05,
        "epoch": 0.5327235248647256,
        "step": 4135
    },
    {
        "loss": 1.5871,
        "grad_norm": 3.930828094482422,
        "learning_rate": 3.522949113594469e-05,
        "epoch": 0.5328523576397836,
        "step": 4136
    },
    {
        "loss": 1.8331,
        "grad_norm": 2.8780574798583984,
        "learning_rate": 3.517138826250948e-05,
        "epoch": 0.5329811904148415,
        "step": 4137
    },
    {
        "loss": 1.9794,
        "grad_norm": 2.43739652633667,
        "learning_rate": 3.51133073360954e-05,
        "epoch": 0.5331100231898995,
        "step": 4138
    },
    {
        "loss": 1.8946,
        "grad_norm": 2.1733899116516113,
        "learning_rate": 3.5055248442664996e-05,
        "epoch": 0.5332388559649575,
        "step": 4139
    },
    {
        "loss": 1.5682,
        "grad_norm": 1.1840245723724365,
        "learning_rate": 3.4997211668148053e-05,
        "epoch": 0.5333676887400155,
        "step": 4140
    },
    {
        "loss": 2.2631,
        "grad_norm": 1.5411783456802368,
        "learning_rate": 3.4939197098441625e-05,
        "epoch": 0.5334965215150734,
        "step": 4141
    },
    {
        "loss": 2.2848,
        "grad_norm": 2.074345111846924,
        "learning_rate": 3.488120481940989e-05,
        "epoch": 0.5336253542901314,
        "step": 4142
    },
    {
        "loss": 1.8484,
        "grad_norm": 1.9717663526535034,
        "learning_rate": 3.482323491688407e-05,
        "epoch": 0.5337541870651894,
        "step": 4143
    },
    {
        "loss": 1.8463,
        "grad_norm": 2.3381497859954834,
        "learning_rate": 3.4765287476662324e-05,
        "epoch": 0.5338830198402473,
        "step": 4144
    },
    {
        "loss": 2.3051,
        "grad_norm": 1.6176321506500244,
        "learning_rate": 3.470736258450946e-05,
        "epoch": 0.5340118526153054,
        "step": 4145
    },
    {
        "loss": 1.1815,
        "grad_norm": 2.819699287414551,
        "learning_rate": 3.4649460326156926e-05,
        "epoch": 0.5341406853903633,
        "step": 4146
    },
    {
        "loss": 1.3487,
        "grad_norm": 3.352060079574585,
        "learning_rate": 3.45915807873027e-05,
        "epoch": 0.5342695181654212,
        "step": 4147
    },
    {
        "loss": 1.8106,
        "grad_norm": 2.2666399478912354,
        "learning_rate": 3.453372405361124e-05,
        "epoch": 0.5343983509404793,
        "step": 4148
    },
    {
        "loss": 2.1635,
        "grad_norm": 1.5089155435562134,
        "learning_rate": 3.447589021071306e-05,
        "epoch": 0.5345271837155372,
        "step": 4149
    },
    {
        "loss": 2.4968,
        "grad_norm": 1.745666265487671,
        "learning_rate": 3.441807934420489e-05,
        "epoch": 0.5346560164905952,
        "step": 4150
    },
    {
        "loss": 2.1379,
        "grad_norm": 1.7037447690963745,
        "learning_rate": 3.436029153964952e-05,
        "epoch": 0.5347848492656532,
        "step": 4151
    },
    {
        "loss": 2.4571,
        "grad_norm": 2.47786808013916,
        "learning_rate": 3.4302526882575456e-05,
        "epoch": 0.5349136820407111,
        "step": 4152
    },
    {
        "loss": 1.7762,
        "grad_norm": 2.6296067237854004,
        "learning_rate": 3.424478545847703e-05,
        "epoch": 0.5350425148157691,
        "step": 4153
    },
    {
        "loss": 1.4952,
        "grad_norm": 2.365800619125366,
        "learning_rate": 3.4187067352814186e-05,
        "epoch": 0.5351713475908271,
        "step": 4154
    },
    {
        "loss": 2.1276,
        "grad_norm": 2.387695550918579,
        "learning_rate": 3.412937265101236e-05,
        "epoch": 0.5353001803658851,
        "step": 4155
    },
    {
        "loss": 2.3312,
        "grad_norm": 1.7157845497131348,
        "learning_rate": 3.407170143846235e-05,
        "epoch": 0.535429013140943,
        "step": 4156
    },
    {
        "loss": 2.1121,
        "grad_norm": 2.935539484024048,
        "learning_rate": 3.401405380052016e-05,
        "epoch": 0.5355578459160011,
        "step": 4157
    },
    {
        "loss": 2.2756,
        "grad_norm": 2.094088315963745,
        "learning_rate": 3.395642982250697e-05,
        "epoch": 0.535686678691059,
        "step": 4158
    },
    {
        "loss": 1.7104,
        "grad_norm": 2.977473020553589,
        "learning_rate": 3.389882958970887e-05,
        "epoch": 0.535815511466117,
        "step": 4159
    },
    {
        "loss": 1.9605,
        "grad_norm": 2.6649935245513916,
        "learning_rate": 3.384125318737677e-05,
        "epoch": 0.535944344241175,
        "step": 4160
    },
    {
        "loss": 0.8485,
        "grad_norm": 1.7712560892105103,
        "learning_rate": 3.3783700700726417e-05,
        "epoch": 0.5360731770162329,
        "step": 4161
    },
    {
        "loss": 1.6341,
        "grad_norm": 3.530503273010254,
        "learning_rate": 3.372617221493815e-05,
        "epoch": 0.536202009791291,
        "step": 4162
    },
    {
        "loss": 1.6825,
        "grad_norm": 2.172154188156128,
        "learning_rate": 3.366866781515671e-05,
        "epoch": 0.5363308425663489,
        "step": 4163
    },
    {
        "loss": 2.0006,
        "grad_norm": 2.687168836593628,
        "learning_rate": 3.3611187586491134e-05,
        "epoch": 0.5364596753414068,
        "step": 4164
    },
    {
        "loss": 2.0558,
        "grad_norm": 2.7652719020843506,
        "learning_rate": 3.355373161401497e-05,
        "epoch": 0.5365885081164649,
        "step": 4165
    },
    {
        "loss": 1.2826,
        "grad_norm": 2.095167398452759,
        "learning_rate": 3.349629998276559e-05,
        "epoch": 0.5367173408915228,
        "step": 4166
    },
    {
        "loss": 1.3208,
        "grad_norm": 3.0183956623077393,
        "learning_rate": 3.34388927777444e-05,
        "epoch": 0.5368461736665808,
        "step": 4167
    },
    {
        "loss": 1.6871,
        "grad_norm": 3.08848237991333,
        "learning_rate": 3.338151008391669e-05,
        "epoch": 0.5369750064416388,
        "step": 4168
    },
    {
        "loss": 1.744,
        "grad_norm": 1.7235687971115112,
        "learning_rate": 3.33241519862115e-05,
        "epoch": 0.5371038392166967,
        "step": 4169
    },
    {
        "loss": 2.2697,
        "grad_norm": 1.8781073093414307,
        "learning_rate": 3.32668185695214e-05,
        "epoch": 0.5372326719917547,
        "step": 4170
    },
    {
        "loss": 2.5758,
        "grad_norm": 1.4227004051208496,
        "learning_rate": 3.320950991870244e-05,
        "epoch": 0.5373615047668127,
        "step": 4171
    },
    {
        "loss": 2.1336,
        "grad_norm": 1.8653478622436523,
        "learning_rate": 3.3152226118574017e-05,
        "epoch": 0.5374903375418707,
        "step": 4172
    },
    {
        "loss": 2.1517,
        "grad_norm": 1.9861328601837158,
        "learning_rate": 3.3094967253918865e-05,
        "epoch": 0.5376191703169286,
        "step": 4173
    },
    {
        "loss": 2.1278,
        "grad_norm": 2.392167568206787,
        "learning_rate": 3.303773340948263e-05,
        "epoch": 0.5377480030919866,
        "step": 4174
    },
    {
        "loss": 2.2614,
        "grad_norm": 1.8939571380615234,
        "learning_rate": 3.298052466997403e-05,
        "epoch": 0.5378768358670446,
        "step": 4175
    },
    {
        "loss": 1.0372,
        "grad_norm": 3.3450284004211426,
        "learning_rate": 3.2923341120064646e-05,
        "epoch": 0.5380056686421025,
        "step": 4176
    },
    {
        "loss": 1.9933,
        "grad_norm": 1.4190480709075928,
        "learning_rate": 3.2866182844388705e-05,
        "epoch": 0.5381345014171606,
        "step": 4177
    },
    {
        "loss": 1.8605,
        "grad_norm": 2.1439900398254395,
        "learning_rate": 3.280904992754302e-05,
        "epoch": 0.5382633341922185,
        "step": 4178
    },
    {
        "loss": 1.7081,
        "grad_norm": 2.4478750228881836,
        "learning_rate": 3.2751942454086945e-05,
        "epoch": 0.5383921669672764,
        "step": 4179
    },
    {
        "loss": 1.8125,
        "grad_norm": 2.8941891193389893,
        "learning_rate": 3.269486050854219e-05,
        "epoch": 0.5385209997423345,
        "step": 4180
    },
    {
        "loss": 1.8156,
        "grad_norm": 2.147531270980835,
        "learning_rate": 3.263780417539253e-05,
        "epoch": 0.5386498325173924,
        "step": 4181
    },
    {
        "loss": 2.0209,
        "grad_norm": 2.2133121490478516,
        "learning_rate": 3.258077353908403e-05,
        "epoch": 0.5387786652924504,
        "step": 4182
    },
    {
        "loss": 2.3452,
        "grad_norm": 1.8999769687652588,
        "learning_rate": 3.252376868402463e-05,
        "epoch": 0.5389074980675084,
        "step": 4183
    },
    {
        "loss": 1.7344,
        "grad_norm": 2.77712345123291,
        "learning_rate": 3.24667896945841e-05,
        "epoch": 0.5390363308425663,
        "step": 4184
    },
    {
        "loss": 1.4519,
        "grad_norm": 2.795415163040161,
        "learning_rate": 3.240983665509389e-05,
        "epoch": 0.5391651636176243,
        "step": 4185
    },
    {
        "loss": 1.7129,
        "grad_norm": 1.7754167318344116,
        "learning_rate": 3.235290964984714e-05,
        "epoch": 0.5392939963926823,
        "step": 4186
    },
    {
        "loss": 2.1502,
        "grad_norm": 2.16925311088562,
        "learning_rate": 3.2296008763098425e-05,
        "epoch": 0.5394228291677403,
        "step": 4187
    },
    {
        "loss": 1.5273,
        "grad_norm": 2.1656970977783203,
        "learning_rate": 3.2239134079063635e-05,
        "epoch": 0.5395516619427982,
        "step": 4188
    },
    {
        "loss": 2.2065,
        "grad_norm": 2.255584955215454,
        "learning_rate": 3.2182285681919864e-05,
        "epoch": 0.5396804947178562,
        "step": 4189
    },
    {
        "loss": 1.2175,
        "grad_norm": 2.573939561843872,
        "learning_rate": 3.212546365580532e-05,
        "epoch": 0.5398093274929142,
        "step": 4190
    },
    {
        "loss": 2.2008,
        "grad_norm": 2.3136119842529297,
        "learning_rate": 3.206866808481928e-05,
        "epoch": 0.5399381602679721,
        "step": 4191
    },
    {
        "loss": 2.1533,
        "grad_norm": 1.8032525777816772,
        "learning_rate": 3.201189905302173e-05,
        "epoch": 0.5400669930430302,
        "step": 4192
    },
    {
        "loss": 2.2509,
        "grad_norm": 1.9636033773422241,
        "learning_rate": 3.1955156644433395e-05,
        "epoch": 0.5401958258180881,
        "step": 4193
    },
    {
        "loss": 2.0009,
        "grad_norm": 2.0204360485076904,
        "learning_rate": 3.1898440943035684e-05,
        "epoch": 0.540324658593146,
        "step": 4194
    },
    {
        "loss": 2.6027,
        "grad_norm": 1.7534548044204712,
        "learning_rate": 3.184175203277039e-05,
        "epoch": 0.5404534913682041,
        "step": 4195
    },
    {
        "loss": 1.9742,
        "grad_norm": 2.272923231124878,
        "learning_rate": 3.1785089997539655e-05,
        "epoch": 0.540582324143262,
        "step": 4196
    },
    {
        "loss": 1.45,
        "grad_norm": 2.3908751010894775,
        "learning_rate": 3.172845492120589e-05,
        "epoch": 0.5407111569183201,
        "step": 4197
    },
    {
        "loss": 2.0516,
        "grad_norm": 3.0906007289886475,
        "learning_rate": 3.1671846887591605e-05,
        "epoch": 0.540839989693378,
        "step": 4198
    },
    {
        "loss": 1.8312,
        "grad_norm": 2.690171480178833,
        "learning_rate": 3.161526598047926e-05,
        "epoch": 0.5409688224684359,
        "step": 4199
    },
    {
        "loss": 1.8027,
        "grad_norm": 2.9891245365142822,
        "learning_rate": 3.155871228361118e-05,
        "epoch": 0.541097655243494,
        "step": 4200
    },
    {
        "loss": 1.6983,
        "grad_norm": 2.789045572280884,
        "learning_rate": 3.150218588068946e-05,
        "epoch": 0.5412264880185519,
        "step": 4201
    },
    {
        "loss": 1.5013,
        "grad_norm": 2.797004461288452,
        "learning_rate": 3.144568685537569e-05,
        "epoch": 0.5413553207936099,
        "step": 4202
    },
    {
        "loss": 1.6701,
        "grad_norm": 2.553992509841919,
        "learning_rate": 3.1389215291290994e-05,
        "epoch": 0.5414841535686679,
        "step": 4203
    },
    {
        "loss": 1.7969,
        "grad_norm": 3.195621967315674,
        "learning_rate": 3.133277127201588e-05,
        "epoch": 0.5416129863437258,
        "step": 4204
    },
    {
        "loss": 1.7047,
        "grad_norm": 2.917940616607666,
        "learning_rate": 3.1276354881090114e-05,
        "epoch": 0.5417418191187838,
        "step": 4205
    },
    {
        "loss": 2.555,
        "grad_norm": 1.7796472311019897,
        "learning_rate": 3.121996620201248e-05,
        "epoch": 0.5418706518938418,
        "step": 4206
    },
    {
        "loss": 2.0161,
        "grad_norm": 2.7035152912139893,
        "learning_rate": 3.1163605318240715e-05,
        "epoch": 0.5419994846688998,
        "step": 4207
    },
    {
        "loss": 2.4511,
        "grad_norm": 1.8104383945465088,
        "learning_rate": 3.110727231319167e-05,
        "epoch": 0.5421283174439577,
        "step": 4208
    },
    {
        "loss": 1.9875,
        "grad_norm": 1.6755094528198242,
        "learning_rate": 3.105096727024067e-05,
        "epoch": 0.5422571502190158,
        "step": 4209
    },
    {
        "loss": 1.727,
        "grad_norm": 3.4373655319213867,
        "learning_rate": 3.0994690272721715e-05,
        "epoch": 0.5423859829940737,
        "step": 4210
    },
    {
        "loss": 2.2518,
        "grad_norm": 2.5552120208740234,
        "learning_rate": 3.093844140392735e-05,
        "epoch": 0.5425148157691316,
        "step": 4211
    },
    {
        "loss": 1.778,
        "grad_norm": 2.7785420417785645,
        "learning_rate": 3.088222074710851e-05,
        "epoch": 0.5426436485441897,
        "step": 4212
    },
    {
        "loss": 0.602,
        "grad_norm": 2.9637227058410645,
        "learning_rate": 3.082602838547429e-05,
        "epoch": 0.5427724813192476,
        "step": 4213
    },
    {
        "loss": 1.9356,
        "grad_norm": 2.704298257827759,
        "learning_rate": 3.0769864402191904e-05,
        "epoch": 0.5429013140943056,
        "step": 4214
    },
    {
        "loss": 0.8197,
        "grad_norm": 2.8884246349334717,
        "learning_rate": 3.071372888038663e-05,
        "epoch": 0.5430301468693636,
        "step": 4215
    },
    {
        "loss": 2.4041,
        "grad_norm": 2.099337100982666,
        "learning_rate": 3.065762190314168e-05,
        "epoch": 0.5431589796444215,
        "step": 4216
    },
    {
        "loss": 2.4341,
        "grad_norm": 1.8110198974609375,
        "learning_rate": 3.060154355349785e-05,
        "epoch": 0.5432878124194795,
        "step": 4217
    },
    {
        "loss": 2.1527,
        "grad_norm": 1.828027367591858,
        "learning_rate": 3.054549391445367e-05,
        "epoch": 0.5434166451945375,
        "step": 4218
    },
    {
        "loss": 2.1018,
        "grad_norm": 2.2114675045013428,
        "learning_rate": 3.0489473068965202e-05,
        "epoch": 0.5435454779695955,
        "step": 4219
    },
    {
        "loss": 1.9452,
        "grad_norm": 2.141282558441162,
        "learning_rate": 3.043348109994582e-05,
        "epoch": 0.5436743107446534,
        "step": 4220
    },
    {
        "loss": 2.3345,
        "grad_norm": 2.9298133850097656,
        "learning_rate": 3.0377518090266154e-05,
        "epoch": 0.5438031435197114,
        "step": 4221
    },
    {
        "loss": 2.2487,
        "grad_norm": 2.6176211833953857,
        "learning_rate": 3.032158412275405e-05,
        "epoch": 0.5439319762947694,
        "step": 4222
    },
    {
        "loss": 2.2269,
        "grad_norm": 1.3317654132843018,
        "learning_rate": 3.0265679280194348e-05,
        "epoch": 0.5440608090698273,
        "step": 4223
    },
    {
        "loss": 1.8194,
        "grad_norm": 2.0327913761138916,
        "learning_rate": 3.020980364532867e-05,
        "epoch": 0.5441896418448854,
        "step": 4224
    },
    {
        "loss": 1.6712,
        "grad_norm": 2.4608421325683594,
        "learning_rate": 3.0153957300855605e-05,
        "epoch": 0.5443184746199433,
        "step": 4225
    },
    {
        "loss": 1.9659,
        "grad_norm": 2.6162502765655518,
        "learning_rate": 3.0098140329430302e-05,
        "epoch": 0.5444473073950012,
        "step": 4226
    },
    {
        "loss": 1.8873,
        "grad_norm": 1.992943525314331,
        "learning_rate": 3.0042352813664353e-05,
        "epoch": 0.5445761401700593,
        "step": 4227
    },
    {
        "loss": 2.4678,
        "grad_norm": 1.2997729778289795,
        "learning_rate": 2.9986594836125815e-05,
        "epoch": 0.5447049729451172,
        "step": 4228
    },
    {
        "loss": 1.7479,
        "grad_norm": 1.7270969152450562,
        "learning_rate": 2.993086647933907e-05,
        "epoch": 0.5448338057201753,
        "step": 4229
    },
    {
        "loss": 1.9224,
        "grad_norm": 1.347291350364685,
        "learning_rate": 2.987516782578465e-05,
        "epoch": 0.5449626384952332,
        "step": 4230
    },
    {
        "loss": 2.3118,
        "grad_norm": 1.7658203840255737,
        "learning_rate": 2.9819498957899073e-05,
        "epoch": 0.5450914712702911,
        "step": 4231
    },
    {
        "loss": 1.9489,
        "grad_norm": 2.581472158432007,
        "learning_rate": 2.9763859958074704e-05,
        "epoch": 0.5452203040453492,
        "step": 4232
    },
    {
        "loss": 2.173,
        "grad_norm": 2.3141627311706543,
        "learning_rate": 2.9708250908659986e-05,
        "epoch": 0.5453491368204071,
        "step": 4233
    },
    {
        "loss": 2.3229,
        "grad_norm": 1.2412304878234863,
        "learning_rate": 2.965267189195877e-05,
        "epoch": 0.5454779695954651,
        "step": 4234
    },
    {
        "loss": 2.1125,
        "grad_norm": 1.7011494636535645,
        "learning_rate": 2.9597122990230476e-05,
        "epoch": 0.5456068023705231,
        "step": 4235
    },
    {
        "loss": 2.222,
        "grad_norm": 3.5517687797546387,
        "learning_rate": 2.954160428569005e-05,
        "epoch": 0.545735635145581,
        "step": 4236
    },
    {
        "loss": 1.9285,
        "grad_norm": 2.483228921890259,
        "learning_rate": 2.9486115860507747e-05,
        "epoch": 0.545864467920639,
        "step": 4237
    },
    {
        "loss": 0.7992,
        "grad_norm": 1.2932839393615723,
        "learning_rate": 2.9430657796808913e-05,
        "epoch": 0.545993300695697,
        "step": 4238
    },
    {
        "loss": 1.8373,
        "grad_norm": 2.0441489219665527,
        "learning_rate": 2.937523017667401e-05,
        "epoch": 0.546122133470755,
        "step": 4239
    },
    {
        "loss": 1.9029,
        "grad_norm": 2.3118958473205566,
        "learning_rate": 2.931983308213844e-05,
        "epoch": 0.5462509662458129,
        "step": 4240
    },
    {
        "loss": 2.3523,
        "grad_norm": 1.7413405179977417,
        "learning_rate": 2.9264466595192452e-05,
        "epoch": 0.5463797990208709,
        "step": 4241
    },
    {
        "loss": 2.2314,
        "grad_norm": 1.8712671995162964,
        "learning_rate": 2.920913079778097e-05,
        "epoch": 0.5465086317959289,
        "step": 4242
    },
    {
        "loss": 0.5799,
        "grad_norm": 3.2125728130340576,
        "learning_rate": 2.9153825771803462e-05,
        "epoch": 0.5466374645709868,
        "step": 4243
    },
    {
        "loss": 1.5959,
        "grad_norm": 2.746572971343994,
        "learning_rate": 2.9098551599113965e-05,
        "epoch": 0.5467662973460449,
        "step": 4244
    },
    {
        "loss": 2.3458,
        "grad_norm": 2.3121209144592285,
        "learning_rate": 2.9043308361520728e-05,
        "epoch": 0.5468951301211028,
        "step": 4245
    },
    {
        "loss": 0.9978,
        "grad_norm": 2.9678092002868652,
        "learning_rate": 2.8988096140786204e-05,
        "epoch": 0.5470239628961607,
        "step": 4246
    },
    {
        "loss": 1.7817,
        "grad_norm": 2.6979589462280273,
        "learning_rate": 2.893291501862705e-05,
        "epoch": 0.5471527956712188,
        "step": 4247
    },
    {
        "loss": 2.1924,
        "grad_norm": 2.1671714782714844,
        "learning_rate": 2.8877765076713857e-05,
        "epoch": 0.5472816284462767,
        "step": 4248
    },
    {
        "loss": 2.2384,
        "grad_norm": 2.046806573867798,
        "learning_rate": 2.882264639667106e-05,
        "epoch": 0.5474104612213347,
        "step": 4249
    },
    {
        "loss": 2.6094,
        "grad_norm": 1.2201166152954102,
        "learning_rate": 2.8767559060076676e-05,
        "epoch": 0.5475392939963927,
        "step": 4250
    },
    {
        "loss": 1.8403,
        "grad_norm": 2.217958450317383,
        "learning_rate": 2.871250314846268e-05,
        "epoch": 0.5476681267714506,
        "step": 4251
    },
    {
        "loss": 2.0945,
        "grad_norm": 1.221456527709961,
        "learning_rate": 2.865747874331426e-05,
        "epoch": 0.5477969595465086,
        "step": 4252
    },
    {
        "loss": 2.0139,
        "grad_norm": 2.545728921890259,
        "learning_rate": 2.8602485926069976e-05,
        "epoch": 0.5479257923215666,
        "step": 4253
    },
    {
        "loss": 1.6782,
        "grad_norm": 3.5905683040618896,
        "learning_rate": 2.8547524778121755e-05,
        "epoch": 0.5480546250966246,
        "step": 4254
    },
    {
        "loss": 2.3903,
        "grad_norm": 1.9150646924972534,
        "learning_rate": 2.8492595380814634e-05,
        "epoch": 0.5481834578716825,
        "step": 4255
    },
    {
        "loss": 1.2996,
        "grad_norm": 3.239271879196167,
        "learning_rate": 2.843769781544657e-05,
        "epoch": 0.5483122906467406,
        "step": 4256
    },
    {
        "loss": 1.9083,
        "grad_norm": 2.8530681133270264,
        "learning_rate": 2.8382832163268453e-05,
        "epoch": 0.5484411234217985,
        "step": 4257
    },
    {
        "loss": 1.705,
        "grad_norm": 2.8037946224212646,
        "learning_rate": 2.8327998505483955e-05,
        "epoch": 0.5485699561968564,
        "step": 4258
    },
    {
        "loss": 2.0608,
        "grad_norm": 2.4012649059295654,
        "learning_rate": 2.8273196923249458e-05,
        "epoch": 0.5486987889719145,
        "step": 4259
    },
    {
        "loss": 1.8353,
        "grad_norm": 1.9158504009246826,
        "learning_rate": 2.8218427497673715e-05,
        "epoch": 0.5488276217469724,
        "step": 4260
    },
    {
        "loss": 1.9764,
        "grad_norm": 1.8513671159744263,
        "learning_rate": 2.8163690309818023e-05,
        "epoch": 0.5489564545220305,
        "step": 4261
    },
    {
        "loss": 1.7608,
        "grad_norm": 2.475557565689087,
        "learning_rate": 2.8108985440695935e-05,
        "epoch": 0.5490852872970884,
        "step": 4262
    },
    {
        "loss": 1.9699,
        "grad_norm": 2.4106345176696777,
        "learning_rate": 2.805431297127311e-05,
        "epoch": 0.5492141200721463,
        "step": 4263
    },
    {
        "loss": 1.6797,
        "grad_norm": 2.832732677459717,
        "learning_rate": 2.7999672982467274e-05,
        "epoch": 0.5493429528472044,
        "step": 4264
    },
    {
        "loss": 1.8161,
        "grad_norm": 2.3360397815704346,
        "learning_rate": 2.7945065555148113e-05,
        "epoch": 0.5494717856222623,
        "step": 4265
    },
    {
        "loss": 1.6148,
        "grad_norm": 2.458266258239746,
        "learning_rate": 2.789049077013715e-05,
        "epoch": 0.5496006183973203,
        "step": 4266
    },
    {
        "loss": 2.2631,
        "grad_norm": 2.790072202682495,
        "learning_rate": 2.7835948708207486e-05,
        "epoch": 0.5497294511723783,
        "step": 4267
    },
    {
        "loss": 2.324,
        "grad_norm": 1.5546233654022217,
        "learning_rate": 2.7781439450083924e-05,
        "epoch": 0.5498582839474362,
        "step": 4268
    },
    {
        "loss": 0.9319,
        "grad_norm": 3.4388070106506348,
        "learning_rate": 2.7726963076442636e-05,
        "epoch": 0.5499871167224942,
        "step": 4269
    },
    {
        "loss": 1.7705,
        "grad_norm": 3.0516200065612793,
        "learning_rate": 2.7672519667911153e-05,
        "epoch": 0.5501159494975522,
        "step": 4270
    },
    {
        "loss": 2.2273,
        "grad_norm": 2.1674704551696777,
        "learning_rate": 2.761810930506813e-05,
        "epoch": 0.5502447822726102,
        "step": 4271
    },
    {
        "loss": 1.8773,
        "grad_norm": 1.9590009450912476,
        "learning_rate": 2.7563732068443436e-05,
        "epoch": 0.5503736150476681,
        "step": 4272
    },
    {
        "loss": 2.3179,
        "grad_norm": 1.6813865900039673,
        "learning_rate": 2.750938803851789e-05,
        "epoch": 0.5505024478227261,
        "step": 4273
    },
    {
        "loss": 1.8228,
        "grad_norm": 2.222899913787842,
        "learning_rate": 2.7455077295723076e-05,
        "epoch": 0.5506312805977841,
        "step": 4274
    },
    {
        "loss": 2.2297,
        "grad_norm": 1.5780607461929321,
        "learning_rate": 2.7400799920441306e-05,
        "epoch": 0.550760113372842,
        "step": 4275
    },
    {
        "loss": 2.2165,
        "grad_norm": 2.040799856185913,
        "learning_rate": 2.734655599300575e-05,
        "epoch": 0.5508889461479001,
        "step": 4276
    },
    {
        "loss": 1.89,
        "grad_norm": 3.0228536128997803,
        "learning_rate": 2.729234559369978e-05,
        "epoch": 0.551017778922958,
        "step": 4277
    },
    {
        "loss": 1.6078,
        "grad_norm": 2.5778424739837646,
        "learning_rate": 2.7238168802757253e-05,
        "epoch": 0.5511466116980159,
        "step": 4278
    },
    {
        "loss": 1.4283,
        "grad_norm": 2.2804489135742188,
        "learning_rate": 2.7184025700362302e-05,
        "epoch": 0.551275444473074,
        "step": 4279
    },
    {
        "loss": 1.4823,
        "grad_norm": 2.8635010719299316,
        "learning_rate": 2.712991636664924e-05,
        "epoch": 0.5514042772481319,
        "step": 4280
    },
    {
        "loss": 2.1272,
        "grad_norm": 1.8269100189208984,
        "learning_rate": 2.7075840881702296e-05,
        "epoch": 0.5515331100231899,
        "step": 4281
    },
    {
        "loss": 1.2099,
        "grad_norm": 3.4928581714630127,
        "learning_rate": 2.702179932555561e-05,
        "epoch": 0.5516619427982479,
        "step": 4282
    },
    {
        "loss": 2.6869,
        "grad_norm": 1.7027841806411743,
        "learning_rate": 2.6967791778193196e-05,
        "epoch": 0.5517907755733058,
        "step": 4283
    },
    {
        "loss": 2.4055,
        "grad_norm": 2.4752254486083984,
        "learning_rate": 2.691381831954869e-05,
        "epoch": 0.5519196083483638,
        "step": 4284
    },
    {
        "loss": 1.4183,
        "grad_norm": 2.8875489234924316,
        "learning_rate": 2.6859879029505264e-05,
        "epoch": 0.5520484411234218,
        "step": 4285
    },
    {
        "loss": 2.3765,
        "grad_norm": 1.3535836935043335,
        "learning_rate": 2.6805973987895516e-05,
        "epoch": 0.5521772738984798,
        "step": 4286
    },
    {
        "loss": 1.9256,
        "grad_norm": 1.6368058919906616,
        "learning_rate": 2.6752103274501415e-05,
        "epoch": 0.5523061066735377,
        "step": 4287
    },
    {
        "loss": 1.1144,
        "grad_norm": 2.950307607650757,
        "learning_rate": 2.669826696905402e-05,
        "epoch": 0.5524349394485957,
        "step": 4288
    },
    {
        "loss": 1.546,
        "grad_norm": 3.68575119972229,
        "learning_rate": 2.6644465151233504e-05,
        "epoch": 0.5525637722236537,
        "step": 4289
    },
    {
        "loss": 2.3625,
        "grad_norm": 2.174349546432495,
        "learning_rate": 2.6590697900669036e-05,
        "epoch": 0.5526926049987116,
        "step": 4290
    },
    {
        "loss": 1.9882,
        "grad_norm": 1.56046462059021,
        "learning_rate": 2.6536965296938632e-05,
        "epoch": 0.5528214377737697,
        "step": 4291
    },
    {
        "loss": 1.7447,
        "grad_norm": 1.5386759042739868,
        "learning_rate": 2.6483267419568874e-05,
        "epoch": 0.5529502705488276,
        "step": 4292
    },
    {
        "loss": 1.5195,
        "grad_norm": 1.8287962675094604,
        "learning_rate": 2.64296043480352e-05,
        "epoch": 0.5530791033238855,
        "step": 4293
    },
    {
        "loss": 2.1855,
        "grad_norm": 2.273735523223877,
        "learning_rate": 2.6375976161761386e-05,
        "epoch": 0.5532079360989436,
        "step": 4294
    },
    {
        "loss": 2.282,
        "grad_norm": 1.8151732683181763,
        "learning_rate": 2.632238294011955e-05,
        "epoch": 0.5533367688740015,
        "step": 4295
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.251283884048462,
        "learning_rate": 2.6268824762430084e-05,
        "epoch": 0.5534656016490596,
        "step": 4296
    },
    {
        "loss": 1.9496,
        "grad_norm": 2.8170485496520996,
        "learning_rate": 2.6215301707961558e-05,
        "epoch": 0.5535944344241175,
        "step": 4297
    },
    {
        "loss": 2.3524,
        "grad_norm": 1.3633959293365479,
        "learning_rate": 2.616181385593055e-05,
        "epoch": 0.5537232671991754,
        "step": 4298
    },
    {
        "loss": 2.1236,
        "grad_norm": 2.3868961334228516,
        "learning_rate": 2.6108361285501527e-05,
        "epoch": 0.5538520999742335,
        "step": 4299
    },
    {
        "loss": 1.4943,
        "grad_norm": 3.2670609951019287,
        "learning_rate": 2.6054944075786657e-05,
        "epoch": 0.5539809327492914,
        "step": 4300
    },
    {
        "loss": 1.9493,
        "grad_norm": 1.5454879999160767,
        "learning_rate": 2.6001562305845855e-05,
        "epoch": 0.5541097655243494,
        "step": 4301
    },
    {
        "loss": 2.4143,
        "grad_norm": 1.6383863687515259,
        "learning_rate": 2.594821605468669e-05,
        "epoch": 0.5542385982994074,
        "step": 4302
    },
    {
        "loss": 1.9582,
        "grad_norm": 2.612102508544922,
        "learning_rate": 2.5894905401263934e-05,
        "epoch": 0.5543674310744653,
        "step": 4303
    },
    {
        "loss": 2.4537,
        "grad_norm": 2.3263397216796875,
        "learning_rate": 2.5841630424479813e-05,
        "epoch": 0.5544962638495233,
        "step": 4304
    },
    {
        "loss": 1.5746,
        "grad_norm": 2.0603132247924805,
        "learning_rate": 2.578839120318376e-05,
        "epoch": 0.5546250966245813,
        "step": 4305
    },
    {
        "loss": 1.7514,
        "grad_norm": 1.917104959487915,
        "learning_rate": 2.573518781617219e-05,
        "epoch": 0.5547539293996393,
        "step": 4306
    },
    {
        "loss": 1.89,
        "grad_norm": 1.914339303970337,
        "learning_rate": 2.5682020342188545e-05,
        "epoch": 0.5548827621746972,
        "step": 4307
    },
    {
        "loss": 1.6277,
        "grad_norm": 2.18961501121521,
        "learning_rate": 2.5628888859923127e-05,
        "epoch": 0.5550115949497553,
        "step": 4308
    },
    {
        "loss": 1.3751,
        "grad_norm": 3.5830960273742676,
        "learning_rate": 2.557579344801296e-05,
        "epoch": 0.5551404277248132,
        "step": 4309
    },
    {
        "loss": 2.4131,
        "grad_norm": 1.1875708103179932,
        "learning_rate": 2.5522734185041657e-05,
        "epoch": 0.5552692604998711,
        "step": 4310
    },
    {
        "loss": 1.9048,
        "grad_norm": 2.360607385635376,
        "learning_rate": 2.5469711149539362e-05,
        "epoch": 0.5553980932749292,
        "step": 4311
    },
    {
        "loss": 2.0489,
        "grad_norm": 2.6990368366241455,
        "learning_rate": 2.5416724419982607e-05,
        "epoch": 0.5555269260499871,
        "step": 4312
    },
    {
        "loss": 2.3807,
        "grad_norm": 1.2478538751602173,
        "learning_rate": 2.5363774074794146e-05,
        "epoch": 0.5556557588250451,
        "step": 4313
    },
    {
        "loss": 1.9755,
        "grad_norm": 2.355621337890625,
        "learning_rate": 2.5310860192342845e-05,
        "epoch": 0.5557845916001031,
        "step": 4314
    },
    {
        "loss": 2.397,
        "grad_norm": 2.0355935096740723,
        "learning_rate": 2.5257982850943714e-05,
        "epoch": 0.555913424375161,
        "step": 4315
    },
    {
        "loss": 2.2651,
        "grad_norm": 1.9592267274856567,
        "learning_rate": 2.520514212885764e-05,
        "epoch": 0.556042257150219,
        "step": 4316
    },
    {
        "loss": 2.0977,
        "grad_norm": 2.080785036087036,
        "learning_rate": 2.5152338104291308e-05,
        "epoch": 0.556171089925277,
        "step": 4317
    },
    {
        "loss": 2.1979,
        "grad_norm": 2.142601251602173,
        "learning_rate": 2.5099570855396958e-05,
        "epoch": 0.556299922700335,
        "step": 4318
    },
    {
        "loss": 2.1579,
        "grad_norm": 1.9568171501159668,
        "learning_rate": 2.50468404602727e-05,
        "epoch": 0.5564287554753929,
        "step": 4319
    },
    {
        "loss": 2.1754,
        "grad_norm": 1.628585696220398,
        "learning_rate": 2.4994146996961866e-05,
        "epoch": 0.5565575882504509,
        "step": 4320
    },
    {
        "loss": 1.5865,
        "grad_norm": 2.2578768730163574,
        "learning_rate": 2.494149054345312e-05,
        "epoch": 0.5566864210255089,
        "step": 4321
    },
    {
        "loss": 1.8351,
        "grad_norm": 2.709317445755005,
        "learning_rate": 2.4888871177680457e-05,
        "epoch": 0.5568152538005668,
        "step": 4322
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.017313241958618,
        "learning_rate": 2.483628897752297e-05,
        "epoch": 0.5569440865756249,
        "step": 4323
    },
    {
        "loss": 2.4188,
        "grad_norm": 1.605434536933899,
        "learning_rate": 2.4783744020804662e-05,
        "epoch": 0.5570729193506828,
        "step": 4324
    },
    {
        "loss": 1.9857,
        "grad_norm": 2.2521886825561523,
        "learning_rate": 2.4731236385294454e-05,
        "epoch": 0.5572017521257407,
        "step": 4325
    },
    {
        "loss": 2.3742,
        "grad_norm": 1.8881945610046387,
        "learning_rate": 2.467876614870607e-05,
        "epoch": 0.5573305849007988,
        "step": 4326
    },
    {
        "loss": 1.8277,
        "grad_norm": 1.8377269506454468,
        "learning_rate": 2.4626333388697838e-05,
        "epoch": 0.5574594176758567,
        "step": 4327
    },
    {
        "loss": 2.0915,
        "grad_norm": 2.4581193923950195,
        "learning_rate": 2.4573938182872647e-05,
        "epoch": 0.5575882504509148,
        "step": 4328
    },
    {
        "loss": 2.1759,
        "grad_norm": 1.6544164419174194,
        "learning_rate": 2.4521580608777776e-05,
        "epoch": 0.5577170832259727,
        "step": 4329
    },
    {
        "loss": 2.4272,
        "grad_norm": 2.409897565841675,
        "learning_rate": 2.446926074390487e-05,
        "epoch": 0.5578459160010306,
        "step": 4330
    },
    {
        "loss": 2.0137,
        "grad_norm": 3.2771103382110596,
        "learning_rate": 2.4416978665689665e-05,
        "epoch": 0.5579747487760887,
        "step": 4331
    },
    {
        "loss": 1.6787,
        "grad_norm": 1.8907396793365479,
        "learning_rate": 2.4364734451511994e-05,
        "epoch": 0.5581035815511466,
        "step": 4332
    },
    {
        "loss": 2.0222,
        "grad_norm": 3.4501569271087646,
        "learning_rate": 2.4312528178695703e-05,
        "epoch": 0.5582324143262046,
        "step": 4333
    },
    {
        "loss": 2.5889,
        "grad_norm": 1.686996579170227,
        "learning_rate": 2.426035992450848e-05,
        "epoch": 0.5583612471012626,
        "step": 4334
    },
    {
        "loss": 1.7186,
        "grad_norm": 2.695263147354126,
        "learning_rate": 2.4208229766161663e-05,
        "epoch": 0.5584900798763205,
        "step": 4335
    },
    {
        "loss": 2.5266,
        "grad_norm": 2.5340042114257812,
        "learning_rate": 2.4156137780810307e-05,
        "epoch": 0.5586189126513785,
        "step": 4336
    },
    {
        "loss": 1.9988,
        "grad_norm": 1.8859946727752686,
        "learning_rate": 2.4104084045552956e-05,
        "epoch": 0.5587477454264365,
        "step": 4337
    },
    {
        "loss": 2.3908,
        "grad_norm": 1.5193730592727661,
        "learning_rate": 2.405206863743145e-05,
        "epoch": 0.5588765782014945,
        "step": 4338
    },
    {
        "loss": 2.2607,
        "grad_norm": 1.3649299144744873,
        "learning_rate": 2.4000091633430967e-05,
        "epoch": 0.5590054109765524,
        "step": 4339
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.28242826461792,
        "learning_rate": 2.3948153110479853e-05,
        "epoch": 0.5591342437516104,
        "step": 4340
    },
    {
        "loss": 2.2494,
        "grad_norm": 1.9143010377883911,
        "learning_rate": 2.3896253145449538e-05,
        "epoch": 0.5592630765266684,
        "step": 4341
    },
    {
        "loss": 2.4387,
        "grad_norm": 1.7834488153457642,
        "learning_rate": 2.384439181515427e-05,
        "epoch": 0.5593919093017263,
        "step": 4342
    },
    {
        "loss": 2.3196,
        "grad_norm": 1.7976692914962769,
        "learning_rate": 2.3792569196351184e-05,
        "epoch": 0.5595207420767844,
        "step": 4343
    },
    {
        "loss": 2.1372,
        "grad_norm": 1.1337764263153076,
        "learning_rate": 2.3740785365740116e-05,
        "epoch": 0.5596495748518423,
        "step": 4344
    },
    {
        "loss": 2.1141,
        "grad_norm": 1.7562298774719238,
        "learning_rate": 2.3689040399963573e-05,
        "epoch": 0.5597784076269002,
        "step": 4345
    },
    {
        "loss": 1.8856,
        "grad_norm": 1.62247633934021,
        "learning_rate": 2.3637334375606367e-05,
        "epoch": 0.5599072404019583,
        "step": 4346
    },
    {
        "loss": 2.2887,
        "grad_norm": 1.8294061422348022,
        "learning_rate": 2.3585667369195814e-05,
        "epoch": 0.5600360731770162,
        "step": 4347
    },
    {
        "loss": 1.9966,
        "grad_norm": 1.5284758806228638,
        "learning_rate": 2.353403945720145e-05,
        "epoch": 0.5601649059520742,
        "step": 4348
    },
    {
        "loss": 1.9242,
        "grad_norm": 1.681111216545105,
        "learning_rate": 2.348245071603494e-05,
        "epoch": 0.5602937387271322,
        "step": 4349
    },
    {
        "loss": 2.2115,
        "grad_norm": 2.7471446990966797,
        "learning_rate": 2.3430901222049916e-05,
        "epoch": 0.5604225715021901,
        "step": 4350
    },
    {
        "loss": 2.0079,
        "grad_norm": 2.0720162391662598,
        "learning_rate": 2.3379391051542016e-05,
        "epoch": 0.5605514042772481,
        "step": 4351
    },
    {
        "loss": 2.2534,
        "grad_norm": 1.7016500234603882,
        "learning_rate": 2.3327920280748646e-05,
        "epoch": 0.5606802370523061,
        "step": 4352
    },
    {
        "loss": 2.3159,
        "grad_norm": 2.2085177898406982,
        "learning_rate": 2.3276488985848878e-05,
        "epoch": 0.5608090698273641,
        "step": 4353
    },
    {
        "loss": 2.1427,
        "grad_norm": 2.8123152256011963,
        "learning_rate": 2.322509724296339e-05,
        "epoch": 0.560937902602422,
        "step": 4354
    },
    {
        "loss": 2.1989,
        "grad_norm": 2.2014856338500977,
        "learning_rate": 2.3173745128154328e-05,
        "epoch": 0.5610667353774801,
        "step": 4355
    },
    {
        "loss": 1.2341,
        "grad_norm": 3.2073557376861572,
        "learning_rate": 2.3122432717425107e-05,
        "epoch": 0.561195568152538,
        "step": 4356
    },
    {
        "loss": 1.4132,
        "grad_norm": 2.9300479888916016,
        "learning_rate": 2.3071160086720444e-05,
        "epoch": 0.561324400927596,
        "step": 4357
    },
    {
        "loss": 2.2747,
        "grad_norm": 1.7845127582550049,
        "learning_rate": 2.301992731192618e-05,
        "epoch": 0.561453233702654,
        "step": 4358
    },
    {
        "loss": 2.1325,
        "grad_norm": 2.323782205581665,
        "learning_rate": 2.2968734468869195e-05,
        "epoch": 0.5615820664777119,
        "step": 4359
    },
    {
        "loss": 1.5519,
        "grad_norm": 2.625734329223633,
        "learning_rate": 2.2917581633317194e-05,
        "epoch": 0.56171089925277,
        "step": 4360
    },
    {
        "loss": 2.2936,
        "grad_norm": 2.465670347213745,
        "learning_rate": 2.2866468880978593e-05,
        "epoch": 0.5618397320278279,
        "step": 4361
    },
    {
        "loss": 1.9281,
        "grad_norm": 2.465132474899292,
        "learning_rate": 2.2815396287502817e-05,
        "epoch": 0.5619685648028858,
        "step": 4362
    },
    {
        "loss": 1.9357,
        "grad_norm": 1.872986912727356,
        "learning_rate": 2.2764363928479516e-05,
        "epoch": 0.5620973975779439,
        "step": 4363
    },
    {
        "loss": 2.132,
        "grad_norm": 1.772826075553894,
        "learning_rate": 2.2713371879438882e-05,
        "epoch": 0.5622262303530018,
        "step": 4364
    },
    {
        "loss": 1.6336,
        "grad_norm": 2.138155221939087,
        "learning_rate": 2.26624202158515e-05,
        "epoch": 0.5623550631280598,
        "step": 4365
    },
    {
        "loss": 2.0681,
        "grad_norm": 1.905314326286316,
        "learning_rate": 2.2611509013128174e-05,
        "epoch": 0.5624838959031178,
        "step": 4366
    },
    {
        "loss": 2.1714,
        "grad_norm": 1.4211043119430542,
        "learning_rate": 2.2560638346619807e-05,
        "epoch": 0.5626127286781757,
        "step": 4367
    },
    {
        "loss": 2.1016,
        "grad_norm": 2.3251430988311768,
        "learning_rate": 2.2509808291617246e-05,
        "epoch": 0.5627415614532337,
        "step": 4368
    },
    {
        "loss": 2.1326,
        "grad_norm": 2.0205202102661133,
        "learning_rate": 2.245901892335129e-05,
        "epoch": 0.5628703942282917,
        "step": 4369
    },
    {
        "loss": 2.3413,
        "grad_norm": 2.624546527862549,
        "learning_rate": 2.240827031699263e-05,
        "epoch": 0.5629992270033497,
        "step": 4370
    },
    {
        "loss": 1.6476,
        "grad_norm": 2.33730149269104,
        "learning_rate": 2.2357562547651402e-05,
        "epoch": 0.5631280597784076,
        "step": 4371
    },
    {
        "loss": 2.4381,
        "grad_norm": 1.7092506885528564,
        "learning_rate": 2.2306895690377454e-05,
        "epoch": 0.5632568925534656,
        "step": 4372
    },
    {
        "loss": 2.0101,
        "grad_norm": 2.059006452560425,
        "learning_rate": 2.2256269820160093e-05,
        "epoch": 0.5633857253285236,
        "step": 4373
    },
    {
        "loss": 0.9811,
        "grad_norm": 2.4867537021636963,
        "learning_rate": 2.220568501192784e-05,
        "epoch": 0.5635145581035815,
        "step": 4374
    },
    {
        "loss": 1.7523,
        "grad_norm": 2.025838851928711,
        "learning_rate": 2.2155141340548525e-05,
        "epoch": 0.5636433908786396,
        "step": 4375
    },
    {
        "loss": 1.6983,
        "grad_norm": 3.068713903427124,
        "learning_rate": 2.210463888082911e-05,
        "epoch": 0.5637722236536975,
        "step": 4376
    },
    {
        "loss": 1.9844,
        "grad_norm": 3.14654803276062,
        "learning_rate": 2.2054177707515562e-05,
        "epoch": 0.5639010564287554,
        "step": 4377
    },
    {
        "loss": 1.8769,
        "grad_norm": 2.101109027862549,
        "learning_rate": 2.2003757895292616e-05,
        "epoch": 0.5640298892038135,
        "step": 4378
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.2787206172943115,
        "learning_rate": 2.195337951878402e-05,
        "epoch": 0.5641587219788714,
        "step": 4379
    },
    {
        "loss": 1.5788,
        "grad_norm": 1.85578453540802,
        "learning_rate": 2.1903042652552002e-05,
        "epoch": 0.5642875547539294,
        "step": 4380
    },
    {
        "loss": 2.3613,
        "grad_norm": 1.740525245666504,
        "learning_rate": 2.1852747371097447e-05,
        "epoch": 0.5644163875289874,
        "step": 4381
    },
    {
        "loss": 2.1925,
        "grad_norm": 2.1167144775390625,
        "learning_rate": 2.180249374885961e-05,
        "epoch": 0.5645452203040453,
        "step": 4382
    },
    {
        "loss": 2.257,
        "grad_norm": 1.204880952835083,
        "learning_rate": 2.175228186021617e-05,
        "epoch": 0.5646740530791033,
        "step": 4383
    },
    {
        "loss": 1.7757,
        "grad_norm": 2.2720117568969727,
        "learning_rate": 2.1702111779483042e-05,
        "epoch": 0.5648028858541613,
        "step": 4384
    },
    {
        "loss": 1.6684,
        "grad_norm": 2.429187774658203,
        "learning_rate": 2.1651983580914222e-05,
        "epoch": 0.5649317186292193,
        "step": 4385
    },
    {
        "loss": 2.2768,
        "grad_norm": 1.544911503791809,
        "learning_rate": 2.1601897338701617e-05,
        "epoch": 0.5650605514042772,
        "step": 4386
    },
    {
        "loss": 2.003,
        "grad_norm": 2.261458396911621,
        "learning_rate": 2.1551853126975335e-05,
        "epoch": 0.5651893841793352,
        "step": 4387
    },
    {
        "loss": 2.5258,
        "grad_norm": 1.4770596027374268,
        "learning_rate": 2.150185101980298e-05,
        "epoch": 0.5653182169543932,
        "step": 4388
    },
    {
        "loss": 2.4134,
        "grad_norm": 1.7735034227371216,
        "learning_rate": 2.1451891091189957e-05,
        "epoch": 0.5654470497294511,
        "step": 4389
    },
    {
        "loss": 1.4386,
        "grad_norm": 3.5857467651367188,
        "learning_rate": 2.1401973415079245e-05,
        "epoch": 0.5655758825045092,
        "step": 4390
    },
    {
        "loss": 2.2355,
        "grad_norm": 2.2270328998565674,
        "learning_rate": 2.1352098065351316e-05,
        "epoch": 0.5657047152795671,
        "step": 4391
    },
    {
        "loss": 1.1701,
        "grad_norm": 3.465327262878418,
        "learning_rate": 2.1302265115823917e-05,
        "epoch": 0.565833548054625,
        "step": 4392
    },
    {
        "loss": 2.4645,
        "grad_norm": 1.8254603147506714,
        "learning_rate": 2.1252474640252075e-05,
        "epoch": 0.5659623808296831,
        "step": 4393
    },
    {
        "loss": 1.7593,
        "grad_norm": 2.0366971492767334,
        "learning_rate": 2.1202726712327987e-05,
        "epoch": 0.566091213604741,
        "step": 4394
    },
    {
        "loss": 1.7953,
        "grad_norm": 2.0521039962768555,
        "learning_rate": 2.1153021405680863e-05,
        "epoch": 0.5662200463797991,
        "step": 4395
    },
    {
        "loss": 1.9164,
        "grad_norm": 2.66025710105896,
        "learning_rate": 2.1103358793876792e-05,
        "epoch": 0.566348879154857,
        "step": 4396
    },
    {
        "loss": 2.1166,
        "grad_norm": 1.2526756525039673,
        "learning_rate": 2.105373895041872e-05,
        "epoch": 0.5664777119299149,
        "step": 4397
    },
    {
        "loss": 2.1003,
        "grad_norm": 1.627601981163025,
        "learning_rate": 2.1004161948746305e-05,
        "epoch": 0.566606544704973,
        "step": 4398
    },
    {
        "loss": 1.7622,
        "grad_norm": 2.1555778980255127,
        "learning_rate": 2.0954627862235754e-05,
        "epoch": 0.5667353774800309,
        "step": 4399
    },
    {
        "loss": 1.6779,
        "grad_norm": 1.7899940013885498,
        "learning_rate": 2.0905136764199724e-05,
        "epoch": 0.5668642102550889,
        "step": 4400
    },
    {
        "loss": 2.057,
        "grad_norm": 2.642047882080078,
        "learning_rate": 2.085568872788733e-05,
        "epoch": 0.5669930430301469,
        "step": 4401
    },
    {
        "loss": 1.3667,
        "grad_norm": 3.1541645526885986,
        "learning_rate": 2.0806283826483943e-05,
        "epoch": 0.5671218758052048,
        "step": 4402
    },
    {
        "loss": 2.5436,
        "grad_norm": 1.3433001041412354,
        "learning_rate": 2.0756922133111063e-05,
        "epoch": 0.5672507085802628,
        "step": 4403
    },
    {
        "loss": 1.9664,
        "grad_norm": 2.005730628967285,
        "learning_rate": 2.0707603720826142e-05,
        "epoch": 0.5673795413553208,
        "step": 4404
    },
    {
        "loss": 2.394,
        "grad_norm": 2.607083797454834,
        "learning_rate": 2.065832866262285e-05,
        "epoch": 0.5675083741303788,
        "step": 4405
    },
    {
        "loss": 2.3024,
        "grad_norm": 2.0955262184143066,
        "learning_rate": 2.0609097031430425e-05,
        "epoch": 0.5676372069054367,
        "step": 4406
    },
    {
        "loss": 2.1272,
        "grad_norm": 2.2330222129821777,
        "learning_rate": 2.055990890011391e-05,
        "epoch": 0.5677660396804948,
        "step": 4407
    },
    {
        "loss": 1.606,
        "grad_norm": 2.8729865550994873,
        "learning_rate": 2.0510764341474016e-05,
        "epoch": 0.5678948724555527,
        "step": 4408
    },
    {
        "loss": 1.1509,
        "grad_norm": 3.2549667358398438,
        "learning_rate": 2.0461663428246946e-05,
        "epoch": 0.5680237052306106,
        "step": 4409
    },
    {
        "loss": 1.9309,
        "grad_norm": 2.155700206756592,
        "learning_rate": 2.041260623310427e-05,
        "epoch": 0.5681525380056687,
        "step": 4410
    },
    {
        "loss": 2.6852,
        "grad_norm": 2.0978009700775146,
        "learning_rate": 2.0363592828652828e-05,
        "epoch": 0.5682813707807266,
        "step": 4411
    },
    {
        "loss": 2.1285,
        "grad_norm": 1.93950617313385,
        "learning_rate": 2.0314623287434698e-05,
        "epoch": 0.5684102035557846,
        "step": 4412
    },
    {
        "loss": 1.8783,
        "grad_norm": 2.698066473007202,
        "learning_rate": 2.026569768192713e-05,
        "epoch": 0.5685390363308426,
        "step": 4413
    },
    {
        "loss": 1.5622,
        "grad_norm": 2.596806287765503,
        "learning_rate": 2.0216816084542155e-05,
        "epoch": 0.5686678691059005,
        "step": 4414
    },
    {
        "loss": 2.0524,
        "grad_norm": 2.3011951446533203,
        "learning_rate": 2.0167978567626794e-05,
        "epoch": 0.5687967018809585,
        "step": 4415
    },
    {
        "loss": 1.8604,
        "grad_norm": 2.7242319583892822,
        "learning_rate": 2.0119185203462814e-05,
        "epoch": 0.5689255346560165,
        "step": 4416
    },
    {
        "loss": 1.799,
        "grad_norm": 2.3214259147644043,
        "learning_rate": 2.0070436064266624e-05,
        "epoch": 0.5690543674310745,
        "step": 4417
    },
    {
        "loss": 1.5813,
        "grad_norm": 2.4787092208862305,
        "learning_rate": 2.0021731222189115e-05,
        "epoch": 0.5691832002061324,
        "step": 4418
    },
    {
        "loss": 1.6754,
        "grad_norm": 3.304647445678711,
        "learning_rate": 1.9973070749315708e-05,
        "epoch": 0.5693120329811904,
        "step": 4419
    },
    {
        "loss": 2.8875,
        "grad_norm": 2.227548360824585,
        "learning_rate": 1.9924454717666157e-05,
        "epoch": 0.5694408657562484,
        "step": 4420
    },
    {
        "loss": 2.1303,
        "grad_norm": 1.5044376850128174,
        "learning_rate": 1.98758831991943e-05,
        "epoch": 0.5695696985313063,
        "step": 4421
    },
    {
        "loss": 2.0796,
        "grad_norm": 1.6679738759994507,
        "learning_rate": 1.9827356265788306e-05,
        "epoch": 0.5696985313063644,
        "step": 4422
    },
    {
        "loss": 2.5794,
        "grad_norm": 1.836583137512207,
        "learning_rate": 1.977887398927024e-05,
        "epoch": 0.5698273640814223,
        "step": 4423
    },
    {
        "loss": 1.7276,
        "grad_norm": 2.4483492374420166,
        "learning_rate": 1.9730436441396033e-05,
        "epoch": 0.5699561968564802,
        "step": 4424
    },
    {
        "loss": 1.5325,
        "grad_norm": 4.166105270385742,
        "learning_rate": 1.968204369385547e-05,
        "epoch": 0.5700850296315383,
        "step": 4425
    },
    {
        "loss": 1.5696,
        "grad_norm": 2.6869680881500244,
        "learning_rate": 1.9633695818272026e-05,
        "epoch": 0.5702138624065962,
        "step": 4426
    },
    {
        "loss": 2.2601,
        "grad_norm": 2.3591291904449463,
        "learning_rate": 1.9585392886202802e-05,
        "epoch": 0.5703426951816543,
        "step": 4427
    },
    {
        "loss": 2.5003,
        "grad_norm": 2.1557180881500244,
        "learning_rate": 1.953713496913831e-05,
        "epoch": 0.5704715279567122,
        "step": 4428
    },
    {
        "loss": 1.8494,
        "grad_norm": 2.1714284420013428,
        "learning_rate": 1.9488922138502368e-05,
        "epoch": 0.5706003607317701,
        "step": 4429
    },
    {
        "loss": 2.1985,
        "grad_norm": 1.3767163753509521,
        "learning_rate": 1.9440754465652334e-05,
        "epoch": 0.5707291935068282,
        "step": 4430
    },
    {
        "loss": 1.7558,
        "grad_norm": 2.6787517070770264,
        "learning_rate": 1.9392632021878497e-05,
        "epoch": 0.5708580262818861,
        "step": 4431
    },
    {
        "loss": 2.1055,
        "grad_norm": 2.2028870582580566,
        "learning_rate": 1.9344554878404214e-05,
        "epoch": 0.5709868590569441,
        "step": 4432
    },
    {
        "loss": 2.3317,
        "grad_norm": 1.5193710327148438,
        "learning_rate": 1.9296523106385888e-05,
        "epoch": 0.5711156918320021,
        "step": 4433
    },
    {
        "loss": 1.8406,
        "grad_norm": 2.900721788406372,
        "learning_rate": 1.9248536776912747e-05,
        "epoch": 0.57124452460706,
        "step": 4434
    },
    {
        "loss": 2.1861,
        "grad_norm": 1.8992242813110352,
        "learning_rate": 1.9200595961006746e-05,
        "epoch": 0.571373357382118,
        "step": 4435
    },
    {
        "loss": 1.8789,
        "grad_norm": 2.769948720932007,
        "learning_rate": 1.9152700729622424e-05,
        "epoch": 0.571502190157176,
        "step": 4436
    },
    {
        "loss": 1.8056,
        "grad_norm": 2.695610761642456,
        "learning_rate": 1.9104851153646942e-05,
        "epoch": 0.571631022932234,
        "step": 4437
    },
    {
        "loss": 1.918,
        "grad_norm": 1.5552233457565308,
        "learning_rate": 1.905704730389985e-05,
        "epoch": 0.5717598557072919,
        "step": 4438
    },
    {
        "loss": 1.974,
        "grad_norm": 2.4739270210266113,
        "learning_rate": 1.900928925113303e-05,
        "epoch": 0.5718886884823499,
        "step": 4439
    },
    {
        "loss": 2.2035,
        "grad_norm": 3.242945432662964,
        "learning_rate": 1.8961577066030574e-05,
        "epoch": 0.5720175212574079,
        "step": 4440
    },
    {
        "loss": 1.7585,
        "grad_norm": 2.832932233810425,
        "learning_rate": 1.8913910819208724e-05,
        "epoch": 0.5721463540324658,
        "step": 4441
    },
    {
        "loss": 1.3274,
        "grad_norm": 2.8990304470062256,
        "learning_rate": 1.8866290581215647e-05,
        "epoch": 0.5722751868075239,
        "step": 4442
    },
    {
        "loss": 1.6149,
        "grad_norm": 2.6186165809631348,
        "learning_rate": 1.8818716422531463e-05,
        "epoch": 0.5724040195825818,
        "step": 4443
    },
    {
        "loss": 2.0287,
        "grad_norm": 2.0449392795562744,
        "learning_rate": 1.877118841356812e-05,
        "epoch": 0.5725328523576397,
        "step": 4444
    },
    {
        "loss": 2.1281,
        "grad_norm": 2.7778682708740234,
        "learning_rate": 1.8723706624669263e-05,
        "epoch": 0.5726616851326978,
        "step": 4445
    },
    {
        "loss": 1.9766,
        "grad_norm": 2.155965566635132,
        "learning_rate": 1.8676271126110023e-05,
        "epoch": 0.5727905179077557,
        "step": 4446
    },
    {
        "loss": 2.0475,
        "grad_norm": 1.3805798292160034,
        "learning_rate": 1.862888198809717e-05,
        "epoch": 0.5729193506828137,
        "step": 4447
    },
    {
        "loss": 2.1301,
        "grad_norm": 2.2439470291137695,
        "learning_rate": 1.8581539280768777e-05,
        "epoch": 0.5730481834578717,
        "step": 4448
    },
    {
        "loss": 2.1793,
        "grad_norm": 1.5725785493850708,
        "learning_rate": 1.8534243074194225e-05,
        "epoch": 0.5731770162329296,
        "step": 4449
    },
    {
        "loss": 2.3765,
        "grad_norm": 1.4551684856414795,
        "learning_rate": 1.8486993438373996e-05,
        "epoch": 0.5733058490079876,
        "step": 4450
    },
    {
        "loss": 2.2173,
        "grad_norm": 2.8075263500213623,
        "learning_rate": 1.843979044323973e-05,
        "epoch": 0.5734346817830456,
        "step": 4451
    },
    {
        "loss": 1.8162,
        "grad_norm": 2.3257291316986084,
        "learning_rate": 1.839263415865406e-05,
        "epoch": 0.5735635145581036,
        "step": 4452
    },
    {
        "loss": 2.514,
        "grad_norm": 1.6807280778884888,
        "learning_rate": 1.834552465441038e-05,
        "epoch": 0.5736923473331615,
        "step": 4453
    },
    {
        "loss": 1.9598,
        "grad_norm": 2.2794415950775146,
        "learning_rate": 1.8298462000232897e-05,
        "epoch": 0.5738211801082196,
        "step": 4454
    },
    {
        "loss": 2.2082,
        "grad_norm": 2.429226875305176,
        "learning_rate": 1.8251446265776494e-05,
        "epoch": 0.5739500128832775,
        "step": 4455
    },
    {
        "loss": 2.1013,
        "grad_norm": 2.1380035877227783,
        "learning_rate": 1.8204477520626645e-05,
        "epoch": 0.5740788456583354,
        "step": 4456
    },
    {
        "loss": 2.0289,
        "grad_norm": 2.9026803970336914,
        "learning_rate": 1.8157555834299183e-05,
        "epoch": 0.5742076784333935,
        "step": 4457
    },
    {
        "loss": 1.8529,
        "grad_norm": 2.751213312149048,
        "learning_rate": 1.8110681276240365e-05,
        "epoch": 0.5743365112084514,
        "step": 4458
    },
    {
        "loss": 1.9184,
        "grad_norm": 2.3974616527557373,
        "learning_rate": 1.8063853915826706e-05,
        "epoch": 0.5744653439835095,
        "step": 4459
    },
    {
        "loss": 2.057,
        "grad_norm": 1.1660280227661133,
        "learning_rate": 1.801707382236479e-05,
        "epoch": 0.5745941767585674,
        "step": 4460
    },
    {
        "loss": 2.2775,
        "grad_norm": 1.7282906770706177,
        "learning_rate": 1.797034106509127e-05,
        "epoch": 0.5747230095336253,
        "step": 4461
    },
    {
        "loss": 1.9764,
        "grad_norm": 2.888810634613037,
        "learning_rate": 1.7923655713172772e-05,
        "epoch": 0.5748518423086834,
        "step": 4462
    },
    {
        "loss": 1.7723,
        "grad_norm": 1.4876759052276611,
        "learning_rate": 1.7877017835705774e-05,
        "epoch": 0.5749806750837413,
        "step": 4463
    },
    {
        "loss": 2.3063,
        "grad_norm": 1.109479546546936,
        "learning_rate": 1.78304275017164e-05,
        "epoch": 0.5751095078587993,
        "step": 4464
    },
    {
        "loss": 2.0339,
        "grad_norm": 1.4807707071304321,
        "learning_rate": 1.778388478016053e-05,
        "epoch": 0.5752383406338573,
        "step": 4465
    },
    {
        "loss": 1.901,
        "grad_norm": 1.4355549812316895,
        "learning_rate": 1.773738973992351e-05,
        "epoch": 0.5753671734089152,
        "step": 4466
    },
    {
        "loss": 2.0296,
        "grad_norm": 1.492990493774414,
        "learning_rate": 1.7690942449820118e-05,
        "epoch": 0.5754960061839732,
        "step": 4467
    },
    {
        "loss": 1.9874,
        "grad_norm": 2.4699251651763916,
        "learning_rate": 1.76445429785944e-05,
        "epoch": 0.5756248389590312,
        "step": 4468
    },
    {
        "loss": 2.2281,
        "grad_norm": 1.9326767921447754,
        "learning_rate": 1.759819139491974e-05,
        "epoch": 0.5757536717340892,
        "step": 4469
    },
    {
        "loss": 1.8706,
        "grad_norm": 2.748032569885254,
        "learning_rate": 1.755188776739861e-05,
        "epoch": 0.5758825045091471,
        "step": 4470
    },
    {
        "loss": 1.8275,
        "grad_norm": 3.6458826065063477,
        "learning_rate": 1.7505632164562454e-05,
        "epoch": 0.5760113372842051,
        "step": 4471
    },
    {
        "loss": 2.2067,
        "grad_norm": 2.343459129333496,
        "learning_rate": 1.745942465487161e-05,
        "epoch": 0.5761401700592631,
        "step": 4472
    },
    {
        "loss": 0.7364,
        "grad_norm": 3.3252761363983154,
        "learning_rate": 1.7413265306715454e-05,
        "epoch": 0.576269002834321,
        "step": 4473
    },
    {
        "loss": 2.157,
        "grad_norm": 1.7862985134124756,
        "learning_rate": 1.7367154188411826e-05,
        "epoch": 0.5763978356093791,
        "step": 4474
    },
    {
        "loss": 1.8141,
        "grad_norm": 2.2475602626800537,
        "learning_rate": 1.732109136820728e-05,
        "epoch": 0.576526668384437,
        "step": 4475
    },
    {
        "loss": 2.0994,
        "grad_norm": 1.9969866275787354,
        "learning_rate": 1.7275076914276905e-05,
        "epoch": 0.5766555011594949,
        "step": 4476
    },
    {
        "loss": 1.9271,
        "grad_norm": 2.4810659885406494,
        "learning_rate": 1.722911089472423e-05,
        "epoch": 0.576784333934553,
        "step": 4477
    },
    {
        "loss": 2.0501,
        "grad_norm": 1.7409272193908691,
        "learning_rate": 1.718319337758102e-05,
        "epoch": 0.5769131667096109,
        "step": 4478
    },
    {
        "loss": 2.0235,
        "grad_norm": 2.0078208446502686,
        "learning_rate": 1.713732443080726e-05,
        "epoch": 0.5770419994846689,
        "step": 4479
    },
    {
        "loss": 2.0053,
        "grad_norm": 1.8293006420135498,
        "learning_rate": 1.709150412229112e-05,
        "epoch": 0.5771708322597269,
        "step": 4480
    },
    {
        "loss": 1.9415,
        "grad_norm": 2.5637030601501465,
        "learning_rate": 1.7045732519848734e-05,
        "epoch": 0.5772996650347848,
        "step": 4481
    },
    {
        "loss": 2.0921,
        "grad_norm": 1.2560933828353882,
        "learning_rate": 1.7000009691224172e-05,
        "epoch": 0.5774284978098428,
        "step": 4482
    },
    {
        "loss": 1.7609,
        "grad_norm": 2.7789220809936523,
        "learning_rate": 1.6954335704089297e-05,
        "epoch": 0.5775573305849008,
        "step": 4483
    },
    {
        "loss": 1.9702,
        "grad_norm": 1.9627405405044556,
        "learning_rate": 1.6908710626043712e-05,
        "epoch": 0.5776861633599588,
        "step": 4484
    },
    {
        "loss": 1.2955,
        "grad_norm": 2.3027000427246094,
        "learning_rate": 1.686313452461461e-05,
        "epoch": 0.5778149961350167,
        "step": 4485
    },
    {
        "loss": 2.1911,
        "grad_norm": 1.6911346912384033,
        "learning_rate": 1.6817607467256647e-05,
        "epoch": 0.5779438289100747,
        "step": 4486
    },
    {
        "loss": 2.2188,
        "grad_norm": 1.4732692241668701,
        "learning_rate": 1.6772129521351982e-05,
        "epoch": 0.5780726616851327,
        "step": 4487
    },
    {
        "loss": 1.7311,
        "grad_norm": 3.499722719192505,
        "learning_rate": 1.6726700754210073e-05,
        "epoch": 0.5782014944601906,
        "step": 4488
    },
    {
        "loss": 2.2031,
        "grad_norm": 4.1694722175598145,
        "learning_rate": 1.6681321233067465e-05,
        "epoch": 0.5783303272352487,
        "step": 4489
    },
    {
        "loss": 2.0403,
        "grad_norm": 1.8508514165878296,
        "learning_rate": 1.6635991025088032e-05,
        "epoch": 0.5784591600103066,
        "step": 4490
    },
    {
        "loss": 1.5011,
        "grad_norm": 3.049159526824951,
        "learning_rate": 1.659071019736253e-05,
        "epoch": 0.5785879927853645,
        "step": 4491
    },
    {
        "loss": 1.7159,
        "grad_norm": 1.6757584810256958,
        "learning_rate": 1.654547881690861e-05,
        "epoch": 0.5787168255604226,
        "step": 4492
    },
    {
        "loss": 1.6704,
        "grad_norm": 2.615665912628174,
        "learning_rate": 1.6500296950670773e-05,
        "epoch": 0.5788456583354805,
        "step": 4493
    },
    {
        "loss": 2.1461,
        "grad_norm": 1.7367501258850098,
        "learning_rate": 1.6455164665520257e-05,
        "epoch": 0.5789744911105386,
        "step": 4494
    },
    {
        "loss": 2.2224,
        "grad_norm": 1.619291067123413,
        "learning_rate": 1.641008202825493e-05,
        "epoch": 0.5791033238855965,
        "step": 4495
    },
    {
        "loss": 1.9912,
        "grad_norm": 2.375993490219116,
        "learning_rate": 1.6365049105599124e-05,
        "epoch": 0.5792321566606544,
        "step": 4496
    },
    {
        "loss": 1.5637,
        "grad_norm": 2.593306064605713,
        "learning_rate": 1.6320065964203575e-05,
        "epoch": 0.5793609894357125,
        "step": 4497
    },
    {
        "loss": 1.3542,
        "grad_norm": 2.141918420791626,
        "learning_rate": 1.6275132670645394e-05,
        "epoch": 0.5794898222107704,
        "step": 4498
    },
    {
        "loss": 1.5436,
        "grad_norm": 2.6563644409179688,
        "learning_rate": 1.623024929142799e-05,
        "epoch": 0.5796186549858284,
        "step": 4499
    },
    {
        "loss": 1.5426,
        "grad_norm": 2.7519922256469727,
        "learning_rate": 1.618541589298072e-05,
        "epoch": 0.5797474877608864,
        "step": 4500
    },
    {
        "loss": 2.2037,
        "grad_norm": 2.659477710723877,
        "learning_rate": 1.614063254165909e-05,
        "epoch": 0.5798763205359443,
        "step": 4501
    },
    {
        "loss": 1.2527,
        "grad_norm": 3.0768508911132812,
        "learning_rate": 1.6095899303744528e-05,
        "epoch": 0.5800051533110023,
        "step": 4502
    },
    {
        "loss": 1.9242,
        "grad_norm": 2.5588512420654297,
        "learning_rate": 1.6051216245444216e-05,
        "epoch": 0.5801339860860603,
        "step": 4503
    },
    {
        "loss": 1.9735,
        "grad_norm": 1.3273518085479736,
        "learning_rate": 1.600658343289112e-05,
        "epoch": 0.5802628188611183,
        "step": 4504
    },
    {
        "loss": 1.4001,
        "grad_norm": 2.1263575553894043,
        "learning_rate": 1.5962000932143855e-05,
        "epoch": 0.5803916516361762,
        "step": 4505
    },
    {
        "loss": 1.9877,
        "grad_norm": 2.6084465980529785,
        "learning_rate": 1.5917468809186553e-05,
        "epoch": 0.5805204844112343,
        "step": 4506
    },
    {
        "loss": 2.4087,
        "grad_norm": 1.5273572206497192,
        "learning_rate": 1.587298712992878e-05,
        "epoch": 0.5806493171862922,
        "step": 4507
    },
    {
        "loss": 1.3756,
        "grad_norm": 3.052528142929077,
        "learning_rate": 1.5828555960205456e-05,
        "epoch": 0.5807781499613501,
        "step": 4508
    },
    {
        "loss": 1.1134,
        "grad_norm": 2.8348333835601807,
        "learning_rate": 1.5784175365776764e-05,
        "epoch": 0.5809069827364082,
        "step": 4509
    },
    {
        "loss": 1.4704,
        "grad_norm": 3.190308094024658,
        "learning_rate": 1.5739845412327976e-05,
        "epoch": 0.5810358155114661,
        "step": 4510
    },
    {
        "loss": 1.6059,
        "grad_norm": 3.360243320465088,
        "learning_rate": 1.5695566165469417e-05,
        "epoch": 0.5811646482865241,
        "step": 4511
    },
    {
        "loss": 2.0527,
        "grad_norm": 1.808766484260559,
        "learning_rate": 1.565133769073641e-05,
        "epoch": 0.5812934810615821,
        "step": 4512
    },
    {
        "loss": 2.6249,
        "grad_norm": 1.6553871631622314,
        "learning_rate": 1.560716005358914e-05,
        "epoch": 0.58142231383664,
        "step": 4513
    },
    {
        "loss": 1.4767,
        "grad_norm": 3.3942208290100098,
        "learning_rate": 1.5563033319412523e-05,
        "epoch": 0.581551146611698,
        "step": 4514
    },
    {
        "loss": 1.0607,
        "grad_norm": 2.891948938369751,
        "learning_rate": 1.5518957553516035e-05,
        "epoch": 0.581679979386756,
        "step": 4515
    },
    {
        "loss": 2.4084,
        "grad_norm": 2.3147242069244385,
        "learning_rate": 1.5474932821133964e-05,
        "epoch": 0.581808812161814,
        "step": 4516
    },
    {
        "loss": 2.4518,
        "grad_norm": 1.3362061977386475,
        "learning_rate": 1.5430959187424877e-05,
        "epoch": 0.581937644936872,
        "step": 4517
    },
    {
        "loss": 1.8835,
        "grad_norm": 1.6891825199127197,
        "learning_rate": 1.5387036717471705e-05,
        "epoch": 0.5820664777119299,
        "step": 4518
    },
    {
        "loss": 1.4352,
        "grad_norm": 2.5174381732940674,
        "learning_rate": 1.5343165476281734e-05,
        "epoch": 0.5821953104869879,
        "step": 4519
    },
    {
        "loss": 2.3252,
        "grad_norm": 2.516582489013672,
        "learning_rate": 1.529934552878644e-05,
        "epoch": 0.5823241432620458,
        "step": 4520
    },
    {
        "loss": 2.4781,
        "grad_norm": 1.5019129514694214,
        "learning_rate": 1.5255576939841288e-05,
        "epoch": 0.5824529760371039,
        "step": 4521
    },
    {
        "loss": 1.54,
        "grad_norm": 3.4462358951568604,
        "learning_rate": 1.521185977422579e-05,
        "epoch": 0.5825818088121618,
        "step": 4522
    },
    {
        "loss": 1.907,
        "grad_norm": 2.543351888656616,
        "learning_rate": 1.5168194096643334e-05,
        "epoch": 0.5827106415872197,
        "step": 4523
    },
    {
        "loss": 1.7383,
        "grad_norm": 3.154928207397461,
        "learning_rate": 1.512457997172117e-05,
        "epoch": 0.5828394743622778,
        "step": 4524
    },
    {
        "loss": 2.4149,
        "grad_norm": 1.6054627895355225,
        "learning_rate": 1.5081017464010144e-05,
        "epoch": 0.5829683071373357,
        "step": 4525
    },
    {
        "loss": 1.5402,
        "grad_norm": 3.4579946994781494,
        "learning_rate": 1.5037506637984761e-05,
        "epoch": 0.5830971399123938,
        "step": 4526
    },
    {
        "loss": 1.9248,
        "grad_norm": 2.1505608558654785,
        "learning_rate": 1.4994047558043062e-05,
        "epoch": 0.5832259726874517,
        "step": 4527
    },
    {
        "loss": 1.981,
        "grad_norm": 1.730193018913269,
        "learning_rate": 1.4950640288506435e-05,
        "epoch": 0.5833548054625096,
        "step": 4528
    },
    {
        "loss": 2.1285,
        "grad_norm": 2.288961410522461,
        "learning_rate": 1.4907284893619583e-05,
        "epoch": 0.5834836382375677,
        "step": 4529
    },
    {
        "loss": 2.4071,
        "grad_norm": 1.3340834379196167,
        "learning_rate": 1.48639814375505e-05,
        "epoch": 0.5836124710126256,
        "step": 4530
    },
    {
        "loss": 2.1241,
        "grad_norm": 1.6039005517959595,
        "learning_rate": 1.4820729984390291e-05,
        "epoch": 0.5837413037876836,
        "step": 4531
    },
    {
        "loss": 1.6001,
        "grad_norm": 1.6609188318252563,
        "learning_rate": 1.4777530598153033e-05,
        "epoch": 0.5838701365627416,
        "step": 4532
    },
    {
        "loss": 1.1151,
        "grad_norm": 3.2086987495422363,
        "learning_rate": 1.4734383342775832e-05,
        "epoch": 0.5839989693377995,
        "step": 4533
    },
    {
        "loss": 1.6415,
        "grad_norm": 2.0959866046905518,
        "learning_rate": 1.4691288282118587e-05,
        "epoch": 0.5841278021128575,
        "step": 4534
    },
    {
        "loss": 1.5975,
        "grad_norm": 3.517280101776123,
        "learning_rate": 1.4648245479963957e-05,
        "epoch": 0.5842566348879155,
        "step": 4535
    },
    {
        "loss": 2.0451,
        "grad_norm": 2.56915020942688,
        "learning_rate": 1.4605255000017204e-05,
        "epoch": 0.5843854676629735,
        "step": 4536
    },
    {
        "loss": 0.9691,
        "grad_norm": 3.0911762714385986,
        "learning_rate": 1.4562316905906226e-05,
        "epoch": 0.5845143004380314,
        "step": 4537
    },
    {
        "loss": 2.2464,
        "grad_norm": 2.2908737659454346,
        "learning_rate": 1.4519431261181398e-05,
        "epoch": 0.5846431332130894,
        "step": 4538
    },
    {
        "loss": 1.8099,
        "grad_norm": 2.925877571105957,
        "learning_rate": 1.4476598129315372e-05,
        "epoch": 0.5847719659881474,
        "step": 4539
    },
    {
        "loss": 1.3562,
        "grad_norm": 3.6738669872283936,
        "learning_rate": 1.443381757370314e-05,
        "epoch": 0.5849007987632053,
        "step": 4540
    },
    {
        "loss": 1.0624,
        "grad_norm": 2.677882671356201,
        "learning_rate": 1.4391089657661878e-05,
        "epoch": 0.5850296315382634,
        "step": 4541
    },
    {
        "loss": 1.478,
        "grad_norm": 2.9488720893859863,
        "learning_rate": 1.4348414444430886e-05,
        "epoch": 0.5851584643133213,
        "step": 4542
    },
    {
        "loss": 1.4181,
        "grad_norm": 3.3306264877319336,
        "learning_rate": 1.4305791997171392e-05,
        "epoch": 0.5852872970883792,
        "step": 4543
    },
    {
        "loss": 2.1889,
        "grad_norm": 1.932983636856079,
        "learning_rate": 1.4263222378966562e-05,
        "epoch": 0.5854161298634373,
        "step": 4544
    },
    {
        "loss": 2.1585,
        "grad_norm": 1.9453006982803345,
        "learning_rate": 1.422070565282141e-05,
        "epoch": 0.5855449626384952,
        "step": 4545
    },
    {
        "loss": 2.5498,
        "grad_norm": 1.1887606382369995,
        "learning_rate": 1.4178241881662591e-05,
        "epoch": 0.5856737954135532,
        "step": 4546
    },
    {
        "loss": 1.9456,
        "grad_norm": 2.183501958847046,
        "learning_rate": 1.4135831128338401e-05,
        "epoch": 0.5858026281886112,
        "step": 4547
    },
    {
        "loss": 1.689,
        "grad_norm": 2.9394690990448,
        "learning_rate": 1.4093473455618712e-05,
        "epoch": 0.5859314609636691,
        "step": 4548
    },
    {
        "loss": 2.2915,
        "grad_norm": 1.9104892015457153,
        "learning_rate": 1.4051168926194808e-05,
        "epoch": 0.5860602937387271,
        "step": 4549
    },
    {
        "loss": 1.9092,
        "grad_norm": 1.7169078588485718,
        "learning_rate": 1.4008917602679328e-05,
        "epoch": 0.5861891265137851,
        "step": 4550
    },
    {
        "loss": 2.1911,
        "grad_norm": 1.6905105113983154,
        "learning_rate": 1.396671954760615e-05,
        "epoch": 0.5863179592888431,
        "step": 4551
    },
    {
        "loss": 2.3593,
        "grad_norm": 1.7948253154754639,
        "learning_rate": 1.3924574823430353e-05,
        "epoch": 0.586446792063901,
        "step": 4552
    },
    {
        "loss": 1.8758,
        "grad_norm": 3.180230140686035,
        "learning_rate": 1.3882483492527998e-05,
        "epoch": 0.5865756248389591,
        "step": 4553
    },
    {
        "loss": 2.1204,
        "grad_norm": 2.410515785217285,
        "learning_rate": 1.3840445617196185e-05,
        "epoch": 0.586704457614017,
        "step": 4554
    },
    {
        "loss": 2.3985,
        "grad_norm": 1.491215467453003,
        "learning_rate": 1.379846125965289e-05,
        "epoch": 0.586833290389075,
        "step": 4555
    },
    {
        "loss": 2.0203,
        "grad_norm": 2.1750102043151855,
        "learning_rate": 1.3756530482036906e-05,
        "epoch": 0.586962123164133,
        "step": 4556
    },
    {
        "loss": 2.4916,
        "grad_norm": 1.7046597003936768,
        "learning_rate": 1.3714653346407658e-05,
        "epoch": 0.5870909559391909,
        "step": 4557
    },
    {
        "loss": 2.331,
        "grad_norm": 2.0480122566223145,
        "learning_rate": 1.367282991474516e-05,
        "epoch": 0.587219788714249,
        "step": 4558
    },
    {
        "loss": 2.3805,
        "grad_norm": 1.9906667470932007,
        "learning_rate": 1.3631060248950138e-05,
        "epoch": 0.5873486214893069,
        "step": 4559
    },
    {
        "loss": 1.6118,
        "grad_norm": 3.082608222961426,
        "learning_rate": 1.358934441084353e-05,
        "epoch": 0.5874774542643648,
        "step": 4560
    },
    {
        "loss": 2.1131,
        "grad_norm": 1.6351243257522583,
        "learning_rate": 1.354768246216664e-05,
        "epoch": 0.5876062870394229,
        "step": 4561
    },
    {
        "loss": 2.3026,
        "grad_norm": 1.473785638809204,
        "learning_rate": 1.3506074464581081e-05,
        "epoch": 0.5877351198144808,
        "step": 4562
    },
    {
        "loss": 1.8166,
        "grad_norm": 3.2738983631134033,
        "learning_rate": 1.3464520479668608e-05,
        "epoch": 0.5878639525895388,
        "step": 4563
    },
    {
        "loss": 2.0261,
        "grad_norm": 2.740522861480713,
        "learning_rate": 1.3423020568931005e-05,
        "epoch": 0.5879927853645968,
        "step": 4564
    },
    {
        "loss": 2.2799,
        "grad_norm": 1.4912035465240479,
        "learning_rate": 1.3381574793789992e-05,
        "epoch": 0.5881216181396547,
        "step": 4565
    },
    {
        "loss": 2.0414,
        "grad_norm": 1.8478204011917114,
        "learning_rate": 1.3340183215587203e-05,
        "epoch": 0.5882504509147127,
        "step": 4566
    },
    {
        "loss": 2.0659,
        "grad_norm": 2.2263827323913574,
        "learning_rate": 1.329884589558415e-05,
        "epoch": 0.5883792836897707,
        "step": 4567
    },
    {
        "loss": 2.0991,
        "grad_norm": 1.2792797088623047,
        "learning_rate": 1.3257562894961862e-05,
        "epoch": 0.5885081164648287,
        "step": 4568
    },
    {
        "loss": 2.3246,
        "grad_norm": 1.848716139793396,
        "learning_rate": 1.3216334274821085e-05,
        "epoch": 0.5886369492398866,
        "step": 4569
    },
    {
        "loss": 2.3239,
        "grad_norm": 1.3648163080215454,
        "learning_rate": 1.3175160096182093e-05,
        "epoch": 0.5887657820149446,
        "step": 4570
    },
    {
        "loss": 2.0559,
        "grad_norm": 2.049790859222412,
        "learning_rate": 1.3134040419984507e-05,
        "epoch": 0.5888946147900026,
        "step": 4571
    },
    {
        "loss": 2.6175,
        "grad_norm": 1.262576699256897,
        "learning_rate": 1.3092975307087307e-05,
        "epoch": 0.5890234475650605,
        "step": 4572
    },
    {
        "loss": 2.0414,
        "grad_norm": 1.867118239402771,
        "learning_rate": 1.3051964818268753e-05,
        "epoch": 0.5891522803401186,
        "step": 4573
    },
    {
        "loss": 2.263,
        "grad_norm": 1.785902738571167,
        "learning_rate": 1.3011009014226272e-05,
        "epoch": 0.5892811131151765,
        "step": 4574
    },
    {
        "loss": 1.0937,
        "grad_norm": 3.387336254119873,
        "learning_rate": 1.2970107955576227e-05,
        "epoch": 0.5894099458902344,
        "step": 4575
    },
    {
        "loss": 2.3524,
        "grad_norm": 1.9508684873580933,
        "learning_rate": 1.2929261702854145e-05,
        "epoch": 0.5895387786652925,
        "step": 4576
    },
    {
        "loss": 2.0378,
        "grad_norm": 2.3388242721557617,
        "learning_rate": 1.2888470316514328e-05,
        "epoch": 0.5896676114403504,
        "step": 4577
    },
    {
        "loss": 1.6351,
        "grad_norm": 2.212376594543457,
        "learning_rate": 1.2847733856929873e-05,
        "epoch": 0.5897964442154084,
        "step": 4578
    },
    {
        "loss": 2.5175,
        "grad_norm": 1.6901508569717407,
        "learning_rate": 1.2807052384392554e-05,
        "epoch": 0.5899252769904664,
        "step": 4579
    },
    {
        "loss": 1.8013,
        "grad_norm": 2.453273296356201,
        "learning_rate": 1.2766425959112838e-05,
        "epoch": 0.5900541097655243,
        "step": 4580
    },
    {
        "loss": 2.2481,
        "grad_norm": 1.9975616931915283,
        "learning_rate": 1.2725854641219703e-05,
        "epoch": 0.5901829425405823,
        "step": 4581
    },
    {
        "loss": 1.9675,
        "grad_norm": 2.0279500484466553,
        "learning_rate": 1.2685338490760552e-05,
        "epoch": 0.5903117753156403,
        "step": 4582
    },
    {
        "loss": 1.8588,
        "grad_norm": 3.0000011920928955,
        "learning_rate": 1.2644877567701041e-05,
        "epoch": 0.5904406080906983,
        "step": 4583
    },
    {
        "loss": 1.6228,
        "grad_norm": 2.4321987628936768,
        "learning_rate": 1.260447193192531e-05,
        "epoch": 0.5905694408657562,
        "step": 4584
    },
    {
        "loss": 1.7564,
        "grad_norm": 2.693965196609497,
        "learning_rate": 1.2564121643235505e-05,
        "epoch": 0.5906982736408142,
        "step": 4585
    },
    {
        "loss": 2.1713,
        "grad_norm": 1.3983556032180786,
        "learning_rate": 1.2523826761351842e-05,
        "epoch": 0.5908271064158722,
        "step": 4586
    },
    {
        "loss": 2.0982,
        "grad_norm": 2.519131660461426,
        "learning_rate": 1.2483587345912612e-05,
        "epoch": 0.5909559391909301,
        "step": 4587
    },
    {
        "loss": 1.3546,
        "grad_norm": 2.425551176071167,
        "learning_rate": 1.2443403456474017e-05,
        "epoch": 0.5910847719659882,
        "step": 4588
    },
    {
        "loss": 1.7883,
        "grad_norm": 2.0429635047912598,
        "learning_rate": 1.2403275152509997e-05,
        "epoch": 0.5912136047410461,
        "step": 4589
    },
    {
        "loss": 1.8955,
        "grad_norm": 1.4720566272735596,
        "learning_rate": 1.236320249341228e-05,
        "epoch": 0.591342437516104,
        "step": 4590
    },
    {
        "loss": 2.1326,
        "grad_norm": 2.044759750366211,
        "learning_rate": 1.2323185538490229e-05,
        "epoch": 0.5914712702911621,
        "step": 4591
    },
    {
        "loss": 2.0146,
        "grad_norm": 1.5181684494018555,
        "learning_rate": 1.2283224346970767e-05,
        "epoch": 0.59160010306622,
        "step": 4592
    },
    {
        "loss": 2.1041,
        "grad_norm": 1.8449444770812988,
        "learning_rate": 1.224331897799828e-05,
        "epoch": 0.5917289358412781,
        "step": 4593
    },
    {
        "loss": 2.1821,
        "grad_norm": 1.5053272247314453,
        "learning_rate": 1.220346949063455e-05,
        "epoch": 0.591857768616336,
        "step": 4594
    },
    {
        "loss": 2.1823,
        "grad_norm": 2.542813777923584,
        "learning_rate": 1.2163675943858615e-05,
        "epoch": 0.5919866013913939,
        "step": 4595
    },
    {
        "loss": 2.0961,
        "grad_norm": 2.8654732704162598,
        "learning_rate": 1.2123938396566753e-05,
        "epoch": 0.592115434166452,
        "step": 4596
    },
    {
        "loss": 1.0874,
        "grad_norm": 3.661302089691162,
        "learning_rate": 1.2084256907572305e-05,
        "epoch": 0.5922442669415099,
        "step": 4597
    },
    {
        "loss": 2.0786,
        "grad_norm": 1.4165763854980469,
        "learning_rate": 1.2044631535605705e-05,
        "epoch": 0.5923730997165679,
        "step": 4598
    },
    {
        "loss": 1.8509,
        "grad_norm": 1.840736746788025,
        "learning_rate": 1.2005062339314332e-05,
        "epoch": 0.5925019324916259,
        "step": 4599
    },
    {
        "loss": 2.3302,
        "grad_norm": 2.793091297149658,
        "learning_rate": 1.1965549377262352e-05,
        "epoch": 0.5926307652666838,
        "step": 4600
    },
    {
        "loss": 1.5745,
        "grad_norm": 2.2587950229644775,
        "learning_rate": 1.1926092707930797e-05,
        "epoch": 0.5927595980417418,
        "step": 4601
    },
    {
        "loss": 2.123,
        "grad_norm": 1.9966682195663452,
        "learning_rate": 1.1886692389717346e-05,
        "epoch": 0.5928884308167998,
        "step": 4602
    },
    {
        "loss": 1.4446,
        "grad_norm": 3.4244260787963867,
        "learning_rate": 1.1847348480936226e-05,
        "epoch": 0.5930172635918578,
        "step": 4603
    },
    {
        "loss": 2.3083,
        "grad_norm": 1.6190472841262817,
        "learning_rate": 1.1808061039818214e-05,
        "epoch": 0.5931460963669157,
        "step": 4604
    },
    {
        "loss": 1.7655,
        "grad_norm": 2.3931918144226074,
        "learning_rate": 1.1768830124510527e-05,
        "epoch": 0.5932749291419738,
        "step": 4605
    },
    {
        "loss": 2.2008,
        "grad_norm": 2.455918073654175,
        "learning_rate": 1.1729655793076732e-05,
        "epoch": 0.5934037619170317,
        "step": 4606
    },
    {
        "loss": 2.2124,
        "grad_norm": 2.0766918659210205,
        "learning_rate": 1.1690538103496589e-05,
        "epoch": 0.5935325946920896,
        "step": 4607
    },
    {
        "loss": 1.8703,
        "grad_norm": 1.9983015060424805,
        "learning_rate": 1.1651477113666059e-05,
        "epoch": 0.5936614274671477,
        "step": 4608
    },
    {
        "loss": 1.9365,
        "grad_norm": 2.262561559677124,
        "learning_rate": 1.161247288139719e-05,
        "epoch": 0.5937902602422056,
        "step": 4609
    },
    {
        "loss": 1.3926,
        "grad_norm": 3.082270383834839,
        "learning_rate": 1.1573525464418077e-05,
        "epoch": 0.5939190930172636,
        "step": 4610
    },
    {
        "loss": 1.7426,
        "grad_norm": 2.6269614696502686,
        "learning_rate": 1.15346349203726e-05,
        "epoch": 0.5940479257923216,
        "step": 4611
    },
    {
        "loss": 2.3172,
        "grad_norm": 2.5835342407226562,
        "learning_rate": 1.1495801306820569e-05,
        "epoch": 0.5941767585673795,
        "step": 4612
    },
    {
        "loss": 1.7247,
        "grad_norm": 2.634798288345337,
        "learning_rate": 1.145702468123751e-05,
        "epoch": 0.5943055913424375,
        "step": 4613
    },
    {
        "loss": 2.126,
        "grad_norm": 2.359908103942871,
        "learning_rate": 1.1418305101014604e-05,
        "epoch": 0.5944344241174955,
        "step": 4614
    },
    {
        "loss": 2.1063,
        "grad_norm": 2.449775218963623,
        "learning_rate": 1.1379642623458547e-05,
        "epoch": 0.5945632568925535,
        "step": 4615
    },
    {
        "loss": 1.078,
        "grad_norm": 2.964960813522339,
        "learning_rate": 1.1341037305791596e-05,
        "epoch": 0.5946920896676114,
        "step": 4616
    },
    {
        "loss": 2.0494,
        "grad_norm": 2.9226295948028564,
        "learning_rate": 1.1302489205151413e-05,
        "epoch": 0.5948209224426694,
        "step": 4617
    },
    {
        "loss": 2.4203,
        "grad_norm": 1.7864441871643066,
        "learning_rate": 1.1263998378590851e-05,
        "epoch": 0.5949497552177274,
        "step": 4618
    },
    {
        "loss": 1.9734,
        "grad_norm": 1.4380125999450684,
        "learning_rate": 1.1225564883078181e-05,
        "epoch": 0.5950785879927853,
        "step": 4619
    },
    {
        "loss": 2.2408,
        "grad_norm": 2.072847366333008,
        "learning_rate": 1.1187188775496726e-05,
        "epoch": 0.5952074207678434,
        "step": 4620
    },
    {
        "loss": 1.9991,
        "grad_norm": 1.2855790853500366,
        "learning_rate": 1.1148870112644832e-05,
        "epoch": 0.5953362535429013,
        "step": 4621
    },
    {
        "loss": 2.0122,
        "grad_norm": 2.6437644958496094,
        "learning_rate": 1.1110608951235862e-05,
        "epoch": 0.5954650863179592,
        "step": 4622
    },
    {
        "loss": 2.2544,
        "grad_norm": 2.0223581790924072,
        "learning_rate": 1.1072405347898096e-05,
        "epoch": 0.5955939190930173,
        "step": 4623
    },
    {
        "loss": 2.374,
        "grad_norm": 1.649881362915039,
        "learning_rate": 1.1034259359174636e-05,
        "epoch": 0.5957227518680752,
        "step": 4624
    },
    {
        "loss": 2.0567,
        "grad_norm": 2.160651445388794,
        "learning_rate": 1.0996171041523262e-05,
        "epoch": 0.5958515846431333,
        "step": 4625
    },
    {
        "loss": 1.8675,
        "grad_norm": 3.1809487342834473,
        "learning_rate": 1.0958140451316324e-05,
        "epoch": 0.5959804174181912,
        "step": 4626
    },
    {
        "loss": 1.9444,
        "grad_norm": 1.9835398197174072,
        "learning_rate": 1.0920167644841001e-05,
        "epoch": 0.5961092501932491,
        "step": 4627
    },
    {
        "loss": 1.8687,
        "grad_norm": 2.3042960166931152,
        "learning_rate": 1.0882252678298698e-05,
        "epoch": 0.5962380829683072,
        "step": 4628
    },
    {
        "loss": 1.7148,
        "grad_norm": 1.5670275688171387,
        "learning_rate": 1.0844395607805263e-05,
        "epoch": 0.5963669157433651,
        "step": 4629
    },
    {
        "loss": 2.0928,
        "grad_norm": 2.598961114883423,
        "learning_rate": 1.0806596489390914e-05,
        "epoch": 0.5964957485184231,
        "step": 4630
    },
    {
        "loss": 1.919,
        "grad_norm": 1.5074316263198853,
        "learning_rate": 1.076885537900008e-05,
        "epoch": 0.5966245812934811,
        "step": 4631
    },
    {
        "loss": 2.296,
        "grad_norm": 1.8151812553405762,
        "learning_rate": 1.073117233249133e-05,
        "epoch": 0.596753414068539,
        "step": 4632
    },
    {
        "loss": 2.4972,
        "grad_norm": 2.1825404167175293,
        "learning_rate": 1.0693547405637245e-05,
        "epoch": 0.596882246843597,
        "step": 4633
    },
    {
        "loss": 1.2503,
        "grad_norm": 2.8155088424682617,
        "learning_rate": 1.065598065412446e-05,
        "epoch": 0.597011079618655,
        "step": 4634
    },
    {
        "loss": 1.7584,
        "grad_norm": 3.9801583290100098,
        "learning_rate": 1.0618472133553476e-05,
        "epoch": 0.597139912393713,
        "step": 4635
    },
    {
        "loss": 1.9938,
        "grad_norm": 2.077831268310547,
        "learning_rate": 1.0581021899438615e-05,
        "epoch": 0.5972687451687709,
        "step": 4636
    },
    {
        "loss": 2.318,
        "grad_norm": 2.0678465366363525,
        "learning_rate": 1.0543630007207932e-05,
        "epoch": 0.5973975779438289,
        "step": 4637
    },
    {
        "loss": 2.0218,
        "grad_norm": 2.131866693496704,
        "learning_rate": 1.0506296512203161e-05,
        "epoch": 0.5975264107188869,
        "step": 4638
    },
    {
        "loss": 1.9941,
        "grad_norm": 2.807323694229126,
        "learning_rate": 1.0469021469679535e-05,
        "epoch": 0.5976552434939448,
        "step": 4639
    },
    {
        "loss": 1.6411,
        "grad_norm": 2.878526210784912,
        "learning_rate": 1.0431804934805811e-05,
        "epoch": 0.5977840762690029,
        "step": 4640
    },
    {
        "loss": 1.9156,
        "grad_norm": 3.263026475906372,
        "learning_rate": 1.0394646962664172e-05,
        "epoch": 0.5979129090440608,
        "step": 4641
    },
    {
        "loss": 1.9767,
        "grad_norm": 1.731433391571045,
        "learning_rate": 1.035754760825014e-05,
        "epoch": 0.5980417418191187,
        "step": 4642
    },
    {
        "loss": 2.1666,
        "grad_norm": 2.8274247646331787,
        "learning_rate": 1.0320506926472368e-05,
        "epoch": 0.5981705745941768,
        "step": 4643
    },
    {
        "loss": 2.0509,
        "grad_norm": 3.1720962524414062,
        "learning_rate": 1.0283524972152842e-05,
        "epoch": 0.5982994073692347,
        "step": 4644
    },
    {
        "loss": 2.2538,
        "grad_norm": 1.5117802619934082,
        "learning_rate": 1.0246601800026512e-05,
        "epoch": 0.5984282401442927,
        "step": 4645
    },
    {
        "loss": 1.7863,
        "grad_norm": 1.470784306526184,
        "learning_rate": 1.020973746474136e-05,
        "epoch": 0.5985570729193507,
        "step": 4646
    },
    {
        "loss": 1.4992,
        "grad_norm": 2.550997495651245,
        "learning_rate": 1.0172932020858238e-05,
        "epoch": 0.5986859056944086,
        "step": 4647
    },
    {
        "loss": 2.1418,
        "grad_norm": 2.0429341793060303,
        "learning_rate": 1.0136185522850916e-05,
        "epoch": 0.5988147384694666,
        "step": 4648
    },
    {
        "loss": 2.0201,
        "grad_norm": 3.3460378646850586,
        "learning_rate": 1.0099498025105902e-05,
        "epoch": 0.5989435712445246,
        "step": 4649
    },
    {
        "loss": 2.1523,
        "grad_norm": 1.920525312423706,
        "learning_rate": 1.0062869581922351e-05,
        "epoch": 0.5990724040195826,
        "step": 4650
    },
    {
        "loss": 2.2847,
        "grad_norm": 2.2732532024383545,
        "learning_rate": 1.0026300247512005e-05,
        "epoch": 0.5992012367946405,
        "step": 4651
    },
    {
        "loss": 1.8891,
        "grad_norm": 1.6284900903701782,
        "learning_rate": 9.989790075999145e-06,
        "epoch": 0.5993300695696985,
        "step": 4652
    },
    {
        "loss": 2.2189,
        "grad_norm": 2.238814115524292,
        "learning_rate": 9.953339121420535e-06,
        "epoch": 0.5994589023447565,
        "step": 4653
    },
    {
        "loss": 2.1655,
        "grad_norm": 3.2720375061035156,
        "learning_rate": 9.916947437725204e-06,
        "epoch": 0.5995877351198144,
        "step": 4654
    },
    {
        "loss": 2.3071,
        "grad_norm": 2.259827136993408,
        "learning_rate": 9.88061507877453e-06,
        "epoch": 0.5997165678948725,
        "step": 4655
    },
    {
        "loss": 2.0178,
        "grad_norm": 1.7914189100265503,
        "learning_rate": 9.844342098342068e-06,
        "epoch": 0.5998454006699304,
        "step": 4656
    },
    {
        "loss": 1.8309,
        "grad_norm": 2.1302621364593506,
        "learning_rate": 9.808128550113454e-06,
        "epoch": 0.5999742334449885,
        "step": 4657
    },
    {
        "loss": 1.836,
        "grad_norm": 2.236123561859131,
        "learning_rate": 9.771974487686392e-06,
        "epoch": 0.6001030662200464,
        "step": 4658
    },
    {
        "loss": 2.2317,
        "grad_norm": 3.0939040184020996,
        "learning_rate": 9.735879964570543e-06,
        "epoch": 0.6002318989951043,
        "step": 4659
    },
    {
        "loss": 1.3237,
        "grad_norm": 2.972625494003296,
        "learning_rate": 9.69984503418747e-06,
        "epoch": 0.6003607317701624,
        "step": 4660
    },
    {
        "loss": 2.051,
        "grad_norm": 2.5224316120147705,
        "learning_rate": 9.663869749870485e-06,
        "epoch": 0.6004895645452203,
        "step": 4661
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.5029734373092651,
        "learning_rate": 9.62795416486466e-06,
        "epoch": 0.6006183973202783,
        "step": 4662
    },
    {
        "loss": 1.3146,
        "grad_norm": 2.5326952934265137,
        "learning_rate": 9.592098332326704e-06,
        "epoch": 0.6007472300953363,
        "step": 4663
    },
    {
        "loss": 1.3549,
        "grad_norm": 3.337437391281128,
        "learning_rate": 9.556302305324899e-06,
        "epoch": 0.6008760628703942,
        "step": 4664
    },
    {
        "loss": 1.5251,
        "grad_norm": 2.6887104511260986,
        "learning_rate": 9.520566136838944e-06,
        "epoch": 0.6010048956454522,
        "step": 4665
    },
    {
        "loss": 2.0646,
        "grad_norm": 2.3367669582366943,
        "learning_rate": 9.484889879760023e-06,
        "epoch": 0.6011337284205102,
        "step": 4666
    },
    {
        "loss": 1.3893,
        "grad_norm": 2.871716022491455,
        "learning_rate": 9.449273586890656e-06,
        "epoch": 0.6012625611955682,
        "step": 4667
    },
    {
        "loss": 1.9419,
        "grad_norm": 1.3082709312438965,
        "learning_rate": 9.413717310944542e-06,
        "epoch": 0.6013913939706261,
        "step": 4668
    },
    {
        "loss": 2.0158,
        "grad_norm": 1.4597209692001343,
        "learning_rate": 9.378221104546541e-06,
        "epoch": 0.6015202267456841,
        "step": 4669
    },
    {
        "loss": 1.6772,
        "grad_norm": 3.4358837604522705,
        "learning_rate": 9.34278502023278e-06,
        "epoch": 0.6016490595207421,
        "step": 4670
    },
    {
        "loss": 2.3575,
        "grad_norm": 1.4996517896652222,
        "learning_rate": 9.307409110450199e-06,
        "epoch": 0.6017778922958,
        "step": 4671
    },
    {
        "loss": 1.9505,
        "grad_norm": 3.2146084308624268,
        "learning_rate": 9.272093427556754e-06,
        "epoch": 0.6019067250708581,
        "step": 4672
    },
    {
        "loss": 2.3686,
        "grad_norm": 1.4148555994033813,
        "learning_rate": 9.236838023821287e-06,
        "epoch": 0.602035557845916,
        "step": 4673
    },
    {
        "loss": 1.7563,
        "grad_norm": 2.8414266109466553,
        "learning_rate": 9.201642951423422e-06,
        "epoch": 0.6021643906209739,
        "step": 4674
    },
    {
        "loss": 1.437,
        "grad_norm": 3.5477216243743896,
        "learning_rate": 9.166508262453443e-06,
        "epoch": 0.602293223396032,
        "step": 4675
    },
    {
        "loss": 2.159,
        "grad_norm": 2.7425496578216553,
        "learning_rate": 9.13143400891227e-06,
        "epoch": 0.6024220561710899,
        "step": 4676
    },
    {
        "loss": 2.4994,
        "grad_norm": 1.1768566370010376,
        "learning_rate": 9.096420242711417e-06,
        "epoch": 0.6025508889461479,
        "step": 4677
    },
    {
        "loss": 1.641,
        "grad_norm": 1.9639599323272705,
        "learning_rate": 9.061467015672858e-06,
        "epoch": 0.6026797217212059,
        "step": 4678
    },
    {
        "loss": 1.799,
        "grad_norm": 2.6722872257232666,
        "learning_rate": 9.026574379528963e-06,
        "epoch": 0.6028085544962638,
        "step": 4679
    },
    {
        "loss": 2.4461,
        "grad_norm": 2.6755077838897705,
        "learning_rate": 8.99174238592242e-06,
        "epoch": 0.6029373872713218,
        "step": 4680
    },
    {
        "loss": 2.1427,
        "grad_norm": 2.045713186264038,
        "learning_rate": 8.95697108640618e-06,
        "epoch": 0.6030662200463798,
        "step": 4681
    },
    {
        "loss": 1.2983,
        "grad_norm": 2.545428991317749,
        "learning_rate": 8.92226053244335e-06,
        "epoch": 0.6031950528214378,
        "step": 4682
    },
    {
        "loss": 1.518,
        "grad_norm": 1.6461013555526733,
        "learning_rate": 8.887610775407101e-06,
        "epoch": 0.6033238855964957,
        "step": 4683
    },
    {
        "loss": 1.7511,
        "grad_norm": 1.7745082378387451,
        "learning_rate": 8.853021866580669e-06,
        "epoch": 0.6034527183715537,
        "step": 4684
    },
    {
        "loss": 2.4629,
        "grad_norm": 1.5949198007583618,
        "learning_rate": 8.81849385715725e-06,
        "epoch": 0.6035815511466117,
        "step": 4685
    },
    {
        "loss": 2.0015,
        "grad_norm": 2.9845826625823975,
        "learning_rate": 8.784026798239786e-06,
        "epoch": 0.6037103839216696,
        "step": 4686
    },
    {
        "loss": 2.2546,
        "grad_norm": 2.5775697231292725,
        "learning_rate": 8.74962074084118e-06,
        "epoch": 0.6038392166967277,
        "step": 4687
    },
    {
        "loss": 1.355,
        "grad_norm": 1.867079257965088,
        "learning_rate": 8.71527573588396e-06,
        "epoch": 0.6039680494717856,
        "step": 4688
    },
    {
        "loss": 1.6819,
        "grad_norm": 2.741260051727295,
        "learning_rate": 8.680991834200258e-06,
        "epoch": 0.6040968822468435,
        "step": 4689
    },
    {
        "loss": 2.4226,
        "grad_norm": 1.424020767211914,
        "learning_rate": 8.646769086531819e-06,
        "epoch": 0.6042257150219016,
        "step": 4690
    },
    {
        "loss": 2.0082,
        "grad_norm": 2.235004186630249,
        "learning_rate": 8.612607543529872e-06,
        "epoch": 0.6043545477969595,
        "step": 4691
    },
    {
        "loss": 2.1048,
        "grad_norm": 1.678411841392517,
        "learning_rate": 8.578507255755092e-06,
        "epoch": 0.6044833805720176,
        "step": 4692
    },
    {
        "loss": 1.878,
        "grad_norm": 2.270500659942627,
        "learning_rate": 8.544468273677431e-06,
        "epoch": 0.6046122133470755,
        "step": 4693
    },
    {
        "loss": 2.1741,
        "grad_norm": 1.5868377685546875,
        "learning_rate": 8.510490647676107e-06,
        "epoch": 0.6047410461221334,
        "step": 4694
    },
    {
        "loss": 1.9162,
        "grad_norm": 2.8098835945129395,
        "learning_rate": 8.476574428039558e-06,
        "epoch": 0.6048698788971915,
        "step": 4695
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.763080358505249,
        "learning_rate": 8.442719664965404e-06,
        "epoch": 0.6049987116722494,
        "step": 4696
    },
    {
        "loss": 2.2291,
        "grad_norm": 3.432857036590576,
        "learning_rate": 8.40892640856017e-06,
        "epoch": 0.6051275444473074,
        "step": 4697
    },
    {
        "loss": 1.5282,
        "grad_norm": 3.117725372314453,
        "learning_rate": 8.375194708839435e-06,
        "epoch": 0.6052563772223654,
        "step": 4698
    },
    {
        "loss": 1.082,
        "grad_norm": 3.372607946395874,
        "learning_rate": 8.341524615727664e-06,
        "epoch": 0.6053852099974233,
        "step": 4699
    },
    {
        "loss": 1.4369,
        "grad_norm": 3.373652935028076,
        "learning_rate": 8.307916179058134e-06,
        "epoch": 0.6055140427724813,
        "step": 4700
    },
    {
        "loss": 1.571,
        "grad_norm": 3.1697018146514893,
        "learning_rate": 8.274369448572821e-06,
        "epoch": 0.6056428755475393,
        "step": 4701
    },
    {
        "loss": 2.6296,
        "grad_norm": 1.7269448041915894,
        "learning_rate": 8.24088447392245e-06,
        "epoch": 0.6057717083225973,
        "step": 4702
    },
    {
        "loss": 1.5714,
        "grad_norm": 1.750127911567688,
        "learning_rate": 8.207461304666298e-06,
        "epoch": 0.6059005410976552,
        "step": 4703
    },
    {
        "loss": 2.2378,
        "grad_norm": 2.11599063873291,
        "learning_rate": 8.174099990272183e-06,
        "epoch": 0.6060293738727133,
        "step": 4704
    },
    {
        "loss": 1.8416,
        "grad_norm": 3.6170856952667236,
        "learning_rate": 8.14080058011637e-06,
        "epoch": 0.6061582066477712,
        "step": 4705
    },
    {
        "loss": 1.8035,
        "grad_norm": 2.1931159496307373,
        "learning_rate": 8.107563123483525e-06,
        "epoch": 0.6062870394228291,
        "step": 4706
    },
    {
        "loss": 2.3788,
        "grad_norm": 1.6472556591033936,
        "learning_rate": 8.07438766956657e-06,
        "epoch": 0.6064158721978872,
        "step": 4707
    },
    {
        "loss": 2.1689,
        "grad_norm": 1.677533507347107,
        "learning_rate": 8.041274267466687e-06,
        "epoch": 0.6065447049729451,
        "step": 4708
    },
    {
        "loss": 2.0736,
        "grad_norm": 2.088773727416992,
        "learning_rate": 8.008222966193225e-06,
        "epoch": 0.6066735377480031,
        "step": 4709
    },
    {
        "loss": 2.2146,
        "grad_norm": 1.7748585939407349,
        "learning_rate": 7.975233814663652e-06,
        "epoch": 0.6068023705230611,
        "step": 4710
    },
    {
        "loss": 2.1464,
        "grad_norm": 2.061896800994873,
        "learning_rate": 7.94230686170338e-06,
        "epoch": 0.606931203298119,
        "step": 4711
    },
    {
        "loss": 1.4347,
        "grad_norm": 2.451019525527954,
        "learning_rate": 7.909442156045743e-06,
        "epoch": 0.607060036073177,
        "step": 4712
    },
    {
        "loss": 2.3107,
        "grad_norm": 1.2340033054351807,
        "learning_rate": 7.876639746332121e-06,
        "epoch": 0.607188868848235,
        "step": 4713
    },
    {
        "loss": 1.8601,
        "grad_norm": 1.4265506267547607,
        "learning_rate": 7.843899681111528e-06,
        "epoch": 0.607317701623293,
        "step": 4714
    },
    {
        "loss": 1.6778,
        "grad_norm": 1.9531091451644897,
        "learning_rate": 7.81122200884072e-06,
        "epoch": 0.607446534398351,
        "step": 4715
    },
    {
        "loss": 1.9386,
        "grad_norm": 2.63999605178833,
        "learning_rate": 7.778606777884168e-06,
        "epoch": 0.6075753671734089,
        "step": 4716
    },
    {
        "loss": 2.0047,
        "grad_norm": 1.957579255104065,
        "learning_rate": 7.746054036513923e-06,
        "epoch": 0.6077041999484669,
        "step": 4717
    },
    {
        "loss": 2.2737,
        "grad_norm": 1.7577831745147705,
        "learning_rate": 7.713563832909487e-06,
        "epoch": 0.6078330327235248,
        "step": 4718
    },
    {
        "loss": 2.1062,
        "grad_norm": 1.4300508499145508,
        "learning_rate": 7.681136215157841e-06,
        "epoch": 0.6079618654985829,
        "step": 4719
    },
    {
        "loss": 2.6698,
        "grad_norm": 1.2862390279769897,
        "learning_rate": 7.648771231253348e-06,
        "epoch": 0.6080906982736408,
        "step": 4720
    },
    {
        "loss": 1.7897,
        "grad_norm": 3.2918126583099365,
        "learning_rate": 7.616468929097681e-06,
        "epoch": 0.6082195310486987,
        "step": 4721
    },
    {
        "loss": 1.598,
        "grad_norm": 2.6642374992370605,
        "learning_rate": 7.584229356499695e-06,
        "epoch": 0.6083483638237568,
        "step": 4722
    },
    {
        "loss": 1.8619,
        "grad_norm": 1.6348657608032227,
        "learning_rate": 7.552052561175433e-06,
        "epoch": 0.6084771965988147,
        "step": 4723
    },
    {
        "loss": 2.6223,
        "grad_norm": 1.5429315567016602,
        "learning_rate": 7.519938590748044e-06,
        "epoch": 0.6086060293738728,
        "step": 4724
    },
    {
        "loss": 2.3922,
        "grad_norm": 1.4421206712722778,
        "learning_rate": 7.487887492747642e-06,
        "epoch": 0.6087348621489307,
        "step": 4725
    },
    {
        "loss": 2.0691,
        "grad_norm": 1.7981456518173218,
        "learning_rate": 7.455899314611292e-06,
        "epoch": 0.6088636949239886,
        "step": 4726
    },
    {
        "loss": 2.0137,
        "grad_norm": 2.248913049697876,
        "learning_rate": 7.423974103682974e-06,
        "epoch": 0.6089925276990467,
        "step": 4727
    },
    {
        "loss": 1.7091,
        "grad_norm": 3.4247891902923584,
        "learning_rate": 7.392111907213473e-06,
        "epoch": 0.6091213604741046,
        "step": 4728
    },
    {
        "loss": 2.289,
        "grad_norm": 1.7951735258102417,
        "learning_rate": 7.3603127723602565e-06,
        "epoch": 0.6092501932491626,
        "step": 4729
    },
    {
        "loss": 1.9539,
        "grad_norm": 2.413450002670288,
        "learning_rate": 7.328576746187527e-06,
        "epoch": 0.6093790260242206,
        "step": 4730
    },
    {
        "loss": 1.7778,
        "grad_norm": 2.3458170890808105,
        "learning_rate": 7.29690387566605e-06,
        "epoch": 0.6095078587992785,
        "step": 4731
    },
    {
        "loss": 2.397,
        "grad_norm": 1.5526409149169922,
        "learning_rate": 7.265294207673112e-06,
        "epoch": 0.6096366915743365,
        "step": 4732
    },
    {
        "loss": 2.2611,
        "grad_norm": 1.4627455472946167,
        "learning_rate": 7.233747788992435e-06,
        "epoch": 0.6097655243493945,
        "step": 4733
    },
    {
        "loss": 2.5689,
        "grad_norm": 1.51776123046875,
        "learning_rate": 7.202264666314179e-06,
        "epoch": 0.6098943571244525,
        "step": 4734
    },
    {
        "loss": 2.1717,
        "grad_norm": 2.429877758026123,
        "learning_rate": 7.1708448862348174e-06,
        "epoch": 0.6100231898995104,
        "step": 4735
    },
    {
        "loss": 2.0925,
        "grad_norm": 2.4678876399993896,
        "learning_rate": 7.13948849525703e-06,
        "epoch": 0.6101520226745684,
        "step": 4736
    },
    {
        "loss": 1.7039,
        "grad_norm": 3.002207040786743,
        "learning_rate": 7.108195539789663e-06,
        "epoch": 0.6102808554496264,
        "step": 4737
    },
    {
        "loss": 2.298,
        "grad_norm": 2.0284552574157715,
        "learning_rate": 7.076966066147822e-06,
        "epoch": 0.6104096882246843,
        "step": 4738
    },
    {
        "loss": 2.1815,
        "grad_norm": 2.413628101348877,
        "learning_rate": 7.045800120552487e-06,
        "epoch": 0.6105385209997424,
        "step": 4739
    },
    {
        "loss": 0.9989,
        "grad_norm": 3.5940003395080566,
        "learning_rate": 7.014697749130672e-06,
        "epoch": 0.6106673537748003,
        "step": 4740
    },
    {
        "loss": 1.5804,
        "grad_norm": 2.6152591705322266,
        "learning_rate": 6.983658997915321e-06,
        "epoch": 0.6107961865498582,
        "step": 4741
    },
    {
        "loss": 1.7566,
        "grad_norm": 1.9121102094650269,
        "learning_rate": 6.952683912845221e-06,
        "epoch": 0.6109250193249163,
        "step": 4742
    },
    {
        "loss": 2.1741,
        "grad_norm": 2.1118171215057373,
        "learning_rate": 6.921772539764882e-06,
        "epoch": 0.6110538520999742,
        "step": 4743
    },
    {
        "loss": 2.1912,
        "grad_norm": 1.4347808361053467,
        "learning_rate": 6.89092492442453e-06,
        "epoch": 0.6111826848750322,
        "step": 4744
    },
    {
        "loss": 1.9891,
        "grad_norm": 2.8462438583374023,
        "learning_rate": 6.860141112480062e-06,
        "epoch": 0.6113115176500902,
        "step": 4745
    },
    {
        "loss": 1.5805,
        "grad_norm": 2.9164557456970215,
        "learning_rate": 6.829421149492915e-06,
        "epoch": 0.6114403504251481,
        "step": 4746
    },
    {
        "loss": 1.7628,
        "grad_norm": 2.2763266563415527,
        "learning_rate": 6.798765080930048e-06,
        "epoch": 0.6115691832002061,
        "step": 4747
    },
    {
        "loss": 2.3376,
        "grad_norm": 2.2723519802093506,
        "learning_rate": 6.768172952163831e-06,
        "epoch": 0.6116980159752641,
        "step": 4748
    },
    {
        "loss": 1.7718,
        "grad_norm": 2.665421962738037,
        "learning_rate": 6.737644808472021e-06,
        "epoch": 0.6118268487503221,
        "step": 4749
    },
    {
        "loss": 2.5563,
        "grad_norm": 2.054652690887451,
        "learning_rate": 6.707180695037668e-06,
        "epoch": 0.61195568152538,
        "step": 4750
    },
    {
        "loss": 2.5184,
        "grad_norm": 1.3082849979400635,
        "learning_rate": 6.676780656949006e-06,
        "epoch": 0.612084514300438,
        "step": 4751
    },
    {
        "loss": 1.8873,
        "grad_norm": 7.130313873291016,
        "learning_rate": 6.6464447391995036e-06,
        "epoch": 0.612213347075496,
        "step": 4752
    },
    {
        "loss": 1.0896,
        "grad_norm": 1.7439274787902832,
        "learning_rate": 6.616172986687719e-06,
        "epoch": 0.612342179850554,
        "step": 4753
    },
    {
        "loss": 2.36,
        "grad_norm": 2.878970146179199,
        "learning_rate": 6.585965444217201e-06,
        "epoch": 0.612471012625612,
        "step": 4754
    },
    {
        "loss": 1.9112,
        "grad_norm": 2.1399588584899902,
        "learning_rate": 6.55582215649645e-06,
        "epoch": 0.6125998454006699,
        "step": 4755
    },
    {
        "loss": 1.788,
        "grad_norm": 2.0401995182037354,
        "learning_rate": 6.525743168139009e-06,
        "epoch": 0.612728678175728,
        "step": 4756
    },
    {
        "loss": 2.2787,
        "grad_norm": 1.7717995643615723,
        "learning_rate": 6.495728523663081e-06,
        "epoch": 0.6128575109507859,
        "step": 4757
    },
    {
        "loss": 2.6346,
        "grad_norm": 2.205775260925293,
        "learning_rate": 6.465778267491707e-06,
        "epoch": 0.6129863437258438,
        "step": 4758
    },
    {
        "loss": 1.4993,
        "grad_norm": 3.287301778793335,
        "learning_rate": 6.4358924439526515e-06,
        "epoch": 0.6131151765009019,
        "step": 4759
    },
    {
        "loss": 1.5587,
        "grad_norm": 2.446641206741333,
        "learning_rate": 6.4060710972783046e-06,
        "epoch": 0.6132440092759598,
        "step": 4760
    },
    {
        "loss": 2.1502,
        "grad_norm": 2.9689078330993652,
        "learning_rate": 6.37631427160561e-06,
        "epoch": 0.6133728420510178,
        "step": 4761
    },
    {
        "loss": 2.0701,
        "grad_norm": 2.1503093242645264,
        "learning_rate": 6.346622010975989e-06,
        "epoch": 0.6135016748260758,
        "step": 4762
    },
    {
        "loss": 2.3711,
        "grad_norm": 2.3638410568237305,
        "learning_rate": 6.316994359335354e-06,
        "epoch": 0.6136305076011337,
        "step": 4763
    },
    {
        "loss": 2.4446,
        "grad_norm": 2.111194372177124,
        "learning_rate": 6.28743136053404e-06,
        "epoch": 0.6137593403761917,
        "step": 4764
    },
    {
        "loss": 2.1513,
        "grad_norm": 2.453373432159424,
        "learning_rate": 6.257933058326565e-06,
        "epoch": 0.6138881731512497,
        "step": 4765
    },
    {
        "loss": 1.8101,
        "grad_norm": 2.3114209175109863,
        "learning_rate": 6.228499496371787e-06,
        "epoch": 0.6140170059263077,
        "step": 4766
    },
    {
        "loss": 2.3845,
        "grad_norm": 2.4830360412597656,
        "learning_rate": 6.199130718232737e-06,
        "epoch": 0.6141458387013656,
        "step": 4767
    },
    {
        "loss": 2.2895,
        "grad_norm": 1.312630534172058,
        "learning_rate": 6.169826767376513e-06,
        "epoch": 0.6142746714764236,
        "step": 4768
    },
    {
        "loss": 2.1467,
        "grad_norm": 1.8265752792358398,
        "learning_rate": 6.140587687174304e-06,
        "epoch": 0.6144035042514816,
        "step": 4769
    },
    {
        "loss": 2.4456,
        "grad_norm": 1.4741878509521484,
        "learning_rate": 6.111413520901282e-06,
        "epoch": 0.6145323370265395,
        "step": 4770
    },
    {
        "loss": 2.1238,
        "grad_norm": 1.6803888082504272,
        "learning_rate": 6.08230431173657e-06,
        "epoch": 0.6146611698015976,
        "step": 4771
    },
    {
        "loss": 1.8213,
        "grad_norm": 1.7999284267425537,
        "learning_rate": 6.0532601027630586e-06,
        "epoch": 0.6147900025766555,
        "step": 4772
    },
    {
        "loss": 1.9174,
        "grad_norm": 1.8487802743911743,
        "learning_rate": 6.024280936967569e-06,
        "epoch": 0.6149188353517134,
        "step": 4773
    },
    {
        "loss": 2.5667,
        "grad_norm": 1.7843711376190186,
        "learning_rate": 5.995366857240581e-06,
        "epoch": 0.6150476681267715,
        "step": 4774
    },
    {
        "loss": 1.7531,
        "grad_norm": 4.3467936515808105,
        "learning_rate": 5.966517906376234e-06,
        "epoch": 0.6151765009018294,
        "step": 4775
    },
    {
        "loss": 1.7273,
        "grad_norm": 2.9613544940948486,
        "learning_rate": 5.9377341270722694e-06,
        "epoch": 0.6153053336768874,
        "step": 4776
    },
    {
        "loss": 1.7134,
        "grad_norm": 1.7980977296829224,
        "learning_rate": 5.909015561930004e-06,
        "epoch": 0.6154341664519454,
        "step": 4777
    },
    {
        "loss": 2.4164,
        "grad_norm": 1.5105576515197754,
        "learning_rate": 5.880362253454247e-06,
        "epoch": 0.6155629992270033,
        "step": 4778
    },
    {
        "loss": 2.1871,
        "grad_norm": 2.3335115909576416,
        "learning_rate": 5.8517742440531934e-06,
        "epoch": 0.6156918320020613,
        "step": 4779
    },
    {
        "loss": 2.1318,
        "grad_norm": 2.1664071083068848,
        "learning_rate": 5.823251576038341e-06,
        "epoch": 0.6158206647771193,
        "step": 4780
    },
    {
        "loss": 2.1918,
        "grad_norm": 2.2430591583251953,
        "learning_rate": 5.794794291624622e-06,
        "epoch": 0.6159494975521773,
        "step": 4781
    },
    {
        "loss": 2.1226,
        "grad_norm": 2.0263092517852783,
        "learning_rate": 5.766402432930101e-06,
        "epoch": 0.6160783303272352,
        "step": 4782
    },
    {
        "loss": 1.679,
        "grad_norm": 2.707279920578003,
        "learning_rate": 5.738076041975976e-06,
        "epoch": 0.6162071631022932,
        "step": 4783
    },
    {
        "loss": 1.986,
        "grad_norm": 1.6590595245361328,
        "learning_rate": 5.709815160686627e-06,
        "epoch": 0.6163359958773512,
        "step": 4784
    },
    {
        "loss": 1.613,
        "grad_norm": 2.9489831924438477,
        "learning_rate": 5.681619830889457e-06,
        "epoch": 0.6164648286524091,
        "step": 4785
    },
    {
        "loss": 1.684,
        "grad_norm": 2.27439022064209,
        "learning_rate": 5.653490094314818e-06,
        "epoch": 0.6165936614274672,
        "step": 4786
    },
    {
        "loss": 1.8906,
        "grad_norm": 2.871554374694824,
        "learning_rate": 5.625425992596001e-06,
        "epoch": 0.6167224942025251,
        "step": 4787
    },
    {
        "loss": 2.337,
        "grad_norm": 1.9475746154785156,
        "learning_rate": 5.597427567269159e-06,
        "epoch": 0.616851326977583,
        "step": 4788
    },
    {
        "loss": 1.7459,
        "grad_norm": 1.9895682334899902,
        "learning_rate": 5.569494859773244e-06,
        "epoch": 0.6169801597526411,
        "step": 4789
    },
    {
        "loss": 1.1625,
        "grad_norm": 2.916489839553833,
        "learning_rate": 5.541627911449937e-06,
        "epoch": 0.617108992527699,
        "step": 4790
    },
    {
        "loss": 1.9879,
        "grad_norm": 2.3767340183258057,
        "learning_rate": 5.513826763543584e-06,
        "epoch": 0.6172378253027571,
        "step": 4791
    },
    {
        "loss": 1.9394,
        "grad_norm": 2.2817959785461426,
        "learning_rate": 5.486091457201181e-06,
        "epoch": 0.617366658077815,
        "step": 4792
    },
    {
        "loss": 1.6833,
        "grad_norm": 2.3040332794189453,
        "learning_rate": 5.458422033472216e-06,
        "epoch": 0.6174954908528729,
        "step": 4793
    },
    {
        "loss": 2.0431,
        "grad_norm": 2.0800325870513916,
        "learning_rate": 5.430818533308685e-06,
        "epoch": 0.617624323627931,
        "step": 4794
    },
    {
        "loss": 2.3598,
        "grad_norm": 2.6293113231658936,
        "learning_rate": 5.403280997565041e-06,
        "epoch": 0.6177531564029889,
        "step": 4795
    },
    {
        "loss": 1.784,
        "grad_norm": 2.2707746028900146,
        "learning_rate": 5.375809466998099e-06,
        "epoch": 0.6178819891780469,
        "step": 4796
    },
    {
        "loss": 1.9685,
        "grad_norm": 3.401547431945801,
        "learning_rate": 5.348403982266958e-06,
        "epoch": 0.6180108219531049,
        "step": 4797
    },
    {
        "loss": 1.5026,
        "grad_norm": 2.6012282371520996,
        "learning_rate": 5.321064583933006e-06,
        "epoch": 0.6181396547281628,
        "step": 4798
    },
    {
        "loss": 1.3969,
        "grad_norm": 3.057417392730713,
        "learning_rate": 5.293791312459801e-06,
        "epoch": 0.6182684875032208,
        "step": 4799
    },
    {
        "loss": 2.0476,
        "grad_norm": 2.7025465965270996,
        "learning_rate": 5.266584208213027e-06,
        "epoch": 0.6183973202782788,
        "step": 4800
    },
    {
        "loss": 1.4458,
        "grad_norm": 3.6433966159820557,
        "learning_rate": 5.239443311460402e-06,
        "epoch": 0.6185261530533368,
        "step": 4801
    },
    {
        "loss": 1.9076,
        "grad_norm": 2.2731575965881348,
        "learning_rate": 5.212368662371725e-06,
        "epoch": 0.6186549858283947,
        "step": 4802
    },
    {
        "loss": 1.4663,
        "grad_norm": 3.7913825511932373,
        "learning_rate": 5.1853603010187154e-06,
        "epoch": 0.6187838186034528,
        "step": 4803
    },
    {
        "loss": 2.5055,
        "grad_norm": 2.2938787937164307,
        "learning_rate": 5.158418267374959e-06,
        "epoch": 0.6189126513785107,
        "step": 4804
    },
    {
        "loss": 1.8398,
        "grad_norm": 1.0654724836349487,
        "learning_rate": 5.131542601315892e-06,
        "epoch": 0.6190414841535686,
        "step": 4805
    },
    {
        "loss": 1.3532,
        "grad_norm": 2.998415231704712,
        "learning_rate": 5.104733342618717e-06,
        "epoch": 0.6191703169286267,
        "step": 4806
    },
    {
        "loss": 2.3387,
        "grad_norm": 1.2137898206710815,
        "learning_rate": 5.077990530962401e-06,
        "epoch": 0.6192991497036846,
        "step": 4807
    },
    {
        "loss": 2.233,
        "grad_norm": 1.1749157905578613,
        "learning_rate": 5.0513142059274824e-06,
        "epoch": 0.6194279824787426,
        "step": 4808
    },
    {
        "loss": 1.8884,
        "grad_norm": 3.1887423992156982,
        "learning_rate": 5.024704406996167e-06,
        "epoch": 0.6195568152538006,
        "step": 4809
    },
    {
        "loss": 2.4296,
        "grad_norm": 1.9332600831985474,
        "learning_rate": 4.998161173552157e-06,
        "epoch": 0.6196856480288585,
        "step": 4810
    },
    {
        "loss": 2.1201,
        "grad_norm": 1.2781062126159668,
        "learning_rate": 4.971684544880656e-06,
        "epoch": 0.6198144808039165,
        "step": 4811
    },
    {
        "loss": 2.2093,
        "grad_norm": 1.7298299074172974,
        "learning_rate": 4.945274560168256e-06,
        "epoch": 0.6199433135789745,
        "step": 4812
    },
    {
        "loss": 2.2842,
        "grad_norm": 1.845005750656128,
        "learning_rate": 4.918931258502951e-06,
        "epoch": 0.6200721463540325,
        "step": 4813
    },
    {
        "loss": 1.9743,
        "grad_norm": 1.6744639873504639,
        "learning_rate": 4.892654678874042e-06,
        "epoch": 0.6202009791290904,
        "step": 4814
    },
    {
        "loss": 2.0774,
        "grad_norm": 1.8878830671310425,
        "learning_rate": 4.866444860172037e-06,
        "epoch": 0.6203298119041484,
        "step": 4815
    },
    {
        "loss": 1.3939,
        "grad_norm": 2.442014455795288,
        "learning_rate": 4.840301841188688e-06,
        "epoch": 0.6204586446792064,
        "step": 4816
    },
    {
        "loss": 1.9267,
        "grad_norm": 1.5251803398132324,
        "learning_rate": 4.814225660616867e-06,
        "epoch": 0.6205874774542643,
        "step": 4817
    },
    {
        "loss": 2.2695,
        "grad_norm": 2.598623752593994,
        "learning_rate": 4.788216357050473e-06,
        "epoch": 0.6207163102293224,
        "step": 4818
    },
    {
        "loss": 1.7911,
        "grad_norm": 2.1534297466278076,
        "learning_rate": 4.7622739689844705e-06,
        "epoch": 0.6208451430043803,
        "step": 4819
    },
    {
        "loss": 1.0099,
        "grad_norm": 3.1766741275787354,
        "learning_rate": 4.736398534814785e-06,
        "epoch": 0.6209739757794382,
        "step": 4820
    },
    {
        "loss": 2.034,
        "grad_norm": 1.8498046398162842,
        "learning_rate": 4.710590092838252e-06,
        "epoch": 0.6211028085544963,
        "step": 4821
    },
    {
        "loss": 2.3115,
        "grad_norm": 1.6983983516693115,
        "learning_rate": 4.684848681252513e-06,
        "epoch": 0.6212316413295542,
        "step": 4822
    },
    {
        "loss": 2.3013,
        "grad_norm": 1.5942022800445557,
        "learning_rate": 4.659174338156008e-06,
        "epoch": 0.6213604741046123,
        "step": 4823
    },
    {
        "loss": 1.8355,
        "grad_norm": 2.3582189083099365,
        "learning_rate": 4.63356710154802e-06,
        "epoch": 0.6214893068796702,
        "step": 4824
    },
    {
        "loss": 1.9349,
        "grad_norm": 2.822448968887329,
        "learning_rate": 4.60802700932837e-06,
        "epoch": 0.6216181396547281,
        "step": 4825
    },
    {
        "loss": 1.4561,
        "grad_norm": 2.5675559043884277,
        "learning_rate": 4.582554099297548e-06,
        "epoch": 0.6217469724297862,
        "step": 4826
    },
    {
        "loss": 1.7622,
        "grad_norm": 2.7009971141815186,
        "learning_rate": 4.5571484091566415e-06,
        "epoch": 0.6218758052048441,
        "step": 4827
    },
    {
        "loss": 1.1407,
        "grad_norm": 3.4896137714385986,
        "learning_rate": 4.531809976507229e-06,
        "epoch": 0.6220046379799021,
        "step": 4828
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.7875562906265259,
        "learning_rate": 4.50653883885136e-06,
        "epoch": 0.6221334707549601,
        "step": 4829
    },
    {
        "loss": 1.8387,
        "grad_norm": 2.752315044403076,
        "learning_rate": 4.481335033591427e-06,
        "epoch": 0.622262303530018,
        "step": 4830
    },
    {
        "loss": 2.5455,
        "grad_norm": 2.1799445152282715,
        "learning_rate": 4.4561985980302425e-06,
        "epoch": 0.622391136305076,
        "step": 4831
    },
    {
        "loss": 1.9344,
        "grad_norm": 2.224149465560913,
        "learning_rate": 4.431129569370867e-06,
        "epoch": 0.622519969080134,
        "step": 4832
    },
    {
        "loss": 2.2433,
        "grad_norm": 1.6997278928756714,
        "learning_rate": 4.406127984716613e-06,
        "epoch": 0.622648801855192,
        "step": 4833
    },
    {
        "loss": 1.9958,
        "grad_norm": 2.5225563049316406,
        "learning_rate": 4.381193881070961e-06,
        "epoch": 0.6227776346302499,
        "step": 4834
    },
    {
        "loss": 2.329,
        "grad_norm": 1.429647445678711,
        "learning_rate": 4.356327295337542e-06,
        "epoch": 0.6229064674053079,
        "step": 4835
    },
    {
        "loss": 1.6908,
        "grad_norm": 3.085292100906372,
        "learning_rate": 4.3315282643200065e-06,
        "epoch": 0.6230353001803659,
        "step": 4836
    },
    {
        "loss": 2.0663,
        "grad_norm": 2.774216890335083,
        "learning_rate": 4.306796824722048e-06,
        "epoch": 0.6231641329554238,
        "step": 4837
    },
    {
        "loss": 2.2763,
        "grad_norm": 2.3316454887390137,
        "learning_rate": 4.282133013147338e-06,
        "epoch": 0.6232929657304819,
        "step": 4838
    },
    {
        "loss": 1.9579,
        "grad_norm": 1.9623557329177856,
        "learning_rate": 4.257536866099454e-06,
        "epoch": 0.6234217985055398,
        "step": 4839
    },
    {
        "loss": 1.9544,
        "grad_norm": 2.5194551944732666,
        "learning_rate": 4.233008419981771e-06,
        "epoch": 0.6235506312805977,
        "step": 4840
    },
    {
        "loss": 1.9745,
        "grad_norm": 2.78812837600708,
        "learning_rate": 4.208547711097549e-06,
        "epoch": 0.6236794640556558,
        "step": 4841
    },
    {
        "loss": 1.3116,
        "grad_norm": 5.445509910583496,
        "learning_rate": 4.184154775649762e-06,
        "epoch": 0.6238082968307137,
        "step": 4842
    },
    {
        "loss": 1.8446,
        "grad_norm": 2.6396243572235107,
        "learning_rate": 4.159829649741043e-06,
        "epoch": 0.6239371296057717,
        "step": 4843
    },
    {
        "loss": 2.1762,
        "grad_norm": 2.129988431930542,
        "learning_rate": 4.135572369373669e-06,
        "epoch": 0.6240659623808297,
        "step": 4844
    },
    {
        "loss": 2.2981,
        "grad_norm": 1.8561033010482788,
        "learning_rate": 4.11138297044954e-06,
        "epoch": 0.6241947951558876,
        "step": 4845
    },
    {
        "loss": 1.721,
        "grad_norm": 1.817958116531372,
        "learning_rate": 4.087261488770077e-06,
        "epoch": 0.6243236279309456,
        "step": 4846
    },
    {
        "loss": 2.5214,
        "grad_norm": 1.612720251083374,
        "learning_rate": 4.0632079600361696e-06,
        "epoch": 0.6244524607060036,
        "step": 4847
    },
    {
        "loss": 2.0823,
        "grad_norm": 1.5745372772216797,
        "learning_rate": 4.039222419848121e-06,
        "epoch": 0.6245812934810616,
        "step": 4848
    },
    {
        "loss": 2.3936,
        "grad_norm": 2.340351104736328,
        "learning_rate": 4.015304903705608e-06,
        "epoch": 0.6247101262561195,
        "step": 4849
    },
    {
        "loss": 2.5598,
        "grad_norm": 1.7912580966949463,
        "learning_rate": 3.9914554470077085e-06,
        "epoch": 0.6248389590311775,
        "step": 4850
    },
    {
        "loss": 2.5035,
        "grad_norm": 2.058938980102539,
        "learning_rate": 3.9676740850526525e-06,
        "epoch": 0.6249677918062355,
        "step": 4851
    },
    {
        "loss": 1.9794,
        "grad_norm": 2.3825628757476807,
        "learning_rate": 3.943960853037948e-06,
        "epoch": 0.6250966245812934,
        "step": 4852
    },
    {
        "loss": 1.3972,
        "grad_norm": 3.920366048812866,
        "learning_rate": 3.920315786060286e-06,
        "epoch": 0.6252254573563515,
        "step": 4853
    },
    {
        "loss": 2.271,
        "grad_norm": 1.9711904525756836,
        "learning_rate": 3.896738919115417e-06,
        "epoch": 0.6253542901314094,
        "step": 4854
    },
    {
        "loss": 2.076,
        "grad_norm": 3.0058677196502686,
        "learning_rate": 3.873230287098173e-06,
        "epoch": 0.6254831229064675,
        "step": 4855
    },
    {
        "loss": 1.837,
        "grad_norm": 2.0141782760620117,
        "learning_rate": 3.849789924802411e-06,
        "epoch": 0.6256119556815254,
        "step": 4856
    },
    {
        "loss": 2.384,
        "grad_norm": 2.0863609313964844,
        "learning_rate": 3.826417866920934e-06,
        "epoch": 0.6257407884565833,
        "step": 4857
    },
    {
        "loss": 1.6876,
        "grad_norm": 2.391308069229126,
        "learning_rate": 3.8031141480454612e-06,
        "epoch": 0.6258696212316414,
        "step": 4858
    },
    {
        "loss": 1.7726,
        "grad_norm": 1.3745192289352417,
        "learning_rate": 3.779878802666559e-06,
        "epoch": 0.6259984540066993,
        "step": 4859
    },
    {
        "loss": 2.0859,
        "grad_norm": 2.5263171195983887,
        "learning_rate": 3.7567118651735898e-06,
        "epoch": 0.6261272867817573,
        "step": 4860
    },
    {
        "loss": 1.43,
        "grad_norm": 3.3614752292633057,
        "learning_rate": 3.7336133698546883e-06,
        "epoch": 0.6262561195568153,
        "step": 4861
    },
    {
        "loss": 2.0067,
        "grad_norm": 2.8086464405059814,
        "learning_rate": 3.7105833508966513e-06,
        "epoch": 0.6263849523318732,
        "step": 4862
    },
    {
        "loss": 2.0554,
        "grad_norm": 2.358752489089966,
        "learning_rate": 3.6876218423849806e-06,
        "epoch": 0.6265137851069312,
        "step": 4863
    },
    {
        "loss": 1.8478,
        "grad_norm": 2.4790518283843994,
        "learning_rate": 3.6647288783037526e-06,
        "epoch": 0.6266426178819892,
        "step": 4864
    },
    {
        "loss": 2.4543,
        "grad_norm": 2.0280609130859375,
        "learning_rate": 3.641904492535608e-06,
        "epoch": 0.6267714506570472,
        "step": 4865
    },
    {
        "loss": 2.2687,
        "grad_norm": 2.084994316101074,
        "learning_rate": 3.6191487188616258e-06,
        "epoch": 0.6269002834321051,
        "step": 4866
    },
    {
        "loss": 1.0305,
        "grad_norm": 2.3213369846343994,
        "learning_rate": 3.596461590961464e-06,
        "epoch": 0.6270291162071631,
        "step": 4867
    },
    {
        "loss": 2.2313,
        "grad_norm": 1.5819205045700073,
        "learning_rate": 3.573843142413069e-06,
        "epoch": 0.6271579489822211,
        "step": 4868
    },
    {
        "loss": 1.5684,
        "grad_norm": 2.598789930343628,
        "learning_rate": 3.5512934066927626e-06,
        "epoch": 0.627286781757279,
        "step": 4869
    },
    {
        "loss": 0.4665,
        "grad_norm": 1.3762223720550537,
        "learning_rate": 3.5288124171751923e-06,
        "epoch": 0.6274156145323371,
        "step": 4870
    },
    {
        "loss": 2.1017,
        "grad_norm": 1.7935972213745117,
        "learning_rate": 3.50640020713327e-06,
        "epoch": 0.627544447307395,
        "step": 4871
    },
    {
        "loss": 2.2617,
        "grad_norm": 2.002384662628174,
        "learning_rate": 3.4840568097380623e-06,
        "epoch": 0.6276732800824529,
        "step": 4872
    },
    {
        "loss": 2.1068,
        "grad_norm": 2.0290427207946777,
        "learning_rate": 3.4617822580588098e-06,
        "epoch": 0.627802112857511,
        "step": 4873
    },
    {
        "loss": 1.6917,
        "grad_norm": 2.3975179195404053,
        "learning_rate": 3.4395765850628703e-06,
        "epoch": 0.6279309456325689,
        "step": 4874
    },
    {
        "loss": 2.1247,
        "grad_norm": 2.712599754333496,
        "learning_rate": 3.417439823615676e-06,
        "epoch": 0.628059778407627,
        "step": 4875
    },
    {
        "loss": 2.163,
        "grad_norm": 1.6202671527862549,
        "learning_rate": 3.395372006480613e-06,
        "epoch": 0.6281886111826849,
        "step": 4876
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.041128396987915,
        "learning_rate": 3.373373166319066e-06,
        "epoch": 0.6283174439577428,
        "step": 4877
    },
    {
        "loss": 2.4062,
        "grad_norm": 1.329463005065918,
        "learning_rate": 3.3514433356903297e-06,
        "epoch": 0.6284462767328008,
        "step": 4878
    },
    {
        "loss": 2.2631,
        "grad_norm": 2.030280590057373,
        "learning_rate": 3.3295825470515574e-06,
        "epoch": 0.6285751095078588,
        "step": 4879
    },
    {
        "loss": 2.559,
        "grad_norm": 1.8128715753555298,
        "learning_rate": 3.3077908327576846e-06,
        "epoch": 0.6287039422829168,
        "step": 4880
    },
    {
        "loss": 1.8517,
        "grad_norm": 2.3458940982818604,
        "learning_rate": 3.286068225061456e-06,
        "epoch": 0.6288327750579747,
        "step": 4881
    },
    {
        "loss": 2.1661,
        "grad_norm": 1.3191590309143066,
        "learning_rate": 3.2644147561133374e-06,
        "epoch": 0.6289616078330327,
        "step": 4882
    },
    {
        "loss": 1.6739,
        "grad_norm": 2.312944173812866,
        "learning_rate": 3.2428304579614044e-06,
        "epoch": 0.6290904406080907,
        "step": 4883
    },
    {
        "loss": 2.2458,
        "grad_norm": 1.9525240659713745,
        "learning_rate": 3.2213153625514523e-06,
        "epoch": 0.6292192733831486,
        "step": 4884
    },
    {
        "loss": 1.7648,
        "grad_norm": 2.82108736038208,
        "learning_rate": 3.1998695017267986e-06,
        "epoch": 0.6293481061582067,
        "step": 4885
    },
    {
        "loss": 1.2703,
        "grad_norm": 3.3743507862091064,
        "learning_rate": 3.17849290722827e-06,
        "epoch": 0.6294769389332646,
        "step": 4886
    },
    {
        "loss": 2.2456,
        "grad_norm": 1.9291129112243652,
        "learning_rate": 3.157185610694202e-06,
        "epoch": 0.6296057717083225,
        "step": 4887
    },
    {
        "loss": 2.1178,
        "grad_norm": 1.9378153085708618,
        "learning_rate": 3.1359476436603753e-06,
        "epoch": 0.6297346044833806,
        "step": 4888
    },
    {
        "loss": 2.0698,
        "grad_norm": 2.472242832183838,
        "learning_rate": 3.1147790375599615e-06,
        "epoch": 0.6298634372584385,
        "step": 4889
    },
    {
        "loss": 2.7658,
        "grad_norm": 2.0938358306884766,
        "learning_rate": 3.09367982372345e-06,
        "epoch": 0.6299922700334966,
        "step": 4890
    },
    {
        "loss": 2.0479,
        "grad_norm": 2.247159004211426,
        "learning_rate": 3.072650033378616e-06,
        "epoch": 0.6301211028085545,
        "step": 4891
    },
    {
        "loss": 2.2339,
        "grad_norm": 2.271824836730957,
        "learning_rate": 3.0516896976505172e-06,
        "epoch": 0.6302499355836124,
        "step": 4892
    },
    {
        "loss": 2.0957,
        "grad_norm": 1.7623063325881958,
        "learning_rate": 3.0307988475614448e-06,
        "epoch": 0.6303787683586705,
        "step": 4893
    },
    {
        "loss": 1.9822,
        "grad_norm": 2.071913003921509,
        "learning_rate": 3.0099775140307705e-06,
        "epoch": 0.6305076011337284,
        "step": 4894
    },
    {
        "loss": 2.0423,
        "grad_norm": 2.4538795948028564,
        "learning_rate": 2.9892257278750222e-06,
        "epoch": 0.6306364339087864,
        "step": 4895
    },
    {
        "loss": 1.8704,
        "grad_norm": 1.5473198890686035,
        "learning_rate": 2.9685435198078094e-06,
        "epoch": 0.6307652666838444,
        "step": 4896
    },
    {
        "loss": 2.1106,
        "grad_norm": 1.8079111576080322,
        "learning_rate": 2.947930920439729e-06,
        "epoch": 0.6308940994589023,
        "step": 4897
    },
    {
        "loss": 2.1236,
        "grad_norm": 1.4899029731750488,
        "learning_rate": 2.927387960278355e-06,
        "epoch": 0.6310229322339603,
        "step": 4898
    },
    {
        "loss": 1.3392,
        "grad_norm": 3.4981424808502197,
        "learning_rate": 2.9069146697282114e-06,
        "epoch": 0.6311517650090183,
        "step": 4899
    },
    {
        "loss": 1.8665,
        "grad_norm": 1.7587755918502808,
        "learning_rate": 2.886511079090709e-06,
        "epoch": 0.6312805977840763,
        "step": 4900
    },
    {
        "loss": 1.8796,
        "grad_norm": 1.8441787958145142,
        "learning_rate": 2.8661772185640866e-06,
        "epoch": 0.6314094305591342,
        "step": 4901
    },
    {
        "loss": 2.1551,
        "grad_norm": 1.8532779216766357,
        "learning_rate": 2.845913118243393e-06,
        "epoch": 0.6315382633341923,
        "step": 4902
    },
    {
        "loss": 1.917,
        "grad_norm": 3.7733898162841797,
        "learning_rate": 2.8257188081204256e-06,
        "epoch": 0.6316670961092502,
        "step": 4903
    },
    {
        "loss": 1.8289,
        "grad_norm": 2.571922779083252,
        "learning_rate": 2.805594318083665e-06,
        "epoch": 0.6317959288843081,
        "step": 4904
    },
    {
        "loss": 1.6997,
        "grad_norm": 2.8046486377716064,
        "learning_rate": 2.7855396779182698e-06,
        "epoch": 0.6319247616593662,
        "step": 4905
    },
    {
        "loss": 1.9373,
        "grad_norm": 2.0908589363098145,
        "learning_rate": 2.7655549173060348e-06,
        "epoch": 0.6320535944344241,
        "step": 4906
    },
    {
        "loss": 1.8833,
        "grad_norm": 2.1386196613311768,
        "learning_rate": 2.7456400658253332e-06,
        "epoch": 0.6321824272094821,
        "step": 4907
    },
    {
        "loss": 1.8812,
        "grad_norm": 1.659326434135437,
        "learning_rate": 2.7257951529510274e-06,
        "epoch": 0.6323112599845401,
        "step": 4908
    },
    {
        "loss": 1.9375,
        "grad_norm": 1.5367369651794434,
        "learning_rate": 2.706020208054466e-06,
        "epoch": 0.632440092759598,
        "step": 4909
    },
    {
        "loss": 1.7287,
        "grad_norm": 2.3841066360473633,
        "learning_rate": 2.6863152604035323e-06,
        "epoch": 0.632568925534656,
        "step": 4910
    },
    {
        "loss": 1.9596,
        "grad_norm": 2.6741833686828613,
        "learning_rate": 2.666680339162425e-06,
        "epoch": 0.632697758309714,
        "step": 4911
    },
    {
        "loss": 1.9663,
        "grad_norm": 2.6239356994628906,
        "learning_rate": 2.6471154733916983e-06,
        "epoch": 0.632826591084772,
        "step": 4912
    },
    {
        "loss": 1.6278,
        "grad_norm": 3.193328380584717,
        "learning_rate": 2.6276206920482603e-06,
        "epoch": 0.63295542385983,
        "step": 4913
    },
    {
        "loss": 2.3047,
        "grad_norm": 2.8467626571655273,
        "learning_rate": 2.6081960239852822e-06,
        "epoch": 0.6330842566348879,
        "step": 4914
    },
    {
        "loss": 1.6946,
        "grad_norm": 1.674752116203308,
        "learning_rate": 2.588841497952166e-06,
        "epoch": 0.6332130894099459,
        "step": 4915
    },
    {
        "loss": 2.3935,
        "grad_norm": 0.9784587621688843,
        "learning_rate": 2.569557142594459e-06,
        "epoch": 0.6333419221850038,
        "step": 4916
    },
    {
        "loss": 2.3406,
        "grad_norm": 2.098738670349121,
        "learning_rate": 2.550342986453902e-06,
        "epoch": 0.6334707549600619,
        "step": 4917
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.627414345741272,
        "learning_rate": 2.5311990579683364e-06,
        "epoch": 0.6335995877351198,
        "step": 4918
    },
    {
        "loss": 2.2114,
        "grad_norm": 1.3265016078948975,
        "learning_rate": 2.5121253854716365e-06,
        "epoch": 0.6337284205101777,
        "step": 4919
    },
    {
        "loss": 1.5469,
        "grad_norm": 2.8488850593566895,
        "learning_rate": 2.493121997193709e-06,
        "epoch": 0.6338572532852358,
        "step": 4920
    },
    {
        "loss": 2.0897,
        "grad_norm": 1.7243155241012573,
        "learning_rate": 2.474188921260445e-06,
        "epoch": 0.6339860860602937,
        "step": 4921
    },
    {
        "loss": 1.9332,
        "grad_norm": 2.6884284019470215,
        "learning_rate": 2.4553261856936404e-06,
        "epoch": 0.6341149188353518,
        "step": 4922
    },
    {
        "loss": 2.0605,
        "grad_norm": 1.4912467002868652,
        "learning_rate": 2.4365338184110074e-06,
        "epoch": 0.6342437516104097,
        "step": 4923
    },
    {
        "loss": 2.2737,
        "grad_norm": 2.033399820327759,
        "learning_rate": 2.4178118472261144e-06,
        "epoch": 0.6343725843854676,
        "step": 4924
    },
    {
        "loss": 2.3486,
        "grad_norm": 2.3510119915008545,
        "learning_rate": 2.399160299848335e-06,
        "epoch": 0.6345014171605257,
        "step": 4925
    },
    {
        "loss": 2.3526,
        "grad_norm": 2.3849027156829834,
        "learning_rate": 2.380579203882788e-06,
        "epoch": 0.6346302499355836,
        "step": 4926
    },
    {
        "loss": 1.9886,
        "grad_norm": 2.7441654205322266,
        "learning_rate": 2.362068586830363e-06,
        "epoch": 0.6347590827106416,
        "step": 4927
    },
    {
        "loss": 1.7563,
        "grad_norm": 3.308645248413086,
        "learning_rate": 2.3436284760876224e-06,
        "epoch": 0.6348879154856996,
        "step": 4928
    },
    {
        "loss": 2.1415,
        "grad_norm": 2.9291090965270996,
        "learning_rate": 2.325258898946758e-06,
        "epoch": 0.6350167482607575,
        "step": 4929
    },
    {
        "loss": 1.9336,
        "grad_norm": 2.059157133102417,
        "learning_rate": 2.3069598825955773e-06,
        "epoch": 0.6351455810358155,
        "step": 4930
    },
    {
        "loss": 1.5871,
        "grad_norm": 2.6466753482818604,
        "learning_rate": 2.28873145411746e-06,
        "epoch": 0.6352744138108735,
        "step": 4931
    },
    {
        "loss": 1.4631,
        "grad_norm": 3.1386332511901855,
        "learning_rate": 2.270573640491325e-06,
        "epoch": 0.6354032465859315,
        "step": 4932
    },
    {
        "loss": 2.0257,
        "grad_norm": 1.9239863157272339,
        "learning_rate": 2.2524864685915526e-06,
        "epoch": 0.6355320793609894,
        "step": 4933
    },
    {
        "loss": 2.2521,
        "grad_norm": 1.9810130596160889,
        "learning_rate": 2.234469965187952e-06,
        "epoch": 0.6356609121360474,
        "step": 4934
    },
    {
        "loss": 2.105,
        "grad_norm": 2.8729088306427,
        "learning_rate": 2.2165241569458315e-06,
        "epoch": 0.6357897449111054,
        "step": 4935
    },
    {
        "loss": 2.3003,
        "grad_norm": 2.2014904022216797,
        "learning_rate": 2.1986490704257733e-06,
        "epoch": 0.6359185776861633,
        "step": 4936
    },
    {
        "loss": 1.9475,
        "grad_norm": 1.6363842487335205,
        "learning_rate": 2.180844732083709e-06,
        "epoch": 0.6360474104612214,
        "step": 4937
    },
    {
        "loss": 2.0766,
        "grad_norm": 2.370424270629883,
        "learning_rate": 2.163111168270887e-06,
        "epoch": 0.6361762432362793,
        "step": 4938
    },
    {
        "loss": 2.2792,
        "grad_norm": 1.396783471107483,
        "learning_rate": 2.1454484052337963e-06,
        "epoch": 0.6363050760113372,
        "step": 4939
    },
    {
        "loss": 1.9024,
        "grad_norm": 1.3654779195785522,
        "learning_rate": 2.1278564691141203e-06,
        "epoch": 0.6364339087863953,
        "step": 4940
    },
    {
        "loss": 1.9253,
        "grad_norm": 2.2566771507263184,
        "learning_rate": 2.110335385948714e-06,
        "epoch": 0.6365627415614532,
        "step": 4941
    },
    {
        "loss": 2.0009,
        "grad_norm": 2.2177176475524902,
        "learning_rate": 2.0928851816695904e-06,
        "epoch": 0.6366915743365112,
        "step": 4942
    },
    {
        "loss": 2.2188,
        "grad_norm": 1.8001290559768677,
        "learning_rate": 2.0755058821038507e-06,
        "epoch": 0.6368204071115692,
        "step": 4943
    },
    {
        "loss": 1.7357,
        "grad_norm": 2.151526927947998,
        "learning_rate": 2.0581975129736473e-06,
        "epoch": 0.6369492398866271,
        "step": 4944
    },
    {
        "loss": 1.744,
        "grad_norm": 2.240642547607422,
        "learning_rate": 2.040960099896155e-06,
        "epoch": 0.6370780726616851,
        "step": 4945
    },
    {
        "loss": 2.3065,
        "grad_norm": 1.4350587129592896,
        "learning_rate": 2.023793668383539e-06,
        "epoch": 0.6372069054367431,
        "step": 4946
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.4253146648406982,
        "learning_rate": 2.006698243842897e-06,
        "epoch": 0.6373357382118011,
        "step": 4947
    },
    {
        "loss": 2.1924,
        "grad_norm": 2.1501095294952393,
        "learning_rate": 1.9896738515762237e-06,
        "epoch": 0.637464570986859,
        "step": 4948
    },
    {
        "loss": 2.2875,
        "grad_norm": 1.3093397617340088,
        "learning_rate": 1.972720516780402e-06,
        "epoch": 0.637593403761917,
        "step": 4949
    },
    {
        "loss": 2.577,
        "grad_norm": 1.4893375635147095,
        "learning_rate": 1.9558382645471497e-06,
        "epoch": 0.637722236536975,
        "step": 4950
    },
    {
        "loss": 1.8348,
        "grad_norm": 2.0641698837280273,
        "learning_rate": 1.9390271198629464e-06,
        "epoch": 0.637851069312033,
        "step": 4951
    },
    {
        "loss": 1.9418,
        "grad_norm": 1.701564908027649,
        "learning_rate": 1.9222871076090774e-06,
        "epoch": 0.637979902087091,
        "step": 4952
    },
    {
        "loss": 1.687,
        "grad_norm": 1.782712459564209,
        "learning_rate": 1.9056182525615252e-06,
        "epoch": 0.6381087348621489,
        "step": 4953
    },
    {
        "loss": 2.2282,
        "grad_norm": 2.7308616638183594,
        "learning_rate": 1.8890205793909377e-06,
        "epoch": 0.638237567637207,
        "step": 4954
    },
    {
        "loss": 2.0821,
        "grad_norm": 2.02024507522583,
        "learning_rate": 1.8724941126626316e-06,
        "epoch": 0.6383664004122649,
        "step": 4955
    },
    {
        "loss": 1.282,
        "grad_norm": 3.2902753353118896,
        "learning_rate": 1.85603887683653e-06,
        "epoch": 0.6384952331873228,
        "step": 4956
    },
    {
        "loss": 2.2505,
        "grad_norm": 2.8090596199035645,
        "learning_rate": 1.8396548962671512e-06,
        "epoch": 0.6386240659623809,
        "step": 4957
    },
    {
        "loss": 2.247,
        "grad_norm": 1.1568551063537598,
        "learning_rate": 1.8233421952035145e-06,
        "epoch": 0.6387528987374388,
        "step": 4958
    },
    {
        "loss": 1.4452,
        "grad_norm": 1.9318636655807495,
        "learning_rate": 1.8071007977891574e-06,
        "epoch": 0.6388817315124968,
        "step": 4959
    },
    {
        "loss": 1.4101,
        "grad_norm": 1.9448500871658325,
        "learning_rate": 1.790930728062079e-06,
        "epoch": 0.6390105642875548,
        "step": 4960
    },
    {
        "loss": 1.771,
        "grad_norm": 3.035371780395508,
        "learning_rate": 1.7748320099547576e-06,
        "epoch": 0.6391393970626127,
        "step": 4961
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.752037525177002,
        "learning_rate": 1.7588046672940006e-06,
        "epoch": 0.6392682298376707,
        "step": 4962
    },
    {
        "loss": 2.088,
        "grad_norm": 2.3457791805267334,
        "learning_rate": 1.7428487238010106e-06,
        "epoch": 0.6393970626127287,
        "step": 4963
    },
    {
        "loss": 1.958,
        "grad_norm": 1.867544412612915,
        "learning_rate": 1.7269642030913135e-06,
        "epoch": 0.6395258953877867,
        "step": 4964
    },
    {
        "loss": 1.1318,
        "grad_norm": 2.1419904232025146,
        "learning_rate": 1.7111511286747373e-06,
        "epoch": 0.6396547281628446,
        "step": 4965
    },
    {
        "loss": 2.037,
        "grad_norm": 2.3533475399017334,
        "learning_rate": 1.6954095239553215e-06,
        "epoch": 0.6397835609379026,
        "step": 4966
    },
    {
        "loss": 2.3227,
        "grad_norm": 1.194403886795044,
        "learning_rate": 1.6797394122313791e-06,
        "epoch": 0.6399123937129606,
        "step": 4967
    },
    {
        "loss": 2.0493,
        "grad_norm": 2.223435401916504,
        "learning_rate": 1.6641408166953977e-06,
        "epoch": 0.6400412264880185,
        "step": 4968
    },
    {
        "loss": 2.168,
        "grad_norm": 1.6454781293869019,
        "learning_rate": 1.6486137604339758e-06,
        "epoch": 0.6401700592630766,
        "step": 4969
    },
    {
        "loss": 1.7777,
        "grad_norm": 2.6926379203796387,
        "learning_rate": 1.6331582664279032e-06,
        "epoch": 0.6402988920381345,
        "step": 4970
    },
    {
        "loss": 2.1567,
        "grad_norm": 1.669799566268921,
        "learning_rate": 1.6177743575520044e-06,
        "epoch": 0.6404277248131924,
        "step": 4971
    },
    {
        "loss": 1.8941,
        "grad_norm": 2.826523542404175,
        "learning_rate": 1.6024620565751548e-06,
        "epoch": 0.6405565575882505,
        "step": 4972
    },
    {
        "loss": 1.4033,
        "grad_norm": 3.506758451461792,
        "learning_rate": 1.5872213861602592e-06,
        "epoch": 0.6406853903633084,
        "step": 4973
    },
    {
        "loss": 1.7948,
        "grad_norm": 2.0369186401367188,
        "learning_rate": 1.5720523688642186e-06,
        "epoch": 0.6408142231383664,
        "step": 4974
    },
    {
        "loss": 1.7912,
        "grad_norm": 1.9393408298492432,
        "learning_rate": 1.5569550271378575e-06,
        "epoch": 0.6409430559134244,
        "step": 4975
    },
    {
        "loss": 1.9695,
        "grad_norm": 2.3987674713134766,
        "learning_rate": 1.541929383325935e-06,
        "epoch": 0.6410718886884823,
        "step": 4976
    },
    {
        "loss": 2.132,
        "grad_norm": 2.5848124027252197,
        "learning_rate": 1.5269754596670628e-06,
        "epoch": 0.6412007214635403,
        "step": 4977
    },
    {
        "loss": 1.3104,
        "grad_norm": 3.303400993347168,
        "learning_rate": 1.5120932782937746e-06,
        "epoch": 0.6413295542385983,
        "step": 4978
    },
    {
        "loss": 2.118,
        "grad_norm": 1.8027303218841553,
        "learning_rate": 1.4972828612323575e-06,
        "epoch": 0.6414583870136563,
        "step": 4979
    },
    {
        "loss": 2.1334,
        "grad_norm": 2.2799906730651855,
        "learning_rate": 1.4825442304028937e-06,
        "epoch": 0.6415872197887142,
        "step": 4980
    },
    {
        "loss": 1.4426,
        "grad_norm": 3.4690349102020264,
        "learning_rate": 1.4678774076192337e-06,
        "epoch": 0.6417160525637722,
        "step": 4981
    },
    {
        "loss": 2.2923,
        "grad_norm": 1.824745535850525,
        "learning_rate": 1.453282414588958e-06,
        "epoch": 0.6418448853388302,
        "step": 4982
    },
    {
        "loss": 2.2092,
        "grad_norm": 1.92579185962677,
        "learning_rate": 1.4387592729133037e-06,
        "epoch": 0.6419737181138881,
        "step": 4983
    },
    {
        "loss": 2.3588,
        "grad_norm": 1.5329042673110962,
        "learning_rate": 1.4243080040871882e-06,
        "epoch": 0.6421025508889462,
        "step": 4984
    },
    {
        "loss": 2.1126,
        "grad_norm": 1.9663134813308716,
        "learning_rate": 1.4099286294991466e-06,
        "epoch": 0.6422313836640041,
        "step": 4985
    },
    {
        "loss": 2.4375,
        "grad_norm": 1.7520331144332886,
        "learning_rate": 1.3956211704313104e-06,
        "epoch": 0.642360216439062,
        "step": 4986
    },
    {
        "loss": 2.3921,
        "grad_norm": 2.0396909713745117,
        "learning_rate": 1.381385648059369e-06,
        "epoch": 0.6424890492141201,
        "step": 4987
    },
    {
        "loss": 1.8307,
        "grad_norm": 2.754920721054077,
        "learning_rate": 1.3672220834525462e-06,
        "epoch": 0.642617881989178,
        "step": 4988
    },
    {
        "loss": 2.4199,
        "grad_norm": 2.571031093597412,
        "learning_rate": 1.3531304975735737e-06,
        "epoch": 0.6427467147642361,
        "step": 4989
    },
    {
        "loss": 1.4352,
        "grad_norm": 2.8182315826416016,
        "learning_rate": 1.3391109112786348e-06,
        "epoch": 0.642875547539294,
        "step": 4990
    },
    {
        "loss": 1.2033,
        "grad_norm": 3.0690243244171143,
        "learning_rate": 1.3251633453173372e-06,
        "epoch": 0.6430043803143519,
        "step": 4991
    },
    {
        "loss": 0.906,
        "grad_norm": 3.2034518718719482,
        "learning_rate": 1.311287820332735e-06,
        "epoch": 0.64313321308941,
        "step": 4992
    },
    {
        "loss": 2.0832,
        "grad_norm": 1.9300264120101929,
        "learning_rate": 1.2974843568612282e-06,
        "epoch": 0.6432620458644679,
        "step": 4993
    },
    {
        "loss": 2.0229,
        "grad_norm": 2.0835342407226562,
        "learning_rate": 1.2837529753325694e-06,
        "epoch": 0.6433908786395259,
        "step": 4994
    },
    {
        "loss": 2.064,
        "grad_norm": 2.0850045680999756,
        "learning_rate": 1.2700936960698405e-06,
        "epoch": 0.6435197114145839,
        "step": 4995
    },
    {
        "loss": 0.9675,
        "grad_norm": 3.375422477722168,
        "learning_rate": 1.2565065392893982e-06,
        "epoch": 0.6436485441896418,
        "step": 4996
    },
    {
        "loss": 2.2973,
        "grad_norm": 2.0211753845214844,
        "learning_rate": 1.2429915251008506e-06,
        "epoch": 0.6437773769646998,
        "step": 4997
    },
    {
        "loss": 2.3739,
        "grad_norm": 2.03920841217041,
        "learning_rate": 1.229548673507025e-06,
        "epoch": 0.6439062097397578,
        "step": 4998
    },
    {
        "loss": 2.2293,
        "grad_norm": 1.9569778442382812,
        "learning_rate": 1.216178004403956e-06,
        "epoch": 0.6440350425148158,
        "step": 4999
    },
    {
        "loss": 1.9356,
        "grad_norm": 1.2356760501861572,
        "learning_rate": 1.2028795375808587e-06,
        "epoch": 0.6441638752898737,
        "step": 5000
    },
    {
        "loss": 2.3501,
        "grad_norm": 1.4328619241714478,
        "learning_rate": 1.1896532927200555e-06,
        "epoch": 0.6442927080649318,
        "step": 5001
    },
    {
        "loss": 1.9709,
        "grad_norm": 3.244136333465576,
        "learning_rate": 1.1764992893969928e-06,
        "epoch": 0.6444215408399897,
        "step": 5002
    },
    {
        "loss": 1.3247,
        "grad_norm": 2.7717056274414062,
        "learning_rate": 1.1634175470801922e-06,
        "epoch": 0.6445503736150476,
        "step": 5003
    },
    {
        "loss": 1.4597,
        "grad_norm": 2.9699108600616455,
        "learning_rate": 1.1504080851312438e-06,
        "epoch": 0.6446792063901057,
        "step": 5004
    },
    {
        "loss": 2.3006,
        "grad_norm": 1.531005859375,
        "learning_rate": 1.1374709228047286e-06,
        "epoch": 0.6448080391651636,
        "step": 5005
    },
    {
        "loss": 2.4128,
        "grad_norm": 2.1679608821868896,
        "learning_rate": 1.1246060792482416e-06,
        "epoch": 0.6449368719402216,
        "step": 5006
    },
    {
        "loss": 2.0812,
        "grad_norm": 2.8900082111358643,
        "learning_rate": 1.1118135735023516e-06,
        "epoch": 0.6450657047152796,
        "step": 5007
    },
    {
        "loss": 2.2527,
        "grad_norm": 1.8889214992523193,
        "learning_rate": 1.0990934245005358e-06,
        "epoch": 0.6451945374903375,
        "step": 5008
    },
    {
        "loss": 1.6879,
        "grad_norm": 2.5456044673919678,
        "learning_rate": 1.0864456510691846e-06,
        "epoch": 0.6453233702653955,
        "step": 5009
    },
    {
        "loss": 1.6659,
        "grad_norm": 1.703578233718872,
        "learning_rate": 1.0738702719275907e-06,
        "epoch": 0.6454522030404535,
        "step": 5010
    },
    {
        "loss": 2.2044,
        "grad_norm": 1.6859639883041382,
        "learning_rate": 1.0613673056878882e-06,
        "epoch": 0.6455810358155115,
        "step": 5011
    },
    {
        "loss": 2.1766,
        "grad_norm": 2.2520012855529785,
        "learning_rate": 1.0489367708550412e-06,
        "epoch": 0.6457098685905694,
        "step": 5012
    },
    {
        "loss": 1.9345,
        "grad_norm": 2.7806320190429688,
        "learning_rate": 1.0365786858268001e-06,
        "epoch": 0.6458387013656274,
        "step": 5013
    },
    {
        "loss": 1.8012,
        "grad_norm": 3.070996046066284,
        "learning_rate": 1.0242930688937002e-06,
        "epoch": 0.6459675341406854,
        "step": 5014
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.9628461599349976,
        "learning_rate": 1.0120799382390133e-06,
        "epoch": 0.6460963669157433,
        "step": 5015
    },
    {
        "loss": 2.5348,
        "grad_norm": 1.6356383562088013,
        "learning_rate": 9.999393119387246e-07,
        "epoch": 0.6462251996908014,
        "step": 5016
    },
    {
        "loss": 1.9232,
        "grad_norm": 2.0544567108154297,
        "learning_rate": 9.87871207961516e-07,
        "epoch": 0.6463540324658593,
        "step": 5017
    },
    {
        "loss": 1.7357,
        "grad_norm": 2.4496076107025146,
        "learning_rate": 9.75875644168739e-07,
        "epoch": 0.6464828652409172,
        "step": 5018
    },
    {
        "loss": 2.9135,
        "grad_norm": 1.8733978271484375,
        "learning_rate": 9.639526383143692e-07,
        "epoch": 0.6466116980159753,
        "step": 5019
    },
    {
        "loss": 2.0965,
        "grad_norm": 2.3047914505004883,
        "learning_rate": 9.521022080449748e-07,
        "epoch": 0.6467405307910332,
        "step": 5020
    },
    {
        "loss": 2.6202,
        "grad_norm": 1.4687135219573975,
        "learning_rate": 9.403243708997811e-07,
        "epoch": 0.6468693635660913,
        "step": 5021
    },
    {
        "loss": 2.5413,
        "grad_norm": 1.699979543685913,
        "learning_rate": 9.286191443104941e-07,
        "epoch": 0.6469981963411492,
        "step": 5022
    },
    {
        "loss": 2.338,
        "grad_norm": 2.109441041946411,
        "learning_rate": 9.169865456013782e-07,
        "epoch": 0.6471270291162071,
        "step": 5023
    },
    {
        "loss": 2.1605,
        "grad_norm": 1.3365685939788818,
        "learning_rate": 9.05426591989228e-07,
        "epoch": 0.6472558618912652,
        "step": 5024
    },
    {
        "loss": 2.2095,
        "grad_norm": 2.212510108947754,
        "learning_rate": 8.939393005832908e-07,
        "epoch": 0.6473846946663231,
        "step": 5025
    },
    {
        "loss": 1.5639,
        "grad_norm": 2.580867290496826,
        "learning_rate": 8.825246883852889e-07,
        "epoch": 0.6475135274413811,
        "step": 5026
    },
    {
        "loss": 1.3586,
        "grad_norm": 2.3810231685638428,
        "learning_rate": 8.711827722893529e-07,
        "epoch": 0.6476423602164391,
        "step": 5027
    },
    {
        "loss": 2.2181,
        "grad_norm": 1.8415956497192383,
        "learning_rate": 8.599135690820492e-07,
        "epoch": 0.647771192991497,
        "step": 5028
    },
    {
        "loss": 1.5977,
        "grad_norm": 3.5246994495391846,
        "learning_rate": 8.487170954423029e-07,
        "epoch": 0.647900025766555,
        "step": 5029
    },
    {
        "loss": 2.2082,
        "grad_norm": 1.2283210754394531,
        "learning_rate": 8.375933679414138e-07,
        "epoch": 0.648028858541613,
        "step": 5030
    },
    {
        "loss": 2.0162,
        "grad_norm": 2.0974111557006836,
        "learning_rate": 8.265424030430013e-07,
        "epoch": 0.648157691316671,
        "step": 5031
    },
    {
        "loss": 0.526,
        "grad_norm": 1.3572410345077515,
        "learning_rate": 8.155642171030043e-07,
        "epoch": 0.6482865240917289,
        "step": 5032
    },
    {
        "loss": 2.0478,
        "grad_norm": 2.4301531314849854,
        "learning_rate": 8.046588263696308e-07,
        "epoch": 0.6484153568667869,
        "step": 5033
    },
    {
        "loss": 1.089,
        "grad_norm": 2.7755963802337646,
        "learning_rate": 7.938262469833535e-07,
        "epoch": 0.6485441896418449,
        "step": 5034
    },
    {
        "loss": 1.9006,
        "grad_norm": 1.5717034339904785,
        "learning_rate": 7.830664949768918e-07,
        "epoch": 0.6486730224169028,
        "step": 5035
    },
    {
        "loss": 1.9845,
        "grad_norm": 1.9049384593963623,
        "learning_rate": 7.723795862751681e-07,
        "epoch": 0.6488018551919609,
        "step": 5036
    },
    {
        "loss": 2.4658,
        "grad_norm": 1.490976333618164,
        "learning_rate": 7.617655366952747e-07,
        "epoch": 0.6489306879670188,
        "step": 5037
    },
    {
        "loss": 2.0923,
        "grad_norm": 2.705068588256836,
        "learning_rate": 7.512243619465176e-07,
        "epoch": 0.6490595207420767,
        "step": 5038
    },
    {
        "loss": 2.3947,
        "grad_norm": 1.5705188512802124,
        "learning_rate": 7.407560776303058e-07,
        "epoch": 0.6491883535171348,
        "step": 5039
    },
    {
        "loss": 1.6526,
        "grad_norm": 3.0417256355285645,
        "learning_rate": 7.30360699240179e-07,
        "epoch": 0.6493171862921927,
        "step": 5040
    },
    {
        "loss": 2.132,
        "grad_norm": 1.414363980293274,
        "learning_rate": 7.20038242161758e-07,
        "epoch": 0.6494460190672507,
        "step": 5041
    },
    {
        "loss": 1.952,
        "grad_norm": 2.6750540733337402,
        "learning_rate": 7.097887216727606e-07,
        "epoch": 0.6495748518423087,
        "step": 5042
    },
    {
        "loss": 2.0654,
        "grad_norm": 1.5669969320297241,
        "learning_rate": 6.996121529429411e-07,
        "epoch": 0.6497036846173666,
        "step": 5043
    },
    {
        "loss": 0.5303,
        "grad_norm": 1.5930633544921875,
        "learning_rate": 6.89508551034096e-07,
        "epoch": 0.6498325173924246,
        "step": 5044
    },
    {
        "loss": 1.327,
        "grad_norm": 2.4667534828186035,
        "learning_rate": 6.794779308999965e-07,
        "epoch": 0.6499613501674826,
        "step": 5045
    },
    {
        "loss": 2.2812,
        "grad_norm": 1.4273338317871094,
        "learning_rate": 6.695203073864231e-07,
        "epoch": 0.6500901829425406,
        "step": 5046
    },
    {
        "loss": 1.945,
        "grad_norm": 1.4857168197631836,
        "learning_rate": 6.596356952311255e-07,
        "epoch": 0.6502190157175985,
        "step": 5047
    },
    {
        "loss": 2.2572,
        "grad_norm": 2.3177831172943115,
        "learning_rate": 6.498241090637735e-07,
        "epoch": 0.6503478484926565,
        "step": 5048
    },
    {
        "loss": 2.2034,
        "grad_norm": 1.791251540184021,
        "learning_rate": 6.400855634059566e-07,
        "epoch": 0.6504766812677145,
        "step": 5049
    },
    {
        "loss": 2.7393,
        "grad_norm": 1.3721671104431152,
        "learning_rate": 6.304200726711729e-07,
        "epoch": 0.6506055140427724,
        "step": 5050
    },
    {
        "loss": 2.1599,
        "grad_norm": 1.5305155515670776,
        "learning_rate": 6.208276511647903e-07,
        "epoch": 0.6507343468178305,
        "step": 5051
    },
    {
        "loss": 2.1805,
        "grad_norm": 2.245270013809204,
        "learning_rate": 6.113083130840136e-07,
        "epoch": 0.6508631795928884,
        "step": 5052
    },
    {
        "loss": 1.6987,
        "grad_norm": 1.288173794746399,
        "learning_rate": 6.018620725179169e-07,
        "epoch": 0.6509920123679465,
        "step": 5053
    },
    {
        "loss": 1.8417,
        "grad_norm": 2.0014567375183105,
        "learning_rate": 5.924889434473558e-07,
        "epoch": 0.6511208451430044,
        "step": 5054
    },
    {
        "loss": 1.9161,
        "grad_norm": 2.451936960220337,
        "learning_rate": 5.831889397449942e-07,
        "epoch": 0.6512496779180623,
        "step": 5055
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.972057819366455,
        "learning_rate": 5.73962075175255e-07,
        "epoch": 0.6513785106931204,
        "step": 5056
    },
    {
        "loss": 1.6721,
        "grad_norm": 2.3962864875793457,
        "learning_rate": 5.648083633943313e-07,
        "epoch": 0.6515073434681783,
        "step": 5057
    },
    {
        "loss": 2.5032,
        "grad_norm": 2.298461675643921,
        "learning_rate": 5.557278179501246e-07,
        "epoch": 0.6516361762432363,
        "step": 5058
    },
    {
        "loss": 2.1656,
        "grad_norm": 2.164818048477173,
        "learning_rate": 5.467204522822511e-07,
        "epoch": 0.6517650090182943,
        "step": 5059
    },
    {
        "loss": 1.4376,
        "grad_norm": 2.7241897583007812,
        "learning_rate": 5.377862797220246e-07,
        "epoch": 0.6518938417933522,
        "step": 5060
    },
    {
        "loss": 1.5621,
        "grad_norm": 1.7908990383148193,
        "learning_rate": 5.289253134924399e-07,
        "epoch": 0.6520226745684102,
        "step": 5061
    },
    {
        "loss": 2.0491,
        "grad_norm": 2.8990490436553955,
        "learning_rate": 5.201375667081176e-07,
        "epoch": 0.6521515073434682,
        "step": 5062
    },
    {
        "loss": 2.1798,
        "grad_norm": 2.1834423542022705,
        "learning_rate": 5.11423052375315e-07,
        "epoch": 0.6522803401185262,
        "step": 5063
    },
    {
        "loss": 1.463,
        "grad_norm": 3.0097572803497314,
        "learning_rate": 5.027817833919368e-07,
        "epoch": 0.6524091728935841,
        "step": 5064
    },
    {
        "loss": 2.0693,
        "grad_norm": 1.7637686729431152,
        "learning_rate": 4.942137725474416e-07,
        "epoch": 0.6525380056686421,
        "step": 5065
    },
    {
        "loss": 1.4209,
        "grad_norm": 2.0669772624969482,
        "learning_rate": 4.857190325228689e-07,
        "epoch": 0.6526668384437001,
        "step": 5066
    },
    {
        "loss": 2.3275,
        "grad_norm": 1.8505878448486328,
        "learning_rate": 4.772975758908338e-07,
        "epoch": 0.652795671218758,
        "step": 5067
    },
    {
        "loss": 2.0362,
        "grad_norm": 2.098440647125244,
        "learning_rate": 4.6894941511547163e-07,
        "epoch": 0.6529245039938161,
        "step": 5068
    },
    {
        "loss": 1.9704,
        "grad_norm": 2.2470381259918213,
        "learning_rate": 4.6067456255243223e-07,
        "epoch": 0.653053336768874,
        "step": 5069
    },
    {
        "loss": 2.535,
        "grad_norm": 2.1483750343322754,
        "learning_rate": 4.5247303044888554e-07,
        "epoch": 0.6531821695439319,
        "step": 5070
    },
    {
        "loss": 2.1679,
        "grad_norm": 1.873471736907959,
        "learning_rate": 4.4434483094346057e-07,
        "epoch": 0.65331100231899,
        "step": 5071
    },
    {
        "loss": 2.1935,
        "grad_norm": 1.6369531154632568,
        "learning_rate": 4.3628997606627865e-07,
        "epoch": 0.6534398350940479,
        "step": 5072
    },
    {
        "loss": 2.3267,
        "grad_norm": 2.1485610008239746,
        "learning_rate": 4.283084777388813e-07,
        "epoch": 0.653568667869106,
        "step": 5073
    },
    {
        "loss": 2.2292,
        "grad_norm": 1.299729347229004,
        "learning_rate": 4.2040034777425796e-07,
        "epoch": 0.6536975006441639,
        "step": 5074
    },
    {
        "loss": 1.6498,
        "grad_norm": 2.6245193481445312,
        "learning_rate": 4.1256559787679616e-07,
        "epoch": 0.6538263334192218,
        "step": 5075
    },
    {
        "loss": 2.0474,
        "grad_norm": 2.477961301803589,
        "learning_rate": 4.048042396422813e-07,
        "epoch": 0.6539551661942798,
        "step": 5076
    },
    {
        "loss": 2.0935,
        "grad_norm": 2.761949062347412,
        "learning_rate": 3.971162845578802e-07,
        "epoch": 0.6540839989693378,
        "step": 5077
    },
    {
        "loss": 1.7577,
        "grad_norm": 2.3450429439544678,
        "learning_rate": 3.8950174400211315e-07,
        "epoch": 0.6542128317443958,
        "step": 5078
    },
    {
        "loss": 1.8326,
        "grad_norm": 2.0101516246795654,
        "learning_rate": 3.819606292448541e-07,
        "epoch": 0.6543416645194537,
        "step": 5079
    },
    {
        "loss": 2.263,
        "grad_norm": 2.2742972373962402,
        "learning_rate": 3.744929514472806e-07,
        "epoch": 0.6544704972945117,
        "step": 5080
    },
    {
        "loss": 1.7268,
        "grad_norm": 2.959563970565796,
        "learning_rate": 3.6709872166191815e-07,
        "epoch": 0.6545993300695697,
        "step": 5081
    },
    {
        "loss": 2.0745,
        "grad_norm": 1.2580280303955078,
        "learning_rate": 3.5977795083255714e-07,
        "epoch": 0.6547281628446276,
        "step": 5082
    },
    {
        "loss": 1.5657,
        "grad_norm": 2.3098254203796387,
        "learning_rate": 3.5253064979426377e-07,
        "epoch": 0.6548569956196857,
        "step": 5083
    },
    {
        "loss": 2.1118,
        "grad_norm": 2.64846134185791,
        "learning_rate": 3.4535682927337443e-07,
        "epoch": 0.6549858283947436,
        "step": 5084
    },
    {
        "loss": 1.8175,
        "grad_norm": 2.38952898979187,
        "learning_rate": 3.382564998874793e-07,
        "epoch": 0.6551146611698015,
        "step": 5085
    },
    {
        "loss": 1.9592,
        "grad_norm": 2.887876510620117,
        "learning_rate": 3.3122967214538893e-07,
        "epoch": 0.6552434939448596,
        "step": 5086
    },
    {
        "loss": 1.686,
        "grad_norm": 2.196084976196289,
        "learning_rate": 3.2427635644712297e-07,
        "epoch": 0.6553723267199175,
        "step": 5087
    },
    {
        "loss": 1.4228,
        "grad_norm": 2.4517359733581543,
        "learning_rate": 3.1739656308389374e-07,
        "epoch": 0.6555011594949756,
        "step": 5088
    },
    {
        "loss": 2.2595,
        "grad_norm": 1.6630910634994507,
        "learning_rate": 3.10590302238134e-07,
        "epoch": 0.6556299922700335,
        "step": 5089
    },
    {
        "loss": 2.1899,
        "grad_norm": 1.6645406484603882,
        "learning_rate": 3.0385758398341345e-07,
        "epoch": 0.6557588250450914,
        "step": 5090
    },
    {
        "loss": 2.5133,
        "grad_norm": 1.6772923469543457,
        "learning_rate": 2.9719841828445004e-07,
        "epoch": 0.6558876578201495,
        "step": 5091
    },
    {
        "loss": 2.1829,
        "grad_norm": 2.8133437633514404,
        "learning_rate": 2.9061281499711546e-07,
        "epoch": 0.6560164905952074,
        "step": 5092
    },
    {
        "loss": 2.1602,
        "grad_norm": 1.7564035654067993,
        "learning_rate": 2.841007838684129e-07,
        "epoch": 0.6561453233702654,
        "step": 5093
    },
    {
        "loss": 1.4053,
        "grad_norm": 4.2160468101501465,
        "learning_rate": 2.7766233453644376e-07,
        "epoch": 0.6562741561453234,
        "step": 5094
    },
    {
        "loss": 1.817,
        "grad_norm": 3.3377139568328857,
        "learning_rate": 2.7129747653039107e-07,
        "epoch": 0.6564029889203813,
        "step": 5095
    },
    {
        "loss": 2.3776,
        "grad_norm": 1.2741552591323853,
        "learning_rate": 2.6500621927054715e-07,
        "epoch": 0.6565318216954393,
        "step": 5096
    },
    {
        "loss": 2.3025,
        "grad_norm": 2.0730533599853516,
        "learning_rate": 2.587885720682581e-07,
        "epoch": 0.6566606544704973,
        "step": 5097
    },
    {
        "loss": 2.1334,
        "grad_norm": 1.9758663177490234,
        "learning_rate": 2.5264454412592954e-07,
        "epoch": 0.6567894872455553,
        "step": 5098
    },
    {
        "loss": 2.4996,
        "grad_norm": 2.0215775966644287,
        "learning_rate": 2.465741445369929e-07,
        "epoch": 0.6569183200206132,
        "step": 5099
    },
    {
        "loss": 2.2208,
        "grad_norm": 1.7734609842300415,
        "learning_rate": 2.405773822859281e-07,
        "epoch": 0.6570471527956712,
        "step": 5100
    },
    {
        "loss": 1.782,
        "grad_norm": 2.6990041732788086,
        "learning_rate": 2.3465426624820764e-07,
        "epoch": 0.6571759855707292,
        "step": 5101
    },
    {
        "loss": 2.3552,
        "grad_norm": 1.3655401468276978,
        "learning_rate": 2.28804805190308e-07,
        "epoch": 0.6573048183457871,
        "step": 5102
    },
    {
        "loss": 2.0158,
        "grad_norm": 2.0597622394561768,
        "learning_rate": 2.2302900776971502e-07,
        "epoch": 0.6574336511208452,
        "step": 5103
    },
    {
        "loss": 1.8893,
        "grad_norm": 2.246507167816162,
        "learning_rate": 2.1732688253485733e-07,
        "epoch": 0.6575624838959031,
        "step": 5104
    },
    {
        "loss": 0.7745,
        "grad_norm": 3.456942081451416,
        "learning_rate": 2.1169843792515076e-07,
        "epoch": 0.6576913166709611,
        "step": 5105
    },
    {
        "loss": 2.2059,
        "grad_norm": 1.9356414079666138,
        "learning_rate": 2.0614368227093173e-07,
        "epoch": 0.6578201494460191,
        "step": 5106
    },
    {
        "loss": 2.0659,
        "grad_norm": 2.618260383605957,
        "learning_rate": 2.006626237935183e-07,
        "epoch": 0.657948982221077,
        "step": 5107
    },
    {
        "loss": 2.2212,
        "grad_norm": 3.1957011222839355,
        "learning_rate": 1.952552706051103e-07,
        "epoch": 0.658077814996135,
        "step": 5108
    },
    {
        "loss": 1.7488,
        "grad_norm": 2.0048906803131104,
        "learning_rate": 1.8992163070883918e-07,
        "epoch": 0.658206647771193,
        "step": 5109
    },
    {
        "loss": 2.102,
        "grad_norm": 2.4353699684143066,
        "learning_rate": 1.8466171199873483e-07,
        "epoch": 0.658335480546251,
        "step": 5110
    },
    {
        "loss": 1.0459,
        "grad_norm": 2.484964609146118,
        "learning_rate": 1.7947552225971997e-07,
        "epoch": 0.658464313321309,
        "step": 5111
    },
    {
        "loss": 1.8443,
        "grad_norm": 2.0487782955169678,
        "learning_rate": 1.7436306916758793e-07,
        "epoch": 0.6585931460963669,
        "step": 5112
    },
    {
        "loss": 2.0663,
        "grad_norm": 1.902079701423645,
        "learning_rate": 1.6932436028900266e-07,
        "epoch": 0.6587219788714249,
        "step": 5113
    },
    {
        "loss": 1.4909,
        "grad_norm": 3.329864740371704,
        "learning_rate": 1.6435940308147658e-07,
        "epoch": 0.6588508116464828,
        "step": 5114
    },
    {
        "loss": 1.7579,
        "grad_norm": 1.6423393487930298,
        "learning_rate": 1.5946820489339266e-07,
        "epoch": 0.6589796444215409,
        "step": 5115
    },
    {
        "loss": 1.3153,
        "grad_norm": 3.04984450340271,
        "learning_rate": 1.5465077296393238e-07,
        "epoch": 0.6591084771965988,
        "step": 5116
    },
    {
        "loss": 2.0296,
        "grad_norm": 2.274230480194092,
        "learning_rate": 1.49907114423109e-07,
        "epoch": 0.6592373099716567,
        "step": 5117
    },
    {
        "loss": 1.98,
        "grad_norm": 1.5529954433441162,
        "learning_rate": 1.452372362917731e-07,
        "epoch": 0.6593661427467148,
        "step": 5118
    },
    {
        "loss": 1.8675,
        "grad_norm": 2.305934190750122,
        "learning_rate": 1.4064114548153483e-07,
        "epoch": 0.6594949755217727,
        "step": 5119
    },
    {
        "loss": 2.053,
        "grad_norm": 2.0514562129974365,
        "learning_rate": 1.3611884879483616e-07,
        "epoch": 0.6596238082968308,
        "step": 5120
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.4069024324417114,
        "learning_rate": 1.316703529248675e-07,
        "epoch": 0.6597526410718887,
        "step": 5121
    },
    {
        "loss": 2.0588,
        "grad_norm": 1.4381465911865234,
        "learning_rate": 1.272956644556178e-07,
        "epoch": 0.6598814738469466,
        "step": 5122
    },
    {
        "loss": 2.1276,
        "grad_norm": 1.410714030265808,
        "learning_rate": 1.2299478986181335e-07,
        "epoch": 0.6600103066220047,
        "step": 5123
    },
    {
        "loss": 2.066,
        "grad_norm": 1.3621681928634644,
        "learning_rate": 1.187677355089567e-07,
        "epoch": 0.6601391393970626,
        "step": 5124
    },
    {
        "loss": 2.1301,
        "grad_norm": 1.4005240201950073,
        "learning_rate": 1.1461450765327675e-07,
        "epoch": 0.6602679721721206,
        "step": 5125
    },
    {
        "loss": 1.2117,
        "grad_norm": 2.6197235584259033,
        "learning_rate": 1.1053511244173975e-07,
        "epoch": 0.6603968049471786,
        "step": 5126
    },
    {
        "loss": 2.0319,
        "grad_norm": 2.7337958812713623,
        "learning_rate": 1.0652955591203274e-07,
        "epoch": 0.6605256377222365,
        "step": 5127
    },
    {
        "loss": 2.0809,
        "grad_norm": 2.7206921577453613,
        "learning_rate": 1.0259784399256344e-07,
        "epoch": 0.6606544704972945,
        "step": 5128
    },
    {
        "loss": 1.3285,
        "grad_norm": 3.1113829612731934,
        "learning_rate": 9.873998250244931e-08,
        "epoch": 0.6607833032723525,
        "step": 5129
    },
    {
        "loss": 2.2988,
        "grad_norm": 1.6426677703857422,
        "learning_rate": 9.495597715149518e-08,
        "epoch": 0.6609121360474105,
        "step": 5130
    },
    {
        "loss": 2.1513,
        "grad_norm": 3.0647196769714355,
        "learning_rate": 9.124583354019889e-08,
        "epoch": 0.6610409688224684,
        "step": 5131
    },
    {
        "loss": 2.0688,
        "grad_norm": 2.2264556884765625,
        "learning_rate": 8.760955715975683e-08,
        "epoch": 0.6611698015975264,
        "step": 5132
    },
    {
        "loss": 2.1298,
        "grad_norm": 1.8101500272750854,
        "learning_rate": 8.404715339201397e-08,
        "epoch": 0.6612986343725844,
        "step": 5133
    },
    {
        "loss": 2.1698,
        "grad_norm": 3.1682302951812744,
        "learning_rate": 8.055862750948607e-08,
        "epoch": 0.6614274671476423,
        "step": 5134
    },
    {
        "loss": 2.1511,
        "grad_norm": 2.2608959674835205,
        "learning_rate": 7.714398467535411e-08,
        "epoch": 0.6615562999227004,
        "step": 5135
    },
    {
        "loss": 2.2174,
        "grad_norm": 2.019951820373535,
        "learning_rate": 7.380322994345323e-08,
        "epoch": 0.6616851326977583,
        "step": 5136
    },
    {
        "loss": 1.3163,
        "grad_norm": 3.0596396923065186,
        "learning_rate": 7.05363682582394e-08,
        "epoch": 0.6618139654728162,
        "step": 5137
    },
    {
        "loss": 1.2731,
        "grad_norm": 2.4347751140594482,
        "learning_rate": 6.734340445482823e-08,
        "epoch": 0.6619427982478743,
        "step": 5138
    },
    {
        "loss": 2.2371,
        "grad_norm": 1.5650793313980103,
        "learning_rate": 6.422434325893955e-08,
        "epoch": 0.6620716310229322,
        "step": 5139
    },
    {
        "loss": 1.1724,
        "grad_norm": 3.049515962600708,
        "learning_rate": 6.117918928693623e-08,
        "epoch": 0.6622004637979902,
        "step": 5140
    },
    {
        "loss": 2.7958,
        "grad_norm": 2.0666751861572266,
        "learning_rate": 5.8207947045779696e-08,
        "epoch": 0.6623292965730482,
        "step": 5141
    },
    {
        "loss": 1.6219,
        "grad_norm": 1.4976259469985962,
        "learning_rate": 5.531062093304673e-08,
        "epoch": 0.6624581293481061,
        "step": 5142
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.6407819986343384,
        "learning_rate": 5.248721523691269e-08,
        "epoch": 0.6625869621231641,
        "step": 5143
    },
    {
        "loss": 1.8501,
        "grad_norm": 2.7726588249206543,
        "learning_rate": 4.973773413614602e-08,
        "epoch": 0.6627157948982221,
        "step": 5144
    },
    {
        "loss": 1.9208,
        "grad_norm": 2.579235553741455,
        "learning_rate": 4.706218170010268e-08,
        "epoch": 0.6628446276732801,
        "step": 5145
    },
    {
        "loss": 2.2009,
        "grad_norm": 1.559669017791748,
        "learning_rate": 4.4460561888720606e-08,
        "epoch": 0.662973460448338,
        "step": 5146
    },
    {
        "loss": 2.2553,
        "grad_norm": 2.139479398727417,
        "learning_rate": 4.193287855251971e-08,
        "epoch": 0.663102293223396,
        "step": 5147
    },
    {
        "loss": 2.3122,
        "grad_norm": 2.553011178970337,
        "learning_rate": 3.947913543257964e-08,
        "epoch": 0.663231125998454,
        "step": 5148
    },
    {
        "loss": 1.9627,
        "grad_norm": 2.608717203140259,
        "learning_rate": 3.70993361605565e-08,
        "epoch": 0.663359958773512,
        "step": 5149
    },
    {
        "loss": 1.9314,
        "grad_norm": 1.8492498397827148,
        "learning_rate": 3.479348425866613e-08,
        "epoch": 0.66348879154857,
        "step": 5150
    },
    {
        "loss": 1.7535,
        "grad_norm": 2.9859018325805664,
        "learning_rate": 3.256158313967306e-08,
        "epoch": 0.6636176243236279,
        "step": 5151
    },
    {
        "loss": 1.6943,
        "grad_norm": 3.960793972015381,
        "learning_rate": 3.0403636106884906e-08,
        "epoch": 0.663746457098686,
        "step": 5152
    },
    {
        "loss": 1.9793,
        "grad_norm": 2.074054479598999,
        "learning_rate": 2.831964635416906e-08,
        "epoch": 0.6638752898737439,
        "step": 5153
    },
    {
        "loss": 1.8508,
        "grad_norm": 2.003363609313965,
        "learning_rate": 2.6309616965924932e-08,
        "epoch": 0.6640041226488018,
        "step": 5154
    },
    {
        "loss": 2.2011,
        "grad_norm": 2.18359637260437,
        "learning_rate": 2.437355091708393e-08,
        "epoch": 0.6641329554238599,
        "step": 5155
    },
    {
        "loss": 2.429,
        "grad_norm": 2.203728199005127,
        "learning_rate": 2.251145107311503e-08,
        "epoch": 0.6642617881989178,
        "step": 5156
    },
    {
        "loss": 1.5257,
        "grad_norm": 2.0995607376098633,
        "learning_rate": 2.072332019000811e-08,
        "epoch": 0.6643906209739758,
        "step": 5157
    },
    {
        "loss": 0.7634,
        "grad_norm": 2.4453799724578857,
        "learning_rate": 1.900916091427951e-08,
        "epoch": 0.6645194537490338,
        "step": 5158
    },
    {
        "loss": 2.2526,
        "grad_norm": 3.031989097595215,
        "learning_rate": 1.7368975782960927e-08,
        "epoch": 0.6646482865240917,
        "step": 5159
    },
    {
        "loss": 1.8745,
        "grad_norm": 2.489816904067993,
        "learning_rate": 1.5802767223610515e-08,
        "epoch": 0.6647771192991497,
        "step": 5160
    },
    {
        "loss": 1.6327,
        "grad_norm": 2.941969394683838,
        "learning_rate": 1.4310537554274028e-08,
        "epoch": 0.6649059520742077,
        "step": 5161
    },
    {
        "loss": 1.4258,
        "grad_norm": 2.343446731567383,
        "learning_rate": 1.2892288983534783e-08,
        "epoch": 0.6650347848492657,
        "step": 5162
    },
    {
        "loss": 2.1651,
        "grad_norm": 2.701429605484009,
        "learning_rate": 1.1548023610452596e-08,
        "epoch": 0.6651636176243236,
        "step": 5163
    },
    {
        "loss": 1.7515,
        "grad_norm": 3.1417860984802246,
        "learning_rate": 1.0277743424608189e-08,
        "epoch": 0.6652924503993816,
        "step": 5164
    },
    {
        "loss": 2.3926,
        "grad_norm": 1.3173357248306274,
        "learning_rate": 9.081450306069883e-09,
        "epoch": 0.6654212831744396,
        "step": 5165
    },
    {
        "loss": 2.1261,
        "grad_norm": 1.2218537330627441,
        "learning_rate": 7.959146025410258e-09,
        "epoch": 0.6655501159494975,
        "step": 5166
    },
    {
        "loss": 1.451,
        "grad_norm": 3.3888871669769287,
        "learning_rate": 6.91083224368394e-09,
        "epoch": 0.6656789487245556,
        "step": 5167
    },
    {
        "loss": 2.1186,
        "grad_norm": 1.6588451862335205,
        "learning_rate": 5.936510512455362e-09,
        "epoch": 0.6658077814996135,
        "step": 5168
    },
    {
        "loss": 2.4354,
        "grad_norm": 1.6449589729309082,
        "learning_rate": 5.036182273754353e-09,
        "epoch": 0.6659366142746714,
        "step": 5169
    },
    {
        "loss": 1.4435,
        "grad_norm": 2.7026596069335938,
        "learning_rate": 4.209848860109444e-09,
        "epoch": 0.6660654470497295,
        "step": 5170
    },
    {
        "loss": 1.4706,
        "grad_norm": 2.492279052734375,
        "learning_rate": 3.457511494536769e-09,
        "epoch": 0.6661942798247874,
        "step": 5171
    },
    {
        "loss": 1.4318,
        "grad_norm": 2.5296361446380615,
        "learning_rate": 2.779171290523408e-09,
        "epoch": 0.6663231125998454,
        "step": 5172
    },
    {
        "loss": 1.844,
        "grad_norm": 3.363011598587036,
        "learning_rate": 2.174829252049593e-09,
        "epoch": 0.6664519453749034,
        "step": 5173
    },
    {
        "loss": 2.3783,
        "grad_norm": 2.792774200439453,
        "learning_rate": 1.6444862735665035e-09,
        "epoch": 0.6665807781499613,
        "step": 5174
    },
    {
        "loss": 1.5996,
        "grad_norm": 2.5360896587371826,
        "learning_rate": 1.1881431400073695e-09,
        "epoch": 0.6667096109250193,
        "step": 5175
    },
    {
        "loss": 1.5813,
        "grad_norm": 3.005871057510376,
        "learning_rate": 8.05800526776368e-10,
        "epoch": 0.6668384437000773,
        "step": 5176
    },
    {
        "loss": 1.6942,
        "grad_norm": 1.9016673564910889,
        "learning_rate": 4.974589997597257e-10,
        "epoch": 0.6669672764751353,
        "step": 5177
    },
    {
        "loss": 1.9617,
        "grad_norm": 1.8991641998291016,
        "learning_rate": 2.6311901532571904e-10,
        "epoch": 0.6670961092501932,
        "step": 5178
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.7172517776489258,
        "learning_rate": 1.0278092029136766e-10,
        "epoch": 0.6672249420252512,
        "step": 5179
    },
    {
        "loss": 1.8398,
        "grad_norm": 2.815484046936035,
        "learning_rate": 1.6444951977945267e-11,
        "epoch": 0.6673537748003092,
        "step": 5180
    },
    {
        "loss": 1.3627,
        "grad_norm": 2.732496976852417,
        "learning_rate": 9.999999588876183e-05,
        "epoch": 0.6674826075753671,
        "step": 5181
    },
    {
        "loss": 1.5693,
        "grad_norm": 4.627473831176758,
        "learning_rate": 9.999993422020291e-05,
        "epoch": 0.6676114403504252,
        "step": 5182
    },
    {
        "loss": 1.7952,
        "grad_norm": 2.6672980785369873,
        "learning_rate": 9.999979854946249e-05,
        "epoch": 0.6677402731254831,
        "step": 5183
    },
    {
        "loss": 1.4048,
        "grad_norm": 2.5680525302886963,
        "learning_rate": 9.999958887674139e-05,
        "epoch": 0.667869105900541,
        "step": 5184
    },
    {
        "loss": 2.2452,
        "grad_norm": 2.475720167160034,
        "learning_rate": 9.999930520234994e-05,
        "epoch": 0.6679979386755991,
        "step": 5185
    },
    {
        "loss": 2.0214,
        "grad_norm": 2.1969425678253174,
        "learning_rate": 9.999894752670797e-05,
        "epoch": 0.668126771450657,
        "step": 5186
    },
    {
        "loss": 2.137,
        "grad_norm": 2.1302411556243896,
        "learning_rate": 9.999851585034489e-05,
        "epoch": 0.6682556042257151,
        "step": 5187
    },
    {
        "loss": 2.249,
        "grad_norm": 2.2481980323791504,
        "learning_rate": 9.999801017389958e-05,
        "epoch": 0.668384437000773,
        "step": 5188
    },
    {
        "loss": 1.8136,
        "grad_norm": 3.4590554237365723,
        "learning_rate": 9.999743049812045e-05,
        "epoch": 0.6685132697758309,
        "step": 5189
    },
    {
        "loss": 2.4126,
        "grad_norm": 1.721361517906189,
        "learning_rate": 9.999677682386548e-05,
        "epoch": 0.668642102550889,
        "step": 5190
    },
    {
        "loss": 2.0442,
        "grad_norm": 1.7781437635421753,
        "learning_rate": 9.999604915210212e-05,
        "epoch": 0.6687709353259469,
        "step": 5191
    },
    {
        "loss": 1.2978,
        "grad_norm": 2.787220001220703,
        "learning_rate": 9.999524748390734e-05,
        "epoch": 0.6688997681010049,
        "step": 5192
    },
    {
        "loss": 2.6076,
        "grad_norm": 2.226313352584839,
        "learning_rate": 9.999437182046766e-05,
        "epoch": 0.6690286008760629,
        "step": 5193
    },
    {
        "loss": 1.5591,
        "grad_norm": 2.916980266571045,
        "learning_rate": 9.999342216307914e-05,
        "epoch": 0.6691574336511208,
        "step": 5194
    },
    {
        "loss": 2.0899,
        "grad_norm": 2.100858211517334,
        "learning_rate": 9.999239851314726e-05,
        "epoch": 0.6692862664261788,
        "step": 5195
    },
    {
        "loss": 2.4065,
        "grad_norm": 1.8920609951019287,
        "learning_rate": 9.99913008721871e-05,
        "epoch": 0.6694150992012368,
        "step": 5196
    },
    {
        "loss": 2.012,
        "grad_norm": 2.796332836151123,
        "learning_rate": 9.99901292418232e-05,
        "epoch": 0.6695439319762948,
        "step": 5197
    },
    {
        "loss": 2.0343,
        "grad_norm": 2.5208654403686523,
        "learning_rate": 9.998888362378962e-05,
        "epoch": 0.6696727647513527,
        "step": 5198
    },
    {
        "loss": 1.3493,
        "grad_norm": 4.448210716247559,
        "learning_rate": 9.998756401992997e-05,
        "epoch": 0.6698015975264107,
        "step": 5199
    },
    {
        "loss": 1.7788,
        "grad_norm": 3.5231213569641113,
        "learning_rate": 9.998617043219729e-05,
        "epoch": 0.6699304303014687,
        "step": 5200
    },
    {
        "loss": 1.5628,
        "grad_norm": 3.447465181350708,
        "learning_rate": 9.998470286265416e-05,
        "epoch": 0.6700592630765266,
        "step": 5201
    },
    {
        "loss": 2.2655,
        "grad_norm": 2.081524610519409,
        "learning_rate": 9.998316131347265e-05,
        "epoch": 0.6701880958515847,
        "step": 5202
    },
    {
        "loss": 1.8004,
        "grad_norm": 3.264080047607422,
        "learning_rate": 9.998154578693435e-05,
        "epoch": 0.6703169286266426,
        "step": 5203
    },
    {
        "loss": 2.1357,
        "grad_norm": 2.096132755279541,
        "learning_rate": 9.997985628543027e-05,
        "epoch": 0.6704457614017006,
        "step": 5204
    },
    {
        "loss": 2.2218,
        "grad_norm": 1.7405788898468018,
        "learning_rate": 9.997809281146097e-05,
        "epoch": 0.6705745941767586,
        "step": 5205
    },
    {
        "loss": 1.9142,
        "grad_norm": 1.8924109935760498,
        "learning_rate": 9.997625536763647e-05,
        "epoch": 0.6707034269518165,
        "step": 5206
    },
    {
        "loss": 2.1934,
        "grad_norm": 2.0759551525115967,
        "learning_rate": 9.997434395667628e-05,
        "epoch": 0.6708322597268745,
        "step": 5207
    },
    {
        "loss": 1.2106,
        "grad_norm": 2.833336114883423,
        "learning_rate": 9.997235858140935e-05,
        "epoch": 0.6709610925019325,
        "step": 5208
    },
    {
        "loss": 1.5006,
        "grad_norm": 2.237321138381958,
        "learning_rate": 9.997029924477416e-05,
        "epoch": 0.6710899252769905,
        "step": 5209
    },
    {
        "loss": 1.9107,
        "grad_norm": 3.1934866905212402,
        "learning_rate": 9.996816594981859e-05,
        "epoch": 0.6712187580520484,
        "step": 5210
    },
    {
        "loss": 1.9822,
        "grad_norm": 2.9902048110961914,
        "learning_rate": 9.996595869970004e-05,
        "epoch": 0.6713475908271064,
        "step": 5211
    },
    {
        "loss": 1.7546,
        "grad_norm": 1.9599764347076416,
        "learning_rate": 9.996367749768532e-05,
        "epoch": 0.6714764236021644,
        "step": 5212
    },
    {
        "loss": 2.083,
        "grad_norm": 2.0402255058288574,
        "learning_rate": 9.996132234715072e-05,
        "epoch": 0.6716052563772223,
        "step": 5213
    },
    {
        "loss": 2.206,
        "grad_norm": 1.6967960596084595,
        "learning_rate": 9.995889325158199e-05,
        "epoch": 0.6717340891522804,
        "step": 5214
    },
    {
        "loss": 2.1914,
        "grad_norm": 2.5769588947296143,
        "learning_rate": 9.995639021457425e-05,
        "epoch": 0.6718629219273383,
        "step": 5215
    },
    {
        "loss": 1.2724,
        "grad_norm": 2.571629524230957,
        "learning_rate": 9.995381323983218e-05,
        "epoch": 0.6719917547023962,
        "step": 5216
    },
    {
        "loss": 1.9054,
        "grad_norm": 3.3798701763153076,
        "learning_rate": 9.995116233116975e-05,
        "epoch": 0.6721205874774543,
        "step": 5217
    },
    {
        "loss": 1.8206,
        "grad_norm": 2.7186355590820312,
        "learning_rate": 9.994843749251048e-05,
        "epoch": 0.6722494202525122,
        "step": 5218
    },
    {
        "loss": 2.5373,
        "grad_norm": 2.0317575931549072,
        "learning_rate": 9.994563872788724e-05,
        "epoch": 0.6723782530275703,
        "step": 5219
    },
    {
        "loss": 2.2964,
        "grad_norm": 2.184276819229126,
        "learning_rate": 9.99427660414423e-05,
        "epoch": 0.6725070858026282,
        "step": 5220
    },
    {
        "loss": 1.7771,
        "grad_norm": 2.1897435188293457,
        "learning_rate": 9.993981943742741e-05,
        "epoch": 0.6726359185776861,
        "step": 5221
    },
    {
        "loss": 1.2763,
        "grad_norm": 3.1923511028289795,
        "learning_rate": 9.993679892020365e-05,
        "epoch": 0.6727647513527442,
        "step": 5222
    },
    {
        "loss": 1.6906,
        "grad_norm": 2.3864004611968994,
        "learning_rate": 9.993370449424153e-05,
        "epoch": 0.6728935841278021,
        "step": 5223
    },
    {
        "loss": 2.506,
        "grad_norm": 1.857530951499939,
        "learning_rate": 9.993053616412094e-05,
        "epoch": 0.6730224169028601,
        "step": 5224
    },
    {
        "loss": 2.1725,
        "grad_norm": 2.4874014854431152,
        "learning_rate": 9.992729393453117e-05,
        "epoch": 0.6731512496779181,
        "step": 5225
    },
    {
        "loss": 2.3901,
        "grad_norm": 2.530369997024536,
        "learning_rate": 9.992397781027085e-05,
        "epoch": 0.673280082452976,
        "step": 5226
    },
    {
        "loss": 2.3134,
        "grad_norm": 1.6697667837142944,
        "learning_rate": 9.992058779624797e-05,
        "epoch": 0.673408915228034,
        "step": 5227
    },
    {
        "loss": 1.9845,
        "grad_norm": 2.6218390464782715,
        "learning_rate": 9.991712389747998e-05,
        "epoch": 0.673537748003092,
        "step": 5228
    },
    {
        "loss": 1.5795,
        "grad_norm": 2.9205920696258545,
        "learning_rate": 9.991358611909353e-05,
        "epoch": 0.67366658077815,
        "step": 5229
    },
    {
        "loss": 2.0596,
        "grad_norm": 1.1846774816513062,
        "learning_rate": 9.990997446632473e-05,
        "epoch": 0.6737954135532079,
        "step": 5230
    },
    {
        "loss": 2.0849,
        "grad_norm": 2.5787034034729004,
        "learning_rate": 9.990628894451899e-05,
        "epoch": 0.6739242463282659,
        "step": 5231
    },
    {
        "loss": 2.4993,
        "grad_norm": 2.473325490951538,
        "learning_rate": 9.990252955913104e-05,
        "epoch": 0.6740530791033239,
        "step": 5232
    },
    {
        "loss": 1.3662,
        "grad_norm": 3.0550224781036377,
        "learning_rate": 9.989869631572495e-05,
        "epoch": 0.6741819118783818,
        "step": 5233
    },
    {
        "loss": 2.02,
        "grad_norm": 1.4300644397735596,
        "learning_rate": 9.989478921997411e-05,
        "epoch": 0.6743107446534399,
        "step": 5234
    },
    {
        "loss": 1.9359,
        "grad_norm": 1.9607080221176147,
        "learning_rate": 9.989080827766116e-05,
        "epoch": 0.6744395774284978,
        "step": 5235
    },
    {
        "loss": 1.4702,
        "grad_norm": 3.4258503913879395,
        "learning_rate": 9.988675349467811e-05,
        "epoch": 0.6745684102035557,
        "step": 5236
    },
    {
        "loss": 2.09,
        "grad_norm": 1.9680372476577759,
        "learning_rate": 9.988262487702621e-05,
        "epoch": 0.6746972429786138,
        "step": 5237
    },
    {
        "loss": 1.8148,
        "grad_norm": 1.7093126773834229,
        "learning_rate": 9.9878422430816e-05,
        "epoch": 0.6748260757536717,
        "step": 5238
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.8264214992523193,
        "learning_rate": 9.987414616226731e-05,
        "epoch": 0.6749549085287297,
        "step": 5239
    },
    {
        "loss": 1.5027,
        "grad_norm": 2.861807107925415,
        "learning_rate": 9.986979607770918e-05,
        "epoch": 0.6750837413037877,
        "step": 5240
    },
    {
        "loss": 1.4538,
        "grad_norm": 3.548956871032715,
        "learning_rate": 9.986537218357997e-05,
        "epoch": 0.6752125740788456,
        "step": 5241
    },
    {
        "loss": 2.1699,
        "grad_norm": 1.618749976158142,
        "learning_rate": 9.98608744864272e-05,
        "epoch": 0.6753414068539036,
        "step": 5242
    },
    {
        "loss": 2.2553,
        "grad_norm": 1.5406849384307861,
        "learning_rate": 9.985630299290771e-05,
        "epoch": 0.6754702396289616,
        "step": 5243
    },
    {
        "loss": 2.1587,
        "grad_norm": 2.8330652713775635,
        "learning_rate": 9.98516577097875e-05,
        "epoch": 0.6755990724040196,
        "step": 5244
    },
    {
        "loss": 2.3238,
        "grad_norm": 1.6702193021774292,
        "learning_rate": 9.984693864394181e-05,
        "epoch": 0.6757279051790775,
        "step": 5245
    },
    {
        "loss": 1.9132,
        "grad_norm": 1.7368569374084473,
        "learning_rate": 9.984214580235506e-05,
        "epoch": 0.6758567379541355,
        "step": 5246
    },
    {
        "loss": 2.1877,
        "grad_norm": 2.0036001205444336,
        "learning_rate": 9.983727919212089e-05,
        "epoch": 0.6759855707291935,
        "step": 5247
    },
    {
        "loss": 2.2293,
        "grad_norm": 2.2523460388183594,
        "learning_rate": 9.98323388204421e-05,
        "epoch": 0.6761144035042514,
        "step": 5248
    },
    {
        "loss": 1.9413,
        "grad_norm": 2.6509313583374023,
        "learning_rate": 9.982732469463064e-05,
        "epoch": 0.6762432362793095,
        "step": 5249
    },
    {
        "loss": 1.9937,
        "grad_norm": 1.8481523990631104,
        "learning_rate": 9.982223682210767e-05,
        "epoch": 0.6763720690543674,
        "step": 5250
    },
    {
        "loss": 2.3793,
        "grad_norm": 1.8446725606918335,
        "learning_rate": 9.981707521040347e-05,
        "epoch": 0.6765009018294255,
        "step": 5251
    },
    {
        "loss": 2.0507,
        "grad_norm": 2.2043254375457764,
        "learning_rate": 9.981183986715745e-05,
        "epoch": 0.6766297346044834,
        "step": 5252
    },
    {
        "loss": 2.3863,
        "grad_norm": 2.2598955631256104,
        "learning_rate": 9.980653080011819e-05,
        "epoch": 0.6767585673795413,
        "step": 5253
    },
    {
        "loss": 2.2781,
        "grad_norm": 1.8289002180099487,
        "learning_rate": 9.98011480171433e-05,
        "epoch": 0.6768874001545994,
        "step": 5254
    },
    {
        "loss": 1.9297,
        "grad_norm": 1.9299952983856201,
        "learning_rate": 9.979569152619959e-05,
        "epoch": 0.6770162329296573,
        "step": 5255
    },
    {
        "loss": 1.7289,
        "grad_norm": 2.414571762084961,
        "learning_rate": 9.979016133536288e-05,
        "epoch": 0.6771450657047153,
        "step": 5256
    },
    {
        "loss": 2.3388,
        "grad_norm": 1.9164392948150635,
        "learning_rate": 9.978455745281814e-05,
        "epoch": 0.6772738984797733,
        "step": 5257
    },
    {
        "loss": 2.6655,
        "grad_norm": 1.4833627939224243,
        "learning_rate": 9.977887988685933e-05,
        "epoch": 0.6774027312548312,
        "step": 5258
    },
    {
        "loss": 2.1998,
        "grad_norm": 1.9776675701141357,
        "learning_rate": 9.977312864588955e-05,
        "epoch": 0.6775315640298892,
        "step": 5259
    },
    {
        "loss": 1.9275,
        "grad_norm": 2.489600896835327,
        "learning_rate": 9.976730373842084e-05,
        "epoch": 0.6776603968049472,
        "step": 5260
    },
    {
        "loss": 1.4851,
        "grad_norm": 3.32624888420105,
        "learning_rate": 9.976140517307441e-05,
        "epoch": 0.6777892295800052,
        "step": 5261
    },
    {
        "loss": 1.6688,
        "grad_norm": 2.971022605895996,
        "learning_rate": 9.975543295858035e-05,
        "epoch": 0.6779180623550631,
        "step": 5262
    },
    {
        "loss": 1.9828,
        "grad_norm": 2.5170698165893555,
        "learning_rate": 9.97493871037778e-05,
        "epoch": 0.6780468951301211,
        "step": 5263
    },
    {
        "loss": 1.6699,
        "grad_norm": 2.1919281482696533,
        "learning_rate": 9.974326761761493e-05,
        "epoch": 0.6781757279051791,
        "step": 5264
    },
    {
        "loss": 2.0899,
        "grad_norm": 2.480591058731079,
        "learning_rate": 9.973707450914885e-05,
        "epoch": 0.678304560680237,
        "step": 5265
    },
    {
        "loss": 0.8459,
        "grad_norm": 3.749572992324829,
        "learning_rate": 9.973080778754564e-05,
        "epoch": 0.6784333934552951,
        "step": 5266
    },
    {
        "loss": 2.1495,
        "grad_norm": 1.74640953540802,
        "learning_rate": 9.972446746208035e-05,
        "epoch": 0.678562226230353,
        "step": 5267
    },
    {
        "loss": 2.2891,
        "grad_norm": 2.4673635959625244,
        "learning_rate": 9.971805354213692e-05,
        "epoch": 0.6786910590054109,
        "step": 5268
    },
    {
        "loss": 2.255,
        "grad_norm": 2.5048041343688965,
        "learning_rate": 9.971156603720828e-05,
        "epoch": 0.678819891780469,
        "step": 5269
    },
    {
        "loss": 2.0528,
        "grad_norm": 2.2306833267211914,
        "learning_rate": 9.97050049568962e-05,
        "epoch": 0.6789487245555269,
        "step": 5270
    },
    {
        "loss": 2.2774,
        "grad_norm": 2.330646276473999,
        "learning_rate": 9.969837031091139e-05,
        "epoch": 0.679077557330585,
        "step": 5271
    },
    {
        "loss": 1.7639,
        "grad_norm": 3.349430561065674,
        "learning_rate": 9.969166210907344e-05,
        "epoch": 0.6792063901056429,
        "step": 5272
    },
    {
        "loss": 1.0908,
        "grad_norm": 3.516505718231201,
        "learning_rate": 9.968488036131078e-05,
        "epoch": 0.6793352228807008,
        "step": 5273
    },
    {
        "loss": 2.0939,
        "grad_norm": 1.6858270168304443,
        "learning_rate": 9.967802507766071e-05,
        "epoch": 0.6794640556557588,
        "step": 5274
    },
    {
        "loss": 1.6814,
        "grad_norm": 2.853269338607788,
        "learning_rate": 9.967109626826936e-05,
        "epoch": 0.6795928884308168,
        "step": 5275
    },
    {
        "loss": 2.0196,
        "grad_norm": 2.1737399101257324,
        "learning_rate": 9.96640939433917e-05,
        "epoch": 0.6797217212058748,
        "step": 5276
    },
    {
        "loss": 2.2619,
        "grad_norm": 1.5659961700439453,
        "learning_rate": 9.965701811339145e-05,
        "epoch": 0.6798505539809327,
        "step": 5277
    },
    {
        "loss": 1.6547,
        "grad_norm": 5.205936908721924,
        "learning_rate": 9.964986878874119e-05,
        "epoch": 0.6799793867559907,
        "step": 5278
    },
    {
        "loss": 2.1687,
        "grad_norm": 1.8472638130187988,
        "learning_rate": 9.964264598002226e-05,
        "epoch": 0.6801082195310487,
        "step": 5279
    },
    {
        "loss": 1.864,
        "grad_norm": 1.4550479650497437,
        "learning_rate": 9.963534969792474e-05,
        "epoch": 0.6802370523061066,
        "step": 5280
    },
    {
        "loss": 2.2424,
        "grad_norm": 1.4441508054733276,
        "learning_rate": 9.962797995324741e-05,
        "epoch": 0.6803658850811647,
        "step": 5281
    },
    {
        "loss": 2.0166,
        "grad_norm": 2.015075922012329,
        "learning_rate": 9.96205367568979e-05,
        "epoch": 0.6804947178562226,
        "step": 5282
    },
    {
        "loss": 2.4497,
        "grad_norm": 1.6883463859558105,
        "learning_rate": 9.961302011989244e-05,
        "epoch": 0.6806235506312805,
        "step": 5283
    },
    {
        "loss": 2.5067,
        "grad_norm": 1.5618637800216675,
        "learning_rate": 9.960543005335601e-05,
        "epoch": 0.6807523834063386,
        "step": 5284
    },
    {
        "loss": 2.13,
        "grad_norm": 1.6524087190628052,
        "learning_rate": 9.959776656852222e-05,
        "epoch": 0.6808812161813965,
        "step": 5285
    },
    {
        "loss": 2.2934,
        "grad_norm": 2.3355720043182373,
        "learning_rate": 9.959002967673343e-05,
        "epoch": 0.6810100489564546,
        "step": 5286
    },
    {
        "loss": 2.5741,
        "grad_norm": 2.2838709354400635,
        "learning_rate": 9.958221938944057e-05,
        "epoch": 0.6811388817315125,
        "step": 5287
    },
    {
        "loss": 0.8508,
        "grad_norm": 2.809921979904175,
        "learning_rate": 9.95743357182032e-05,
        "epoch": 0.6812677145065704,
        "step": 5288
    },
    {
        "loss": 2.4777,
        "grad_norm": 1.672101616859436,
        "learning_rate": 9.956637867468953e-05,
        "epoch": 0.6813965472816285,
        "step": 5289
    },
    {
        "loss": 1.8387,
        "grad_norm": 1.920436978340149,
        "learning_rate": 9.955834827067636e-05,
        "epoch": 0.6815253800566864,
        "step": 5290
    },
    {
        "loss": 1.8806,
        "grad_norm": 2.5341055393218994,
        "learning_rate": 9.955024451804905e-05,
        "epoch": 0.6816542128317444,
        "step": 5291
    },
    {
        "loss": 1.2493,
        "grad_norm": 3.0740914344787598,
        "learning_rate": 9.954206742880149e-05,
        "epoch": 0.6817830456068024,
        "step": 5292
    },
    {
        "loss": 2.6434,
        "grad_norm": 1.716382384300232,
        "learning_rate": 9.953381701503616e-05,
        "epoch": 0.6819118783818603,
        "step": 5293
    },
    {
        "loss": 1.4778,
        "grad_norm": 2.007800817489624,
        "learning_rate": 9.952549328896409e-05,
        "epoch": 0.6820407111569183,
        "step": 5294
    },
    {
        "loss": 2.16,
        "grad_norm": 3.4662883281707764,
        "learning_rate": 9.951709626290473e-05,
        "epoch": 0.6821695439319763,
        "step": 5295
    },
    {
        "loss": 2.4309,
        "grad_norm": 1.9957356452941895,
        "learning_rate": 9.950862594928607e-05,
        "epoch": 0.6822983767070343,
        "step": 5296
    },
    {
        "loss": 2.1153,
        "grad_norm": 2.5511581897735596,
        "learning_rate": 9.950008236064457e-05,
        "epoch": 0.6824272094820922,
        "step": 5297
    },
    {
        "loss": 1.5007,
        "grad_norm": 2.244488000869751,
        "learning_rate": 9.949146550962511e-05,
        "epoch": 0.6825560422571502,
        "step": 5298
    },
    {
        "loss": 2.387,
        "grad_norm": 1.5130505561828613,
        "learning_rate": 9.948277540898105e-05,
        "epoch": 0.6826848750322082,
        "step": 5299
    },
    {
        "loss": 2.2173,
        "grad_norm": 1.4093033075332642,
        "learning_rate": 9.94740120715741e-05,
        "epoch": 0.6828137078072661,
        "step": 5300
    },
    {
        "loss": 2.4436,
        "grad_norm": 1.543558955192566,
        "learning_rate": 9.946517551037445e-05,
        "epoch": 0.6829425405823242,
        "step": 5301
    },
    {
        "loss": 2.1593,
        "grad_norm": 2.9633378982543945,
        "learning_rate": 9.945626573846057e-05,
        "epoch": 0.6830713733573821,
        "step": 5302
    },
    {
        "loss": 2.278,
        "grad_norm": 2.1014163494110107,
        "learning_rate": 9.944728276901934e-05,
        "epoch": 0.6832002061324401,
        "step": 5303
    },
    {
        "loss": 1.4799,
        "grad_norm": 3.1125948429107666,
        "learning_rate": 9.943822661534595e-05,
        "epoch": 0.6833290389074981,
        "step": 5304
    },
    {
        "loss": 2.0577,
        "grad_norm": 2.0012621879577637,
        "learning_rate": 9.942909729084396e-05,
        "epoch": 0.683457871682556,
        "step": 5305
    },
    {
        "loss": 2.0468,
        "grad_norm": 2.623929262161255,
        "learning_rate": 9.941989480902517e-05,
        "epoch": 0.683586704457614,
        "step": 5306
    },
    {
        "loss": 2.5411,
        "grad_norm": 1.2676068544387817,
        "learning_rate": 9.941061918350963e-05,
        "epoch": 0.683715537232672,
        "step": 5307
    },
    {
        "loss": 1.3918,
        "grad_norm": 3.24338698387146,
        "learning_rate": 9.940127042802575e-05,
        "epoch": 0.68384437000773,
        "step": 5308
    },
    {
        "loss": 1.8817,
        "grad_norm": 1.6102231740951538,
        "learning_rate": 9.939184855641009e-05,
        "epoch": 0.683973202782788,
        "step": 5309
    },
    {
        "loss": 2.1907,
        "grad_norm": 1.6578642129898071,
        "learning_rate": 9.938235358260741e-05,
        "epoch": 0.6841020355578459,
        "step": 5310
    },
    {
        "loss": 2.1199,
        "grad_norm": 2.056370735168457,
        "learning_rate": 9.937278552067078e-05,
        "epoch": 0.6842308683329039,
        "step": 5311
    },
    {
        "loss": 1.1253,
        "grad_norm": 5.598197937011719,
        "learning_rate": 9.936314438476131e-05,
        "epoch": 0.6843597011079618,
        "step": 5312
    },
    {
        "loss": 1.9046,
        "grad_norm": 3.062091827392578,
        "learning_rate": 9.935343018914835e-05,
        "epoch": 0.6844885338830199,
        "step": 5313
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.5045607089996338,
        "learning_rate": 9.934364294820932e-05,
        "epoch": 0.6846173666580778,
        "step": 5314
    },
    {
        "loss": 2.5423,
        "grad_norm": 1.5422223806381226,
        "learning_rate": 9.933378267642983e-05,
        "epoch": 0.6847461994331357,
        "step": 5315
    },
    {
        "loss": 2.0343,
        "grad_norm": 3.2695653438568115,
        "learning_rate": 9.93238493884035e-05,
        "epoch": 0.6848750322081938,
        "step": 5316
    },
    {
        "loss": 2.5017,
        "grad_norm": 1.2814908027648926,
        "learning_rate": 9.931384309883204e-05,
        "epoch": 0.6850038649832517,
        "step": 5317
    },
    {
        "loss": 1.9677,
        "grad_norm": 1.9311326742172241,
        "learning_rate": 9.930376382252524e-05,
        "epoch": 0.6851326977583098,
        "step": 5318
    },
    {
        "loss": 1.5866,
        "grad_norm": 2.9687044620513916,
        "learning_rate": 9.929361157440089e-05,
        "epoch": 0.6852615305333677,
        "step": 5319
    },
    {
        "loss": 1.8963,
        "grad_norm": 1.7735345363616943,
        "learning_rate": 9.928338636948475e-05,
        "epoch": 0.6853903633084256,
        "step": 5320
    },
    {
        "loss": 1.5138,
        "grad_norm": 3.4187355041503906,
        "learning_rate": 9.927308822291061e-05,
        "epoch": 0.6855191960834837,
        "step": 5321
    },
    {
        "loss": 1.8121,
        "grad_norm": 1.7607011795043945,
        "learning_rate": 9.92627171499202e-05,
        "epoch": 0.6856480288585416,
        "step": 5322
    },
    {
        "loss": 1.8169,
        "grad_norm": 2.0293145179748535,
        "learning_rate": 9.925227316586317e-05,
        "epoch": 0.6857768616335996,
        "step": 5323
    },
    {
        "loss": 2.0208,
        "grad_norm": 2.1829819679260254,
        "learning_rate": 9.92417562861971e-05,
        "epoch": 0.6859056944086576,
        "step": 5324
    },
    {
        "loss": 2.1148,
        "grad_norm": 1.762637972831726,
        "learning_rate": 9.923116652648744e-05,
        "epoch": 0.6860345271837155,
        "step": 5325
    },
    {
        "loss": 1.926,
        "grad_norm": 3.112258195877075,
        "learning_rate": 9.922050390240753e-05,
        "epoch": 0.6861633599587735,
        "step": 5326
    },
    {
        "loss": 1.566,
        "grad_norm": 2.7389867305755615,
        "learning_rate": 9.920976842973853e-05,
        "epoch": 0.6862921927338315,
        "step": 5327
    },
    {
        "loss": 1.6385,
        "grad_norm": 2.683342933654785,
        "learning_rate": 9.919896012436942e-05,
        "epoch": 0.6864210255088895,
        "step": 5328
    },
    {
        "loss": 1.9668,
        "grad_norm": 2.311415433883667,
        "learning_rate": 9.918807900229701e-05,
        "epoch": 0.6865498582839474,
        "step": 5329
    },
    {
        "loss": 1.7177,
        "grad_norm": 2.5813159942626953,
        "learning_rate": 9.917712507962587e-05,
        "epoch": 0.6866786910590054,
        "step": 5330
    },
    {
        "loss": 2.1063,
        "grad_norm": 2.2857320308685303,
        "learning_rate": 9.916609837256825e-05,
        "epoch": 0.6868075238340634,
        "step": 5331
    },
    {
        "loss": 1.6043,
        "grad_norm": 3.0859997272491455,
        "learning_rate": 9.915499889744422e-05,
        "epoch": 0.6869363566091213,
        "step": 5332
    },
    {
        "loss": 2.2052,
        "grad_norm": 1.5649713277816772,
        "learning_rate": 9.914382667068149e-05,
        "epoch": 0.6870651893841794,
        "step": 5333
    },
    {
        "loss": 2.1974,
        "grad_norm": 2.209338665008545,
        "learning_rate": 9.913258170881547e-05,
        "epoch": 0.6871940221592373,
        "step": 5334
    },
    {
        "loss": 1.7953,
        "grad_norm": 2.0133614540100098,
        "learning_rate": 9.912126402848922e-05,
        "epoch": 0.6873228549342952,
        "step": 5335
    },
    {
        "loss": 1.9738,
        "grad_norm": 1.8097844123840332,
        "learning_rate": 9.910987364645343e-05,
        "epoch": 0.6874516877093533,
        "step": 5336
    },
    {
        "loss": 2.2533,
        "grad_norm": 2.24588680267334,
        "learning_rate": 9.909841057956636e-05,
        "epoch": 0.6875805204844112,
        "step": 5337
    },
    {
        "loss": 1.9081,
        "grad_norm": 2.284862518310547,
        "learning_rate": 9.908687484479389e-05,
        "epoch": 0.6877093532594692,
        "step": 5338
    },
    {
        "loss": 1.1151,
        "grad_norm": 3.3130152225494385,
        "learning_rate": 9.907526645920944e-05,
        "epoch": 0.6878381860345272,
        "step": 5339
    },
    {
        "loss": 1.8417,
        "grad_norm": 2.414047956466675,
        "learning_rate": 9.906358543999392e-05,
        "epoch": 0.6879670188095851,
        "step": 5340
    },
    {
        "loss": 1.8341,
        "grad_norm": 2.864192485809326,
        "learning_rate": 9.905183180443582e-05,
        "epoch": 0.6880958515846431,
        "step": 5341
    },
    {
        "loss": 2.5291,
        "grad_norm": 2.466601848602295,
        "learning_rate": 9.9040005569931e-05,
        "epoch": 0.6882246843597011,
        "step": 5342
    },
    {
        "loss": 1.3947,
        "grad_norm": 2.5482707023620605,
        "learning_rate": 9.902810675398287e-05,
        "epoch": 0.6883535171347591,
        "step": 5343
    },
    {
        "loss": 1.8553,
        "grad_norm": 2.9301369190216064,
        "learning_rate": 9.90161353742022e-05,
        "epoch": 0.688482349909817,
        "step": 5344
    },
    {
        "loss": 2.3043,
        "grad_norm": 1.5554544925689697,
        "learning_rate": 9.900409144830715e-05,
        "epoch": 0.688611182684875,
        "step": 5345
    },
    {
        "loss": 2.3753,
        "grad_norm": 1.7762266397476196,
        "learning_rate": 9.899197499412333e-05,
        "epoch": 0.688740015459933,
        "step": 5346
    },
    {
        "loss": 1.8223,
        "grad_norm": 3.3219337463378906,
        "learning_rate": 9.897978602958365e-05,
        "epoch": 0.688868848234991,
        "step": 5347
    },
    {
        "loss": 1.3859,
        "grad_norm": 3.0123417377471924,
        "learning_rate": 9.896752457272829e-05,
        "epoch": 0.688997681010049,
        "step": 5348
    },
    {
        "loss": 1.7195,
        "grad_norm": 3.110630989074707,
        "learning_rate": 9.895519064170478e-05,
        "epoch": 0.6891265137851069,
        "step": 5349
    },
    {
        "loss": 1.5866,
        "grad_norm": 4.053679466247559,
        "learning_rate": 9.89427842547679e-05,
        "epoch": 0.689255346560165,
        "step": 5350
    },
    {
        "loss": 2.5861,
        "grad_norm": 1.6221736669540405,
        "learning_rate": 9.893030543027966e-05,
        "epoch": 0.6893841793352229,
        "step": 5351
    },
    {
        "loss": 1.7225,
        "grad_norm": 3.116251230239868,
        "learning_rate": 9.891775418670932e-05,
        "epoch": 0.6895130121102808,
        "step": 5352
    },
    {
        "loss": 2.218,
        "grad_norm": 1.9511799812316895,
        "learning_rate": 9.890513054263326e-05,
        "epoch": 0.6896418448853389,
        "step": 5353
    },
    {
        "loss": 1.8967,
        "grad_norm": 1.8004891872406006,
        "learning_rate": 9.889243451673504e-05,
        "epoch": 0.6897706776603968,
        "step": 5354
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.174062728881836,
        "learning_rate": 9.887966612780541e-05,
        "epoch": 0.6898995104354548,
        "step": 5355
    },
    {
        "loss": 1.9686,
        "grad_norm": 3.15903377532959,
        "learning_rate": 9.886682539474214e-05,
        "epoch": 0.6900283432105128,
        "step": 5356
    },
    {
        "loss": 2.3001,
        "grad_norm": 2.285311460494995,
        "learning_rate": 9.885391233655008e-05,
        "epoch": 0.6901571759855707,
        "step": 5357
    },
    {
        "loss": 2.2444,
        "grad_norm": 2.9444639682769775,
        "learning_rate": 9.884092697234117e-05,
        "epoch": 0.6902860087606287,
        "step": 5358
    },
    {
        "loss": 2.0659,
        "grad_norm": 2.219665765762329,
        "learning_rate": 9.882786932133431e-05,
        "epoch": 0.6904148415356867,
        "step": 5359
    },
    {
        "loss": 1.833,
        "grad_norm": 2.284274101257324,
        "learning_rate": 9.881473940285545e-05,
        "epoch": 0.6905436743107447,
        "step": 5360
    },
    {
        "loss": 2.0785,
        "grad_norm": 1.8529363870620728,
        "learning_rate": 9.880153723633745e-05,
        "epoch": 0.6906725070858026,
        "step": 5361
    },
    {
        "loss": 1.7739,
        "grad_norm": 2.453641414642334,
        "learning_rate": 9.878826284132012e-05,
        "epoch": 0.6908013398608606,
        "step": 5362
    },
    {
        "loss": 2.2406,
        "grad_norm": 2.518507242202759,
        "learning_rate": 9.877491623745018e-05,
        "epoch": 0.6909301726359186,
        "step": 5363
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.9817842245101929,
        "learning_rate": 9.876149744448122e-05,
        "epoch": 0.6910590054109765,
        "step": 5364
    },
    {
        "loss": 2.0736,
        "grad_norm": 2.3416686058044434,
        "learning_rate": 9.874800648227365e-05,
        "epoch": 0.6911878381860346,
        "step": 5365
    },
    {
        "loss": 2.7698,
        "grad_norm": 1.2946252822875977,
        "learning_rate": 9.873444337079472e-05,
        "epoch": 0.6913166709610925,
        "step": 5366
    },
    {
        "loss": 1.568,
        "grad_norm": 2.3981950283050537,
        "learning_rate": 9.872080813011841e-05,
        "epoch": 0.6914455037361504,
        "step": 5367
    },
    {
        "loss": 1.6754,
        "grad_norm": 1.7599496841430664,
        "learning_rate": 9.870710078042555e-05,
        "epoch": 0.6915743365112085,
        "step": 5368
    },
    {
        "loss": 1.7806,
        "grad_norm": 2.139155626296997,
        "learning_rate": 9.869332134200364e-05,
        "epoch": 0.6917031692862664,
        "step": 5369
    },
    {
        "loss": 2.169,
        "grad_norm": 2.0590906143188477,
        "learning_rate": 9.867946983524685e-05,
        "epoch": 0.6918320020613244,
        "step": 5370
    },
    {
        "loss": 1.604,
        "grad_norm": 2.728623390197754,
        "learning_rate": 9.866554628065602e-05,
        "epoch": 0.6919608348363824,
        "step": 5371
    },
    {
        "loss": 1.5906,
        "grad_norm": 2.8775103092193604,
        "learning_rate": 9.865155069883873e-05,
        "epoch": 0.6920896676114403,
        "step": 5372
    },
    {
        "loss": 2.215,
        "grad_norm": 2.1967546939849854,
        "learning_rate": 9.863748311050899e-05,
        "epoch": 0.6922185003864983,
        "step": 5373
    },
    {
        "loss": 2.2472,
        "grad_norm": 2.3090076446533203,
        "learning_rate": 9.862334353648753e-05,
        "epoch": 0.6923473331615563,
        "step": 5374
    },
    {
        "loss": 2.3711,
        "grad_norm": 2.0169594287872314,
        "learning_rate": 9.860913199770154e-05,
        "epoch": 0.6924761659366143,
        "step": 5375
    },
    {
        "loss": 1.0968,
        "grad_norm": 2.396456241607666,
        "learning_rate": 9.859484851518472e-05,
        "epoch": 0.6926049987116722,
        "step": 5376
    },
    {
        "loss": 2.1354,
        "grad_norm": 1.7330410480499268,
        "learning_rate": 9.858049311007734e-05,
        "epoch": 0.6927338314867302,
        "step": 5377
    },
    {
        "loss": 1.9934,
        "grad_norm": 2.408466339111328,
        "learning_rate": 9.8566065803626e-05,
        "epoch": 0.6928626642617882,
        "step": 5378
    },
    {
        "loss": 2.2132,
        "grad_norm": 2.111604690551758,
        "learning_rate": 9.855156661718377e-05,
        "epoch": 0.6929914970368461,
        "step": 5379
    },
    {
        "loss": 2.0504,
        "grad_norm": 1.2245948314666748,
        "learning_rate": 9.853699557221015e-05,
        "epoch": 0.6931203298119042,
        "step": 5380
    },
    {
        "loss": 2.2478,
        "grad_norm": 1.7042070627212524,
        "learning_rate": 9.852235269027093e-05,
        "epoch": 0.6932491625869621,
        "step": 5381
    },
    {
        "loss": 2.0679,
        "grad_norm": 1.7417430877685547,
        "learning_rate": 9.850763799303821e-05,
        "epoch": 0.69337799536202,
        "step": 5382
    },
    {
        "loss": 2.1203,
        "grad_norm": 2.498793601989746,
        "learning_rate": 9.849285150229046e-05,
        "epoch": 0.6935068281370781,
        "step": 5383
    },
    {
        "loss": 2.4471,
        "grad_norm": 1.7150577306747437,
        "learning_rate": 9.847799323991234e-05,
        "epoch": 0.693635660912136,
        "step": 5384
    },
    {
        "loss": 2.1603,
        "grad_norm": 1.798287034034729,
        "learning_rate": 9.846306322789474e-05,
        "epoch": 0.6937644936871941,
        "step": 5385
    },
    {
        "loss": 1.6496,
        "grad_norm": 3.006657123565674,
        "learning_rate": 9.844806148833479e-05,
        "epoch": 0.693893326462252,
        "step": 5386
    },
    {
        "loss": 1.8109,
        "grad_norm": 2.0408122539520264,
        "learning_rate": 9.843298804343572e-05,
        "epoch": 0.6940221592373099,
        "step": 5387
    },
    {
        "loss": 2.1313,
        "grad_norm": 2.256051540374756,
        "learning_rate": 9.841784291550693e-05,
        "epoch": 0.694150992012368,
        "step": 5388
    },
    {
        "loss": 1.9971,
        "grad_norm": 1.9218560457229614,
        "learning_rate": 9.84026261269639e-05,
        "epoch": 0.6942798247874259,
        "step": 5389
    },
    {
        "loss": 2.3543,
        "grad_norm": 2.3944551944732666,
        "learning_rate": 9.838733770032817e-05,
        "epoch": 0.6944086575624839,
        "step": 5390
    },
    {
        "loss": 2.311,
        "grad_norm": 2.0576560497283936,
        "learning_rate": 9.837197765822732e-05,
        "epoch": 0.6945374903375419,
        "step": 5391
    },
    {
        "loss": 2.0797,
        "grad_norm": 1.886305332183838,
        "learning_rate": 9.835654602339488e-05,
        "epoch": 0.6946663231125998,
        "step": 5392
    },
    {
        "loss": 1.858,
        "grad_norm": 2.382066249847412,
        "learning_rate": 9.83410428186704e-05,
        "epoch": 0.6947951558876578,
        "step": 5393
    },
    {
        "loss": 1.8329,
        "grad_norm": 1.6703375577926636,
        "learning_rate": 9.832546806699934e-05,
        "epoch": 0.6949239886627158,
        "step": 5394
    },
    {
        "loss": 1.286,
        "grad_norm": 3.464637517929077,
        "learning_rate": 9.8309821791433e-05,
        "epoch": 0.6950528214377738,
        "step": 5395
    },
    {
        "loss": 1.5892,
        "grad_norm": 2.9031105041503906,
        "learning_rate": 9.82941040151286e-05,
        "epoch": 0.6951816542128317,
        "step": 5396
    },
    {
        "loss": 2.0675,
        "grad_norm": 1.9218086004257202,
        "learning_rate": 9.827831476134917e-05,
        "epoch": 0.6953104869878897,
        "step": 5397
    },
    {
        "loss": 2.219,
        "grad_norm": 2.131833553314209,
        "learning_rate": 9.826245405346353e-05,
        "epoch": 0.6954393197629477,
        "step": 5398
    },
    {
        "loss": 1.7253,
        "grad_norm": 2.0215911865234375,
        "learning_rate": 9.824652191494623e-05,
        "epoch": 0.6955681525380056,
        "step": 5399
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.713958978652954,
        "learning_rate": 9.82305183693776e-05,
        "epoch": 0.6956969853130637,
        "step": 5400
    },
    {
        "loss": 2.0112,
        "grad_norm": 2.6477901935577393,
        "learning_rate": 9.821444344044358e-05,
        "epoch": 0.6958258180881216,
        "step": 5401
    },
    {
        "loss": 1.9311,
        "grad_norm": 2.671802282333374,
        "learning_rate": 9.819829715193583e-05,
        "epoch": 0.6959546508631796,
        "step": 5402
    },
    {
        "loss": 2.2631,
        "grad_norm": 2.257065534591675,
        "learning_rate": 9.818207952775156e-05,
        "epoch": 0.6960834836382376,
        "step": 5403
    },
    {
        "loss": 2.0403,
        "grad_norm": 2.110966205596924,
        "learning_rate": 9.816579059189361e-05,
        "epoch": 0.6962123164132955,
        "step": 5404
    },
    {
        "loss": 1.8218,
        "grad_norm": 2.6850273609161377,
        "learning_rate": 9.814943036847032e-05,
        "epoch": 0.6963411491883535,
        "step": 5405
    },
    {
        "loss": 2.2743,
        "grad_norm": 2.731942653656006,
        "learning_rate": 9.813299888169562e-05,
        "epoch": 0.6964699819634115,
        "step": 5406
    },
    {
        "loss": 2.3694,
        "grad_norm": 2.3375871181488037,
        "learning_rate": 9.811649615588881e-05,
        "epoch": 0.6965988147384695,
        "step": 5407
    },
    {
        "loss": 1.8484,
        "grad_norm": 1.5902236700057983,
        "learning_rate": 9.809992221547473e-05,
        "epoch": 0.6967276475135274,
        "step": 5408
    },
    {
        "loss": 1.6303,
        "grad_norm": 3.059628486633301,
        "learning_rate": 9.80832770849835e-05,
        "epoch": 0.6968564802885854,
        "step": 5409
    },
    {
        "loss": 1.7569,
        "grad_norm": 2.587371349334717,
        "learning_rate": 9.806656078905073e-05,
        "epoch": 0.6969853130636434,
        "step": 5410
    },
    {
        "loss": 2.3891,
        "grad_norm": 1.986634373664856,
        "learning_rate": 9.804977335241726e-05,
        "epoch": 0.6971141458387013,
        "step": 5411
    },
    {
        "loss": 2.528,
        "grad_norm": 1.868036150932312,
        "learning_rate": 9.803291479992926e-05,
        "epoch": 0.6972429786137594,
        "step": 5412
    },
    {
        "loss": 2.1144,
        "grad_norm": 1.5721447467803955,
        "learning_rate": 9.80159851565382e-05,
        "epoch": 0.6973718113888173,
        "step": 5413
    },
    {
        "loss": 1.7421,
        "grad_norm": 2.572664260864258,
        "learning_rate": 9.799898444730066e-05,
        "epoch": 0.6975006441638752,
        "step": 5414
    },
    {
        "loss": 2.1328,
        "grad_norm": 1.560965657234192,
        "learning_rate": 9.798191269737853e-05,
        "epoch": 0.6976294769389333,
        "step": 5415
    },
    {
        "loss": 2.4332,
        "grad_norm": 1.7294139862060547,
        "learning_rate": 9.796476993203876e-05,
        "epoch": 0.6977583097139912,
        "step": 5416
    },
    {
        "loss": 2.0725,
        "grad_norm": 1.9375300407409668,
        "learning_rate": 9.79475561766534e-05,
        "epoch": 0.6978871424890493,
        "step": 5417
    },
    {
        "loss": 1.6394,
        "grad_norm": 1.9392145872116089,
        "learning_rate": 9.79302714566996e-05,
        "epoch": 0.6980159752641072,
        "step": 5418
    },
    {
        "loss": 2.167,
        "grad_norm": 1.9218144416809082,
        "learning_rate": 9.791291579775955e-05,
        "epoch": 0.6981448080391651,
        "step": 5419
    },
    {
        "loss": 1.7302,
        "grad_norm": 2.608811378479004,
        "learning_rate": 9.789548922552039e-05,
        "epoch": 0.6982736408142232,
        "step": 5420
    },
    {
        "loss": 1.9085,
        "grad_norm": 2.0856025218963623,
        "learning_rate": 9.787799176577425e-05,
        "epoch": 0.6984024735892811,
        "step": 5421
    },
    {
        "loss": 1.939,
        "grad_norm": 1.7279163599014282,
        "learning_rate": 9.786042344441819e-05,
        "epoch": 0.6985313063643391,
        "step": 5422
    },
    {
        "loss": 1.4658,
        "grad_norm": 2.3463571071624756,
        "learning_rate": 9.78427842874541e-05,
        "epoch": 0.6986601391393971,
        "step": 5423
    },
    {
        "loss": 2.2637,
        "grad_norm": 1.5823580026626587,
        "learning_rate": 9.782507432098875e-05,
        "epoch": 0.698788971914455,
        "step": 5424
    },
    {
        "loss": 2.0103,
        "grad_norm": 1.7290149927139282,
        "learning_rate": 9.780729357123371e-05,
        "epoch": 0.698917804689513,
        "step": 5425
    },
    {
        "loss": 1.3147,
        "grad_norm": 3.2586944103240967,
        "learning_rate": 9.778944206450525e-05,
        "epoch": 0.699046637464571,
        "step": 5426
    },
    {
        "loss": 2.2428,
        "grad_norm": 1.353317379951477,
        "learning_rate": 9.777151982722447e-05,
        "epoch": 0.699175470239629,
        "step": 5427
    },
    {
        "loss": 2.4229,
        "grad_norm": 1.8544424772262573,
        "learning_rate": 9.775352688591708e-05,
        "epoch": 0.6993043030146869,
        "step": 5428
    },
    {
        "loss": 1.1279,
        "grad_norm": 3.2262699604034424,
        "learning_rate": 9.773546326721343e-05,
        "epoch": 0.6994331357897449,
        "step": 5429
    },
    {
        "loss": 1.8437,
        "grad_norm": 2.164669990539551,
        "learning_rate": 9.771732899784855e-05,
        "epoch": 0.6995619685648029,
        "step": 5430
    },
    {
        "loss": 1.6676,
        "grad_norm": 3.4162113666534424,
        "learning_rate": 9.76991241046619e-05,
        "epoch": 0.6996908013398608,
        "step": 5431
    },
    {
        "loss": 2.0287,
        "grad_norm": 1.481572151184082,
        "learning_rate": 9.768084861459765e-05,
        "epoch": 0.6998196341149189,
        "step": 5432
    },
    {
        "loss": 1.4473,
        "grad_norm": 2.2040817737579346,
        "learning_rate": 9.76625025547043e-05,
        "epoch": 0.6999484668899768,
        "step": 5433
    },
    {
        "loss": 2.0234,
        "grad_norm": 1.986051082611084,
        "learning_rate": 9.764408595213488e-05,
        "epoch": 0.7000772996650347,
        "step": 5434
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.6560049057006836,
        "learning_rate": 9.762559883414677e-05,
        "epoch": 0.7002061324400928,
        "step": 5435
    },
    {
        "loss": 1.9431,
        "grad_norm": 1.7410634756088257,
        "learning_rate": 9.760704122810176e-05,
        "epoch": 0.7003349652151507,
        "step": 5436
    },
    {
        "loss": 1.9888,
        "grad_norm": 1.716046690940857,
        "learning_rate": 9.758841316146597e-05,
        "epoch": 0.7004637979902087,
        "step": 5437
    },
    {
        "loss": 2.1574,
        "grad_norm": 1.9087750911712646,
        "learning_rate": 9.756971466180978e-05,
        "epoch": 0.7005926307652667,
        "step": 5438
    },
    {
        "loss": 1.7934,
        "grad_norm": 3.414809465408325,
        "learning_rate": 9.755094575680779e-05,
        "epoch": 0.7007214635403246,
        "step": 5439
    },
    {
        "loss": 1.1721,
        "grad_norm": 3.108367443084717,
        "learning_rate": 9.753210647423888e-05,
        "epoch": 0.7008502963153826,
        "step": 5440
    },
    {
        "loss": 2.361,
        "grad_norm": 1.8089581727981567,
        "learning_rate": 9.751319684198606e-05,
        "epoch": 0.7009791290904406,
        "step": 5441
    },
    {
        "loss": 2.257,
        "grad_norm": 1.3950241804122925,
        "learning_rate": 9.749421688803639e-05,
        "epoch": 0.7011079618654986,
        "step": 5442
    },
    {
        "loss": 1.7686,
        "grad_norm": 2.2892677783966064,
        "learning_rate": 9.747516664048109e-05,
        "epoch": 0.7012367946405565,
        "step": 5443
    },
    {
        "loss": 1.2862,
        "grad_norm": 3.330803632736206,
        "learning_rate": 9.745604612751542e-05,
        "epoch": 0.7013656274156145,
        "step": 5444
    },
    {
        "loss": 1.6968,
        "grad_norm": 2.289550542831421,
        "learning_rate": 9.743685537743856e-05,
        "epoch": 0.7014944601906725,
        "step": 5445
    },
    {
        "loss": 2.1774,
        "grad_norm": 1.656843662261963,
        "learning_rate": 9.741759441865375e-05,
        "epoch": 0.7016232929657304,
        "step": 5446
    },
    {
        "loss": 1.9795,
        "grad_norm": 3.1017403602600098,
        "learning_rate": 9.739826327966804e-05,
        "epoch": 0.7017521257407885,
        "step": 5447
    },
    {
        "loss": 1.7234,
        "grad_norm": 2.752264976501465,
        "learning_rate": 9.737886198909242e-05,
        "epoch": 0.7018809585158464,
        "step": 5448
    },
    {
        "loss": 1.9637,
        "grad_norm": 2.061204433441162,
        "learning_rate": 9.735939057564168e-05,
        "epoch": 0.7020097912909045,
        "step": 5449
    },
    {
        "loss": 2.0057,
        "grad_norm": 2.5425190925598145,
        "learning_rate": 9.733984906813442e-05,
        "epoch": 0.7021386240659624,
        "step": 5450
    },
    {
        "loss": 1.1394,
        "grad_norm": 2.8205840587615967,
        "learning_rate": 9.732023749549293e-05,
        "epoch": 0.7022674568410203,
        "step": 5451
    },
    {
        "loss": 1.7128,
        "grad_norm": 2.328278064727783,
        "learning_rate": 9.730055588674325e-05,
        "epoch": 0.7023962896160784,
        "step": 5452
    },
    {
        "loss": 1.9034,
        "grad_norm": 2.1842033863067627,
        "learning_rate": 9.728080427101505e-05,
        "epoch": 0.7025251223911363,
        "step": 5453
    },
    {
        "loss": 2.0114,
        "grad_norm": 2.433077812194824,
        "learning_rate": 9.726098267754162e-05,
        "epoch": 0.7026539551661943,
        "step": 5454
    },
    {
        "loss": 1.846,
        "grad_norm": 2.5189778804779053,
        "learning_rate": 9.724109113565985e-05,
        "epoch": 0.7027827879412523,
        "step": 5455
    },
    {
        "loss": 2.0451,
        "grad_norm": 2.6966612339019775,
        "learning_rate": 9.722112967481007e-05,
        "epoch": 0.7029116207163102,
        "step": 5456
    },
    {
        "loss": 2.4436,
        "grad_norm": 1.3444080352783203,
        "learning_rate": 9.720109832453618e-05,
        "epoch": 0.7030404534913682,
        "step": 5457
    },
    {
        "loss": 2.6081,
        "grad_norm": 2.105926752090454,
        "learning_rate": 9.718099711448554e-05,
        "epoch": 0.7031692862664262,
        "step": 5458
    },
    {
        "loss": 1.8302,
        "grad_norm": 2.4984798431396484,
        "learning_rate": 9.716082607440879e-05,
        "epoch": 0.7032981190414842,
        "step": 5459
    },
    {
        "loss": 2.0306,
        "grad_norm": 1.9701184034347534,
        "learning_rate": 9.714058523416001e-05,
        "epoch": 0.7034269518165421,
        "step": 5460
    },
    {
        "loss": 1.5932,
        "grad_norm": 2.8255622386932373,
        "learning_rate": 9.712027462369656e-05,
        "epoch": 0.7035557845916001,
        "step": 5461
    },
    {
        "loss": 2.2465,
        "grad_norm": 1.7051892280578613,
        "learning_rate": 9.709989427307908e-05,
        "epoch": 0.7036846173666581,
        "step": 5462
    },
    {
        "loss": 2.0992,
        "grad_norm": 1.6761157512664795,
        "learning_rate": 9.707944421247146e-05,
        "epoch": 0.703813450141716,
        "step": 5463
    },
    {
        "loss": 2.5048,
        "grad_norm": 1.8258824348449707,
        "learning_rate": 9.705892447214064e-05,
        "epoch": 0.7039422829167741,
        "step": 5464
    },
    {
        "loss": 1.0909,
        "grad_norm": 2.783975839614868,
        "learning_rate": 9.703833508245678e-05,
        "epoch": 0.704071115691832,
        "step": 5465
    },
    {
        "loss": 1.8065,
        "grad_norm": 2.9859564304351807,
        "learning_rate": 9.701767607389321e-05,
        "epoch": 0.7041999484668899,
        "step": 5466
    },
    {
        "loss": 2.1703,
        "grad_norm": 2.4149444103240967,
        "learning_rate": 9.69969474770261e-05,
        "epoch": 0.704328781241948,
        "step": 5467
    },
    {
        "loss": 2.4093,
        "grad_norm": 2.052236318588257,
        "learning_rate": 9.697614932253475e-05,
        "epoch": 0.7044576140170059,
        "step": 5468
    },
    {
        "loss": 1.9652,
        "grad_norm": 1.9403131008148193,
        "learning_rate": 9.695528164120142e-05,
        "epoch": 0.704586446792064,
        "step": 5469
    },
    {
        "loss": 1.3899,
        "grad_norm": 4.409753799438477,
        "learning_rate": 9.693434446391115e-05,
        "epoch": 0.7047152795671219,
        "step": 5470
    },
    {
        "loss": 1.3188,
        "grad_norm": 2.4523799419403076,
        "learning_rate": 9.691333782165197e-05,
        "epoch": 0.7048441123421798,
        "step": 5471
    },
    {
        "loss": 1.7639,
        "grad_norm": 2.6281330585479736,
        "learning_rate": 9.689226174551467e-05,
        "epoch": 0.7049729451172378,
        "step": 5472
    },
    {
        "loss": 2.4541,
        "grad_norm": 1.6840770244598389,
        "learning_rate": 9.687111626669277e-05,
        "epoch": 0.7051017778922958,
        "step": 5473
    },
    {
        "loss": 2.29,
        "grad_norm": 1.7290832996368408,
        "learning_rate": 9.684990141648258e-05,
        "epoch": 0.7052306106673538,
        "step": 5474
    },
    {
        "loss": 1.8396,
        "grad_norm": 1.5039241313934326,
        "learning_rate": 9.682861722628301e-05,
        "epoch": 0.7053594434424117,
        "step": 5475
    },
    {
        "loss": 1.7962,
        "grad_norm": 1.8140281438827515,
        "learning_rate": 9.680726372759565e-05,
        "epoch": 0.7054882762174697,
        "step": 5476
    },
    {
        "loss": 1.4323,
        "grad_norm": 3.222027540206909,
        "learning_rate": 9.67858409520247e-05,
        "epoch": 0.7056171089925277,
        "step": 5477
    },
    {
        "loss": 1.9657,
        "grad_norm": 2.8980774879455566,
        "learning_rate": 9.676434893127675e-05,
        "epoch": 0.7057459417675856,
        "step": 5478
    },
    {
        "loss": 1.9532,
        "grad_norm": 2.29343581199646,
        "learning_rate": 9.674278769716101e-05,
        "epoch": 0.7058747745426437,
        "step": 5479
    },
    {
        "loss": 1.6067,
        "grad_norm": 1.8531147241592407,
        "learning_rate": 9.672115728158913e-05,
        "epoch": 0.7060036073177016,
        "step": 5480
    },
    {
        "loss": 2.39,
        "grad_norm": 1.5709971189498901,
        "learning_rate": 9.669945771657505e-05,
        "epoch": 0.7061324400927596,
        "step": 5481
    },
    {
        "loss": 2.4113,
        "grad_norm": 1.3562071323394775,
        "learning_rate": 9.667768903423511e-05,
        "epoch": 0.7062612728678176,
        "step": 5482
    },
    {
        "loss": 1.0977,
        "grad_norm": 2.9003994464874268,
        "learning_rate": 9.665585126678805e-05,
        "epoch": 0.7063901056428755,
        "step": 5483
    },
    {
        "loss": 1.954,
        "grad_norm": 2.204096555709839,
        "learning_rate": 9.663394444655465e-05,
        "epoch": 0.7065189384179336,
        "step": 5484
    },
    {
        "loss": 2.223,
        "grad_norm": 1.5904089212417603,
        "learning_rate": 9.661196860595804e-05,
        "epoch": 0.7066477711929915,
        "step": 5485
    },
    {
        "loss": 2.1806,
        "grad_norm": 1.8430113792419434,
        "learning_rate": 9.658992377752348e-05,
        "epoch": 0.7067766039680494,
        "step": 5486
    },
    {
        "loss": 2.0813,
        "grad_norm": 1.6201776266098022,
        "learning_rate": 9.656780999387831e-05,
        "epoch": 0.7069054367431075,
        "step": 5487
    },
    {
        "loss": 1.6107,
        "grad_norm": 2.9120583534240723,
        "learning_rate": 9.654562728775197e-05,
        "epoch": 0.7070342695181654,
        "step": 5488
    },
    {
        "loss": 1.0535,
        "grad_norm": 1.8695815801620483,
        "learning_rate": 9.652337569197583e-05,
        "epoch": 0.7071631022932234,
        "step": 5489
    },
    {
        "loss": 1.932,
        "grad_norm": 1.7825146913528442,
        "learning_rate": 9.650105523948327e-05,
        "epoch": 0.7072919350682814,
        "step": 5490
    },
    {
        "loss": 1.6565,
        "grad_norm": 3.468740463256836,
        "learning_rate": 9.647866596330958e-05,
        "epoch": 0.7074207678433393,
        "step": 5491
    },
    {
        "loss": 2.1011,
        "grad_norm": 2.37943434715271,
        "learning_rate": 9.645620789659194e-05,
        "epoch": 0.7075496006183973,
        "step": 5492
    },
    {
        "loss": 2.1373,
        "grad_norm": 2.6031861305236816,
        "learning_rate": 9.643368107256928e-05,
        "epoch": 0.7076784333934553,
        "step": 5493
    },
    {
        "loss": 1.9424,
        "grad_norm": 2.5405213832855225,
        "learning_rate": 9.641108552458234e-05,
        "epoch": 0.7078072661685133,
        "step": 5494
    },
    {
        "loss": 1.6514,
        "grad_norm": 2.150563955307007,
        "learning_rate": 9.638842128607356e-05,
        "epoch": 0.7079360989435712,
        "step": 5495
    },
    {
        "loss": 1.6714,
        "grad_norm": 2.229024887084961,
        "learning_rate": 9.636568839058704e-05,
        "epoch": 0.7080649317186292,
        "step": 5496
    },
    {
        "loss": 2.2097,
        "grad_norm": 2.043022632598877,
        "learning_rate": 9.63428868717685e-05,
        "epoch": 0.7081937644936872,
        "step": 5497
    },
    {
        "loss": 1.7358,
        "grad_norm": 2.1499805450439453,
        "learning_rate": 9.632001676336523e-05,
        "epoch": 0.7083225972687451,
        "step": 5498
    },
    {
        "loss": 1.6863,
        "grad_norm": 1.7362221479415894,
        "learning_rate": 9.629707809922604e-05,
        "epoch": 0.7084514300438032,
        "step": 5499
    },
    {
        "loss": 2.3987,
        "grad_norm": 2.857635498046875,
        "learning_rate": 9.627407091330118e-05,
        "epoch": 0.7085802628188611,
        "step": 5500
    },
    {
        "loss": 2.6082,
        "grad_norm": 1.5229690074920654,
        "learning_rate": 9.625099523964236e-05,
        "epoch": 0.7087090955939191,
        "step": 5501
    },
    {
        "loss": 2.0635,
        "grad_norm": 2.9344425201416016,
        "learning_rate": 9.622785111240264e-05,
        "epoch": 0.7088379283689771,
        "step": 5502
    },
    {
        "loss": 2.2199,
        "grad_norm": 1.9259111881256104,
        "learning_rate": 9.620463856583633e-05,
        "epoch": 0.708966761144035,
        "step": 5503
    },
    {
        "loss": 2.3802,
        "grad_norm": 1.8648009300231934,
        "learning_rate": 9.61813576342991e-05,
        "epoch": 0.709095593919093,
        "step": 5504
    },
    {
        "loss": 2.3396,
        "grad_norm": 1.8476791381835938,
        "learning_rate": 9.615800835224778e-05,
        "epoch": 0.709224426694151,
        "step": 5505
    },
    {
        "loss": 1.812,
        "grad_norm": 2.0832345485687256,
        "learning_rate": 9.613459075424035e-05,
        "epoch": 0.709353259469209,
        "step": 5506
    },
    {
        "loss": 1.6903,
        "grad_norm": 3.0748026371002197,
        "learning_rate": 9.611110487493594e-05,
        "epoch": 0.709482092244267,
        "step": 5507
    },
    {
        "loss": 1.6262,
        "grad_norm": 2.001384973526001,
        "learning_rate": 9.608755074909473e-05,
        "epoch": 0.7096109250193249,
        "step": 5508
    },
    {
        "loss": 0.9747,
        "grad_norm": 2.886780261993408,
        "learning_rate": 9.606392841157787e-05,
        "epoch": 0.7097397577943829,
        "step": 5509
    },
    {
        "loss": 2.146,
        "grad_norm": 2.019319772720337,
        "learning_rate": 9.604023789734753e-05,
        "epoch": 0.7098685905694408,
        "step": 5510
    },
    {
        "loss": 1.8807,
        "grad_norm": 2.295102834701538,
        "learning_rate": 9.601647924146673e-05,
        "epoch": 0.7099974233444989,
        "step": 5511
    },
    {
        "loss": 1.6128,
        "grad_norm": 3.3315541744232178,
        "learning_rate": 9.599265247909938e-05,
        "epoch": 0.7101262561195568,
        "step": 5512
    },
    {
        "loss": 2.081,
        "grad_norm": 1.5687036514282227,
        "learning_rate": 9.596875764551019e-05,
        "epoch": 0.7102550888946148,
        "step": 5513
    },
    {
        "loss": 2.2082,
        "grad_norm": 2.2210237979888916,
        "learning_rate": 9.594479477606456e-05,
        "epoch": 0.7103839216696728,
        "step": 5514
    },
    {
        "loss": 1.9358,
        "grad_norm": 1.877769947052002,
        "learning_rate": 9.592076390622864e-05,
        "epoch": 0.7105127544447307,
        "step": 5515
    },
    {
        "loss": 2.5084,
        "grad_norm": 1.922873854637146,
        "learning_rate": 9.589666507156922e-05,
        "epoch": 0.7106415872197888,
        "step": 5516
    },
    {
        "loss": 2.1581,
        "grad_norm": 2.122659921646118,
        "learning_rate": 9.587249830775367e-05,
        "epoch": 0.7107704199948467,
        "step": 5517
    },
    {
        "loss": 2.257,
        "grad_norm": 2.327314853668213,
        "learning_rate": 9.584826365054991e-05,
        "epoch": 0.7108992527699046,
        "step": 5518
    },
    {
        "loss": 1.7155,
        "grad_norm": 3.5776116847991943,
        "learning_rate": 9.582396113582634e-05,
        "epoch": 0.7110280855449627,
        "step": 5519
    },
    {
        "loss": 2.3848,
        "grad_norm": 1.5936251878738403,
        "learning_rate": 9.579959079955179e-05,
        "epoch": 0.7111569183200206,
        "step": 5520
    },
    {
        "loss": 1.8068,
        "grad_norm": 2.265456199645996,
        "learning_rate": 9.577515267779544e-05,
        "epoch": 0.7112857510950786,
        "step": 5521
    },
    {
        "loss": 2.1654,
        "grad_norm": 1.7048325538635254,
        "learning_rate": 9.575064680672684e-05,
        "epoch": 0.7114145838701366,
        "step": 5522
    },
    {
        "loss": 2.1258,
        "grad_norm": 2.2852606773376465,
        "learning_rate": 9.572607322261581e-05,
        "epoch": 0.7115434166451945,
        "step": 5523
    },
    {
        "loss": 2.0934,
        "grad_norm": 1.6487373113632202,
        "learning_rate": 9.570143196183239e-05,
        "epoch": 0.7116722494202525,
        "step": 5524
    },
    {
        "loss": 1.1495,
        "grad_norm": 3.439791679382324,
        "learning_rate": 9.56767230608467e-05,
        "epoch": 0.7118010821953105,
        "step": 5525
    },
    {
        "loss": 1.6854,
        "grad_norm": 2.796956777572632,
        "learning_rate": 9.565194655622909e-05,
        "epoch": 0.7119299149703685,
        "step": 5526
    },
    {
        "loss": 2.2708,
        "grad_norm": 2.683267831802368,
        "learning_rate": 9.562710248464994e-05,
        "epoch": 0.7120587477454264,
        "step": 5527
    },
    {
        "loss": 2.2369,
        "grad_norm": 1.8100414276123047,
        "learning_rate": 9.560219088287955e-05,
        "epoch": 0.7121875805204844,
        "step": 5528
    },
    {
        "loss": 1.8374,
        "grad_norm": 2.299313545227051,
        "learning_rate": 9.557721178778829e-05,
        "epoch": 0.7123164132955424,
        "step": 5529
    },
    {
        "loss": 2.3552,
        "grad_norm": 2.78835391998291,
        "learning_rate": 9.555216523634632e-05,
        "epoch": 0.7124452460706003,
        "step": 5530
    },
    {
        "loss": 2.2868,
        "grad_norm": 2.3525984287261963,
        "learning_rate": 9.552705126562365e-05,
        "epoch": 0.7125740788456584,
        "step": 5531
    },
    {
        "loss": 2.2928,
        "grad_norm": 2.6154537200927734,
        "learning_rate": 9.550186991279014e-05,
        "epoch": 0.7127029116207163,
        "step": 5532
    },
    {
        "loss": 1.841,
        "grad_norm": 3.2569289207458496,
        "learning_rate": 9.547662121511534e-05,
        "epoch": 0.7128317443957742,
        "step": 5533
    },
    {
        "loss": 0.6994,
        "grad_norm": 3.974637508392334,
        "learning_rate": 9.545130520996844e-05,
        "epoch": 0.7129605771708323,
        "step": 5534
    },
    {
        "loss": 2.3581,
        "grad_norm": 1.5744233131408691,
        "learning_rate": 9.542592193481833e-05,
        "epoch": 0.7130894099458902,
        "step": 5535
    },
    {
        "loss": 2.2024,
        "grad_norm": 2.5987114906311035,
        "learning_rate": 9.540047142723338e-05,
        "epoch": 0.7132182427209482,
        "step": 5536
    },
    {
        "loss": 1.536,
        "grad_norm": 3.1082446575164795,
        "learning_rate": 9.537495372488153e-05,
        "epoch": 0.7133470754960062,
        "step": 5537
    },
    {
        "loss": 1.5516,
        "grad_norm": 2.282534599304199,
        "learning_rate": 9.534936886553012e-05,
        "epoch": 0.7134759082710641,
        "step": 5538
    },
    {
        "loss": 2.3028,
        "grad_norm": 1.805279016494751,
        "learning_rate": 9.532371688704591e-05,
        "epoch": 0.7136047410461221,
        "step": 5539
    },
    {
        "loss": 1.9991,
        "grad_norm": 2.2268292903900146,
        "learning_rate": 9.529799782739499e-05,
        "epoch": 0.7137335738211801,
        "step": 5540
    },
    {
        "loss": 2.6138,
        "grad_norm": 2.581482172012329,
        "learning_rate": 9.527221172464277e-05,
        "epoch": 0.7138624065962381,
        "step": 5541
    },
    {
        "loss": 2.2071,
        "grad_norm": 2.268507480621338,
        "learning_rate": 9.524635861695382e-05,
        "epoch": 0.713991239371296,
        "step": 5542
    },
    {
        "loss": 2.1926,
        "grad_norm": 1.8082600831985474,
        "learning_rate": 9.522043854259193e-05,
        "epoch": 0.714120072146354,
        "step": 5543
    },
    {
        "loss": 2.1294,
        "grad_norm": 2.4251890182495117,
        "learning_rate": 9.519445153992002e-05,
        "epoch": 0.714248904921412,
        "step": 5544
    },
    {
        "loss": 2.2974,
        "grad_norm": 1.9340516328811646,
        "learning_rate": 9.51683976474e-05,
        "epoch": 0.71437773769647,
        "step": 5545
    },
    {
        "loss": 1.7507,
        "grad_norm": 2.0723211765289307,
        "learning_rate": 9.514227690359283e-05,
        "epoch": 0.714506570471528,
        "step": 5546
    },
    {
        "loss": 2.4734,
        "grad_norm": 1.6121184825897217,
        "learning_rate": 9.51160893471584e-05,
        "epoch": 0.7146354032465859,
        "step": 5547
    },
    {
        "loss": 2.2216,
        "grad_norm": 1.7022032737731934,
        "learning_rate": 9.508983501685552e-05,
        "epoch": 0.714764236021644,
        "step": 5548
    },
    {
        "loss": 1.941,
        "grad_norm": 2.5997426509857178,
        "learning_rate": 9.506351395154176e-05,
        "epoch": 0.7148930687967019,
        "step": 5549
    },
    {
        "loss": 2.1994,
        "grad_norm": 2.501156806945801,
        "learning_rate": 9.503712619017348e-05,
        "epoch": 0.7150219015717598,
        "step": 5550
    },
    {
        "loss": 1.9031,
        "grad_norm": 2.3834245204925537,
        "learning_rate": 9.501067177180579e-05,
        "epoch": 0.7151507343468179,
        "step": 5551
    },
    {
        "loss": 1.3719,
        "grad_norm": 2.482266902923584,
        "learning_rate": 9.498415073559246e-05,
        "epoch": 0.7152795671218758,
        "step": 5552
    },
    {
        "loss": 1.84,
        "grad_norm": 1.917097568511963,
        "learning_rate": 9.49575631207858e-05,
        "epoch": 0.7154083998969338,
        "step": 5553
    },
    {
        "loss": 2.0957,
        "grad_norm": 2.809234380722046,
        "learning_rate": 9.49309089667367e-05,
        "epoch": 0.7155372326719918,
        "step": 5554
    },
    {
        "loss": 2.064,
        "grad_norm": 2.436617851257324,
        "learning_rate": 9.490418831289456e-05,
        "epoch": 0.7156660654470497,
        "step": 5555
    },
    {
        "loss": 2.2016,
        "grad_norm": 2.3395376205444336,
        "learning_rate": 9.48774011988071e-05,
        "epoch": 0.7157948982221077,
        "step": 5556
    },
    {
        "loss": 2.0255,
        "grad_norm": 2.3810200691223145,
        "learning_rate": 9.485054766412049e-05,
        "epoch": 0.7159237309971657,
        "step": 5557
    },
    {
        "loss": 1.7105,
        "grad_norm": 2.0330874919891357,
        "learning_rate": 9.48236277485792e-05,
        "epoch": 0.7160525637722237,
        "step": 5558
    },
    {
        "loss": 1.6035,
        "grad_norm": 3.0759832859039307,
        "learning_rate": 9.479664149202594e-05,
        "epoch": 0.7161813965472816,
        "step": 5559
    },
    {
        "loss": 2.364,
        "grad_norm": 2.0480587482452393,
        "learning_rate": 9.476958893440157e-05,
        "epoch": 0.7163102293223396,
        "step": 5560
    },
    {
        "loss": 2.0615,
        "grad_norm": 1.6840535402297974,
        "learning_rate": 9.474247011574513e-05,
        "epoch": 0.7164390620973976,
        "step": 5561
    },
    {
        "loss": 1.766,
        "grad_norm": 2.9208943843841553,
        "learning_rate": 9.471528507619371e-05,
        "epoch": 0.7165678948724555,
        "step": 5562
    },
    {
        "loss": 2.0586,
        "grad_norm": 1.832494854927063,
        "learning_rate": 9.468803385598242e-05,
        "epoch": 0.7166967276475136,
        "step": 5563
    },
    {
        "loss": 2.0842,
        "grad_norm": 1.6655054092407227,
        "learning_rate": 9.466071649544425e-05,
        "epoch": 0.7168255604225715,
        "step": 5564
    },
    {
        "loss": 2.1243,
        "grad_norm": 2.574185848236084,
        "learning_rate": 9.46333330350102e-05,
        "epoch": 0.7169543931976294,
        "step": 5565
    },
    {
        "loss": 1.6645,
        "grad_norm": 2.9562184810638428,
        "learning_rate": 9.460588351520903e-05,
        "epoch": 0.7170832259726875,
        "step": 5566
    },
    {
        "loss": 1.4411,
        "grad_norm": 2.9274046421051025,
        "learning_rate": 9.457836797666723e-05,
        "epoch": 0.7172120587477454,
        "step": 5567
    },
    {
        "loss": 2.076,
        "grad_norm": 1.8386025428771973,
        "learning_rate": 9.455078646010908e-05,
        "epoch": 0.7173408915228034,
        "step": 5568
    },
    {
        "loss": 1.3557,
        "grad_norm": 1.9325439929962158,
        "learning_rate": 9.452313900635654e-05,
        "epoch": 0.7174697242978614,
        "step": 5569
    },
    {
        "loss": 1.2677,
        "grad_norm": 3.4347777366638184,
        "learning_rate": 9.449542565632901e-05,
        "epoch": 0.7175985570729193,
        "step": 5570
    },
    {
        "loss": 1.7283,
        "grad_norm": 2.0971872806549072,
        "learning_rate": 9.446764645104356e-05,
        "epoch": 0.7177273898479773,
        "step": 5571
    },
    {
        "loss": 1.5872,
        "grad_norm": 3.311427593231201,
        "learning_rate": 9.443980143161468e-05,
        "epoch": 0.7178562226230353,
        "step": 5572
    },
    {
        "loss": 1.8145,
        "grad_norm": 2.3755767345428467,
        "learning_rate": 9.441189063925426e-05,
        "epoch": 0.7179850553980933,
        "step": 5573
    },
    {
        "loss": 2.0521,
        "grad_norm": 2.112436056137085,
        "learning_rate": 9.438391411527156e-05,
        "epoch": 0.7181138881731512,
        "step": 5574
    },
    {
        "loss": 2.1542,
        "grad_norm": 3.3654398918151855,
        "learning_rate": 9.435587190107307e-05,
        "epoch": 0.7182427209482092,
        "step": 5575
    },
    {
        "loss": 1.9282,
        "grad_norm": 2.9315848350524902,
        "learning_rate": 9.432776403816255e-05,
        "epoch": 0.7183715537232672,
        "step": 5576
    },
    {
        "loss": 1.9094,
        "grad_norm": 3.172574281692505,
        "learning_rate": 9.4299590568141e-05,
        "epoch": 0.7185003864983251,
        "step": 5577
    },
    {
        "loss": 2.0181,
        "grad_norm": 2.5890955924987793,
        "learning_rate": 9.427135153270632e-05,
        "epoch": 0.7186292192733832,
        "step": 5578
    },
    {
        "loss": 1.8013,
        "grad_norm": 2.680934190750122,
        "learning_rate": 9.424304697365364e-05,
        "epoch": 0.7187580520484411,
        "step": 5579
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.182800054550171,
        "learning_rate": 9.421467693287503e-05,
        "epoch": 0.718886884823499,
        "step": 5580
    },
    {
        "loss": 2.1413,
        "grad_norm": 2.2269997596740723,
        "learning_rate": 9.418624145235936e-05,
        "epoch": 0.7190157175985571,
        "step": 5581
    },
    {
        "loss": 2.54,
        "grad_norm": 1.9559444189071655,
        "learning_rate": 9.415774057419249e-05,
        "epoch": 0.719144550373615,
        "step": 5582
    },
    {
        "loss": 2.6802,
        "grad_norm": 1.9694581031799316,
        "learning_rate": 9.412917434055698e-05,
        "epoch": 0.7192733831486731,
        "step": 5583
    },
    {
        "loss": 1.5843,
        "grad_norm": 3.4502432346343994,
        "learning_rate": 9.41005427937322e-05,
        "epoch": 0.719402215923731,
        "step": 5584
    },
    {
        "loss": 1.1918,
        "grad_norm": 3.026721239089966,
        "learning_rate": 9.407184597609411e-05,
        "epoch": 0.7195310486987889,
        "step": 5585
    },
    {
        "loss": 1.4522,
        "grad_norm": 1.789908766746521,
        "learning_rate": 9.404308393011533e-05,
        "epoch": 0.719659881473847,
        "step": 5586
    },
    {
        "loss": 2.2381,
        "grad_norm": 1.0703047513961792,
        "learning_rate": 9.4014256698365e-05,
        "epoch": 0.7197887142489049,
        "step": 5587
    },
    {
        "loss": 1.9407,
        "grad_norm": 1.9794719219207764,
        "learning_rate": 9.398536432350876e-05,
        "epoch": 0.7199175470239629,
        "step": 5588
    },
    {
        "loss": 1.7562,
        "grad_norm": 1.7446479797363281,
        "learning_rate": 9.395640684830858e-05,
        "epoch": 0.7200463797990209,
        "step": 5589
    },
    {
        "loss": 2.4233,
        "grad_norm": 2.0240912437438965,
        "learning_rate": 9.392738431562287e-05,
        "epoch": 0.7201752125740788,
        "step": 5590
    },
    {
        "loss": 1.9125,
        "grad_norm": 1.6672194004058838,
        "learning_rate": 9.389829676840633e-05,
        "epoch": 0.7203040453491368,
        "step": 5591
    },
    {
        "loss": 2.2483,
        "grad_norm": 2.689816474914551,
        "learning_rate": 9.386914424970981e-05,
        "epoch": 0.7204328781241948,
        "step": 5592
    },
    {
        "loss": 1.6557,
        "grad_norm": 3.1142711639404297,
        "learning_rate": 9.383992680268034e-05,
        "epoch": 0.7205617108992528,
        "step": 5593
    },
    {
        "loss": 2.5604,
        "grad_norm": 2.8175134658813477,
        "learning_rate": 9.381064447056121e-05,
        "epoch": 0.7206905436743107,
        "step": 5594
    },
    {
        "loss": 2.6682,
        "grad_norm": 1.4204835891723633,
        "learning_rate": 9.378129729669147e-05,
        "epoch": 0.7208193764493687,
        "step": 5595
    },
    {
        "loss": 2.5787,
        "grad_norm": 1.6240235567092896,
        "learning_rate": 9.375188532450634e-05,
        "epoch": 0.7209482092244267,
        "step": 5596
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.853248953819275,
        "learning_rate": 9.372240859753685e-05,
        "epoch": 0.7210770419994846,
        "step": 5597
    },
    {
        "loss": 1.3255,
        "grad_norm": 3.8032264709472656,
        "learning_rate": 9.369286715940991e-05,
        "epoch": 0.7212058747745427,
        "step": 5598
    },
    {
        "loss": 1.901,
        "grad_norm": 1.8133747577667236,
        "learning_rate": 9.366326105384822e-05,
        "epoch": 0.7213347075496006,
        "step": 5599
    },
    {
        "loss": 1.2917,
        "grad_norm": 3.042371988296509,
        "learning_rate": 9.363359032467012e-05,
        "epoch": 0.7214635403246586,
        "step": 5600
    },
    {
        "loss": 1.7992,
        "grad_norm": 3.2634830474853516,
        "learning_rate": 9.360385501578966e-05,
        "epoch": 0.7215923730997166,
        "step": 5601
    },
    {
        "loss": 2.0054,
        "grad_norm": 1.6805559396743774,
        "learning_rate": 9.357405517121644e-05,
        "epoch": 0.7217212058747745,
        "step": 5602
    },
    {
        "loss": 2.1434,
        "grad_norm": 2.1604106426239014,
        "learning_rate": 9.354419083505561e-05,
        "epoch": 0.7218500386498325,
        "step": 5603
    },
    {
        "loss": 1.9596,
        "grad_norm": 2.3008463382720947,
        "learning_rate": 9.351426205150774e-05,
        "epoch": 0.7219788714248905,
        "step": 5604
    },
    {
        "loss": 1.9531,
        "grad_norm": 3.090147018432617,
        "learning_rate": 9.348426886486885e-05,
        "epoch": 0.7221077041999485,
        "step": 5605
    },
    {
        "loss": 2.1201,
        "grad_norm": 1.7554349899291992,
        "learning_rate": 9.345421131953012e-05,
        "epoch": 0.7222365369750064,
        "step": 5606
    },
    {
        "loss": 1.7515,
        "grad_norm": 2.5286262035369873,
        "learning_rate": 9.342408945997814e-05,
        "epoch": 0.7223653697500644,
        "step": 5607
    },
    {
        "loss": 1.7654,
        "grad_norm": 2.4206929206848145,
        "learning_rate": 9.339390333079462e-05,
        "epoch": 0.7224942025251224,
        "step": 5608
    },
    {
        "loss": 2.2488,
        "grad_norm": 1.709182620048523,
        "learning_rate": 9.336365297665644e-05,
        "epoch": 0.7226230353001803,
        "step": 5609
    },
    {
        "loss": 2.1491,
        "grad_norm": 2.2404680252075195,
        "learning_rate": 9.33333384423355e-05,
        "epoch": 0.7227518680752384,
        "step": 5610
    },
    {
        "loss": 2.1419,
        "grad_norm": 1.3737266063690186,
        "learning_rate": 9.33029597726986e-05,
        "epoch": 0.7228807008502963,
        "step": 5611
    },
    {
        "loss": 1.463,
        "grad_norm": 3.524895429611206,
        "learning_rate": 9.327251701270765e-05,
        "epoch": 0.7230095336253543,
        "step": 5612
    },
    {
        "loss": 1.9429,
        "grad_norm": 2.1656908988952637,
        "learning_rate": 9.324201020741935e-05,
        "epoch": 0.7231383664004123,
        "step": 5613
    },
    {
        "loss": 2.1012,
        "grad_norm": 2.315194606781006,
        "learning_rate": 9.321143940198509e-05,
        "epoch": 0.7232671991754702,
        "step": 5614
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.1848111152648926,
        "learning_rate": 9.318080464165106e-05,
        "epoch": 0.7233960319505283,
        "step": 5615
    },
    {
        "loss": 1.71,
        "grad_norm": 2.7492473125457764,
        "learning_rate": 9.315010597175817e-05,
        "epoch": 0.7235248647255862,
        "step": 5616
    },
    {
        "loss": 1.9028,
        "grad_norm": 1.8680179119110107,
        "learning_rate": 9.311934343774176e-05,
        "epoch": 0.7236536975006441,
        "step": 5617
    },
    {
        "loss": 1.5739,
        "grad_norm": 2.8656060695648193,
        "learning_rate": 9.308851708513182e-05,
        "epoch": 0.7237825302757022,
        "step": 5618
    },
    {
        "loss": 1.745,
        "grad_norm": 2.5854408740997314,
        "learning_rate": 9.305762695955277e-05,
        "epoch": 0.7239113630507601,
        "step": 5619
    },
    {
        "loss": 2.091,
        "grad_norm": 1.2877097129821777,
        "learning_rate": 9.302667310672338e-05,
        "epoch": 0.7240401958258181,
        "step": 5620
    },
    {
        "loss": 1.6946,
        "grad_norm": 2.376009702682495,
        "learning_rate": 9.299565557245678e-05,
        "epoch": 0.7241690286008761,
        "step": 5621
    },
    {
        "loss": 2.1412,
        "grad_norm": 2.02097225189209,
        "learning_rate": 9.296457440266034e-05,
        "epoch": 0.724297861375934,
        "step": 5622
    },
    {
        "loss": 2.1414,
        "grad_norm": 2.1596271991729736,
        "learning_rate": 9.293342964333558e-05,
        "epoch": 0.724426694150992,
        "step": 5623
    },
    {
        "loss": 2.0316,
        "grad_norm": 2.1520378589630127,
        "learning_rate": 9.290222134057824e-05,
        "epoch": 0.72455552692605,
        "step": 5624
    },
    {
        "loss": 2.5265,
        "grad_norm": 1.492763638496399,
        "learning_rate": 9.287094954057792e-05,
        "epoch": 0.724684359701108,
        "step": 5625
    },
    {
        "loss": 1.6377,
        "grad_norm": 3.1126327514648438,
        "learning_rate": 9.283961428961837e-05,
        "epoch": 0.7248131924761659,
        "step": 5626
    },
    {
        "loss": 1.533,
        "grad_norm": 2.6497178077697754,
        "learning_rate": 9.280821563407723e-05,
        "epoch": 0.7249420252512239,
        "step": 5627
    },
    {
        "loss": 2.5376,
        "grad_norm": 1.7163904905319214,
        "learning_rate": 9.27767536204258e-05,
        "epoch": 0.7250708580262819,
        "step": 5628
    },
    {
        "loss": 1.6698,
        "grad_norm": 3.2333195209503174,
        "learning_rate": 9.274522829522942e-05,
        "epoch": 0.7251996908013398,
        "step": 5629
    },
    {
        "loss": 1.4158,
        "grad_norm": 2.249058723449707,
        "learning_rate": 9.271363970514701e-05,
        "epoch": 0.7253285235763979,
        "step": 5630
    },
    {
        "loss": 2.2941,
        "grad_norm": 2.199098825454712,
        "learning_rate": 9.268198789693108e-05,
        "epoch": 0.7254573563514558,
        "step": 5631
    },
    {
        "loss": 1.8096,
        "grad_norm": 3.571416139602661,
        "learning_rate": 9.265027291742773e-05,
        "epoch": 0.7255861891265137,
        "step": 5632
    },
    {
        "loss": 1.889,
        "grad_norm": 2.2961294651031494,
        "learning_rate": 9.26184948135766e-05,
        "epoch": 0.7257150219015718,
        "step": 5633
    },
    {
        "loss": 1.3001,
        "grad_norm": 3.98993182182312,
        "learning_rate": 9.258665363241075e-05,
        "epoch": 0.7258438546766297,
        "step": 5634
    },
    {
        "loss": 2.1921,
        "grad_norm": 1.6830089092254639,
        "learning_rate": 9.255474942105659e-05,
        "epoch": 0.7259726874516877,
        "step": 5635
    },
    {
        "loss": 1.6763,
        "grad_norm": 2.346595048904419,
        "learning_rate": 9.252278222673372e-05,
        "epoch": 0.7261015202267457,
        "step": 5636
    },
    {
        "loss": 2.1424,
        "grad_norm": 1.409416913986206,
        "learning_rate": 9.249075209675512e-05,
        "epoch": 0.7262303530018036,
        "step": 5637
    },
    {
        "loss": 1.822,
        "grad_norm": 3.3139402866363525,
        "learning_rate": 9.245865907852686e-05,
        "epoch": 0.7263591857768616,
        "step": 5638
    },
    {
        "loss": 1.2157,
        "grad_norm": 3.271186113357544,
        "learning_rate": 9.242650321954802e-05,
        "epoch": 0.7264880185519196,
        "step": 5639
    },
    {
        "loss": 1.4503,
        "grad_norm": 2.2448861598968506,
        "learning_rate": 9.239428456741074e-05,
        "epoch": 0.7266168513269776,
        "step": 5640
    },
    {
        "loss": 2.2349,
        "grad_norm": 1.825012445449829,
        "learning_rate": 9.236200316980016e-05,
        "epoch": 0.7267456841020355,
        "step": 5641
    },
    {
        "loss": 2.2907,
        "grad_norm": 1.518498420715332,
        "learning_rate": 9.232965907449412e-05,
        "epoch": 0.7268745168770935,
        "step": 5642
    },
    {
        "loss": 1.2822,
        "grad_norm": 3.412747383117676,
        "learning_rate": 9.22972523293634e-05,
        "epoch": 0.7270033496521515,
        "step": 5643
    },
    {
        "loss": 1.7275,
        "grad_norm": 3.121502637863159,
        "learning_rate": 9.226478298237147e-05,
        "epoch": 0.7271321824272095,
        "step": 5644
    },
    {
        "loss": 1.196,
        "grad_norm": 2.6523566246032715,
        "learning_rate": 9.223225108157443e-05,
        "epoch": 0.7272610152022675,
        "step": 5645
    },
    {
        "loss": 2.0833,
        "grad_norm": 2.5454773902893066,
        "learning_rate": 9.219965667512097e-05,
        "epoch": 0.7273898479773254,
        "step": 5646
    },
    {
        "loss": 1.7025,
        "grad_norm": 2.512026786804199,
        "learning_rate": 9.216699981125231e-05,
        "epoch": 0.7275186807523834,
        "step": 5647
    },
    {
        "loss": 2.266,
        "grad_norm": 2.2068145275115967,
        "learning_rate": 9.213428053830211e-05,
        "epoch": 0.7276475135274414,
        "step": 5648
    },
    {
        "loss": 1.365,
        "grad_norm": 3.2283217906951904,
        "learning_rate": 9.21014989046964e-05,
        "epoch": 0.7277763463024993,
        "step": 5649
    },
    {
        "loss": 2.0912,
        "grad_norm": 2.429424524307251,
        "learning_rate": 9.206865495895343e-05,
        "epoch": 0.7279051790775574,
        "step": 5650
    },
    {
        "loss": 1.7297,
        "grad_norm": 1.7425544261932373,
        "learning_rate": 9.203574874968376e-05,
        "epoch": 0.7280340118526153,
        "step": 5651
    },
    {
        "loss": 1.8196,
        "grad_norm": 2.9590747356414795,
        "learning_rate": 9.200278032559012e-05,
        "epoch": 0.7281628446276733,
        "step": 5652
    },
    {
        "loss": 2.1985,
        "grad_norm": 2.141214370727539,
        "learning_rate": 9.196974973546718e-05,
        "epoch": 0.7282916774027313,
        "step": 5653
    },
    {
        "loss": 1.9462,
        "grad_norm": 2.6658124923706055,
        "learning_rate": 9.193665702820182e-05,
        "epoch": 0.7284205101777892,
        "step": 5654
    },
    {
        "loss": 1.9682,
        "grad_norm": 1.7612305879592896,
        "learning_rate": 9.190350225277278e-05,
        "epoch": 0.7285493429528472,
        "step": 5655
    },
    {
        "loss": 1.5346,
        "grad_norm": 2.6395788192749023,
        "learning_rate": 9.187028545825052e-05,
        "epoch": 0.7286781757279052,
        "step": 5656
    },
    {
        "loss": 1.6548,
        "grad_norm": 3.436824321746826,
        "learning_rate": 9.183700669379748e-05,
        "epoch": 0.7288070085029632,
        "step": 5657
    },
    {
        "loss": 2.3564,
        "grad_norm": 1.5665475130081177,
        "learning_rate": 9.180366600866775e-05,
        "epoch": 0.7289358412780211,
        "step": 5658
    },
    {
        "loss": 2.0741,
        "grad_norm": 2.75840425491333,
        "learning_rate": 9.177026345220703e-05,
        "epoch": 0.7290646740530791,
        "step": 5659
    },
    {
        "loss": 2.2896,
        "grad_norm": 1.7157511711120605,
        "learning_rate": 9.173679907385269e-05,
        "epoch": 0.7291935068281371,
        "step": 5660
    },
    {
        "loss": 2.5951,
        "grad_norm": 1.7854444980621338,
        "learning_rate": 9.170327292313346e-05,
        "epoch": 0.729322339603195,
        "step": 5661
    },
    {
        "loss": 1.7351,
        "grad_norm": 2.345548629760742,
        "learning_rate": 9.166968504966957e-05,
        "epoch": 0.7294511723782531,
        "step": 5662
    },
    {
        "loss": 2.032,
        "grad_norm": 2.3325700759887695,
        "learning_rate": 9.163603550317271e-05,
        "epoch": 0.729580005153311,
        "step": 5663
    },
    {
        "loss": 2.3943,
        "grad_norm": 3.003143787384033,
        "learning_rate": 9.160232433344563e-05,
        "epoch": 0.7297088379283689,
        "step": 5664
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.8381208181381226,
        "learning_rate": 9.156855159038245e-05,
        "epoch": 0.729837670703427,
        "step": 5665
    },
    {
        "loss": 1.8802,
        "grad_norm": 1.7347668409347534,
        "learning_rate": 9.15347173239684e-05,
        "epoch": 0.7299665034784849,
        "step": 5666
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.8834140300750732,
        "learning_rate": 9.150082158427963e-05,
        "epoch": 0.730095336253543,
        "step": 5667
    },
    {
        "loss": 1.5541,
        "grad_norm": 2.3109660148620605,
        "learning_rate": 9.146686442148344e-05,
        "epoch": 0.7302241690286009,
        "step": 5668
    },
    {
        "loss": 2.0505,
        "grad_norm": 2.4706814289093018,
        "learning_rate": 9.1432845885838e-05,
        "epoch": 0.7303530018036588,
        "step": 5669
    },
    {
        "loss": 1.7166,
        "grad_norm": 2.9866299629211426,
        "learning_rate": 9.139876602769228e-05,
        "epoch": 0.7304818345787168,
        "step": 5670
    },
    {
        "loss": 2.2255,
        "grad_norm": 2.1570885181427,
        "learning_rate": 9.136462489748601e-05,
        "epoch": 0.7306106673537748,
        "step": 5671
    },
    {
        "loss": 1.3144,
        "grad_norm": 3.181807041168213,
        "learning_rate": 9.133042254574962e-05,
        "epoch": 0.7307395001288328,
        "step": 5672
    },
    {
        "loss": 1.1648,
        "grad_norm": 2.9999444484710693,
        "learning_rate": 9.129615902310416e-05,
        "epoch": 0.7308683329038907,
        "step": 5673
    },
    {
        "loss": 2.1606,
        "grad_norm": 1.513468861579895,
        "learning_rate": 9.126183438026125e-05,
        "epoch": 0.7309971656789487,
        "step": 5674
    },
    {
        "loss": 2.4921,
        "grad_norm": 1.298196792602539,
        "learning_rate": 9.122744866802283e-05,
        "epoch": 0.7311259984540067,
        "step": 5675
    },
    {
        "loss": 1.3445,
        "grad_norm": 3.3528316020965576,
        "learning_rate": 9.119300193728137e-05,
        "epoch": 0.7312548312290647,
        "step": 5676
    },
    {
        "loss": 2.5354,
        "grad_norm": 1.504403829574585,
        "learning_rate": 9.115849423901963e-05,
        "epoch": 0.7313836640041227,
        "step": 5677
    },
    {
        "loss": 1.2238,
        "grad_norm": 5.563939094543457,
        "learning_rate": 9.112392562431052e-05,
        "epoch": 0.7315124967791806,
        "step": 5678
    },
    {
        "loss": 1.8908,
        "grad_norm": 2.810046434402466,
        "learning_rate": 9.108929614431717e-05,
        "epoch": 0.7316413295542386,
        "step": 5679
    },
    {
        "loss": 1.203,
        "grad_norm": 4.208348274230957,
        "learning_rate": 9.105460585029285e-05,
        "epoch": 0.7317701623292966,
        "step": 5680
    },
    {
        "loss": 2.2114,
        "grad_norm": 1.5570399761199951,
        "learning_rate": 9.10198547935807e-05,
        "epoch": 0.7318989951043545,
        "step": 5681
    },
    {
        "loss": 2.3047,
        "grad_norm": 1.9447509050369263,
        "learning_rate": 9.098504302561393e-05,
        "epoch": 0.7320278278794126,
        "step": 5682
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.8656939268112183,
        "learning_rate": 9.09501705979155e-05,
        "epoch": 0.7321566606544705,
        "step": 5683
    },
    {
        "loss": 2.317,
        "grad_norm": 1.7415159940719604,
        "learning_rate": 9.091523756209823e-05,
        "epoch": 0.7322854934295284,
        "step": 5684
    },
    {
        "loss": 1.8832,
        "grad_norm": 3.2269415855407715,
        "learning_rate": 9.08802439698646e-05,
        "epoch": 0.7324143262045865,
        "step": 5685
    },
    {
        "loss": 2.3618,
        "grad_norm": 1.5941988229751587,
        "learning_rate": 9.084518987300667e-05,
        "epoch": 0.7325431589796444,
        "step": 5686
    },
    {
        "loss": 1.0481,
        "grad_norm": 3.1903176307678223,
        "learning_rate": 9.081007532340616e-05,
        "epoch": 0.7326719917547024,
        "step": 5687
    },
    {
        "loss": 2.3217,
        "grad_norm": 2.3351638317108154,
        "learning_rate": 9.077490037303415e-05,
        "epoch": 0.7328008245297604,
        "step": 5688
    },
    {
        "loss": 2.4964,
        "grad_norm": 1.9282490015029907,
        "learning_rate": 9.073966507395122e-05,
        "epoch": 0.7329296573048183,
        "step": 5689
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.0311777591705322,
        "learning_rate": 9.070436947830719e-05,
        "epoch": 0.7330584900798763,
        "step": 5690
    },
    {
        "loss": 2.5128,
        "grad_norm": 1.5338661670684814,
        "learning_rate": 9.066901363834121e-05,
        "epoch": 0.7331873228549343,
        "step": 5691
    },
    {
        "loss": 2.4664,
        "grad_norm": 1.4617351293563843,
        "learning_rate": 9.063359760638144e-05,
        "epoch": 0.7333161556299923,
        "step": 5692
    },
    {
        "loss": 1.9359,
        "grad_norm": 1.9236832857131958,
        "learning_rate": 9.059812143484525e-05,
        "epoch": 0.7334449884050502,
        "step": 5693
    },
    {
        "loss": 2.2566,
        "grad_norm": 1.666237711906433,
        "learning_rate": 9.056258517623903e-05,
        "epoch": 0.7335738211801082,
        "step": 5694
    },
    {
        "loss": 1.7523,
        "grad_norm": 2.606785774230957,
        "learning_rate": 9.052698888315802e-05,
        "epoch": 0.7337026539551662,
        "step": 5695
    },
    {
        "loss": 1.5906,
        "grad_norm": 2.5357978343963623,
        "learning_rate": 9.049133260828638e-05,
        "epoch": 0.7338314867302241,
        "step": 5696
    },
    {
        "loss": 2.425,
        "grad_norm": 1.5138475894927979,
        "learning_rate": 9.045561640439702e-05,
        "epoch": 0.7339603195052822,
        "step": 5697
    },
    {
        "loss": 1.6181,
        "grad_norm": 3.0305581092834473,
        "learning_rate": 9.041984032435156e-05,
        "epoch": 0.7340891522803401,
        "step": 5698
    },
    {
        "loss": 2.2254,
        "grad_norm": 2.7233095169067383,
        "learning_rate": 9.038400442110023e-05,
        "epoch": 0.7342179850553981,
        "step": 5699
    },
    {
        "loss": 2.08,
        "grad_norm": 2.0682485103607178,
        "learning_rate": 9.034810874768175e-05,
        "epoch": 0.7343468178304561,
        "step": 5700
    },
    {
        "loss": 1.7626,
        "grad_norm": 2.446662425994873,
        "learning_rate": 9.03121533572234e-05,
        "epoch": 0.734475650605514,
        "step": 5701
    },
    {
        "loss": 1.2496,
        "grad_norm": 2.7817680835723877,
        "learning_rate": 9.027613830294082e-05,
        "epoch": 0.734604483380572,
        "step": 5702
    },
    {
        "loss": 1.5803,
        "grad_norm": 2.267683744430542,
        "learning_rate": 9.024006363813787e-05,
        "epoch": 0.73473331615563,
        "step": 5703
    },
    {
        "loss": 2.1904,
        "grad_norm": 2.18552303314209,
        "learning_rate": 9.02039294162067e-05,
        "epoch": 0.734862148930688,
        "step": 5704
    },
    {
        "loss": 2.0223,
        "grad_norm": 2.9743521213531494,
        "learning_rate": 9.016773569062765e-05,
        "epoch": 0.734990981705746,
        "step": 5705
    },
    {
        "loss": 1.7265,
        "grad_norm": 2.9092376232147217,
        "learning_rate": 9.013148251496904e-05,
        "epoch": 0.7351198144808039,
        "step": 5706
    },
    {
        "loss": 2.2593,
        "grad_norm": 2.33624529838562,
        "learning_rate": 9.009516994288727e-05,
        "epoch": 0.7352486472558619,
        "step": 5707
    },
    {
        "loss": 2.2718,
        "grad_norm": 1.5825868844985962,
        "learning_rate": 9.005879802812656e-05,
        "epoch": 0.7353774800309199,
        "step": 5708
    },
    {
        "loss": 1.7373,
        "grad_norm": 3.552117347717285,
        "learning_rate": 9.002236682451902e-05,
        "epoch": 0.7355063128059779,
        "step": 5709
    },
    {
        "loss": 1.8996,
        "grad_norm": 1.685525894165039,
        "learning_rate": 8.998587638598455e-05,
        "epoch": 0.7356351455810358,
        "step": 5710
    },
    {
        "loss": 2.4471,
        "grad_norm": 2.2645959854125977,
        "learning_rate": 8.994932676653054e-05,
        "epoch": 0.7357639783560938,
        "step": 5711
    },
    {
        "loss": 2.3805,
        "grad_norm": 1.5834803581237793,
        "learning_rate": 8.991271802025216e-05,
        "epoch": 0.7358928111311518,
        "step": 5712
    },
    {
        "loss": 1.2079,
        "grad_norm": 3.9165546894073486,
        "learning_rate": 8.987605020133203e-05,
        "epoch": 0.7360216439062097,
        "step": 5713
    },
    {
        "loss": 1.1653,
        "grad_norm": 2.8433361053466797,
        "learning_rate": 8.983932336404017e-05,
        "epoch": 0.7361504766812678,
        "step": 5714
    },
    {
        "loss": 1.8892,
        "grad_norm": 1.845870018005371,
        "learning_rate": 8.980253756273398e-05,
        "epoch": 0.7362793094563257,
        "step": 5715
    },
    {
        "loss": 1.8638,
        "grad_norm": 2.16158390045166,
        "learning_rate": 8.976569285185818e-05,
        "epoch": 0.7364081422313836,
        "step": 5716
    },
    {
        "loss": 2.0586,
        "grad_norm": 2.229234457015991,
        "learning_rate": 8.972878928594452e-05,
        "epoch": 0.7365369750064417,
        "step": 5717
    },
    {
        "loss": 2.0402,
        "grad_norm": 2.2102577686309814,
        "learning_rate": 8.969182691961201e-05,
        "epoch": 0.7366658077814996,
        "step": 5718
    },
    {
        "loss": 2.0498,
        "grad_norm": 1.7759233713150024,
        "learning_rate": 8.965480580756665e-05,
        "epoch": 0.7367946405565576,
        "step": 5719
    },
    {
        "loss": 1.0228,
        "grad_norm": 3.680363655090332,
        "learning_rate": 8.961772600460135e-05,
        "epoch": 0.7369234733316156,
        "step": 5720
    },
    {
        "loss": 1.7244,
        "grad_norm": 2.7052159309387207,
        "learning_rate": 8.958058756559594e-05,
        "epoch": 0.7370523061066735,
        "step": 5721
    },
    {
        "loss": 2.0652,
        "grad_norm": 1.4251821041107178,
        "learning_rate": 8.95433905455169e-05,
        "epoch": 0.7371811388817315,
        "step": 5722
    },
    {
        "loss": 1.7565,
        "grad_norm": 2.326697826385498,
        "learning_rate": 8.950613499941766e-05,
        "epoch": 0.7373099716567895,
        "step": 5723
    },
    {
        "loss": 1.5528,
        "grad_norm": 2.8502039909362793,
        "learning_rate": 8.946882098243807e-05,
        "epoch": 0.7374388044318475,
        "step": 5724
    },
    {
        "loss": 1.8715,
        "grad_norm": 2.5231213569641113,
        "learning_rate": 8.943144854980454e-05,
        "epoch": 0.7375676372069054,
        "step": 5725
    },
    {
        "loss": 2.4921,
        "grad_norm": 1.7249747514724731,
        "learning_rate": 8.939401775683e-05,
        "epoch": 0.7376964699819634,
        "step": 5726
    },
    {
        "loss": 2.0234,
        "grad_norm": 2.360158920288086,
        "learning_rate": 8.935652865891377e-05,
        "epoch": 0.7378253027570214,
        "step": 5727
    },
    {
        "loss": 2.0321,
        "grad_norm": 1.9515178203582764,
        "learning_rate": 8.931898131154135e-05,
        "epoch": 0.7379541355320793,
        "step": 5728
    },
    {
        "loss": 1.8174,
        "grad_norm": 2.41715145111084,
        "learning_rate": 8.928137577028455e-05,
        "epoch": 0.7380829683071374,
        "step": 5729
    },
    {
        "loss": 2.4539,
        "grad_norm": 2.0191473960876465,
        "learning_rate": 8.924371209080128e-05,
        "epoch": 0.7382118010821953,
        "step": 5730
    },
    {
        "loss": 2.1525,
        "grad_norm": 2.648299217224121,
        "learning_rate": 8.920599032883552e-05,
        "epoch": 0.7383406338572532,
        "step": 5731
    },
    {
        "loss": 1.7152,
        "grad_norm": 2.2733874320983887,
        "learning_rate": 8.91682105402172e-05,
        "epoch": 0.7384694666323113,
        "step": 5732
    },
    {
        "loss": 1.793,
        "grad_norm": 1.9697229862213135,
        "learning_rate": 8.913037278086211e-05,
        "epoch": 0.7385982994073692,
        "step": 5733
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.2060322761535645,
        "learning_rate": 8.909247710677189e-05,
        "epoch": 0.7387271321824272,
        "step": 5734
    },
    {
        "loss": 2.3785,
        "grad_norm": 1.8527770042419434,
        "learning_rate": 8.905452357403388e-05,
        "epoch": 0.7388559649574852,
        "step": 5735
    },
    {
        "loss": 2.2655,
        "grad_norm": 2.399291753768921,
        "learning_rate": 8.901651223882094e-05,
        "epoch": 0.7389847977325431,
        "step": 5736
    },
    {
        "loss": 2.3934,
        "grad_norm": 1.6672340631484985,
        "learning_rate": 8.897844315739166e-05,
        "epoch": 0.7391136305076011,
        "step": 5737
    },
    {
        "loss": 2.0626,
        "grad_norm": 1.9093683958053589,
        "learning_rate": 8.894031638609004e-05,
        "epoch": 0.7392424632826591,
        "step": 5738
    },
    {
        "loss": 2.3201,
        "grad_norm": 1.628670573234558,
        "learning_rate": 8.89021319813453e-05,
        "epoch": 0.7393712960577171,
        "step": 5739
    },
    {
        "loss": 1.6883,
        "grad_norm": 3.093310594558716,
        "learning_rate": 8.886388999967223e-05,
        "epoch": 0.739500128832775,
        "step": 5740
    },
    {
        "loss": 1.4003,
        "grad_norm": 2.2485265731811523,
        "learning_rate": 8.882559049767074e-05,
        "epoch": 0.739628961607833,
        "step": 5741
    },
    {
        "loss": 1.349,
        "grad_norm": 3.259258985519409,
        "learning_rate": 8.878723353202571e-05,
        "epoch": 0.739757794382891,
        "step": 5742
    },
    {
        "loss": 2.1632,
        "grad_norm": 2.689729690551758,
        "learning_rate": 8.874881915950726e-05,
        "epoch": 0.739886627157949,
        "step": 5743
    },
    {
        "loss": 1.7761,
        "grad_norm": 2.2730841636657715,
        "learning_rate": 8.871034743697041e-05,
        "epoch": 0.740015459933007,
        "step": 5744
    },
    {
        "loss": 2.0602,
        "grad_norm": 1.958685040473938,
        "learning_rate": 8.867181842135505e-05,
        "epoch": 0.7401442927080649,
        "step": 5745
    },
    {
        "loss": 2.3234,
        "grad_norm": 2.4539926052093506,
        "learning_rate": 8.863323216968596e-05,
        "epoch": 0.7402731254831229,
        "step": 5746
    },
    {
        "loss": 1.9405,
        "grad_norm": 2.1143109798431396,
        "learning_rate": 8.859458873907241e-05,
        "epoch": 0.7404019582581809,
        "step": 5747
    },
    {
        "loss": 1.2263,
        "grad_norm": 3.0875630378723145,
        "learning_rate": 8.855588818670847e-05,
        "epoch": 0.7405307910332388,
        "step": 5748
    },
    {
        "loss": 2.0964,
        "grad_norm": 2.087639570236206,
        "learning_rate": 8.851713056987287e-05,
        "epoch": 0.7406596238082969,
        "step": 5749
    },
    {
        "loss": 1.5079,
        "grad_norm": 2.46024227142334,
        "learning_rate": 8.847831594592849e-05,
        "epoch": 0.7407884565833548,
        "step": 5750
    },
    {
        "loss": 0.8565,
        "grad_norm": 3.2991719245910645,
        "learning_rate": 8.843944437232279e-05,
        "epoch": 0.7409172893584128,
        "step": 5751
    },
    {
        "loss": 2.3493,
        "grad_norm": 1.8674230575561523,
        "learning_rate": 8.840051590658752e-05,
        "epoch": 0.7410461221334708,
        "step": 5752
    },
    {
        "loss": 1.5145,
        "grad_norm": 3.6317267417907715,
        "learning_rate": 8.836153060633849e-05,
        "epoch": 0.7411749549085287,
        "step": 5753
    },
    {
        "loss": 1.8494,
        "grad_norm": 3.0524179935455322,
        "learning_rate": 8.832248852927579e-05,
        "epoch": 0.7413037876835867,
        "step": 5754
    },
    {
        "loss": 2.15,
        "grad_norm": 1.8831138610839844,
        "learning_rate": 8.828338973318342e-05,
        "epoch": 0.7414326204586447,
        "step": 5755
    },
    {
        "loss": 2.1555,
        "grad_norm": 3.138495922088623,
        "learning_rate": 8.824423427592943e-05,
        "epoch": 0.7415614532337027,
        "step": 5756
    },
    {
        "loss": 2.719,
        "grad_norm": 1.5965012311935425,
        "learning_rate": 8.820502221546565e-05,
        "epoch": 0.7416902860087606,
        "step": 5757
    },
    {
        "loss": 2.2804,
        "grad_norm": 2.048438310623169,
        "learning_rate": 8.816575360982775e-05,
        "epoch": 0.7418191187838186,
        "step": 5758
    },
    {
        "loss": 1.9204,
        "grad_norm": 2.1215898990631104,
        "learning_rate": 8.812642851713501e-05,
        "epoch": 0.7419479515588766,
        "step": 5759
    },
    {
        "loss": 2.2904,
        "grad_norm": 2.149925947189331,
        "learning_rate": 8.808704699559047e-05,
        "epoch": 0.7420767843339345,
        "step": 5760
    },
    {
        "loss": 2.302,
        "grad_norm": 1.4146997928619385,
        "learning_rate": 8.804760910348043e-05,
        "epoch": 0.7422056171089926,
        "step": 5761
    },
    {
        "loss": 1.212,
        "grad_norm": 3.1749513149261475,
        "learning_rate": 8.800811489917483e-05,
        "epoch": 0.7423344498840505,
        "step": 5762
    },
    {
        "loss": 2.0612,
        "grad_norm": 3.0933282375335693,
        "learning_rate": 8.796856444112693e-05,
        "epoch": 0.7424632826591084,
        "step": 5763
    },
    {
        "loss": 2.2825,
        "grad_norm": 2.264537811279297,
        "learning_rate": 8.792895778787313e-05,
        "epoch": 0.7425921154341665,
        "step": 5764
    },
    {
        "loss": 2.0719,
        "grad_norm": 2.2414183616638184,
        "learning_rate": 8.788929499803309e-05,
        "epoch": 0.7427209482092244,
        "step": 5765
    },
    {
        "loss": 1.7816,
        "grad_norm": 2.2951457500457764,
        "learning_rate": 8.784957613030966e-05,
        "epoch": 0.7428497809842824,
        "step": 5766
    },
    {
        "loss": 2.1442,
        "grad_norm": 1.9858521223068237,
        "learning_rate": 8.780980124348845e-05,
        "epoch": 0.7429786137593404,
        "step": 5767
    },
    {
        "loss": 2.4369,
        "grad_norm": 1.8044661283493042,
        "learning_rate": 8.776997039643818e-05,
        "epoch": 0.7431074465343983,
        "step": 5768
    },
    {
        "loss": 2.1765,
        "grad_norm": 2.3804829120635986,
        "learning_rate": 8.773008364811026e-05,
        "epoch": 0.7432362793094563,
        "step": 5769
    },
    {
        "loss": 1.3675,
        "grad_norm": 3.588561534881592,
        "learning_rate": 8.769014105753895e-05,
        "epoch": 0.7433651120845143,
        "step": 5770
    },
    {
        "loss": 2.2551,
        "grad_norm": 2.031815767288208,
        "learning_rate": 8.76501426838411e-05,
        "epoch": 0.7434939448595723,
        "step": 5771
    },
    {
        "loss": 2.1478,
        "grad_norm": 2.9346466064453125,
        "learning_rate": 8.761008858621609e-05,
        "epoch": 0.7436227776346302,
        "step": 5772
    },
    {
        "loss": 2.0552,
        "grad_norm": 2.274012327194214,
        "learning_rate": 8.756997882394577e-05,
        "epoch": 0.7437516104096882,
        "step": 5773
    },
    {
        "loss": 2.1025,
        "grad_norm": 2.320465326309204,
        "learning_rate": 8.752981345639455e-05,
        "epoch": 0.7438804431847462,
        "step": 5774
    },
    {
        "loss": 2.3601,
        "grad_norm": 1.9278960227966309,
        "learning_rate": 8.74895925430089e-05,
        "epoch": 0.7440092759598042,
        "step": 5775
    },
    {
        "loss": 2.2125,
        "grad_norm": 1.1544679403305054,
        "learning_rate": 8.744931614331763e-05,
        "epoch": 0.7441381087348622,
        "step": 5776
    },
    {
        "loss": 2.1433,
        "grad_norm": 2.6478497982025146,
        "learning_rate": 8.74089843169317e-05,
        "epoch": 0.7442669415099201,
        "step": 5777
    },
    {
        "loss": 1.8065,
        "grad_norm": 2.179605007171631,
        "learning_rate": 8.736859712354395e-05,
        "epoch": 0.744395774284978,
        "step": 5778
    },
    {
        "loss": 2.1872,
        "grad_norm": 1.9699302911758423,
        "learning_rate": 8.732815462292932e-05,
        "epoch": 0.7445246070600361,
        "step": 5779
    },
    {
        "loss": 2.2024,
        "grad_norm": 1.9654581546783447,
        "learning_rate": 8.728765687494454e-05,
        "epoch": 0.744653439835094,
        "step": 5780
    },
    {
        "loss": 1.8135,
        "grad_norm": 3.4512789249420166,
        "learning_rate": 8.724710393952814e-05,
        "epoch": 0.7447822726101521,
        "step": 5781
    },
    {
        "loss": 2.1274,
        "grad_norm": 1.6014842987060547,
        "learning_rate": 8.720649587670032e-05,
        "epoch": 0.74491110538521,
        "step": 5782
    },
    {
        "loss": 2.0551,
        "grad_norm": 1.4939045906066895,
        "learning_rate": 8.716583274656284e-05,
        "epoch": 0.7450399381602679,
        "step": 5783
    },
    {
        "loss": 2.1555,
        "grad_norm": 2.0438239574432373,
        "learning_rate": 8.7125114609299e-05,
        "epoch": 0.745168770935326,
        "step": 5784
    },
    {
        "loss": 2.022,
        "grad_norm": 3.0558981895446777,
        "learning_rate": 8.708434152517355e-05,
        "epoch": 0.7452976037103839,
        "step": 5785
    },
    {
        "loss": 1.6971,
        "grad_norm": 2.4107108116149902,
        "learning_rate": 8.70435135545324e-05,
        "epoch": 0.7454264364854419,
        "step": 5786
    },
    {
        "loss": 1.956,
        "grad_norm": 2.756586790084839,
        "learning_rate": 8.700263075780289e-05,
        "epoch": 0.7455552692604999,
        "step": 5787
    },
    {
        "loss": 2.5371,
        "grad_norm": 2.483635187149048,
        "learning_rate": 8.696169319549341e-05,
        "epoch": 0.7456841020355578,
        "step": 5788
    },
    {
        "loss": 2.0719,
        "grad_norm": 1.9226956367492676,
        "learning_rate": 8.692070092819339e-05,
        "epoch": 0.7458129348106158,
        "step": 5789
    },
    {
        "loss": 1.3442,
        "grad_norm": 2.6190004348754883,
        "learning_rate": 8.68796540165732e-05,
        "epoch": 0.7459417675856738,
        "step": 5790
    },
    {
        "loss": 1.2833,
        "grad_norm": 2.437516212463379,
        "learning_rate": 8.683855252138433e-05,
        "epoch": 0.7460706003607318,
        "step": 5791
    },
    {
        "loss": 2.0101,
        "grad_norm": 2.2810285091400146,
        "learning_rate": 8.679739650345868e-05,
        "epoch": 0.7461994331357897,
        "step": 5792
    },
    {
        "loss": 1.9475,
        "grad_norm": 3.060887336730957,
        "learning_rate": 8.675618602370911e-05,
        "epoch": 0.7463282659108477,
        "step": 5793
    },
    {
        "loss": 1.9414,
        "grad_norm": 2.763709545135498,
        "learning_rate": 8.6714921143129e-05,
        "epoch": 0.7464570986859057,
        "step": 5794
    },
    {
        "loss": 2.4997,
        "grad_norm": 2.0088510513305664,
        "learning_rate": 8.667360192279226e-05,
        "epoch": 0.7465859314609636,
        "step": 5795
    },
    {
        "loss": 1.9524,
        "grad_norm": 2.096356153488159,
        "learning_rate": 8.663222842385323e-05,
        "epoch": 0.7467147642360217,
        "step": 5796
    },
    {
        "loss": 2.4891,
        "grad_norm": 1.363727331161499,
        "learning_rate": 8.659080070754653e-05,
        "epoch": 0.7468435970110796,
        "step": 5797
    },
    {
        "loss": 2.5808,
        "grad_norm": 1.840407133102417,
        "learning_rate": 8.65493188351871e-05,
        "epoch": 0.7469724297861376,
        "step": 5798
    },
    {
        "loss": 2.1458,
        "grad_norm": 2.479097604751587,
        "learning_rate": 8.650778286816997e-05,
        "epoch": 0.7471012625611956,
        "step": 5799
    },
    {
        "loss": 1.4515,
        "grad_norm": 4.693636417388916,
        "learning_rate": 8.646619286797031e-05,
        "epoch": 0.7472300953362535,
        "step": 5800
    },
    {
        "loss": 2.2965,
        "grad_norm": 1.1903237104415894,
        "learning_rate": 8.642454889614318e-05,
        "epoch": 0.7473589281113115,
        "step": 5801
    },
    {
        "loss": 1.5396,
        "grad_norm": 2.364114284515381,
        "learning_rate": 8.638285101432361e-05,
        "epoch": 0.7474877608863695,
        "step": 5802
    },
    {
        "loss": 1.6993,
        "grad_norm": 2.6266050338745117,
        "learning_rate": 8.634109928422631e-05,
        "epoch": 0.7476165936614275,
        "step": 5803
    },
    {
        "loss": 1.8826,
        "grad_norm": 2.0272066593170166,
        "learning_rate": 8.629929376764574e-05,
        "epoch": 0.7477454264364854,
        "step": 5804
    },
    {
        "loss": 1.9315,
        "grad_norm": 2.1505300998687744,
        "learning_rate": 8.6257434526456e-05,
        "epoch": 0.7478742592115434,
        "step": 5805
    },
    {
        "loss": 2.1391,
        "grad_norm": 1.8465090990066528,
        "learning_rate": 8.621552162261067e-05,
        "epoch": 0.7480030919866014,
        "step": 5806
    },
    {
        "loss": 1.8092,
        "grad_norm": 3.557405710220337,
        "learning_rate": 8.617355511814274e-05,
        "epoch": 0.7481319247616594,
        "step": 5807
    },
    {
        "loss": 2.1521,
        "grad_norm": 1.4509074687957764,
        "learning_rate": 8.613153507516456e-05,
        "epoch": 0.7482607575367174,
        "step": 5808
    },
    {
        "loss": 2.4598,
        "grad_norm": 1.3367986679077148,
        "learning_rate": 8.608946155586771e-05,
        "epoch": 0.7483895903117753,
        "step": 5809
    },
    {
        "loss": 1.4907,
        "grad_norm": 2.0188040733337402,
        "learning_rate": 8.604733462252297e-05,
        "epoch": 0.7485184230868333,
        "step": 5810
    },
    {
        "loss": 2.0867,
        "grad_norm": 2.4094717502593994,
        "learning_rate": 8.600515433748003e-05,
        "epoch": 0.7486472558618913,
        "step": 5811
    },
    {
        "loss": 1.5042,
        "grad_norm": 1.941408634185791,
        "learning_rate": 8.596292076316767e-05,
        "epoch": 0.7487760886369492,
        "step": 5812
    },
    {
        "loss": 2.1059,
        "grad_norm": 2.278329610824585,
        "learning_rate": 8.592063396209355e-05,
        "epoch": 0.7489049214120073,
        "step": 5813
    },
    {
        "loss": 2.093,
        "grad_norm": 2.878014087677002,
        "learning_rate": 8.587829399684397e-05,
        "epoch": 0.7490337541870652,
        "step": 5814
    },
    {
        "loss": 2.142,
        "grad_norm": 1.8224101066589355,
        "learning_rate": 8.583590093008407e-05,
        "epoch": 0.7491625869621231,
        "step": 5815
    },
    {
        "loss": 2.3967,
        "grad_norm": 1.4888705015182495,
        "learning_rate": 8.579345482455749e-05,
        "epoch": 0.7492914197371812,
        "step": 5816
    },
    {
        "loss": 2.2921,
        "grad_norm": 2.161102294921875,
        "learning_rate": 8.575095574308644e-05,
        "epoch": 0.7494202525122391,
        "step": 5817
    },
    {
        "loss": 2.5224,
        "grad_norm": 1.6305289268493652,
        "learning_rate": 8.570840374857148e-05,
        "epoch": 0.7495490852872971,
        "step": 5818
    },
    {
        "loss": 1.3372,
        "grad_norm": 2.1774868965148926,
        "learning_rate": 8.566579890399148e-05,
        "epoch": 0.7496779180623551,
        "step": 5819
    },
    {
        "loss": 1.8836,
        "grad_norm": 2.7942614555358887,
        "learning_rate": 8.562314127240358e-05,
        "epoch": 0.749806750837413,
        "step": 5820
    },
    {
        "loss": 1.6383,
        "grad_norm": 1.9451578855514526,
        "learning_rate": 8.558043091694306e-05,
        "epoch": 0.749935583612471,
        "step": 5821
    },
    {
        "loss": 2.0098,
        "grad_norm": 2.328395128250122,
        "learning_rate": 8.55376679008231e-05,
        "epoch": 0.750064416387529,
        "step": 5822
    },
    {
        "loss": 1.9486,
        "grad_norm": 3.446118116378784,
        "learning_rate": 8.549485228733495e-05,
        "epoch": 0.750193249162587,
        "step": 5823
    },
    {
        "loss": 1.6478,
        "grad_norm": 2.8806610107421875,
        "learning_rate": 8.545198413984772e-05,
        "epoch": 0.7503220819376449,
        "step": 5824
    },
    {
        "loss": 2.0904,
        "grad_norm": 1.7592344284057617,
        "learning_rate": 8.540906352180809e-05,
        "epoch": 0.7504509147127029,
        "step": 5825
    },
    {
        "loss": 1.99,
        "grad_norm": 3.87612247467041,
        "learning_rate": 8.536609049674064e-05,
        "epoch": 0.7505797474877609,
        "step": 5826
    },
    {
        "loss": 1.6511,
        "grad_norm": 1.958071231842041,
        "learning_rate": 8.532306512824746e-05,
        "epoch": 0.7507085802628188,
        "step": 5827
    },
    {
        "loss": 1.5278,
        "grad_norm": 3.091951847076416,
        "learning_rate": 8.527998748000794e-05,
        "epoch": 0.7508374130378769,
        "step": 5828
    },
    {
        "loss": 1.7581,
        "grad_norm": 3.85276460647583,
        "learning_rate": 8.523685761577898e-05,
        "epoch": 0.7509662458129348,
        "step": 5829
    },
    {
        "loss": 2.4954,
        "grad_norm": 1.7511396408081055,
        "learning_rate": 8.519367559939477e-05,
        "epoch": 0.7510950785879927,
        "step": 5830
    },
    {
        "loss": 1.811,
        "grad_norm": 2.2617616653442383,
        "learning_rate": 8.515044149476666e-05,
        "epoch": 0.7512239113630508,
        "step": 5831
    },
    {
        "loss": 1.8427,
        "grad_norm": 2.0847115516662598,
        "learning_rate": 8.510715536588315e-05,
        "epoch": 0.7513527441381087,
        "step": 5832
    },
    {
        "loss": 2.1844,
        "grad_norm": 1.7072575092315674,
        "learning_rate": 8.506381727680952e-05,
        "epoch": 0.7514815769131667,
        "step": 5833
    },
    {
        "loss": 2.1392,
        "grad_norm": 1.4205979108810425,
        "learning_rate": 8.502042729168829e-05,
        "epoch": 0.7516104096882247,
        "step": 5834
    },
    {
        "loss": 1.7491,
        "grad_norm": 2.7686328887939453,
        "learning_rate": 8.49769854747386e-05,
        "epoch": 0.7517392424632826,
        "step": 5835
    },
    {
        "loss": 1.867,
        "grad_norm": 3.2447729110717773,
        "learning_rate": 8.493349189025621e-05,
        "epoch": 0.7518680752383406,
        "step": 5836
    },
    {
        "loss": 2.0783,
        "grad_norm": 2.686213254928589,
        "learning_rate": 8.488994660261367e-05,
        "epoch": 0.7519969080133986,
        "step": 5837
    },
    {
        "loss": 1.7046,
        "grad_norm": 2.488651752471924,
        "learning_rate": 8.484634967626004e-05,
        "epoch": 0.7521257407884566,
        "step": 5838
    },
    {
        "loss": 1.7684,
        "grad_norm": 2.4472391605377197,
        "learning_rate": 8.480270117572067e-05,
        "epoch": 0.7522545735635146,
        "step": 5839
    },
    {
        "loss": 2.3542,
        "grad_norm": 1.8650553226470947,
        "learning_rate": 8.475900116559734e-05,
        "epoch": 0.7523834063385725,
        "step": 5840
    },
    {
        "loss": 1.7179,
        "grad_norm": 2.474684238433838,
        "learning_rate": 8.47152497105681e-05,
        "epoch": 0.7525122391136305,
        "step": 5841
    },
    {
        "loss": 1.6269,
        "grad_norm": 2.3305928707122803,
        "learning_rate": 8.467144687538707e-05,
        "epoch": 0.7526410718886885,
        "step": 5842
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.065601348876953,
        "learning_rate": 8.462759272488446e-05,
        "epoch": 0.7527699046637465,
        "step": 5843
    },
    {
        "loss": 1.9547,
        "grad_norm": 2.3696234226226807,
        "learning_rate": 8.45836873239664e-05,
        "epoch": 0.7528987374388044,
        "step": 5844
    },
    {
        "loss": 2.0862,
        "grad_norm": 1.8392908573150635,
        "learning_rate": 8.453973073761489e-05,
        "epoch": 0.7530275702138624,
        "step": 5845
    },
    {
        "loss": 2.1147,
        "grad_norm": 1.848353624343872,
        "learning_rate": 8.449572303088774e-05,
        "epoch": 0.7531564029889204,
        "step": 5846
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.8839845657348633,
        "learning_rate": 8.445166426891824e-05,
        "epoch": 0.7532852357639783,
        "step": 5847
    },
    {
        "loss": 2.369,
        "grad_norm": 1.380116581916809,
        "learning_rate": 8.440755451691544e-05,
        "epoch": 0.7534140685390364,
        "step": 5848
    },
    {
        "loss": 2.0903,
        "grad_norm": 2.6773674488067627,
        "learning_rate": 8.43633938401638e-05,
        "epoch": 0.7535429013140943,
        "step": 5849
    },
    {
        "loss": 2.1458,
        "grad_norm": 2.272125244140625,
        "learning_rate": 8.431918230402302e-05,
        "epoch": 0.7536717340891523,
        "step": 5850
    },
    {
        "loss": 1.7699,
        "grad_norm": 2.6880247592926025,
        "learning_rate": 8.427491997392831e-05,
        "epoch": 0.7538005668642103,
        "step": 5851
    },
    {
        "loss": 2.3189,
        "grad_norm": 2.0161449909210205,
        "learning_rate": 8.423060691538996e-05,
        "epoch": 0.7539293996392682,
        "step": 5852
    },
    {
        "loss": 2.0111,
        "grad_norm": 2.040018320083618,
        "learning_rate": 8.418624319399322e-05,
        "epoch": 0.7540582324143262,
        "step": 5853
    },
    {
        "loss": 2.443,
        "grad_norm": 2.1149916648864746,
        "learning_rate": 8.414182887539843e-05,
        "epoch": 0.7541870651893842,
        "step": 5854
    },
    {
        "loss": 2.4451,
        "grad_norm": 2.0464136600494385,
        "learning_rate": 8.409736402534084e-05,
        "epoch": 0.7543158979644422,
        "step": 5855
    },
    {
        "loss": 1.925,
        "grad_norm": 1.6951557397842407,
        "learning_rate": 8.405284870963045e-05,
        "epoch": 0.7544447307395001,
        "step": 5856
    },
    {
        "loss": 2.3192,
        "grad_norm": 1.867125391960144,
        "learning_rate": 8.4008282994152e-05,
        "epoch": 0.7545735635145581,
        "step": 5857
    },
    {
        "loss": 2.1993,
        "grad_norm": 1.7012234926223755,
        "learning_rate": 8.396366694486469e-05,
        "epoch": 0.7547023962896161,
        "step": 5858
    },
    {
        "loss": 1.4284,
        "grad_norm": 1.9876147508621216,
        "learning_rate": 8.391900062780227e-05,
        "epoch": 0.754831229064674,
        "step": 5859
    },
    {
        "loss": 2.2432,
        "grad_norm": 2.0076005458831787,
        "learning_rate": 8.387428410907312e-05,
        "epoch": 0.7549600618397321,
        "step": 5860
    },
    {
        "loss": 2.2258,
        "grad_norm": 2.9357035160064697,
        "learning_rate": 8.382951745485956e-05,
        "epoch": 0.75508889461479,
        "step": 5861
    },
    {
        "loss": 2.4842,
        "grad_norm": 1.8538484573364258,
        "learning_rate": 8.378470073141833e-05,
        "epoch": 0.7552177273898479,
        "step": 5862
    },
    {
        "loss": 2.2675,
        "grad_norm": 1.5899298191070557,
        "learning_rate": 8.373983400508028e-05,
        "epoch": 0.755346560164906,
        "step": 5863
    },
    {
        "loss": 2.2122,
        "grad_norm": 1.6269315481185913,
        "learning_rate": 8.369491734225008e-05,
        "epoch": 0.7554753929399639,
        "step": 5864
    },
    {
        "loss": 1.6136,
        "grad_norm": 2.4450178146362305,
        "learning_rate": 8.364995080940652e-05,
        "epoch": 0.755604225715022,
        "step": 5865
    },
    {
        "loss": 1.9779,
        "grad_norm": 2.6962053775787354,
        "learning_rate": 8.360493447310211e-05,
        "epoch": 0.7557330584900799,
        "step": 5866
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.3510318994522095,
        "learning_rate": 8.355986839996308e-05,
        "epoch": 0.7558618912651378,
        "step": 5867
    },
    {
        "loss": 0.8188,
        "grad_norm": 2.5497567653656006,
        "learning_rate": 8.351475265668925e-05,
        "epoch": 0.7559907240401958,
        "step": 5868
    },
    {
        "loss": 1.955,
        "grad_norm": 1.8297375440597534,
        "learning_rate": 8.346958731005401e-05,
        "epoch": 0.7561195568152538,
        "step": 5869
    },
    {
        "loss": 1.9622,
        "grad_norm": 1.3699538707733154,
        "learning_rate": 8.34243724269041e-05,
        "epoch": 0.7562483895903118,
        "step": 5870
    },
    {
        "loss": 1.8296,
        "grad_norm": 1.3374578952789307,
        "learning_rate": 8.337910807415969e-05,
        "epoch": 0.7563772223653698,
        "step": 5871
    },
    {
        "loss": 0.9554,
        "grad_norm": 3.325051784515381,
        "learning_rate": 8.333379431881397e-05,
        "epoch": 0.7565060551404277,
        "step": 5872
    },
    {
        "loss": 1.7029,
        "grad_norm": 5.541061878204346,
        "learning_rate": 8.328843122793341e-05,
        "epoch": 0.7566348879154857,
        "step": 5873
    },
    {
        "loss": 1.5121,
        "grad_norm": 2.710458755493164,
        "learning_rate": 8.324301886865751e-05,
        "epoch": 0.7567637206905437,
        "step": 5874
    },
    {
        "loss": 1.7201,
        "grad_norm": 2.6735281944274902,
        "learning_rate": 8.319755730819851e-05,
        "epoch": 0.7568925534656017,
        "step": 5875
    },
    {
        "loss": 2.2716,
        "grad_norm": 2.2369518280029297,
        "learning_rate": 8.315204661384162e-05,
        "epoch": 0.7570213862406596,
        "step": 5876
    },
    {
        "loss": 2.2833,
        "grad_norm": 2.6388707160949707,
        "learning_rate": 8.310648685294487e-05,
        "epoch": 0.7571502190157176,
        "step": 5877
    },
    {
        "loss": 2.3697,
        "grad_norm": 1.7749961614608765,
        "learning_rate": 8.306087809293866e-05,
        "epoch": 0.7572790517907756,
        "step": 5878
    },
    {
        "loss": 2.259,
        "grad_norm": 1.2988766431808472,
        "learning_rate": 8.301522040132605e-05,
        "epoch": 0.7574078845658335,
        "step": 5879
    },
    {
        "loss": 1.6282,
        "grad_norm": 3.3412017822265625,
        "learning_rate": 8.296951384568251e-05,
        "epoch": 0.7575367173408916,
        "step": 5880
    },
    {
        "loss": 1.9513,
        "grad_norm": 1.452135443687439,
        "learning_rate": 8.292375849365586e-05,
        "epoch": 0.7576655501159495,
        "step": 5881
    },
    {
        "loss": 2.278,
        "grad_norm": 1.5932320356369019,
        "learning_rate": 8.287795441296611e-05,
        "epoch": 0.7577943828910074,
        "step": 5882
    },
    {
        "loss": 2.416,
        "grad_norm": 1.7597377300262451,
        "learning_rate": 8.283210167140531e-05,
        "epoch": 0.7579232156660655,
        "step": 5883
    },
    {
        "loss": 2.1805,
        "grad_norm": 1.9712271690368652,
        "learning_rate": 8.278620033683767e-05,
        "epoch": 0.7580520484411234,
        "step": 5884
    },
    {
        "loss": 1.5079,
        "grad_norm": 2.0365793704986572,
        "learning_rate": 8.274025047719924e-05,
        "epoch": 0.7581808812161814,
        "step": 5885
    },
    {
        "loss": 1.7194,
        "grad_norm": 2.9118330478668213,
        "learning_rate": 8.26942521604979e-05,
        "epoch": 0.7583097139912394,
        "step": 5886
    },
    {
        "loss": 2.1912,
        "grad_norm": 1.7049946784973145,
        "learning_rate": 8.264820545481326e-05,
        "epoch": 0.7584385467662973,
        "step": 5887
    },
    {
        "loss": 1.8529,
        "grad_norm": 1.7830548286437988,
        "learning_rate": 8.260211042829661e-05,
        "epoch": 0.7585673795413553,
        "step": 5888
    },
    {
        "loss": 1.8327,
        "grad_norm": 1.9765530824661255,
        "learning_rate": 8.25559671491706e-05,
        "epoch": 0.7586962123164133,
        "step": 5889
    },
    {
        "loss": 2.6316,
        "grad_norm": 1.3235149383544922,
        "learning_rate": 8.250977568572939e-05,
        "epoch": 0.7588250450914713,
        "step": 5890
    },
    {
        "loss": 1.2603,
        "grad_norm": 2.6492693424224854,
        "learning_rate": 8.246353610633848e-05,
        "epoch": 0.7589538778665292,
        "step": 5891
    },
    {
        "loss": 2.3065,
        "grad_norm": 2.6489317417144775,
        "learning_rate": 8.241724847943455e-05,
        "epoch": 0.7590827106415872,
        "step": 5892
    },
    {
        "loss": 1.6689,
        "grad_norm": 3.695204257965088,
        "learning_rate": 8.237091287352539e-05,
        "epoch": 0.7592115434166452,
        "step": 5893
    },
    {
        "loss": 2.1461,
        "grad_norm": 1.6205700635910034,
        "learning_rate": 8.232452935718982e-05,
        "epoch": 0.7593403761917031,
        "step": 5894
    },
    {
        "loss": 1.5517,
        "grad_norm": 3.058283567428589,
        "learning_rate": 8.227809799907755e-05,
        "epoch": 0.7594692089667612,
        "step": 5895
    },
    {
        "loss": 1.1019,
        "grad_norm": 2.629518747329712,
        "learning_rate": 8.223161886790918e-05,
        "epoch": 0.7595980417418191,
        "step": 5896
    },
    {
        "loss": 1.3161,
        "grad_norm": 3.249149799346924,
        "learning_rate": 8.21850920324758e-05,
        "epoch": 0.7597268745168771,
        "step": 5897
    },
    {
        "loss": 1.9947,
        "grad_norm": 2.5506129264831543,
        "learning_rate": 8.213851756163933e-05,
        "epoch": 0.7598557072919351,
        "step": 5898
    },
    {
        "loss": 2.3913,
        "grad_norm": 1.862952709197998,
        "learning_rate": 8.209189552433216e-05,
        "epoch": 0.759984540066993,
        "step": 5899
    },
    {
        "loss": 2.0678,
        "grad_norm": 3.5266382694244385,
        "learning_rate": 8.20452259895569e-05,
        "epoch": 0.760113372842051,
        "step": 5900
    },
    {
        "loss": 2.2733,
        "grad_norm": 1.8496018648147583,
        "learning_rate": 8.199850902638667e-05,
        "epoch": 0.760242205617109,
        "step": 5901
    },
    {
        "loss": 1.8497,
        "grad_norm": 1.680959939956665,
        "learning_rate": 8.195174470396469e-05,
        "epoch": 0.760371038392167,
        "step": 5902
    },
    {
        "loss": 1.3799,
        "grad_norm": 3.2768707275390625,
        "learning_rate": 8.19049330915043e-05,
        "epoch": 0.760499871167225,
        "step": 5903
    },
    {
        "loss": 2.1285,
        "grad_norm": 1.7935478687286377,
        "learning_rate": 8.18580742582888e-05,
        "epoch": 0.7606287039422829,
        "step": 5904
    },
    {
        "loss": 1.5822,
        "grad_norm": 2.981741428375244,
        "learning_rate": 8.181116827367141e-05,
        "epoch": 0.7607575367173409,
        "step": 5905
    },
    {
        "loss": 1.904,
        "grad_norm": 2.160654067993164,
        "learning_rate": 8.176421520707514e-05,
        "epoch": 0.7608863694923989,
        "step": 5906
    },
    {
        "loss": 2.1252,
        "grad_norm": 2.2636780738830566,
        "learning_rate": 8.17172151279927e-05,
        "epoch": 0.7610152022674569,
        "step": 5907
    },
    {
        "loss": 2.3076,
        "grad_norm": 2.3288064002990723,
        "learning_rate": 8.167016810598624e-05,
        "epoch": 0.7611440350425148,
        "step": 5908
    },
    {
        "loss": 2.1773,
        "grad_norm": 1.5274344682693481,
        "learning_rate": 8.162307421068758e-05,
        "epoch": 0.7612728678175728,
        "step": 5909
    },
    {
        "loss": 2.1838,
        "grad_norm": 1.8997737169265747,
        "learning_rate": 8.15759335117978e-05,
        "epoch": 0.7614017005926308,
        "step": 5910
    },
    {
        "loss": 1.8185,
        "grad_norm": 1.8385266065597534,
        "learning_rate": 8.152874607908729e-05,
        "epoch": 0.7615305333676887,
        "step": 5911
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.473923683166504,
        "learning_rate": 8.14815119823956e-05,
        "epoch": 0.7616593661427468,
        "step": 5912
    },
    {
        "loss": 2.0342,
        "grad_norm": 2.2422935962677,
        "learning_rate": 8.143423129163143e-05,
        "epoch": 0.7617881989178047,
        "step": 5913
    },
    {
        "loss": 1.532,
        "grad_norm": 3.3519227504730225,
        "learning_rate": 8.138690407677221e-05,
        "epoch": 0.7619170316928626,
        "step": 5914
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.208603858947754,
        "learning_rate": 8.133953040786444e-05,
        "epoch": 0.7620458644679207,
        "step": 5915
    },
    {
        "loss": 1.9834,
        "grad_norm": 2.506756544113159,
        "learning_rate": 8.12921103550233e-05,
        "epoch": 0.7621746972429786,
        "step": 5916
    },
    {
        "loss": 1.7975,
        "grad_norm": 2.1809449195861816,
        "learning_rate": 8.124464398843267e-05,
        "epoch": 0.7623035300180366,
        "step": 5917
    },
    {
        "loss": 1.9986,
        "grad_norm": 1.8865058422088623,
        "learning_rate": 8.119713137834495e-05,
        "epoch": 0.7624323627930946,
        "step": 5918
    },
    {
        "loss": 1.919,
        "grad_norm": 2.1803362369537354,
        "learning_rate": 8.114957259508081e-05,
        "epoch": 0.7625611955681525,
        "step": 5919
    },
    {
        "loss": 1.8054,
        "grad_norm": 1.9740240573883057,
        "learning_rate": 8.11019677090296e-05,
        "epoch": 0.7626900283432105,
        "step": 5920
    },
    {
        "loss": 2.5248,
        "grad_norm": 1.7716647386550903,
        "learning_rate": 8.10543167906487e-05,
        "epoch": 0.7628188611182685,
        "step": 5921
    },
    {
        "loss": 1.5115,
        "grad_norm": 2.1250483989715576,
        "learning_rate": 8.100661991046354e-05,
        "epoch": 0.7629476938933265,
        "step": 5922
    },
    {
        "loss": 2.38,
        "grad_norm": 2.3293771743774414,
        "learning_rate": 8.095887713906774e-05,
        "epoch": 0.7630765266683844,
        "step": 5923
    },
    {
        "loss": 2.6108,
        "grad_norm": 1.859100103378296,
        "learning_rate": 8.091108854712283e-05,
        "epoch": 0.7632053594434424,
        "step": 5924
    },
    {
        "loss": 1.9951,
        "grad_norm": 2.0950889587402344,
        "learning_rate": 8.086325420535798e-05,
        "epoch": 0.7633341922185004,
        "step": 5925
    },
    {
        "loss": 2.1004,
        "grad_norm": 3.3070998191833496,
        "learning_rate": 8.081537418457026e-05,
        "epoch": 0.7634630249935583,
        "step": 5926
    },
    {
        "loss": 1.1902,
        "grad_norm": 2.7095065116882324,
        "learning_rate": 8.076744855562427e-05,
        "epoch": 0.7635918577686164,
        "step": 5927
    },
    {
        "loss": 1.5469,
        "grad_norm": 2.7928314208984375,
        "learning_rate": 8.071947738945216e-05,
        "epoch": 0.7637206905436743,
        "step": 5928
    },
    {
        "loss": 2.1919,
        "grad_norm": 2.0061352252960205,
        "learning_rate": 8.06714607570534e-05,
        "epoch": 0.7638495233187322,
        "step": 5929
    },
    {
        "loss": 2.1848,
        "grad_norm": 2.05532169342041,
        "learning_rate": 8.062339872949482e-05,
        "epoch": 0.7639783560937903,
        "step": 5930
    },
    {
        "loss": 1.8552,
        "grad_norm": 2.10573410987854,
        "learning_rate": 8.057529137791041e-05,
        "epoch": 0.7641071888688482,
        "step": 5931
    },
    {
        "loss": 1.7771,
        "grad_norm": 1.6512326002120972,
        "learning_rate": 8.052713877350129e-05,
        "epoch": 0.7642360216439062,
        "step": 5932
    },
    {
        "loss": 1.4378,
        "grad_norm": 3.8611268997192383,
        "learning_rate": 8.047894098753541e-05,
        "epoch": 0.7643648544189642,
        "step": 5933
    },
    {
        "loss": 1.783,
        "grad_norm": 2.641667366027832,
        "learning_rate": 8.043069809134774e-05,
        "epoch": 0.7644936871940221,
        "step": 5934
    },
    {
        "loss": 1.9732,
        "grad_norm": 2.789384603500366,
        "learning_rate": 8.038241015634001e-05,
        "epoch": 0.7646225199690801,
        "step": 5935
    },
    {
        "loss": 2.1233,
        "grad_norm": 2.556464195251465,
        "learning_rate": 8.033407725398039e-05,
        "epoch": 0.7647513527441381,
        "step": 5936
    },
    {
        "loss": 1.882,
        "grad_norm": 1.974693775177002,
        "learning_rate": 8.028569945580398e-05,
        "epoch": 0.7648801855191961,
        "step": 5937
    },
    {
        "loss": 1.8912,
        "grad_norm": 1.913769006729126,
        "learning_rate": 8.023727683341211e-05,
        "epoch": 0.765009018294254,
        "step": 5938
    },
    {
        "loss": 1.4752,
        "grad_norm": 3.144584894180298,
        "learning_rate": 8.018880945847235e-05,
        "epoch": 0.765137851069312,
        "step": 5939
    },
    {
        "loss": 1.8496,
        "grad_norm": 1.8240793943405151,
        "learning_rate": 8.014029740271869e-05,
        "epoch": 0.76526668384437,
        "step": 5940
    },
    {
        "loss": 2.0881,
        "grad_norm": 2.3410937786102295,
        "learning_rate": 8.009174073795116e-05,
        "epoch": 0.765395516619428,
        "step": 5941
    },
    {
        "loss": 2.4631,
        "grad_norm": 1.9498344659805298,
        "learning_rate": 8.004313953603588e-05,
        "epoch": 0.765524349394486,
        "step": 5942
    },
    {
        "loss": 1.6962,
        "grad_norm": 2.823314666748047,
        "learning_rate": 7.999449386890486e-05,
        "epoch": 0.7656531821695439,
        "step": 5943
    },
    {
        "loss": 2.4576,
        "grad_norm": 2.049896478652954,
        "learning_rate": 7.994580380855575e-05,
        "epoch": 0.7657820149446019,
        "step": 5944
    },
    {
        "loss": 1.897,
        "grad_norm": 2.668776035308838,
        "learning_rate": 7.989706942705221e-05,
        "epoch": 0.7659108477196599,
        "step": 5945
    },
    {
        "loss": 2.1181,
        "grad_norm": 2.176832675933838,
        "learning_rate": 7.984829079652339e-05,
        "epoch": 0.7660396804947178,
        "step": 5946
    },
    {
        "loss": 2.1573,
        "grad_norm": 1.826141357421875,
        "learning_rate": 7.979946798916374e-05,
        "epoch": 0.7661685132697759,
        "step": 5947
    },
    {
        "loss": 2.0373,
        "grad_norm": 2.013673782348633,
        "learning_rate": 7.975060107723329e-05,
        "epoch": 0.7662973460448338,
        "step": 5948
    },
    {
        "loss": 2.1903,
        "grad_norm": 2.392838478088379,
        "learning_rate": 7.970169013305738e-05,
        "epoch": 0.7664261788198918,
        "step": 5949
    },
    {
        "loss": 1.9167,
        "grad_norm": 2.32584547996521,
        "learning_rate": 7.96527352290263e-05,
        "epoch": 0.7665550115949498,
        "step": 5950
    },
    {
        "loss": 2.2648,
        "grad_norm": 3.387277364730835,
        "learning_rate": 7.960373643759563e-05,
        "epoch": 0.7666838443700077,
        "step": 5951
    },
    {
        "loss": 1.8189,
        "grad_norm": 1.8616607189178467,
        "learning_rate": 7.955469383128576e-05,
        "epoch": 0.7668126771450657,
        "step": 5952
    },
    {
        "loss": 1.862,
        "grad_norm": 2.8578665256500244,
        "learning_rate": 7.950560748268203e-05,
        "epoch": 0.7669415099201237,
        "step": 5953
    },
    {
        "loss": 2.1142,
        "grad_norm": 1.4525682926177979,
        "learning_rate": 7.945647746443445e-05,
        "epoch": 0.7670703426951817,
        "step": 5954
    },
    {
        "loss": 1.9867,
        "grad_norm": 1.772811770439148,
        "learning_rate": 7.940730384925769e-05,
        "epoch": 0.7671991754702396,
        "step": 5955
    },
    {
        "loss": 1.7745,
        "grad_norm": 2.6477811336517334,
        "learning_rate": 7.935808670993096e-05,
        "epoch": 0.7673280082452976,
        "step": 5956
    },
    {
        "loss": 1.9191,
        "grad_norm": 1.6036450862884521,
        "learning_rate": 7.930882611929791e-05,
        "epoch": 0.7674568410203556,
        "step": 5957
    },
    {
        "loss": 1.8943,
        "grad_norm": 2.299678325653076,
        "learning_rate": 7.925952215026632e-05,
        "epoch": 0.7675856737954135,
        "step": 5958
    },
    {
        "loss": 2.1904,
        "grad_norm": 4.148815631866455,
        "learning_rate": 7.921017487580842e-05,
        "epoch": 0.7677145065704716,
        "step": 5959
    },
    {
        "loss": 1.723,
        "grad_norm": 2.314772129058838,
        "learning_rate": 7.916078436896044e-05,
        "epoch": 0.7678433393455295,
        "step": 5960
    },
    {
        "loss": 1.9047,
        "grad_norm": 2.6855287551879883,
        "learning_rate": 7.911135070282249e-05,
        "epoch": 0.7679721721205874,
        "step": 5961
    },
    {
        "loss": 1.7666,
        "grad_norm": 2.6364498138427734,
        "learning_rate": 7.906187395055864e-05,
        "epoch": 0.7681010048956455,
        "step": 5962
    },
    {
        "loss": 1.8957,
        "grad_norm": 3.0377633571624756,
        "learning_rate": 7.901235418539692e-05,
        "epoch": 0.7682298376707034,
        "step": 5963
    },
    {
        "loss": 2.1316,
        "grad_norm": 1.7713472843170166,
        "learning_rate": 7.896279148062864e-05,
        "epoch": 0.7683586704457614,
        "step": 5964
    },
    {
        "loss": 2.3327,
        "grad_norm": 1.874359130859375,
        "learning_rate": 7.891318590960896e-05,
        "epoch": 0.7684875032208194,
        "step": 5965
    },
    {
        "loss": 1.9617,
        "grad_norm": 2.438319444656372,
        "learning_rate": 7.886353754575635e-05,
        "epoch": 0.7686163359958773,
        "step": 5966
    },
    {
        "loss": 1.7759,
        "grad_norm": 2.352052688598633,
        "learning_rate": 7.881384646255267e-05,
        "epoch": 0.7687451687709353,
        "step": 5967
    },
    {
        "loss": 1.2405,
        "grad_norm": 2.404200553894043,
        "learning_rate": 7.876411273354305e-05,
        "epoch": 0.7688740015459933,
        "step": 5968
    },
    {
        "loss": 2.333,
        "grad_norm": 1.6484787464141846,
        "learning_rate": 7.871433643233554e-05,
        "epoch": 0.7690028343210513,
        "step": 5969
    },
    {
        "loss": 0.8887,
        "grad_norm": 2.7358953952789307,
        "learning_rate": 7.866451763260136e-05,
        "epoch": 0.7691316670961093,
        "step": 5970
    },
    {
        "loss": 1.4811,
        "grad_norm": 2.3209919929504395,
        "learning_rate": 7.861465640807476e-05,
        "epoch": 0.7692604998711672,
        "step": 5971
    },
    {
        "loss": 1.6235,
        "grad_norm": 2.1746902465820312,
        "learning_rate": 7.856475283255244e-05,
        "epoch": 0.7693893326462252,
        "step": 5972
    },
    {
        "loss": 1.7939,
        "grad_norm": 2.4251418113708496,
        "learning_rate": 7.851480697989407e-05,
        "epoch": 0.7695181654212832,
        "step": 5973
    },
    {
        "loss": 1.9606,
        "grad_norm": 1.5028018951416016,
        "learning_rate": 7.84648189240218e-05,
        "epoch": 0.7696469981963412,
        "step": 5974
    },
    {
        "loss": 1.6968,
        "grad_norm": 2.611602783203125,
        "learning_rate": 7.841478873892012e-05,
        "epoch": 0.7697758309713991,
        "step": 5975
    },
    {
        "loss": 1.6065,
        "grad_norm": 2.331787586212158,
        "learning_rate": 7.836471649863605e-05,
        "epoch": 0.769904663746457,
        "step": 5976
    },
    {
        "loss": 2.0682,
        "grad_norm": 1.551697015762329,
        "learning_rate": 7.83146022772788e-05,
        "epoch": 0.7700334965215151,
        "step": 5977
    },
    {
        "loss": 2.2045,
        "grad_norm": 2.977642774581909,
        "learning_rate": 7.826444614901971e-05,
        "epoch": 0.770162329296573,
        "step": 5978
    },
    {
        "loss": 0.961,
        "grad_norm": 2.1182281970977783,
        "learning_rate": 7.82142481880921e-05,
        "epoch": 0.7702911620716311,
        "step": 5979
    },
    {
        "loss": 1.086,
        "grad_norm": 3.28883695602417,
        "learning_rate": 7.816400846879127e-05,
        "epoch": 0.770419994846689,
        "step": 5980
    },
    {
        "loss": 2.4222,
        "grad_norm": 2.3512089252471924,
        "learning_rate": 7.811372706547426e-05,
        "epoch": 0.7705488276217469,
        "step": 5981
    },
    {
        "loss": 1.6146,
        "grad_norm": 3.0037944316864014,
        "learning_rate": 7.806340405255994e-05,
        "epoch": 0.770677660396805,
        "step": 5982
    },
    {
        "loss": 2.0554,
        "grad_norm": 3.135789394378662,
        "learning_rate": 7.801303950452851e-05,
        "epoch": 0.7708064931718629,
        "step": 5983
    },
    {
        "loss": 2.3048,
        "grad_norm": 2.3962950706481934,
        "learning_rate": 7.796263349592188e-05,
        "epoch": 0.7709353259469209,
        "step": 5984
    },
    {
        "loss": 1.8744,
        "grad_norm": 2.1231415271759033,
        "learning_rate": 7.791218610134329e-05,
        "epoch": 0.7710641587219789,
        "step": 5985
    },
    {
        "loss": 1.4804,
        "grad_norm": 2.2900898456573486,
        "learning_rate": 7.786169739545706e-05,
        "epoch": 0.7711929914970368,
        "step": 5986
    },
    {
        "loss": 2.1832,
        "grad_norm": 2.4796080589294434,
        "learning_rate": 7.781116745298878e-05,
        "epoch": 0.7713218242720948,
        "step": 5987
    },
    {
        "loss": 2.0193,
        "grad_norm": 1.6891056299209595,
        "learning_rate": 7.776059634872524e-05,
        "epoch": 0.7714506570471528,
        "step": 5988
    },
    {
        "loss": 1.5756,
        "grad_norm": 3.318479537963867,
        "learning_rate": 7.770998415751381e-05,
        "epoch": 0.7715794898222108,
        "step": 5989
    },
    {
        "loss": 1.6418,
        "grad_norm": 2.4299991130828857,
        "learning_rate": 7.76593309542629e-05,
        "epoch": 0.7717083225972687,
        "step": 5990
    },
    {
        "loss": 2.085,
        "grad_norm": 1.688239336013794,
        "learning_rate": 7.760863681394156e-05,
        "epoch": 0.7718371553723267,
        "step": 5991
    },
    {
        "loss": 1.5828,
        "grad_norm": 4.15061616897583,
        "learning_rate": 7.755790181157942e-05,
        "epoch": 0.7719659881473847,
        "step": 5992
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.6424810886383057,
        "learning_rate": 7.750712602226667e-05,
        "epoch": 0.7720948209224426,
        "step": 5993
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.3318593502044678,
        "learning_rate": 7.745630952115363e-05,
        "epoch": 0.7722236536975007,
        "step": 5994
    },
    {
        "loss": 1.6371,
        "grad_norm": 1.7697207927703857,
        "learning_rate": 7.740545238345116e-05,
        "epoch": 0.7723524864725586,
        "step": 5995
    },
    {
        "loss": 2.6877,
        "grad_norm": 2.0885262489318848,
        "learning_rate": 7.73545546844301e-05,
        "epoch": 0.7724813192476166,
        "step": 5996
    },
    {
        "loss": 2.2778,
        "grad_norm": 2.391449451446533,
        "learning_rate": 7.730361649942137e-05,
        "epoch": 0.7726101520226746,
        "step": 5997
    },
    {
        "loss": 2.4355,
        "grad_norm": 1.6142576932907104,
        "learning_rate": 7.725263790381583e-05,
        "epoch": 0.7727389847977325,
        "step": 5998
    },
    {
        "loss": 1.5825,
        "grad_norm": 2.624124765396118,
        "learning_rate": 7.720161897306415e-05,
        "epoch": 0.7728678175727905,
        "step": 5999
    },
    {
        "loss": 2.2045,
        "grad_norm": 2.2634477615356445,
        "learning_rate": 7.715055978267658e-05,
        "epoch": 0.7729966503478485,
        "step": 6000
    },
    {
        "loss": 2.4332,
        "grad_norm": 1.600100040435791,
        "learning_rate": 7.709946040822308e-05,
        "epoch": 0.7731254831229065,
        "step": 6001
    },
    {
        "loss": 2.0352,
        "grad_norm": 2.6940667629241943,
        "learning_rate": 7.704832092533307e-05,
        "epoch": 0.7732543158979645,
        "step": 6002
    },
    {
        "loss": 1.9829,
        "grad_norm": 2.840123414993286,
        "learning_rate": 7.699714140969534e-05,
        "epoch": 0.7733831486730224,
        "step": 6003
    },
    {
        "loss": 2.1726,
        "grad_norm": 2.1339192390441895,
        "learning_rate": 7.694592193705787e-05,
        "epoch": 0.7735119814480804,
        "step": 6004
    },
    {
        "loss": 1.0874,
        "grad_norm": 1.1860853433609009,
        "learning_rate": 7.689466258322781e-05,
        "epoch": 0.7736408142231384,
        "step": 6005
    },
    {
        "loss": 1.3603,
        "grad_norm": 2.000657320022583,
        "learning_rate": 7.684336342407139e-05,
        "epoch": 0.7737696469981964,
        "step": 6006
    },
    {
        "loss": 2.057,
        "grad_norm": 2.107729434967041,
        "learning_rate": 7.679202453551371e-05,
        "epoch": 0.7738984797732543,
        "step": 6007
    },
    {
        "loss": 2.1765,
        "grad_norm": 2.4480724334716797,
        "learning_rate": 7.674064599353856e-05,
        "epoch": 0.7740273125483123,
        "step": 6008
    },
    {
        "loss": 2.2913,
        "grad_norm": 2.0612268447875977,
        "learning_rate": 7.668922787418859e-05,
        "epoch": 0.7741561453233703,
        "step": 6009
    },
    {
        "loss": 2.3149,
        "grad_norm": 3.2337679862976074,
        "learning_rate": 7.663777025356501e-05,
        "epoch": 0.7742849780984282,
        "step": 6010
    },
    {
        "loss": 2.4526,
        "grad_norm": 2.9684245586395264,
        "learning_rate": 7.658627320782733e-05,
        "epoch": 0.7744138108734863,
        "step": 6011
    },
    {
        "loss": 1.8996,
        "grad_norm": 1.6502801179885864,
        "learning_rate": 7.653473681319362e-05,
        "epoch": 0.7745426436485442,
        "step": 6012
    },
    {
        "loss": 1.1773,
        "grad_norm": 3.5271358489990234,
        "learning_rate": 7.648316114594004e-05,
        "epoch": 0.7746714764236021,
        "step": 6013
    },
    {
        "loss": 2.2127,
        "grad_norm": 3.318542957305908,
        "learning_rate": 7.643154628240095e-05,
        "epoch": 0.7748003091986602,
        "step": 6014
    },
    {
        "loss": 1.2622,
        "grad_norm": 2.8934903144836426,
        "learning_rate": 7.637989229896869e-05,
        "epoch": 0.7749291419737181,
        "step": 6015
    },
    {
        "loss": 1.8017,
        "grad_norm": 2.992218017578125,
        "learning_rate": 7.632819927209351e-05,
        "epoch": 0.7750579747487761,
        "step": 6016
    },
    {
        "loss": 2.0522,
        "grad_norm": 2.053493022918701,
        "learning_rate": 7.627646727828348e-05,
        "epoch": 0.7751868075238341,
        "step": 6017
    },
    {
        "loss": 2.1616,
        "grad_norm": 1.9904026985168457,
        "learning_rate": 7.622469639410432e-05,
        "epoch": 0.775315640298892,
        "step": 6018
    },
    {
        "loss": 2.1093,
        "grad_norm": 2.2235379219055176,
        "learning_rate": 7.61728866961792e-05,
        "epoch": 0.77544447307395,
        "step": 6019
    },
    {
        "loss": 1.9767,
        "grad_norm": 1.4290387630462646,
        "learning_rate": 7.612103826118891e-05,
        "epoch": 0.775573305849008,
        "step": 6020
    },
    {
        "loss": 1.5615,
        "grad_norm": 2.7443768978118896,
        "learning_rate": 7.606915116587148e-05,
        "epoch": 0.775702138624066,
        "step": 6021
    },
    {
        "loss": 2.0012,
        "grad_norm": 2.170206308364868,
        "learning_rate": 7.601722548702218e-05,
        "epoch": 0.7758309713991239,
        "step": 6022
    },
    {
        "loss": 2.2701,
        "grad_norm": 2.606494188308716,
        "learning_rate": 7.596526130149336e-05,
        "epoch": 0.7759598041741819,
        "step": 6023
    },
    {
        "loss": 1.6359,
        "grad_norm": 2.0919179916381836,
        "learning_rate": 7.591325868619447e-05,
        "epoch": 0.7760886369492399,
        "step": 6024
    },
    {
        "loss": 2.1172,
        "grad_norm": 1.7875280380249023,
        "learning_rate": 7.586121771809163e-05,
        "epoch": 0.7762174697242978,
        "step": 6025
    },
    {
        "loss": 1.9845,
        "grad_norm": 1.5095582008361816,
        "learning_rate": 7.580913847420788e-05,
        "epoch": 0.7763463024993559,
        "step": 6026
    },
    {
        "loss": 1.7185,
        "grad_norm": 2.1476242542266846,
        "learning_rate": 7.575702103162291e-05,
        "epoch": 0.7764751352744138,
        "step": 6027
    },
    {
        "loss": 2.1496,
        "grad_norm": 1.622496247291565,
        "learning_rate": 7.570486546747288e-05,
        "epoch": 0.7766039680494717,
        "step": 6028
    },
    {
        "loss": 2.2528,
        "grad_norm": 1.538138747215271,
        "learning_rate": 7.565267185895049e-05,
        "epoch": 0.7767328008245298,
        "step": 6029
    },
    {
        "loss": 2.0234,
        "grad_norm": 1.4364854097366333,
        "learning_rate": 7.560044028330444e-05,
        "epoch": 0.7768616335995877,
        "step": 6030
    },
    {
        "loss": 2.0723,
        "grad_norm": 1.685806155204773,
        "learning_rate": 7.554817081784007e-05,
        "epoch": 0.7769904663746457,
        "step": 6031
    },
    {
        "loss": 1.7513,
        "grad_norm": 3.622934341430664,
        "learning_rate": 7.549586353991854e-05,
        "epoch": 0.7771192991497037,
        "step": 6032
    },
    {
        "loss": 2.243,
        "grad_norm": 1.4343178272247314,
        "learning_rate": 7.544351852695689e-05,
        "epoch": 0.7772481319247616,
        "step": 6033
    },
    {
        "loss": 1.6625,
        "grad_norm": 2.2774012088775635,
        "learning_rate": 7.539113585642818e-05,
        "epoch": 0.7773769646998197,
        "step": 6034
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.4908910989761353,
        "learning_rate": 7.533871560586123e-05,
        "epoch": 0.7775057974748776,
        "step": 6035
    },
    {
        "loss": 2.1835,
        "grad_norm": 2.4428958892822266,
        "learning_rate": 7.528625785284026e-05,
        "epoch": 0.7776346302499356,
        "step": 6036
    },
    {
        "loss": 2.092,
        "grad_norm": 2.6088924407958984,
        "learning_rate": 7.523376267500521e-05,
        "epoch": 0.7777634630249936,
        "step": 6037
    },
    {
        "loss": 2.1345,
        "grad_norm": 2.223634719848633,
        "learning_rate": 7.518123015005132e-05,
        "epoch": 0.7778922958000515,
        "step": 6038
    },
    {
        "loss": 1.7208,
        "grad_norm": 1.7855701446533203,
        "learning_rate": 7.512866035572913e-05,
        "epoch": 0.7780211285751095,
        "step": 6039
    },
    {
        "loss": 2.2141,
        "grad_norm": 2.845036029815674,
        "learning_rate": 7.507605336984434e-05,
        "epoch": 0.7781499613501675,
        "step": 6040
    },
    {
        "loss": 2.3133,
        "grad_norm": 1.4519977569580078,
        "learning_rate": 7.502340927025766e-05,
        "epoch": 0.7782787941252255,
        "step": 6041
    },
    {
        "loss": 2.5924,
        "grad_norm": 2.112706422805786,
        "learning_rate": 7.497072813488479e-05,
        "epoch": 0.7784076269002834,
        "step": 6042
    },
    {
        "loss": 2.2836,
        "grad_norm": 1.8798900842666626,
        "learning_rate": 7.491801004169625e-05,
        "epoch": 0.7785364596753414,
        "step": 6043
    },
    {
        "loss": 1.5952,
        "grad_norm": 1.769405484199524,
        "learning_rate": 7.48652550687171e-05,
        "epoch": 0.7786652924503994,
        "step": 6044
    },
    {
        "loss": 2.4251,
        "grad_norm": 1.223618984222412,
        "learning_rate": 7.481246329402718e-05,
        "epoch": 0.7787941252254573,
        "step": 6045
    },
    {
        "loss": 1.6616,
        "grad_norm": 3.44618821144104,
        "learning_rate": 7.475963479576078e-05,
        "epoch": 0.7789229580005154,
        "step": 6046
    },
    {
        "loss": 1.573,
        "grad_norm": 2.7839293479919434,
        "learning_rate": 7.470676965210629e-05,
        "epoch": 0.7790517907755733,
        "step": 6047
    },
    {
        "loss": 1.2387,
        "grad_norm": 2.8164596557617188,
        "learning_rate": 7.465386794130672e-05,
        "epoch": 0.7791806235506313,
        "step": 6048
    },
    {
        "loss": 1.9435,
        "grad_norm": 2.1781227588653564,
        "learning_rate": 7.460092974165902e-05,
        "epoch": 0.7793094563256893,
        "step": 6049
    },
    {
        "loss": 1.7629,
        "grad_norm": 1.989916205406189,
        "learning_rate": 7.4547955131514e-05,
        "epoch": 0.7794382891007472,
        "step": 6050
    },
    {
        "loss": 1.7905,
        "grad_norm": 2.9881484508514404,
        "learning_rate": 7.449494418927658e-05,
        "epoch": 0.7795671218758052,
        "step": 6051
    },
    {
        "loss": 1.9245,
        "grad_norm": 2.5739364624023438,
        "learning_rate": 7.444189699340536e-05,
        "epoch": 0.7796959546508632,
        "step": 6052
    },
    {
        "loss": 0.9854,
        "grad_norm": 6.67407751083374,
        "learning_rate": 7.43888136224126e-05,
        "epoch": 0.7798247874259212,
        "step": 6053
    },
    {
        "loss": 1.8774,
        "grad_norm": 2.70672869682312,
        "learning_rate": 7.433569415486419e-05,
        "epoch": 0.7799536202009791,
        "step": 6054
    },
    {
        "loss": 2.349,
        "grad_norm": 1.3515424728393555,
        "learning_rate": 7.428253866937922e-05,
        "epoch": 0.7800824529760371,
        "step": 6055
    },
    {
        "loss": 1.4688,
        "grad_norm": 3.4426045417785645,
        "learning_rate": 7.422934724463024e-05,
        "epoch": 0.7802112857510951,
        "step": 6056
    },
    {
        "loss": 1.7472,
        "grad_norm": 1.9761210680007935,
        "learning_rate": 7.417611995934316e-05,
        "epoch": 0.780340118526153,
        "step": 6057
    },
    {
        "loss": 1.7263,
        "grad_norm": 1.9576725959777832,
        "learning_rate": 7.412285689229661e-05,
        "epoch": 0.7804689513012111,
        "step": 6058
    },
    {
        "loss": 1.9959,
        "grad_norm": 2.1175377368927,
        "learning_rate": 7.406955812232242e-05,
        "epoch": 0.780597784076269,
        "step": 6059
    },
    {
        "loss": 2.1843,
        "grad_norm": 2.77323317527771,
        "learning_rate": 7.401622372830525e-05,
        "epoch": 0.7807266168513269,
        "step": 6060
    },
    {
        "loss": 1.7449,
        "grad_norm": 2.410923480987549,
        "learning_rate": 7.396285378918235e-05,
        "epoch": 0.780855449626385,
        "step": 6061
    },
    {
        "loss": 1.8306,
        "grad_norm": 2.3639893531799316,
        "learning_rate": 7.390944838394366e-05,
        "epoch": 0.7809842824014429,
        "step": 6062
    },
    {
        "loss": 2.1427,
        "grad_norm": 3.0559885501861572,
        "learning_rate": 7.385600759163167e-05,
        "epoch": 0.781113115176501,
        "step": 6063
    },
    {
        "loss": 1.3817,
        "grad_norm": 3.247981309890747,
        "learning_rate": 7.380253149134115e-05,
        "epoch": 0.7812419479515589,
        "step": 6064
    },
    {
        "loss": 1.2982,
        "grad_norm": 2.0821073055267334,
        "learning_rate": 7.37490201622192e-05,
        "epoch": 0.7813707807266168,
        "step": 6065
    },
    {
        "loss": 2.5561,
        "grad_norm": 2.1811625957489014,
        "learning_rate": 7.369547368346499e-05,
        "epoch": 0.7814996135016749,
        "step": 6066
    },
    {
        "loss": 1.8902,
        "grad_norm": 2.4860281944274902,
        "learning_rate": 7.364189213432977e-05,
        "epoch": 0.7816284462767328,
        "step": 6067
    },
    {
        "loss": 1.8355,
        "grad_norm": 2.791963815689087,
        "learning_rate": 7.358827559411673e-05,
        "epoch": 0.7817572790517908,
        "step": 6068
    },
    {
        "loss": 2.0262,
        "grad_norm": 2.904947280883789,
        "learning_rate": 7.353462414218068e-05,
        "epoch": 0.7818861118268488,
        "step": 6069
    },
    {
        "loss": 2.1437,
        "grad_norm": 4.1139235496521,
        "learning_rate": 7.348093785792826e-05,
        "epoch": 0.7820149446019067,
        "step": 6070
    },
    {
        "loss": 1.4474,
        "grad_norm": 5.4641337394714355,
        "learning_rate": 7.342721682081769e-05,
        "epoch": 0.7821437773769647,
        "step": 6071
    },
    {
        "loss": 2.392,
        "grad_norm": 2.544995069503784,
        "learning_rate": 7.337346111035844e-05,
        "epoch": 0.7822726101520227,
        "step": 6072
    },
    {
        "loss": 1.6612,
        "grad_norm": 3.0427966117858887,
        "learning_rate": 7.331967080611138e-05,
        "epoch": 0.7824014429270807,
        "step": 6073
    },
    {
        "loss": 1.942,
        "grad_norm": 1.574832558631897,
        "learning_rate": 7.326584598768882e-05,
        "epoch": 0.7825302757021386,
        "step": 6074
    },
    {
        "loss": 1.9163,
        "grad_norm": 1.8113470077514648,
        "learning_rate": 7.321198673475375e-05,
        "epoch": 0.7826591084771966,
        "step": 6075
    },
    {
        "loss": 2.2366,
        "grad_norm": 1.634124755859375,
        "learning_rate": 7.315809312702038e-05,
        "epoch": 0.7827879412522546,
        "step": 6076
    },
    {
        "loss": 1.6247,
        "grad_norm": 1.9908965826034546,
        "learning_rate": 7.310416524425373e-05,
        "epoch": 0.7829167740273125,
        "step": 6077
    },
    {
        "loss": 2.5374,
        "grad_norm": 1.6240293979644775,
        "learning_rate": 7.305020316626951e-05,
        "epoch": 0.7830456068023706,
        "step": 6078
    },
    {
        "loss": 2.589,
        "grad_norm": 1.8769302368164062,
        "learning_rate": 7.29962069729341e-05,
        "epoch": 0.7831744395774285,
        "step": 6079
    },
    {
        "loss": 1.0745,
        "grad_norm": 2.8371222019195557,
        "learning_rate": 7.294217674416422e-05,
        "epoch": 0.7833032723524864,
        "step": 6080
    },
    {
        "loss": 1.3128,
        "grad_norm": 3.488590955734253,
        "learning_rate": 7.288811255992707e-05,
        "epoch": 0.7834321051275445,
        "step": 6081
    },
    {
        "loss": 2.4125,
        "grad_norm": 2.6175663471221924,
        "learning_rate": 7.283401450024028e-05,
        "epoch": 0.7835609379026024,
        "step": 6082
    },
    {
        "loss": 2.1857,
        "grad_norm": 2.0947988033294678,
        "learning_rate": 7.277988264517127e-05,
        "epoch": 0.7836897706776604,
        "step": 6083
    },
    {
        "loss": 2.7174,
        "grad_norm": 2.00810170173645,
        "learning_rate": 7.272571707483772e-05,
        "epoch": 0.7838186034527184,
        "step": 6084
    },
    {
        "loss": 2.4997,
        "grad_norm": 1.4240436553955078,
        "learning_rate": 7.267151786940718e-05,
        "epoch": 0.7839474362277763,
        "step": 6085
    },
    {
        "loss": 1.8707,
        "grad_norm": 2.138289451599121,
        "learning_rate": 7.261728510909685e-05,
        "epoch": 0.7840762690028343,
        "step": 6086
    },
    {
        "loss": 1.4661,
        "grad_norm": 2.2565019130706787,
        "learning_rate": 7.256301887417375e-05,
        "epoch": 0.7842051017778923,
        "step": 6087
    },
    {
        "loss": 1.7949,
        "grad_norm": 2.581603765487671,
        "learning_rate": 7.250871924495434e-05,
        "epoch": 0.7843339345529503,
        "step": 6088
    },
    {
        "loss": 1.8785,
        "grad_norm": 2.876105308532715,
        "learning_rate": 7.24543863018046e-05,
        "epoch": 0.7844627673280082,
        "step": 6089
    },
    {
        "loss": 2.0223,
        "grad_norm": 2.3089494705200195,
        "learning_rate": 7.240002012513973e-05,
        "epoch": 0.7845916001030662,
        "step": 6090
    },
    {
        "loss": 1.6443,
        "grad_norm": 2.7821197509765625,
        "learning_rate": 7.234562079542413e-05,
        "epoch": 0.7847204328781242,
        "step": 6091
    },
    {
        "loss": 2.0818,
        "grad_norm": 2.0518057346343994,
        "learning_rate": 7.229118839317133e-05,
        "epoch": 0.7848492656531821,
        "step": 6092
    },
    {
        "loss": 1.6985,
        "grad_norm": 1.9203855991363525,
        "learning_rate": 7.223672299894381e-05,
        "epoch": 0.7849780984282402,
        "step": 6093
    },
    {
        "loss": 1.548,
        "grad_norm": 2.2564899921417236,
        "learning_rate": 7.21822246933527e-05,
        "epoch": 0.7851069312032981,
        "step": 6094
    },
    {
        "loss": 1.7784,
        "grad_norm": 1.7795058488845825,
        "learning_rate": 7.212769355705808e-05,
        "epoch": 0.785235763978356,
        "step": 6095
    },
    {
        "loss": 2.3349,
        "grad_norm": 2.1867570877075195,
        "learning_rate": 7.207312967076853e-05,
        "epoch": 0.7853645967534141,
        "step": 6096
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.8977570533752441,
        "learning_rate": 7.201853311524099e-05,
        "epoch": 0.785493429528472,
        "step": 6097
    },
    {
        "loss": 1.9086,
        "grad_norm": 2.71866774559021,
        "learning_rate": 7.19639039712809e-05,
        "epoch": 0.78562226230353,
        "step": 6098
    },
    {
        "loss": 2.0187,
        "grad_norm": 1.5661567449569702,
        "learning_rate": 7.190924231974189e-05,
        "epoch": 0.785751095078588,
        "step": 6099
    },
    {
        "loss": 1.8998,
        "grad_norm": 2.2496747970581055,
        "learning_rate": 7.185454824152568e-05,
        "epoch": 0.785879927853646,
        "step": 6100
    },
    {
        "loss": 2.0706,
        "grad_norm": 2.3964638710021973,
        "learning_rate": 7.179982181758204e-05,
        "epoch": 0.786008760628704,
        "step": 6101
    },
    {
        "loss": 2.4194,
        "grad_norm": 1.3935139179229736,
        "learning_rate": 7.174506312890853e-05,
        "epoch": 0.7861375934037619,
        "step": 6102
    },
    {
        "loss": 1.6792,
        "grad_norm": 2.831493854522705,
        "learning_rate": 7.169027225655053e-05,
        "epoch": 0.7862664261788199,
        "step": 6103
    },
    {
        "loss": 2.2125,
        "grad_norm": 1.6946842670440674,
        "learning_rate": 7.163544928160107e-05,
        "epoch": 0.7863952589538779,
        "step": 6104
    },
    {
        "loss": 2.4539,
        "grad_norm": 1.862055778503418,
        "learning_rate": 7.158059428520055e-05,
        "epoch": 0.7865240917289359,
        "step": 6105
    },
    {
        "loss": 1.7302,
        "grad_norm": 3.047098398208618,
        "learning_rate": 7.152570734853691e-05,
        "epoch": 0.7866529245039938,
        "step": 6106
    },
    {
        "loss": 1.041,
        "grad_norm": 3.3045742511749268,
        "learning_rate": 7.147078855284531e-05,
        "epoch": 0.7867817572790518,
        "step": 6107
    },
    {
        "loss": 1.4386,
        "grad_norm": 3.0075249671936035,
        "learning_rate": 7.141583797940809e-05,
        "epoch": 0.7869105900541098,
        "step": 6108
    },
    {
        "loss": 1.9107,
        "grad_norm": 2.5588526725769043,
        "learning_rate": 7.136085570955461e-05,
        "epoch": 0.7870394228291677,
        "step": 6109
    },
    {
        "loss": 2.0487,
        "grad_norm": 1.2839962244033813,
        "learning_rate": 7.130584182466116e-05,
        "epoch": 0.7871682556042258,
        "step": 6110
    },
    {
        "loss": 2.0145,
        "grad_norm": 2.7431349754333496,
        "learning_rate": 7.125079640615071e-05,
        "epoch": 0.7872970883792837,
        "step": 6111
    },
    {
        "loss": 1.857,
        "grad_norm": 3.183863878250122,
        "learning_rate": 7.119571953549305e-05,
        "epoch": 0.7874259211543416,
        "step": 6112
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.6793696880340576,
        "learning_rate": 7.114061129420442e-05,
        "epoch": 0.7875547539293997,
        "step": 6113
    },
    {
        "loss": 1.6235,
        "grad_norm": 2.0314457416534424,
        "learning_rate": 7.108547176384758e-05,
        "epoch": 0.7876835867044576,
        "step": 6114
    },
    {
        "loss": 2.147,
        "grad_norm": 2.1771442890167236,
        "learning_rate": 7.103030102603156e-05,
        "epoch": 0.7878124194795156,
        "step": 6115
    },
    {
        "loss": 1.9762,
        "grad_norm": 2.093665361404419,
        "learning_rate": 7.097509916241141e-05,
        "epoch": 0.7879412522545736,
        "step": 6116
    },
    {
        "loss": 2.1835,
        "grad_norm": 1.54505455493927,
        "learning_rate": 7.09198662546886e-05,
        "epoch": 0.7880700850296315,
        "step": 6117
    },
    {
        "loss": 1.8265,
        "grad_norm": 2.305424451828003,
        "learning_rate": 7.086460238461032e-05,
        "epoch": 0.7881989178046895,
        "step": 6118
    },
    {
        "loss": 1.5693,
        "grad_norm": 2.3370423316955566,
        "learning_rate": 7.08093076339695e-05,
        "epoch": 0.7883277505797475,
        "step": 6119
    },
    {
        "loss": 1.7178,
        "grad_norm": 2.335204839706421,
        "learning_rate": 7.0753982084605e-05,
        "epoch": 0.7884565833548055,
        "step": 6120
    },
    {
        "loss": 2.4675,
        "grad_norm": 1.681472659111023,
        "learning_rate": 7.069862581840115e-05,
        "epoch": 0.7885854161298634,
        "step": 6121
    },
    {
        "loss": 2.1873,
        "grad_norm": 1.9264105558395386,
        "learning_rate": 7.064323891728765e-05,
        "epoch": 0.7887142489049214,
        "step": 6122
    },
    {
        "loss": 1.4924,
        "grad_norm": 2.658665657043457,
        "learning_rate": 7.058782146323971e-05,
        "epoch": 0.7888430816799794,
        "step": 6123
    },
    {
        "loss": 1.9116,
        "grad_norm": 2.35260272026062,
        "learning_rate": 7.053237353827767e-05,
        "epoch": 0.7889719144550373,
        "step": 6124
    },
    {
        "loss": 2.5753,
        "grad_norm": 1.7377521991729736,
        "learning_rate": 7.0476895224467e-05,
        "epoch": 0.7891007472300954,
        "step": 6125
    },
    {
        "loss": 1.2215,
        "grad_norm": Infinity,
        "learning_rate": 7.0476895224467e-05,
        "epoch": 0.7892295800051533,
        "step": 6126
    },
    {
        "loss": 2.7254,
        "grad_norm": 1.883315920829773,
        "learning_rate": 7.042138660391813e-05,
        "epoch": 0.7893584127802112,
        "step": 6127
    },
    {
        "loss": 1.2349,
        "grad_norm": 2.6184489727020264,
        "learning_rate": 7.036584775878634e-05,
        "epoch": 0.7894872455552693,
        "step": 6128
    },
    {
        "loss": 2.5707,
        "grad_norm": 3.002920627593994,
        "learning_rate": 7.031027877127165e-05,
        "epoch": 0.7896160783303272,
        "step": 6129
    },
    {
        "loss": 1.7111,
        "grad_norm": 2.948835611343384,
        "learning_rate": 7.025467972361879e-05,
        "epoch": 0.7897449111053852,
        "step": 6130
    },
    {
        "loss": 2.188,
        "grad_norm": 1.2284468412399292,
        "learning_rate": 7.019905069811672e-05,
        "epoch": 0.7898737438804432,
        "step": 6131
    },
    {
        "loss": 2.2814,
        "grad_norm": 1.4598925113677979,
        "learning_rate": 7.014339177709901e-05,
        "epoch": 0.7900025766555011,
        "step": 6132
    },
    {
        "loss": 1.6466,
        "grad_norm": 2.3514809608459473,
        "learning_rate": 7.008770304294345e-05,
        "epoch": 0.7901314094305592,
        "step": 6133
    },
    {
        "loss": 2.0631,
        "grad_norm": 3.590657949447632,
        "learning_rate": 7.003198457807174e-05,
        "epoch": 0.7902602422056171,
        "step": 6134
    },
    {
        "loss": 2.4361,
        "grad_norm": 1.5012978315353394,
        "learning_rate": 6.997623646494992e-05,
        "epoch": 0.7903890749806751,
        "step": 6135
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.3056696653366089,
        "learning_rate": 6.992045878608776e-05,
        "epoch": 0.790517907755733,
        "step": 6136
    },
    {
        "loss": 1.9803,
        "grad_norm": 2.648096799850464,
        "learning_rate": 6.98646516240386e-05,
        "epoch": 0.790646740530791,
        "step": 6137
    },
    {
        "loss": 1.4042,
        "grad_norm": 2.9892687797546387,
        "learning_rate": 6.980881506139969e-05,
        "epoch": 0.790775573305849,
        "step": 6138
    },
    {
        "loss": 2.2632,
        "grad_norm": 1.9631733894348145,
        "learning_rate": 6.975294918081167e-05,
        "epoch": 0.790904406080907,
        "step": 6139
    },
    {
        "loss": 1.5693,
        "grad_norm": 2.4319331645965576,
        "learning_rate": 6.969705406495857e-05,
        "epoch": 0.791033238855965,
        "step": 6140
    },
    {
        "loss": 0.9828,
        "grad_norm": 3.3755364418029785,
        "learning_rate": 6.96411297965678e-05,
        "epoch": 0.7911620716310229,
        "step": 6141
    },
    {
        "loss": 2.058,
        "grad_norm": 2.286149501800537,
        "learning_rate": 6.958517645840962e-05,
        "epoch": 0.7912909044060809,
        "step": 6142
    },
    {
        "loss": 2.1056,
        "grad_norm": 1.9113363027572632,
        "learning_rate": 6.952919413329771e-05,
        "epoch": 0.7914197371811389,
        "step": 6143
    },
    {
        "loss": 2.0215,
        "grad_norm": 1.7376712560653687,
        "learning_rate": 6.947318290408849e-05,
        "epoch": 0.7915485699561968,
        "step": 6144
    },
    {
        "loss": 2.1122,
        "grad_norm": 1.354943871498108,
        "learning_rate": 6.941714285368095e-05,
        "epoch": 0.7916774027312549,
        "step": 6145
    },
    {
        "loss": 1.9146,
        "grad_norm": 1.8352433443069458,
        "learning_rate": 6.936107406501705e-05,
        "epoch": 0.7918062355063128,
        "step": 6146
    },
    {
        "loss": 1.8842,
        "grad_norm": 1.7634605169296265,
        "learning_rate": 6.930497662108118e-05,
        "epoch": 0.7919350682813708,
        "step": 6147
    },
    {
        "loss": 2.2698,
        "grad_norm": 2.6279964447021484,
        "learning_rate": 6.924885060489999e-05,
        "epoch": 0.7920639010564288,
        "step": 6148
    },
    {
        "loss": 2.051,
        "grad_norm": 2.6690776348114014,
        "learning_rate": 6.919269609954257e-05,
        "epoch": 0.7921927338314867,
        "step": 6149
    },
    {
        "loss": 2.1057,
        "grad_norm": 2.758331060409546,
        "learning_rate": 6.913651318812022e-05,
        "epoch": 0.7923215666065447,
        "step": 6150
    },
    {
        "loss": 1.2253,
        "grad_norm": 3.3854880332946777,
        "learning_rate": 6.908030195378615e-05,
        "epoch": 0.7924503993816027,
        "step": 6151
    },
    {
        "loss": 2.3483,
        "grad_norm": 1.703788161277771,
        "learning_rate": 6.902406247973559e-05,
        "epoch": 0.7925792321566607,
        "step": 6152
    },
    {
        "loss": 1.5366,
        "grad_norm": 3.983064651489258,
        "learning_rate": 6.896779484920549e-05,
        "epoch": 0.7927080649317186,
        "step": 6153
    },
    {
        "loss": 2.2541,
        "grad_norm": 2.2978787422180176,
        "learning_rate": 6.891149914547452e-05,
        "epoch": 0.7928368977067766,
        "step": 6154
    },
    {
        "loss": 0.842,
        "grad_norm": 3.4322705268859863,
        "learning_rate": 6.885517545186298e-05,
        "epoch": 0.7929657304818346,
        "step": 6155
    },
    {
        "loss": 2.2974,
        "grad_norm": 2.584653377532959,
        "learning_rate": 6.879882385173233e-05,
        "epoch": 0.7930945632568925,
        "step": 6156
    },
    {
        "loss": 1.9844,
        "grad_norm": 1.4270259141921997,
        "learning_rate": 6.874244442848559e-05,
        "epoch": 0.7932233960319506,
        "step": 6157
    },
    {
        "loss": 2.2764,
        "grad_norm": 2.277496099472046,
        "learning_rate": 6.868603726556693e-05,
        "epoch": 0.7933522288070085,
        "step": 6158
    },
    {
        "loss": 1.6436,
        "grad_norm": 2.8688056468963623,
        "learning_rate": 6.862960244646135e-05,
        "epoch": 0.7934810615820664,
        "step": 6159
    },
    {
        "loss": 2.0412,
        "grad_norm": 4.091868877410889,
        "learning_rate": 6.857314005469515e-05,
        "epoch": 0.7936098943571245,
        "step": 6160
    },
    {
        "loss": 1.6791,
        "grad_norm": 2.070359706878662,
        "learning_rate": 6.85166501738352e-05,
        "epoch": 0.7937387271321824,
        "step": 6161
    },
    {
        "loss": 1.8487,
        "grad_norm": 2.4847209453582764,
        "learning_rate": 6.846013288748902e-05,
        "epoch": 0.7938675599072404,
        "step": 6162
    },
    {
        "loss": 2.3466,
        "grad_norm": 1.9125902652740479,
        "learning_rate": 6.840358827930483e-05,
        "epoch": 0.7939963926822984,
        "step": 6163
    },
    {
        "loss": 1.9892,
        "grad_norm": 2.012561798095703,
        "learning_rate": 6.834701643297118e-05,
        "epoch": 0.7941252254573563,
        "step": 6164
    },
    {
        "loss": 1.6473,
        "grad_norm": 2.807112216949463,
        "learning_rate": 6.829041743221703e-05,
        "epoch": 0.7942540582324144,
        "step": 6165
    },
    {
        "loss": 1.9894,
        "grad_norm": 2.1211166381835938,
        "learning_rate": 6.823379136081152e-05,
        "epoch": 0.7943828910074723,
        "step": 6166
    },
    {
        "loss": 1.7642,
        "grad_norm": 2.226608991622925,
        "learning_rate": 6.81771383025637e-05,
        "epoch": 0.7945117237825303,
        "step": 6167
    },
    {
        "loss": 1.8502,
        "grad_norm": 1.8851207494735718,
        "learning_rate": 6.812045834132268e-05,
        "epoch": 0.7946405565575883,
        "step": 6168
    },
    {
        "loss": 1.7326,
        "grad_norm": 2.3573594093322754,
        "learning_rate": 6.806375156097756e-05,
        "epoch": 0.7947693893326462,
        "step": 6169
    },
    {
        "loss": 1.8339,
        "grad_norm": 2.471796751022339,
        "learning_rate": 6.800701804545677e-05,
        "epoch": 0.7948982221077042,
        "step": 6170
    },
    {
        "loss": 2.1523,
        "grad_norm": 1.7018882036209106,
        "learning_rate": 6.795025787872858e-05,
        "epoch": 0.7950270548827622,
        "step": 6171
    },
    {
        "loss": 2.0978,
        "grad_norm": 2.6823394298553467,
        "learning_rate": 6.789347114480069e-05,
        "epoch": 0.7951558876578202,
        "step": 6172
    },
    {
        "loss": 1.904,
        "grad_norm": 2.6199758052825928,
        "learning_rate": 6.783665792771992e-05,
        "epoch": 0.7952847204328781,
        "step": 6173
    },
    {
        "loss": 2.0994,
        "grad_norm": 2.0798840522766113,
        "learning_rate": 6.777981831157245e-05,
        "epoch": 0.795413553207936,
        "step": 6174
    },
    {
        "loss": 2.2509,
        "grad_norm": 2.220506191253662,
        "learning_rate": 6.772295238048354e-05,
        "epoch": 0.7955423859829941,
        "step": 6175
    },
    {
        "loss": 1.3459,
        "grad_norm": 3.243415117263794,
        "learning_rate": 6.766606021861738e-05,
        "epoch": 0.795671218758052,
        "step": 6176
    },
    {
        "loss": 2.4114,
        "grad_norm": 2.045452833175659,
        "learning_rate": 6.760914191017691e-05,
        "epoch": 0.7958000515331101,
        "step": 6177
    },
    {
        "loss": 2.054,
        "grad_norm": 1.9481452703475952,
        "learning_rate": 6.755219753940385e-05,
        "epoch": 0.795928884308168,
        "step": 6178
    },
    {
        "loss": 2.2606,
        "grad_norm": 1.4957560300827026,
        "learning_rate": 6.749522719057847e-05,
        "epoch": 0.7960577170832259,
        "step": 6179
    },
    {
        "loss": 2.414,
        "grad_norm": 2.1249873638153076,
        "learning_rate": 6.743823094801953e-05,
        "epoch": 0.796186549858284,
        "step": 6180
    },
    {
        "loss": 2.349,
        "grad_norm": 3.026858329772949,
        "learning_rate": 6.738120889608394e-05,
        "epoch": 0.7963153826333419,
        "step": 6181
    },
    {
        "loss": 2.1735,
        "grad_norm": 1.5589860677719116,
        "learning_rate": 6.732416111916702e-05,
        "epoch": 0.7964442154083999,
        "step": 6182
    },
    {
        "loss": 2.0292,
        "grad_norm": 2.228637456893921,
        "learning_rate": 6.726708770170212e-05,
        "epoch": 0.7965730481834579,
        "step": 6183
    },
    {
        "loss": 2.6046,
        "grad_norm": 4.113323211669922,
        "learning_rate": 6.720998872816037e-05,
        "epoch": 0.7967018809585158,
        "step": 6184
    },
    {
        "loss": 1.5984,
        "grad_norm": 2.497455596923828,
        "learning_rate": 6.715286428305086e-05,
        "epoch": 0.7968307137335738,
        "step": 6185
    },
    {
        "loss": 2.3114,
        "grad_norm": 1.947611689567566,
        "learning_rate": 6.709571445092058e-05,
        "epoch": 0.7969595465086318,
        "step": 6186
    },
    {
        "loss": 1.6677,
        "grad_norm": 2.4047727584838867,
        "learning_rate": 6.703853931635369e-05,
        "epoch": 0.7970883792836898,
        "step": 6187
    },
    {
        "loss": 1.9825,
        "grad_norm": 2.0081098079681396,
        "learning_rate": 6.698133896397205e-05,
        "epoch": 0.7972172120587477,
        "step": 6188
    },
    {
        "loss": 2.0454,
        "grad_norm": 2.0270864963531494,
        "learning_rate": 6.692411347843476e-05,
        "epoch": 0.7973460448338057,
        "step": 6189
    },
    {
        "loss": 2.1183,
        "grad_norm": 2.4737279415130615,
        "learning_rate": 6.68668629444382e-05,
        "epoch": 0.7974748776088637,
        "step": 6190
    },
    {
        "loss": 1.9945,
        "grad_norm": 2.6056931018829346,
        "learning_rate": 6.680958744671581e-05,
        "epoch": 0.7976037103839216,
        "step": 6191
    },
    {
        "loss": 1.1025,
        "grad_norm": 2.711702823638916,
        "learning_rate": 6.67522870700378e-05,
        "epoch": 0.7977325431589797,
        "step": 6192
    },
    {
        "loss": 2.0605,
        "grad_norm": 2.209991931915283,
        "learning_rate": 6.669496189921141e-05,
        "epoch": 0.7978613759340376,
        "step": 6193
    },
    {
        "loss": 2.1623,
        "grad_norm": 2.4132301807403564,
        "learning_rate": 6.663761201908052e-05,
        "epoch": 0.7979902087090955,
        "step": 6194
    },
    {
        "loss": 2.5904,
        "grad_norm": 1.772286295890808,
        "learning_rate": 6.658023751452555e-05,
        "epoch": 0.7981190414841536,
        "step": 6195
    },
    {
        "loss": 1.5714,
        "grad_norm": 3.5405378341674805,
        "learning_rate": 6.652283847046337e-05,
        "epoch": 0.7982478742592115,
        "step": 6196
    },
    {
        "loss": 1.9046,
        "grad_norm": 1.7262802124023438,
        "learning_rate": 6.646541497184728e-05,
        "epoch": 0.7983767070342696,
        "step": 6197
    },
    {
        "loss": 1.7713,
        "grad_norm": 2.4895098209381104,
        "learning_rate": 6.640796710366652e-05,
        "epoch": 0.7985055398093275,
        "step": 6198
    },
    {
        "loss": 1.8527,
        "grad_norm": 2.588682174682617,
        "learning_rate": 6.635049495094663e-05,
        "epoch": 0.7986343725843855,
        "step": 6199
    },
    {
        "loss": 2.1451,
        "grad_norm": 1.9284170866012573,
        "learning_rate": 6.6292998598749e-05,
        "epoch": 0.7987632053594435,
        "step": 6200
    },
    {
        "loss": 1.7549,
        "grad_norm": 1.690356731414795,
        "learning_rate": 6.623547813217086e-05,
        "epoch": 0.7988920381345014,
        "step": 6201
    },
    {
        "loss": 1.5067,
        "grad_norm": 2.9870662689208984,
        "learning_rate": 6.617793363634513e-05,
        "epoch": 0.7990208709095594,
        "step": 6202
    },
    {
        "loss": 1.8253,
        "grad_norm": 2.3265841007232666,
        "learning_rate": 6.612036519644029e-05,
        "epoch": 0.7991497036846174,
        "step": 6203
    },
    {
        "loss": 1.4127,
        "grad_norm": 3.3008179664611816,
        "learning_rate": 6.606277289766023e-05,
        "epoch": 0.7992785364596754,
        "step": 6204
    },
    {
        "loss": 1.4758,
        "grad_norm": 2.607177257537842,
        "learning_rate": 6.600515682524428e-05,
        "epoch": 0.7994073692347333,
        "step": 6205
    },
    {
        "loss": 2.3129,
        "grad_norm": 2.0034382343292236,
        "learning_rate": 6.594751706446668e-05,
        "epoch": 0.7995362020097913,
        "step": 6206
    },
    {
        "loss": 1.1907,
        "grad_norm": 3.266972303390503,
        "learning_rate": 6.588985370063699e-05,
        "epoch": 0.7996650347848493,
        "step": 6207
    },
    {
        "loss": 2.1847,
        "grad_norm": 2.1976394653320312,
        "learning_rate": 6.583216681909969e-05,
        "epoch": 0.7997938675599072,
        "step": 6208
    },
    {
        "loss": 1.5806,
        "grad_norm": 2.593158721923828,
        "learning_rate": 6.577445650523385e-05,
        "epoch": 0.7999227003349653,
        "step": 6209
    },
    {
        "loss": 2.1526,
        "grad_norm": 2.1269640922546387,
        "learning_rate": 6.571672284445342e-05,
        "epoch": 0.8000515331100232,
        "step": 6210
    },
    {
        "loss": 1.8698,
        "grad_norm": 2.7535245418548584,
        "learning_rate": 6.565896592220688e-05,
        "epoch": 0.8001803658850811,
        "step": 6211
    },
    {
        "loss": 1.5657,
        "grad_norm": 2.636540174484253,
        "learning_rate": 6.56011858239771e-05,
        "epoch": 0.8003091986601392,
        "step": 6212
    },
    {
        "loss": 0.828,
        "grad_norm": 6.292338848114014,
        "learning_rate": 6.554338263528124e-05,
        "epoch": 0.8004380314351971,
        "step": 6213
    },
    {
        "loss": 2.262,
        "grad_norm": 1.663412094116211,
        "learning_rate": 6.548555644167065e-05,
        "epoch": 0.8005668642102551,
        "step": 6214
    },
    {
        "loss": 1.8237,
        "grad_norm": 2.077658176422119,
        "learning_rate": 6.542770732873075e-05,
        "epoch": 0.8006956969853131,
        "step": 6215
    },
    {
        "loss": 2.135,
        "grad_norm": 1.7114235162734985,
        "learning_rate": 6.536983538208092e-05,
        "epoch": 0.800824529760371,
        "step": 6216
    },
    {
        "loss": 2.4162,
        "grad_norm": 1.5164130926132202,
        "learning_rate": 6.531194068737417e-05,
        "epoch": 0.800953362535429,
        "step": 6217
    },
    {
        "loss": 2.0285,
        "grad_norm": 2.084458827972412,
        "learning_rate": 6.525402333029733e-05,
        "epoch": 0.801082195310487,
        "step": 6218
    },
    {
        "loss": 0.925,
        "grad_norm": 3.392493963241577,
        "learning_rate": 6.519608339657071e-05,
        "epoch": 0.801211028085545,
        "step": 6219
    },
    {
        "loss": 1.1196,
        "grad_norm": 3.421036720275879,
        "learning_rate": 6.51381209719481e-05,
        "epoch": 0.8013398608606029,
        "step": 6220
    },
    {
        "loss": 2.0682,
        "grad_norm": 3.02138614654541,
        "learning_rate": 6.50801361422165e-05,
        "epoch": 0.8014686936356609,
        "step": 6221
    },
    {
        "loss": 1.8101,
        "grad_norm": 3.771296977996826,
        "learning_rate": 6.502212899319617e-05,
        "epoch": 0.8015975264107189,
        "step": 6222
    },
    {
        "loss": 2.0954,
        "grad_norm": 1.9006305932998657,
        "learning_rate": 6.49640996107402e-05,
        "epoch": 0.8017263591857768,
        "step": 6223
    },
    {
        "loss": 2.0334,
        "grad_norm": 2.0701587200164795,
        "learning_rate": 6.490604808073479e-05,
        "epoch": 0.8018551919608349,
        "step": 6224
    },
    {
        "loss": 1.2884,
        "grad_norm": 2.5695369243621826,
        "learning_rate": 6.484797448909886e-05,
        "epoch": 0.8019840247358928,
        "step": 6225
    },
    {
        "loss": 1.6396,
        "grad_norm": 2.6399614810943604,
        "learning_rate": 6.478987892178396e-05,
        "epoch": 0.8021128575109507,
        "step": 6226
    },
    {
        "loss": 1.2808,
        "grad_norm": 5.044217586517334,
        "learning_rate": 6.473176146477424e-05,
        "epoch": 0.8022416902860088,
        "step": 6227
    },
    {
        "loss": 2.1756,
        "grad_norm": 2.215427875518799,
        "learning_rate": 6.467362220408602e-05,
        "epoch": 0.8023705230610667,
        "step": 6228
    },
    {
        "loss": 1.79,
        "grad_norm": 2.5434608459472656,
        "learning_rate": 6.461546122576821e-05,
        "epoch": 0.8024993558361248,
        "step": 6229
    },
    {
        "loss": 2.7487,
        "grad_norm": 2.007120132446289,
        "learning_rate": 6.455727861590172e-05,
        "epoch": 0.8026281886111827,
        "step": 6230
    },
    {
        "loss": 1.909,
        "grad_norm": 1.1249706745147705,
        "learning_rate": 6.449907446059936e-05,
        "epoch": 0.8027570213862406,
        "step": 6231
    },
    {
        "loss": 1.8131,
        "grad_norm": 2.9187920093536377,
        "learning_rate": 6.444084884600595e-05,
        "epoch": 0.8028858541612987,
        "step": 6232
    },
    {
        "loss": 1.9385,
        "grad_norm": 2.340989589691162,
        "learning_rate": 6.438260185829815e-05,
        "epoch": 0.8030146869363566,
        "step": 6233
    },
    {
        "loss": 2.1329,
        "grad_norm": 2.165292978286743,
        "learning_rate": 6.432433358368403e-05,
        "epoch": 0.8031435197114146,
        "step": 6234
    },
    {
        "loss": 1.657,
        "grad_norm": 3.0037620067596436,
        "learning_rate": 6.426604410840336e-05,
        "epoch": 0.8032723524864726,
        "step": 6235
    },
    {
        "loss": 1.3821,
        "grad_norm": 1.7923493385314941,
        "learning_rate": 6.42077335187272e-05,
        "epoch": 0.8034011852615305,
        "step": 6236
    },
    {
        "loss": 2.1461,
        "grad_norm": 1.4927048683166504,
        "learning_rate": 6.414940190095788e-05,
        "epoch": 0.8035300180365885,
        "step": 6237
    },
    {
        "loss": 2.0891,
        "grad_norm": 1.591445803642273,
        "learning_rate": 6.409104934142889e-05,
        "epoch": 0.8036588508116465,
        "step": 6238
    },
    {
        "loss": 2.0764,
        "grad_norm": 3.1863763332366943,
        "learning_rate": 6.403267592650464e-05,
        "epoch": 0.8037876835867045,
        "step": 6239
    },
    {
        "loss": 1.7308,
        "grad_norm": 1.8645520210266113,
        "learning_rate": 6.397428174258045e-05,
        "epoch": 0.8039165163617624,
        "step": 6240
    },
    {
        "loss": 2.1581,
        "grad_norm": 2.0339744091033936,
        "learning_rate": 6.391586687608245e-05,
        "epoch": 0.8040453491368204,
        "step": 6241
    },
    {
        "loss": 1.6046,
        "grad_norm": 1.948264241218567,
        "learning_rate": 6.385743141346715e-05,
        "epoch": 0.8041741819118784,
        "step": 6242
    },
    {
        "loss": 2.0915,
        "grad_norm": 1.1675277948379517,
        "learning_rate": 6.37989754412218e-05,
        "epoch": 0.8043030146869363,
        "step": 6243
    },
    {
        "loss": 1.5084,
        "grad_norm": 2.358721971511841,
        "learning_rate": 6.374049904586394e-05,
        "epoch": 0.8044318474619944,
        "step": 6244
    },
    {
        "loss": 2.1614,
        "grad_norm": 2.912153959274292,
        "learning_rate": 6.368200231394112e-05,
        "epoch": 0.8045606802370523,
        "step": 6245
    },
    {
        "loss": 1.7817,
        "grad_norm": 2.5044667720794678,
        "learning_rate": 6.362348533203136e-05,
        "epoch": 0.8046895130121103,
        "step": 6246
    },
    {
        "loss": 0.7164,
        "grad_norm": 2.4924662113189697,
        "learning_rate": 6.356494818674245e-05,
        "epoch": 0.8048183457871683,
        "step": 6247
    },
    {
        "loss": 2.1763,
        "grad_norm": 1.382383942604065,
        "learning_rate": 6.350639096471192e-05,
        "epoch": 0.8049471785622262,
        "step": 6248
    },
    {
        "loss": 1.7771,
        "grad_norm": 2.536275625228882,
        "learning_rate": 6.344781375260719e-05,
        "epoch": 0.8050760113372842,
        "step": 6249
    },
    {
        "loss": 1.3642,
        "grad_norm": 2.1366395950317383,
        "learning_rate": 6.338921663712518e-05,
        "epoch": 0.8052048441123422,
        "step": 6250
    },
    {
        "loss": 1.9495,
        "grad_norm": 2.742828845977783,
        "learning_rate": 6.333059970499234e-05,
        "epoch": 0.8053336768874002,
        "step": 6251
    },
    {
        "loss": 1.1083,
        "grad_norm": 3.4525833129882812,
        "learning_rate": 6.327196304296441e-05,
        "epoch": 0.8054625096624581,
        "step": 6252
    },
    {
        "loss": 2.5742,
        "grad_norm": 1.348085880279541,
        "learning_rate": 6.321330673782624e-05,
        "epoch": 0.8055913424375161,
        "step": 6253
    },
    {
        "loss": 1.8087,
        "grad_norm": 2.438758134841919,
        "learning_rate": 6.315463087639179e-05,
        "epoch": 0.8057201752125741,
        "step": 6254
    },
    {
        "loss": 1.7352,
        "grad_norm": 3.2653422355651855,
        "learning_rate": 6.309593554550424e-05,
        "epoch": 0.805849007987632,
        "step": 6255
    },
    {
        "loss": 1.7919,
        "grad_norm": 2.895744800567627,
        "learning_rate": 6.303722083203515e-05,
        "epoch": 0.8059778407626901,
        "step": 6256
    },
    {
        "loss": 2.4878,
        "grad_norm": 1.621348261833191,
        "learning_rate": 6.297848682288503e-05,
        "epoch": 0.806106673537748,
        "step": 6257
    },
    {
        "loss": 1.699,
        "grad_norm": 3.5898492336273193,
        "learning_rate": 6.291973360498295e-05,
        "epoch": 0.8062355063128059,
        "step": 6258
    },
    {
        "loss": 1.9972,
        "grad_norm": 2.951666831970215,
        "learning_rate": 6.286096126528625e-05,
        "epoch": 0.806364339087864,
        "step": 6259
    },
    {
        "loss": 0.7929,
        "grad_norm": 3.6182749271392822,
        "learning_rate": 6.280216989078071e-05,
        "epoch": 0.8064931718629219,
        "step": 6260
    },
    {
        "loss": 2.1892,
        "grad_norm": 1.478331446647644,
        "learning_rate": 6.274335956848025e-05,
        "epoch": 0.80662200463798,
        "step": 6261
    },
    {
        "loss": 2.1783,
        "grad_norm": 2.5444693565368652,
        "learning_rate": 6.268453038542682e-05,
        "epoch": 0.8067508374130379,
        "step": 6262
    },
    {
        "loss": 2.1627,
        "grad_norm": 2.9155614376068115,
        "learning_rate": 6.262568242869032e-05,
        "epoch": 0.8068796701880958,
        "step": 6263
    },
    {
        "loss": 2.1008,
        "grad_norm": 2.249404191970825,
        "learning_rate": 6.256681578536837e-05,
        "epoch": 0.8070085029631539,
        "step": 6264
    },
    {
        "loss": 0.8448,
        "grad_norm": 3.142587423324585,
        "learning_rate": 6.250793054258631e-05,
        "epoch": 0.8071373357382118,
        "step": 6265
    },
    {
        "loss": 2.0436,
        "grad_norm": 2.776536226272583,
        "learning_rate": 6.244902678749704e-05,
        "epoch": 0.8072661685132698,
        "step": 6266
    },
    {
        "loss": 2.0255,
        "grad_norm": 2.29103422164917,
        "learning_rate": 6.239010460728066e-05,
        "epoch": 0.8073950012883278,
        "step": 6267
    },
    {
        "loss": 2.0523,
        "grad_norm": 2.9641306400299072,
        "learning_rate": 6.233116408914477e-05,
        "epoch": 0.8075238340633857,
        "step": 6268
    },
    {
        "loss": 2.2635,
        "grad_norm": 2.649625778198242,
        "learning_rate": 6.22722053203241e-05,
        "epoch": 0.8076526668384437,
        "step": 6269
    },
    {
        "loss": 2.141,
        "grad_norm": 1.6070393323898315,
        "learning_rate": 6.221322838808014e-05,
        "epoch": 0.8077814996135017,
        "step": 6270
    },
    {
        "loss": 1.9708,
        "grad_norm": 2.486034870147705,
        "learning_rate": 6.215423337970146e-05,
        "epoch": 0.8079103323885597,
        "step": 6271
    },
    {
        "loss": 0.8628,
        "grad_norm": 4.538581848144531,
        "learning_rate": 6.209522038250357e-05,
        "epoch": 0.8080391651636176,
        "step": 6272
    },
    {
        "loss": 1.73,
        "grad_norm": 1.6701234579086304,
        "learning_rate": 6.203618948382819e-05,
        "epoch": 0.8081679979386756,
        "step": 6273
    },
    {
        "loss": 2.4394,
        "grad_norm": 1.9568815231323242,
        "learning_rate": 6.19771407710438e-05,
        "epoch": 0.8082968307137336,
        "step": 6274
    },
    {
        "loss": 2.2042,
        "grad_norm": 2.492340564727783,
        "learning_rate": 6.191807433154518e-05,
        "epoch": 0.8084256634887915,
        "step": 6275
    },
    {
        "loss": 2.4672,
        "grad_norm": 2.3273842334747314,
        "learning_rate": 6.185899025275336e-05,
        "epoch": 0.8085544962638496,
        "step": 6276
    },
    {
        "loss": 1.73,
        "grad_norm": 2.6178102493286133,
        "learning_rate": 6.179988862211554e-05,
        "epoch": 0.8086833290389075,
        "step": 6277
    },
    {
        "loss": 2.101,
        "grad_norm": 1.7775299549102783,
        "learning_rate": 6.174076952710472e-05,
        "epoch": 0.8088121618139654,
        "step": 6278
    },
    {
        "loss": 2.7855,
        "grad_norm": 1.533044695854187,
        "learning_rate": 6.168163305521978e-05,
        "epoch": 0.8089409945890235,
        "step": 6279
    },
    {
        "loss": 2.2225,
        "grad_norm": 1.6345155239105225,
        "learning_rate": 6.162247929398569e-05,
        "epoch": 0.8090698273640814,
        "step": 6280
    },
    {
        "loss": 2.1755,
        "grad_norm": 1.8245340585708618,
        "learning_rate": 6.156330833095244e-05,
        "epoch": 0.8091986601391394,
        "step": 6281
    },
    {
        "loss": 1.8532,
        "grad_norm": 2.84067440032959,
        "learning_rate": 6.150412025369584e-05,
        "epoch": 0.8093274929141974,
        "step": 6282
    },
    {
        "loss": 2.3812,
        "grad_norm": 1.8408602476119995,
        "learning_rate": 6.1444915149817e-05,
        "epoch": 0.8094563256892553,
        "step": 6283
    },
    {
        "loss": 1.682,
        "grad_norm": 2.946672201156616,
        "learning_rate": 6.138569310694206e-05,
        "epoch": 0.8095851584643133,
        "step": 6284
    },
    {
        "loss": 2.0257,
        "grad_norm": 2.2637619972229004,
        "learning_rate": 6.132645421272242e-05,
        "epoch": 0.8097139912393713,
        "step": 6285
    },
    {
        "loss": 1.8289,
        "grad_norm": 2.6047794818878174,
        "learning_rate": 6.12671985548343e-05,
        "epoch": 0.8098428240144293,
        "step": 6286
    },
    {
        "loss": 1.8846,
        "grad_norm": 2.182468891143799,
        "learning_rate": 6.12079262209788e-05,
        "epoch": 0.8099716567894872,
        "step": 6287
    },
    {
        "loss": 2.0163,
        "grad_norm": 2.6259405612945557,
        "learning_rate": 6.114863729888168e-05,
        "epoch": 0.8101004895645452,
        "step": 6288
    },
    {
        "loss": 2.0015,
        "grad_norm": 1.9611026048660278,
        "learning_rate": 6.108933187629324e-05,
        "epoch": 0.8102293223396032,
        "step": 6289
    },
    {
        "loss": 2.1003,
        "grad_norm": 1.6455479860305786,
        "learning_rate": 6.1030010040988186e-05,
        "epoch": 0.8103581551146611,
        "step": 6290
    },
    {
        "loss": 2.353,
        "grad_norm": 1.542396903038025,
        "learning_rate": 6.097067188076564e-05,
        "epoch": 0.8104869878897192,
        "step": 6291
    },
    {
        "loss": 1.1557,
        "grad_norm": 2.999673366546631,
        "learning_rate": 6.0911317483448636e-05,
        "epoch": 0.8106158206647771,
        "step": 6292
    },
    {
        "loss": 1.8605,
        "grad_norm": 2.590588331222534,
        "learning_rate": 6.085194693688445e-05,
        "epoch": 0.810744653439835,
        "step": 6293
    },
    {
        "loss": 2.1095,
        "grad_norm": 1.60880708694458,
        "learning_rate": 6.079256032894428e-05,
        "epoch": 0.8108734862148931,
        "step": 6294
    },
    {
        "loss": 2.3827,
        "grad_norm": 1.2440309524536133,
        "learning_rate": 6.0733157747522865e-05,
        "epoch": 0.811002318989951,
        "step": 6295
    },
    {
        "loss": 2.0581,
        "grad_norm": 1.7574958801269531,
        "learning_rate": 6.067373928053871e-05,
        "epoch": 0.811131151765009,
        "step": 6296
    },
    {
        "loss": 2.0602,
        "grad_norm": 2.3667774200439453,
        "learning_rate": 6.061430501593411e-05,
        "epoch": 0.811259984540067,
        "step": 6297
    },
    {
        "loss": 2.2189,
        "grad_norm": 1.6870561838150024,
        "learning_rate": 6.055485504167424e-05,
        "epoch": 0.811388817315125,
        "step": 6298
    },
    {
        "loss": 1.9559,
        "grad_norm": 2.120490550994873,
        "learning_rate": 6.049538944574785e-05,
        "epoch": 0.811517650090183,
        "step": 6299
    },
    {
        "loss": 2.2911,
        "grad_norm": 2.4415040016174316,
        "learning_rate": 6.043590831616675e-05,
        "epoch": 0.8116464828652409,
        "step": 6300
    },
    {
        "loss": 1.5979,
        "grad_norm": 2.8289055824279785,
        "learning_rate": 6.037641174096569e-05,
        "epoch": 0.8117753156402989,
        "step": 6301
    },
    {
        "loss": 1.7298,
        "grad_norm": 2.065025568008423,
        "learning_rate": 6.03168998082024e-05,
        "epoch": 0.8119041484153569,
        "step": 6302
    },
    {
        "loss": 1.8775,
        "grad_norm": 2.833428382873535,
        "learning_rate": 6.025737260595713e-05,
        "epoch": 0.8120329811904149,
        "step": 6303
    },
    {
        "loss": 1.942,
        "grad_norm": 2.721353054046631,
        "learning_rate": 6.01978302223329e-05,
        "epoch": 0.8121618139654728,
        "step": 6304
    },
    {
        "loss": 1.315,
        "grad_norm": 3.1175973415374756,
        "learning_rate": 6.013827274545514e-05,
        "epoch": 0.8122906467405308,
        "step": 6305
    },
    {
        "loss": 1.9883,
        "grad_norm": 1.8749116659164429,
        "learning_rate": 6.0078700263471656e-05,
        "epoch": 0.8124194795155888,
        "step": 6306
    },
    {
        "loss": 1.8196,
        "grad_norm": 3.1570870876312256,
        "learning_rate": 6.001911286455243e-05,
        "epoch": 0.8125483122906467,
        "step": 6307
    },
    {
        "loss": 2.0668,
        "grad_norm": 2.1972365379333496,
        "learning_rate": 5.995951063688959e-05,
        "epoch": 0.8126771450657048,
        "step": 6308
    },
    {
        "loss": 2.3444,
        "grad_norm": 1.5335453748703003,
        "learning_rate": 5.9899893668697024e-05,
        "epoch": 0.8128059778407627,
        "step": 6309
    },
    {
        "loss": 1.9216,
        "grad_norm": 1.8510733842849731,
        "learning_rate": 5.984026204821063e-05,
        "epoch": 0.8129348106158206,
        "step": 6310
    },
    {
        "loss": 1.4482,
        "grad_norm": 3.4001426696777344,
        "learning_rate": 5.978061586368791e-05,
        "epoch": 0.8130636433908787,
        "step": 6311
    },
    {
        "loss": 2.4242,
        "grad_norm": 2.114061117172241,
        "learning_rate": 5.9720955203407945e-05,
        "epoch": 0.8131924761659366,
        "step": 6312
    },
    {
        "loss": 1.7013,
        "grad_norm": 2.4562463760375977,
        "learning_rate": 5.9661280155671296e-05,
        "epoch": 0.8133213089409946,
        "step": 6313
    },
    {
        "loss": 1.8473,
        "grad_norm": 1.8872379064559937,
        "learning_rate": 5.960159080879955e-05,
        "epoch": 0.8134501417160526,
        "step": 6314
    },
    {
        "loss": 2.4941,
        "grad_norm": 1.3090494871139526,
        "learning_rate": 5.954188725113586e-05,
        "epoch": 0.8135789744911105,
        "step": 6315
    },
    {
        "loss": 1.0374,
        "grad_norm": 3.5201354026794434,
        "learning_rate": 5.948216957104422e-05,
        "epoch": 0.8137078072661685,
        "step": 6316
    },
    {
        "loss": 1.7183,
        "grad_norm": 2.2365036010742188,
        "learning_rate": 5.942243785690939e-05,
        "epoch": 0.8138366400412265,
        "step": 6317
    },
    {
        "loss": 1.0188,
        "grad_norm": Infinity,
        "learning_rate": 5.942243785690939e-05,
        "epoch": 0.8139654728162845,
        "step": 6318
    },
    {
        "loss": 1.4012,
        "grad_norm": 3.3168559074401855,
        "learning_rate": 5.936269219713707e-05,
        "epoch": 0.8140943055913424,
        "step": 6319
    },
    {
        "loss": 1.6602,
        "grad_norm": 2.819171190261841,
        "learning_rate": 5.9302932680153665e-05,
        "epoch": 0.8142231383664004,
        "step": 6320
    },
    {
        "loss": 2.3638,
        "grad_norm": 2.5206570625305176,
        "learning_rate": 5.9243159394405825e-05,
        "epoch": 0.8143519711414584,
        "step": 6321
    },
    {
        "loss": 2.0569,
        "grad_norm": 2.5446152687072754,
        "learning_rate": 5.918337242836081e-05,
        "epoch": 0.8144808039165163,
        "step": 6322
    },
    {
        "loss": 2.1674,
        "grad_norm": 2.3544650077819824,
        "learning_rate": 5.912357187050603e-05,
        "epoch": 0.8146096366915744,
        "step": 6323
    },
    {
        "loss": 2.1239,
        "grad_norm": 2.553659200668335,
        "learning_rate": 5.906375780934907e-05,
        "epoch": 0.8147384694666323,
        "step": 6324
    },
    {
        "loss": 2.2783,
        "grad_norm": 2.0340137481689453,
        "learning_rate": 5.9003930333417465e-05,
        "epoch": 0.8148673022416902,
        "step": 6325
    },
    {
        "loss": 1.2471,
        "grad_norm": 3.044443368911743,
        "learning_rate": 5.89440895312586e-05,
        "epoch": 0.8149961350167483,
        "step": 6326
    },
    {
        "loss": 1.3879,
        "grad_norm": 2.245145797729492,
        "learning_rate": 5.8884235491439575e-05,
        "epoch": 0.8151249677918062,
        "step": 6327
    },
    {
        "loss": 2.1495,
        "grad_norm": 1.5121381282806396,
        "learning_rate": 5.882436830254717e-05,
        "epoch": 0.8152538005668643,
        "step": 6328
    },
    {
        "loss": 1.7682,
        "grad_norm": 2.70322847366333,
        "learning_rate": 5.876448805318745e-05,
        "epoch": 0.8153826333419222,
        "step": 6329
    },
    {
        "loss": 2.4273,
        "grad_norm": 1.9849269390106201,
        "learning_rate": 5.870459483198594e-05,
        "epoch": 0.8155114661169801,
        "step": 6330
    },
    {
        "loss": 2.0428,
        "grad_norm": 2.847418785095215,
        "learning_rate": 5.86446887275874e-05,
        "epoch": 0.8156402988920382,
        "step": 6331
    },
    {
        "loss": 1.3607,
        "grad_norm": 3.887697219848633,
        "learning_rate": 5.858476982865544e-05,
        "epoch": 0.8157691316670961,
        "step": 6332
    },
    {
        "loss": 2.0504,
        "grad_norm": 1.7163817882537842,
        "learning_rate": 5.852483822387291e-05,
        "epoch": 0.8158979644421541,
        "step": 6333
    },
    {
        "loss": 1.6114,
        "grad_norm": 3.7950527667999268,
        "learning_rate": 5.846489400194136e-05,
        "epoch": 0.816026797217212,
        "step": 6334
    },
    {
        "loss": 1.4704,
        "grad_norm": 3.172415018081665,
        "learning_rate": 5.8404937251580815e-05,
        "epoch": 0.81615562999227,
        "step": 6335
    },
    {
        "loss": 2.3471,
        "grad_norm": 1.887261152267456,
        "learning_rate": 5.8344968061530056e-05,
        "epoch": 0.816284462767328,
        "step": 6336
    },
    {
        "loss": 1.8674,
        "grad_norm": 2.253434181213379,
        "learning_rate": 5.828498652054625e-05,
        "epoch": 0.816413295542386,
        "step": 6337
    },
    {
        "loss": 2.3507,
        "grad_norm": 1.73883056640625,
        "learning_rate": 5.822499271740478e-05,
        "epoch": 0.816542128317444,
        "step": 6338
    },
    {
        "loss": 2.3342,
        "grad_norm": 2.1243135929107666,
        "learning_rate": 5.816498674089928e-05,
        "epoch": 0.8166709610925019,
        "step": 6339
    },
    {
        "loss": 2.5002,
        "grad_norm": 1.6288810968399048,
        "learning_rate": 5.810496867984115e-05,
        "epoch": 0.8167997938675599,
        "step": 6340
    },
    {
        "loss": 2.0017,
        "grad_norm": 2.030499219894409,
        "learning_rate": 5.804493862306004e-05,
        "epoch": 0.8169286266426179,
        "step": 6341
    },
    {
        "loss": 2.1565,
        "grad_norm": 2.4392359256744385,
        "learning_rate": 5.7984896659403166e-05,
        "epoch": 0.8170574594176758,
        "step": 6342
    },
    {
        "loss": 1.7986,
        "grad_norm": 2.6375346183776855,
        "learning_rate": 5.792484287773524e-05,
        "epoch": 0.8171862921927339,
        "step": 6343
    },
    {
        "loss": 2.2224,
        "grad_norm": 2.1268179416656494,
        "learning_rate": 5.786477736693867e-05,
        "epoch": 0.8173151249677918,
        "step": 6344
    },
    {
        "loss": 2.2799,
        "grad_norm": 1.6623488664627075,
        "learning_rate": 5.7804700215913224e-05,
        "epoch": 0.8174439577428498,
        "step": 6345
    },
    {
        "loss": 2.1083,
        "grad_norm": 1.9355030059814453,
        "learning_rate": 5.7744611513575666e-05,
        "epoch": 0.8175727905179078,
        "step": 6346
    },
    {
        "loss": 2.0993,
        "grad_norm": 1.8526434898376465,
        "learning_rate": 5.7684511348860104e-05,
        "epoch": 0.8177016232929657,
        "step": 6347
    },
    {
        "loss": 1.7917,
        "grad_norm": 4.152881145477295,
        "learning_rate": 5.76243998107175e-05,
        "epoch": 0.8178304560680237,
        "step": 6348
    },
    {
        "loss": 1.6877,
        "grad_norm": 2.8363234996795654,
        "learning_rate": 5.7564276988115685e-05,
        "epoch": 0.8179592888430817,
        "step": 6349
    },
    {
        "loss": 2.2942,
        "grad_norm": 2.1993558406829834,
        "learning_rate": 5.7504142970039165e-05,
        "epoch": 0.8180881216181397,
        "step": 6350
    },
    {
        "loss": 1.6291,
        "grad_norm": 2.2655422687530518,
        "learning_rate": 5.744399784548905e-05,
        "epoch": 0.8182169543931976,
        "step": 6351
    },
    {
        "loss": 2.3153,
        "grad_norm": 2.307938814163208,
        "learning_rate": 5.7383841703482835e-05,
        "epoch": 0.8183457871682556,
        "step": 6352
    },
    {
        "loss": 1.8155,
        "grad_norm": 2.424607753753662,
        "learning_rate": 5.7323674633054444e-05,
        "epoch": 0.8184746199433136,
        "step": 6353
    },
    {
        "loss": 1.9301,
        "grad_norm": 2.5044848918914795,
        "learning_rate": 5.726349672325375e-05,
        "epoch": 0.8186034527183715,
        "step": 6354
    },
    {
        "loss": 1.6457,
        "grad_norm": 2.442272186279297,
        "learning_rate": 5.7203308063146845e-05,
        "epoch": 0.8187322854934296,
        "step": 6355
    },
    {
        "loss": 1.8094,
        "grad_norm": 1.941394567489624,
        "learning_rate": 5.714310874181577e-05,
        "epoch": 0.8188611182684875,
        "step": 6356
    },
    {
        "loss": 2.3807,
        "grad_norm": 2.1368935108184814,
        "learning_rate": 5.708289884835807e-05,
        "epoch": 0.8189899510435454,
        "step": 6357
    },
    {
        "loss": 2.1056,
        "grad_norm": 2.305450201034546,
        "learning_rate": 5.702267847188731e-05,
        "epoch": 0.8191187838186035,
        "step": 6358
    },
    {
        "loss": 2.0517,
        "grad_norm": 2.4456114768981934,
        "learning_rate": 5.696244770153243e-05,
        "epoch": 0.8192476165936614,
        "step": 6359
    },
    {
        "loss": 2.4844,
        "grad_norm": 1.5783281326293945,
        "learning_rate": 5.6902206626437536e-05,
        "epoch": 0.8193764493687195,
        "step": 6360
    },
    {
        "loss": 1.7068,
        "grad_norm": 4.738074779510498,
        "learning_rate": 5.6841955335762274e-05,
        "epoch": 0.8195052821437774,
        "step": 6361
    },
    {
        "loss": 2.1493,
        "grad_norm": 2.197481870651245,
        "learning_rate": 5.678169391868127e-05,
        "epoch": 0.8196341149188353,
        "step": 6362
    },
    {
        "loss": 2.1354,
        "grad_norm": 2.0979855060577393,
        "learning_rate": 5.672142246438419e-05,
        "epoch": 0.8197629476938934,
        "step": 6363
    },
    {
        "loss": 1.5811,
        "grad_norm": 3.851517677307129,
        "learning_rate": 5.6661141062075586e-05,
        "epoch": 0.8198917804689513,
        "step": 6364
    },
    {
        "loss": 2.4599,
        "grad_norm": 2.8324310779571533,
        "learning_rate": 5.6600849800974544e-05,
        "epoch": 0.8200206132440093,
        "step": 6365
    },
    {
        "loss": 1.9931,
        "grad_norm": 2.209406852722168,
        "learning_rate": 5.6540548770314886e-05,
        "epoch": 0.8201494460190673,
        "step": 6366
    },
    {
        "loss": 1.7143,
        "grad_norm": 1.9860221147537231,
        "learning_rate": 5.648023805934508e-05,
        "epoch": 0.8202782787941252,
        "step": 6367
    },
    {
        "loss": 2.1857,
        "grad_norm": 2.6653995513916016,
        "learning_rate": 5.641991775732754e-05,
        "epoch": 0.8204071115691832,
        "step": 6368
    },
    {
        "loss": 2.2191,
        "grad_norm": 1.9072980880737305,
        "learning_rate": 5.635958795353912e-05,
        "epoch": 0.8205359443442412,
        "step": 6369
    },
    {
        "loss": 1.7128,
        "grad_norm": 2.442101240158081,
        "learning_rate": 5.629924873727075e-05,
        "epoch": 0.8206647771192992,
        "step": 6370
    },
    {
        "loss": 2.1097,
        "grad_norm": 1.1564968824386597,
        "learning_rate": 5.6238900197827074e-05,
        "epoch": 0.8207936098943571,
        "step": 6371
    },
    {
        "loss": 1.9597,
        "grad_norm": 1.4126615524291992,
        "learning_rate": 5.617854242452676e-05,
        "epoch": 0.820922442669415,
        "step": 6372
    },
    {
        "loss": 1.2451,
        "grad_norm": 2.815305709838867,
        "learning_rate": 5.6118175506702054e-05,
        "epoch": 0.8210512754444731,
        "step": 6373
    },
    {
        "loss": 1.6831,
        "grad_norm": 2.627072811126709,
        "learning_rate": 5.605779953369875e-05,
        "epoch": 0.821180108219531,
        "step": 6374
    },
    {
        "loss": 0.781,
        "grad_norm": 3.080446243286133,
        "learning_rate": 5.5997414594876055e-05,
        "epoch": 0.8213089409945891,
        "step": 6375
    },
    {
        "loss": 2.018,
        "grad_norm": 2.836665630340576,
        "learning_rate": 5.5937020779606405e-05,
        "epoch": 0.821437773769647,
        "step": 6376
    },
    {
        "loss": 2.0703,
        "grad_norm": 1.45601487159729,
        "learning_rate": 5.587661817727542e-05,
        "epoch": 0.8215666065447049,
        "step": 6377
    },
    {
        "loss": 1.7329,
        "grad_norm": 2.0905377864837646,
        "learning_rate": 5.581620687728175e-05,
        "epoch": 0.821695439319763,
        "step": 6378
    },
    {
        "loss": 1.7898,
        "grad_norm": 2.2021729946136475,
        "learning_rate": 5.5755786969036775e-05,
        "epoch": 0.8218242720948209,
        "step": 6379
    },
    {
        "loss": 1.6546,
        "grad_norm": 2.172260284423828,
        "learning_rate": 5.5695358541964735e-05,
        "epoch": 0.8219531048698789,
        "step": 6380
    },
    {
        "loss": 1.7187,
        "grad_norm": 3.3888983726501465,
        "learning_rate": 5.563492168550255e-05,
        "epoch": 0.8220819376449369,
        "step": 6381
    },
    {
        "loss": 2.2334,
        "grad_norm": 1.7669390439987183,
        "learning_rate": 5.557447648909938e-05,
        "epoch": 0.8222107704199948,
        "step": 6382
    },
    {
        "loss": 1.4311,
        "grad_norm": 2.9386229515075684,
        "learning_rate": 5.551402304221687e-05,
        "epoch": 0.8223396031950528,
        "step": 6383
    },
    {
        "loss": 1.9397,
        "grad_norm": 2.8101019859313965,
        "learning_rate": 5.545356143432908e-05,
        "epoch": 0.8224684359701108,
        "step": 6384
    },
    {
        "loss": 1.7979,
        "grad_norm": 3.951986074447632,
        "learning_rate": 5.5393091754921745e-05,
        "epoch": 0.8225972687451688,
        "step": 6385
    },
    {
        "loss": 2.0308,
        "grad_norm": 1.8228672742843628,
        "learning_rate": 5.533261409349282e-05,
        "epoch": 0.8227261015202267,
        "step": 6386
    },
    {
        "loss": 2.1775,
        "grad_norm": 2.4219322204589844,
        "learning_rate": 5.527212853955199e-05,
        "epoch": 0.8228549342952847,
        "step": 6387
    },
    {
        "loss": 1.9791,
        "grad_norm": 2.5375261306762695,
        "learning_rate": 5.521163518262065e-05,
        "epoch": 0.8229837670703427,
        "step": 6388
    },
    {
        "loss": 2.086,
        "grad_norm": 2.0886878967285156,
        "learning_rate": 5.515113411223175e-05,
        "epoch": 0.8231125998454006,
        "step": 6389
    },
    {
        "loss": 1.6881,
        "grad_norm": 2.0023980140686035,
        "learning_rate": 5.5090625417929545e-05,
        "epoch": 0.8232414326204587,
        "step": 6390
    },
    {
        "loss": 2.2926,
        "grad_norm": 3.25777006149292,
        "learning_rate": 5.503010918926969e-05,
        "epoch": 0.8233702653955166,
        "step": 6391
    },
    {
        "loss": 2.166,
        "grad_norm": 1.2245579957962036,
        "learning_rate": 5.496958551581899e-05,
        "epoch": 0.8234990981705745,
        "step": 6392
    },
    {
        "loss": 1.62,
        "grad_norm": 4.1702494621276855,
        "learning_rate": 5.490905448715523e-05,
        "epoch": 0.8236279309456326,
        "step": 6393
    },
    {
        "loss": 1.4422,
        "grad_norm": 2.8447940349578857,
        "learning_rate": 5.4848516192867083e-05,
        "epoch": 0.8237567637206905,
        "step": 6394
    },
    {
        "loss": 1.462,
        "grad_norm": 4.688989639282227,
        "learning_rate": 5.478797072255407e-05,
        "epoch": 0.8238855964957486,
        "step": 6395
    },
    {
        "loss": 1.9715,
        "grad_norm": 1.8168128728866577,
        "learning_rate": 5.4727418165826096e-05,
        "epoch": 0.8240144292708065,
        "step": 6396
    },
    {
        "loss": 2.1361,
        "grad_norm": 2.135773181915283,
        "learning_rate": 5.4666858612303775e-05,
        "epoch": 0.8241432620458645,
        "step": 6397
    },
    {
        "loss": 0.7766,
        "grad_norm": 3.280000686645508,
        "learning_rate": 5.4606292151618e-05,
        "epoch": 0.8242720948209225,
        "step": 6398
    },
    {
        "loss": 2.2603,
        "grad_norm": 3.613173723220825,
        "learning_rate": 5.454571887340991e-05,
        "epoch": 0.8244009275959804,
        "step": 6399
    },
    {
        "loss": 1.5303,
        "grad_norm": 2.913278579711914,
        "learning_rate": 5.44851388673307e-05,
        "epoch": 0.8245297603710384,
        "step": 6400
    },
    {
        "loss": 1.7192,
        "grad_norm": 1.8126660585403442,
        "learning_rate": 5.442455222304156e-05,
        "epoch": 0.8246585931460964,
        "step": 6401
    },
    {
        "loss": 2.1254,
        "grad_norm": 2.0673294067382812,
        "learning_rate": 5.436395903021346e-05,
        "epoch": 0.8247874259211544,
        "step": 6402
    },
    {
        "loss": 2.0844,
        "grad_norm": 1.8538262844085693,
        "learning_rate": 5.430335937852717e-05,
        "epoch": 0.8249162586962123,
        "step": 6403
    },
    {
        "loss": 1.9084,
        "grad_norm": 1.7893160581588745,
        "learning_rate": 5.4242753357672815e-05,
        "epoch": 0.8250450914712703,
        "step": 6404
    },
    {
        "loss": 2.2333,
        "grad_norm": 2.679903984069824,
        "learning_rate": 5.418214105735011e-05,
        "epoch": 0.8251739242463283,
        "step": 6405
    },
    {
        "loss": 1.7662,
        "grad_norm": 3.4877283573150635,
        "learning_rate": 5.412152256726809e-05,
        "epoch": 0.8253027570213862,
        "step": 6406
    },
    {
        "loss": 1.9948,
        "grad_norm": 1.6346375942230225,
        "learning_rate": 5.4060897977144765e-05,
        "epoch": 0.8254315897964443,
        "step": 6407
    },
    {
        "loss": 1.7726,
        "grad_norm": 2.1449403762817383,
        "learning_rate": 5.400026737670736e-05,
        "epoch": 0.8255604225715022,
        "step": 6408
    },
    {
        "loss": 2.0208,
        "grad_norm": 1.5324835777282715,
        "learning_rate": 5.3939630855691904e-05,
        "epoch": 0.8256892553465601,
        "step": 6409
    },
    {
        "loss": 1.2828,
        "grad_norm": 2.7805488109588623,
        "learning_rate": 5.387898850384323e-05,
        "epoch": 0.8258180881216182,
        "step": 6410
    },
    {
        "loss": 2.0149,
        "grad_norm": 2.442988872528076,
        "learning_rate": 5.381834041091478e-05,
        "epoch": 0.8259469208966761,
        "step": 6411
    },
    {
        "loss": 2.5436,
        "grad_norm": 1.3985371589660645,
        "learning_rate": 5.375768666666849e-05,
        "epoch": 0.8260757536717341,
        "step": 6412
    },
    {
        "loss": 2.5571,
        "grad_norm": 1.9256349802017212,
        "learning_rate": 5.3697027360874686e-05,
        "epoch": 0.8262045864467921,
        "step": 6413
    },
    {
        "loss": 2.232,
        "grad_norm": 2.6801211833953857,
        "learning_rate": 5.363636258331195e-05,
        "epoch": 0.82633341922185,
        "step": 6414
    },
    {
        "loss": 1.7484,
        "grad_norm": 3.0203843116760254,
        "learning_rate": 5.3575692423766776e-05,
        "epoch": 0.826462251996908,
        "step": 6415
    },
    {
        "loss": 2.1205,
        "grad_norm": 2.541246175765991,
        "learning_rate": 5.351501697203386e-05,
        "epoch": 0.826591084771966,
        "step": 6416
    },
    {
        "loss": 1.5828,
        "grad_norm": 2.504086971282959,
        "learning_rate": 5.345433631791562e-05,
        "epoch": 0.826719917547024,
        "step": 6417
    },
    {
        "loss": 1.8602,
        "grad_norm": 2.194563627243042,
        "learning_rate": 5.339365055122219e-05,
        "epoch": 0.8268487503220819,
        "step": 6418
    },
    {
        "loss": 1.2627,
        "grad_norm": 3.5811524391174316,
        "learning_rate": 5.3332959761771275e-05,
        "epoch": 0.8269775830971399,
        "step": 6419
    },
    {
        "loss": 2.2039,
        "grad_norm": 1.869355320930481,
        "learning_rate": 5.327226403938808e-05,
        "epoch": 0.8271064158721979,
        "step": 6420
    },
    {
        "loss": 1.3819,
        "grad_norm": 2.060434103012085,
        "learning_rate": 5.321156347390489e-05,
        "epoch": 0.8272352486472558,
        "step": 6421
    },
    {
        "loss": 1.9451,
        "grad_norm": 2.2153687477111816,
        "learning_rate": 5.3150858155161384e-05,
        "epoch": 0.8273640814223139,
        "step": 6422
    },
    {
        "loss": 2.5667,
        "grad_norm": 1.953220009803772,
        "learning_rate": 5.309014817300421e-05,
        "epoch": 0.8274929141973718,
        "step": 6423
    },
    {
        "loss": 2.1079,
        "grad_norm": 1.8257163763046265,
        "learning_rate": 5.3029433617286896e-05,
        "epoch": 0.8276217469724297,
        "step": 6424
    },
    {
        "loss": 2.1245,
        "grad_norm": 1.8555128574371338,
        "learning_rate": 5.296871457786983e-05,
        "epoch": 0.8277505797474878,
        "step": 6425
    },
    {
        "loss": 2.3757,
        "grad_norm": 1.3530396223068237,
        "learning_rate": 5.290799114461974e-05,
        "epoch": 0.8278794125225457,
        "step": 6426
    },
    {
        "loss": 2.2227,
        "grad_norm": 1.8125964403152466,
        "learning_rate": 5.284726340741029e-05,
        "epoch": 0.8280082452976038,
        "step": 6427
    },
    {
        "loss": 1.4693,
        "grad_norm": 3.620797634124756,
        "learning_rate": 5.278653145612129e-05,
        "epoch": 0.8281370780726617,
        "step": 6428
    },
    {
        "loss": 2.1933,
        "grad_norm": 1.8982435464859009,
        "learning_rate": 5.2725795380638686e-05,
        "epoch": 0.8282659108477196,
        "step": 6429
    },
    {
        "loss": 2.056,
        "grad_norm": 1.4196014404296875,
        "learning_rate": 5.266505527085468e-05,
        "epoch": 0.8283947436227777,
        "step": 6430
    },
    {
        "loss": 1.4341,
        "grad_norm": 2.32381272315979,
        "learning_rate": 5.260431121666747e-05,
        "epoch": 0.8285235763978356,
        "step": 6431
    },
    {
        "loss": 2.1699,
        "grad_norm": 2.0846893787384033,
        "learning_rate": 5.254356330798089e-05,
        "epoch": 0.8286524091728936,
        "step": 6432
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.402723550796509,
        "learning_rate": 5.24828116347047e-05,
        "epoch": 0.8287812419479516,
        "step": 6433
    },
    {
        "loss": 1.8213,
        "grad_norm": 3.0353071689605713,
        "learning_rate": 5.2422056286754114e-05,
        "epoch": 0.8289100747230095,
        "step": 6434
    },
    {
        "loss": 1.8732,
        "grad_norm": 3.047116994857788,
        "learning_rate": 5.236129735404985e-05,
        "epoch": 0.8290389074980675,
        "step": 6435
    },
    {
        "loss": 1.3424,
        "grad_norm": 3.5188443660736084,
        "learning_rate": 5.230053492651787e-05,
        "epoch": 0.8291677402731255,
        "step": 6436
    },
    {
        "loss": 2.2823,
        "grad_norm": 2.1244215965270996,
        "learning_rate": 5.2239769094089364e-05,
        "epoch": 0.8292965730481835,
        "step": 6437
    },
    {
        "loss": 1.6796,
        "grad_norm": 3.0832579135894775,
        "learning_rate": 5.2178999946700516e-05,
        "epoch": 0.8294254058232414,
        "step": 6438
    },
    {
        "loss": 1.6274,
        "grad_norm": 2.2329609394073486,
        "learning_rate": 5.211822757429249e-05,
        "epoch": 0.8295542385982994,
        "step": 6439
    },
    {
        "loss": 1.9318,
        "grad_norm": 2.3641788959503174,
        "learning_rate": 5.2057452066811055e-05,
        "epoch": 0.8296830713733574,
        "step": 6440
    },
    {
        "loss": 1.2848,
        "grad_norm": 2.6808557510375977,
        "learning_rate": 5.1996673514206776e-05,
        "epoch": 0.8298119041484153,
        "step": 6441
    },
    {
        "loss": 2.2541,
        "grad_norm": 1.6934771537780762,
        "learning_rate": 5.193589200643477e-05,
        "epoch": 0.8299407369234734,
        "step": 6442
    },
    {
        "loss": 2.2728,
        "grad_norm": 1.9426124095916748,
        "learning_rate": 5.187510763345422e-05,
        "epoch": 0.8300695696985313,
        "step": 6443
    },
    {
        "loss": 2.0438,
        "grad_norm": 7.437810897827148,
        "learning_rate": 5.1814320485228995e-05,
        "epoch": 0.8301984024735893,
        "step": 6444
    },
    {
        "loss": 1.2932,
        "grad_norm": 2.482949733734131,
        "learning_rate": 5.175353065172682e-05,
        "epoch": 0.8303272352486473,
        "step": 6445
    },
    {
        "loss": 2.5457,
        "grad_norm": 1.8077442646026611,
        "learning_rate": 5.16927382229193e-05,
        "epoch": 0.8304560680237052,
        "step": 6446
    },
    {
        "loss": 2.4344,
        "grad_norm": 2.6864819526672363,
        "learning_rate": 5.163194328878205e-05,
        "epoch": 0.8305849007987632,
        "step": 6447
    },
    {
        "loss": 1.181,
        "grad_norm": 2.6807045936584473,
        "learning_rate": 5.157114593929435e-05,
        "epoch": 0.8307137335738212,
        "step": 6448
    },
    {
        "loss": 1.001,
        "grad_norm": 5.447211742401123,
        "learning_rate": 5.151034626443906e-05,
        "epoch": 0.8308425663488792,
        "step": 6449
    },
    {
        "loss": 1.9512,
        "grad_norm": 2.4186856746673584,
        "learning_rate": 5.144954435420253e-05,
        "epoch": 0.8309713991239371,
        "step": 6450
    },
    {
        "loss": 2.0004,
        "grad_norm": 3.097052574157715,
        "learning_rate": 5.138874029857422e-05,
        "epoch": 0.8311002318989951,
        "step": 6451
    },
    {
        "loss": 1.8124,
        "grad_norm": 1.718656063079834,
        "learning_rate": 5.132793418754691e-05,
        "epoch": 0.8312290646740531,
        "step": 6452
    },
    {
        "loss": 2.6785,
        "grad_norm": 2.1782567501068115,
        "learning_rate": 5.12671261111166e-05,
        "epoch": 0.831357897449111,
        "step": 6453
    },
    {
        "loss": 1.8247,
        "grad_norm": 3.23494553565979,
        "learning_rate": 5.120631615928185e-05,
        "epoch": 0.8314867302241691,
        "step": 6454
    },
    {
        "loss": 2.0339,
        "grad_norm": 2.3500139713287354,
        "learning_rate": 5.11455044220442e-05,
        "epoch": 0.831615562999227,
        "step": 6455
    },
    {
        "loss": 1.8702,
        "grad_norm": 2.0465331077575684,
        "learning_rate": 5.108469098940786e-05,
        "epoch": 0.8317443957742849,
        "step": 6456
    },
    {
        "loss": 1.8524,
        "grad_norm": 3.2839157581329346,
        "learning_rate": 5.102387595137934e-05,
        "epoch": 0.831873228549343,
        "step": 6457
    },
    {
        "loss": 2.1505,
        "grad_norm": 1.5461844205856323,
        "learning_rate": 5.096305939796776e-05,
        "epoch": 0.8320020613244009,
        "step": 6458
    },
    {
        "loss": 1.3631,
        "grad_norm": 2.3264284133911133,
        "learning_rate": 5.090224141918437e-05,
        "epoch": 0.832130894099459,
        "step": 6459
    },
    {
        "loss": 1.9647,
        "grad_norm": 1.8472704887390137,
        "learning_rate": 5.084142210504256e-05,
        "epoch": 0.8322597268745169,
        "step": 6460
    },
    {
        "loss": 2.0057,
        "grad_norm": 2.502037286758423,
        "learning_rate": 5.078060154555768e-05,
        "epoch": 0.8323885596495748,
        "step": 6461
    },
    {
        "loss": 1.8725,
        "grad_norm": 2.6087090969085693,
        "learning_rate": 5.071977983074695e-05,
        "epoch": 0.8325173924246329,
        "step": 6462
    },
    {
        "loss": 1.9716,
        "grad_norm": 2.0048322677612305,
        "learning_rate": 5.0658957050629266e-05,
        "epoch": 0.8326462251996908,
        "step": 6463
    },
    {
        "loss": 2.2072,
        "grad_norm": 2.841679334640503,
        "learning_rate": 5.059813329522517e-05,
        "epoch": 0.8327750579747488,
        "step": 6464
    },
    {
        "loss": 2.4961,
        "grad_norm": 1.95627760887146,
        "learning_rate": 5.053730865455647e-05,
        "epoch": 0.8329038907498068,
        "step": 6465
    },
    {
        "loss": 2.3575,
        "grad_norm": 1.7368857860565186,
        "learning_rate": 5.0476483218646475e-05,
        "epoch": 0.8330327235248647,
        "step": 6466
    },
    {
        "loss": 2.3316,
        "grad_norm": 2.700239896774292,
        "learning_rate": 5.0415657077519664e-05,
        "epoch": 0.8331615562999227,
        "step": 6467
    },
    {
        "loss": 2.0982,
        "grad_norm": 2.712174415588379,
        "learning_rate": 5.0354830321201375e-05,
        "epoch": 0.8332903890749807,
        "step": 6468
    },
    {
        "loss": 2.1862,
        "grad_norm": 1.9941850900650024,
        "learning_rate": 5.029400303971796e-05,
        "epoch": 0.8334192218500387,
        "step": 6469
    },
    {
        "loss": 1.7267,
        "grad_norm": 2.706049680709839,
        "learning_rate": 5.0233175323096805e-05,
        "epoch": 0.8335480546250966,
        "step": 6470
    },
    {
        "loss": 2.2247,
        "grad_norm": 1.3150666952133179,
        "learning_rate": 5.017234726136545e-05,
        "epoch": 0.8336768874001546,
        "step": 6471
    },
    {
        "loss": 1.6922,
        "grad_norm": 2.992685317993164,
        "learning_rate": 5.01115189445523e-05,
        "epoch": 0.8338057201752126,
        "step": 6472
    },
    {
        "loss": 1.5525,
        "grad_norm": 3.178161144256592,
        "learning_rate": 5.005069046268603e-05,
        "epoch": 0.8339345529502705,
        "step": 6473
    },
    {
        "loss": 1.6351,
        "grad_norm": 3.0589754581451416,
        "learning_rate": 4.9989861905795574e-05,
        "epoch": 0.8340633857253286,
        "step": 6474
    },
    {
        "loss": 1.3369,
        "grad_norm": 3.000521183013916,
        "learning_rate": 4.992903336391001e-05,
        "epoch": 0.8341922185003865,
        "step": 6475
    },
    {
        "loss": 2.1508,
        "grad_norm": 1.6165765523910522,
        "learning_rate": 4.9868204927058244e-05,
        "epoch": 0.8343210512754444,
        "step": 6476
    },
    {
        "loss": 2.1292,
        "grad_norm": 1.9087800979614258,
        "learning_rate": 4.9807376685269124e-05,
        "epoch": 0.8344498840505025,
        "step": 6477
    },
    {
        "loss": 2.1149,
        "grad_norm": 1.9411466121673584,
        "learning_rate": 4.974654872857143e-05,
        "epoch": 0.8345787168255604,
        "step": 6478
    },
    {
        "loss": 1.6128,
        "grad_norm": 3.1156108379364014,
        "learning_rate": 4.96857211469931e-05,
        "epoch": 0.8347075496006184,
        "step": 6479
    },
    {
        "loss": 1.7386,
        "grad_norm": 2.5752315521240234,
        "learning_rate": 4.962489403056179e-05,
        "epoch": 0.8348363823756764,
        "step": 6480
    },
    {
        "loss": 1.6493,
        "grad_norm": 3.4005894660949707,
        "learning_rate": 4.9564067469304485e-05,
        "epoch": 0.8349652151507343,
        "step": 6481
    },
    {
        "loss": 1.6682,
        "grad_norm": 2.132563591003418,
        "learning_rate": 4.950324155324712e-05,
        "epoch": 0.8350940479257923,
        "step": 6482
    },
    {
        "loss": 1.8332,
        "grad_norm": 2.306284189224243,
        "learning_rate": 4.94424163724149e-05,
        "epoch": 0.8352228807008503,
        "step": 6483
    },
    {
        "loss": 1.7941,
        "grad_norm": 2.702376127243042,
        "learning_rate": 4.938159201683186e-05,
        "epoch": 0.8353517134759083,
        "step": 6484
    },
    {
        "loss": 1.7998,
        "grad_norm": 2.4922027587890625,
        "learning_rate": 4.932076857652082e-05,
        "epoch": 0.8354805462509662,
        "step": 6485
    },
    {
        "loss": 1.8664,
        "grad_norm": 1.573467493057251,
        "learning_rate": 4.925994614150324e-05,
        "epoch": 0.8356093790260242,
        "step": 6486
    },
    {
        "loss": 1.7602,
        "grad_norm": 2.9583699703216553,
        "learning_rate": 4.9199124801799105e-05,
        "epoch": 0.8357382118010822,
        "step": 6487
    },
    {
        "loss": 1.8484,
        "grad_norm": 2.787639617919922,
        "learning_rate": 4.9138304647426775e-05,
        "epoch": 0.8358670445761401,
        "step": 6488
    },
    {
        "loss": 2.3078,
        "grad_norm": 1.4230365753173828,
        "learning_rate": 4.90774857684029e-05,
        "epoch": 0.8359958773511982,
        "step": 6489
    },
    {
        "loss": 1.899,
        "grad_norm": 2.7945668697357178,
        "learning_rate": 4.901666825474206e-05,
        "epoch": 0.8361247101262561,
        "step": 6490
    },
    {
        "loss": 1.9827,
        "grad_norm": 1.8367255926132202,
        "learning_rate": 4.895585219645702e-05,
        "epoch": 0.836253542901314,
        "step": 6491
    },
    {
        "loss": 1.6969,
        "grad_norm": 1.214051365852356,
        "learning_rate": 4.889503768355839e-05,
        "epoch": 0.8363823756763721,
        "step": 6492
    },
    {
        "loss": 1.6169,
        "grad_norm": 3.0255661010742188,
        "learning_rate": 4.883422480605427e-05,
        "epoch": 0.83651120845143,
        "step": 6493
    },
    {
        "loss": 2.206,
        "grad_norm": 2.3008406162261963,
        "learning_rate": 4.877341365395052e-05,
        "epoch": 0.836640041226488,
        "step": 6494
    },
    {
        "loss": 1.9737,
        "grad_norm": 4.395083904266357,
        "learning_rate": 4.8712604317250596e-05,
        "epoch": 0.836768874001546,
        "step": 6495
    },
    {
        "loss": 2.058,
        "grad_norm": 1.9951794147491455,
        "learning_rate": 4.865179688595489e-05,
        "epoch": 0.836897706776604,
        "step": 6496
    },
    {
        "loss": 1.8333,
        "grad_norm": 3.3004181385040283,
        "learning_rate": 4.859099145006124e-05,
        "epoch": 0.837026539551662,
        "step": 6497
    },
    {
        "loss": 2.2721,
        "grad_norm": 2.4143130779266357,
        "learning_rate": 4.853018809956445e-05,
        "epoch": 0.8371553723267199,
        "step": 6498
    },
    {
        "loss": 1.6912,
        "grad_norm": 2.9950368404388428,
        "learning_rate": 4.846938692445627e-05,
        "epoch": 0.8372842051017779,
        "step": 6499
    },
    {
        "loss": 0.548,
        "grad_norm": 2.6841468811035156,
        "learning_rate": 4.8408588014725264e-05,
        "epoch": 0.8374130378768359,
        "step": 6500
    },
    {
        "loss": 2.284,
        "grad_norm": 1.8061449527740479,
        "learning_rate": 4.8347791460356464e-05,
        "epoch": 0.8375418706518939,
        "step": 6501
    },
    {
        "loss": 2.0722,
        "grad_norm": 1.891397476196289,
        "learning_rate": 4.8286997351331606e-05,
        "epoch": 0.8376707034269518,
        "step": 6502
    },
    {
        "loss": 2.4394,
        "grad_norm": 2.177083730697632,
        "learning_rate": 4.822620577762875e-05,
        "epoch": 0.8377995362020098,
        "step": 6503
    },
    {
        "loss": 1.8766,
        "grad_norm": 3.5477399826049805,
        "learning_rate": 4.8165416829222184e-05,
        "epoch": 0.8379283689770678,
        "step": 6504
    },
    {
        "loss": 1.834,
        "grad_norm": 1.737483024597168,
        "learning_rate": 4.8104630596082315e-05,
        "epoch": 0.8380572017521257,
        "step": 6505
    },
    {
        "loss": 2.1089,
        "grad_norm": 1.72579026222229,
        "learning_rate": 4.804384716817562e-05,
        "epoch": 0.8381860345271838,
        "step": 6506
    },
    {
        "loss": 2.093,
        "grad_norm": 2.998182773590088,
        "learning_rate": 4.79830666354642e-05,
        "epoch": 0.8383148673022417,
        "step": 6507
    },
    {
        "loss": 2.3548,
        "grad_norm": 1.6387742757797241,
        "learning_rate": 4.792228908790609e-05,
        "epoch": 0.8384437000772996,
        "step": 6508
    },
    {
        "loss": 1.7968,
        "grad_norm": 2.27122163772583,
        "learning_rate": 4.7861514615454825e-05,
        "epoch": 0.8385725328523577,
        "step": 6509
    },
    {
        "loss": 2.6628,
        "grad_norm": 2.490898370742798,
        "learning_rate": 4.7800743308059394e-05,
        "epoch": 0.8387013656274156,
        "step": 6510
    },
    {
        "loss": 2.7271,
        "grad_norm": 1.4904522895812988,
        "learning_rate": 4.77399752556641e-05,
        "epoch": 0.8388301984024736,
        "step": 6511
    },
    {
        "loss": 2.1824,
        "grad_norm": 1.8398454189300537,
        "learning_rate": 4.7679210548208434e-05,
        "epoch": 0.8389590311775316,
        "step": 6512
    },
    {
        "loss": 2.2706,
        "grad_norm": 2.1947734355926514,
        "learning_rate": 4.761844927562694e-05,
        "epoch": 0.8390878639525895,
        "step": 6513
    },
    {
        "loss": 2.0576,
        "grad_norm": 3.3039677143096924,
        "learning_rate": 4.7557691527849116e-05,
        "epoch": 0.8392166967276475,
        "step": 6514
    },
    {
        "loss": 1.8403,
        "grad_norm": 1.718908667564392,
        "learning_rate": 4.749693739479908e-05,
        "epoch": 0.8393455295027055,
        "step": 6515
    },
    {
        "loss": 1.8643,
        "grad_norm": 3.2125675678253174,
        "learning_rate": 4.743618696639578e-05,
        "epoch": 0.8394743622777635,
        "step": 6516
    },
    {
        "loss": 1.3721,
        "grad_norm": 1.9999302625656128,
        "learning_rate": 4.737544033255269e-05,
        "epoch": 0.8396031950528214,
        "step": 6517
    },
    {
        "loss": 1.4263,
        "grad_norm": 2.3747706413269043,
        "learning_rate": 4.7314697583177456e-05,
        "epoch": 0.8397320278278794,
        "step": 6518
    },
    {
        "loss": 2.6923,
        "grad_norm": 1.5247585773468018,
        "learning_rate": 4.725395880817219e-05,
        "epoch": 0.8398608606029374,
        "step": 6519
    },
    {
        "loss": 1.5169,
        "grad_norm": 2.273909091949463,
        "learning_rate": 4.719322409743304e-05,
        "epoch": 0.8399896933779953,
        "step": 6520
    },
    {
        "loss": 2.1764,
        "grad_norm": 2.5295209884643555,
        "learning_rate": 4.713249354085015e-05,
        "epoch": 0.8401185261530534,
        "step": 6521
    },
    {
        "loss": 2.024,
        "grad_norm": 2.4535627365112305,
        "learning_rate": 4.7071767228307514e-05,
        "epoch": 0.8402473589281113,
        "step": 6522
    },
    {
        "loss": 1.819,
        "grad_norm": 2.142320156097412,
        "learning_rate": 4.701104524968283e-05,
        "epoch": 0.8403761917031692,
        "step": 6523
    },
    {
        "loss": 1.8087,
        "grad_norm": 2.8254141807556152,
        "learning_rate": 4.6950327694847416e-05,
        "epoch": 0.8405050244782273,
        "step": 6524
    },
    {
        "loss": 2.3799,
        "grad_norm": 1.6168302297592163,
        "learning_rate": 4.688961465366606e-05,
        "epoch": 0.8406338572532852,
        "step": 6525
    },
    {
        "loss": 1.9617,
        "grad_norm": 2.319272518157959,
        "learning_rate": 4.6828906215996727e-05,
        "epoch": 0.8407626900283433,
        "step": 6526
    },
    {
        "loss": 2.5936,
        "grad_norm": 1.400589942932129,
        "learning_rate": 4.6768202471690716e-05,
        "epoch": 0.8408915228034012,
        "step": 6527
    },
    {
        "loss": 1.8988,
        "grad_norm": 2.867734670639038,
        "learning_rate": 4.670750351059242e-05,
        "epoch": 0.8410203555784591,
        "step": 6528
    },
    {
        "loss": 1.6432,
        "grad_norm": 2.166505813598633,
        "learning_rate": 4.664680942253884e-05,
        "epoch": 0.8411491883535172,
        "step": 6529
    },
    {
        "loss": 1.8387,
        "grad_norm": 2.524094343185425,
        "learning_rate": 4.6586120297360205e-05,
        "epoch": 0.8412780211285751,
        "step": 6530
    },
    {
        "loss": 1.418,
        "grad_norm": 4.062884330749512,
        "learning_rate": 4.6525436224879196e-05,
        "epoch": 0.8414068539036331,
        "step": 6531
    },
    {
        "loss": 1.779,
        "grad_norm": 2.228020429611206,
        "learning_rate": 4.646475729491087e-05,
        "epoch": 0.841535686678691,
        "step": 6532
    },
    {
        "loss": 0.9834,
        "grad_norm": 3.5862722396850586,
        "learning_rate": 4.640408359726287e-05,
        "epoch": 0.841664519453749,
        "step": 6533
    },
    {
        "loss": 2.2774,
        "grad_norm": 2.7001161575317383,
        "learning_rate": 4.634341522173504e-05,
        "epoch": 0.841793352228807,
        "step": 6534
    },
    {
        "loss": 1.7756,
        "grad_norm": 2.3542540073394775,
        "learning_rate": 4.628275225811933e-05,
        "epoch": 0.841922185003865,
        "step": 6535
    },
    {
        "loss": 1.868,
        "grad_norm": 2.4067044258117676,
        "learning_rate": 4.622209479619978e-05,
        "epoch": 0.842051017778923,
        "step": 6536
    },
    {
        "loss": 1.9724,
        "grad_norm": 2.7566399574279785,
        "learning_rate": 4.6161442925751964e-05,
        "epoch": 0.8421798505539809,
        "step": 6537
    },
    {
        "loss": 2.2424,
        "grad_norm": 2.429011583328247,
        "learning_rate": 4.610079673654365e-05,
        "epoch": 0.8423086833290389,
        "step": 6538
    },
    {
        "loss": 2.1653,
        "grad_norm": 2.7947425842285156,
        "learning_rate": 4.6040156318333935e-05,
        "epoch": 0.8424375161040969,
        "step": 6539
    },
    {
        "loss": 2.6852,
        "grad_norm": 2.1054375171661377,
        "learning_rate": 4.597952176087328e-05,
        "epoch": 0.8425663488791548,
        "step": 6540
    },
    {
        "loss": 2.4072,
        "grad_norm": 1.7601261138916016,
        "learning_rate": 4.5918893153903656e-05,
        "epoch": 0.8426951816542129,
        "step": 6541
    },
    {
        "loss": 2.4205,
        "grad_norm": 2.499549627304077,
        "learning_rate": 4.585827058715824e-05,
        "epoch": 0.8428240144292708,
        "step": 6542
    },
    {
        "loss": 1.9015,
        "grad_norm": 3.0091536045074463,
        "learning_rate": 4.579765415036105e-05,
        "epoch": 0.8429528472043288,
        "step": 6543
    },
    {
        "loss": 2.1252,
        "grad_norm": 1.3215973377227783,
        "learning_rate": 4.573704393322726e-05,
        "epoch": 0.8430816799793868,
        "step": 6544
    },
    {
        "loss": 2.3332,
        "grad_norm": 1.7757036685943604,
        "learning_rate": 4.567644002546273e-05,
        "epoch": 0.8432105127544447,
        "step": 6545
    },
    {
        "loss": 1.9747,
        "grad_norm": 2.063525438308716,
        "learning_rate": 4.561584251676402e-05,
        "epoch": 0.8433393455295027,
        "step": 6546
    },
    {
        "loss": 1.1538,
        "grad_norm": 3.717863082885742,
        "learning_rate": 4.555525149681819e-05,
        "epoch": 0.8434681783045607,
        "step": 6547
    },
    {
        "loss": 2.2019,
        "grad_norm": 2.3552768230438232,
        "learning_rate": 4.5494667055302733e-05,
        "epoch": 0.8435970110796187,
        "step": 6548
    },
    {
        "loss": 1.9257,
        "grad_norm": 1.7061920166015625,
        "learning_rate": 4.5434089281885364e-05,
        "epoch": 0.8437258438546766,
        "step": 6549
    },
    {
        "loss": 2.2963,
        "grad_norm": 1.6443692445755005,
        "learning_rate": 4.537351826622403e-05,
        "epoch": 0.8438546766297346,
        "step": 6550
    },
    {
        "loss": 2.1273,
        "grad_norm": 2.2949273586273193,
        "learning_rate": 4.5312954097966454e-05,
        "epoch": 0.8439835094047926,
        "step": 6551
    },
    {
        "loss": 1.7831,
        "grad_norm": 2.2246851921081543,
        "learning_rate": 4.525239686675043e-05,
        "epoch": 0.8441123421798505,
        "step": 6552
    },
    {
        "loss": 1.3648,
        "grad_norm": 3.9168686866760254,
        "learning_rate": 4.51918466622035e-05,
        "epoch": 0.8442411749549086,
        "step": 6553
    },
    {
        "loss": 1.7892,
        "grad_norm": 2.531956434249878,
        "learning_rate": 4.513130357394251e-05,
        "epoch": 0.8443700077299665,
        "step": 6554
    },
    {
        "loss": 1.0415,
        "grad_norm": 2.740530252456665,
        "learning_rate": 4.50707676915742e-05,
        "epoch": 0.8444988405050244,
        "step": 6555
    },
    {
        "loss": 1.9797,
        "grad_norm": 2.321272850036621,
        "learning_rate": 4.501023910469442e-05,
        "epoch": 0.8446276732800825,
        "step": 6556
    },
    {
        "loss": 1.9975,
        "grad_norm": 3.0574517250061035,
        "learning_rate": 4.494971790288813e-05,
        "epoch": 0.8447565060551404,
        "step": 6557
    },
    {
        "loss": 1.8946,
        "grad_norm": 1.5312350988388062,
        "learning_rate": 4.488920417572951e-05,
        "epoch": 0.8448853388301985,
        "step": 6558
    },
    {
        "loss": 2.135,
        "grad_norm": 1.7899689674377441,
        "learning_rate": 4.482869801278165e-05,
        "epoch": 0.8450141716052564,
        "step": 6559
    },
    {
        "loss": 1.3446,
        "grad_norm": 2.810694694519043,
        "learning_rate": 4.476819950359641e-05,
        "epoch": 0.8451430043803143,
        "step": 6560
    },
    {
        "loss": 1.7413,
        "grad_norm": 2.2550408840179443,
        "learning_rate": 4.4707708737714435e-05,
        "epoch": 0.8452718371553724,
        "step": 6561
    },
    {
        "loss": 1.2305,
        "grad_norm": 3.4352240562438965,
        "learning_rate": 4.464722580466467e-05,
        "epoch": 0.8454006699304303,
        "step": 6562
    },
    {
        "loss": 2.2056,
        "grad_norm": 2.245138645172119,
        "learning_rate": 4.458675079396459e-05,
        "epoch": 0.8455295027054883,
        "step": 6563
    },
    {
        "loss": 1.6257,
        "grad_norm": 2.518666982650757,
        "learning_rate": 4.4526283795120196e-05,
        "epoch": 0.8456583354805463,
        "step": 6564
    },
    {
        "loss": 2.384,
        "grad_norm": 1.7886637449264526,
        "learning_rate": 4.446582489762519e-05,
        "epoch": 0.8457871682556042,
        "step": 6565
    },
    {
        "loss": 2.0863,
        "grad_norm": 2.390425205230713,
        "learning_rate": 4.440537419096157e-05,
        "epoch": 0.8459160010306622,
        "step": 6566
    },
    {
        "loss": 2.1911,
        "grad_norm": 2.499706268310547,
        "learning_rate": 4.4344931764599194e-05,
        "epoch": 0.8460448338057202,
        "step": 6567
    },
    {
        "loss": 1.854,
        "grad_norm": 3.040996789932251,
        "learning_rate": 4.428449770799548e-05,
        "epoch": 0.8461736665807782,
        "step": 6568
    },
    {
        "loss": 2.2187,
        "grad_norm": 2.508793354034424,
        "learning_rate": 4.422407211059567e-05,
        "epoch": 0.8463024993558361,
        "step": 6569
    },
    {
        "loss": 1.206,
        "grad_norm": 3.486818313598633,
        "learning_rate": 4.4163655061832364e-05,
        "epoch": 0.846431332130894,
        "step": 6570
    },
    {
        "loss": 2.2159,
        "grad_norm": 1.9071577787399292,
        "learning_rate": 4.410324665112557e-05,
        "epoch": 0.8465601649059521,
        "step": 6571
    },
    {
        "loss": 2.1311,
        "grad_norm": 3.095935583114624,
        "learning_rate": 4.404284696788249e-05,
        "epoch": 0.84668899768101,
        "step": 6572
    },
    {
        "loss": 1.3625,
        "grad_norm": 2.008862257003784,
        "learning_rate": 4.3982456101497404e-05,
        "epoch": 0.8468178304560681,
        "step": 6573
    },
    {
        "loss": 1.5906,
        "grad_norm": 2.4741568565368652,
        "learning_rate": 4.392207414135155e-05,
        "epoch": 0.846946663231126,
        "step": 6574
    },
    {
        "loss": 1.7166,
        "grad_norm": 4.281582832336426,
        "learning_rate": 4.386170117681304e-05,
        "epoch": 0.8470754960061839,
        "step": 6575
    },
    {
        "loss": 2.1999,
        "grad_norm": 2.005141496658325,
        "learning_rate": 4.380133729723649e-05,
        "epoch": 0.847204328781242,
        "step": 6576
    },
    {
        "loss": 2.354,
        "grad_norm": 1.8628731966018677,
        "learning_rate": 4.374098259196325e-05,
        "epoch": 0.8473331615562999,
        "step": 6577
    },
    {
        "loss": 1.4035,
        "grad_norm": 2.902665138244629,
        "learning_rate": 4.3680637150321126e-05,
        "epoch": 0.8474619943313579,
        "step": 6578
    },
    {
        "loss": 2.0805,
        "grad_norm": 2.906033992767334,
        "learning_rate": 4.3620301061623947e-05,
        "epoch": 0.8475908271064159,
        "step": 6579
    },
    {
        "loss": 2.3774,
        "grad_norm": 1.3921338319778442,
        "learning_rate": 4.35599744151719e-05,
        "epoch": 0.8477196598814738,
        "step": 6580
    },
    {
        "loss": 1.9351,
        "grad_norm": 3.1141040325164795,
        "learning_rate": 4.3499657300251376e-05,
        "epoch": 0.8478484926565318,
        "step": 6581
    },
    {
        "loss": 2.2424,
        "grad_norm": 2.8124938011169434,
        "learning_rate": 4.3439349806134236e-05,
        "epoch": 0.8479773254315898,
        "step": 6582
    },
    {
        "loss": 1.937,
        "grad_norm": 2.411200761795044,
        "learning_rate": 4.3379052022078396e-05,
        "epoch": 0.8481061582066478,
        "step": 6583
    },
    {
        "loss": 1.7754,
        "grad_norm": 2.7405996322631836,
        "learning_rate": 4.331876403732732e-05,
        "epoch": 0.8482349909817057,
        "step": 6584
    },
    {
        "loss": 2.2089,
        "grad_norm": 1.726048231124878,
        "learning_rate": 4.3258485941109995e-05,
        "epoch": 0.8483638237567637,
        "step": 6585
    },
    {
        "loss": 1.5436,
        "grad_norm": 2.717850923538208,
        "learning_rate": 4.31982178226408e-05,
        "epoch": 0.8484926565318217,
        "step": 6586
    },
    {
        "loss": 2.5891,
        "grad_norm": 1.4170624017715454,
        "learning_rate": 4.313795977111915e-05,
        "epoch": 0.8486214893068796,
        "step": 6587
    },
    {
        "loss": 1.827,
        "grad_norm": 2.3976457118988037,
        "learning_rate": 4.3077711875729725e-05,
        "epoch": 0.8487503220819377,
        "step": 6588
    },
    {
        "loss": 1.2395,
        "grad_norm": 3.209829092025757,
        "learning_rate": 4.301747422564235e-05,
        "epoch": 0.8488791548569956,
        "step": 6589
    },
    {
        "loss": 1.7281,
        "grad_norm": 1.8749560117721558,
        "learning_rate": 4.29572469100113e-05,
        "epoch": 0.8490079876320535,
        "step": 6590
    },
    {
        "loss": 2.2313,
        "grad_norm": 2.0481879711151123,
        "learning_rate": 4.28970300179758e-05,
        "epoch": 0.8491368204071116,
        "step": 6591
    },
    {
        "loss": 1.2373,
        "grad_norm": 3.2429757118225098,
        "learning_rate": 4.2836823638659684e-05,
        "epoch": 0.8492656531821695,
        "step": 6592
    },
    {
        "loss": 1.4137,
        "grad_norm": 2.613532781600952,
        "learning_rate": 4.2776627861170984e-05,
        "epoch": 0.8493944859572276,
        "step": 6593
    },
    {
        "loss": 0.7143,
        "grad_norm": 2.5814428329467773,
        "learning_rate": 4.271644277460228e-05,
        "epoch": 0.8495233187322855,
        "step": 6594
    },
    {
        "loss": 1.7553,
        "grad_norm": 2.467750072479248,
        "learning_rate": 4.2656268468030216e-05,
        "epoch": 0.8496521515073435,
        "step": 6595
    },
    {
        "loss": 1.8952,
        "grad_norm": 2.0523197650909424,
        "learning_rate": 4.259610503051552e-05,
        "epoch": 0.8497809842824015,
        "step": 6596
    },
    {
        "loss": 1.7956,
        "grad_norm": 2.4562580585479736,
        "learning_rate": 4.253595255110283e-05,
        "epoch": 0.8499098170574594,
        "step": 6597
    },
    {
        "loss": 2.1186,
        "grad_norm": 2.0470149517059326,
        "learning_rate": 4.247581111882055e-05,
        "epoch": 0.8500386498325174,
        "step": 6598
    },
    {
        "loss": 2.09,
        "grad_norm": 1.5795165300369263,
        "learning_rate": 4.2415680822680736e-05,
        "epoch": 0.8501674826075754,
        "step": 6599
    },
    {
        "loss": 1.6243,
        "grad_norm": 2.1533236503601074,
        "learning_rate": 4.2355561751679045e-05,
        "epoch": 0.8502963153826334,
        "step": 6600
    },
    {
        "loss": 2.3001,
        "grad_norm": 1.7701290845870972,
        "learning_rate": 4.229545399479431e-05,
        "epoch": 0.8504251481576913,
        "step": 6601
    },
    {
        "loss": 1.4584,
        "grad_norm": 3.155308961868286,
        "learning_rate": 4.2235357640988796e-05,
        "epoch": 0.8505539809327493,
        "step": 6602
    },
    {
        "loss": 1.8233,
        "grad_norm": 2.8102099895477295,
        "learning_rate": 4.2175272779207956e-05,
        "epoch": 0.8506828137078073,
        "step": 6603
    },
    {
        "loss": 1.6977,
        "grad_norm": 3.1654462814331055,
        "learning_rate": 4.211519949837994e-05,
        "epoch": 0.8508116464828652,
        "step": 6604
    },
    {
        "loss": 1.8432,
        "grad_norm": 1.9126032590866089,
        "learning_rate": 4.205513788741603e-05,
        "epoch": 0.8509404792579233,
        "step": 6605
    },
    {
        "loss": 1.9132,
        "grad_norm": 3.1204946041107178,
        "learning_rate": 4.1995088035210144e-05,
        "epoch": 0.8510693120329812,
        "step": 6606
    },
    {
        "loss": 1.9288,
        "grad_norm": 2.3581182956695557,
        "learning_rate": 4.193505003063881e-05,
        "epoch": 0.8511981448080391,
        "step": 6607
    },
    {
        "loss": 1.656,
        "grad_norm": 2.3627805709838867,
        "learning_rate": 4.1875023962561e-05,
        "epoch": 0.8513269775830972,
        "step": 6608
    },
    {
        "loss": 1.7901,
        "grad_norm": 2.0996510982513428,
        "learning_rate": 4.181500991981804e-05,
        "epoch": 0.8514558103581551,
        "step": 6609
    },
    {
        "loss": 1.5676,
        "grad_norm": 3.246370553970337,
        "learning_rate": 4.1755007991233444e-05,
        "epoch": 0.8515846431332131,
        "step": 6610
    },
    {
        "loss": 2.0487,
        "grad_norm": 2.087996244430542,
        "learning_rate": 4.169501826561287e-05,
        "epoch": 0.8517134759082711,
        "step": 6611
    },
    {
        "loss": 1.7609,
        "grad_norm": 3.3607022762298584,
        "learning_rate": 4.163504083174371e-05,
        "epoch": 0.851842308683329,
        "step": 6612
    },
    {
        "loss": 1.8607,
        "grad_norm": 3.0625009536743164,
        "learning_rate": 4.1575075778395366e-05,
        "epoch": 0.851971141458387,
        "step": 6613
    },
    {
        "loss": 1.3414,
        "grad_norm": 2.8997764587402344,
        "learning_rate": 4.1515123194318846e-05,
        "epoch": 0.852099974233445,
        "step": 6614
    },
    {
        "loss": 1.293,
        "grad_norm": 3.1811258792877197,
        "learning_rate": 4.145518316824671e-05,
        "epoch": 0.852228807008503,
        "step": 6615
    },
    {
        "loss": 2.3299,
        "grad_norm": 3.0353968143463135,
        "learning_rate": 4.1395255788892935e-05,
        "epoch": 0.8523576397835609,
        "step": 6616
    },
    {
        "loss": 2.3161,
        "grad_norm": 1.921990990638733,
        "learning_rate": 4.133534114495283e-05,
        "epoch": 0.8524864725586189,
        "step": 6617
    },
    {
        "loss": 1.6403,
        "grad_norm": 2.616537094116211,
        "learning_rate": 4.127543932510268e-05,
        "epoch": 0.8526153053336769,
        "step": 6618
    },
    {
        "loss": 1.3709,
        "grad_norm": 2.424994468688965,
        "learning_rate": 4.121555041799997e-05,
        "epoch": 0.8527441381087348,
        "step": 6619
    },
    {
        "loss": 2.1702,
        "grad_norm": 2.2390170097351074,
        "learning_rate": 4.115567451228299e-05,
        "epoch": 0.8528729708837929,
        "step": 6620
    },
    {
        "loss": 2.3208,
        "grad_norm": 1.024027705192566,
        "learning_rate": 4.109581169657084e-05,
        "epoch": 0.8530018036588508,
        "step": 6621
    },
    {
        "loss": 1.6046,
        "grad_norm": 3.0971732139587402,
        "learning_rate": 4.103596205946326e-05,
        "epoch": 0.8531306364339087,
        "step": 6622
    },
    {
        "loss": 2.1898,
        "grad_norm": 2.6649866104125977,
        "learning_rate": 4.097612568954026e-05,
        "epoch": 0.8532594692089668,
        "step": 6623
    },
    {
        "loss": 2.0058,
        "grad_norm": 2.852884531021118,
        "learning_rate": 4.0916302675362576e-05,
        "epoch": 0.8533883019840247,
        "step": 6624
    },
    {
        "loss": 2.6173,
        "grad_norm": 1.281887412071228,
        "learning_rate": 4.085649310547101e-05,
        "epoch": 0.8535171347590828,
        "step": 6625
    },
    {
        "loss": 2.1144,
        "grad_norm": 2.436692237854004,
        "learning_rate": 4.079669706838632e-05,
        "epoch": 0.8536459675341407,
        "step": 6626
    },
    {
        "loss": 2.2375,
        "grad_norm": 2.5555949211120605,
        "learning_rate": 4.073691465260941e-05,
        "epoch": 0.8537748003091986,
        "step": 6627
    },
    {
        "loss": 2.1006,
        "grad_norm": 1.4311420917510986,
        "learning_rate": 4.067714594662108e-05,
        "epoch": 0.8539036330842567,
        "step": 6628
    },
    {
        "loss": 1.951,
        "grad_norm": 2.643768548965454,
        "learning_rate": 4.061739103888157e-05,
        "epoch": 0.8540324658593146,
        "step": 6629
    },
    {
        "loss": 1.0271,
        "grad_norm": 2.8329224586486816,
        "learning_rate": 4.055765001783096e-05,
        "epoch": 0.8541612986343726,
        "step": 6630
    },
    {
        "loss": 1.84,
        "grad_norm": 2.4009339809417725,
        "learning_rate": 4.049792297188867e-05,
        "epoch": 0.8542901314094306,
        "step": 6631
    },
    {
        "loss": 2.309,
        "grad_norm": 1.8062189817428589,
        "learning_rate": 4.043820998945346e-05,
        "epoch": 0.8544189641844885,
        "step": 6632
    },
    {
        "loss": 1.259,
        "grad_norm": 2.3193860054016113,
        "learning_rate": 4.0378511158903255e-05,
        "epoch": 0.8545477969595465,
        "step": 6633
    },
    {
        "loss": 1.5344,
        "grad_norm": 4.159897804260254,
        "learning_rate": 4.031882656859506e-05,
        "epoch": 0.8546766297346045,
        "step": 6634
    },
    {
        "loss": 1.1141,
        "grad_norm": 1.907800555229187,
        "learning_rate": 4.025915630686477e-05,
        "epoch": 0.8548054625096625,
        "step": 6635
    },
    {
        "loss": 2.31,
        "grad_norm": 2.5460333824157715,
        "learning_rate": 4.0199500462027186e-05,
        "epoch": 0.8549342952847204,
        "step": 6636
    },
    {
        "loss": 1.2246,
        "grad_norm": 3.1657683849334717,
        "learning_rate": 4.013985912237551e-05,
        "epoch": 0.8550631280597784,
        "step": 6637
    },
    {
        "loss": 1.542,
        "grad_norm": 2.7681097984313965,
        "learning_rate": 4.008023237618173e-05,
        "epoch": 0.8551919608348364,
        "step": 6638
    },
    {
        "loss": 1.6772,
        "grad_norm": 2.5681920051574707,
        "learning_rate": 4.002062031169621e-05,
        "epoch": 0.8553207936098943,
        "step": 6639
    },
    {
        "loss": 0.8741,
        "grad_norm": 3.299926996231079,
        "learning_rate": 3.9961023017147323e-05,
        "epoch": 0.8554496263849524,
        "step": 6640
    },
    {
        "loss": 1.9392,
        "grad_norm": 2.4980671405792236,
        "learning_rate": 3.990144058074198e-05,
        "epoch": 0.8555784591600103,
        "step": 6641
    },
    {
        "loss": 1.597,
        "grad_norm": 2.105264186859131,
        "learning_rate": 3.9841873090664904e-05,
        "epoch": 0.8557072919350682,
        "step": 6642
    },
    {
        "loss": 2.086,
        "grad_norm": 2.541563034057617,
        "learning_rate": 3.978232063507856e-05,
        "epoch": 0.8558361247101263,
        "step": 6643
    },
    {
        "loss": 2.0836,
        "grad_norm": 2.5848898887634277,
        "learning_rate": 3.9722783302123376e-05,
        "epoch": 0.8559649574851842,
        "step": 6644
    },
    {
        "loss": 1.6768,
        "grad_norm": 3.005326509475708,
        "learning_rate": 3.9663261179917296e-05,
        "epoch": 0.8560937902602422,
        "step": 6645
    },
    {
        "loss": 2.018,
        "grad_norm": 3.129589557647705,
        "learning_rate": 3.960375435655579e-05,
        "epoch": 0.8562226230353002,
        "step": 6646
    },
    {
        "loss": 2.1718,
        "grad_norm": 1.9672049283981323,
        "learning_rate": 3.954426292011173e-05,
        "epoch": 0.8563514558103582,
        "step": 6647
    },
    {
        "loss": 2.0829,
        "grad_norm": 2.28651762008667,
        "learning_rate": 3.9484786958634946e-05,
        "epoch": 0.8564802885854161,
        "step": 6648
    },
    {
        "loss": 2.399,
        "grad_norm": 1.5371425151824951,
        "learning_rate": 3.942532656015278e-05,
        "epoch": 0.8566091213604741,
        "step": 6649
    },
    {
        "loss": 2.0096,
        "grad_norm": 2.8781964778900146,
        "learning_rate": 3.936588181266934e-05,
        "epoch": 0.8567379541355321,
        "step": 6650
    },
    {
        "loss": 1.5196,
        "grad_norm": 2.8117306232452393,
        "learning_rate": 3.9306452804165405e-05,
        "epoch": 0.85686678691059,
        "step": 6651
    },
    {
        "loss": 2.2878,
        "grad_norm": 2.2598743438720703,
        "learning_rate": 3.9247039622598705e-05,
        "epoch": 0.8569956196856481,
        "step": 6652
    },
    {
        "loss": 2.491,
        "grad_norm": 1.883349895477295,
        "learning_rate": 3.9187642355903524e-05,
        "epoch": 0.857124452460706,
        "step": 6653
    },
    {
        "loss": 2.0976,
        "grad_norm": 1.2939584255218506,
        "learning_rate": 3.912826109199037e-05,
        "epoch": 0.8572532852357639,
        "step": 6654
    },
    {
        "loss": 2.0204,
        "grad_norm": 2.26106333732605,
        "learning_rate": 3.906889591874631e-05,
        "epoch": 0.857382118010822,
        "step": 6655
    },
    {
        "loss": 2.3414,
        "grad_norm": 2.7555043697357178,
        "learning_rate": 3.900954692403448e-05,
        "epoch": 0.8575109507858799,
        "step": 6656
    },
    {
        "loss": 2.0097,
        "grad_norm": 3.3051745891571045,
        "learning_rate": 3.895021419569411e-05,
        "epoch": 0.857639783560938,
        "step": 6657
    },
    {
        "loss": 2.3154,
        "grad_norm": 1.8295170068740845,
        "learning_rate": 3.8890897821540355e-05,
        "epoch": 0.8577686163359959,
        "step": 6658
    },
    {
        "loss": 1.8477,
        "grad_norm": 2.2868051528930664,
        "learning_rate": 3.883159788936414e-05,
        "epoch": 0.8578974491110538,
        "step": 6659
    },
    {
        "loss": 1.5201,
        "grad_norm": 2.03901743888855,
        "learning_rate": 3.877231448693206e-05,
        "epoch": 0.8580262818861119,
        "step": 6660
    },
    {
        "loss": 2.2933,
        "grad_norm": 2.158717393875122,
        "learning_rate": 3.8713047701986336e-05,
        "epoch": 0.8581551146611698,
        "step": 6661
    },
    {
        "loss": 2.4405,
        "grad_norm": 2.593522787094116,
        "learning_rate": 3.8653797622244366e-05,
        "epoch": 0.8582839474362278,
        "step": 6662
    },
    {
        "loss": 2.1619,
        "grad_norm": 2.6191627979278564,
        "learning_rate": 3.859456433539904e-05,
        "epoch": 0.8584127802112858,
        "step": 6663
    },
    {
        "loss": 2.1317,
        "grad_norm": 1.6268712282180786,
        "learning_rate": 3.853534792911838e-05,
        "epoch": 0.8585416129863437,
        "step": 6664
    },
    {
        "loss": 2.2744,
        "grad_norm": 2.598961353302002,
        "learning_rate": 3.847614849104524e-05,
        "epoch": 0.8586704457614017,
        "step": 6665
    },
    {
        "loss": 1.4747,
        "grad_norm": 2.3879265785217285,
        "learning_rate": 3.8416966108797494e-05,
        "epoch": 0.8587992785364597,
        "step": 6666
    },
    {
        "loss": 1.9096,
        "grad_norm": 3.813227415084839,
        "learning_rate": 3.8357800869967944e-05,
        "epoch": 0.8589281113115177,
        "step": 6667
    },
    {
        "loss": 2.4155,
        "grad_norm": 1.3938019275665283,
        "learning_rate": 3.829865286212366e-05,
        "epoch": 0.8590569440865756,
        "step": 6668
    },
    {
        "loss": 1.6801,
        "grad_norm": 1.70194673538208,
        "learning_rate": 3.823952217280647e-05,
        "epoch": 0.8591857768616336,
        "step": 6669
    },
    {
        "loss": 2.1891,
        "grad_norm": 2.8513922691345215,
        "learning_rate": 3.8180408889532446e-05,
        "epoch": 0.8593146096366916,
        "step": 6670
    },
    {
        "loss": 1.9851,
        "grad_norm": 1.9053059816360474,
        "learning_rate": 3.8121313099791966e-05,
        "epoch": 0.8594434424117495,
        "step": 6671
    },
    {
        "loss": 1.901,
        "grad_norm": 4.000324726104736,
        "learning_rate": 3.806223489104956e-05,
        "epoch": 0.8595722751868076,
        "step": 6672
    },
    {
        "loss": 2.0407,
        "grad_norm": 2.0442795753479004,
        "learning_rate": 3.8003174350743534e-05,
        "epoch": 0.8597011079618655,
        "step": 6673
    },
    {
        "loss": 2.0445,
        "grad_norm": 2.2288079261779785,
        "learning_rate": 3.7944131566286174e-05,
        "epoch": 0.8598299407369234,
        "step": 6674
    },
    {
        "loss": 2.3418,
        "grad_norm": 2.252825975418091,
        "learning_rate": 3.788510662506369e-05,
        "epoch": 0.8599587735119815,
        "step": 6675
    },
    {
        "loss": 2.0245,
        "grad_norm": 2.0612688064575195,
        "learning_rate": 3.782609961443551e-05,
        "epoch": 0.8600876062870394,
        "step": 6676
    },
    {
        "loss": 1.8239,
        "grad_norm": 2.1878106594085693,
        "learning_rate": 3.776711062173473e-05,
        "epoch": 0.8602164390620974,
        "step": 6677
    },
    {
        "loss": 1.7674,
        "grad_norm": 2.3864047527313232,
        "learning_rate": 3.770813973426785e-05,
        "epoch": 0.8603452718371554,
        "step": 6678
    },
    {
        "loss": 2.0488,
        "grad_norm": 2.6935360431671143,
        "learning_rate": 3.764918703931431e-05,
        "epoch": 0.8604741046122133,
        "step": 6679
    },
    {
        "loss": 2.0008,
        "grad_norm": 1.391862392425537,
        "learning_rate": 3.759025262412686e-05,
        "epoch": 0.8606029373872713,
        "step": 6680
    },
    {
        "loss": 2.2492,
        "grad_norm": 1.3875471353530884,
        "learning_rate": 3.753133657593112e-05,
        "epoch": 0.8607317701623293,
        "step": 6681
    },
    {
        "loss": 1.5288,
        "grad_norm": 2.3415305614471436,
        "learning_rate": 3.747243898192553e-05,
        "epoch": 0.8608606029373873,
        "step": 6682
    },
    {
        "loss": 1.8063,
        "grad_norm": 2.08250093460083,
        "learning_rate": 3.7413559929281213e-05,
        "epoch": 0.8609894357124452,
        "step": 6683
    },
    {
        "loss": 1.5367,
        "grad_norm": 2.4758405685424805,
        "learning_rate": 3.7354699505141855e-05,
        "epoch": 0.8611182684875032,
        "step": 6684
    },
    {
        "loss": 1.9801,
        "grad_norm": 2.5646700859069824,
        "learning_rate": 3.7295857796623556e-05,
        "epoch": 0.8612471012625612,
        "step": 6685
    },
    {
        "loss": 1.7018,
        "grad_norm": 2.254943609237671,
        "learning_rate": 3.72370348908148e-05,
        "epoch": 0.8613759340376191,
        "step": 6686
    },
    {
        "loss": 1.8637,
        "grad_norm": 2.3529906272888184,
        "learning_rate": 3.717823087477603e-05,
        "epoch": 0.8615047668126772,
        "step": 6687
    },
    {
        "loss": 2.4081,
        "grad_norm": 1.2182142734527588,
        "learning_rate": 3.711944583553994e-05,
        "epoch": 0.8616335995877351,
        "step": 6688
    },
    {
        "loss": 2.3359,
        "grad_norm": 2.2241523265838623,
        "learning_rate": 3.7060679860111134e-05,
        "epoch": 0.861762432362793,
        "step": 6689
    },
    {
        "loss": 1.6143,
        "grad_norm": 4.9671759605407715,
        "learning_rate": 3.700193303546581e-05,
        "epoch": 0.8618912651378511,
        "step": 6690
    },
    {
        "loss": 1.8004,
        "grad_norm": 2.783567428588867,
        "learning_rate": 3.6943205448551924e-05,
        "epoch": 0.862020097912909,
        "step": 6691
    },
    {
        "loss": 1.7432,
        "grad_norm": 1.7144830226898193,
        "learning_rate": 3.68844971862892e-05,
        "epoch": 0.862148930687967,
        "step": 6692
    },
    {
        "loss": 1.8492,
        "grad_norm": 2.1176071166992188,
        "learning_rate": 3.682580833556834e-05,
        "epoch": 0.862277763463025,
        "step": 6693
    },
    {
        "loss": 1.9015,
        "grad_norm": 3.1473896503448486,
        "learning_rate": 3.676713898325157e-05,
        "epoch": 0.862406596238083,
        "step": 6694
    },
    {
        "loss": 1.3534,
        "grad_norm": 2.6317927837371826,
        "learning_rate": 3.670848921617222e-05,
        "epoch": 0.862535429013141,
        "step": 6695
    },
    {
        "loss": 1.8243,
        "grad_norm": 3.219851016998291,
        "learning_rate": 3.664985912113461e-05,
        "epoch": 0.8626642617881989,
        "step": 6696
    },
    {
        "loss": 2.4296,
        "grad_norm": 1.7254520654678345,
        "learning_rate": 3.6591248784914025e-05,
        "epoch": 0.8627930945632569,
        "step": 6697
    },
    {
        "loss": 1.205,
        "grad_norm": 2.8964462280273438,
        "learning_rate": 3.65326582942563e-05,
        "epoch": 0.8629219273383149,
        "step": 6698
    },
    {
        "loss": 2.2224,
        "grad_norm": 3.1496334075927734,
        "learning_rate": 3.6474087735878096e-05,
        "epoch": 0.8630507601133729,
        "step": 6699
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.76943039894104,
        "learning_rate": 3.641553719646651e-05,
        "epoch": 0.8631795928884308,
        "step": 6700
    },
    {
        "loss": 2.0482,
        "grad_norm": 2.5611696243286133,
        "learning_rate": 3.635700676267901e-05,
        "epoch": 0.8633084256634888,
        "step": 6701
    },
    {
        "loss": 1.7369,
        "grad_norm": 2.640255928039551,
        "learning_rate": 3.62984965211433e-05,
        "epoch": 0.8634372584385468,
        "step": 6702
    },
    {
        "loss": 2.0594,
        "grad_norm": 2.3352365493774414,
        "learning_rate": 3.624000655845729e-05,
        "epoch": 0.8635660912136047,
        "step": 6703
    },
    {
        "loss": 1.2023,
        "grad_norm": 2.890291213989258,
        "learning_rate": 3.618153696118863e-05,
        "epoch": 0.8636949239886628,
        "step": 6704
    },
    {
        "loss": 1.7895,
        "grad_norm": 1.8094074726104736,
        "learning_rate": 3.6123087815875075e-05,
        "epoch": 0.8638237567637207,
        "step": 6705
    },
    {
        "loss": 1.9814,
        "grad_norm": 3.049875020980835,
        "learning_rate": 3.606465920902401e-05,
        "epoch": 0.8639525895387786,
        "step": 6706
    },
    {
        "loss": 2.0495,
        "grad_norm": 1.489317774772644,
        "learning_rate": 3.6006251227112444e-05,
        "epoch": 0.8640814223138367,
        "step": 6707
    },
    {
        "loss": 1.875,
        "grad_norm": 1.883433222770691,
        "learning_rate": 3.594786395658685e-05,
        "epoch": 0.8642102550888946,
        "step": 6708
    },
    {
        "loss": 1.8713,
        "grad_norm": 2.03110933303833,
        "learning_rate": 3.588949748386305e-05,
        "epoch": 0.8643390878639526,
        "step": 6709
    },
    {
        "loss": 1.5419,
        "grad_norm": 2.5548760890960693,
        "learning_rate": 3.5831151895326086e-05,
        "epoch": 0.8644679206390106,
        "step": 6710
    },
    {
        "loss": 0.9392,
        "grad_norm": 3.409458875656128,
        "learning_rate": 3.5772827277330165e-05,
        "epoch": 0.8645967534140685,
        "step": 6711
    },
    {
        "loss": 2.0818,
        "grad_norm": 1.402594804763794,
        "learning_rate": 3.571452371619823e-05,
        "epoch": 0.8647255861891265,
        "step": 6712
    },
    {
        "loss": 2.0569,
        "grad_norm": 1.6923930644989014,
        "learning_rate": 3.565624129822227e-05,
        "epoch": 0.8648544189641845,
        "step": 6713
    },
    {
        "loss": 1.2691,
        "grad_norm": 3.144618034362793,
        "learning_rate": 3.5597980109662996e-05,
        "epoch": 0.8649832517392425,
        "step": 6714
    },
    {
        "loss": 2.0744,
        "grad_norm": 2.671201229095459,
        "learning_rate": 3.55397402367495e-05,
        "epoch": 0.8651120845143004,
        "step": 6715
    },
    {
        "loss": 1.4858,
        "grad_norm": 3.4967591762542725,
        "learning_rate": 3.5481521765679505e-05,
        "epoch": 0.8652409172893584,
        "step": 6716
    },
    {
        "loss": 1.7785,
        "grad_norm": 2.5961596965789795,
        "learning_rate": 3.5423324782618994e-05,
        "epoch": 0.8653697500644164,
        "step": 6717
    },
    {
        "loss": 0.4424,
        "grad_norm": 2.6828341484069824,
        "learning_rate": 3.536514937370218e-05,
        "epoch": 0.8654985828394743,
        "step": 6718
    },
    {
        "loss": 1.8875,
        "grad_norm": 4.063940048217773,
        "learning_rate": 3.530699562503129e-05,
        "epoch": 0.8656274156145324,
        "step": 6719
    },
    {
        "loss": 2.166,
        "grad_norm": 2.1854770183563232,
        "learning_rate": 3.524886362267653e-05,
        "epoch": 0.8657562483895903,
        "step": 6720
    },
    {
        "loss": 2.0659,
        "grad_norm": 3.047980785369873,
        "learning_rate": 3.519075345267592e-05,
        "epoch": 0.8658850811646482,
        "step": 6721
    },
    {
        "loss": 1.9408,
        "grad_norm": 2.780099630355835,
        "learning_rate": 3.513266520103522e-05,
        "epoch": 0.8660139139397063,
        "step": 6722
    },
    {
        "loss": 1.1534,
        "grad_norm": 2.570244073867798,
        "learning_rate": 3.507459895372757e-05,
        "epoch": 0.8661427467147642,
        "step": 6723
    },
    {
        "loss": 1.608,
        "grad_norm": 3.3596479892730713,
        "learning_rate": 3.501655479669371e-05,
        "epoch": 0.8662715794898223,
        "step": 6724
    },
    {
        "loss": 1.2137,
        "grad_norm": 3.599395990371704,
        "learning_rate": 3.495853281584165e-05,
        "epoch": 0.8664004122648802,
        "step": 6725
    },
    {
        "loss": 1.9636,
        "grad_norm": 2.5929224491119385,
        "learning_rate": 3.490053309704657e-05,
        "epoch": 0.8665292450399381,
        "step": 6726
    },
    {
        "loss": 1.9413,
        "grad_norm": 2.4061009883880615,
        "learning_rate": 3.484255572615068e-05,
        "epoch": 0.8666580778149962,
        "step": 6727
    },
    {
        "loss": 1.9014,
        "grad_norm": 3.3615329265594482,
        "learning_rate": 3.478460078896323e-05,
        "epoch": 0.8667869105900541,
        "step": 6728
    },
    {
        "loss": 1.5252,
        "grad_norm": 3.1839592456817627,
        "learning_rate": 3.4726668371259996e-05,
        "epoch": 0.8669157433651121,
        "step": 6729
    },
    {
        "loss": 1.6779,
        "grad_norm": 1.2236623764038086,
        "learning_rate": 3.4668758558783686e-05,
        "epoch": 0.86704457614017,
        "step": 6730
    },
    {
        "loss": 2.1554,
        "grad_norm": 1.6051206588745117,
        "learning_rate": 3.4610871437243454e-05,
        "epoch": 0.867173408915228,
        "step": 6731
    },
    {
        "loss": 1.4772,
        "grad_norm": 1.678943395614624,
        "learning_rate": 3.455300709231488e-05,
        "epoch": 0.867302241690286,
        "step": 6732
    },
    {
        "loss": 1.5369,
        "grad_norm": 2.6304454803466797,
        "learning_rate": 3.44951656096399e-05,
        "epoch": 0.867431074465344,
        "step": 6733
    },
    {
        "loss": 2.0428,
        "grad_norm": 1.702022910118103,
        "learning_rate": 3.4437347074826354e-05,
        "epoch": 0.867559907240402,
        "step": 6734
    },
    {
        "loss": 1.98,
        "grad_norm": 2.3079710006713867,
        "learning_rate": 3.4379551573448486e-05,
        "epoch": 0.8676887400154599,
        "step": 6735
    },
    {
        "loss": 2.6815,
        "grad_norm": 1.9601881504058838,
        "learning_rate": 3.43217791910463e-05,
        "epoch": 0.8678175727905179,
        "step": 6736
    },
    {
        "loss": 1.6027,
        "grad_norm": 3.0172319412231445,
        "learning_rate": 3.42640300131254e-05,
        "epoch": 0.8679464055655759,
        "step": 6737
    },
    {
        "loss": 1.4671,
        "grad_norm": 2.1627368927001953,
        "learning_rate": 3.4206304125157265e-05,
        "epoch": 0.8680752383406338,
        "step": 6738
    },
    {
        "loss": 1.5169,
        "grad_norm": 2.8449156284332275,
        "learning_rate": 3.4148601612578926e-05,
        "epoch": 0.8682040711156919,
        "step": 6739
    },
    {
        "loss": 1.5056,
        "grad_norm": 2.55600643157959,
        "learning_rate": 3.4090922560792585e-05,
        "epoch": 0.8683329038907498,
        "step": 6740
    },
    {
        "loss": 2.0882,
        "grad_norm": 1.6665476560592651,
        "learning_rate": 3.403326705516594e-05,
        "epoch": 0.8684617366658077,
        "step": 6741
    },
    {
        "loss": 0.9008,
        "grad_norm": 2.153994083404541,
        "learning_rate": 3.397563518103176e-05,
        "epoch": 0.8685905694408658,
        "step": 6742
    },
    {
        "loss": 2.4713,
        "grad_norm": 2.1680171489715576,
        "learning_rate": 3.391802702368787e-05,
        "epoch": 0.8687194022159237,
        "step": 6743
    },
    {
        "loss": 0.7186,
        "grad_norm": 2.4576401710510254,
        "learning_rate": 3.386044266839693e-05,
        "epoch": 0.8688482349909817,
        "step": 6744
    },
    {
        "loss": 2.0629,
        "grad_norm": 3.2940444946289062,
        "learning_rate": 3.380288220038644e-05,
        "epoch": 0.8689770677660397,
        "step": 6745
    },
    {
        "loss": 2.2782,
        "grad_norm": 2.351917028427124,
        "learning_rate": 3.37453457048485e-05,
        "epoch": 0.8691059005410977,
        "step": 6746
    },
    {
        "loss": 2.4669,
        "grad_norm": 2.2579989433288574,
        "learning_rate": 3.368783326693981e-05,
        "epoch": 0.8692347333161556,
        "step": 6747
    },
    {
        "loss": 1.9895,
        "grad_norm": 1.7257102727890015,
        "learning_rate": 3.363034497178128e-05,
        "epoch": 0.8693635660912136,
        "step": 6748
    },
    {
        "loss": 2.0888,
        "grad_norm": 2.916372299194336,
        "learning_rate": 3.357288090445827e-05,
        "epoch": 0.8694923988662716,
        "step": 6749
    },
    {
        "loss": 1.0591,
        "grad_norm": 2.908536195755005,
        "learning_rate": 3.351544115002028e-05,
        "epoch": 0.8696212316413295,
        "step": 6750
    },
    {
        "loss": 2.1214,
        "grad_norm": 2.4966585636138916,
        "learning_rate": 3.345802579348056e-05,
        "epoch": 0.8697500644163876,
        "step": 6751
    },
    {
        "loss": 2.4601,
        "grad_norm": 2.3539819717407227,
        "learning_rate": 3.340063491981667e-05,
        "epoch": 0.8698788971914455,
        "step": 6752
    },
    {
        "loss": 1.6203,
        "grad_norm": 3.127155065536499,
        "learning_rate": 3.334326861396971e-05,
        "epoch": 0.8700077299665034,
        "step": 6753
    },
    {
        "loss": 1.8985,
        "grad_norm": 1.977695107460022,
        "learning_rate": 3.3285926960844296e-05,
        "epoch": 0.8701365627415615,
        "step": 6754
    },
    {
        "loss": 2.0383,
        "grad_norm": 1.839026689529419,
        "learning_rate": 3.322861004530875e-05,
        "epoch": 0.8702653955166194,
        "step": 6755
    },
    {
        "loss": 2.0041,
        "grad_norm": 1.9137688875198364,
        "learning_rate": 3.317131795219472e-05,
        "epoch": 0.8703942282916775,
        "step": 6756
    },
    {
        "loss": 0.991,
        "grad_norm": 2.806318998336792,
        "learning_rate": 3.311405076629712e-05,
        "epoch": 0.8705230610667354,
        "step": 6757
    },
    {
        "loss": 1.2522,
        "grad_norm": 2.763273000717163,
        "learning_rate": 3.305680857237406e-05,
        "epoch": 0.8706518938417933,
        "step": 6758
    },
    {
        "loss": 1.2772,
        "grad_norm": 3.0761382579803467,
        "learning_rate": 3.299959145514647e-05,
        "epoch": 0.8707807266168514,
        "step": 6759
    },
    {
        "loss": 1.7124,
        "grad_norm": 2.779207706451416,
        "learning_rate": 3.294239949929827e-05,
        "epoch": 0.8709095593919093,
        "step": 6760
    },
    {
        "loss": 2.2647,
        "grad_norm": 1.686597228050232,
        "learning_rate": 3.288523278947638e-05,
        "epoch": 0.8710383921669673,
        "step": 6761
    },
    {
        "loss": 1.1658,
        "grad_norm": 3.3129451274871826,
        "learning_rate": 3.282809141028993e-05,
        "epoch": 0.8711672249420253,
        "step": 6762
    },
    {
        "loss": 2.1789,
        "grad_norm": 1.4966074228286743,
        "learning_rate": 3.277097544631083e-05,
        "epoch": 0.8712960577170832,
        "step": 6763
    },
    {
        "loss": 2.2852,
        "grad_norm": 1.7260184288024902,
        "learning_rate": 3.271388498207337e-05,
        "epoch": 0.8714248904921412,
        "step": 6764
    },
    {
        "loss": 2.0772,
        "grad_norm": 1.984832525253296,
        "learning_rate": 3.2656820102073925e-05,
        "epoch": 0.8715537232671992,
        "step": 6765
    },
    {
        "loss": 1.7648,
        "grad_norm": 2.607602119445801,
        "learning_rate": 3.259978089077118e-05,
        "epoch": 0.8716825560422572,
        "step": 6766
    },
    {
        "loss": 2.0623,
        "grad_norm": 1.5881198644638062,
        "learning_rate": 3.254276743258575e-05,
        "epoch": 0.8718113888173151,
        "step": 6767
    },
    {
        "loss": 1.325,
        "grad_norm": 2.617189407348633,
        "learning_rate": 3.2485779811900185e-05,
        "epoch": 0.871940221592373,
        "step": 6768
    },
    {
        "loss": 1.8045,
        "grad_norm": 2.5042901039123535,
        "learning_rate": 3.2428818113058725e-05,
        "epoch": 0.8720690543674311,
        "step": 6769
    },
    {
        "loss": 1.3378,
        "grad_norm": 2.2762749195098877,
        "learning_rate": 3.237188242036729e-05,
        "epoch": 0.872197887142489,
        "step": 6770
    },
    {
        "loss": 1.5769,
        "grad_norm": 1.9264695644378662,
        "learning_rate": 3.2314972818093316e-05,
        "epoch": 0.8723267199175471,
        "step": 6771
    },
    {
        "loss": 1.2373,
        "grad_norm": 3.2784149646759033,
        "learning_rate": 3.2258089390465646e-05,
        "epoch": 0.872455552692605,
        "step": 6772
    },
    {
        "loss": 1.4304,
        "grad_norm": 2.3889658451080322,
        "learning_rate": 3.2201232221674235e-05,
        "epoch": 0.8725843854676629,
        "step": 6773
    },
    {
        "loss": 1.7848,
        "grad_norm": 2.841482400894165,
        "learning_rate": 3.214440139587036e-05,
        "epoch": 0.872713218242721,
        "step": 6774
    },
    {
        "loss": 2.4048,
        "grad_norm": 1.8203351497650146,
        "learning_rate": 3.208759699716628e-05,
        "epoch": 0.8728420510177789,
        "step": 6775
    },
    {
        "loss": 1.8085,
        "grad_norm": 3.0892181396484375,
        "learning_rate": 3.203081910963496e-05,
        "epoch": 0.8729708837928369,
        "step": 6776
    },
    {
        "loss": 2.6004,
        "grad_norm": 2.446901559829712,
        "learning_rate": 3.197406781731027e-05,
        "epoch": 0.8730997165678949,
        "step": 6777
    },
    {
        "loss": 1.5589,
        "grad_norm": 2.363193988800049,
        "learning_rate": 3.191734320418691e-05,
        "epoch": 0.8732285493429528,
        "step": 6778
    },
    {
        "loss": 2.2522,
        "grad_norm": 1.999310851097107,
        "learning_rate": 3.186064535421969e-05,
        "epoch": 0.8733573821180108,
        "step": 6779
    },
    {
        "loss": 2.262,
        "grad_norm": 2.4761900901794434,
        "learning_rate": 3.180397435132408e-05,
        "epoch": 0.8734862148930688,
        "step": 6780
    },
    {
        "loss": 2.4474,
        "grad_norm": 1.7536072731018066,
        "learning_rate": 3.174733027937574e-05,
        "epoch": 0.8736150476681268,
        "step": 6781
    },
    {
        "loss": 1.2553,
        "grad_norm": 3.940215587615967,
        "learning_rate": 3.169071322221049e-05,
        "epoch": 0.8737438804431847,
        "step": 6782
    },
    {
        "loss": 2.0593,
        "grad_norm": 1.6456313133239746,
        "learning_rate": 3.1634123263624204e-05,
        "epoch": 0.8738727132182427,
        "step": 6783
    },
    {
        "loss": 1.03,
        "grad_norm": 2.071641206741333,
        "learning_rate": 3.157756048737249e-05,
        "epoch": 0.8740015459933007,
        "step": 6784
    },
    {
        "loss": 1.927,
        "grad_norm": 2.3280909061431885,
        "learning_rate": 3.152102497717082e-05,
        "epoch": 0.8741303787683586,
        "step": 6785
    },
    {
        "loss": 1.7058,
        "grad_norm": 2.662269353866577,
        "learning_rate": 3.146451681669453e-05,
        "epoch": 0.8742592115434167,
        "step": 6786
    },
    {
        "loss": 1.49,
        "grad_norm": 2.7429487705230713,
        "learning_rate": 3.140803608957807e-05,
        "epoch": 0.8743880443184746,
        "step": 6787
    },
    {
        "loss": 2.1868,
        "grad_norm": 2.527221202850342,
        "learning_rate": 3.1351582879415556e-05,
        "epoch": 0.8745168770935325,
        "step": 6788
    },
    {
        "loss": 1.4133,
        "grad_norm": 2.8862175941467285,
        "learning_rate": 3.129515726976038e-05,
        "epoch": 0.8746457098685906,
        "step": 6789
    },
    {
        "loss": 1.9689,
        "grad_norm": 1.8637462854385376,
        "learning_rate": 3.123875934412488e-05,
        "epoch": 0.8747745426436485,
        "step": 6790
    },
    {
        "loss": 1.7861,
        "grad_norm": 2.9818263053894043,
        "learning_rate": 3.118238918598063e-05,
        "epoch": 0.8749033754187066,
        "step": 6791
    },
    {
        "loss": 2.5446,
        "grad_norm": 2.082127571105957,
        "learning_rate": 3.1126046878758043e-05,
        "epoch": 0.8750322081937645,
        "step": 6792
    },
    {
        "loss": 2.0413,
        "grad_norm": 2.144098997116089,
        "learning_rate": 3.106973250584628e-05,
        "epoch": 0.8751610409688225,
        "step": 6793
    },
    {
        "loss": 1.0083,
        "grad_norm": 2.7610509395599365,
        "learning_rate": 3.10134461505932e-05,
        "epoch": 0.8752898737438805,
        "step": 6794
    },
    {
        "loss": 1.8695,
        "grad_norm": 1.999421238899231,
        "learning_rate": 3.095718789630514e-05,
        "epoch": 0.8754187065189384,
        "step": 6795
    },
    {
        "loss": 1.7042,
        "grad_norm": 2.2665536403656006,
        "learning_rate": 3.090095782624691e-05,
        "epoch": 0.8755475392939964,
        "step": 6796
    },
    {
        "loss": 2.1983,
        "grad_norm": 3.4195525646209717,
        "learning_rate": 3.084475602364164e-05,
        "epoch": 0.8756763720690544,
        "step": 6797
    },
    {
        "loss": 2.274,
        "grad_norm": 1.7763491868972778,
        "learning_rate": 3.078858257167042e-05,
        "epoch": 0.8758052048441124,
        "step": 6798
    },
    {
        "loss": 1.6962,
        "grad_norm": 2.2384250164031982,
        "learning_rate": 3.073243755347259e-05,
        "epoch": 0.8759340376191703,
        "step": 6799
    },
    {
        "loss": 2.1475,
        "grad_norm": 1.8151768445968628,
        "learning_rate": 3.06763210521454e-05,
        "epoch": 0.8760628703942283,
        "step": 6800
    },
    {
        "loss": 1.8856,
        "grad_norm": 2.18298602104187,
        "learning_rate": 3.0620233150743695e-05,
        "epoch": 0.8761917031692863,
        "step": 6801
    },
    {
        "loss": 2.0297,
        "grad_norm": 2.2700817584991455,
        "learning_rate": 3.056417393228012e-05,
        "epoch": 0.8763205359443442,
        "step": 6802
    },
    {
        "loss": 2.1961,
        "grad_norm": 1.9568628072738647,
        "learning_rate": 3.0508143479725093e-05,
        "epoch": 0.8764493687194023,
        "step": 6803
    },
    {
        "loss": 2.3989,
        "grad_norm": 1.5816664695739746,
        "learning_rate": 3.0452141876006022e-05,
        "epoch": 0.8765782014944602,
        "step": 6804
    },
    {
        "loss": 2.1423,
        "grad_norm": 1.7043415307998657,
        "learning_rate": 3.0396169204007896e-05,
        "epoch": 0.8767070342695181,
        "step": 6805
    },
    {
        "loss": 2.4752,
        "grad_norm": 2.428528308868408,
        "learning_rate": 3.034022554657283e-05,
        "epoch": 0.8768358670445762,
        "step": 6806
    },
    {
        "loss": 1.6149,
        "grad_norm": 2.0667431354522705,
        "learning_rate": 3.0284310986499986e-05,
        "epoch": 0.8769646998196341,
        "step": 6807
    },
    {
        "loss": 1.7342,
        "grad_norm": 2.249305486679077,
        "learning_rate": 3.022842560654552e-05,
        "epoch": 0.8770935325946921,
        "step": 6808
    },
    {
        "loss": 1.6041,
        "grad_norm": 3.0527474880218506,
        "learning_rate": 3.017256948942222e-05,
        "epoch": 0.8772223653697501,
        "step": 6809
    },
    {
        "loss": 1.6377,
        "grad_norm": 3.0886788368225098,
        "learning_rate": 3.0116742717799745e-05,
        "epoch": 0.877351198144808,
        "step": 6810
    },
    {
        "loss": 2.0282,
        "grad_norm": 1.7883204221725464,
        "learning_rate": 3.006094537430426e-05,
        "epoch": 0.877480030919866,
        "step": 6811
    },
    {
        "loss": 1.8609,
        "grad_norm": 1.9993467330932617,
        "learning_rate": 3.0005177541518374e-05,
        "epoch": 0.877608863694924,
        "step": 6812
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.8778095245361328,
        "learning_rate": 2.994943930198103e-05,
        "epoch": 0.877737696469982,
        "step": 6813
    },
    {
        "loss": 2.2943,
        "grad_norm": 1.844181776046753,
        "learning_rate": 2.989373073818742e-05,
        "epoch": 0.8778665292450399,
        "step": 6814
    },
    {
        "loss": 1.3603,
        "grad_norm": 2.9498045444488525,
        "learning_rate": 2.983805193258863e-05,
        "epoch": 0.8779953620200979,
        "step": 6815
    },
    {
        "loss": 1.8447,
        "grad_norm": 2.704132318496704,
        "learning_rate": 2.9782402967591892e-05,
        "epoch": 0.8781241947951559,
        "step": 6816
    },
    {
        "loss": 2.3858,
        "grad_norm": 1.7305231094360352,
        "learning_rate": 2.9726783925560225e-05,
        "epoch": 0.8782530275702138,
        "step": 6817
    },
    {
        "loss": 2.0311,
        "grad_norm": 2.4655327796936035,
        "learning_rate": 2.967119488881234e-05,
        "epoch": 0.8783818603452719,
        "step": 6818
    },
    {
        "loss": 1.837,
        "grad_norm": 2.6103711128234863,
        "learning_rate": 2.9615635939622594e-05,
        "epoch": 0.8785106931203298,
        "step": 6819
    },
    {
        "loss": 1.4907,
        "grad_norm": 2.3243019580841064,
        "learning_rate": 2.9560107160220586e-05,
        "epoch": 0.8786395258953877,
        "step": 6820
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.5947691202163696,
        "learning_rate": 2.9504608632791624e-05,
        "epoch": 0.8787683586704458,
        "step": 6821
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.4985123872756958,
        "learning_rate": 2.9449140439476064e-05,
        "epoch": 0.8788971914455037,
        "step": 6822
    },
    {
        "loss": 1.9015,
        "grad_norm": 2.178577184677124,
        "learning_rate": 2.9393702662369226e-05,
        "epoch": 0.8790260242205618,
        "step": 6823
    },
    {
        "loss": 1.4063,
        "grad_norm": 2.407377243041992,
        "learning_rate": 2.9338295383521624e-05,
        "epoch": 0.8791548569956197,
        "step": 6824
    },
    {
        "loss": 1.8025,
        "grad_norm": 2.4350008964538574,
        "learning_rate": 2.9282918684938613e-05,
        "epoch": 0.8792836897706776,
        "step": 6825
    },
    {
        "loss": 1.7099,
        "grad_norm": 2.714341402053833,
        "learning_rate": 2.9227572648580105e-05,
        "epoch": 0.8794125225457357,
        "step": 6826
    },
    {
        "loss": 1.9898,
        "grad_norm": 2.7865633964538574,
        "learning_rate": 2.9172257356360792e-05,
        "epoch": 0.8795413553207936,
        "step": 6827
    },
    {
        "loss": 2.1372,
        "grad_norm": 1.6702728271484375,
        "learning_rate": 2.9116972890149875e-05,
        "epoch": 0.8796701880958516,
        "step": 6828
    },
    {
        "loss": 1.7486,
        "grad_norm": 2.882272481918335,
        "learning_rate": 2.9061719331770854e-05,
        "epoch": 0.8797990208709096,
        "step": 6829
    },
    {
        "loss": 1.624,
        "grad_norm": 2.3831703662872314,
        "learning_rate": 2.9006496763001516e-05,
        "epoch": 0.8799278536459675,
        "step": 6830
    },
    {
        "loss": 1.55,
        "grad_norm": 3.2346582412719727,
        "learning_rate": 2.8951305265573813e-05,
        "epoch": 0.8800566864210255,
        "step": 6831
    },
    {
        "loss": 1.3752,
        "grad_norm": 4.190098285675049,
        "learning_rate": 2.8896144921173622e-05,
        "epoch": 0.8801855191960835,
        "step": 6832
    },
    {
        "loss": 2.3613,
        "grad_norm": 2.054912805557251,
        "learning_rate": 2.884101581144085e-05,
        "epoch": 0.8803143519711415,
        "step": 6833
    },
    {
        "loss": 1.9023,
        "grad_norm": 3.6171555519104004,
        "learning_rate": 2.878591801796898e-05,
        "epoch": 0.8804431847461994,
        "step": 6834
    },
    {
        "loss": 1.6553,
        "grad_norm": 2.13535475730896,
        "learning_rate": 2.873085162230532e-05,
        "epoch": 0.8805720175212574,
        "step": 6835
    },
    {
        "loss": 1.8462,
        "grad_norm": 2.84232234954834,
        "learning_rate": 2.8675816705950687e-05,
        "epoch": 0.8807008502963154,
        "step": 6836
    },
    {
        "loss": 1.0856,
        "grad_norm": 3.7306127548217773,
        "learning_rate": 2.86208133503591e-05,
        "epoch": 0.8808296830713733,
        "step": 6837
    },
    {
        "loss": 1.9961,
        "grad_norm": 2.2766189575195312,
        "learning_rate": 2.8565841636938208e-05,
        "epoch": 0.8809585158464314,
        "step": 6838
    },
    {
        "loss": 2.2097,
        "grad_norm": 1.8897992372512817,
        "learning_rate": 2.851090164704865e-05,
        "epoch": 0.8810873486214893,
        "step": 6839
    },
    {
        "loss": 1.7448,
        "grad_norm": 2.6693947315216064,
        "learning_rate": 2.8455993462003998e-05,
        "epoch": 0.8812161813965472,
        "step": 6840
    },
    {
        "loss": 2.0162,
        "grad_norm": 2.7636964321136475,
        "learning_rate": 2.8401117163070927e-05,
        "epoch": 0.8813450141716053,
        "step": 6841
    },
    {
        "loss": 2.2295,
        "grad_norm": 2.4787349700927734,
        "learning_rate": 2.8346272831468866e-05,
        "epoch": 0.8814738469466632,
        "step": 6842
    },
    {
        "loss": 1.1027,
        "grad_norm": 3.7853572368621826,
        "learning_rate": 2.829146054836992e-05,
        "epoch": 0.8816026797217212,
        "step": 6843
    },
    {
        "loss": 1.7543,
        "grad_norm": 2.2989490032196045,
        "learning_rate": 2.8236680394898817e-05,
        "epoch": 0.8817315124967792,
        "step": 6844
    },
    {
        "loss": 1.8843,
        "grad_norm": 1.8587067127227783,
        "learning_rate": 2.818193245213252e-05,
        "epoch": 0.8818603452718372,
        "step": 6845
    },
    {
        "loss": 0.694,
        "grad_norm": 3.0367376804351807,
        "learning_rate": 2.8127216801100652e-05,
        "epoch": 0.8819891780468951,
        "step": 6846
    },
    {
        "loss": 2.3155,
        "grad_norm": 2.4315097332000732,
        "learning_rate": 2.8072533522784882e-05,
        "epoch": 0.8821180108219531,
        "step": 6847
    },
    {
        "loss": 1.1074,
        "grad_norm": 2.4828500747680664,
        "learning_rate": 2.80178826981188e-05,
        "epoch": 0.8822468435970111,
        "step": 6848
    },
    {
        "loss": 2.1588,
        "grad_norm": 2.189023733139038,
        "learning_rate": 2.796326440798821e-05,
        "epoch": 0.882375676372069,
        "step": 6849
    },
    {
        "loss": 1.41,
        "grad_norm": 2.3652329444885254,
        "learning_rate": 2.790867873323071e-05,
        "epoch": 0.8825045091471271,
        "step": 6850
    },
    {
        "loss": 2.0426,
        "grad_norm": 2.711266040802002,
        "learning_rate": 2.7854125754635473e-05,
        "epoch": 0.882633341922185,
        "step": 6851
    },
    {
        "loss": 2.1733,
        "grad_norm": 1.7730244398117065,
        "learning_rate": 2.7799605552943465e-05,
        "epoch": 0.8827621746972429,
        "step": 6852
    },
    {
        "loss": 2.0488,
        "grad_norm": 2.360647678375244,
        "learning_rate": 2.7745118208847055e-05,
        "epoch": 0.882891007472301,
        "step": 6853
    },
    {
        "loss": 2.1275,
        "grad_norm": 3.035801649093628,
        "learning_rate": 2.7690663802989996e-05,
        "epoch": 0.8830198402473589,
        "step": 6854
    },
    {
        "loss": 1.8593,
        "grad_norm": 2.287290096282959,
        "learning_rate": 2.76362424159673e-05,
        "epoch": 0.883148673022417,
        "step": 6855
    },
    {
        "loss": 1.8052,
        "grad_norm": 2.5857231616973877,
        "learning_rate": 2.7581854128325097e-05,
        "epoch": 0.8832775057974749,
        "step": 6856
    },
    {
        "loss": 1.8856,
        "grad_norm": 2.7542192935943604,
        "learning_rate": 2.7527499020560526e-05,
        "epoch": 0.8834063385725328,
        "step": 6857
    },
    {
        "loss": 2.2471,
        "grad_norm": 2.25287127494812,
        "learning_rate": 2.747317717312169e-05,
        "epoch": 0.8835351713475909,
        "step": 6858
    },
    {
        "loss": 2.3537,
        "grad_norm": 1.3094327449798584,
        "learning_rate": 2.7418888666407284e-05,
        "epoch": 0.8836640041226488,
        "step": 6859
    },
    {
        "loss": 2.322,
        "grad_norm": 1.7272257804870605,
        "learning_rate": 2.7364633580766815e-05,
        "epoch": 0.8837928368977068,
        "step": 6860
    },
    {
        "loss": 1.6205,
        "grad_norm": 2.932560682296753,
        "learning_rate": 2.7310411996500373e-05,
        "epoch": 0.8839216696727648,
        "step": 6861
    },
    {
        "loss": 2.2541,
        "grad_norm": 1.909941554069519,
        "learning_rate": 2.7256223993858144e-05,
        "epoch": 0.8840505024478227,
        "step": 6862
    },
    {
        "loss": 1.5783,
        "grad_norm": 2.874556064605713,
        "learning_rate": 2.720206965304106e-05,
        "epoch": 0.8841793352228807,
        "step": 6863
    },
    {
        "loss": 1.8591,
        "grad_norm": 2.769301176071167,
        "learning_rate": 2.7147949054200028e-05,
        "epoch": 0.8843081679979387,
        "step": 6864
    },
    {
        "loss": 1.7061,
        "grad_norm": 1.8077235221862793,
        "learning_rate": 2.7093862277435844e-05,
        "epoch": 0.8844370007729967,
        "step": 6865
    },
    {
        "loss": 1.8815,
        "grad_norm": 2.749812602996826,
        "learning_rate": 2.7039809402799467e-05,
        "epoch": 0.8845658335480546,
        "step": 6866
    },
    {
        "loss": 1.7735,
        "grad_norm": 1.6182212829589844,
        "learning_rate": 2.6985790510291632e-05,
        "epoch": 0.8846946663231126,
        "step": 6867
    },
    {
        "loss": 1.5108,
        "grad_norm": 2.2839291095733643,
        "learning_rate": 2.6931805679862754e-05,
        "epoch": 0.8848234990981706,
        "step": 6868
    },
    {
        "loss": 2.2989,
        "grad_norm": 1.8501272201538086,
        "learning_rate": 2.6877854991412898e-05,
        "epoch": 0.8849523318732285,
        "step": 6869
    },
    {
        "loss": 2.5301,
        "grad_norm": 1.691141963005066,
        "learning_rate": 2.6823938524791426e-05,
        "epoch": 0.8850811646482866,
        "step": 6870
    },
    {
        "loss": 2.1008,
        "grad_norm": 2.0470104217529297,
        "learning_rate": 2.6770056359797173e-05,
        "epoch": 0.8852099974233445,
        "step": 6871
    },
    {
        "loss": 1.744,
        "grad_norm": 2.289766788482666,
        "learning_rate": 2.671620857617837e-05,
        "epoch": 0.8853388301984024,
        "step": 6872
    },
    {
        "loss": 2.2466,
        "grad_norm": 1.9506174325942993,
        "learning_rate": 2.6662395253632012e-05,
        "epoch": 0.8854676629734605,
        "step": 6873
    },
    {
        "loss": 2.039,
        "grad_norm": 3.4622504711151123,
        "learning_rate": 2.6608616471804336e-05,
        "epoch": 0.8855964957485184,
        "step": 6874
    },
    {
        "loss": 1.2229,
        "grad_norm": 3.076779842376709,
        "learning_rate": 2.655487231029044e-05,
        "epoch": 0.8857253285235764,
        "step": 6875
    },
    {
        "loss": 2.3226,
        "grad_norm": 2.681326389312744,
        "learning_rate": 2.650116284863402e-05,
        "epoch": 0.8858541612986344,
        "step": 6876
    },
    {
        "loss": 1.8374,
        "grad_norm": 5.7574238777160645,
        "learning_rate": 2.644748816632758e-05,
        "epoch": 0.8859829940736923,
        "step": 6877
    },
    {
        "loss": 0.8513,
        "grad_norm": 2.8743202686309814,
        "learning_rate": 2.6393848342812105e-05,
        "epoch": 0.8861118268487503,
        "step": 6878
    },
    {
        "loss": 1.8486,
        "grad_norm": 3.2283663749694824,
        "learning_rate": 2.6340243457476975e-05,
        "epoch": 0.8862406596238083,
        "step": 6879
    },
    {
        "loss": 2.2028,
        "grad_norm": 1.9965848922729492,
        "learning_rate": 2.628667358965986e-05,
        "epoch": 0.8863694923988663,
        "step": 6880
    },
    {
        "loss": 2.2862,
        "grad_norm": 2.8350954055786133,
        "learning_rate": 2.623313881864663e-05,
        "epoch": 0.8864983251739242,
        "step": 6881
    },
    {
        "loss": 2.2335,
        "grad_norm": 1.9450525045394897,
        "learning_rate": 2.6179639223671138e-05,
        "epoch": 0.8866271579489822,
        "step": 6882
    },
    {
        "loss": 2.2151,
        "grad_norm": 2.0912551879882812,
        "learning_rate": 2.6126174883915316e-05,
        "epoch": 0.8867559907240402,
        "step": 6883
    },
    {
        "loss": 1.4638,
        "grad_norm": 2.366912841796875,
        "learning_rate": 2.6072745878508697e-05,
        "epoch": 0.8868848234990981,
        "step": 6884
    },
    {
        "loss": 2.1448,
        "grad_norm": 2.016737937927246,
        "learning_rate": 2.6019352286528697e-05,
        "epoch": 0.8870136562741562,
        "step": 6885
    },
    {
        "loss": 1.3948,
        "grad_norm": 2.232954263687134,
        "learning_rate": 2.5965994187000343e-05,
        "epoch": 0.8871424890492141,
        "step": 6886
    },
    {
        "loss": 2.1317,
        "grad_norm": 2.2094264030456543,
        "learning_rate": 2.5912671658895916e-05,
        "epoch": 0.887271321824272,
        "step": 6887
    },
    {
        "loss": 2.2054,
        "grad_norm": 1.8489049673080444,
        "learning_rate": 2.5859384781135205e-05,
        "epoch": 0.8874001545993301,
        "step": 6888
    },
    {
        "loss": 2.0219,
        "grad_norm": 2.0543766021728516,
        "learning_rate": 2.5806133632585385e-05,
        "epoch": 0.887528987374388,
        "step": 6889
    },
    {
        "loss": 1.488,
        "grad_norm": 2.1775031089782715,
        "learning_rate": 2.57529182920604e-05,
        "epoch": 0.887657820149446,
        "step": 6890
    },
    {
        "loss": 1.9925,
        "grad_norm": 2.292226552963257,
        "learning_rate": 2.5699738838321452e-05,
        "epoch": 0.887786652924504,
        "step": 6891
    },
    {
        "loss": 1.1764,
        "grad_norm": 2.769998073577881,
        "learning_rate": 2.564659535007654e-05,
        "epoch": 0.887915485699562,
        "step": 6892
    },
    {
        "loss": 2.0689,
        "grad_norm": 2.7111594676971436,
        "learning_rate": 2.5593487905980473e-05,
        "epoch": 0.88804431847462,
        "step": 6893
    },
    {
        "loss": 1.6145,
        "grad_norm": 2.7669408321380615,
        "learning_rate": 2.5540416584634724e-05,
        "epoch": 0.8881731512496779,
        "step": 6894
    },
    {
        "loss": 1.8391,
        "grad_norm": 2.5600016117095947,
        "learning_rate": 2.5487381464587167e-05,
        "epoch": 0.8883019840247359,
        "step": 6895
    },
    {
        "loss": 1.3052,
        "grad_norm": 2.2490744590759277,
        "learning_rate": 2.543438262433226e-05,
        "epoch": 0.8884308167997939,
        "step": 6896
    },
    {
        "loss": 0.8014,
        "grad_norm": 2.6968345642089844,
        "learning_rate": 2.5381420142310675e-05,
        "epoch": 0.8885596495748519,
        "step": 6897
    },
    {
        "loss": 2.1294,
        "grad_norm": 1.2715400457382202,
        "learning_rate": 2.5328494096909304e-05,
        "epoch": 0.8886884823499098,
        "step": 6898
    },
    {
        "loss": 2.1891,
        "grad_norm": 2.3039984703063965,
        "learning_rate": 2.527560456646113e-05,
        "epoch": 0.8888173151249678,
        "step": 6899
    },
    {
        "loss": 2.0313,
        "grad_norm": 2.67460560798645,
        "learning_rate": 2.5222751629245108e-05,
        "epoch": 0.8889461479000258,
        "step": 6900
    },
    {
        "loss": 1.1052,
        "grad_norm": 2.5084052085876465,
        "learning_rate": 2.516993536348588e-05,
        "epoch": 0.8890749806750837,
        "step": 6901
    },
    {
        "loss": 2.2263,
        "grad_norm": 2.1073412895202637,
        "learning_rate": 2.511715584735399e-05,
        "epoch": 0.8892038134501418,
        "step": 6902
    },
    {
        "loss": 1.7999,
        "grad_norm": 2.1762092113494873,
        "learning_rate": 2.506441315896553e-05,
        "epoch": 0.8893326462251997,
        "step": 6903
    },
    {
        "loss": 2.159,
        "grad_norm": 1.7304623126983643,
        "learning_rate": 2.5011707376382064e-05,
        "epoch": 0.8894614790002576,
        "step": 6904
    },
    {
        "loss": 1.6807,
        "grad_norm": 1.7236889600753784,
        "learning_rate": 2.4959038577610578e-05,
        "epoch": 0.8895903117753157,
        "step": 6905
    },
    {
        "loss": 2.3444,
        "grad_norm": 2.531738519668579,
        "learning_rate": 2.4906406840603282e-05,
        "epoch": 0.8897191445503736,
        "step": 6906
    },
    {
        "loss": 1.0406,
        "grad_norm": 2.609487533569336,
        "learning_rate": 2.485381224325754e-05,
        "epoch": 0.8898479773254316,
        "step": 6907
    },
    {
        "loss": 1.7546,
        "grad_norm": 2.880812644958496,
        "learning_rate": 2.4801254863415825e-05,
        "epoch": 0.8899768101004896,
        "step": 6908
    },
    {
        "loss": 2.2238,
        "grad_norm": 1.876666784286499,
        "learning_rate": 2.4748734778865323e-05,
        "epoch": 0.8901056428755475,
        "step": 6909
    },
    {
        "loss": 1.9147,
        "grad_norm": 3.1725428104400635,
        "learning_rate": 2.4696252067338223e-05,
        "epoch": 0.8902344756506055,
        "step": 6910
    },
    {
        "loss": 2.0803,
        "grad_norm": 2.376375675201416,
        "learning_rate": 2.46438068065114e-05,
        "epoch": 0.8903633084256635,
        "step": 6911
    },
    {
        "loss": 1.9449,
        "grad_norm": 2.4666318893432617,
        "learning_rate": 2.4591399074006082e-05,
        "epoch": 0.8904921412007215,
        "step": 6912
    },
    {
        "loss": 1.5939,
        "grad_norm": 3.3143937587738037,
        "learning_rate": 2.4539028947388178e-05,
        "epoch": 0.8906209739757794,
        "step": 6913
    },
    {
        "loss": 1.6234,
        "grad_norm": 2.6769325733184814,
        "learning_rate": 2.448669650416785e-05,
        "epoch": 0.8907498067508374,
        "step": 6914
    },
    {
        "loss": 1.9342,
        "grad_norm": 2.2111544609069824,
        "learning_rate": 2.4434401821799518e-05,
        "epoch": 0.8908786395258954,
        "step": 6915
    },
    {
        "loss": 1.7029,
        "grad_norm": 2.7502589225769043,
        "learning_rate": 2.4382144977681686e-05,
        "epoch": 0.8910074723009533,
        "step": 6916
    },
    {
        "loss": 2.0719,
        "grad_norm": 2.8213984966278076,
        "learning_rate": 2.4329926049156866e-05,
        "epoch": 0.8911363050760114,
        "step": 6917
    },
    {
        "loss": 2.487,
        "grad_norm": 1.9785220623016357,
        "learning_rate": 2.4277745113511447e-05,
        "epoch": 0.8912651378510693,
        "step": 6918
    },
    {
        "loss": 2.1458,
        "grad_norm": 2.0772271156311035,
        "learning_rate": 2.4225602247975654e-05,
        "epoch": 0.8913939706261272,
        "step": 6919
    },
    {
        "loss": 2.0125,
        "grad_norm": 1.5149565935134888,
        "learning_rate": 2.4173497529723194e-05,
        "epoch": 0.8915228034011853,
        "step": 6920
    },
    {
        "loss": 1.956,
        "grad_norm": 2.3413476943969727,
        "learning_rate": 2.4121431035871473e-05,
        "epoch": 0.8916516361762432,
        "step": 6921
    },
    {
        "loss": 1.3252,
        "grad_norm": 2.7580337524414062,
        "learning_rate": 2.406940284348128e-05,
        "epoch": 0.8917804689513013,
        "step": 6922
    },
    {
        "loss": 1.9022,
        "grad_norm": 3.066373348236084,
        "learning_rate": 2.4017413029556717e-05,
        "epoch": 0.8919093017263592,
        "step": 6923
    },
    {
        "loss": 1.3871,
        "grad_norm": 2.7520487308502197,
        "learning_rate": 2.396546167104507e-05,
        "epoch": 0.8920381345014171,
        "step": 6924
    },
    {
        "loss": 2.2636,
        "grad_norm": 3.006641387939453,
        "learning_rate": 2.3913548844836776e-05,
        "epoch": 0.8921669672764752,
        "step": 6925
    },
    {
        "loss": 0.6129,
        "grad_norm": 3.3509104251861572,
        "learning_rate": 2.3861674627765067e-05,
        "epoch": 0.8922958000515331,
        "step": 6926
    },
    {
        "loss": 0.9393,
        "grad_norm": 4.419435501098633,
        "learning_rate": 2.3809839096606205e-05,
        "epoch": 0.8924246328265911,
        "step": 6927
    },
    {
        "loss": 1.5377,
        "grad_norm": 3.1661617755889893,
        "learning_rate": 2.375804232807915e-05,
        "epoch": 0.892553465601649,
        "step": 6928
    },
    {
        "loss": 2.6884,
        "grad_norm": 1.9978997707366943,
        "learning_rate": 2.3706284398845473e-05,
        "epoch": 0.892682298376707,
        "step": 6929
    },
    {
        "loss": 2.3355,
        "grad_norm": 2.2515203952789307,
        "learning_rate": 2.3654565385509343e-05,
        "epoch": 0.892811131151765,
        "step": 6930
    },
    {
        "loss": 1.9789,
        "grad_norm": 2.4130406379699707,
        "learning_rate": 2.3602885364617045e-05,
        "epoch": 0.892939963926823,
        "step": 6931
    },
    {
        "loss": 2.3213,
        "grad_norm": 2.6932055950164795,
        "learning_rate": 2.3551244412657558e-05,
        "epoch": 0.893068796701881,
        "step": 6932
    },
    {
        "loss": 1.9969,
        "grad_norm": 3.2584991455078125,
        "learning_rate": 2.349964260606185e-05,
        "epoch": 0.8931976294769389,
        "step": 6933
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.4874825477600098,
        "learning_rate": 2.3448080021202822e-05,
        "epoch": 0.8933264622519969,
        "step": 6934
    },
    {
        "loss": 1.875,
        "grad_norm": 1.8771164417266846,
        "learning_rate": 2.339655673439552e-05,
        "epoch": 0.8934552950270549,
        "step": 6935
    },
    {
        "loss": 1.6694,
        "grad_norm": 2.0268709659576416,
        "learning_rate": 2.3345072821896802e-05,
        "epoch": 0.8935841278021128,
        "step": 6936
    },
    {
        "loss": 2.5223,
        "grad_norm": 2.339674711227417,
        "learning_rate": 2.3293628359905095e-05,
        "epoch": 0.8937129605771709,
        "step": 6937
    },
    {
        "loss": 1.8416,
        "grad_norm": 2.382188320159912,
        "learning_rate": 2.3242223424560606e-05,
        "epoch": 0.8938417933522288,
        "step": 6938
    },
    {
        "loss": 2.1998,
        "grad_norm": 2.6592907905578613,
        "learning_rate": 2.319085809194498e-05,
        "epoch": 0.8939706261272867,
        "step": 6939
    },
    {
        "loss": 2.1111,
        "grad_norm": 2.077686071395874,
        "learning_rate": 2.313953243808125e-05,
        "epoch": 0.8940994589023448,
        "step": 6940
    },
    {
        "loss": 2.228,
        "grad_norm": 1.626186728477478,
        "learning_rate": 2.3088246538933728e-05,
        "epoch": 0.8942282916774027,
        "step": 6941
    },
    {
        "loss": 2.4191,
        "grad_norm": 1.5986863374710083,
        "learning_rate": 2.3037000470407875e-05,
        "epoch": 0.8943571244524607,
        "step": 6942
    },
    {
        "loss": 1.6914,
        "grad_norm": 2.863316774368286,
        "learning_rate": 2.2985794308350218e-05,
        "epoch": 0.8944859572275187,
        "step": 6943
    },
    {
        "loss": 1.03,
        "grad_norm": 2.7371740341186523,
        "learning_rate": 2.2934628128548253e-05,
        "epoch": 0.8946147900025767,
        "step": 6944
    },
    {
        "loss": 1.1616,
        "grad_norm": 2.5522301197052,
        "learning_rate": 2.2883502006730155e-05,
        "epoch": 0.8947436227776346,
        "step": 6945
    },
    {
        "loss": 1.5875,
        "grad_norm": 2.1569457054138184,
        "learning_rate": 2.283241601856499e-05,
        "epoch": 0.8948724555526926,
        "step": 6946
    },
    {
        "loss": 1.5992,
        "grad_norm": 2.6501951217651367,
        "learning_rate": 2.2781370239662358e-05,
        "epoch": 0.8950012883277506,
        "step": 6947
    },
    {
        "loss": 1.1684,
        "grad_norm": 2.238842010498047,
        "learning_rate": 2.2730364745572203e-05,
        "epoch": 0.8951301211028085,
        "step": 6948
    },
    {
        "loss": 1.8429,
        "grad_norm": 2.8475329875946045,
        "learning_rate": 2.267939961178514e-05,
        "epoch": 0.8952589538778666,
        "step": 6949
    },
    {
        "loss": 1.0923,
        "grad_norm": 3.334848165512085,
        "learning_rate": 2.262847491373189e-05,
        "epoch": 0.8953877866529245,
        "step": 6950
    },
    {
        "loss": 1.4187,
        "grad_norm": 2.612884998321533,
        "learning_rate": 2.2577590726783215e-05,
        "epoch": 0.8955166194279824,
        "step": 6951
    },
    {
        "loss": 1.9408,
        "grad_norm": 2.8092336654663086,
        "learning_rate": 2.2526747126250092e-05,
        "epoch": 0.8956454522030405,
        "step": 6952
    },
    {
        "loss": 2.0419,
        "grad_norm": 2.5891613960266113,
        "learning_rate": 2.2475944187383363e-05,
        "epoch": 0.8957742849780984,
        "step": 6953
    },
    {
        "loss": 1.7055,
        "grad_norm": 1.8469983339309692,
        "learning_rate": 2.2425181985373693e-05,
        "epoch": 0.8959031177531565,
        "step": 6954
    },
    {
        "loss": 2.135,
        "grad_norm": 2.6630947589874268,
        "learning_rate": 2.237446059535151e-05,
        "epoch": 0.8960319505282144,
        "step": 6955
    },
    {
        "loss": 1.728,
        "grad_norm": 1.9573845863342285,
        "learning_rate": 2.2323780092386676e-05,
        "epoch": 0.8961607833032723,
        "step": 6956
    },
    {
        "loss": 2.2194,
        "grad_norm": 2.274374485015869,
        "learning_rate": 2.2273140551488635e-05,
        "epoch": 0.8962896160783304,
        "step": 6957
    },
    {
        "loss": 1.5781,
        "grad_norm": 2.4695777893066406,
        "learning_rate": 2.2222542047606397e-05,
        "epoch": 0.8964184488533883,
        "step": 6958
    },
    {
        "loss": 1.8305,
        "grad_norm": 2.549041509628296,
        "learning_rate": 2.2171984655627875e-05,
        "epoch": 0.8965472816284463,
        "step": 6959
    },
    {
        "loss": 1.8948,
        "grad_norm": 2.0221195220947266,
        "learning_rate": 2.212146845038037e-05,
        "epoch": 0.8966761144035043,
        "step": 6960
    },
    {
        "loss": 1.2148,
        "grad_norm": 1.8601899147033691,
        "learning_rate": 2.207099350663025e-05,
        "epoch": 0.8968049471785622,
        "step": 6961
    },
    {
        "loss": 1.1218,
        "grad_norm": 2.7144408226013184,
        "learning_rate": 2.2020559899082578e-05,
        "epoch": 0.8969337799536202,
        "step": 6962
    },
    {
        "loss": 1.7163,
        "grad_norm": 2.905665159225464,
        "learning_rate": 2.197016770238146e-05,
        "epoch": 0.8970626127286782,
        "step": 6963
    },
    {
        "loss": 1.4279,
        "grad_norm": 2.319317102432251,
        "learning_rate": 2.191981699110966e-05,
        "epoch": 0.8971914455037362,
        "step": 6964
    },
    {
        "loss": 1.6998,
        "grad_norm": 3.3197197914123535,
        "learning_rate": 2.1869507839788515e-05,
        "epoch": 0.8973202782787941,
        "step": 6965
    },
    {
        "loss": 2.1287,
        "grad_norm": 1.7439745664596558,
        "learning_rate": 2.181924032287786e-05,
        "epoch": 0.8974491110538521,
        "step": 6966
    },
    {
        "loss": 1.6649,
        "grad_norm": 3.1618597507476807,
        "learning_rate": 2.176901451477592e-05,
        "epoch": 0.8975779438289101,
        "step": 6967
    },
    {
        "loss": 1.9007,
        "grad_norm": 1.672308087348938,
        "learning_rate": 2.171883048981917e-05,
        "epoch": 0.897706776603968,
        "step": 6968
    },
    {
        "loss": 1.1405,
        "grad_norm": 2.518838405609131,
        "learning_rate": 2.1668688322282316e-05,
        "epoch": 0.8978356093790261,
        "step": 6969
    },
    {
        "loss": 2.4648,
        "grad_norm": 2.1939496994018555,
        "learning_rate": 2.1618588086377934e-05,
        "epoch": 0.897964442154084,
        "step": 6970
    },
    {
        "loss": 1.6207,
        "grad_norm": 2.7076773643493652,
        "learning_rate": 2.156852985625672e-05,
        "epoch": 0.8980932749291419,
        "step": 6971
    },
    {
        "loss": 1.8419,
        "grad_norm": 2.1756696701049805,
        "learning_rate": 2.1518513706007186e-05,
        "epoch": 0.8982221077042,
        "step": 6972
    },
    {
        "loss": 1.8905,
        "grad_norm": 1.3686248064041138,
        "learning_rate": 2.146853970965541e-05,
        "epoch": 0.8983509404792579,
        "step": 6973
    },
    {
        "loss": 2.0184,
        "grad_norm": 2.65606689453125,
        "learning_rate": 2.1418607941165177e-05,
        "epoch": 0.8984797732543159,
        "step": 6974
    },
    {
        "loss": 1.4436,
        "grad_norm": 1.783326506614685,
        "learning_rate": 2.136871847443795e-05,
        "epoch": 0.8986086060293739,
        "step": 6975
    },
    {
        "loss": 1.6945,
        "grad_norm": 2.431072473526001,
        "learning_rate": 2.131887138331224e-05,
        "epoch": 0.8987374388044318,
        "step": 6976
    },
    {
        "loss": 2.2151,
        "grad_norm": 1.423326849937439,
        "learning_rate": 2.126906674156408e-05,
        "epoch": 0.8988662715794898,
        "step": 6977
    },
    {
        "loss": 1.8577,
        "grad_norm": 2.49904465675354,
        "learning_rate": 2.1219304622906606e-05,
        "epoch": 0.8989951043545478,
        "step": 6978
    },
    {
        "loss": 1.8329,
        "grad_norm": 6.2837700843811035,
        "learning_rate": 2.1169585100990014e-05,
        "epoch": 0.8991239371296058,
        "step": 6979
    },
    {
        "loss": 2.0561,
        "grad_norm": 2.4494707584381104,
        "learning_rate": 2.111990824940155e-05,
        "epoch": 0.8992527699046637,
        "step": 6980
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.654597520828247,
        "learning_rate": 2.1070274141665057e-05,
        "epoch": 0.8993816026797217,
        "step": 6981
    },
    {
        "loss": 1.6533,
        "grad_norm": 3.5368568897247314,
        "learning_rate": 2.102068285124131e-05,
        "epoch": 0.8995104354547797,
        "step": 6982
    },
    {
        "loss": 1.9012,
        "grad_norm": 3.387265920639038,
        "learning_rate": 2.097113445152783e-05,
        "epoch": 0.8996392682298376,
        "step": 6983
    },
    {
        "loss": 2.5059,
        "grad_norm": 2.0287015438079834,
        "learning_rate": 2.0921629015858346e-05,
        "epoch": 0.8997681010048957,
        "step": 6984
    },
    {
        "loss": 1.3994,
        "grad_norm": 1.749957799911499,
        "learning_rate": 2.087216661750322e-05,
        "epoch": 0.8998969337799536,
        "step": 6985
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.5729548931121826,
        "learning_rate": 2.0822747329669108e-05,
        "epoch": 0.9000257665550115,
        "step": 6986
    },
    {
        "loss": 2.3053,
        "grad_norm": 1.7696592807769775,
        "learning_rate": 2.0773371225498694e-05,
        "epoch": 0.9001545993300696,
        "step": 6987
    },
    {
        "loss": 2.1648,
        "grad_norm": 1.8512988090515137,
        "learning_rate": 2.0724038378070927e-05,
        "epoch": 0.9002834321051275,
        "step": 6988
    },
    {
        "loss": 2.2393,
        "grad_norm": 1.8173720836639404,
        "learning_rate": 2.0674748860400657e-05,
        "epoch": 0.9004122648801856,
        "step": 6989
    },
    {
        "loss": 1.6124,
        "grad_norm": 2.0967190265655518,
        "learning_rate": 2.0625502745438623e-05,
        "epoch": 0.9005410976552435,
        "step": 6990
    },
    {
        "loss": 2.283,
        "grad_norm": 2.4314322471618652,
        "learning_rate": 2.0576300106071322e-05,
        "epoch": 0.9006699304303015,
        "step": 6991
    },
    {
        "loss": 2.3152,
        "grad_norm": 2.1479406356811523,
        "learning_rate": 2.0527141015120898e-05,
        "epoch": 0.9007987632053595,
        "step": 6992
    },
    {
        "loss": 2.0446,
        "grad_norm": 1.9773896932601929,
        "learning_rate": 2.0478025545345054e-05,
        "epoch": 0.9009275959804174,
        "step": 6993
    },
    {
        "loss": 2.5407,
        "grad_norm": 1.97492253780365,
        "learning_rate": 2.042895376943697e-05,
        "epoch": 0.9010564287554754,
        "step": 6994
    },
    {
        "loss": 1.6215,
        "grad_norm": 2.7707834243774414,
        "learning_rate": 2.0379925760025004e-05,
        "epoch": 0.9011852615305334,
        "step": 6995
    },
    {
        "loss": 2.3114,
        "grad_norm": 1.3952540159225464,
        "learning_rate": 2.0330941589672914e-05,
        "epoch": 0.9013140943055914,
        "step": 6996
    },
    {
        "loss": 2.4151,
        "grad_norm": 2.6364994049072266,
        "learning_rate": 2.028200133087953e-05,
        "epoch": 0.9014429270806493,
        "step": 6997
    },
    {
        "loss": 1.5168,
        "grad_norm": 3.474309206008911,
        "learning_rate": 2.0233105056078573e-05,
        "epoch": 0.9015717598557073,
        "step": 6998
    },
    {
        "loss": 1.7301,
        "grad_norm": 3.184771776199341,
        "learning_rate": 2.0184252837638757e-05,
        "epoch": 0.9017005926307653,
        "step": 6999
    },
    {
        "loss": 1.4677,
        "grad_norm": 3.1873769760131836,
        "learning_rate": 2.0135444747863754e-05,
        "epoch": 0.9018294254058232,
        "step": 7000
    },
    {
        "loss": 1.7703,
        "grad_norm": 2.637631416320801,
        "learning_rate": 2.0086680858991596e-05,
        "epoch": 0.9019582581808813,
        "step": 7001
    },
    {
        "loss": 1.3045,
        "grad_norm": 3.3413779735565186,
        "learning_rate": 2.0037961243195126e-05,
        "epoch": 0.9020870909559392,
        "step": 7002
    },
    {
        "loss": 1.7065,
        "grad_norm": 3.629065752029419,
        "learning_rate": 1.9989285972581578e-05,
        "epoch": 0.9022159237309971,
        "step": 7003
    },
    {
        "loss": 1.8699,
        "grad_norm": 2.6363542079925537,
        "learning_rate": 1.9940655119192586e-05,
        "epoch": 0.9023447565060552,
        "step": 7004
    },
    {
        "loss": 2.3311,
        "grad_norm": 2.2977559566497803,
        "learning_rate": 1.989206875500408e-05,
        "epoch": 0.9024735892811131,
        "step": 7005
    },
    {
        "loss": 1.9014,
        "grad_norm": 2.715381622314453,
        "learning_rate": 1.9843526951925982e-05,
        "epoch": 0.9026024220561711,
        "step": 7006
    },
    {
        "loss": 2.3302,
        "grad_norm": 1.7798386812210083,
        "learning_rate": 1.9795029781802434e-05,
        "epoch": 0.9027312548312291,
        "step": 7007
    },
    {
        "loss": 2.1754,
        "grad_norm": 2.1878976821899414,
        "learning_rate": 1.9746577316411447e-05,
        "epoch": 0.902860087606287,
        "step": 7008
    },
    {
        "loss": 2.1318,
        "grad_norm": 1.9053986072540283,
        "learning_rate": 1.9698169627464886e-05,
        "epoch": 0.902988920381345,
        "step": 7009
    },
    {
        "loss": 2.5541,
        "grad_norm": 1.7206029891967773,
        "learning_rate": 1.9649806786608333e-05,
        "epoch": 0.903117753156403,
        "step": 7010
    },
    {
        "loss": 1.1934,
        "grad_norm": 2.2582733631134033,
        "learning_rate": 1.9601488865421076e-05,
        "epoch": 0.903246585931461,
        "step": 7011
    },
    {
        "loss": 1.4029,
        "grad_norm": 3.2057764530181885,
        "learning_rate": 1.9553215935415687e-05,
        "epoch": 0.9033754187065189,
        "step": 7012
    },
    {
        "loss": 1.512,
        "grad_norm": 3.1484813690185547,
        "learning_rate": 1.9504988068038376e-05,
        "epoch": 0.9035042514815769,
        "step": 7013
    },
    {
        "loss": 2.1434,
        "grad_norm": 2.270193338394165,
        "learning_rate": 1.9456805334668593e-05,
        "epoch": 0.9036330842566349,
        "step": 7014
    },
    {
        "loss": 2.0527,
        "grad_norm": 2.8807926177978516,
        "learning_rate": 1.9408667806618975e-05,
        "epoch": 0.9037619170316928,
        "step": 7015
    },
    {
        "loss": 2.0344,
        "grad_norm": 2.539527177810669,
        "learning_rate": 1.9360575555135254e-05,
        "epoch": 0.9038907498067509,
        "step": 7016
    },
    {
        "loss": 1.5347,
        "grad_norm": 2.5003457069396973,
        "learning_rate": 1.9312528651396155e-05,
        "epoch": 0.9040195825818088,
        "step": 7017
    },
    {
        "loss": 2.057,
        "grad_norm": 2.687417507171631,
        "learning_rate": 1.926452716651329e-05,
        "epoch": 0.9041484153568667,
        "step": 7018
    },
    {
        "loss": 1.3553,
        "grad_norm": 3.6292459964752197,
        "learning_rate": 1.9216571171531096e-05,
        "epoch": 0.9042772481319248,
        "step": 7019
    },
    {
        "loss": 2.0705,
        "grad_norm": 2.015183925628662,
        "learning_rate": 1.916866073742652e-05,
        "epoch": 0.9044060809069827,
        "step": 7020
    },
    {
        "loss": 1.8815,
        "grad_norm": 2.602926254272461,
        "learning_rate": 1.9120795935109244e-05,
        "epoch": 0.9045349136820408,
        "step": 7021
    },
    {
        "loss": 2.2219,
        "grad_norm": 1.6191112995147705,
        "learning_rate": 1.9072976835421413e-05,
        "epoch": 0.9046637464570987,
        "step": 7022
    },
    {
        "loss": 2.1229,
        "grad_norm": 1.955360770225525,
        "learning_rate": 1.9025203509137363e-05,
        "epoch": 0.9047925792321566,
        "step": 7023
    },
    {
        "loss": 1.8674,
        "grad_norm": 2.094860553741455,
        "learning_rate": 1.8977476026963854e-05,
        "epoch": 0.9049214120072147,
        "step": 7024
    },
    {
        "loss": 2.1064,
        "grad_norm": 1.708823323249817,
        "learning_rate": 1.8929794459539724e-05,
        "epoch": 0.9050502447822726,
        "step": 7025
    },
    {
        "loss": 2.2532,
        "grad_norm": 2.1575329303741455,
        "learning_rate": 1.888215887743588e-05,
        "epoch": 0.9051790775573306,
        "step": 7026
    },
    {
        "loss": 2.0899,
        "grad_norm": 1.7687017917633057,
        "learning_rate": 1.883456935115514e-05,
        "epoch": 0.9053079103323886,
        "step": 7027
    },
    {
        "loss": 1.7165,
        "grad_norm": 2.9322476387023926,
        "learning_rate": 1.8787025951132185e-05,
        "epoch": 0.9054367431074465,
        "step": 7028
    },
    {
        "loss": 2.072,
        "grad_norm": 1.8750035762786865,
        "learning_rate": 1.8739528747733404e-05,
        "epoch": 0.9055655758825045,
        "step": 7029
    },
    {
        "loss": 2.3045,
        "grad_norm": 1.9233757257461548,
        "learning_rate": 1.8692077811256902e-05,
        "epoch": 0.9056944086575625,
        "step": 7030
    },
    {
        "loss": 2.1756,
        "grad_norm": 1.7574986219406128,
        "learning_rate": 1.864467321193208e-05,
        "epoch": 0.9058232414326205,
        "step": 7031
    },
    {
        "loss": 2.2893,
        "grad_norm": 2.5343408584594727,
        "learning_rate": 1.8597315019919975e-05,
        "epoch": 0.9059520742076784,
        "step": 7032
    },
    {
        "loss": 2.0246,
        "grad_norm": 1.7925808429718018,
        "learning_rate": 1.855000330531293e-05,
        "epoch": 0.9060809069827364,
        "step": 7033
    },
    {
        "loss": 1.5455,
        "grad_norm": 2.1331899166107178,
        "learning_rate": 1.8502738138134274e-05,
        "epoch": 0.9062097397577944,
        "step": 7034
    },
    {
        "loss": 1.7431,
        "grad_norm": 2.6248157024383545,
        "learning_rate": 1.8455519588338767e-05,
        "epoch": 0.9063385725328523,
        "step": 7035
    },
    {
        "loss": 1.5283,
        "grad_norm": 1.8090626001358032,
        "learning_rate": 1.840834772581202e-05,
        "epoch": 0.9064674053079104,
        "step": 7036
    },
    {
        "loss": 2.2514,
        "grad_norm": 1.5143452882766724,
        "learning_rate": 1.8361222620370427e-05,
        "epoch": 0.9065962380829683,
        "step": 7037
    },
    {
        "loss": 1.1447,
        "grad_norm": 3.036695957183838,
        "learning_rate": 1.8314144341761353e-05,
        "epoch": 0.9067250708580262,
        "step": 7038
    },
    {
        "loss": 1.9389,
        "grad_norm": 2.1128735542297363,
        "learning_rate": 1.8267112959662786e-05,
        "epoch": 0.9068539036330843,
        "step": 7039
    },
    {
        "loss": 2.4359,
        "grad_norm": 2.0106966495513916,
        "learning_rate": 1.8220128543683322e-05,
        "epoch": 0.9069827364081422,
        "step": 7040
    },
    {
        "loss": 2.4101,
        "grad_norm": 1.8177927732467651,
        "learning_rate": 1.817319116336209e-05,
        "epoch": 0.9071115691832002,
        "step": 7041
    },
    {
        "loss": 2.3801,
        "grad_norm": 2.2640488147735596,
        "learning_rate": 1.8126300888168397e-05,
        "epoch": 0.9072404019582582,
        "step": 7042
    },
    {
        "loss": 1.706,
        "grad_norm": 3.138876438140869,
        "learning_rate": 1.8079457787502134e-05,
        "epoch": 0.9073692347333162,
        "step": 7043
    },
    {
        "loss": 1.8609,
        "grad_norm": 4.003347396850586,
        "learning_rate": 1.8032661930693234e-05,
        "epoch": 0.9074980675083741,
        "step": 7044
    },
    {
        "loss": 2.2262,
        "grad_norm": 2.5371711254119873,
        "learning_rate": 1.7985913387001597e-05,
        "epoch": 0.9076269002834321,
        "step": 7045
    },
    {
        "loss": 1.6885,
        "grad_norm": 2.5105535984039307,
        "learning_rate": 1.7939212225617257e-05,
        "epoch": 0.9077557330584901,
        "step": 7046
    },
    {
        "loss": 1.7041,
        "grad_norm": 2.4548003673553467,
        "learning_rate": 1.7892558515660086e-05,
        "epoch": 0.907884565833548,
        "step": 7047
    },
    {
        "loss": 1.969,
        "grad_norm": 2.0617105960845947,
        "learning_rate": 1.784595232617961e-05,
        "epoch": 0.9080133986086061,
        "step": 7048
    },
    {
        "loss": 1.268,
        "grad_norm": 4.481029033660889,
        "learning_rate": 1.779939372615517e-05,
        "epoch": 0.908142231383664,
        "step": 7049
    },
    {
        "loss": 1.5429,
        "grad_norm": 3.340710163116455,
        "learning_rate": 1.775288278449562e-05,
        "epoch": 0.9082710641587219,
        "step": 7050
    },
    {
        "loss": 2.1008,
        "grad_norm": 2.4633657932281494,
        "learning_rate": 1.7706419570039257e-05,
        "epoch": 0.90839989693378,
        "step": 7051
    },
    {
        "loss": 2.0774,
        "grad_norm": 1.7980929613113403,
        "learning_rate": 1.7660004151553772e-05,
        "epoch": 0.9085287297088379,
        "step": 7052
    },
    {
        "loss": 2.352,
        "grad_norm": 2.211679220199585,
        "learning_rate": 1.7613636597736093e-05,
        "epoch": 0.908657562483896,
        "step": 7053
    },
    {
        "loss": 2.2766,
        "grad_norm": 3.0415804386138916,
        "learning_rate": 1.7567316977212317e-05,
        "epoch": 0.9087863952589539,
        "step": 7054
    },
    {
        "loss": 1.2144,
        "grad_norm": 2.0446553230285645,
        "learning_rate": 1.7521045358537636e-05,
        "epoch": 0.9089152280340118,
        "step": 7055
    },
    {
        "loss": 2.3535,
        "grad_norm": 2.366952419281006,
        "learning_rate": 1.7474821810196062e-05,
        "epoch": 0.9090440608090699,
        "step": 7056
    },
    {
        "loss": 2.218,
        "grad_norm": 1.671033501625061,
        "learning_rate": 1.74286464006006e-05,
        "epoch": 0.9091728935841278,
        "step": 7057
    },
    {
        "loss": 2.4417,
        "grad_norm": 1.3353513479232788,
        "learning_rate": 1.7382519198093017e-05,
        "epoch": 0.9093017263591858,
        "step": 7058
    },
    {
        "loss": 2.1649,
        "grad_norm": 3.0177624225616455,
        "learning_rate": 1.733644027094351e-05,
        "epoch": 0.9094305591342438,
        "step": 7059
    },
    {
        "loss": 1.7096,
        "grad_norm": 2.776292562484741,
        "learning_rate": 1.7290409687351154e-05,
        "epoch": 0.9095593919093017,
        "step": 7060
    },
    {
        "loss": 2.5807,
        "grad_norm": 1.8639144897460938,
        "learning_rate": 1.7244427515443327e-05,
        "epoch": 0.9096882246843597,
        "step": 7061
    },
    {
        "loss": 1.7405,
        "grad_norm": 2.5634608268737793,
        "learning_rate": 1.7198493823275603e-05,
        "epoch": 0.9098170574594177,
        "step": 7062
    },
    {
        "loss": 1.6401,
        "grad_norm": 2.9053072929382324,
        "learning_rate": 1.7152608678832005e-05,
        "epoch": 0.9099458902344757,
        "step": 7063
    },
    {
        "loss": 1.4485,
        "grad_norm": 2.1285793781280518,
        "learning_rate": 1.7106772150024636e-05,
        "epoch": 0.9100747230095336,
        "step": 7064
    },
    {
        "loss": 1.3044,
        "grad_norm": 2.0043985843658447,
        "learning_rate": 1.7060984304693656e-05,
        "epoch": 0.9102035557845916,
        "step": 7065
    },
    {
        "loss": 2.2187,
        "grad_norm": 2.829315662384033,
        "learning_rate": 1.7015245210607206e-05,
        "epoch": 0.9103323885596496,
        "step": 7066
    },
    {
        "loss": 1.6555,
        "grad_norm": 2.7044379711151123,
        "learning_rate": 1.6969554935461136e-05,
        "epoch": 0.9104612213347075,
        "step": 7067
    },
    {
        "loss": 1.8118,
        "grad_norm": 2.2939727306365967,
        "learning_rate": 1.6923913546879122e-05,
        "epoch": 0.9105900541097656,
        "step": 7068
    },
    {
        "loss": 1.437,
        "grad_norm": 3.381838798522949,
        "learning_rate": 1.6878321112412664e-05,
        "epoch": 0.9107188868848235,
        "step": 7069
    },
    {
        "loss": 2.096,
        "grad_norm": 2.062182664871216,
        "learning_rate": 1.6832777699540492e-05,
        "epoch": 0.9108477196598814,
        "step": 7070
    },
    {
        "loss": 2.0218,
        "grad_norm": 2.1859242916107178,
        "learning_rate": 1.6787283375668973e-05,
        "epoch": 0.9109765524349395,
        "step": 7071
    },
    {
        "loss": 1.7641,
        "grad_norm": 3.2037432193756104,
        "learning_rate": 1.6741838208131838e-05,
        "epoch": 0.9111053852099974,
        "step": 7072
    },
    {
        "loss": 1.9372,
        "grad_norm": 1.404783844947815,
        "learning_rate": 1.6696442264189905e-05,
        "epoch": 0.9112342179850554,
        "step": 7073
    },
    {
        "loss": 2.3485,
        "grad_norm": 1.3640629053115845,
        "learning_rate": 1.6651095611031286e-05,
        "epoch": 0.9113630507601134,
        "step": 7074
    },
    {
        "loss": 1.5992,
        "grad_norm": 2.5700361728668213,
        "learning_rate": 1.66057983157711e-05,
        "epoch": 0.9114918835351713,
        "step": 7075
    },
    {
        "loss": 1.4956,
        "grad_norm": 4.056698322296143,
        "learning_rate": 1.65605504454514e-05,
        "epoch": 0.9116207163102293,
        "step": 7076
    },
    {
        "loss": 2.3907,
        "grad_norm": 1.9410374164581299,
        "learning_rate": 1.6515352067041106e-05,
        "epoch": 0.9117495490852873,
        "step": 7077
    },
    {
        "loss": 1.0698,
        "grad_norm": 2.3320186138153076,
        "learning_rate": 1.6470203247435873e-05,
        "epoch": 0.9118783818603453,
        "step": 7078
    },
    {
        "loss": 1.9112,
        "grad_norm": 2.530608654022217,
        "learning_rate": 1.6425104053458012e-05,
        "epoch": 0.9120072146354032,
        "step": 7079
    },
    {
        "loss": 1.2647,
        "grad_norm": 4.1188178062438965,
        "learning_rate": 1.6380054551856456e-05,
        "epoch": 0.9121360474104612,
        "step": 7080
    },
    {
        "loss": 1.6083,
        "grad_norm": 4.702830791473389,
        "learning_rate": 1.6335054809306384e-05,
        "epoch": 0.9122648801855192,
        "step": 7081
    },
    {
        "loss": 2.28,
        "grad_norm": 2.307798385620117,
        "learning_rate": 1.629010489240953e-05,
        "epoch": 0.9123937129605771,
        "step": 7082
    },
    {
        "loss": 1.4692,
        "grad_norm": 2.006603717803955,
        "learning_rate": 1.6245204867693876e-05,
        "epoch": 0.9125225457356352,
        "step": 7083
    },
    {
        "loss": 2.2114,
        "grad_norm": 1.6992298364639282,
        "learning_rate": 1.62003548016134e-05,
        "epoch": 0.9126513785106931,
        "step": 7084
    },
    {
        "loss": 2.0976,
        "grad_norm": 2.3031582832336426,
        "learning_rate": 1.6155554760548243e-05,
        "epoch": 0.912780211285751,
        "step": 7085
    },
    {
        "loss": 2.0945,
        "grad_norm": 1.6689811944961548,
        "learning_rate": 1.611080481080466e-05,
        "epoch": 0.9129090440608091,
        "step": 7086
    },
    {
        "loss": 1.0528,
        "grad_norm": 1.7379199266433716,
        "learning_rate": 1.606610501861447e-05,
        "epoch": 0.913037876835867,
        "step": 7087
    },
    {
        "loss": 2.5284,
        "grad_norm": 1.8122535943984985,
        "learning_rate": 1.6021455450135442e-05,
        "epoch": 0.913166709610925,
        "step": 7088
    },
    {
        "loss": 1.762,
        "grad_norm": 3.6138429641723633,
        "learning_rate": 1.5976856171450984e-05,
        "epoch": 0.913295542385983,
        "step": 7089
    },
    {
        "loss": 2.0792,
        "grad_norm": 1.7882932424545288,
        "learning_rate": 1.5932307248570054e-05,
        "epoch": 0.9134243751610409,
        "step": 7090
    },
    {
        "loss": 1.3082,
        "grad_norm": 2.926966905593872,
        "learning_rate": 1.5887808747427147e-05,
        "epoch": 0.913553207936099,
        "step": 7091
    },
    {
        "loss": 1.4974,
        "grad_norm": 3.0926918983459473,
        "learning_rate": 1.5843360733881958e-05,
        "epoch": 0.9136820407111569,
        "step": 7092
    },
    {
        "loss": 0.9278,
        "grad_norm": 2.655806064605713,
        "learning_rate": 1.5798963273719637e-05,
        "epoch": 0.9138108734862149,
        "step": 7093
    },
    {
        "loss": 1.8541,
        "grad_norm": 2.5859718322753906,
        "learning_rate": 1.5754616432650444e-05,
        "epoch": 0.9139397062612729,
        "step": 7094
    },
    {
        "loss": 2.2013,
        "grad_norm": 2.3485922813415527,
        "learning_rate": 1.5710320276309716e-05,
        "epoch": 0.9140685390363309,
        "step": 7095
    },
    {
        "loss": 1.4992,
        "grad_norm": 2.9034440517425537,
        "learning_rate": 1.5666074870257813e-05,
        "epoch": 0.9141973718113888,
        "step": 7096
    },
    {
        "loss": 1.8446,
        "grad_norm": 2.4444148540496826,
        "learning_rate": 1.562188027997996e-05,
        "epoch": 0.9143262045864468,
        "step": 7097
    },
    {
        "loss": 2.4434,
        "grad_norm": 1.6435779333114624,
        "learning_rate": 1.557773657088609e-05,
        "epoch": 0.9144550373615048,
        "step": 7098
    },
    {
        "loss": 2.2297,
        "grad_norm": 2.230828046798706,
        "learning_rate": 1.553364380831095e-05,
        "epoch": 0.9145838701365627,
        "step": 7099
    },
    {
        "loss": 1.7428,
        "grad_norm": 2.077462673187256,
        "learning_rate": 1.5489602057513836e-05,
        "epoch": 0.9147127029116208,
        "step": 7100
    },
    {
        "loss": 1.9282,
        "grad_norm": 1.7048628330230713,
        "learning_rate": 1.544561138367855e-05,
        "epoch": 0.9148415356866787,
        "step": 7101
    },
    {
        "loss": 1.6465,
        "grad_norm": 2.312314510345459,
        "learning_rate": 1.540167185191329e-05,
        "epoch": 0.9149703684617366,
        "step": 7102
    },
    {
        "loss": 2.4357,
        "grad_norm": 1.8644975423812866,
        "learning_rate": 1.535778352725057e-05,
        "epoch": 0.9150992012367947,
        "step": 7103
    },
    {
        "loss": 1.9978,
        "grad_norm": 1.8649158477783203,
        "learning_rate": 1.5313946474647108e-05,
        "epoch": 0.9152280340118526,
        "step": 7104
    },
    {
        "loss": 1.6258,
        "grad_norm": 2.7534592151641846,
        "learning_rate": 1.5270160758983787e-05,
        "epoch": 0.9153568667869106,
        "step": 7105
    },
    {
        "loss": 1.4839,
        "grad_norm": 3.599299669265747,
        "learning_rate": 1.5226426445065373e-05,
        "epoch": 0.9154856995619686,
        "step": 7106
    },
    {
        "loss": 1.5841,
        "grad_norm": 2.937629222869873,
        "learning_rate": 1.518274359762068e-05,
        "epoch": 0.9156145323370265,
        "step": 7107
    },
    {
        "loss": 2.496,
        "grad_norm": 2.5405075550079346,
        "learning_rate": 1.5139112281302376e-05,
        "epoch": 0.9157433651120845,
        "step": 7108
    },
    {
        "loss": 1.9537,
        "grad_norm": 1.9294770956039429,
        "learning_rate": 1.5095532560686687e-05,
        "epoch": 0.9158721978871425,
        "step": 7109
    },
    {
        "loss": 1.2372,
        "grad_norm": 2.168304443359375,
        "learning_rate": 1.5052004500273642e-05,
        "epoch": 0.9160010306622005,
        "step": 7110
    },
    {
        "loss": 1.663,
        "grad_norm": 2.0936636924743652,
        "learning_rate": 1.5008528164486768e-05,
        "epoch": 0.9161298634372584,
        "step": 7111
    },
    {
        "loss": 1.8155,
        "grad_norm": 3.0185186862945557,
        "learning_rate": 1.4965103617672993e-05,
        "epoch": 0.9162586962123164,
        "step": 7112
    },
    {
        "loss": 2.1879,
        "grad_norm": 1.92313814163208,
        "learning_rate": 1.4921730924102639e-05,
        "epoch": 0.9163875289873744,
        "step": 7113
    },
    {
        "loss": 2.178,
        "grad_norm": 2.5357391834259033,
        "learning_rate": 1.4878410147969268e-05,
        "epoch": 0.9165163617624323,
        "step": 7114
    },
    {
        "loss": 1.4425,
        "grad_norm": 2.3811421394348145,
        "learning_rate": 1.4835141353389608e-05,
        "epoch": 0.9166451945374904,
        "step": 7115
    },
    {
        "loss": 2.0926,
        "grad_norm": 2.019211530685425,
        "learning_rate": 1.4791924604403496e-05,
        "epoch": 0.9167740273125483,
        "step": 7116
    },
    {
        "loss": 1.8507,
        "grad_norm": 1.5695886611938477,
        "learning_rate": 1.4748759964973591e-05,
        "epoch": 0.9169028600876062,
        "step": 7117
    },
    {
        "loss": 2.0185,
        "grad_norm": 2.0833091735839844,
        "learning_rate": 1.470564749898557e-05,
        "epoch": 0.9170316928626643,
        "step": 7118
    },
    {
        "loss": 1.144,
        "grad_norm": 3.319185256958008,
        "learning_rate": 1.4662587270247851e-05,
        "epoch": 0.9171605256377222,
        "step": 7119
    },
    {
        "loss": 1.7942,
        "grad_norm": 2.3735151290893555,
        "learning_rate": 1.461957934249154e-05,
        "epoch": 0.9172893584127803,
        "step": 7120
    },
    {
        "loss": 1.9165,
        "grad_norm": 2.7436373233795166,
        "learning_rate": 1.4576623779370329e-05,
        "epoch": 0.9174181911878382,
        "step": 7121
    },
    {
        "loss": 1.9158,
        "grad_norm": 2.020556926727295,
        "learning_rate": 1.4533720644460446e-05,
        "epoch": 0.9175470239628961,
        "step": 7122
    },
    {
        "loss": 2.0417,
        "grad_norm": 2.42761492729187,
        "learning_rate": 1.4490870001260409e-05,
        "epoch": 0.9176758567379542,
        "step": 7123
    },
    {
        "loss": 1.0466,
        "grad_norm": 2.2778687477111816,
        "learning_rate": 1.4448071913191152e-05,
        "epoch": 0.9178046895130121,
        "step": 7124
    },
    {
        "loss": 1.7032,
        "grad_norm": 2.885660171508789,
        "learning_rate": 1.4405326443595812e-05,
        "epoch": 0.9179335222880701,
        "step": 7125
    },
    {
        "loss": 2.3107,
        "grad_norm": 1.9354928731918335,
        "learning_rate": 1.4362633655739622e-05,
        "epoch": 0.918062355063128,
        "step": 7126
    },
    {
        "loss": 1.9011,
        "grad_norm": 2.1863656044006348,
        "learning_rate": 1.4319993612809917e-05,
        "epoch": 0.918191187838186,
        "step": 7127
    },
    {
        "loss": 2.5029,
        "grad_norm": 2.1582140922546387,
        "learning_rate": 1.427740637791572e-05,
        "epoch": 0.918320020613244,
        "step": 7128
    },
    {
        "loss": 1.8383,
        "grad_norm": 2.4226174354553223,
        "learning_rate": 1.4234872014088264e-05,
        "epoch": 0.918448853388302,
        "step": 7129
    },
    {
        "loss": 1.4032,
        "grad_norm": 2.817349672317505,
        "learning_rate": 1.4192390584280346e-05,
        "epoch": 0.91857768616336,
        "step": 7130
    },
    {
        "loss": 2.1075,
        "grad_norm": 2.157337188720703,
        "learning_rate": 1.4149962151366303e-05,
        "epoch": 0.9187065189384179,
        "step": 7131
    },
    {
        "loss": 2.1548,
        "grad_norm": 1.8582555055618286,
        "learning_rate": 1.4107586778142196e-05,
        "epoch": 0.9188353517134759,
        "step": 7132
    },
    {
        "loss": 1.527,
        "grad_norm": 1.7622188329696655,
        "learning_rate": 1.4065264527325566e-05,
        "epoch": 0.9189641844885339,
        "step": 7133
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.7322826385498047,
        "learning_rate": 1.4022995461555177e-05,
        "epoch": 0.9190930172635918,
        "step": 7134
    },
    {
        "loss": 2.1652,
        "grad_norm": 1.5396976470947266,
        "learning_rate": 1.3980779643391223e-05,
        "epoch": 0.9192218500386499,
        "step": 7135
    },
    {
        "loss": 2.1681,
        "grad_norm": 2.3210813999176025,
        "learning_rate": 1.393861713531503e-05,
        "epoch": 0.9193506828137078,
        "step": 7136
    },
    {
        "loss": 2.1031,
        "grad_norm": 2.288017749786377,
        "learning_rate": 1.3896507999729041e-05,
        "epoch": 0.9194795155887657,
        "step": 7137
    },
    {
        "loss": 2.4661,
        "grad_norm": 2.62599515914917,
        "learning_rate": 1.3854452298956689e-05,
        "epoch": 0.9196083483638238,
        "step": 7138
    },
    {
        "loss": 2.0522,
        "grad_norm": 2.6679956912994385,
        "learning_rate": 1.3812450095242335e-05,
        "epoch": 0.9197371811388817,
        "step": 7139
    },
    {
        "loss": 1.7073,
        "grad_norm": 2.9088497161865234,
        "learning_rate": 1.3770501450751145e-05,
        "epoch": 0.9198660139139397,
        "step": 7140
    },
    {
        "loss": 1.6195,
        "grad_norm": 1.8919657468795776,
        "learning_rate": 1.372860642756908e-05,
        "epoch": 0.9199948466889977,
        "step": 7141
    },
    {
        "loss": 1.5447,
        "grad_norm": 2.2769551277160645,
        "learning_rate": 1.3686765087702591e-05,
        "epoch": 0.9201236794640557,
        "step": 7142
    },
    {
        "loss": 1.057,
        "grad_norm": 2.7564468383789062,
        "learning_rate": 1.3644977493078803e-05,
        "epoch": 0.9202525122391136,
        "step": 7143
    },
    {
        "loss": 2.3793,
        "grad_norm": 2.256162405014038,
        "learning_rate": 1.3603243705545338e-05,
        "epoch": 0.9203813450141716,
        "step": 7144
    },
    {
        "loss": 1.756,
        "grad_norm": 2.518216848373413,
        "learning_rate": 1.3561563786869924e-05,
        "epoch": 0.9205101777892296,
        "step": 7145
    },
    {
        "loss": 1.8793,
        "grad_norm": 2.2947258949279785,
        "learning_rate": 1.3519937798740896e-05,
        "epoch": 0.9206390105642875,
        "step": 7146
    },
    {
        "loss": 1.9038,
        "grad_norm": 3.4037811756134033,
        "learning_rate": 1.3478365802766618e-05,
        "epoch": 0.9207678433393456,
        "step": 7147
    },
    {
        "loss": 1.8338,
        "grad_norm": 2.279533863067627,
        "learning_rate": 1.3436847860475438e-05,
        "epoch": 0.9208966761144035,
        "step": 7148
    },
    {
        "loss": 1.2803,
        "grad_norm": 1.9768483638763428,
        "learning_rate": 1.3395384033315856e-05,
        "epoch": 0.9210255088894614,
        "step": 7149
    },
    {
        "loss": 2.014,
        "grad_norm": 3.0527100563049316,
        "learning_rate": 1.3353974382656226e-05,
        "epoch": 0.9211543416645195,
        "step": 7150
    },
    {
        "loss": 1.7333,
        "grad_norm": 3.9005985260009766,
        "learning_rate": 1.3312618969784712e-05,
        "epoch": 0.9212831744395774,
        "step": 7151
    },
    {
        "loss": 1.9122,
        "grad_norm": 2.5956475734710693,
        "learning_rate": 1.327131785590926e-05,
        "epoch": 0.9214120072146355,
        "step": 7152
    },
    {
        "loss": 1.7802,
        "grad_norm": 3.1648919582366943,
        "learning_rate": 1.3230071102157265e-05,
        "epoch": 0.9215408399896934,
        "step": 7153
    },
    {
        "loss": 2.1833,
        "grad_norm": 2.301593780517578,
        "learning_rate": 1.3188878769575941e-05,
        "epoch": 0.9216696727647513,
        "step": 7154
    },
    {
        "loss": 0.975,
        "grad_norm": 2.782825231552124,
        "learning_rate": 1.3147740919131813e-05,
        "epoch": 0.9217985055398094,
        "step": 7155
    },
    {
        "loss": 2.2764,
        "grad_norm": 1.882948398590088,
        "learning_rate": 1.3106657611710683e-05,
        "epoch": 0.9219273383148673,
        "step": 7156
    },
    {
        "loss": 1.5157,
        "grad_norm": 2.654883623123169,
        "learning_rate": 1.3065628908117766e-05,
        "epoch": 0.9220561710899253,
        "step": 7157
    },
    {
        "loss": 1.7959,
        "grad_norm": 2.0926191806793213,
        "learning_rate": 1.3024654869077463e-05,
        "epoch": 0.9221850038649833,
        "step": 7158
    },
    {
        "loss": 2.0904,
        "grad_norm": 1.4037513732910156,
        "learning_rate": 1.2983735555233117e-05,
        "epoch": 0.9223138366400412,
        "step": 7159
    },
    {
        "loss": 2.4478,
        "grad_norm": 4.230371475219727,
        "learning_rate": 1.2942871027147235e-05,
        "epoch": 0.9224426694150992,
        "step": 7160
    },
    {
        "loss": 2.0256,
        "grad_norm": 2.6749842166900635,
        "learning_rate": 1.2902061345301192e-05,
        "epoch": 0.9225715021901572,
        "step": 7161
    },
    {
        "loss": 2.3377,
        "grad_norm": 1.199050784111023,
        "learning_rate": 1.2861306570095144e-05,
        "epoch": 0.9227003349652152,
        "step": 7162
    },
    {
        "loss": 1.9888,
        "grad_norm": 2.1282577514648438,
        "learning_rate": 1.2820606761848032e-05,
        "epoch": 0.9228291677402731,
        "step": 7163
    },
    {
        "loss": 1.6403,
        "grad_norm": 2.6274709701538086,
        "learning_rate": 1.2779961980797428e-05,
        "epoch": 0.9229580005153311,
        "step": 7164
    },
    {
        "loss": 1.7941,
        "grad_norm": 2.5345144271850586,
        "learning_rate": 1.2739372287099467e-05,
        "epoch": 0.9230868332903891,
        "step": 7165
    },
    {
        "loss": 1.2521,
        "grad_norm": 3.8374414443969727,
        "learning_rate": 1.2698837740828779e-05,
        "epoch": 0.923215666065447,
        "step": 7166
    },
    {
        "loss": 1.6877,
        "grad_norm": 2.3707473278045654,
        "learning_rate": 1.2658358401978254e-05,
        "epoch": 0.9233444988405051,
        "step": 7167
    },
    {
        "loss": 1.8779,
        "grad_norm": 2.4634923934936523,
        "learning_rate": 1.2617934330459203e-05,
        "epoch": 0.923473331615563,
        "step": 7168
    },
    {
        "loss": 1.7936,
        "grad_norm": 2.727062702178955,
        "learning_rate": 1.2577565586101142e-05,
        "epoch": 0.9236021643906209,
        "step": 7169
    },
    {
        "loss": 2.1892,
        "grad_norm": 1.8947054147720337,
        "learning_rate": 1.2537252228651563e-05,
        "epoch": 0.923730997165679,
        "step": 7170
    },
    {
        "loss": 2.0567,
        "grad_norm": 1.8671976327896118,
        "learning_rate": 1.2496994317776073e-05,
        "epoch": 0.9238598299407369,
        "step": 7171
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.4288984537124634,
        "learning_rate": 1.2456791913058342e-05,
        "epoch": 0.9239886627157949,
        "step": 7172
    },
    {
        "loss": 1.8883,
        "grad_norm": 1.8366727828979492,
        "learning_rate": 1.2416645073999633e-05,
        "epoch": 0.9241174954908529,
        "step": 7173
    },
    {
        "loss": 2.2007,
        "grad_norm": 2.097872018814087,
        "learning_rate": 1.2376553860019147e-05,
        "epoch": 0.9242463282659108,
        "step": 7174
    },
    {
        "loss": 2.1571,
        "grad_norm": 2.998041868209839,
        "learning_rate": 1.2336518330453694e-05,
        "epoch": 0.9243751610409688,
        "step": 7175
    },
    {
        "loss": 2.2201,
        "grad_norm": 1.672738790512085,
        "learning_rate": 1.2296538544557701e-05,
        "epoch": 0.9245039938160268,
        "step": 7176
    },
    {
        "loss": 1.3606,
        "grad_norm": 3.529604434967041,
        "learning_rate": 1.225661456150311e-05,
        "epoch": 0.9246328265910848,
        "step": 7177
    },
    {
        "loss": 1.0588,
        "grad_norm": 2.5133371353149414,
        "learning_rate": 1.221674644037914e-05,
        "epoch": 0.9247616593661427,
        "step": 7178
    },
    {
        "loss": 2.0519,
        "grad_norm": 1.1950314044952393,
        "learning_rate": 1.2176934240192411e-05,
        "epoch": 0.9248904921412007,
        "step": 7179
    },
    {
        "loss": 1.9739,
        "grad_norm": 1.5055046081542969,
        "learning_rate": 1.213717801986698e-05,
        "epoch": 0.9250193249162587,
        "step": 7180
    },
    {
        "loss": 2.337,
        "grad_norm": 1.8309520483016968,
        "learning_rate": 1.2097477838243682e-05,
        "epoch": 0.9251481576913166,
        "step": 7181
    },
    {
        "loss": 1.4329,
        "grad_norm": 3.324631929397583,
        "learning_rate": 1.2057833754080655e-05,
        "epoch": 0.9252769904663747,
        "step": 7182
    },
    {
        "loss": 1.7297,
        "grad_norm": 2.213563919067383,
        "learning_rate": 1.2018245826053e-05,
        "epoch": 0.9254058232414326,
        "step": 7183
    },
    {
        "loss": 2.171,
        "grad_norm": 1.3751609325408936,
        "learning_rate": 1.1978714112752565e-05,
        "epoch": 0.9255346560164905,
        "step": 7184
    },
    {
        "loss": 2.0629,
        "grad_norm": 1.7612855434417725,
        "learning_rate": 1.1939238672688125e-05,
        "epoch": 0.9256634887915486,
        "step": 7185
    },
    {
        "loss": 1.5776,
        "grad_norm": 3.889972686767578,
        "learning_rate": 1.1899819564285136e-05,
        "epoch": 0.9257923215666065,
        "step": 7186
    },
    {
        "loss": 1.4151,
        "grad_norm": 2.377530574798584,
        "learning_rate": 1.1860456845885676e-05,
        "epoch": 0.9259211543416646,
        "step": 7187
    },
    {
        "loss": 2.1564,
        "grad_norm": 2.0167768001556396,
        "learning_rate": 1.1821150575748358e-05,
        "epoch": 0.9260499871167225,
        "step": 7188
    },
    {
        "loss": 1.7875,
        "grad_norm": 1.5412400960922241,
        "learning_rate": 1.1781900812048257e-05,
        "epoch": 0.9261788198917804,
        "step": 7189
    },
    {
        "loss": 1.7897,
        "grad_norm": 3.1935672760009766,
        "learning_rate": 1.1742707612876812e-05,
        "epoch": 0.9263076526668385,
        "step": 7190
    },
    {
        "loss": 2.1461,
        "grad_norm": 3.1133816242218018,
        "learning_rate": 1.1703571036241783e-05,
        "epoch": 0.9264364854418964,
        "step": 7191
    },
    {
        "loss": 1.8176,
        "grad_norm": 2.0947394371032715,
        "learning_rate": 1.1664491140067001e-05,
        "epoch": 0.9265653182169544,
        "step": 7192
    },
    {
        "loss": 1.4177,
        "grad_norm": 4.311549186706543,
        "learning_rate": 1.1625467982192545e-05,
        "epoch": 0.9266941509920124,
        "step": 7193
    },
    {
        "loss": 2.2436,
        "grad_norm": 4.014054775238037,
        "learning_rate": 1.1586501620374518e-05,
        "epoch": 0.9268229837670704,
        "step": 7194
    },
    {
        "loss": 2.3081,
        "grad_norm": 1.56509268283844,
        "learning_rate": 1.1547592112284794e-05,
        "epoch": 0.9269518165421283,
        "step": 7195
    },
    {
        "loss": 2.4499,
        "grad_norm": 1.824495553970337,
        "learning_rate": 1.150873951551124e-05,
        "epoch": 0.9270806493171863,
        "step": 7196
    },
    {
        "loss": 1.4478,
        "grad_norm": 3.2588539123535156,
        "learning_rate": 1.1469943887557593e-05,
        "epoch": 0.9272094820922443,
        "step": 7197
    },
    {
        "loss": 2.3923,
        "grad_norm": 1.9749644994735718,
        "learning_rate": 1.1431205285843027e-05,
        "epoch": 0.9273383148673022,
        "step": 7198
    },
    {
        "loss": 1.8223,
        "grad_norm": 1.8670158386230469,
        "learning_rate": 1.1392523767702478e-05,
        "epoch": 0.9274671476423603,
        "step": 7199
    },
    {
        "loss": 2.3465,
        "grad_norm": 1.7557975053787231,
        "learning_rate": 1.1353899390386352e-05,
        "epoch": 0.9275959804174182,
        "step": 7200
    },
    {
        "loss": 1.58,
        "grad_norm": 3.1015748977661133,
        "learning_rate": 1.1315332211060493e-05,
        "epoch": 0.9277248131924761,
        "step": 7201
    },
    {
        "loss": 2.1099,
        "grad_norm": 1.767288327217102,
        "learning_rate": 1.1276822286806127e-05,
        "epoch": 0.9278536459675342,
        "step": 7202
    },
    {
        "loss": 2.52,
        "grad_norm": 1.8389736413955688,
        "learning_rate": 1.1238369674619609e-05,
        "epoch": 0.9279824787425921,
        "step": 7203
    },
    {
        "loss": 0.742,
        "grad_norm": 1.755676507949829,
        "learning_rate": 1.1199974431412603e-05,
        "epoch": 0.9281113115176501,
        "step": 7204
    },
    {
        "loss": 2.4579,
        "grad_norm": 1.644222378730774,
        "learning_rate": 1.1161636614011817e-05,
        "epoch": 0.9282401442927081,
        "step": 7205
    },
    {
        "loss": 1.5751,
        "grad_norm": 2.1559860706329346,
        "learning_rate": 1.1123356279158965e-05,
        "epoch": 0.928368977067766,
        "step": 7206
    },
    {
        "loss": 2.1447,
        "grad_norm": 2.1163330078125,
        "learning_rate": 1.1085133483510696e-05,
        "epoch": 0.928497809842824,
        "step": 7207
    },
    {
        "loss": 2.3204,
        "grad_norm": 1.2584189176559448,
        "learning_rate": 1.104696828363852e-05,
        "epoch": 0.928626642617882,
        "step": 7208
    },
    {
        "loss": 2.0738,
        "grad_norm": 2.9910483360290527,
        "learning_rate": 1.1008860736028598e-05,
        "epoch": 0.92875547539294,
        "step": 7209
    },
    {
        "loss": 1.9925,
        "grad_norm": 1.6264193058013916,
        "learning_rate": 1.0970810897081879e-05,
        "epoch": 0.9288843081679979,
        "step": 7210
    },
    {
        "loss": 2.2574,
        "grad_norm": 1.388291358947754,
        "learning_rate": 1.0932818823113866e-05,
        "epoch": 0.9290131409430559,
        "step": 7211
    },
    {
        "loss": 2.0492,
        "grad_norm": 1.683797001838684,
        "learning_rate": 1.0894884570354535e-05,
        "epoch": 0.9291419737181139,
        "step": 7212
    },
    {
        "loss": 1.8825,
        "grad_norm": 2.564490795135498,
        "learning_rate": 1.0857008194948326e-05,
        "epoch": 0.9292708064931718,
        "step": 7213
    },
    {
        "loss": 2.0972,
        "grad_norm": 2.0407090187072754,
        "learning_rate": 1.0819189752953995e-05,
        "epoch": 0.9293996392682299,
        "step": 7214
    },
    {
        "loss": 2.0815,
        "grad_norm": 2.432871103286743,
        "learning_rate": 1.0781429300344565e-05,
        "epoch": 0.9295284720432878,
        "step": 7215
    },
    {
        "loss": 1.9884,
        "grad_norm": 2.5099844932556152,
        "learning_rate": 1.0743726893007266e-05,
        "epoch": 0.9296573048183457,
        "step": 7216
    },
    {
        "loss": 1.804,
        "grad_norm": 1.895019292831421,
        "learning_rate": 1.0706082586743299e-05,
        "epoch": 0.9297861375934038,
        "step": 7217
    },
    {
        "loss": 1.4518,
        "grad_norm": 1.7473132610321045,
        "learning_rate": 1.0668496437267995e-05,
        "epoch": 0.9299149703684617,
        "step": 7218
    },
    {
        "loss": 1.9518,
        "grad_norm": 2.7506110668182373,
        "learning_rate": 1.0630968500210604e-05,
        "epoch": 0.9300438031435198,
        "step": 7219
    },
    {
        "loss": 1.7543,
        "grad_norm": 2.241368293762207,
        "learning_rate": 1.0593498831114102e-05,
        "epoch": 0.9301726359185777,
        "step": 7220
    },
    {
        "loss": 2.1129,
        "grad_norm": 2.599416971206665,
        "learning_rate": 1.0556087485435351e-05,
        "epoch": 0.9303014686936356,
        "step": 7221
    },
    {
        "loss": 1.6119,
        "grad_norm": 2.4066286087036133,
        "learning_rate": 1.0518734518544842e-05,
        "epoch": 0.9304303014686937,
        "step": 7222
    },
    {
        "loss": 1.4601,
        "grad_norm": 2.1626851558685303,
        "learning_rate": 1.0481439985726682e-05,
        "epoch": 0.9305591342437516,
        "step": 7223
    },
    {
        "loss": 1.5251,
        "grad_norm": 3.108956813812256,
        "learning_rate": 1.0444203942178465e-05,
        "epoch": 0.9306879670188096,
        "step": 7224
    },
    {
        "loss": 2.1829,
        "grad_norm": 2.7152931690216064,
        "learning_rate": 1.0407026443011237e-05,
        "epoch": 0.9308167997938676,
        "step": 7225
    },
    {
        "loss": 1.7597,
        "grad_norm": 3.5682287216186523,
        "learning_rate": 1.0369907543249397e-05,
        "epoch": 0.9309456325689255,
        "step": 7226
    },
    {
        "loss": 0.7283,
        "grad_norm": 3.419705867767334,
        "learning_rate": 1.0332847297830656e-05,
        "epoch": 0.9310744653439835,
        "step": 7227
    },
    {
        "loss": 2.2086,
        "grad_norm": 1.387538194656372,
        "learning_rate": 1.0295845761605755e-05,
        "epoch": 0.9312032981190415,
        "step": 7228
    },
    {
        "loss": 1.616,
        "grad_norm": 3.7041282653808594,
        "learning_rate": 1.025890298933872e-05,
        "epoch": 0.9313321308940995,
        "step": 7229
    },
    {
        "loss": 1.8753,
        "grad_norm": 2.839782953262329,
        "learning_rate": 1.0222019035706527e-05,
        "epoch": 0.9314609636691574,
        "step": 7230
    },
    {
        "loss": 1.9483,
        "grad_norm": 2.3447773456573486,
        "learning_rate": 1.018519395529911e-05,
        "epoch": 0.9315897964442154,
        "step": 7231
    },
    {
        "loss": 1.7996,
        "grad_norm": 1.9799314737319946,
        "learning_rate": 1.014842780261927e-05,
        "epoch": 0.9317186292192734,
        "step": 7232
    },
    {
        "loss": 2.0013,
        "grad_norm": 2.228698492050171,
        "learning_rate": 1.0111720632082628e-05,
        "epoch": 0.9318474619943313,
        "step": 7233
    },
    {
        "loss": 2.1906,
        "grad_norm": 1.9290452003479004,
        "learning_rate": 1.0075072498017385e-05,
        "epoch": 0.9319762947693894,
        "step": 7234
    },
    {
        "loss": 1.9175,
        "grad_norm": 2.9015235900878906,
        "learning_rate": 1.0038483454664498e-05,
        "epoch": 0.9321051275444473,
        "step": 7235
    },
    {
        "loss": 2.4498,
        "grad_norm": 2.296800136566162,
        "learning_rate": 1.0001953556177412e-05,
        "epoch": 0.9322339603195052,
        "step": 7236
    },
    {
        "loss": 1.3633,
        "grad_norm": 2.9470560550689697,
        "learning_rate": 9.965482856622043e-06,
        "epoch": 0.9323627930945633,
        "step": 7237
    },
    {
        "loss": 2.3425,
        "grad_norm": 2.016960620880127,
        "learning_rate": 9.929071409976732e-06,
        "epoch": 0.9324916258696212,
        "step": 7238
    },
    {
        "loss": 1.0685,
        "grad_norm": 3.1147422790527344,
        "learning_rate": 9.89271927013195e-06,
        "epoch": 0.9326204586446792,
        "step": 7239
    },
    {
        "loss": 0.9667,
        "grad_norm": 2.5540573596954346,
        "learning_rate": 9.856426490890652e-06,
        "epoch": 0.9327492914197372,
        "step": 7240
    },
    {
        "loss": 1.706,
        "grad_norm": 2.7099368572235107,
        "learning_rate": 9.820193125967803e-06,
        "epoch": 0.9328781241947952,
        "step": 7241
    },
    {
        "loss": 2.3971,
        "grad_norm": 1.7427630424499512,
        "learning_rate": 9.784019228990348e-06,
        "epoch": 0.9330069569698531,
        "step": 7242
    },
    {
        "loss": 1.1399,
        "grad_norm": 3.0855674743652344,
        "learning_rate": 9.747904853497346e-06,
        "epoch": 0.9331357897449111,
        "step": 7243
    },
    {
        "loss": 1.725,
        "grad_norm": 2.7076852321624756,
        "learning_rate": 9.71185005293978e-06,
        "epoch": 0.9332646225199691,
        "step": 7244
    },
    {
        "loss": 1.0951,
        "grad_norm": 3.651746988296509,
        "learning_rate": 9.67585488068028e-06,
        "epoch": 0.933393455295027,
        "step": 7245
    },
    {
        "loss": 2.2329,
        "grad_norm": 3.4041266441345215,
        "learning_rate": 9.6399193899934e-06,
        "epoch": 0.9335222880700851,
        "step": 7246
    },
    {
        "loss": 2.1922,
        "grad_norm": 1.608896017074585,
        "learning_rate": 9.604043634065307e-06,
        "epoch": 0.933651120845143,
        "step": 7247
    },
    {
        "loss": 2.0438,
        "grad_norm": 3.2437593936920166,
        "learning_rate": 9.568227665993751e-06,
        "epoch": 0.9337799536202009,
        "step": 7248
    },
    {
        "loss": 0.8287,
        "grad_norm": 3.0929040908813477,
        "learning_rate": 9.532471538787996e-06,
        "epoch": 0.933908786395259,
        "step": 7249
    },
    {
        "loss": 2.2829,
        "grad_norm": 2.116713285446167,
        "learning_rate": 9.496775305368744e-06,
        "epoch": 0.9340376191703169,
        "step": 7250
    },
    {
        "loss": 1.5379,
        "grad_norm": 2.2198002338409424,
        "learning_rate": 9.461139018568043e-06,
        "epoch": 0.934166451945375,
        "step": 7251
    },
    {
        "loss": 1.7921,
        "grad_norm": 2.71170973777771,
        "learning_rate": 9.425562731129261e-06,
        "epoch": 0.9342952847204329,
        "step": 7252
    },
    {
        "loss": 2.1749,
        "grad_norm": 2.083284378051758,
        "learning_rate": 9.39004649570685e-06,
        "epoch": 0.9344241174954908,
        "step": 7253
    },
    {
        "loss": 2.1967,
        "grad_norm": 1.814622402191162,
        "learning_rate": 9.354590364866494e-06,
        "epoch": 0.9345529502705489,
        "step": 7254
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.8870351314544678,
        "learning_rate": 9.31919439108493e-06,
        "epoch": 0.9346817830456068,
        "step": 7255
    },
    {
        "loss": 0.9663,
        "grad_norm": 2.406240701675415,
        "learning_rate": 9.28385862674968e-06,
        "epoch": 0.9348106158206648,
        "step": 7256
    },
    {
        "loss": 1.877,
        "grad_norm": 2.5586400032043457,
        "learning_rate": 9.24858312415941e-06,
        "epoch": 0.9349394485957228,
        "step": 7257
    },
    {
        "loss": 1.7122,
        "grad_norm": 1.7578153610229492,
        "learning_rate": 9.213367935523476e-06,
        "epoch": 0.9350682813707807,
        "step": 7258
    },
    {
        "loss": 1.943,
        "grad_norm": 2.5325100421905518,
        "learning_rate": 9.178213112961881e-06,
        "epoch": 0.9351971141458387,
        "step": 7259
    },
    {
        "loss": 2.1224,
        "grad_norm": 1.7481800317764282,
        "learning_rate": 9.1431187085054e-06,
        "epoch": 0.9353259469208967,
        "step": 7260
    },
    {
        "loss": 2.3392,
        "grad_norm": 2.569399118423462,
        "learning_rate": 9.108084774095365e-06,
        "epoch": 0.9354547796959547,
        "step": 7261
    },
    {
        "loss": 2.0318,
        "grad_norm": 2.6493442058563232,
        "learning_rate": 9.073111361583576e-06,
        "epoch": 0.9355836124710126,
        "step": 7262
    },
    {
        "loss": 2.2743,
        "grad_norm": 2.5052032470703125,
        "learning_rate": 9.038198522732321e-06,
        "epoch": 0.9357124452460706,
        "step": 7263
    },
    {
        "loss": 1.9552,
        "grad_norm": 1.621151089668274,
        "learning_rate": 9.003346309214122e-06,
        "epoch": 0.9358412780211286,
        "step": 7264
    },
    {
        "loss": 2.2159,
        "grad_norm": 1.632015347480774,
        "learning_rate": 8.968554772611843e-06,
        "epoch": 0.9359701107961865,
        "step": 7265
    },
    {
        "loss": 1.613,
        "grad_norm": 2.7764699459075928,
        "learning_rate": 8.933823964418653e-06,
        "epoch": 0.9360989435712446,
        "step": 7266
    },
    {
        "loss": 1.8934,
        "grad_norm": 2.042370557785034,
        "learning_rate": 8.899153936037618e-06,
        "epoch": 0.9362277763463025,
        "step": 7267
    },
    {
        "loss": 2.0958,
        "grad_norm": 2.365405797958374,
        "learning_rate": 8.864544738782004e-06,
        "epoch": 0.9363566091213604,
        "step": 7268
    },
    {
        "loss": 1.8452,
        "grad_norm": 1.4887211322784424,
        "learning_rate": 8.829996423875037e-06,
        "epoch": 0.9364854418964185,
        "step": 7269
    },
    {
        "loss": 1.5761,
        "grad_norm": 2.349311351776123,
        "learning_rate": 8.795509042449717e-06,
        "epoch": 0.9366142746714764,
        "step": 7270
    },
    {
        "loss": 0.8694,
        "grad_norm": 3.5077407360076904,
        "learning_rate": 8.761082645548978e-06,
        "epoch": 0.9367431074465344,
        "step": 7271
    },
    {
        "loss": 1.4961,
        "grad_norm": 2.8164222240448,
        "learning_rate": 8.726717284125463e-06,
        "epoch": 0.9368719402215924,
        "step": 7272
    },
    {
        "loss": 1.2838,
        "grad_norm": 3.1769564151763916,
        "learning_rate": 8.692413009041467e-06,
        "epoch": 0.9370007729966503,
        "step": 7273
    },
    {
        "loss": 1.7345,
        "grad_norm": 2.6404201984405518,
        "learning_rate": 8.658169871068882e-06,
        "epoch": 0.9371296057717083,
        "step": 7274
    },
    {
        "loss": 2.0993,
        "grad_norm": 1.8356668949127197,
        "learning_rate": 8.623987920889115e-06,
        "epoch": 0.9372584385467663,
        "step": 7275
    },
    {
        "loss": 2.1425,
        "grad_norm": 1.8053311109542847,
        "learning_rate": 8.58986720909301e-06,
        "epoch": 0.9373872713218243,
        "step": 7276
    },
    {
        "loss": 1.3298,
        "grad_norm": 2.941161870956421,
        "learning_rate": 8.555807786180826e-06,
        "epoch": 0.9375161040968822,
        "step": 7277
    },
    {
        "loss": 1.9848,
        "grad_norm": 3.1131012439727783,
        "learning_rate": 8.521809702561956e-06,
        "epoch": 0.9376449368719402,
        "step": 7278
    },
    {
        "loss": 1.7367,
        "grad_norm": 2.4173712730407715,
        "learning_rate": 8.48787300855517e-06,
        "epoch": 0.9377737696469982,
        "step": 7279
    },
    {
        "loss": 2.1989,
        "grad_norm": 1.7541254758834839,
        "learning_rate": 8.453997754388354e-06,
        "epoch": 0.9379026024220561,
        "step": 7280
    },
    {
        "loss": 1.6215,
        "grad_norm": 2.7505264282226562,
        "learning_rate": 8.420183990198354e-06,
        "epoch": 0.9380314351971142,
        "step": 7281
    },
    {
        "loss": 1.7963,
        "grad_norm": 3.2809700965881348,
        "learning_rate": 8.386431766031088e-06,
        "epoch": 0.9381602679721721,
        "step": 7282
    },
    {
        "loss": 1.5758,
        "grad_norm": 2.679131031036377,
        "learning_rate": 8.35274113184149e-06,
        "epoch": 0.93828910074723,
        "step": 7283
    },
    {
        "loss": 2.3612,
        "grad_norm": 2.1511483192443848,
        "learning_rate": 8.31911213749313e-06,
        "epoch": 0.9384179335222881,
        "step": 7284
    },
    {
        "loss": 1.8738,
        "grad_norm": 2.8187363147735596,
        "learning_rate": 8.285544832758485e-06,
        "epoch": 0.938546766297346,
        "step": 7285
    },
    {
        "loss": 1.8622,
        "grad_norm": 2.931044340133667,
        "learning_rate": 8.252039267318695e-06,
        "epoch": 0.938675599072404,
        "step": 7286
    },
    {
        "loss": 1.5268,
        "grad_norm": 2.940591335296631,
        "learning_rate": 8.218595490763525e-06,
        "epoch": 0.938804431847462,
        "step": 7287
    },
    {
        "loss": 1.3706,
        "grad_norm": 2.855412244796753,
        "learning_rate": 8.185213552591325e-06,
        "epoch": 0.9389332646225199,
        "step": 7288
    },
    {
        "loss": 1.5171,
        "grad_norm": 2.431347131729126,
        "learning_rate": 8.151893502208812e-06,
        "epoch": 0.939062097397578,
        "step": 7289
    },
    {
        "loss": 1.6776,
        "grad_norm": 2.2218499183654785,
        "learning_rate": 8.118635388931185e-06,
        "epoch": 0.9391909301726359,
        "step": 7290
    },
    {
        "loss": 1.2203,
        "grad_norm": 3.0727643966674805,
        "learning_rate": 8.085439261982063e-06,
        "epoch": 0.9393197629476939,
        "step": 7291
    },
    {
        "loss": 1.2357,
        "grad_norm": 2.466883659362793,
        "learning_rate": 8.052305170493136e-06,
        "epoch": 0.9394485957227519,
        "step": 7292
    },
    {
        "loss": 1.2307,
        "grad_norm": 3.4842963218688965,
        "learning_rate": 8.019233163504392e-06,
        "epoch": 0.9395774284978099,
        "step": 7293
    },
    {
        "loss": 2.2202,
        "grad_norm": 2.9417495727539062,
        "learning_rate": 7.986223289963956e-06,
        "epoch": 0.9397062612728678,
        "step": 7294
    },
    {
        "loss": 2.0082,
        "grad_norm": 2.3547894954681396,
        "learning_rate": 7.953275598727855e-06,
        "epoch": 0.9398350940479258,
        "step": 7295
    },
    {
        "loss": 2.4441,
        "grad_norm": 2.293355703353882,
        "learning_rate": 7.920390138560223e-06,
        "epoch": 0.9399639268229838,
        "step": 7296
    },
    {
        "loss": 1.8772,
        "grad_norm": 3.0031888484954834,
        "learning_rate": 7.887566958133042e-06,
        "epoch": 0.9400927595980417,
        "step": 7297
    },
    {
        "loss": 1.795,
        "grad_norm": 2.5100388526916504,
        "learning_rate": 7.854806106026102e-06,
        "epoch": 0.9402215923730998,
        "step": 7298
    },
    {
        "loss": 1.2835,
        "grad_norm": 2.8201563358306885,
        "learning_rate": 7.822107630726972e-06,
        "epoch": 0.9403504251481577,
        "step": 7299
    },
    {
        "loss": 2.4463,
        "grad_norm": 1.997735857963562,
        "learning_rate": 7.789471580630875e-06,
        "epoch": 0.9404792579232156,
        "step": 7300
    },
    {
        "loss": 1.8574,
        "grad_norm": 2.822406530380249,
        "learning_rate": 7.756898004040664e-06,
        "epoch": 0.9406080906982737,
        "step": 7301
    },
    {
        "loss": 2.2537,
        "grad_norm": 1.871566653251648,
        "learning_rate": 7.72438694916674e-06,
        "epoch": 0.9407369234733316,
        "step": 7302
    },
    {
        "loss": 1.8777,
        "grad_norm": 2.1421351432800293,
        "learning_rate": 7.69193846412688e-06,
        "epoch": 0.9408657562483896,
        "step": 7303
    },
    {
        "loss": 1.9315,
        "grad_norm": 2.6206631660461426,
        "learning_rate": 7.659552596946357e-06,
        "epoch": 0.9409945890234476,
        "step": 7304
    },
    {
        "loss": 1.7882,
        "grad_norm": 2.1022984981536865,
        "learning_rate": 7.6272293955577666e-06,
        "epoch": 0.9411234217985055,
        "step": 7305
    },
    {
        "loss": 1.8749,
        "grad_norm": 1.6470457315444946,
        "learning_rate": 7.594968907800848e-06,
        "epoch": 0.9412522545735635,
        "step": 7306
    },
    {
        "loss": 1.833,
        "grad_norm": 2.542219638824463,
        "learning_rate": 7.562771181422629e-06,
        "epoch": 0.9413810873486215,
        "step": 7307
    },
    {
        "loss": 2.322,
        "grad_norm": 2.497852087020874,
        "learning_rate": 7.530636264077212e-06,
        "epoch": 0.9415099201236795,
        "step": 7308
    },
    {
        "loss": 2.0776,
        "grad_norm": 2.3793013095855713,
        "learning_rate": 7.498564203325742e-06,
        "epoch": 0.9416387528987374,
        "step": 7309
    },
    {
        "loss": 2.2682,
        "grad_norm": 1.5995255708694458,
        "learning_rate": 7.4665550466363254e-06,
        "epoch": 0.9417675856737954,
        "step": 7310
    },
    {
        "loss": 2.4845,
        "grad_norm": 1.8082002401351929,
        "learning_rate": 7.434608841383989e-06,
        "epoch": 0.9418964184488534,
        "step": 7311
    },
    {
        "loss": 1.8745,
        "grad_norm": 1.9330496788024902,
        "learning_rate": 7.402725634850544e-06,
        "epoch": 0.9420252512239113,
        "step": 7312
    },
    {
        "loss": 2.1282,
        "grad_norm": 2.432734727859497,
        "learning_rate": 7.370905474224643e-06,
        "epoch": 0.9421540839989694,
        "step": 7313
    },
    {
        "loss": 2.2379,
        "grad_norm": 2.10150146484375,
        "learning_rate": 7.339148406601498e-06,
        "epoch": 0.9422829167740273,
        "step": 7314
    },
    {
        "loss": 1.7826,
        "grad_norm": 1.937724232673645,
        "learning_rate": 7.307454478983056e-06,
        "epoch": 0.9424117495490852,
        "step": 7315
    },
    {
        "loss": 1.3434,
        "grad_norm": 2.9561166763305664,
        "learning_rate": 7.27582373827777e-06,
        "epoch": 0.9425405823241433,
        "step": 7316
    },
    {
        "loss": 1.6422,
        "grad_norm": 2.562952995300293,
        "learning_rate": 7.244256231300584e-06,
        "epoch": 0.9426694150992012,
        "step": 7317
    },
    {
        "loss": 2.3887,
        "grad_norm": 1.4706485271453857,
        "learning_rate": 7.212752004772844e-06,
        "epoch": 0.9427982478742593,
        "step": 7318
    },
    {
        "loss": 1.9863,
        "grad_norm": 2.3284194469451904,
        "learning_rate": 7.181311105322269e-06,
        "epoch": 0.9429270806493172,
        "step": 7319
    },
    {
        "loss": 1.9971,
        "grad_norm": 2.32631254196167,
        "learning_rate": 7.149933579482765e-06,
        "epoch": 0.9430559134243751,
        "step": 7320
    },
    {
        "loss": 1.565,
        "grad_norm": 2.651611804962158,
        "learning_rate": 7.118619473694527e-06,
        "epoch": 0.9431847461994332,
        "step": 7321
    },
    {
        "loss": 2.0493,
        "grad_norm": 2.054849624633789,
        "learning_rate": 7.087368834303859e-06,
        "epoch": 0.9433135789744911,
        "step": 7322
    },
    {
        "loss": 1.5182,
        "grad_norm": 3.834404230117798,
        "learning_rate": 7.056181707563142e-06,
        "epoch": 0.9434424117495491,
        "step": 7323
    },
    {
        "loss": 2.1869,
        "grad_norm": 1.7002512216567993,
        "learning_rate": 7.025058139630763e-06,
        "epoch": 0.9435712445246071,
        "step": 7324
    },
    {
        "loss": 1.8494,
        "grad_norm": 3.369417905807495,
        "learning_rate": 6.993998176570932e-06,
        "epoch": 0.943700077299665,
        "step": 7325
    },
    {
        "loss": 1.1166,
        "grad_norm": 3.668846607208252,
        "learning_rate": 6.963001864353907e-06,
        "epoch": 0.943828910074723,
        "step": 7326
    },
    {
        "loss": 2.2662,
        "grad_norm": 2.0378217697143555,
        "learning_rate": 6.932069248855655e-06,
        "epoch": 0.943957742849781,
        "step": 7327
    },
    {
        "loss": 2.3916,
        "grad_norm": 1.64710533618927,
        "learning_rate": 6.9012003758577635e-06,
        "epoch": 0.944086575624839,
        "step": 7328
    },
    {
        "loss": 1.868,
        "grad_norm": 2.557619333267212,
        "learning_rate": 6.870395291047615e-06,
        "epoch": 0.9442154083998969,
        "step": 7329
    },
    {
        "loss": 1.369,
        "grad_norm": 3.9579720497131348,
        "learning_rate": 6.839654040018184e-06,
        "epoch": 0.9443442411749549,
        "step": 7330
    },
    {
        "loss": 1.3641,
        "grad_norm": 2.7639782428741455,
        "learning_rate": 6.808976668267841e-06,
        "epoch": 0.9444730739500129,
        "step": 7331
    },
    {
        "loss": 1.7174,
        "grad_norm": 1.5932737588882446,
        "learning_rate": 6.77836322120054e-06,
        "epoch": 0.9446019067250708,
        "step": 7332
    },
    {
        "loss": 2.0251,
        "grad_norm": 1.6617568731307983,
        "learning_rate": 6.747813744125569e-06,
        "epoch": 0.9447307395001289,
        "step": 7333
    },
    {
        "loss": 1.9306,
        "grad_norm": 1.5515400171279907,
        "learning_rate": 6.717328282257556e-06,
        "epoch": 0.9448595722751868,
        "step": 7334
    },
    {
        "loss": 2.1202,
        "grad_norm": 2.4183859825134277,
        "learning_rate": 6.686906880716365e-06,
        "epoch": 0.9449884050502447,
        "step": 7335
    },
    {
        "loss": 1.4906,
        "grad_norm": 2.531785488128662,
        "learning_rate": 6.656549584527061e-06,
        "epoch": 0.9451172378253028,
        "step": 7336
    },
    {
        "loss": 2.5145,
        "grad_norm": 1.58047354221344,
        "learning_rate": 6.626256438619827e-06,
        "epoch": 0.9452460706003607,
        "step": 7337
    },
    {
        "loss": 2.316,
        "grad_norm": 2.256429433822632,
        "learning_rate": 6.596027487829926e-06,
        "epoch": 0.9453749033754187,
        "step": 7338
    },
    {
        "loss": 2.0437,
        "grad_norm": 1.7932380437850952,
        "learning_rate": 6.565862776897524e-06,
        "epoch": 0.9455037361504767,
        "step": 7339
    },
    {
        "loss": 2.0116,
        "grad_norm": 2.4985344409942627,
        "learning_rate": 6.5357623504678e-06,
        "epoch": 0.9456325689255347,
        "step": 7340
    },
    {
        "loss": 1.916,
        "grad_norm": 3.004589319229126,
        "learning_rate": 6.505726253090799e-06,
        "epoch": 0.9457614017005926,
        "step": 7341
    },
    {
        "loss": 1.5426,
        "grad_norm": 3.2530932426452637,
        "learning_rate": 6.4757545292211845e-06,
        "epoch": 0.9458902344756506,
        "step": 7342
    },
    {
        "loss": 1.737,
        "grad_norm": 2.5533268451690674,
        "learning_rate": 6.445847223218598e-06,
        "epoch": 0.9460190672507086,
        "step": 7343
    },
    {
        "loss": 2.0752,
        "grad_norm": 3.2030253410339355,
        "learning_rate": 6.416004379347229e-06,
        "epoch": 0.9461479000257665,
        "step": 7344
    },
    {
        "loss": 2.6188,
        "grad_norm": 2.4549732208251953,
        "learning_rate": 6.386226041775745e-06,
        "epoch": 0.9462767328008246,
        "step": 7345
    },
    {
        "loss": 1.5687,
        "grad_norm": 2.261228561401367,
        "learning_rate": 6.356512254577496e-06,
        "epoch": 0.9464055655758825,
        "step": 7346
    },
    {
        "loss": 1.4274,
        "grad_norm": 2.9195199012756348,
        "learning_rate": 6.326863061730237e-06,
        "epoch": 0.9465343983509404,
        "step": 7347
    },
    {
        "loss": 2.131,
        "grad_norm": 2.2664434909820557,
        "learning_rate": 6.297278507116128e-06,
        "epoch": 0.9466632311259985,
        "step": 7348
    },
    {
        "loss": 1.2098,
        "grad_norm": 3.328843116760254,
        "learning_rate": 6.267758634521687e-06,
        "epoch": 0.9467920639010564,
        "step": 7349
    },
    {
        "loss": 2.3219,
        "grad_norm": 2.424783945083618,
        "learning_rate": 6.238303487637576e-06,
        "epoch": 0.9469208966761145,
        "step": 7350
    },
    {
        "loss": 2.0421,
        "grad_norm": 3.039297103881836,
        "learning_rate": 6.208913110058856e-06,
        "epoch": 0.9470497294511724,
        "step": 7351
    },
    {
        "loss": 2.1234,
        "grad_norm": 1.4551335573196411,
        "learning_rate": 6.179587545284615e-06,
        "epoch": 0.9471785622262303,
        "step": 7352
    },
    {
        "loss": 0.7873,
        "grad_norm": 1.4567047357559204,
        "learning_rate": 6.150326836717968e-06,
        "epoch": 0.9473073950012884,
        "step": 7353
    },
    {
        "loss": 2.3133,
        "grad_norm": 2.0976953506469727,
        "learning_rate": 6.121131027666122e-06,
        "epoch": 0.9474362277763463,
        "step": 7354
    },
    {
        "loss": 1.9934,
        "grad_norm": 2.0557520389556885,
        "learning_rate": 6.09200016134025e-06,
        "epoch": 0.9475650605514043,
        "step": 7355
    },
    {
        "loss": 1.9116,
        "grad_norm": 1.9360098838806152,
        "learning_rate": 6.062934280855287e-06,
        "epoch": 0.9476938933264623,
        "step": 7356
    },
    {
        "loss": 1.7453,
        "grad_norm": 2.4929287433624268,
        "learning_rate": 6.033933429230099e-06,
        "epoch": 0.9478227261015202,
        "step": 7357
    },
    {
        "loss": 0.8473,
        "grad_norm": 2.088381767272949,
        "learning_rate": 6.004997649387267e-06,
        "epoch": 0.9479515588765782,
        "step": 7358
    },
    {
        "loss": 2.5906,
        "grad_norm": 2.2035388946533203,
        "learning_rate": 5.976126984153063e-06,
        "epoch": 0.9480803916516362,
        "step": 7359
    },
    {
        "loss": 2.5702,
        "grad_norm": 2.3055543899536133,
        "learning_rate": 5.947321476257395e-06,
        "epoch": 0.9482092244266942,
        "step": 7360
    },
    {
        "loss": 2.2536,
        "grad_norm": 1.8176273107528687,
        "learning_rate": 5.918581168333709e-06,
        "epoch": 0.9483380572017521,
        "step": 7361
    },
    {
        "loss": 1.7744,
        "grad_norm": 2.6766433715820312,
        "learning_rate": 5.889906102918985e-06,
        "epoch": 0.9484668899768101,
        "step": 7362
    },
    {
        "loss": 1.7642,
        "grad_norm": 3.035360097885132,
        "learning_rate": 5.861296322453652e-06,
        "epoch": 0.9485957227518681,
        "step": 7363
    },
    {
        "loss": 2.5164,
        "grad_norm": 2.3254761695861816,
        "learning_rate": 5.832751869281428e-06,
        "epoch": 0.948724555526926,
        "step": 7364
    },
    {
        "loss": 1.4378,
        "grad_norm": 3.9860246181488037,
        "learning_rate": 5.804272785649434e-06,
        "epoch": 0.9488533883019841,
        "step": 7365
    },
    {
        "loss": 1.944,
        "grad_norm": 2.0608012676239014,
        "learning_rate": 5.775859113708055e-06,
        "epoch": 0.948982221077042,
        "step": 7366
    },
    {
        "loss": 1.7376,
        "grad_norm": 2.886584997177124,
        "learning_rate": 5.747510895510711e-06,
        "epoch": 0.9491110538520999,
        "step": 7367
    },
    {
        "loss": 2.2881,
        "grad_norm": 2.709630012512207,
        "learning_rate": 5.719228173014157e-06,
        "epoch": 0.949239886627158,
        "step": 7368
    },
    {
        "loss": 2.5739,
        "grad_norm": 2.726377248764038,
        "learning_rate": 5.691010988078111e-06,
        "epoch": 0.9493687194022159,
        "step": 7369
    },
    {
        "loss": 1.5936,
        "grad_norm": 2.3001835346221924,
        "learning_rate": 5.662859382465235e-06,
        "epoch": 0.9494975521772739,
        "step": 7370
    },
    {
        "loss": 2.2033,
        "grad_norm": 2.980733871459961,
        "learning_rate": 5.6347733978412085e-06,
        "epoch": 0.9496263849523319,
        "step": 7371
    },
    {
        "loss": 2.1463,
        "grad_norm": 1.2480709552764893,
        "learning_rate": 5.606753075774585e-06,
        "epoch": 0.9497552177273898,
        "step": 7372
    },
    {
        "loss": 1.8079,
        "grad_norm": 2.8227648735046387,
        "learning_rate": 5.578798457736701e-06,
        "epoch": 0.9498840505024478,
        "step": 7373
    },
    {
        "loss": 1.9551,
        "grad_norm": 1.8311446905136108,
        "learning_rate": 5.55090958510171e-06,
        "epoch": 0.9500128832775058,
        "step": 7374
    },
    {
        "loss": 1.0729,
        "grad_norm": 3.508988380432129,
        "learning_rate": 5.52308649914634e-06,
        "epoch": 0.9501417160525638,
        "step": 7375
    },
    {
        "loss": 2.5151,
        "grad_norm": 2.0666558742523193,
        "learning_rate": 5.49532924105004e-06,
        "epoch": 0.9502705488276217,
        "step": 7376
    },
    {
        "loss": 2.1181,
        "grad_norm": 2.088191032409668,
        "learning_rate": 5.467637851894908e-06,
        "epoch": 0.9503993816026797,
        "step": 7377
    },
    {
        "loss": 2.4046,
        "grad_norm": 1.8012194633483887,
        "learning_rate": 5.44001237266536e-06,
        "epoch": 0.9505282143777377,
        "step": 7378
    },
    {
        "loss": 1.7892,
        "grad_norm": 2.406554937362671,
        "learning_rate": 5.412452844248395e-06,
        "epoch": 0.9506570471527956,
        "step": 7379
    },
    {
        "loss": 1.7436,
        "grad_norm": 2.7112557888031006,
        "learning_rate": 5.384959307433407e-06,
        "epoch": 0.9507858799278537,
        "step": 7380
    },
    {
        "loss": 1.6764,
        "grad_norm": 2.4887335300445557,
        "learning_rate": 5.357531802912019e-06,
        "epoch": 0.9509147127029116,
        "step": 7381
    },
    {
        "loss": 1.813,
        "grad_norm": 3.698413133621216,
        "learning_rate": 5.330170371278226e-06,
        "epoch": 0.9510435454779695,
        "step": 7382
    },
    {
        "loss": 1.5053,
        "grad_norm": 3.04614520072937,
        "learning_rate": 5.302875053028189e-06,
        "epoch": 0.9511723782530276,
        "step": 7383
    },
    {
        "loss": 0.4174,
        "grad_norm": 2.6196510791778564,
        "learning_rate": 5.275645888560221e-06,
        "epoch": 0.9513012110280855,
        "step": 7384
    },
    {
        "loss": 1.0107,
        "grad_norm": 3.4342048168182373,
        "learning_rate": 5.24848291817473e-06,
        "epoch": 0.9514300438031436,
        "step": 7385
    },
    {
        "loss": 1.9363,
        "grad_norm": 1.645866870880127,
        "learning_rate": 5.221386182074145e-06,
        "epoch": 0.9515588765782015,
        "step": 7386
    },
    {
        "loss": 1.6791,
        "grad_norm": 2.2994232177734375,
        "learning_rate": 5.194355720362876e-06,
        "epoch": 0.9516877093532594,
        "step": 7387
    },
    {
        "loss": 1.8755,
        "grad_norm": 2.813385009765625,
        "learning_rate": 5.167391573047264e-06,
        "epoch": 0.9518165421283175,
        "step": 7388
    },
    {
        "loss": 2.0143,
        "grad_norm": 2.1873459815979004,
        "learning_rate": 5.140493780035421e-06,
        "epoch": 0.9519453749033754,
        "step": 7389
    },
    {
        "loss": 1.8986,
        "grad_norm": 2.1252620220184326,
        "learning_rate": 5.11366238113733e-06,
        "epoch": 0.9520742076784334,
        "step": 7390
    },
    {
        "loss": 1.5463,
        "grad_norm": 3.2094333171844482,
        "learning_rate": 5.086897416064734e-06,
        "epoch": 0.9522030404534914,
        "step": 7391
    },
    {
        "loss": 1.8817,
        "grad_norm": 2.5893006324768066,
        "learning_rate": 5.060198924430909e-06,
        "epoch": 0.9523318732285494,
        "step": 7392
    },
    {
        "loss": 1.4687,
        "grad_norm": 2.4558863639831543,
        "learning_rate": 5.033566945750851e-06,
        "epoch": 0.9524607060036073,
        "step": 7393
    },
    {
        "loss": 1.7944,
        "grad_norm": 2.6678576469421387,
        "learning_rate": 5.0070015194412125e-06,
        "epoch": 0.9525895387786653,
        "step": 7394
    },
    {
        "loss": 1.2258,
        "grad_norm": 1.925510287284851,
        "learning_rate": 4.980502684819921e-06,
        "epoch": 0.9527183715537233,
        "step": 7395
    },
    {
        "loss": 1.9209,
        "grad_norm": 2.9014618396759033,
        "learning_rate": 4.954070481106499e-06,
        "epoch": 0.9528472043287812,
        "step": 7396
    },
    {
        "loss": 1.5246,
        "grad_norm": 3.2483232021331787,
        "learning_rate": 4.927704947421813e-06,
        "epoch": 0.9529760371038393,
        "step": 7397
    },
    {
        "loss": 1.8194,
        "grad_norm": 2.7075531482696533,
        "learning_rate": 4.901406122788049e-06,
        "epoch": 0.9531048698788972,
        "step": 7398
    },
    {
        "loss": 1.797,
        "grad_norm": 1.9600309133529663,
        "learning_rate": 4.8751740461286995e-06,
        "epoch": 0.9532337026539551,
        "step": 7399
    },
    {
        "loss": 1.7259,
        "grad_norm": 2.000746250152588,
        "learning_rate": 4.84900875626838e-06,
        "epoch": 0.9533625354290132,
        "step": 7400
    },
    {
        "loss": 2.2564,
        "grad_norm": 1.5372228622436523,
        "learning_rate": 4.822910291932936e-06,
        "epoch": 0.9534913682040711,
        "step": 7401
    },
    {
        "loss": 1.8343,
        "grad_norm": 3.047248601913452,
        "learning_rate": 4.796878691749279e-06,
        "epoch": 0.9536202009791291,
        "step": 7402
    },
    {
        "loss": 2.0724,
        "grad_norm": 1.8845462799072266,
        "learning_rate": 4.770913994245374e-06,
        "epoch": 0.9537490337541871,
        "step": 7403
    },
    {
        "loss": 2.3094,
        "grad_norm": 1.5951085090637207,
        "learning_rate": 4.745016237850153e-06,
        "epoch": 0.953877866529245,
        "step": 7404
    },
    {
        "loss": 1.8871,
        "grad_norm": 1.7474678754806519,
        "learning_rate": 4.719185460893499e-06,
        "epoch": 0.954006699304303,
        "step": 7405
    },
    {
        "loss": 2.452,
        "grad_norm": 1.6811012029647827,
        "learning_rate": 4.693421701606088e-06,
        "epoch": 0.954135532079361,
        "step": 7406
    },
    {
        "loss": 2.184,
        "grad_norm": 2.907560348510742,
        "learning_rate": 4.667724998119483e-06,
        "epoch": 0.954264364854419,
        "step": 7407
    },
    {
        "loss": 1.0244,
        "grad_norm": 3.603229522705078,
        "learning_rate": 4.642095388465983e-06,
        "epoch": 0.9543931976294769,
        "step": 7408
    },
    {
        "loss": 1.4039,
        "grad_norm": 2.648366689682007,
        "learning_rate": 4.616532910578569e-06,
        "epoch": 0.9545220304045349,
        "step": 7409
    },
    {
        "loss": 1.4263,
        "grad_norm": 4.071811676025391,
        "learning_rate": 4.591037602290893e-06,
        "epoch": 0.9546508631795929,
        "step": 7410
    },
    {
        "loss": 2.5518,
        "grad_norm": 1.6036629676818848,
        "learning_rate": 4.565609501337142e-06,
        "epoch": 0.9547796959546508,
        "step": 7411
    },
    {
        "loss": 2.4635,
        "grad_norm": 1.4548507928848267,
        "learning_rate": 4.540248645352086e-06,
        "epoch": 0.9549085287297089,
        "step": 7412
    },
    {
        "loss": 2.2155,
        "grad_norm": 1.2901737689971924,
        "learning_rate": 4.5149550718709686e-06,
        "epoch": 0.9550373615047668,
        "step": 7413
    },
    {
        "loss": 1.5551,
        "grad_norm": 1.8111579418182373,
        "learning_rate": 4.489728818329375e-06,
        "epoch": 0.9551661942798247,
        "step": 7414
    },
    {
        "loss": 2.3332,
        "grad_norm": 1.9973424673080444,
        "learning_rate": 4.464569922063344e-06,
        "epoch": 0.9552950270548828,
        "step": 7415
    },
    {
        "loss": 1.6228,
        "grad_norm": 3.3682916164398193,
        "learning_rate": 4.4394784203092065e-06,
        "epoch": 0.9554238598299407,
        "step": 7416
    },
    {
        "loss": 2.4738,
        "grad_norm": 1.7153908014297485,
        "learning_rate": 4.414454350203484e-06,
        "epoch": 0.9555526926049988,
        "step": 7417
    },
    {
        "loss": 1.8796,
        "grad_norm": 2.5738513469696045,
        "learning_rate": 4.389497748782967e-06,
        "epoch": 0.9556815253800567,
        "step": 7418
    },
    {
        "loss": 1.9065,
        "grad_norm": 2.805398941040039,
        "learning_rate": 4.364608652984553e-06,
        "epoch": 0.9558103581551146,
        "step": 7419
    },
    {
        "loss": 1.5487,
        "grad_norm": 2.5408401489257812,
        "learning_rate": 4.339787099645248e-06,
        "epoch": 0.9559391909301727,
        "step": 7420
    },
    {
        "loss": 1.4541,
        "grad_norm": 2.549255132675171,
        "learning_rate": 4.315033125502083e-06,
        "epoch": 0.9560680237052306,
        "step": 7421
    },
    {
        "loss": 1.8068,
        "grad_norm": 2.329693555831909,
        "learning_rate": 4.290346767192061e-06,
        "epoch": 0.9561968564802886,
        "step": 7422
    },
    {
        "loss": 1.5032,
        "grad_norm": 2.7396395206451416,
        "learning_rate": 4.2657280612521165e-06,
        "epoch": 0.9563256892553466,
        "step": 7423
    },
    {
        "loss": 1.6773,
        "grad_norm": 2.912729501724243,
        "learning_rate": 4.241177044119093e-06,
        "epoch": 0.9564545220304045,
        "step": 7424
    },
    {
        "loss": 1.8135,
        "grad_norm": 2.9090843200683594,
        "learning_rate": 4.216693752129553e-06,
        "epoch": 0.9565833548054625,
        "step": 7425
    },
    {
        "loss": 1.4217,
        "grad_norm": 2.521859645843506,
        "learning_rate": 4.192278221519924e-06,
        "epoch": 0.9567121875805205,
        "step": 7426
    },
    {
        "loss": 1.7053,
        "grad_norm": 2.021575450897217,
        "learning_rate": 4.167930488426286e-06,
        "epoch": 0.9568410203555785,
        "step": 7427
    },
    {
        "loss": 2.2057,
        "grad_norm": 2.86248517036438,
        "learning_rate": 4.143650588884401e-06,
        "epoch": 0.9569698531306364,
        "step": 7428
    },
    {
        "loss": 1.052,
        "grad_norm": 4.87317419052124,
        "learning_rate": 4.119438558829625e-06,
        "epoch": 0.9570986859056944,
        "step": 7429
    },
    {
        "loss": 2.0696,
        "grad_norm": 2.1002538204193115,
        "learning_rate": 4.095294434096897e-06,
        "epoch": 0.9572275186807524,
        "step": 7430
    },
    {
        "loss": 1.8952,
        "grad_norm": 2.5451674461364746,
        "learning_rate": 4.071218250420566e-06,
        "epoch": 0.9573563514558103,
        "step": 7431
    },
    {
        "loss": 1.6699,
        "grad_norm": 2.7217299938201904,
        "learning_rate": 4.047210043434508e-06,
        "epoch": 0.9574851842308684,
        "step": 7432
    },
    {
        "loss": 1.4168,
        "grad_norm": 2.210198163986206,
        "learning_rate": 4.023269848671968e-06,
        "epoch": 0.9576140170059263,
        "step": 7433
    },
    {
        "loss": 1.4387,
        "grad_norm": 3.2820382118225098,
        "learning_rate": 3.999397701565522e-06,
        "epoch": 0.9577428497809842,
        "step": 7434
    },
    {
        "loss": 1.8375,
        "grad_norm": 2.33518123626709,
        "learning_rate": 3.975593637447056e-06,
        "epoch": 0.9578716825560423,
        "step": 7435
    },
    {
        "loss": 2.0993,
        "grad_norm": 2.597963809967041,
        "learning_rate": 3.9518576915476e-06,
        "epoch": 0.9580005153311002,
        "step": 7436
    },
    {
        "loss": 2.0467,
        "grad_norm": 2.8090033531188965,
        "learning_rate": 3.928189898997509e-06,
        "epoch": 0.9581293481061582,
        "step": 7437
    },
    {
        "loss": 2.1912,
        "grad_norm": 3.117791175842285,
        "learning_rate": 3.904590294826199e-06,
        "epoch": 0.9582581808812162,
        "step": 7438
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.0400633811950684,
        "learning_rate": 3.881058913962104e-06,
        "epoch": 0.9583870136562742,
        "step": 7439
    },
    {
        "loss": 1.7962,
        "grad_norm": 2.8175559043884277,
        "learning_rate": 3.857595791232765e-06,
        "epoch": 0.9585158464313321,
        "step": 7440
    },
    {
        "loss": 1.8441,
        "grad_norm": 2.969785213470459,
        "learning_rate": 3.834200961364715e-06,
        "epoch": 0.9586446792063901,
        "step": 7441
    },
    {
        "loss": 2.2913,
        "grad_norm": 1.4372494220733643,
        "learning_rate": 3.8108744589832936e-06,
        "epoch": 0.9587735119814481,
        "step": 7442
    },
    {
        "loss": 2.2089,
        "grad_norm": 2.350733518600464,
        "learning_rate": 3.7876163186128187e-06,
        "epoch": 0.958902344756506,
        "step": 7443
    },
    {
        "loss": 1.9439,
        "grad_norm": 2.2563610076904297,
        "learning_rate": 3.7644265746764053e-06,
        "epoch": 0.9590311775315641,
        "step": 7444
    },
    {
        "loss": 1.5648,
        "grad_norm": 2.846175193786621,
        "learning_rate": 3.741305261495931e-06,
        "epoch": 0.959160010306622,
        "step": 7445
    },
    {
        "loss": 1.8569,
        "grad_norm": 4.277270793914795,
        "learning_rate": 3.718252413291984e-06,
        "epoch": 0.9592888430816799,
        "step": 7446
    },
    {
        "loss": 1.7231,
        "grad_norm": 2.5874361991882324,
        "learning_rate": 3.695268064183843e-06,
        "epoch": 0.959417675856738,
        "step": 7447
    },
    {
        "loss": 1.3381,
        "grad_norm": 2.430532455444336,
        "learning_rate": 3.672352248189381e-06,
        "epoch": 0.9595465086317959,
        "step": 7448
    },
    {
        "loss": 1.7839,
        "grad_norm": 2.7667369842529297,
        "learning_rate": 3.649504999225084e-06,
        "epoch": 0.959675341406854,
        "step": 7449
    },
    {
        "loss": 1.9621,
        "grad_norm": 2.205751419067383,
        "learning_rate": 3.6267263511058634e-06,
        "epoch": 0.9598041741819119,
        "step": 7450
    },
    {
        "loss": 2.1402,
        "grad_norm": 2.804058313369751,
        "learning_rate": 3.604016337545185e-06,
        "epoch": 0.9599330069569698,
        "step": 7451
    },
    {
        "loss": 1.6862,
        "grad_norm": 2.5211005210876465,
        "learning_rate": 3.5813749921549342e-06,
        "epoch": 0.9600618397320279,
        "step": 7452
    },
    {
        "loss": 2.2462,
        "grad_norm": 2.002747058868408,
        "learning_rate": 3.5588023484452558e-06,
        "epoch": 0.9601906725070858,
        "step": 7453
    },
    {
        "loss": 1.9522,
        "grad_norm": 2.4886248111724854,
        "learning_rate": 3.536298439824759e-06,
        "epoch": 0.9603195052821438,
        "step": 7454
    },
    {
        "loss": 2.0252,
        "grad_norm": 1.867362141609192,
        "learning_rate": 3.5138632996002676e-06,
        "epoch": 0.9604483380572018,
        "step": 7455
    },
    {
        "loss": 1.9599,
        "grad_norm": 2.101262331008911,
        "learning_rate": 3.491496960976748e-06,
        "epoch": 0.9605771708322597,
        "step": 7456
    },
    {
        "loss": 1.7894,
        "grad_norm": 4.505338191986084,
        "learning_rate": 3.469199457057437e-06,
        "epoch": 0.9607060036073177,
        "step": 7457
    },
    {
        "loss": 1.5253,
        "grad_norm": 2.6991002559661865,
        "learning_rate": 3.446970820843648e-06,
        "epoch": 0.9608348363823757,
        "step": 7458
    },
    {
        "loss": 2.3954,
        "grad_norm": 2.095510721206665,
        "learning_rate": 3.4248110852347858e-06,
        "epoch": 0.9609636691574337,
        "step": 7459
    },
    {
        "loss": 1.5699,
        "grad_norm": 1.6453684568405151,
        "learning_rate": 3.402720283028288e-06,
        "epoch": 0.9610925019324916,
        "step": 7460
    },
    {
        "loss": 2.2615,
        "grad_norm": 2.5435476303100586,
        "learning_rate": 3.3806984469194947e-06,
        "epoch": 0.9612213347075496,
        "step": 7461
    },
    {
        "loss": 2.1492,
        "grad_norm": 2.3194475173950195,
        "learning_rate": 3.3587456095017343e-06,
        "epoch": 0.9613501674826076,
        "step": 7462
    },
    {
        "loss": 2.2796,
        "grad_norm": 2.3733069896698,
        "learning_rate": 3.3368618032662722e-06,
        "epoch": 0.9614790002576655,
        "step": 7463
    },
    {
        "loss": 2.1292,
        "grad_norm": 2.0131006240844727,
        "learning_rate": 3.3150470606020723e-06,
        "epoch": 0.9616078330327236,
        "step": 7464
    },
    {
        "loss": 2.4054,
        "grad_norm": 1.6288652420043945,
        "learning_rate": 3.293301413795974e-06,
        "epoch": 0.9617366658077815,
        "step": 7465
    },
    {
        "loss": 2.4934,
        "grad_norm": 1.817672610282898,
        "learning_rate": 3.2716248950325434e-06,
        "epoch": 0.9618654985828394,
        "step": 7466
    },
    {
        "loss": 2.0738,
        "grad_norm": 2.274223566055298,
        "learning_rate": 3.2500175363939676e-06,
        "epoch": 0.9619943313578975,
        "step": 7467
    },
    {
        "loss": 2.0355,
        "grad_norm": 2.704970359802246,
        "learning_rate": 3.2284793698601544e-06,
        "epoch": 0.9621231641329554,
        "step": 7468
    },
    {
        "loss": 0.7962,
        "grad_norm": 3.1116135120391846,
        "learning_rate": 3.207010427308571e-06,
        "epoch": 0.9622519969080134,
        "step": 7469
    },
    {
        "loss": 1.5559,
        "grad_norm": 1.3991435766220093,
        "learning_rate": 3.1856107405142223e-06,
        "epoch": 0.9623808296830714,
        "step": 7470
    },
    {
        "loss": 2.1461,
        "grad_norm": 2.350964069366455,
        "learning_rate": 3.1642803411496225e-06,
        "epoch": 0.9625096624581293,
        "step": 7471
    },
    {
        "loss": 0.9004,
        "grad_norm": 3.8270270824432373,
        "learning_rate": 3.1430192607847407e-06,
        "epoch": 0.9626384952331873,
        "step": 7472
    },
    {
        "loss": 2.1511,
        "grad_norm": 3.0889506340026855,
        "learning_rate": 3.1218275308869448e-06,
        "epoch": 0.9627673280082453,
        "step": 7473
    },
    {
        "loss": 1.9432,
        "grad_norm": 1.3816605806350708,
        "learning_rate": 3.100705182820979e-06,
        "epoch": 0.9628961607833033,
        "step": 7474
    },
    {
        "loss": 2.1356,
        "grad_norm": 1.6710121631622314,
        "learning_rate": 3.07965224784883e-06,
        "epoch": 0.9630249935583612,
        "step": 7475
    },
    {
        "loss": 2.3388,
        "grad_norm": 2.0185647010803223,
        "learning_rate": 3.0586687571298456e-06,
        "epoch": 0.9631538263334192,
        "step": 7476
    },
    {
        "loss": 1.9404,
        "grad_norm": 2.18524432182312,
        "learning_rate": 3.0377547417205442e-06,
        "epoch": 0.9632826591084772,
        "step": 7477
    },
    {
        "loss": 1.8427,
        "grad_norm": 4.156127452850342,
        "learning_rate": 3.0169102325745934e-06,
        "epoch": 0.9634114918835351,
        "step": 7478
    },
    {
        "loss": 2.0649,
        "grad_norm": 2.5413053035736084,
        "learning_rate": 2.9961352605428096e-06,
        "epoch": 0.9635403246585932,
        "step": 7479
    },
    {
        "loss": 1.8113,
        "grad_norm": 2.8706915378570557,
        "learning_rate": 2.9754298563731686e-06,
        "epoch": 0.9636691574336511,
        "step": 7480
    },
    {
        "loss": 1.8633,
        "grad_norm": 2.4964332580566406,
        "learning_rate": 2.9547940507105466e-06,
        "epoch": 0.963797990208709,
        "step": 7481
    },
    {
        "loss": 2.3767,
        "grad_norm": 1.0706171989440918,
        "learning_rate": 2.93422787409689e-06,
        "epoch": 0.9639268229837671,
        "step": 7482
    },
    {
        "loss": 1.1792,
        "grad_norm": 2.6621994972229004,
        "learning_rate": 2.913731356971089e-06,
        "epoch": 0.964055655758825,
        "step": 7483
    },
    {
        "loss": 0.6666,
        "grad_norm": 3.4283084869384766,
        "learning_rate": 2.893304529668922e-06,
        "epoch": 0.964184488533883,
        "step": 7484
    },
    {
        "loss": 2.1322,
        "grad_norm": 1.9342700242996216,
        "learning_rate": 2.872947422423039e-06,
        "epoch": 0.964313321308941,
        "step": 7485
    },
    {
        "loss": 1.643,
        "grad_norm": 1.7256468534469604,
        "learning_rate": 2.8526600653628445e-06,
        "epoch": 0.9644421540839989,
        "step": 7486
    },
    {
        "loss": 1.1757,
        "grad_norm": 2.387181520462036,
        "learning_rate": 2.8324424885145528e-06,
        "epoch": 0.964570986859057,
        "step": 7487
    },
    {
        "loss": 1.5192,
        "grad_norm": 3.506131649017334,
        "learning_rate": 2.812294721801173e-06,
        "epoch": 0.9646998196341149,
        "step": 7488
    },
    {
        "loss": 2.095,
        "grad_norm": 2.430006504058838,
        "learning_rate": 2.792216795042235e-06,
        "epoch": 0.9648286524091729,
        "step": 7489
    },
    {
        "loss": 1.8981,
        "grad_norm": 2.3115851879119873,
        "learning_rate": 2.772208737954024e-06,
        "epoch": 0.9649574851842309,
        "step": 7490
    },
    {
        "loss": 1.9368,
        "grad_norm": 2.0291128158569336,
        "learning_rate": 2.7522705801493966e-06,
        "epoch": 0.9650863179592889,
        "step": 7491
    },
    {
        "loss": 1.3485,
        "grad_norm": 3.3905158042907715,
        "learning_rate": 2.7324023511376806e-06,
        "epoch": 0.9652151507343468,
        "step": 7492
    },
    {
        "loss": 2.1828,
        "grad_norm": 1.8182958364486694,
        "learning_rate": 2.7126040803247876e-06,
        "epoch": 0.9653439835094048,
        "step": 7493
    },
    {
        "loss": 2.3043,
        "grad_norm": 2.5087132453918457,
        "learning_rate": 2.6928757970130615e-06,
        "epoch": 0.9654728162844628,
        "step": 7494
    },
    {
        "loss": 1.8545,
        "grad_norm": 1.7191129922866821,
        "learning_rate": 2.673217530401262e-06,
        "epoch": 0.9656016490595207,
        "step": 7495
    },
    {
        "loss": 0.9641,
        "grad_norm": 4.019448757171631,
        "learning_rate": 2.6536293095845156e-06,
        "epoch": 0.9657304818345788,
        "step": 7496
    },
    {
        "loss": 1.7361,
        "grad_norm": 3.176823139190674,
        "learning_rate": 2.6341111635542813e-06,
        "epoch": 0.9658593146096367,
        "step": 7497
    },
    {
        "loss": 1.5002,
        "grad_norm": 2.081251859664917,
        "learning_rate": 2.614663121198313e-06,
        "epoch": 0.9659881473846946,
        "step": 7498
    },
    {
        "loss": 0.5567,
        "grad_norm": 3.0432140827178955,
        "learning_rate": 2.595285211300613e-06,
        "epoch": 0.9661169801597527,
        "step": 7499
    },
    {
        "loss": 2.0157,
        "grad_norm": 1.9926888942718506,
        "learning_rate": 2.5759774625413347e-06,
        "epoch": 0.9662458129348106,
        "step": 7500
    },
    {
        "loss": 1.8595,
        "grad_norm": 1.322135329246521,
        "learning_rate": 2.5567399034968408e-06,
        "epoch": 0.9663746457098686,
        "step": 7501
    },
    {
        "loss": 2.0214,
        "grad_norm": 2.2054224014282227,
        "learning_rate": 2.537572562639623e-06,
        "epoch": 0.9665034784849266,
        "step": 7502
    },
    {
        "loss": 1.4207,
        "grad_norm": 3.0564515590667725,
        "learning_rate": 2.5184754683381716e-06,
        "epoch": 0.9666323112599845,
        "step": 7503
    },
    {
        "loss": 1.2034,
        "grad_norm": 2.740044355392456,
        "learning_rate": 2.499448648857061e-06,
        "epoch": 0.9667611440350425,
        "step": 7504
    },
    {
        "loss": 1.3501,
        "grad_norm": 2.5887351036071777,
        "learning_rate": 2.480492132356915e-06,
        "epoch": 0.9668899768101005,
        "step": 7505
    },
    {
        "loss": 2.038,
        "grad_norm": 1.9701775312423706,
        "learning_rate": 2.461605946894174e-06,
        "epoch": 0.9670188095851585,
        "step": 7506
    },
    {
        "loss": 1.3769,
        "grad_norm": 2.6966333389282227,
        "learning_rate": 2.4427901204212734e-06,
        "epoch": 0.9671476423602164,
        "step": 7507
    },
    {
        "loss": 1.3848,
        "grad_norm": 2.122661828994751,
        "learning_rate": 2.4240446807865026e-06,
        "epoch": 0.9672764751352744,
        "step": 7508
    },
    {
        "loss": 1.0398,
        "grad_norm": 2.7119271755218506,
        "learning_rate": 2.405369655733958e-06,
        "epoch": 0.9674053079103324,
        "step": 7509
    },
    {
        "loss": 2.0913,
        "grad_norm": 1.4699598550796509,
        "learning_rate": 2.3867650729035573e-06,
        "epoch": 0.9675341406853903,
        "step": 7510
    },
    {
        "loss": 1.6064,
        "grad_norm": 3.196134567260742,
        "learning_rate": 2.3682309598308807e-06,
        "epoch": 0.9676629734604484,
        "step": 7511
    },
    {
        "loss": 2.4096,
        "grad_norm": 1.7317864894866943,
        "learning_rate": 2.3497673439472845e-06,
        "epoch": 0.9677918062355063,
        "step": 7512
    },
    {
        "loss": 2.2302,
        "grad_norm": 1.8777986764907837,
        "learning_rate": 2.3313742525797598e-06,
        "epoch": 0.9679206390105642,
        "step": 7513
    },
    {
        "loss": 1.9485,
        "grad_norm": 3.8375418186187744,
        "learning_rate": 2.3130517129509254e-06,
        "epoch": 0.9680494717856223,
        "step": 7514
    },
    {
        "loss": 1.628,
        "grad_norm": 3.2524147033691406,
        "learning_rate": 2.2947997521789666e-06,
        "epoch": 0.9681783045606802,
        "step": 7515
    },
    {
        "loss": 1.2425,
        "grad_norm": 1.9667770862579346,
        "learning_rate": 2.276618397277647e-06,
        "epoch": 0.9683071373357383,
        "step": 7516
    },
    {
        "loss": 1.1903,
        "grad_norm": 3.2476415634155273,
        "learning_rate": 2.258507675156152e-06,
        "epoch": 0.9684359701107962,
        "step": 7517
    },
    {
        "loss": 2.205,
        "grad_norm": 1.7510030269622803,
        "learning_rate": 2.2404676126192016e-06,
        "epoch": 0.9685648028858541,
        "step": 7518
    },
    {
        "loss": 2.1077,
        "grad_norm": 2.965277910232544,
        "learning_rate": 2.22249823636691e-06,
        "epoch": 0.9686936356609122,
        "step": 7519
    },
    {
        "loss": 1.2868,
        "grad_norm": 3.2089641094207764,
        "learning_rate": 2.20459957299477e-06,
        "epoch": 0.9688224684359701,
        "step": 7520
    },
    {
        "loss": 1.9432,
        "grad_norm": 2.8984479904174805,
        "learning_rate": 2.1867716489936353e-06,
        "epoch": 0.9689513012110281,
        "step": 7521
    },
    {
        "loss": 1.8189,
        "grad_norm": 2.80440092086792,
        "learning_rate": 2.169014490749599e-06,
        "epoch": 0.9690801339860861,
        "step": 7522
    },
    {
        "loss": 1.1201,
        "grad_norm": 3.659675121307373,
        "learning_rate": 2.1513281245441107e-06,
        "epoch": 0.969208966761144,
        "step": 7523
    },
    {
        "loss": 2.2295,
        "grad_norm": 1.8922146558761597,
        "learning_rate": 2.133712576553809e-06,
        "epoch": 0.969337799536202,
        "step": 7524
    },
    {
        "loss": 1.5764,
        "grad_norm": 2.101846218109131,
        "learning_rate": 2.116167872850461e-06,
        "epoch": 0.96946663231126,
        "step": 7525
    },
    {
        "loss": 1.6827,
        "grad_norm": 2.8805346488952637,
        "learning_rate": 2.098694039401061e-06,
        "epoch": 0.969595465086318,
        "step": 7526
    },
    {
        "loss": 2.1271,
        "grad_norm": 1.4827735424041748,
        "learning_rate": 2.0812911020676784e-06,
        "epoch": 0.9697242978613759,
        "step": 7527
    },
    {
        "loss": 2.0208,
        "grad_norm": 2.121429204940796,
        "learning_rate": 2.0639590866074256e-06,
        "epoch": 0.9698531306364339,
        "step": 7528
    },
    {
        "loss": 1.9386,
        "grad_norm": 2.2844581604003906,
        "learning_rate": 2.0466980186724883e-06,
        "epoch": 0.9699819634114919,
        "step": 7529
    },
    {
        "loss": 1.3458,
        "grad_norm": 2.7466936111450195,
        "learning_rate": 2.0295079238100424e-06,
        "epoch": 0.9701107961865498,
        "step": 7530
    },
    {
        "loss": 1.4537,
        "grad_norm": 4.0881757736206055,
        "learning_rate": 2.0123888274622083e-06,
        "epoch": 0.9702396289616079,
        "step": 7531
    },
    {
        "loss": 1.7163,
        "grad_norm": 2.384101390838623,
        "learning_rate": 1.9953407549660296e-06,
        "epoch": 0.9703684617366658,
        "step": 7532
    },
    {
        "loss": 1.8069,
        "grad_norm": 2.7369003295898438,
        "learning_rate": 1.9783637315534287e-06,
        "epoch": 0.9704972945117237,
        "step": 7533
    },
    {
        "loss": 1.8552,
        "grad_norm": 2.8210554122924805,
        "learning_rate": 1.961457782351173e-06,
        "epoch": 0.9706261272867818,
        "step": 7534
    },
    {
        "loss": 1.9823,
        "grad_norm": 2.1141576766967773,
        "learning_rate": 1.9446229323808575e-06,
        "epoch": 0.9707549600618397,
        "step": 7535
    },
    {
        "loss": 1.4069,
        "grad_norm": 2.6856508255004883,
        "learning_rate": 1.927859206558791e-06,
        "epoch": 0.9708837928368977,
        "step": 7536
    },
    {
        "loss": 2.134,
        "grad_norm": 2.613858699798584,
        "learning_rate": 1.911166629696076e-06,
        "epoch": 0.9710126256119557,
        "step": 7537
    },
    {
        "loss": 2.3719,
        "grad_norm": 1.540083408355713,
        "learning_rate": 1.8945452264984954e-06,
        "epoch": 0.9711414583870136,
        "step": 7538
    },
    {
        "loss": 2.268,
        "grad_norm": 1.9121941328048706,
        "learning_rate": 1.877995021566431e-06,
        "epoch": 0.9712702911620716,
        "step": 7539
    },
    {
        "loss": 1.9345,
        "grad_norm": 2.4867961406707764,
        "learning_rate": 1.8615160393950004e-06,
        "epoch": 0.9713991239371296,
        "step": 7540
    },
    {
        "loss": 2.2998,
        "grad_norm": 2.0399580001831055,
        "learning_rate": 1.8451083043738494e-06,
        "epoch": 0.9715279567121876,
        "step": 7541
    },
    {
        "loss": 1.3128,
        "grad_norm": 4.478989601135254,
        "learning_rate": 1.8287718407871302e-06,
        "epoch": 0.9716567894872455,
        "step": 7542
    },
    {
        "loss": 2.064,
        "grad_norm": 2.2673685550689697,
        "learning_rate": 1.8125066728135742e-06,
        "epoch": 0.9717856222623036,
        "step": 7543
    },
    {
        "loss": 1.8873,
        "grad_norm": 2.173102378845215,
        "learning_rate": 1.7963128245263694e-06,
        "epoch": 0.9719144550373615,
        "step": 7544
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.8157240152359009,
        "learning_rate": 1.780190319893149e-06,
        "epoch": 0.9720432878124194,
        "step": 7545
    },
    {
        "loss": 2.1867,
        "grad_norm": 1.7094157934188843,
        "learning_rate": 1.7641391827759756e-06,
        "epoch": 0.9721721205874775,
        "step": 7546
    },
    {
        "loss": 1.8351,
        "grad_norm": 3.0399887561798096,
        "learning_rate": 1.7481594369312126e-06,
        "epoch": 0.9723009533625354,
        "step": 7547
    },
    {
        "loss": 1.7269,
        "grad_norm": 1.959022045135498,
        "learning_rate": 1.7322511060096636e-06,
        "epoch": 0.9724297861375935,
        "step": 7548
    },
    {
        "loss": 2.1827,
        "grad_norm": 3.714195966720581,
        "learning_rate": 1.7164142135563776e-06,
        "epoch": 0.9725586189126514,
        "step": 7549
    },
    {
        "loss": 2.3248,
        "grad_norm": 2.1873552799224854,
        "learning_rate": 1.7006487830106499e-06,
        "epoch": 0.9726874516877093,
        "step": 7550
    },
    {
        "loss": 2.3326,
        "grad_norm": 2.449293613433838,
        "learning_rate": 1.6849548377060487e-06,
        "epoch": 0.9728162844627674,
        "step": 7551
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.5162819623947144,
        "learning_rate": 1.6693324008703438e-06,
        "epoch": 0.9729451172378253,
        "step": 7552
    },
    {
        "loss": 2.0709,
        "grad_norm": 2.3058278560638428,
        "learning_rate": 1.6537814956254282e-06,
        "epoch": 0.9730739500128833,
        "step": 7553
    },
    {
        "loss": 1.4597,
        "grad_norm": 1.542252540588379,
        "learning_rate": 1.6383021449873581e-06,
        "epoch": 0.9732027827879413,
        "step": 7554
    },
    {
        "loss": 1.2251,
        "grad_norm": 1.67634117603302,
        "learning_rate": 1.6228943718662847e-06,
        "epoch": 0.9733316155629992,
        "step": 7555
    },
    {
        "loss": 2.3492,
        "grad_norm": 2.3038644790649414,
        "learning_rate": 1.6075581990664169e-06,
        "epoch": 0.9734604483380572,
        "step": 7556
    },
    {
        "loss": 2.0781,
        "grad_norm": 2.6400513648986816,
        "learning_rate": 1.5922936492859752e-06,
        "epoch": 0.9735892811131152,
        "step": 7557
    },
    {
        "loss": 0.5398,
        "grad_norm": 1.9251172542572021,
        "learning_rate": 1.5771007451172048e-06,
        "epoch": 0.9737181138881732,
        "step": 7558
    },
    {
        "loss": 0.8546,
        "grad_norm": 2.8583552837371826,
        "learning_rate": 1.5619795090462852e-06,
        "epoch": 0.9738469466632311,
        "step": 7559
    },
    {
        "loss": 2.3848,
        "grad_norm": 1.7366364002227783,
        "learning_rate": 1.5469299634533641e-06,
        "epoch": 0.9739757794382891,
        "step": 7560
    },
    {
        "loss": 2.0611,
        "grad_norm": 2.3701565265655518,
        "learning_rate": 1.5319521306124075e-06,
        "epoch": 0.9741046122133471,
        "step": 7561
    },
    {
        "loss": 1.9223,
        "grad_norm": 2.3434345722198486,
        "learning_rate": 1.5170460326913105e-06,
        "epoch": 0.974233444988405,
        "step": 7562
    },
    {
        "loss": 2.0209,
        "grad_norm": 2.034627676010132,
        "learning_rate": 1.5022116917518037e-06,
        "epoch": 0.9743622777634631,
        "step": 7563
    },
    {
        "loss": 1.7063,
        "grad_norm": 2.508699655532837,
        "learning_rate": 1.4874491297493243e-06,
        "epoch": 0.974491110538521,
        "step": 7564
    },
    {
        "loss": 2.0785,
        "grad_norm": 2.1921441555023193,
        "learning_rate": 1.4727583685332003e-06,
        "epoch": 0.9746199433135789,
        "step": 7565
    },
    {
        "loss": 1.7393,
        "grad_norm": 3.1591413021087646,
        "learning_rate": 1.4581394298464113e-06,
        "epoch": 0.974748776088637,
        "step": 7566
    },
    {
        "loss": 1.0718,
        "grad_norm": 2.7665696144104004,
        "learning_rate": 1.4435923353256387e-06,
        "epoch": 0.9748776088636949,
        "step": 7567
    },
    {
        "loss": 1.3105,
        "grad_norm": 2.518165349960327,
        "learning_rate": 1.4291171065012487e-06,
        "epoch": 0.9750064416387529,
        "step": 7568
    },
    {
        "loss": 2.2043,
        "grad_norm": 1.9064373970031738,
        "learning_rate": 1.414713764797243e-06,
        "epoch": 0.9751352744138109,
        "step": 7569
    },
    {
        "loss": 2.009,
        "grad_norm": 3.132444381713867,
        "learning_rate": 1.4003823315312247e-06,
        "epoch": 0.9752641071888688,
        "step": 7570
    },
    {
        "loss": 2.4418,
        "grad_norm": 2.335476875305176,
        "learning_rate": 1.3861228279143878e-06,
        "epoch": 0.9753929399639268,
        "step": 7571
    },
    {
        "loss": 0.8673,
        "grad_norm": 2.5328047275543213,
        "learning_rate": 1.371935275051406e-06,
        "epoch": 0.9755217727389848,
        "step": 7572
    },
    {
        "loss": 2.2825,
        "grad_norm": 1.7958937883377075,
        "learning_rate": 1.3578196939405152e-06,
        "epoch": 0.9756506055140428,
        "step": 7573
    },
    {
        "loss": 1.6208,
        "grad_norm": 2.2185466289520264,
        "learning_rate": 1.3437761054734599e-06,
        "epoch": 0.9757794382891007,
        "step": 7574
    },
    {
        "loss": 1.2401,
        "grad_norm": 2.2406344413757324,
        "learning_rate": 1.3298045304353469e-06,
        "epoch": 0.9759082710641587,
        "step": 7575
    },
    {
        "loss": 1.9604,
        "grad_norm": 2.7251667976379395,
        "learning_rate": 1.315904989504757e-06,
        "epoch": 0.9760371038392167,
        "step": 7576
    },
    {
        "loss": 1.4234,
        "grad_norm": 3.337057113647461,
        "learning_rate": 1.302077503253657e-06,
        "epoch": 0.9761659366142746,
        "step": 7577
    },
    {
        "loss": 2.1858,
        "grad_norm": 1.5750128030776978,
        "learning_rate": 1.2883220921473315e-06,
        "epoch": 0.9762947693893327,
        "step": 7578
    },
    {
        "loss": 2.2959,
        "grad_norm": 1.756038784980774,
        "learning_rate": 1.274638776544429e-06,
        "epoch": 0.9764236021643906,
        "step": 7579
    },
    {
        "loss": 1.7612,
        "grad_norm": 2.914517641067505,
        "learning_rate": 1.2610275766968828e-06,
        "epoch": 0.9765524349394485,
        "step": 7580
    },
    {
        "loss": 2.4739,
        "grad_norm": 1.645153284072876,
        "learning_rate": 1.2474885127498837e-06,
        "epoch": 0.9766812677145066,
        "step": 7581
    },
    {
        "loss": 2.7134,
        "grad_norm": 2.00935697555542,
        "learning_rate": 1.234021604741864e-06,
        "epoch": 0.9768101004895645,
        "step": 7582
    },
    {
        "loss": 2.1881,
        "grad_norm": 2.071428060531616,
        "learning_rate": 1.220626872604469e-06,
        "epoch": 0.9769389332646226,
        "step": 7583
    },
    {
        "loss": 0.9005,
        "grad_norm": 2.5006258487701416,
        "learning_rate": 1.207304336162507e-06,
        "epoch": 0.9770677660396805,
        "step": 7584
    },
    {
        "loss": 1.8811,
        "grad_norm": 2.665259599685669,
        "learning_rate": 1.1940540151339497e-06,
        "epoch": 0.9771965988147384,
        "step": 7585
    },
    {
        "loss": 1.6788,
        "grad_norm": 3.2694449424743652,
        "learning_rate": 1.1808759291298544e-06,
        "epoch": 0.9773254315897965,
        "step": 7586
    },
    {
        "loss": 1.7859,
        "grad_norm": 3.1189393997192383,
        "learning_rate": 1.1677700976544025e-06,
        "epoch": 0.9774542643648544,
        "step": 7587
    },
    {
        "loss": 1.7214,
        "grad_norm": 2.8758533000946045,
        "learning_rate": 1.1547365401048283e-06,
        "epoch": 0.9775830971399124,
        "step": 7588
    },
    {
        "loss": 2.428,
        "grad_norm": 2.0715126991271973,
        "learning_rate": 1.1417752757713728e-06,
        "epoch": 0.9777119299149704,
        "step": 7589
    },
    {
        "loss": 1.2751,
        "grad_norm": 3.4852817058563232,
        "learning_rate": 1.128886323837297e-06,
        "epoch": 0.9778407626900284,
        "step": 7590
    },
    {
        "loss": 1.558,
        "grad_norm": 1.9416260719299316,
        "learning_rate": 1.1160697033788748e-06,
        "epoch": 0.9779695954650863,
        "step": 7591
    },
    {
        "loss": 1.7757,
        "grad_norm": 2.785661220550537,
        "learning_rate": 1.1033254333652488e-06,
        "epoch": 0.9780984282401443,
        "step": 7592
    },
    {
        "loss": 1.8714,
        "grad_norm": 2.370670795440674,
        "learning_rate": 1.090653532658531e-06,
        "epoch": 0.9782272610152023,
        "step": 7593
    },
    {
        "loss": 1.5935,
        "grad_norm": 1.8573557138442993,
        "learning_rate": 1.078054020013708e-06,
        "epoch": 0.9783560937902602,
        "step": 7594
    },
    {
        "loss": 1.461,
        "grad_norm": 2.161336898803711,
        "learning_rate": 1.0655269140786462e-06,
        "epoch": 0.9784849265653183,
        "step": 7595
    },
    {
        "loss": 1.6917,
        "grad_norm": 3.005356550216675,
        "learning_rate": 1.0530722333940314e-06,
        "epoch": 0.9786137593403762,
        "step": 7596
    },
    {
        "loss": 2.2864,
        "grad_norm": 2.0991740226745605,
        "learning_rate": 1.0406899963933458e-06,
        "epoch": 0.9787425921154341,
        "step": 7597
    },
    {
        "loss": 1.2352,
        "grad_norm": 6.1200785636901855,
        "learning_rate": 1.0283802214028693e-06,
        "epoch": 0.9788714248904922,
        "step": 7598
    },
    {
        "loss": 1.8275,
        "grad_norm": 2.095818281173706,
        "learning_rate": 1.0161429266416334e-06,
        "epoch": 0.9790002576655501,
        "step": 7599
    },
    {
        "loss": 2.0697,
        "grad_norm": 2.719299077987671,
        "learning_rate": 1.0039781302213836e-06,
        "epoch": 0.9791290904406081,
        "step": 7600
    },
    {
        "loss": 1.1879,
        "grad_norm": 3.1436140537261963,
        "learning_rate": 9.9188585014659e-07,
        "epoch": 0.9792579232156661,
        "step": 7601
    },
    {
        "loss": 2.1004,
        "grad_norm": 1.4146519899368286,
        "learning_rate": 9.798661043143752e-07,
        "epoch": 0.979386755990724,
        "step": 7602
    },
    {
        "loss": 2.2572,
        "grad_norm": 1.7973496913909912,
        "learning_rate": 9.679189105144925e-07,
        "epoch": 0.979515588765782,
        "step": 7603
    },
    {
        "loss": 2.1054,
        "grad_norm": 2.124799966812134,
        "learning_rate": 9.560442864293472e-07,
        "epoch": 0.97964442154084,
        "step": 7604
    },
    {
        "loss": 2.3596,
        "grad_norm": 1.72574782371521,
        "learning_rate": 9.442422496339198e-07,
        "epoch": 0.979773254315898,
        "step": 7605
    },
    {
        "loss": 2.2013,
        "grad_norm": 2.998014211654663,
        "learning_rate": 9.325128175957599e-07,
        "epoch": 0.9799020870909559,
        "step": 7606
    },
    {
        "loss": 1.7138,
        "grad_norm": 2.703094005584717,
        "learning_rate": 9.208560076749751e-07,
        "epoch": 0.9800309198660139,
        "step": 7607
    },
    {
        "loss": 1.854,
        "grad_norm": 2.854586124420166,
        "learning_rate": 9.092718371241593e-07,
        "epoch": 0.9801597526410719,
        "step": 7608
    },
    {
        "loss": 1.3046,
        "grad_norm": 2.748732328414917,
        "learning_rate": 8.977603230884258e-07,
        "epoch": 0.9802885854161298,
        "step": 7609
    },
    {
        "loss": 1.9258,
        "grad_norm": 1.753638744354248,
        "learning_rate": 8.863214826053456e-07,
        "epoch": 0.9804174181911879,
        "step": 7610
    },
    {
        "loss": 2.2794,
        "grad_norm": 1.8594611883163452,
        "learning_rate": 8.749553326048987e-07,
        "epoch": 0.9805462509662458,
        "step": 7611
    },
    {
        "loss": 1.809,
        "grad_norm": 2.9484267234802246,
        "learning_rate": 8.636618899095172e-07,
        "epoch": 0.9806750837413037,
        "step": 7612
    },
    {
        "loss": 1.7224,
        "grad_norm": 2.521031618118286,
        "learning_rate": 8.524411712340308e-07,
        "epoch": 0.9808039165163618,
        "step": 7613
    },
    {
        "loss": 2.3964,
        "grad_norm": 1.9840399026870728,
        "learning_rate": 8.412931931855772e-07,
        "epoch": 0.9809327492914197,
        "step": 7614
    },
    {
        "loss": 1.716,
        "grad_norm": 2.4818668365478516,
        "learning_rate": 8.302179722636971e-07,
        "epoch": 0.9810615820664778,
        "step": 7615
    },
    {
        "loss": 1.4165,
        "grad_norm": 2.34497332572937,
        "learning_rate": 8.192155248602118e-07,
        "epoch": 0.9811904148415357,
        "step": 7616
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.693802833557129,
        "learning_rate": 8.082858672592508e-07,
        "epoch": 0.9813192476165936,
        "step": 7617
    },
    {
        "loss": 1.5675,
        "grad_norm": 3.1114914417266846,
        "learning_rate": 7.974290156372077e-07,
        "epoch": 0.9814480803916517,
        "step": 7618
    },
    {
        "loss": 1.9109,
        "grad_norm": 2.3739206790924072,
        "learning_rate": 7.866449860627124e-07,
        "epoch": 0.9815769131667096,
        "step": 7619
    },
    {
        "loss": 2.1204,
        "grad_norm": 1.6153637170791626,
        "learning_rate": 7.759337944966194e-07,
        "epoch": 0.9817057459417676,
        "step": 7620
    },
    {
        "loss": 1.6965,
        "grad_norm": 1.298244595527649,
        "learning_rate": 7.652954567919979e-07,
        "epoch": 0.9818345787168256,
        "step": 7621
    },
    {
        "loss": 2.2936,
        "grad_norm": 2.8179690837860107,
        "learning_rate": 7.547299886940474e-07,
        "epoch": 0.9819634114918835,
        "step": 7622
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.027855396270752,
        "learning_rate": 7.442374058401535e-07,
        "epoch": 0.9820922442669415,
        "step": 7623
    },
    {
        "loss": 2.1681,
        "grad_norm": 2.3528358936309814,
        "learning_rate": 7.33817723759811e-07,
        "epoch": 0.9822210770419995,
        "step": 7624
    },
    {
        "loss": 2.4124,
        "grad_norm": 2.0018322467803955,
        "learning_rate": 7.234709578746391e-07,
        "epoch": 0.9823499098170575,
        "step": 7625
    },
    {
        "loss": 1.767,
        "grad_norm": 1.9222283363342285,
        "learning_rate": 7.131971234983048e-07,
        "epoch": 0.9824787425921154,
        "step": 7626
    },
    {
        "loss": 1.5176,
        "grad_norm": 2.5061964988708496,
        "learning_rate": 7.029962358365727e-07,
        "epoch": 0.9826075753671734,
        "step": 7627
    },
    {
        "loss": 1.4716,
        "grad_norm": 2.6626806259155273,
        "learning_rate": 6.928683099871935e-07,
        "epoch": 0.9827364081422314,
        "step": 7628
    },
    {
        "loss": 1.7116,
        "grad_norm": 2.810960292816162,
        "learning_rate": 6.828133609399767e-07,
        "epoch": 0.9828652409172893,
        "step": 7629
    },
    {
        "loss": 1.9765,
        "grad_norm": 1.9164811372756958,
        "learning_rate": 6.72831403576707e-07,
        "epoch": 0.9829940736923474,
        "step": 7630
    },
    {
        "loss": 1.1921,
        "grad_norm": 2.434420347213745,
        "learning_rate": 6.629224526711276e-07,
        "epoch": 0.9831229064674053,
        "step": 7631
    },
    {
        "loss": 1.6126,
        "grad_norm": 3.260910749435425,
        "learning_rate": 6.530865228889515e-07,
        "epoch": 0.9832517392424632,
        "step": 7632
    },
    {
        "loss": 1.6824,
        "grad_norm": 3.286127805709839,
        "learning_rate": 6.43323628787773e-07,
        "epoch": 0.9833805720175213,
        "step": 7633
    },
    {
        "loss": 1.9627,
        "grad_norm": 1.6175169944763184,
        "learning_rate": 6.336337848171503e-07,
        "epoch": 0.9835094047925792,
        "step": 7634
    },
    {
        "loss": 1.746,
        "grad_norm": 3.133422374725342,
        "learning_rate": 6.240170053184946e-07,
        "epoch": 0.9836382375676372,
        "step": 7635
    },
    {
        "loss": 1.9188,
        "grad_norm": 1.2908754348754883,
        "learning_rate": 6.144733045250595e-07,
        "epoch": 0.9837670703426952,
        "step": 7636
    },
    {
        "loss": 2.238,
        "grad_norm": 1.5787550210952759,
        "learning_rate": 6.050026965619626e-07,
        "epoch": 0.9838959031177531,
        "step": 7637
    },
    {
        "loss": 1.4904,
        "grad_norm": 2.450422763824463,
        "learning_rate": 5.956051954461472e-07,
        "epoch": 0.9840247358928111,
        "step": 7638
    },
    {
        "loss": 2.1448,
        "grad_norm": 2.1171109676361084,
        "learning_rate": 5.862808150863264e-07,
        "epoch": 0.9841535686678691,
        "step": 7639
    },
    {
        "loss": 1.3495,
        "grad_norm": 2.7878410816192627,
        "learning_rate": 5.770295692830163e-07,
        "epoch": 0.9842824014429271,
        "step": 7640
    },
    {
        "loss": 1.5807,
        "grad_norm": 3.6429502964019775,
        "learning_rate": 5.678514717284811e-07,
        "epoch": 0.984411234217985,
        "step": 7641
    },
    {
        "loss": 2.393,
        "grad_norm": 1.7943816184997559,
        "learning_rate": 5.587465360067268e-07,
        "epoch": 0.9845400669930431,
        "step": 7642
    },
    {
        "loss": 1.5892,
        "grad_norm": 3.486386299133301,
        "learning_rate": 5.497147755934629e-07,
        "epoch": 0.984668899768101,
        "step": 7643
    },
    {
        "loss": 2.0633,
        "grad_norm": 2.7588701248168945,
        "learning_rate": 5.407562038561187e-07,
        "epoch": 0.9847977325431589,
        "step": 7644
    },
    {
        "loss": 2.3427,
        "grad_norm": 1.9365289211273193,
        "learning_rate": 5.318708340537826e-07,
        "epoch": 0.984926565318217,
        "step": 7645
    },
    {
        "loss": 2.4074,
        "grad_norm": 1.85672128200531,
        "learning_rate": 5.230586793372183e-07,
        "epoch": 0.9850553980932749,
        "step": 7646
    },
    {
        "loss": 0.923,
        "grad_norm": 3.92045259475708,
        "learning_rate": 5.143197527487986e-07,
        "epoch": 0.985184230868333,
        "step": 7647
    },
    {
        "loss": 2.2734,
        "grad_norm": 2.5211470127105713,
        "learning_rate": 5.056540672225385e-07,
        "epoch": 0.9853130636433909,
        "step": 7648
    },
    {
        "loss": 2.1287,
        "grad_norm": 2.0847151279449463,
        "learning_rate": 4.970616355840619e-07,
        "epoch": 0.9854418964184488,
        "step": 7649
    },
    {
        "loss": 2.2058,
        "grad_norm": 1.729801058769226,
        "learning_rate": 4.885424705505237e-07,
        "epoch": 0.9855707291935069,
        "step": 7650
    },
    {
        "loss": 1.3869,
        "grad_norm": 3.039402723312378,
        "learning_rate": 4.800965847307159e-07,
        "epoch": 0.9856995619685648,
        "step": 7651
    },
    {
        "loss": 1.7934,
        "grad_norm": 2.3587052822113037,
        "learning_rate": 4.7172399062492266e-07,
        "epoch": 0.9858283947436228,
        "step": 7652
    },
    {
        "loss": 1.8002,
        "grad_norm": 1.7681325674057007,
        "learning_rate": 4.6342470062495367e-07,
        "epoch": 0.9859572275186808,
        "step": 7653
    },
    {
        "loss": 1.6909,
        "grad_norm": 2.6662771701812744,
        "learning_rate": 4.5519872701414443e-07,
        "epoch": 0.9860860602937387,
        "step": 7654
    },
    {
        "loss": 2.2799,
        "grad_norm": 1.587520718574524,
        "learning_rate": 4.4704608196731167e-07,
        "epoch": 0.9862148930687967,
        "step": 7655
    },
    {
        "loss": 2.3809,
        "grad_norm": 2.5327439308166504,
        "learning_rate": 4.3896677755073667e-07,
        "epoch": 0.9863437258438547,
        "step": 7656
    },
    {
        "loss": 2.142,
        "grad_norm": 3.0394229888916016,
        "learning_rate": 4.309608257221709e-07,
        "epoch": 0.9864725586189127,
        "step": 7657
    },
    {
        "loss": 1.078,
        "grad_norm": 2.778059959411621,
        "learning_rate": 4.230282383307582e-07,
        "epoch": 0.9866013913939706,
        "step": 7658
    },
    {
        "loss": 2.4636,
        "grad_norm": 2.3720030784606934,
        "learning_rate": 4.151690271171238e-07,
        "epoch": 0.9867302241690286,
        "step": 7659
    },
    {
        "loss": 2.184,
        "grad_norm": 2.2948899269104004,
        "learning_rate": 4.073832037132519e-07,
        "epoch": 0.9868590569440866,
        "step": 7660
    },
    {
        "loss": 2.1194,
        "grad_norm": 1.737342119216919,
        "learning_rate": 3.99670779642497e-07,
        "epoch": 0.9869878897191445,
        "step": 7661
    },
    {
        "loss": 0.7285,
        "grad_norm": 2.784881830215454,
        "learning_rate": 3.920317663196116e-07,
        "epoch": 0.9871167224942026,
        "step": 7662
    },
    {
        "loss": 2.1306,
        "grad_norm": 2.397627115249634,
        "learning_rate": 3.8446617505068507e-07,
        "epoch": 0.9872455552692605,
        "step": 7663
    },
    {
        "loss": 1.8354,
        "grad_norm": 1.6811861991882324,
        "learning_rate": 3.7697401703313263e-07,
        "epoch": 0.9873743880443184,
        "step": 7664
    },
    {
        "loss": 2.2726,
        "grad_norm": 2.478747844696045,
        "learning_rate": 3.695553033556842e-07,
        "epoch": 0.9875032208193765,
        "step": 7665
    },
    {
        "loss": 2.0699,
        "grad_norm": 1.7523688077926636,
        "learning_rate": 3.622100449983845e-07,
        "epoch": 0.9876320535944344,
        "step": 7666
    },
    {
        "loss": 1.8583,
        "grad_norm": 5.512734413146973,
        "learning_rate": 3.5493825283254846e-07,
        "epoch": 0.9877608863694924,
        "step": 7667
    },
    {
        "loss": 2.1414,
        "grad_norm": 2.297004222869873,
        "learning_rate": 3.477399376207557e-07,
        "epoch": 0.9878897191445504,
        "step": 7668
    },
    {
        "loss": 2.0118,
        "grad_norm": 2.072728395462036,
        "learning_rate": 3.406151100168453e-07,
        "epoch": 0.9880185519196083,
        "step": 7669
    },
    {
        "loss": 1.9464,
        "grad_norm": 2.1356754302978516,
        "learning_rate": 3.3356378056588755e-07,
        "epoch": 0.9881473846946663,
        "step": 7670
    },
    {
        "loss": 2.0441,
        "grad_norm": 2.1707653999328613,
        "learning_rate": 3.265859597041732e-07,
        "epoch": 0.9882762174697243,
        "step": 7671
    },
    {
        "loss": 1.6987,
        "grad_norm": 3.019751787185669,
        "learning_rate": 3.1968165775918546e-07,
        "epoch": 0.9884050502447823,
        "step": 7672
    },
    {
        "loss": 1.5328,
        "grad_norm": 2.364867925643921,
        "learning_rate": 3.128508849496059e-07,
        "epoch": 0.9885338830198402,
        "step": 7673
    },
    {
        "loss": 2.113,
        "grad_norm": 2.2766497135162354,
        "learning_rate": 3.0609365138530853e-07,
        "epoch": 0.9886627157948982,
        "step": 7674
    },
    {
        "loss": 1.5256,
        "grad_norm": 2.614781141281128,
        "learning_rate": 2.994099670672823e-07,
        "epoch": 0.9887915485699562,
        "step": 7675
    },
    {
        "loss": 1.5449,
        "grad_norm": 2.8080484867095947,
        "learning_rate": 2.9279984188769204e-07,
        "epoch": 0.9889203813450141,
        "step": 7676
    },
    {
        "loss": 2.1943,
        "grad_norm": 1.5083494186401367,
        "learning_rate": 2.8626328562983975e-07,
        "epoch": 0.9890492141200722,
        "step": 7677
    },
    {
        "loss": 2.2011,
        "grad_norm": 2.6518571376800537,
        "learning_rate": 2.7980030796810885e-07,
        "epoch": 0.9891780468951301,
        "step": 7678
    },
    {
        "loss": 2.3762,
        "grad_norm": 2.4817957878112793,
        "learning_rate": 2.7341091846800895e-07,
        "epoch": 0.989306879670188,
        "step": 7679
    },
    {
        "loss": 1.3874,
        "grad_norm": 3.1473939418792725,
        "learning_rate": 2.670951265861199e-07,
        "epoch": 0.9894357124452461,
        "step": 7680
    },
    {
        "loss": 2.159,
        "grad_norm": 1.814754605293274,
        "learning_rate": 2.6085294167011443e-07,
        "epoch": 0.989564545220304,
        "step": 7681
    },
    {
        "loss": 2.2432,
        "grad_norm": 2.0606849193573,
        "learning_rate": 2.546843729587023e-07,
        "epoch": 0.9896933779953621,
        "step": 7682
    },
    {
        "loss": 1.1911,
        "grad_norm": 2.4961962699890137,
        "learning_rate": 2.485894295816471e-07,
        "epoch": 0.98982221077042,
        "step": 7683
    },
    {
        "loss": 2.2402,
        "grad_norm": 2.9812586307525635,
        "learning_rate": 2.425681205597385e-07,
        "epoch": 0.9899510435454779,
        "step": 7684
    },
    {
        "loss": 2.2678,
        "grad_norm": 2.448009967803955,
        "learning_rate": 2.366204548047979e-07,
        "epoch": 0.990079876320536,
        "step": 7685
    },
    {
        "loss": 1.6965,
        "grad_norm": 2.4656243324279785,
        "learning_rate": 2.3074644111963917e-07,
        "epoch": 0.9902087090955939,
        "step": 7686
    },
    {
        "loss": 1.5254,
        "grad_norm": 3.0366740226745605,
        "learning_rate": 2.2494608819806363e-07,
        "epoch": 0.9903375418706519,
        "step": 7687
    },
    {
        "loss": 1.6769,
        "grad_norm": 3.7035346031188965,
        "learning_rate": 2.1921940462487077e-07,
        "epoch": 0.9904663746457099,
        "step": 7688
    },
    {
        "loss": 1.7767,
        "grad_norm": 2.3802309036254883,
        "learning_rate": 2.135663988757919e-07,
        "epoch": 0.9905952074207679,
        "step": 7689
    },
    {
        "loss": 1.7284,
        "grad_norm": 2.167170763015747,
        "learning_rate": 2.0798707931755645e-07,
        "epoch": 0.9907240401958258,
        "step": 7690
    },
    {
        "loss": 1.9914,
        "grad_norm": 1.862743854522705,
        "learning_rate": 2.0248145420779796e-07,
        "epoch": 0.9908528729708838,
        "step": 7691
    },
    {
        "loss": 2.1378,
        "grad_norm": 2.298826217651367,
        "learning_rate": 1.9704953169509265e-07,
        "epoch": 0.9909817057459418,
        "step": 7692
    },
    {
        "loss": 1.9401,
        "grad_norm": 3.032979965209961,
        "learning_rate": 1.916913198189374e-07,
        "epoch": 0.9911105385209997,
        "step": 7693
    },
    {
        "loss": 0.6624,
        "grad_norm": 3.9564037322998047,
        "learning_rate": 1.8640682650973295e-07,
        "epoch": 0.9912393712960578,
        "step": 7694
    },
    {
        "loss": 1.7147,
        "grad_norm": 2.019167184829712,
        "learning_rate": 1.8119605958876183e-07,
        "epoch": 0.9913682040711157,
        "step": 7695
    },
    {
        "loss": 2.2294,
        "grad_norm": 2.6403729915618896,
        "learning_rate": 1.760590267682105e-07,
        "epoch": 0.9914970368461736,
        "step": 7696
    },
    {
        "loss": 1.8317,
        "grad_norm": 2.1632421016693115,
        "learning_rate": 1.709957356511027e-07,
        "epoch": 0.9916258696212317,
        "step": 7697
    },
    {
        "loss": 1.4572,
        "grad_norm": 2.5725045204162598,
        "learning_rate": 1.6600619373135506e-07,
        "epoch": 0.9917547023962896,
        "step": 7698
    },
    {
        "loss": 1.3555,
        "grad_norm": 3.501256227493286,
        "learning_rate": 1.6109040839371592e-07,
        "epoch": 0.9918835351713476,
        "step": 7699
    },
    {
        "loss": 2.1526,
        "grad_norm": 2.3073809146881104,
        "learning_rate": 1.5624838691375988e-07,
        "epoch": 0.9920123679464056,
        "step": 7700
    },
    {
        "loss": 2.4058,
        "grad_norm": 1.7094824314117432,
        "learning_rate": 1.5148013645789883e-07,
        "epoch": 0.9921412007214635,
        "step": 7701
    },
    {
        "loss": 2.2177,
        "grad_norm": 1.5626137256622314,
        "learning_rate": 1.4678566408338756e-07,
        "epoch": 0.9922700334965215,
        "step": 7702
    },
    {
        "loss": 1.6539,
        "grad_norm": 2.6420953273773193,
        "learning_rate": 1.4216497673822936e-07,
        "epoch": 0.9923988662715795,
        "step": 7703
    },
    {
        "loss": 1.9874,
        "grad_norm": 2.2944300174713135,
        "learning_rate": 1.3761808126126486e-07,
        "epoch": 0.9925276990466375,
        "step": 7704
    },
    {
        "loss": 0.7383,
        "grad_norm": 4.4434003829956055,
        "learning_rate": 1.331449843821053e-07,
        "epoch": 0.9926565318216954,
        "step": 7705
    },
    {
        "loss": 2.6141,
        "grad_norm": 2.7445120811462402,
        "learning_rate": 1.2874569272113834e-07,
        "epoch": 0.9927853645967534,
        "step": 7706
    },
    {
        "loss": 2.011,
        "grad_norm": 3.118304491043091,
        "learning_rate": 1.2442021278952222e-07,
        "epoch": 0.9929141973718114,
        "step": 7707
    },
    {
        "loss": 1.5457,
        "grad_norm": 2.7274951934814453,
        "learning_rate": 1.2016855098915259e-07,
        "epoch": 0.9930430301468693,
        "step": 7708
    },
    {
        "loss": 1.971,
        "grad_norm": 1.460863709449768,
        "learning_rate": 1.1599071361268477e-07,
        "epoch": 0.9931718629219274,
        "step": 7709
    },
    {
        "loss": 1.7859,
        "grad_norm": 1.5854849815368652,
        "learning_rate": 1.1188670684351144e-07,
        "epoch": 0.9933006956969853,
        "step": 7710
    },
    {
        "loss": 1.9228,
        "grad_norm": 2.2489829063415527,
        "learning_rate": 1.0785653675575158e-07,
        "epoch": 0.9934295284720432,
        "step": 7711
    },
    {
        "loss": 1.9285,
        "grad_norm": 1.995661973953247,
        "learning_rate": 1.0390020931423938e-07,
        "epoch": 0.9935583612471013,
        "step": 7712
    },
    {
        "loss": 1.5544,
        "grad_norm": 3.13140606880188,
        "learning_rate": 1.0001773037451866e-07,
        "epoch": 0.9936871940221592,
        "step": 7713
    },
    {
        "loss": 1.9126,
        "grad_norm": 6.952259540557861,
        "learning_rate": 9.620910568283737e-08,
        "epoch": 0.9938160267972173,
        "step": 7714
    },
    {
        "loss": 1.3805,
        "grad_norm": 3.498603582382202,
        "learning_rate": 9.247434087612527e-08,
        "epoch": 0.9939448595722752,
        "step": 7715
    },
    {
        "loss": 1.5963,
        "grad_norm": 3.3364951610565186,
        "learning_rate": 8.881344148201632e-08,
        "epoch": 0.9940736923473331,
        "step": 7716
    },
    {
        "loss": 2.3503,
        "grad_norm": 2.1554412841796875,
        "learning_rate": 8.522641291880407e-08,
        "epoch": 0.9942025251223912,
        "step": 7717
    },
    {
        "loss": 1.9828,
        "grad_norm": 2.1139485836029053,
        "learning_rate": 8.171326049544736e-08,
        "epoch": 0.9943313578974491,
        "step": 7718
    },
    {
        "loss": 2.0958,
        "grad_norm": 2.0338447093963623,
        "learning_rate": 7.827398941158692e-08,
        "epoch": 0.9944601906725071,
        "step": 7719
    },
    {
        "loss": 2.3366,
        "grad_norm": 2.820754051208496,
        "learning_rate": 7.490860475748429e-08,
        "epoch": 0.9945890234475651,
        "step": 7720
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.751366376876831,
        "learning_rate": 7.161711151407735e-08,
        "epoch": 0.994717856222623,
        "step": 7721
    },
    {
        "loss": 1.9452,
        "grad_norm": 2.53067946434021,
        "learning_rate": 6.839951455291371e-08,
        "epoch": 0.994846688997681,
        "step": 7722
    },
    {
        "loss": 1.3964,
        "grad_norm": 3.145643949508667,
        "learning_rate": 6.525581863618401e-08,
        "epoch": 0.994975521772739,
        "step": 7723
    },
    {
        "loss": 1.4803,
        "grad_norm": 2.2859737873077393,
        "learning_rate": 6.21860284167164e-08,
        "epoch": 0.995104354547797,
        "step": 7724
    },
    {
        "loss": 1.2967,
        "grad_norm": 3.0480525493621826,
        "learning_rate": 5.919014843792092e-08,
        "epoch": 0.9952331873228549,
        "step": 7725
    },
    {
        "loss": 2.19,
        "grad_norm": 1.7186917066574097,
        "learning_rate": 5.626818313385074e-08,
        "epoch": 0.9953620200979129,
        "step": 7726
    },
    {
        "loss": 2.5381,
        "grad_norm": 2.134394884109497,
        "learning_rate": 5.342013682913538e-08,
        "epoch": 0.9954908528729709,
        "step": 7727
    },
    {
        "loss": 2.1165,
        "grad_norm": 2.5943500995635986,
        "learning_rate": 5.0646013739025225e-08,
        "epoch": 0.9956196856480288,
        "step": 7728
    },
    {
        "loss": 2.3076,
        "grad_norm": 2.417051315307617,
        "learning_rate": 4.79458179693415e-08,
        "epoch": 0.9957485184230869,
        "step": 7729
    },
    {
        "loss": 2.0464,
        "grad_norm": 1.7317167520523071,
        "learning_rate": 4.531955351649853e-08,
        "epoch": 0.9958773511981448,
        "step": 7730
    },
    {
        "loss": 1.2986,
        "grad_norm": 3.088134288787842,
        "learning_rate": 4.276722426749258e-08,
        "epoch": 0.9960061839732027,
        "step": 7731
    },
    {
        "loss": 1.8396,
        "grad_norm": 1.3856297731399536,
        "learning_rate": 4.0288833999879706e-08,
        "epoch": 0.9961350167482608,
        "step": 7732
    },
    {
        "loss": 1.6937,
        "grad_norm": 2.0235986709594727,
        "learning_rate": 3.788438638179237e-08,
        "epoch": 0.9962638495233187,
        "step": 7733
    },
    {
        "loss": 1.6181,
        "grad_norm": 2.4430856704711914,
        "learning_rate": 3.5553884971922805e-08,
        "epoch": 0.9963926822983767,
        "step": 7734
    },
    {
        "loss": 0.8478,
        "grad_norm": 3.443678140640259,
        "learning_rate": 3.3297333219522995e-08,
        "epoch": 0.9965215150734347,
        "step": 7735
    },
    {
        "loss": 1.746,
        "grad_norm": 1.7705225944519043,
        "learning_rate": 3.111473446438806e-08,
        "epoch": 0.9966503478484926,
        "step": 7736
    },
    {
        "loss": 1.6302,
        "grad_norm": 2.404949426651001,
        "learning_rate": 2.900609193686177e-08,
        "epoch": 0.9967791806235506,
        "step": 7737
    },
    {
        "loss": 1.6668,
        "grad_norm": 2.4706196784973145,
        "learning_rate": 2.6971408757842097e-08,
        "epoch": 0.9969080133986086,
        "step": 7738
    },
    {
        "loss": 1.8996,
        "grad_norm": 2.4308090209960938,
        "learning_rate": 2.5010687938742393e-08,
        "epoch": 0.9970368461736666,
        "step": 7739
    },
    {
        "loss": 1.9441,
        "grad_norm": 5.87177848815918,
        "learning_rate": 2.312393238151911e-08,
        "epoch": 0.9971656789487245,
        "step": 7740
    },
    {
        "loss": 2.0851,
        "grad_norm": 1.980826497077942,
        "learning_rate": 2.1311144878660705e-08,
        "epoch": 0.9972945117237826,
        "step": 7741
    },
    {
        "loss": 2.179,
        "grad_norm": 1.6362980604171753,
        "learning_rate": 1.9572328113176552e-08,
        "epoch": 0.9974233444988405,
        "step": 7742
    },
    {
        "loss": 1.8299,
        "grad_norm": 2.5315160751342773,
        "learning_rate": 1.7907484658585826e-08,
        "epoch": 0.9975521772738984,
        "step": 7743
    },
    {
        "loss": 2.1719,
        "grad_norm": 1.5855276584625244,
        "learning_rate": 1.6316616978945266e-08,
        "epoch": 0.9976810100489565,
        "step": 7744
    },
    {
        "loss": 1.9893,
        "grad_norm": 1.8761518001556396,
        "learning_rate": 1.4799727428799204e-08,
        "epoch": 0.9978098428240144,
        "step": 7745
    },
    {
        "loss": 1.0704,
        "grad_norm": 2.6667866706848145,
        "learning_rate": 1.335681825322399e-08,
        "epoch": 0.9979386755990725,
        "step": 7746
    },
    {
        "loss": 1.3657,
        "grad_norm": 3.2623353004455566,
        "learning_rate": 1.1987891587783573e-08,
        "epoch": 0.9980675083741304,
        "step": 7747
    },
    {
        "loss": 2.1381,
        "grad_norm": 1.766424536705017,
        "learning_rate": 1.0692949458551704e-08,
        "epoch": 0.9981963411491883,
        "step": 7748
    },
    {
        "loss": 1.2715,
        "grad_norm": 3.0040338039398193,
        "learning_rate": 9.471993782111943e-09,
        "epoch": 0.9983251739242464,
        "step": 7749
    },
    {
        "loss": 2.2781,
        "grad_norm": 2.3997182846069336,
        "learning_rate": 8.325026365518796e-09,
        "epoch": 0.9984540066993043,
        "step": 7750
    },
    {
        "loss": 1.2397,
        "grad_norm": 4.673105716705322,
        "learning_rate": 7.252048906347675e-09,
        "epoch": 0.9985828394743623,
        "step": 7751
    },
    {
        "loss": 2.3494,
        "grad_norm": 2.440746307373047,
        "learning_rate": 6.253062992650494e-09,
        "epoch": 0.9987116722494203,
        "step": 7752
    },
    {
        "loss": 1.8631,
        "grad_norm": 1.4282596111297607,
        "learning_rate": 5.3280701029778666e-09,
        "epoch": 0.9988405050244782,
        "step": 7753
    },
    {
        "loss": 1.7972,
        "grad_norm": 3.803192138671875,
        "learning_rate": 4.477071606345806e-09,
        "epoch": 0.9989693377995362,
        "step": 7754
    },
    {
        "loss": 1.9279,
        "grad_norm": 2.886540412902832,
        "learning_rate": 3.70006876228568e-09,
        "epoch": 0.9990981705745942,
        "step": 7755
    },
    {
        "loss": 1.8658,
        "grad_norm": 1.7715619802474976,
        "learning_rate": 2.9970627207942527e-09,
        "epoch": 0.9992270033496522,
        "step": 7756
    },
    {
        "loss": 1.8601,
        "grad_norm": 2.3042702674865723,
        "learning_rate": 2.368054522350338e-09,
        "epoch": 0.9993558361247101,
        "step": 7757
    },
    {
        "loss": 1.8469,
        "grad_norm": 1.5462830066680908,
        "learning_rate": 1.8130450979148007e-09,
        "epoch": 0.9994846688997681,
        "step": 7758
    },
    {
        "loss": 1.2314,
        "grad_norm": 2.700089454650879,
        "learning_rate": 1.3320352689250026e-09,
        "epoch": 0.9996135016748261,
        "step": 7759
    },
    {
        "loss": 1.7678,
        "grad_norm": 2.2793350219726562,
        "learning_rate": 9.250257473003565e-10,
        "epoch": 0.999742334449884,
        "step": 7760
    },
    {
        "loss": 2.0232,
        "grad_norm": 2.052018165588379,
        "learning_rate": 5.92017135431222e-10,
        "epoch": 0.9998711672249421,
        "step": 7761
    },
    {
        "loss": 2.2391,
        "grad_norm": 4.042721271514893,
        "learning_rate": 3.330099261844577e-10,
        "epoch": 1.0,
        "step": 7762
    },
    {
        "train_runtime": 21411.0201,
        "train_samples_per_second": 0.725,
        "train_steps_per_second": 0.363,
        "total_flos": 3.171059468783616e+17,
        "train_loss": 2.0175741947678616,
        "epoch": 1.0,
        "step": 7762
    }
]