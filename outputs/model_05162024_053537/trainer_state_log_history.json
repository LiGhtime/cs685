[
    {
        "loss": 3.8253,
        "grad_norm": 2.43245792388916,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 0.00027570995312930797,
        "step": 1
    },
    {
        "loss": 3.6297,
        "grad_norm": 2.239762544631958,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.0005514199062586159,
        "step": 2
    },
    {
        "loss": 3.5815,
        "grad_norm": 2.350236177444458,
        "learning_rate": 2.4e-05,
        "epoch": 0.0008271298593879239,
        "step": 3
    },
    {
        "loss": 3.5392,
        "grad_norm": 1.9378409385681152,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.0011028398125172319,
        "step": 4
    },
    {
        "loss": 3.7619,
        "grad_norm": 1.9050679206848145,
        "learning_rate": 4e-05,
        "epoch": 0.0013785497656465398,
        "step": 5
    },
    {
        "loss": 3.6509,
        "grad_norm": 2.1233155727386475,
        "learning_rate": 4.8e-05,
        "epoch": 0.0016542597187758478,
        "step": 6
    },
    {
        "loss": 3.7422,
        "grad_norm": 2.735966205596924,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0019299696719051558,
        "step": 7
    },
    {
        "loss": 3.5747,
        "grad_norm": 2.1638197898864746,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0022056796250344637,
        "step": 8
    },
    {
        "loss": 2.9645,
        "grad_norm": 1.5708873271942139,
        "learning_rate": 7.2e-05,
        "epoch": 0.0024813895781637717,
        "step": 9
    },
    {
        "loss": 3.8799,
        "grad_norm": 2.6818673610687256,
        "learning_rate": 8e-05,
        "epoch": 0.0027570995312930797,
        "step": 10
    },
    {
        "loss": 3.2626,
        "grad_norm": 2.790555238723755,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0030328094844223876,
        "step": 11
    },
    {
        "loss": 3.1189,
        "grad_norm": 1.4310574531555176,
        "learning_rate": 9.6e-05,
        "epoch": 0.0033085194375516956,
        "step": 12
    },
    {
        "loss": 3.3462,
        "grad_norm": 1.8173680305480957,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.0035842293906810036,
        "step": 13
    },
    {
        "loss": 3.1322,
        "grad_norm": 1.4222851991653442,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0038599393438103115,
        "step": 14
    },
    {
        "loss": 3.5307,
        "grad_norm": 1.8190494775772095,
        "learning_rate": 0.00012,
        "epoch": 0.0041356492969396195,
        "step": 15
    },
    {
        "loss": 3.2188,
        "grad_norm": 3.6121461391448975,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0044113592500689275,
        "step": 16
    },
    {
        "loss": 2.9082,
        "grad_norm": 1.6633387804031372,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.0046870692031982355,
        "step": 17
    },
    {
        "loss": 2.8858,
        "grad_norm": 1.5511863231658936,
        "learning_rate": 0.000144,
        "epoch": 0.004962779156327543,
        "step": 18
    },
    {
        "loss": 2.6325,
        "grad_norm": 1.4147216081619263,
        "learning_rate": 0.000152,
        "epoch": 0.005238489109456851,
        "step": 19
    },
    {
        "loss": 3.0323,
        "grad_norm": 1.8688215017318726,
        "learning_rate": 0.00016,
        "epoch": 0.005514199062586159,
        "step": 20
    },
    {
        "loss": 3.0081,
        "grad_norm": 1.8044911623001099,
        "learning_rate": 0.000168,
        "epoch": 0.005789909015715467,
        "step": 21
    },
    {
        "loss": 2.8632,
        "grad_norm": 2.2471518516540527,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.006065618968844775,
        "step": 22
    },
    {
        "loss": 2.4677,
        "grad_norm": 1.1575899124145508,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.006341328921974083,
        "step": 23
    },
    {
        "loss": 2.8318,
        "grad_norm": 1.6836135387420654,
        "learning_rate": 0.000192,
        "epoch": 0.006617038875103391,
        "step": 24
    },
    {
        "loss": 3.0146,
        "grad_norm": 2.2405388355255127,
        "learning_rate": 0.0002,
        "epoch": 0.006892748828232699,
        "step": 25
    },
    {
        "loss": 2.7617,
        "grad_norm": 1.8503254652023315,
        "learning_rate": 0.00019999965768605272,
        "epoch": 0.007168458781362007,
        "step": 26
    },
    {
        "loss": 2.6603,
        "grad_norm": 1.7576936483383179,
        "learning_rate": 0.00019999863074655455,
        "epoch": 0.007444168734491315,
        "step": 27
    },
    {
        "loss": 2.5576,
        "grad_norm": 1.8299611806869507,
        "learning_rate": 0.0001999969191885361,
        "epoch": 0.007719878687620623,
        "step": 28
    },
    {
        "loss": 2.7561,
        "grad_norm": 1.671639084815979,
        "learning_rate": 0.00019999452302371524,
        "epoch": 0.007995588640749932,
        "step": 29
    },
    {
        "loss": 2.7057,
        "grad_norm": 1.3430089950561523,
        "learning_rate": 0.00019999144226849673,
        "epoch": 0.008271298593879239,
        "step": 30
    },
    {
        "loss": 2.7173,
        "grad_norm": 1.4614014625549316,
        "learning_rate": 0.00019998767694397236,
        "epoch": 0.008547008547008548,
        "step": 31
    },
    {
        "loss": 2.8703,
        "grad_norm": 1.3234716653823853,
        "learning_rate": 0.0001999832270759205,
        "epoch": 0.008822718500137855,
        "step": 32
    },
    {
        "loss": 2.7769,
        "grad_norm": 1.35330069065094,
        "learning_rate": 0.00019997809269480625,
        "epoch": 0.009098428453267164,
        "step": 33
    },
    {
        "loss": 2.6884,
        "grad_norm": 1.765812635421753,
        "learning_rate": 0.000199972273835781,
        "epoch": 0.009374138406396471,
        "step": 34
    },
    {
        "loss": 2.5307,
        "grad_norm": 2.265451669692993,
        "learning_rate": 0.00019996577053868229,
        "epoch": 0.00964984835952578,
        "step": 35
    },
    {
        "loss": 2.3585,
        "grad_norm": 2.103180408477783,
        "learning_rate": 0.00019995858284803348,
        "epoch": 0.009925558312655087,
        "step": 36
    },
    {
        "loss": 2.5974,
        "grad_norm": 1.5593221187591553,
        "learning_rate": 0.00019995071081304352,
        "epoch": 0.010201268265784396,
        "step": 37
    },
    {
        "loss": 2.542,
        "grad_norm": 1.7925742864608765,
        "learning_rate": 0.0001999421544876066,
        "epoch": 0.010476978218913703,
        "step": 38
    },
    {
        "loss": 2.4777,
        "grad_norm": 1.8468645811080933,
        "learning_rate": 0.00019993291393030163,
        "epoch": 0.010752688172043012,
        "step": 39
    },
    {
        "loss": 2.6193,
        "grad_norm": 0.910240888595581,
        "learning_rate": 0.00019992298920439207,
        "epoch": 0.011028398125172319,
        "step": 40
    },
    {
        "loss": 1.7809,
        "grad_norm": 1.7580732107162476,
        "learning_rate": 0.0001999123803778254,
        "epoch": 0.011304108078301628,
        "step": 41
    },
    {
        "loss": 2.4851,
        "grad_norm": 1.5866626501083374,
        "learning_rate": 0.00019990108752323258,
        "epoch": 0.011579818031430935,
        "step": 42
    },
    {
        "loss": 2.4365,
        "grad_norm": 1.6345080137252808,
        "learning_rate": 0.00019988911071792763,
        "epoch": 0.011855527984560243,
        "step": 43
    },
    {
        "loss": 2.7,
        "grad_norm": 1.269728422164917,
        "learning_rate": 0.00019987645004390712,
        "epoch": 0.01213123793768955,
        "step": 44
    },
    {
        "loss": 2.4493,
        "grad_norm": 1.319977045059204,
        "learning_rate": 0.00019986310558784957,
        "epoch": 0.01240694789081886,
        "step": 45
    },
    {
        "loss": 2.5962,
        "grad_norm": 1.4880836009979248,
        "learning_rate": 0.00019984907744111482,
        "epoch": 0.012682657843948167,
        "step": 46
    },
    {
        "loss": 1.9385,
        "grad_norm": 2.530529499053955,
        "learning_rate": 0.00019983436569974346,
        "epoch": 0.012958367797077475,
        "step": 47
    },
    {
        "loss": 2.598,
        "grad_norm": 0.9331924915313721,
        "learning_rate": 0.00019981897046445618,
        "epoch": 0.013234077750206782,
        "step": 48
    },
    {
        "loss": 2.3593,
        "grad_norm": 1.0199958086013794,
        "learning_rate": 0.00019980289184065312,
        "epoch": 0.013509787703336091,
        "step": 49
    },
    {
        "loss": 2.6198,
        "grad_norm": 1.1257380247116089,
        "learning_rate": 0.00019978612993841295,
        "epoch": 0.013785497656465398,
        "step": 50
    },
    {
        "loss": 2.6458,
        "grad_norm": 1.3496041297912598,
        "learning_rate": 0.0001997686848724924,
        "epoch": 0.014061207609594707,
        "step": 51
    },
    {
        "loss": 2.6742,
        "grad_norm": 1.511338233947754,
        "learning_rate": 0.00019975055676232516,
        "epoch": 0.014336917562724014,
        "step": 52
    },
    {
        "loss": 2.0612,
        "grad_norm": 2.075019598007202,
        "learning_rate": 0.0001997317457320214,
        "epoch": 0.014612627515853323,
        "step": 53
    },
    {
        "loss": 2.4259,
        "grad_norm": 1.2950536012649536,
        "learning_rate": 0.00019971225191036667,
        "epoch": 0.01488833746898263,
        "step": 54
    },
    {
        "loss": 2.1207,
        "grad_norm": 2.013119697570801,
        "learning_rate": 0.00019969207543082108,
        "epoch": 0.015164047422111939,
        "step": 55
    },
    {
        "loss": 2.4221,
        "grad_norm": 1.9169195890426636,
        "learning_rate": 0.0001996712164315185,
        "epoch": 0.015439757375241246,
        "step": 56
    },
    {
        "loss": 2.5346,
        "grad_norm": 1.6801044940948486,
        "learning_rate": 0.0001996496750552654,
        "epoch": 0.015715467328370553,
        "step": 57
    },
    {
        "loss": 2.1345,
        "grad_norm": 2.2915549278259277,
        "learning_rate": 0.00019962745144954007,
        "epoch": 0.015991177281499864,
        "step": 58
    },
    {
        "loss": 2.653,
        "grad_norm": 1.468557357788086,
        "learning_rate": 0.0001996045457664915,
        "epoch": 0.01626688723462917,
        "step": 59
    },
    {
        "loss": 2.0875,
        "grad_norm": 1.5135546922683716,
        "learning_rate": 0.0001995809581629384,
        "epoch": 0.016542597187758478,
        "step": 60
    },
    {
        "loss": 1.8424,
        "grad_norm": 1.837760329246521,
        "learning_rate": 0.0001995566888003681,
        "epoch": 0.016818307140887785,
        "step": 61
    },
    {
        "loss": 2.4753,
        "grad_norm": 1.140852689743042,
        "learning_rate": 0.0001995317378449354,
        "epoch": 0.017094017094017096,
        "step": 62
    },
    {
        "loss": 2.1285,
        "grad_norm": 2.1408801078796387,
        "learning_rate": 0.0001995061054674615,
        "epoch": 0.017369727047146403,
        "step": 63
    },
    {
        "loss": 2.872,
        "grad_norm": 1.2762343883514404,
        "learning_rate": 0.00019947979184343283,
        "epoch": 0.01764543700027571,
        "step": 64
    },
    {
        "loss": 2.2398,
        "grad_norm": 1.5880796909332275,
        "learning_rate": 0.00019945279715299975,
        "epoch": 0.017921146953405017,
        "step": 65
    },
    {
        "loss": 2.6044,
        "grad_norm": 1.5629794597625732,
        "learning_rate": 0.00019942512158097552,
        "epoch": 0.018196856906534328,
        "step": 66
    },
    {
        "loss": 1.7904,
        "grad_norm": 2.0501041412353516,
        "learning_rate": 0.00019939676531683477,
        "epoch": 0.018472566859663635,
        "step": 67
    },
    {
        "loss": 1.8872,
        "grad_norm": 1.2327547073364258,
        "learning_rate": 0.00019936772855471243,
        "epoch": 0.018748276812792942,
        "step": 68
    },
    {
        "loss": 2.5068,
        "grad_norm": 1.0210851430892944,
        "learning_rate": 0.0001993380114934022,
        "epoch": 0.01902398676592225,
        "step": 69
    },
    {
        "loss": 2.0803,
        "grad_norm": 1.9683656692504883,
        "learning_rate": 0.00019930761433635544,
        "epoch": 0.01929969671905156,
        "step": 70
    },
    {
        "loss": 2.476,
        "grad_norm": 1.031027913093567,
        "learning_rate": 0.00019927653729167957,
        "epoch": 0.019575406672180867,
        "step": 71
    },
    {
        "loss": 2.1361,
        "grad_norm": 1.2171884775161743,
        "learning_rate": 0.00019924478057213664,
        "epoch": 0.019851116625310174,
        "step": 72
    },
    {
        "loss": 2.2848,
        "grad_norm": 1.7502779960632324,
        "learning_rate": 0.0001992123443951421,
        "epoch": 0.02012682657843948,
        "step": 73
    },
    {
        "loss": 1.9769,
        "grad_norm": 1.224792718887329,
        "learning_rate": 0.00019917922898276297,
        "epoch": 0.02040253653156879,
        "step": 74
    },
    {
        "loss": 2.2872,
        "grad_norm": 1.343658685684204,
        "learning_rate": 0.0001991454345617167,
        "epoch": 0.0206782464846981,
        "step": 75
    },
    {
        "loss": 1.7371,
        "grad_norm": 2.4205074310302734,
        "learning_rate": 0.00019911096136336924,
        "epoch": 0.020953956437827406,
        "step": 76
    },
    {
        "loss": 2.1849,
        "grad_norm": 1.579557180404663,
        "learning_rate": 0.00019907580962373378,
        "epoch": 0.021229666390956713,
        "step": 77
    },
    {
        "loss": 2.7684,
        "grad_norm": 1.3247222900390625,
        "learning_rate": 0.0001990399795834689,
        "epoch": 0.021505376344086023,
        "step": 78
    },
    {
        "loss": 2.5589,
        "grad_norm": 1.2097043991088867,
        "learning_rate": 0.00019900347148787707,
        "epoch": 0.02178108629721533,
        "step": 79
    },
    {
        "loss": 2.761,
        "grad_norm": 1.73802649974823,
        "learning_rate": 0.00019896628558690283,
        "epoch": 0.022056796250344637,
        "step": 80
    },
    {
        "loss": 1.5986,
        "grad_norm": 1.6874582767486572,
        "learning_rate": 0.00019892842213513134,
        "epoch": 0.022332506203473945,
        "step": 81
    },
    {
        "loss": 1.8806,
        "grad_norm": 2.598890542984009,
        "learning_rate": 0.0001988898813917863,
        "epoch": 0.022608216156603255,
        "step": 82
    },
    {
        "loss": 2.5012,
        "grad_norm": 1.1864149570465088,
        "learning_rate": 0.0001988506636207284,
        "epoch": 0.022883926109732562,
        "step": 83
    },
    {
        "loss": 2.6949,
        "grad_norm": 1.5998722314834595,
        "learning_rate": 0.0001988107690904534,
        "epoch": 0.02315963606286187,
        "step": 84
    },
    {
        "loss": 2.2295,
        "grad_norm": 4.120745658874512,
        "learning_rate": 0.00019877019807409044,
        "epoch": 0.023435346015991176,
        "step": 85
    },
    {
        "loss": 2.4021,
        "grad_norm": 1.673679232597351,
        "learning_rate": 0.00019872895084939996,
        "epoch": 0.023711055969120487,
        "step": 86
    },
    {
        "loss": 1.8341,
        "grad_norm": 1.9793161153793335,
        "learning_rate": 0.000198687027698772,
        "epoch": 0.023986765922249794,
        "step": 87
    },
    {
        "loss": 2.5801,
        "grad_norm": 1.4676871299743652,
        "learning_rate": 0.00019864442890922415,
        "epoch": 0.0242624758753791,
        "step": 88
    },
    {
        "loss": 1.6899,
        "grad_norm": 2.06534481048584,
        "learning_rate": 0.0001986011547723996,
        "epoch": 0.024538185828508408,
        "step": 89
    },
    {
        "loss": 2.7143,
        "grad_norm": 1.1173206567764282,
        "learning_rate": 0.00019855720558456514,
        "epoch": 0.02481389578163772,
        "step": 90
    },
    {
        "loss": 2.4632,
        "grad_norm": 1.6201198101043701,
        "learning_rate": 0.00019851258164660919,
        "epoch": 0.025089605734767026,
        "step": 91
    },
    {
        "loss": 2.5667,
        "grad_norm": 1.623096227645874,
        "learning_rate": 0.00019846728326403966,
        "epoch": 0.025365315687896333,
        "step": 92
    },
    {
        "loss": 2.4103,
        "grad_norm": 1.5662065744400024,
        "learning_rate": 0.00019842131074698194,
        "epoch": 0.02564102564102564,
        "step": 93
    },
    {
        "loss": 1.8487,
        "grad_norm": 1.5380797386169434,
        "learning_rate": 0.0001983746644101767,
        "epoch": 0.02591673559415495,
        "step": 94
    },
    {
        "loss": 2.384,
        "grad_norm": 1.885238766670227,
        "learning_rate": 0.00019832734457297775,
        "epoch": 0.026192445547284258,
        "step": 95
    },
    {
        "loss": 2.7479,
        "grad_norm": 1.65823233127594,
        "learning_rate": 0.0001982793515593499,
        "epoch": 0.026468155500413565,
        "step": 96
    },
    {
        "loss": 2.5238,
        "grad_norm": 1.7194018363952637,
        "learning_rate": 0.0001982306856978667,
        "epoch": 0.026743865453542872,
        "step": 97
    },
    {
        "loss": 2.6903,
        "grad_norm": 1.083414912223816,
        "learning_rate": 0.00019818134732170828,
        "epoch": 0.027019575406672183,
        "step": 98
    },
    {
        "loss": 2.6505,
        "grad_norm": 0.9327681064605713,
        "learning_rate": 0.00019813133676865885,
        "epoch": 0.02729528535980149,
        "step": 99
    },
    {
        "loss": 2.2578,
        "grad_norm": 1.9629132747650146,
        "learning_rate": 0.00019808065438110464,
        "epoch": 0.027570995312930797,
        "step": 100
    },
    {
        "loss": 2.6832,
        "grad_norm": 0.9875220656394958,
        "learning_rate": 0.00019802930050603142,
        "epoch": 0.027846705266060104,
        "step": 101
    },
    {
        "loss": 2.5975,
        "grad_norm": 1.1079394817352295,
        "learning_rate": 0.00019797727549502206,
        "epoch": 0.028122415219189414,
        "step": 102
    },
    {
        "loss": 2.489,
        "grad_norm": 1.4100780487060547,
        "learning_rate": 0.00019792457970425445,
        "epoch": 0.02839812517231872,
        "step": 103
    },
    {
        "loss": 2.3213,
        "grad_norm": 1.739356279373169,
        "learning_rate": 0.00019787121349449855,
        "epoch": 0.02867383512544803,
        "step": 104
    },
    {
        "loss": 2.2379,
        "grad_norm": 1.3445223569869995,
        "learning_rate": 0.00019781717723111437,
        "epoch": 0.028949545078577336,
        "step": 105
    },
    {
        "loss": 2.1586,
        "grad_norm": 1.4882299900054932,
        "learning_rate": 0.0001977624712840492,
        "epoch": 0.029225255031706646,
        "step": 106
    },
    {
        "loss": 2.4182,
        "grad_norm": 1.3764259815216064,
        "learning_rate": 0.00019770709602783527,
        "epoch": 0.029500964984835953,
        "step": 107
    },
    {
        "loss": 2.1211,
        "grad_norm": 1.831329107284546,
        "learning_rate": 0.000197651051841587,
        "epoch": 0.02977667493796526,
        "step": 108
    },
    {
        "loss": 2.8156,
        "grad_norm": 1.197148323059082,
        "learning_rate": 0.00019759433910899855,
        "epoch": 0.030052384891094568,
        "step": 109
    },
    {
        "loss": 2.4382,
        "grad_norm": 1.29001784324646,
        "learning_rate": 0.00019753695821834105,
        "epoch": 0.030328094844223878,
        "step": 110
    },
    {
        "loss": 2.4684,
        "grad_norm": 1.2199084758758545,
        "learning_rate": 0.0001974789095624601,
        "epoch": 0.030603804797353185,
        "step": 111
    },
    {
        "loss": 1.8128,
        "grad_norm": 1.7164366245269775,
        "learning_rate": 0.00019742019353877302,
        "epoch": 0.030879514750482492,
        "step": 112
    },
    {
        "loss": 2.8897,
        "grad_norm": 1.138394832611084,
        "learning_rate": 0.00019736081054926613,
        "epoch": 0.0311552247036118,
        "step": 113
    },
    {
        "loss": 2.1475,
        "grad_norm": 2.0887744426727295,
        "learning_rate": 0.00019730076100049181,
        "epoch": 0.03143093465674111,
        "step": 114
    },
    {
        "loss": 2.762,
        "grad_norm": 1.0439072847366333,
        "learning_rate": 0.00019724004530356616,
        "epoch": 0.03170664460987042,
        "step": 115
    },
    {
        "loss": 1.8606,
        "grad_norm": 2.4694387912750244,
        "learning_rate": 0.00019717866387416568,
        "epoch": 0.03198235456299973,
        "step": 116
    },
    {
        "loss": 2.4399,
        "grad_norm": 1.0938301086425781,
        "learning_rate": 0.00019711661713252477,
        "epoch": 0.03225806451612903,
        "step": 117
    },
    {
        "loss": 2.5219,
        "grad_norm": 1.5590507984161377,
        "learning_rate": 0.0001970539055034328,
        "epoch": 0.03253377446925834,
        "step": 118
    },
    {
        "loss": 2.2902,
        "grad_norm": 1.2817491292953491,
        "learning_rate": 0.00019699052941623098,
        "epoch": 0.032809484422387646,
        "step": 119
    },
    {
        "loss": 1.9374,
        "grad_norm": 1.6567596197128296,
        "learning_rate": 0.00019692648930480977,
        "epoch": 0.033085194375516956,
        "step": 120
    },
    {
        "loss": 2.1666,
        "grad_norm": 1.6578407287597656,
        "learning_rate": 0.0001968617856076056,
        "epoch": 0.03336090432864627,
        "step": 121
    },
    {
        "loss": 2.4487,
        "grad_norm": 1.0828877687454224,
        "learning_rate": 0.000196796418767598,
        "epoch": 0.03363661428177557,
        "step": 122
    },
    {
        "loss": 1.6932,
        "grad_norm": 1.7189035415649414,
        "learning_rate": 0.00019673038923230662,
        "epoch": 0.03391232423490488,
        "step": 123
    },
    {
        "loss": 2.4846,
        "grad_norm": 1.44881272315979,
        "learning_rate": 0.0001966636974537881,
        "epoch": 0.03418803418803419,
        "step": 124
    },
    {
        "loss": 2.4192,
        "grad_norm": 1.681239366531372,
        "learning_rate": 0.0001965963438886329,
        "epoch": 0.034463744141163495,
        "step": 125
    },
    {
        "loss": 2.4889,
        "grad_norm": 1.3079923391342163,
        "learning_rate": 0.00019652832899796235,
        "epoch": 0.034739454094292806,
        "step": 126
    },
    {
        "loss": 2.5965,
        "grad_norm": 1.2502849102020264,
        "learning_rate": 0.0001964596532474254,
        "epoch": 0.03501516404742211,
        "step": 127
    },
    {
        "loss": 2.3798,
        "grad_norm": 1.8621917963027954,
        "learning_rate": 0.0001963903171071953,
        "epoch": 0.03529087400055142,
        "step": 128
    },
    {
        "loss": 2.6949,
        "grad_norm": 1.3282333612442017,
        "learning_rate": 0.00019632032105196671,
        "epoch": 0.03556658395368073,
        "step": 129
    },
    {
        "loss": 2.5643,
        "grad_norm": 1.1730799674987793,
        "learning_rate": 0.00019624966556095207,
        "epoch": 0.035842293906810034,
        "step": 130
    },
    {
        "loss": 2.6943,
        "grad_norm": 1.2328221797943115,
        "learning_rate": 0.00019617835111787865,
        "epoch": 0.036118003859939345,
        "step": 131
    },
    {
        "loss": 2.1578,
        "grad_norm": 1.9447956085205078,
        "learning_rate": 0.00019610637821098497,
        "epoch": 0.036393713813068655,
        "step": 132
    },
    {
        "loss": 2.2866,
        "grad_norm": 1.5636192560195923,
        "learning_rate": 0.00019603374733301763,
        "epoch": 0.03666942376619796,
        "step": 133
    },
    {
        "loss": 2.5421,
        "grad_norm": 0.9871601462364197,
        "learning_rate": 0.00019596045898122788,
        "epoch": 0.03694513371932727,
        "step": 134
    },
    {
        "loss": 1.915,
        "grad_norm": 1.3449511528015137,
        "learning_rate": 0.00019588651365736825,
        "epoch": 0.03722084367245657,
        "step": 135
    },
    {
        "loss": 1.8053,
        "grad_norm": 2.334648609161377,
        "learning_rate": 0.00019581191186768905,
        "epoch": 0.037496553625585884,
        "step": 136
    },
    {
        "loss": 2.7494,
        "grad_norm": 1.1992794275283813,
        "learning_rate": 0.0001957366541229349,
        "epoch": 0.037772263578715194,
        "step": 137
    },
    {
        "loss": 2.153,
        "grad_norm": 1.749003529548645,
        "learning_rate": 0.00019566074093834134,
        "epoch": 0.0380479735318445,
        "step": 138
    },
    {
        "loss": 2.332,
        "grad_norm": 1.3592007160186768,
        "learning_rate": 0.00019558417283363123,
        "epoch": 0.03832368348497381,
        "step": 139
    },
    {
        "loss": 2.4875,
        "grad_norm": 1.1536030769348145,
        "learning_rate": 0.00019550695033301115,
        "epoch": 0.03859939343810312,
        "step": 140
    },
    {
        "loss": 2.5783,
        "grad_norm": 1.9289448261260986,
        "learning_rate": 0.00019542907396516784,
        "epoch": 0.03887510339123242,
        "step": 141
    },
    {
        "loss": 2.0739,
        "grad_norm": 1.9430716037750244,
        "learning_rate": 0.00019535054426326472,
        "epoch": 0.03915081334436173,
        "step": 142
    },
    {
        "loss": 2.6055,
        "grad_norm": 1.2868279218673706,
        "learning_rate": 0.000195271361764938,
        "epoch": 0.03942652329749104,
        "step": 143
    },
    {
        "loss": 2.0661,
        "grad_norm": 1.4696953296661377,
        "learning_rate": 0.00019519152701229312,
        "epoch": 0.03970223325062035,
        "step": 144
    },
    {
        "loss": 2.7684,
        "grad_norm": 1.0073853731155396,
        "learning_rate": 0.0001951110405519011,
        "epoch": 0.03997794320374966,
        "step": 145
    },
    {
        "loss": 2.4678,
        "grad_norm": 1.2551501989364624,
        "learning_rate": 0.0001950299029347947,
        "epoch": 0.04025365315687896,
        "step": 146
    },
    {
        "loss": 2.6256,
        "grad_norm": 0.9458064436912537,
        "learning_rate": 0.00019494811471646464,
        "epoch": 0.04052936311000827,
        "step": 147
    },
    {
        "loss": 2.4665,
        "grad_norm": 1.380112648010254,
        "learning_rate": 0.00019486567645685593,
        "epoch": 0.04080507306313758,
        "step": 148
    },
    {
        "loss": 2.4256,
        "grad_norm": 1.9171520471572876,
        "learning_rate": 0.00019478258872036386,
        "epoch": 0.041080783016266886,
        "step": 149
    },
    {
        "loss": 2.3956,
        "grad_norm": 1.620111107826233,
        "learning_rate": 0.00019469885207583026,
        "epoch": 0.0413564929693962,
        "step": 150
    },
    {
        "loss": 2.2985,
        "grad_norm": 1.1011996269226074,
        "learning_rate": 0.00019461446709653955,
        "epoch": 0.0416322029225255,
        "step": 151
    },
    {
        "loss": 2.6658,
        "grad_norm": 1.2858136892318726,
        "learning_rate": 0.00019452943436021482,
        "epoch": 0.04190791287565481,
        "step": 152
    },
    {
        "loss": 2.7114,
        "grad_norm": 0.9257626533508301,
        "learning_rate": 0.00019444375444901396,
        "epoch": 0.04218362282878412,
        "step": 153
    },
    {
        "loss": 1.7367,
        "grad_norm": 2.213117837905884,
        "learning_rate": 0.0001943574279495255,
        "epoch": 0.042459332781913425,
        "step": 154
    },
    {
        "loss": 2.229,
        "grad_norm": 1.4843519926071167,
        "learning_rate": 0.00019427045545276474,
        "epoch": 0.042735042735042736,
        "step": 155
    },
    {
        "loss": 2.1424,
        "grad_norm": 1.665511965751648,
        "learning_rate": 0.00019418283755416967,
        "epoch": 0.043010752688172046,
        "step": 156
    },
    {
        "loss": 1.6211,
        "grad_norm": 2.0763068199157715,
        "learning_rate": 0.00019409457485359681,
        "epoch": 0.04328646264130135,
        "step": 157
    },
    {
        "loss": 2.0683,
        "grad_norm": 2.3731274604797363,
        "learning_rate": 0.0001940056679553173,
        "epoch": 0.04356217259443066,
        "step": 158
    },
    {
        "loss": 2.3049,
        "grad_norm": 1.1657897233963013,
        "learning_rate": 0.00019391611746801254,
        "epoch": 0.043837882547559964,
        "step": 159
    },
    {
        "loss": 2.3055,
        "grad_norm": 1.581394076347351,
        "learning_rate": 0.0001938259240047701,
        "epoch": 0.044113592500689275,
        "step": 160
    },
    {
        "loss": 2.5007,
        "grad_norm": 0.9517700672149658,
        "learning_rate": 0.00019373508818307965,
        "epoch": 0.044389302453818585,
        "step": 161
    },
    {
        "loss": 2.4975,
        "grad_norm": 1.4184209108352661,
        "learning_rate": 0.00019364361062482858,
        "epoch": 0.04466501240694789,
        "step": 162
    },
    {
        "loss": 2.4573,
        "grad_norm": 1.1245896816253662,
        "learning_rate": 0.0001935514919562977,
        "epoch": 0.0449407223600772,
        "step": 163
    },
    {
        "loss": 1.862,
        "grad_norm": 2.0056657791137695,
        "learning_rate": 0.00019345873280815712,
        "epoch": 0.04521643231320651,
        "step": 164
    },
    {
        "loss": 2.5549,
        "grad_norm": 1.5903087854385376,
        "learning_rate": 0.0001933653338154619,
        "epoch": 0.045492142266335814,
        "step": 165
    },
    {
        "loss": 2.2248,
        "grad_norm": 1.2600340843200684,
        "learning_rate": 0.00019327129561764753,
        "epoch": 0.045767852219465124,
        "step": 166
    },
    {
        "loss": 2.658,
        "grad_norm": 1.0903594493865967,
        "learning_rate": 0.00019317661885852582,
        "epoch": 0.04604356217259443,
        "step": 167
    },
    {
        "loss": 2.5737,
        "grad_norm": 1.8070449829101562,
        "learning_rate": 0.0001930813041862802,
        "epoch": 0.04631927212572374,
        "step": 168
    },
    {
        "loss": 1.9465,
        "grad_norm": 1.8702553510665894,
        "learning_rate": 0.00019298535225346157,
        "epoch": 0.04659498207885305,
        "step": 169
    },
    {
        "loss": 2.9012,
        "grad_norm": 0.906887948513031,
        "learning_rate": 0.00019288876371698356,
        "epoch": 0.04687069203198235,
        "step": 170
    },
    {
        "loss": 2.1559,
        "grad_norm": 1.4662914276123047,
        "learning_rate": 0.0001927915392381183,
        "epoch": 0.04714640198511166,
        "step": 171
    },
    {
        "loss": 2.0418,
        "grad_norm": 1.7935738563537598,
        "learning_rate": 0.0001926936794824916,
        "epoch": 0.047422111938240974,
        "step": 172
    },
    {
        "loss": 2.3112,
        "grad_norm": 1.3133809566497803,
        "learning_rate": 0.0001925951851200787,
        "epoch": 0.04769782189137028,
        "step": 173
    },
    {
        "loss": 1.8896,
        "grad_norm": 1.6227672100067139,
        "learning_rate": 0.00019249605682519953,
        "epoch": 0.04797353184449959,
        "step": 174
    },
    {
        "loss": 2.565,
        "grad_norm": 0.9354513883590698,
        "learning_rate": 0.000192396295276514,
        "epoch": 0.04824924179762889,
        "step": 175
    },
    {
        "loss": 2.3627,
        "grad_norm": 1.5197665691375732,
        "learning_rate": 0.00019229590115701748,
        "epoch": 0.0485249517507582,
        "step": 176
    },
    {
        "loss": 2.5006,
        "grad_norm": 1.0420042276382446,
        "learning_rate": 0.00019219487515403612,
        "epoch": 0.04880066170388751,
        "step": 177
    },
    {
        "loss": 2.3203,
        "grad_norm": 1.5926110744476318,
        "learning_rate": 0.00019209321795922218,
        "epoch": 0.049076371657016817,
        "step": 178
    },
    {
        "loss": 2.5871,
        "grad_norm": 1.0490940809249878,
        "learning_rate": 0.0001919909302685491,
        "epoch": 0.04935208161014613,
        "step": 179
    },
    {
        "loss": 1.958,
        "grad_norm": 1.9717801809310913,
        "learning_rate": 0.00019188801278230698,
        "epoch": 0.04962779156327544,
        "step": 180
    },
    {
        "loss": 2.4318,
        "grad_norm": 1.8914090394973755,
        "learning_rate": 0.0001917844662050976,
        "epoch": 0.04990350151640474,
        "step": 181
    },
    {
        "loss": 2.4775,
        "grad_norm": 1.0272328853607178,
        "learning_rate": 0.00019168029124582976,
        "epoch": 0.05017921146953405,
        "step": 182
    },
    {
        "loss": 2.4554,
        "grad_norm": 1.493274450302124,
        "learning_rate": 0.00019157548861771425,
        "epoch": 0.050454921422663356,
        "step": 183
    },
    {
        "loss": 1.7767,
        "grad_norm": 1.6637475490570068,
        "learning_rate": 0.00019147005903825915,
        "epoch": 0.050730631375792666,
        "step": 184
    },
    {
        "loss": 2.3219,
        "grad_norm": 1.1707605123519897,
        "learning_rate": 0.00019136400322926472,
        "epoch": 0.05100634132892198,
        "step": 185
    },
    {
        "loss": 2.1566,
        "grad_norm": 0.9300479888916016,
        "learning_rate": 0.00019125732191681866,
        "epoch": 0.05128205128205128,
        "step": 186
    },
    {
        "loss": 1.5546,
        "grad_norm": 1.7209595441818237,
        "learning_rate": 0.0001911500158312909,
        "epoch": 0.05155776123518059,
        "step": 187
    },
    {
        "loss": 1.9789,
        "grad_norm": 2.3499979972839355,
        "learning_rate": 0.00019104208570732895,
        "epoch": 0.0518334711883099,
        "step": 188
    },
    {
        "loss": 2.2699,
        "grad_norm": 1.3021429777145386,
        "learning_rate": 0.00019093353228385245,
        "epoch": 0.052109181141439205,
        "step": 189
    },
    {
        "loss": 2.3503,
        "grad_norm": 1.5630682706832886,
        "learning_rate": 0.0001908243563040485,
        "epoch": 0.052384891094568516,
        "step": 190
    },
    {
        "loss": 2.5374,
        "grad_norm": 1.5725804567337036,
        "learning_rate": 0.00019071455851536623,
        "epoch": 0.05266060104769782,
        "step": 191
    },
    {
        "loss": 2.7065,
        "grad_norm": 1.0600682497024536,
        "learning_rate": 0.000190604139669512,
        "epoch": 0.05293631100082713,
        "step": 192
    },
    {
        "loss": 2.4106,
        "grad_norm": 1.1933012008666992,
        "learning_rate": 0.000190493100522444,
        "epoch": 0.05321202095395644,
        "step": 193
    },
    {
        "loss": 2.0874,
        "grad_norm": 1.7795255184173584,
        "learning_rate": 0.00019038144183436718,
        "epoch": 0.053487730907085744,
        "step": 194
    },
    {
        "loss": 2.4164,
        "grad_norm": 0.8305485248565674,
        "learning_rate": 0.00019026916436972808,
        "epoch": 0.053763440860215055,
        "step": 195
    },
    {
        "loss": 2.0783,
        "grad_norm": 1.9929450750350952,
        "learning_rate": 0.0001901562688972096,
        "epoch": 0.054039150813344365,
        "step": 196
    },
    {
        "loss": 2.7736,
        "grad_norm": 1.534732699394226,
        "learning_rate": 0.00019004275618972555,
        "epoch": 0.05431486076647367,
        "step": 197
    },
    {
        "loss": 2.0095,
        "grad_norm": 1.878953218460083,
        "learning_rate": 0.00018992862702441564,
        "epoch": 0.05459057071960298,
        "step": 198
    },
    {
        "loss": 1.6161,
        "grad_norm": 1.5948686599731445,
        "learning_rate": 0.00018981388218263995,
        "epoch": 0.05486628067273228,
        "step": 199
    },
    {
        "loss": 2.517,
        "grad_norm": 1.2776739597320557,
        "learning_rate": 0.0001896985224499737,
        "epoch": 0.055141990625861594,
        "step": 200
    },
    {
        "loss": 2.5529,
        "grad_norm": 1.5284096002578735,
        "learning_rate": 0.00018958254861620175,
        "epoch": 0.055417700578990904,
        "step": 201
    },
    {
        "loss": 2.5717,
        "grad_norm": 1.106658935546875,
        "learning_rate": 0.00018946596147531336,
        "epoch": 0.05569341053212021,
        "step": 202
    },
    {
        "loss": 2.5439,
        "grad_norm": 1.3587669134140015,
        "learning_rate": 0.0001893487618254966,
        "epoch": 0.05596912048524952,
        "step": 203
    },
    {
        "loss": 2.3346,
        "grad_norm": 1.8430814743041992,
        "learning_rate": 0.000189230950469133,
        "epoch": 0.05624483043837883,
        "step": 204
    },
    {
        "loss": 2.3402,
        "grad_norm": 1.5158708095550537,
        "learning_rate": 0.0001891125282127919,
        "epoch": 0.05652054039150813,
        "step": 205
    },
    {
        "loss": 1.7067,
        "grad_norm": 1.149067759513855,
        "learning_rate": 0.0001889934958672252,
        "epoch": 0.05679625034463744,
        "step": 206
    },
    {
        "loss": 2.776,
        "grad_norm": 1.1150367259979248,
        "learning_rate": 0.00018887385424736143,
        "epoch": 0.05707196029776675,
        "step": 207
    },
    {
        "loss": 2.3121,
        "grad_norm": 1.256217360496521,
        "learning_rate": 0.00018875360417230054,
        "epoch": 0.05734767025089606,
        "step": 208
    },
    {
        "loss": 1.8121,
        "grad_norm": 2.3480279445648193,
        "learning_rate": 0.0001886327464653081,
        "epoch": 0.05762338020402537,
        "step": 209
    },
    {
        "loss": 2.7117,
        "grad_norm": 1.3030025959014893,
        "learning_rate": 0.0001885112819538097,
        "epoch": 0.05789909015715467,
        "step": 210
    },
    {
        "loss": 2.2567,
        "grad_norm": 1.355650544166565,
        "learning_rate": 0.00018838921146938523,
        "epoch": 0.05817480011028398,
        "step": 211
    },
    {
        "loss": 2.4067,
        "grad_norm": 1.2881624698638916,
        "learning_rate": 0.00018826653584776327,
        "epoch": 0.05845051006341329,
        "step": 212
    },
    {
        "loss": 2.2134,
        "grad_norm": 1.0906777381896973,
        "learning_rate": 0.00018814325592881538,
        "epoch": 0.058726220016542596,
        "step": 213
    },
    {
        "loss": 2.5411,
        "grad_norm": 1.0721584558486938,
        "learning_rate": 0.0001880193725565503,
        "epoch": 0.05900192996967191,
        "step": 214
    },
    {
        "loss": 2.347,
        "grad_norm": 0.9380256533622742,
        "learning_rate": 0.00018789488657910806,
        "epoch": 0.05927763992280121,
        "step": 215
    },
    {
        "loss": 2.2378,
        "grad_norm": 1.26317298412323,
        "learning_rate": 0.00018776979884875446,
        "epoch": 0.05955334987593052,
        "step": 216
    },
    {
        "loss": 2.4887,
        "grad_norm": 1.3452935218811035,
        "learning_rate": 0.00018764411022187497,
        "epoch": 0.05982905982905983,
        "step": 217
    },
    {
        "loss": 2.4141,
        "grad_norm": 1.689286470413208,
        "learning_rate": 0.00018751782155896897,
        "epoch": 0.060104769782189135,
        "step": 218
    },
    {
        "loss": 1.917,
        "grad_norm": 1.9409857988357544,
        "learning_rate": 0.00018739093372464397,
        "epoch": 0.060380479735318446,
        "step": 219
    },
    {
        "loss": 2.4881,
        "grad_norm": 1.5810145139694214,
        "learning_rate": 0.00018726344758760935,
        "epoch": 0.060656189688447756,
        "step": 220
    },
    {
        "loss": 2.3941,
        "grad_norm": 1.131204605102539,
        "learning_rate": 0.00018713536402067081,
        "epoch": 0.06093189964157706,
        "step": 221
    },
    {
        "loss": 2.4155,
        "grad_norm": 1.9746642112731934,
        "learning_rate": 0.0001870066839007242,
        "epoch": 0.06120760959470637,
        "step": 222
    },
    {
        "loss": 2.0194,
        "grad_norm": 2.167741298675537,
        "learning_rate": 0.00018687740810874952,
        "epoch": 0.061483319547835674,
        "step": 223
    },
    {
        "loss": 2.9143,
        "grad_norm": 1.3530899286270142,
        "learning_rate": 0.0001867475375298049,
        "epoch": 0.061759029500964985,
        "step": 224
    },
    {
        "loss": 2.3518,
        "grad_norm": 1.4049549102783203,
        "learning_rate": 0.0001866170730530205,
        "epoch": 0.062034739454094295,
        "step": 225
    },
    {
        "loss": 2.4261,
        "grad_norm": 1.2607313394546509,
        "learning_rate": 0.00018648601557159258,
        "epoch": 0.0623104494072236,
        "step": 226
    },
    {
        "loss": 2.4573,
        "grad_norm": 1.5032219886779785,
        "learning_rate": 0.0001863543659827772,
        "epoch": 0.0625861593603529,
        "step": 227
    },
    {
        "loss": 2.031,
        "grad_norm": 1.9953371286392212,
        "learning_rate": 0.00018622212518788417,
        "epoch": 0.06286186931348221,
        "step": 228
    },
    {
        "loss": 2.3372,
        "grad_norm": 1.4585611820220947,
        "learning_rate": 0.0001860892940922708,
        "epoch": 0.06313757926661152,
        "step": 229
    },
    {
        "loss": 1.9895,
        "grad_norm": 1.6047272682189941,
        "learning_rate": 0.00018595587360533596,
        "epoch": 0.06341328921974083,
        "step": 230
    },
    {
        "loss": 2.159,
        "grad_norm": 1.7062424421310425,
        "learning_rate": 0.00018582186464051337,
        "epoch": 0.06368899917287014,
        "step": 231
    },
    {
        "loss": 2.4488,
        "grad_norm": 1.2503068447113037,
        "learning_rate": 0.00018568726811526586,
        "epoch": 0.06396470912599946,
        "step": 232
    },
    {
        "loss": 2.4479,
        "grad_norm": 1.5022766590118408,
        "learning_rate": 0.0001855520849510788,
        "epoch": 0.06424041907912875,
        "step": 233
    },
    {
        "loss": 2.1482,
        "grad_norm": 1.226593017578125,
        "learning_rate": 0.0001854163160734538,
        "epoch": 0.06451612903225806,
        "step": 234
    },
    {
        "loss": 2.4254,
        "grad_norm": 1.2688546180725098,
        "learning_rate": 0.00018527996241190248,
        "epoch": 0.06479183898538737,
        "step": 235
    },
    {
        "loss": 2.1277,
        "grad_norm": 1.2082542181015015,
        "learning_rate": 0.0001851430248999401,
        "epoch": 0.06506754893851668,
        "step": 236
    },
    {
        "loss": 2.6011,
        "grad_norm": 2.276604413986206,
        "learning_rate": 0.00018500550447507895,
        "epoch": 0.065343258891646,
        "step": 237
    },
    {
        "loss": 2.1807,
        "grad_norm": 2.0217764377593994,
        "learning_rate": 0.0001848674020788223,
        "epoch": 0.06561896884477529,
        "step": 238
    },
    {
        "loss": 2.4846,
        "grad_norm": 1.5883897542953491,
        "learning_rate": 0.00018472871865665767,
        "epoch": 0.0658946787979046,
        "step": 239
    },
    {
        "loss": 2.0747,
        "grad_norm": 1.249985933303833,
        "learning_rate": 0.00018458945515805044,
        "epoch": 0.06617038875103391,
        "step": 240
    },
    {
        "loss": 2.2129,
        "grad_norm": 1.2757898569107056,
        "learning_rate": 0.00018444961253643737,
        "epoch": 0.06644609870416322,
        "step": 241
    },
    {
        "loss": 1.8942,
        "grad_norm": 1.8968896865844727,
        "learning_rate": 0.00018430919174922006,
        "epoch": 0.06672180865729253,
        "step": 242
    },
    {
        "loss": 2.3992,
        "grad_norm": 1.4474084377288818,
        "learning_rate": 0.0001841681937577584,
        "epoch": 0.06699751861042183,
        "step": 243
    },
    {
        "loss": 2.2001,
        "grad_norm": 1.021498441696167,
        "learning_rate": 0.00018402661952736393,
        "epoch": 0.06727322856355114,
        "step": 244
    },
    {
        "loss": 2.035,
        "grad_norm": 1.2584826946258545,
        "learning_rate": 0.0001838844700272934,
        "epoch": 0.06754893851668045,
        "step": 245
    },
    {
        "loss": 2.1264,
        "grad_norm": 1.5676040649414062,
        "learning_rate": 0.00018374174623074182,
        "epoch": 0.06782464846980976,
        "step": 246
    },
    {
        "loss": 2.0112,
        "grad_norm": 1.6229571104049683,
        "learning_rate": 0.00018359844911483624,
        "epoch": 0.06810035842293907,
        "step": 247
    },
    {
        "loss": 1.8397,
        "grad_norm": 1.8978078365325928,
        "learning_rate": 0.00018345457966062857,
        "epoch": 0.06837606837606838,
        "step": 248
    },
    {
        "loss": 2.5085,
        "grad_norm": 1.5195709466934204,
        "learning_rate": 0.00018331013885308935,
        "epoch": 0.06865177832919768,
        "step": 249
    },
    {
        "loss": 2.1204,
        "grad_norm": 1.2087724208831787,
        "learning_rate": 0.00018316512768110055,
        "epoch": 0.06892748828232699,
        "step": 250
    },
    {
        "loss": 2.4355,
        "grad_norm": 1.0748069286346436,
        "learning_rate": 0.00018301954713744912,
        "epoch": 0.0692031982354563,
        "step": 251
    },
    {
        "loss": 2.2635,
        "grad_norm": 1.5121744871139526,
        "learning_rate": 0.00018287339821882007,
        "epoch": 0.06947890818858561,
        "step": 252
    },
    {
        "loss": 2.6008,
        "grad_norm": 1.3835989236831665,
        "learning_rate": 0.0001827266819257897,
        "epoch": 0.06975461814171492,
        "step": 253
    },
    {
        "loss": 2.0027,
        "grad_norm": 1.6619468927383423,
        "learning_rate": 0.00018257939926281865,
        "epoch": 0.07003032809484422,
        "step": 254
    },
    {
        "loss": 1.9003,
        "grad_norm": 1.0720338821411133,
        "learning_rate": 0.00018243155123824512,
        "epoch": 0.07030603804797353,
        "step": 255
    },
    {
        "loss": 2.4585,
        "grad_norm": 1.5599452257156372,
        "learning_rate": 0.00018228313886427792,
        "epoch": 0.07058174800110284,
        "step": 256
    },
    {
        "loss": 2.4449,
        "grad_norm": 0.9725430011749268,
        "learning_rate": 0.00018213416315698955,
        "epoch": 0.07085745795423215,
        "step": 257
    },
    {
        "loss": 2.4075,
        "grad_norm": 1.1093553304672241,
        "learning_rate": 0.00018198462513630928,
        "epoch": 0.07113316790736146,
        "step": 258
    },
    {
        "loss": 2.478,
        "grad_norm": 1.141563892364502,
        "learning_rate": 0.00018183452582601613,
        "epoch": 0.07140887786049076,
        "step": 259
    },
    {
        "loss": 2.2206,
        "grad_norm": 1.651060700416565,
        "learning_rate": 0.00018168386625373178,
        "epoch": 0.07168458781362007,
        "step": 260
    },
    {
        "loss": 2.2956,
        "grad_norm": 1.644399642944336,
        "learning_rate": 0.00018153264745091377,
        "epoch": 0.07196029776674938,
        "step": 261
    },
    {
        "loss": 1.8751,
        "grad_norm": 1.366426706314087,
        "learning_rate": 0.00018138087045284812,
        "epoch": 0.07223600771987869,
        "step": 262
    },
    {
        "loss": 1.7834,
        "grad_norm": 1.9019640684127808,
        "learning_rate": 0.00018122853629864261,
        "epoch": 0.072511717673008,
        "step": 263
    },
    {
        "loss": 2.6852,
        "grad_norm": 1.1557302474975586,
        "learning_rate": 0.0001810756460312192,
        "epoch": 0.07278742762613731,
        "step": 264
    },
    {
        "loss": 2.5351,
        "grad_norm": 1.2077122926712036,
        "learning_rate": 0.0001809222006973075,
        "epoch": 0.07306313757926661,
        "step": 265
    },
    {
        "loss": 2.1543,
        "grad_norm": 1.2387932538986206,
        "learning_rate": 0.0001807682013474369,
        "epoch": 0.07333884753239592,
        "step": 266
    },
    {
        "loss": 2.2063,
        "grad_norm": 1.1378819942474365,
        "learning_rate": 0.00018061364903592994,
        "epoch": 0.07361455748552523,
        "step": 267
    },
    {
        "loss": 2.5552,
        "grad_norm": 1.1895016431808472,
        "learning_rate": 0.00018045854482089495,
        "epoch": 0.07389026743865454,
        "step": 268
    },
    {
        "loss": 2.5669,
        "grad_norm": 1.0917057991027832,
        "learning_rate": 0.00018030288976421854,
        "epoch": 0.07416597739178385,
        "step": 269
    },
    {
        "loss": 1.8014,
        "grad_norm": 1.7232311964035034,
        "learning_rate": 0.00018014668493155874,
        "epoch": 0.07444168734491315,
        "step": 270
    },
    {
        "loss": 2.2112,
        "grad_norm": 1.7557618618011475,
        "learning_rate": 0.00017998993139233736,
        "epoch": 0.07471739729804246,
        "step": 271
    },
    {
        "loss": 2.4178,
        "grad_norm": 1.4249415397644043,
        "learning_rate": 0.00017983263021973281,
        "epoch": 0.07499310725117177,
        "step": 272
    },
    {
        "loss": 2.2355,
        "grad_norm": 1.2125372886657715,
        "learning_rate": 0.00017967478249067287,
        "epoch": 0.07526881720430108,
        "step": 273
    },
    {
        "loss": 2.2265,
        "grad_norm": 1.8210912942886353,
        "learning_rate": 0.0001795163892858271,
        "epoch": 0.07554452715743039,
        "step": 274
    },
    {
        "loss": 1.9433,
        "grad_norm": 2.4376721382141113,
        "learning_rate": 0.00017935745168959957,
        "epoch": 0.07582023711055969,
        "step": 275
    },
    {
        "loss": 2.4339,
        "grad_norm": 0.9350355863571167,
        "learning_rate": 0.00017919797079012139,
        "epoch": 0.076095947063689,
        "step": 276
    },
    {
        "loss": 2.4413,
        "grad_norm": 1.122685432434082,
        "learning_rate": 0.00017903794767924324,
        "epoch": 0.0763716570168183,
        "step": 277
    },
    {
        "loss": 2.3,
        "grad_norm": 1.3255646228790283,
        "learning_rate": 0.00017887738345252806,
        "epoch": 0.07664736696994762,
        "step": 278
    },
    {
        "loss": 1.9472,
        "grad_norm": 1.740564227104187,
        "learning_rate": 0.0001787162792092433,
        "epoch": 0.07692307692307693,
        "step": 279
    },
    {
        "loss": 2.1014,
        "grad_norm": 1.2810148000717163,
        "learning_rate": 0.0001785546360523535,
        "epoch": 0.07719878687620624,
        "step": 280
    },
    {
        "loss": 1.8577,
        "grad_norm": 1.461991310119629,
        "learning_rate": 0.0001783924550885129,
        "epoch": 0.07747449682933553,
        "step": 281
    },
    {
        "loss": 2.5236,
        "grad_norm": 1.4253484010696411,
        "learning_rate": 0.00017822973742805752,
        "epoch": 0.07775020678246485,
        "step": 282
    },
    {
        "loss": 2.4818,
        "grad_norm": 1.7796956300735474,
        "learning_rate": 0.0001780664841849979,
        "epoch": 0.07802591673559416,
        "step": 283
    },
    {
        "loss": 2.2866,
        "grad_norm": 1.5204553604125977,
        "learning_rate": 0.00017790269647701128,
        "epoch": 0.07830162668872347,
        "step": 284
    },
    {
        "loss": 2.506,
        "grad_norm": 2.004685401916504,
        "learning_rate": 0.000177738375425434,
        "epoch": 0.07857733664185278,
        "step": 285
    },
    {
        "loss": 2.4215,
        "grad_norm": 1.669006109237671,
        "learning_rate": 0.00017757352215525378,
        "epoch": 0.07885304659498207,
        "step": 286
    },
    {
        "loss": 2.4332,
        "grad_norm": 1.6868199110031128,
        "learning_rate": 0.00017740813779510214,
        "epoch": 0.07912875654811138,
        "step": 287
    },
    {
        "loss": 2.0812,
        "grad_norm": 1.814703106880188,
        "learning_rate": 0.00017724222347724654,
        "epoch": 0.0794044665012407,
        "step": 288
    },
    {
        "loss": 2.2429,
        "grad_norm": 1.3698937892913818,
        "learning_rate": 0.00017707578033758264,
        "epoch": 0.07968017645437,
        "step": 289
    },
    {
        "loss": 1.5912,
        "grad_norm": 1.7720623016357422,
        "learning_rate": 0.00017690880951562666,
        "epoch": 0.07995588640749932,
        "step": 290
    },
    {
        "loss": 2.3529,
        "grad_norm": 1.7114049196243286,
        "learning_rate": 0.00017674131215450736,
        "epoch": 0.08023159636062861,
        "step": 291
    },
    {
        "loss": 2.458,
        "grad_norm": 1.2423263788223267,
        "learning_rate": 0.00017657328940095843,
        "epoch": 0.08050730631375792,
        "step": 292
    },
    {
        "loss": 2.0532,
        "grad_norm": 1.5577954053878784,
        "learning_rate": 0.0001764047424053105,
        "epoch": 0.08078301626688723,
        "step": 293
    },
    {
        "loss": 2.509,
        "grad_norm": 1.5834494829177856,
        "learning_rate": 0.00017623567232148335,
        "epoch": 0.08105872622001654,
        "step": 294
    },
    {
        "loss": 2.2584,
        "grad_norm": 1.819991946220398,
        "learning_rate": 0.0001760660803069779,
        "epoch": 0.08133443617314585,
        "step": 295
    },
    {
        "loss": 2.4381,
        "grad_norm": 1.1121336221694946,
        "learning_rate": 0.00017589596752286838,
        "epoch": 0.08161014612627517,
        "step": 296
    },
    {
        "loss": 2.3849,
        "grad_norm": 1.2841148376464844,
        "learning_rate": 0.0001757253351337944,
        "epoch": 0.08188585607940446,
        "step": 297
    },
    {
        "loss": 2.4476,
        "grad_norm": 1.577321171760559,
        "learning_rate": 0.00017555418430795284,
        "epoch": 0.08216156603253377,
        "step": 298
    },
    {
        "loss": 1.8876,
        "grad_norm": 1.6479686498641968,
        "learning_rate": 0.00017538251621709007,
        "epoch": 0.08243727598566308,
        "step": 299
    },
    {
        "loss": 2.1227,
        "grad_norm": 1.4622124433517456,
        "learning_rate": 0.00017521033203649365,
        "epoch": 0.0827129859387924,
        "step": 300
    },
    {
        "loss": 2.1446,
        "grad_norm": 1.8412861824035645,
        "learning_rate": 0.00017503763294498458,
        "epoch": 0.0829886958919217,
        "step": 301
    },
    {
        "loss": 2.4815,
        "grad_norm": 1.3638392686843872,
        "learning_rate": 0.00017486442012490893,
        "epoch": 0.083264405845051,
        "step": 302
    },
    {
        "loss": 2.3316,
        "grad_norm": 1.1951076984405518,
        "learning_rate": 0.00017469069476213007,
        "epoch": 0.08354011579818031,
        "step": 303
    },
    {
        "loss": 2.3893,
        "grad_norm": 0.9913063049316406,
        "learning_rate": 0.00017451645804602024,
        "epoch": 0.08381582575130962,
        "step": 304
    },
    {
        "loss": 2.4997,
        "grad_norm": 1.5149322748184204,
        "learning_rate": 0.00017434171116945262,
        "epoch": 0.08409153570443893,
        "step": 305
    },
    {
        "loss": 1.5515,
        "grad_norm": 2.1066839694976807,
        "learning_rate": 0.00017416645532879305,
        "epoch": 0.08436724565756824,
        "step": 306
    },
    {
        "loss": 2.0233,
        "grad_norm": 1.8149679899215698,
        "learning_rate": 0.00017399069172389196,
        "epoch": 0.08464295561069754,
        "step": 307
    },
    {
        "loss": 2.0317,
        "grad_norm": 1.8816503286361694,
        "learning_rate": 0.00017381442155807594,
        "epoch": 0.08491866556382685,
        "step": 308
    },
    {
        "loss": 2.3135,
        "grad_norm": 1.365962266921997,
        "learning_rate": 0.00017363764603813974,
        "epoch": 0.08519437551695616,
        "step": 309
    },
    {
        "loss": 2.1477,
        "grad_norm": 1.414953351020813,
        "learning_rate": 0.00017346036637433794,
        "epoch": 0.08547008547008547,
        "step": 310
    },
    {
        "loss": 2.0774,
        "grad_norm": 1.8325036764144897,
        "learning_rate": 0.0001732825837803765,
        "epoch": 0.08574579542321478,
        "step": 311
    },
    {
        "loss": 2.3804,
        "grad_norm": 1.3696835041046143,
        "learning_rate": 0.00017310429947340464,
        "epoch": 0.08602150537634409,
        "step": 312
    },
    {
        "loss": 2.4124,
        "grad_norm": 1.1676216125488281,
        "learning_rate": 0.00017292551467400653,
        "epoch": 0.08629721532947339,
        "step": 313
    },
    {
        "loss": 1.4475,
        "grad_norm": 1.8962688446044922,
        "learning_rate": 0.00017274623060619274,
        "epoch": 0.0865729252826027,
        "step": 314
    },
    {
        "loss": 1.8151,
        "grad_norm": 1.7954890727996826,
        "learning_rate": 0.00017256644849739198,
        "epoch": 0.08684863523573201,
        "step": 315
    },
    {
        "loss": 2.0811,
        "grad_norm": 1.9280569553375244,
        "learning_rate": 0.00017238616957844274,
        "epoch": 0.08712434518886132,
        "step": 316
    },
    {
        "loss": 2.248,
        "grad_norm": 1.1148133277893066,
        "learning_rate": 0.0001722053950835848,
        "epoch": 0.08740005514199063,
        "step": 317
    },
    {
        "loss": 2.6969,
        "grad_norm": 1.2424142360687256,
        "learning_rate": 0.00017202412625045077,
        "epoch": 0.08767576509511993,
        "step": 318
    },
    {
        "loss": 2.3054,
        "grad_norm": 1.5861005783081055,
        "learning_rate": 0.00017184236432005764,
        "epoch": 0.08795147504824924,
        "step": 319
    },
    {
        "loss": 2.2812,
        "grad_norm": 1.7623094320297241,
        "learning_rate": 0.00017166011053679827,
        "epoch": 0.08822718500137855,
        "step": 320
    },
    {
        "loss": 2.3591,
        "grad_norm": 1.6724492311477661,
        "learning_rate": 0.00017147736614843296,
        "epoch": 0.08850289495450786,
        "step": 321
    },
    {
        "loss": 2.0526,
        "grad_norm": 1.8417617082595825,
        "learning_rate": 0.0001712941324060807,
        "epoch": 0.08877860490763717,
        "step": 322
    },
    {
        "loss": 2.3897,
        "grad_norm": 1.3529139757156372,
        "learning_rate": 0.0001711104105642108,
        "epoch": 0.08905431486076647,
        "step": 323
    },
    {
        "loss": 1.7793,
        "grad_norm": 1.3655792474746704,
        "learning_rate": 0.00017092620188063432,
        "epoch": 0.08933002481389578,
        "step": 324
    },
    {
        "loss": 2.0106,
        "grad_norm": 1.3406341075897217,
        "learning_rate": 0.00017074150761649522,
        "epoch": 0.08960573476702509,
        "step": 325
    },
    {
        "loss": 2.5218,
        "grad_norm": 1.368721842765808,
        "learning_rate": 0.00017055632903626195,
        "epoch": 0.0898814447201544,
        "step": 326
    },
    {
        "loss": 2.3568,
        "grad_norm": 1.55442214012146,
        "learning_rate": 0.00017037066740771878,
        "epoch": 0.09015715467328371,
        "step": 327
    },
    {
        "loss": 2.5249,
        "grad_norm": 1.562298059463501,
        "learning_rate": 0.00017018452400195693,
        "epoch": 0.09043286462641302,
        "step": 328
    },
    {
        "loss": 2.3298,
        "grad_norm": 1.666685700416565,
        "learning_rate": 0.00016999790009336615,
        "epoch": 0.09070857457954232,
        "step": 329
    },
    {
        "loss": 2.5115,
        "grad_norm": 1.5269628763198853,
        "learning_rate": 0.0001698107969596258,
        "epoch": 0.09098428453267163,
        "step": 330
    },
    {
        "loss": 1.7042,
        "grad_norm": 2.0381245613098145,
        "learning_rate": 0.000169623215881696,
        "epoch": 0.09125999448580094,
        "step": 331
    },
    {
        "loss": 1.9847,
        "grad_norm": 1.4029576778411865,
        "learning_rate": 0.00016943515814380923,
        "epoch": 0.09153570443893025,
        "step": 332
    },
    {
        "loss": 2.3425,
        "grad_norm": 1.715346336364746,
        "learning_rate": 0.0001692466250334612,
        "epoch": 0.09181141439205956,
        "step": 333
    },
    {
        "loss": 2.219,
        "grad_norm": 0.9989573359489441,
        "learning_rate": 0.0001690576178414022,
        "epoch": 0.09208712434518886,
        "step": 334
    },
    {
        "loss": 2.7573,
        "grad_norm": 1.9766536951065063,
        "learning_rate": 0.00016886813786162814,
        "epoch": 0.09236283429831817,
        "step": 335
    },
    {
        "loss": 2.1389,
        "grad_norm": 1.6470245122909546,
        "learning_rate": 0.00016867818639137183,
        "epoch": 0.09263854425144748,
        "step": 336
    },
    {
        "loss": 1.9067,
        "grad_norm": 1.43853759765625,
        "learning_rate": 0.00016848776473109406,
        "epoch": 0.09291425420457679,
        "step": 337
    },
    {
        "loss": 1.9815,
        "grad_norm": 1.2709368467330933,
        "learning_rate": 0.00016829687418447458,
        "epoch": 0.0931899641577061,
        "step": 338
    },
    {
        "loss": 2.1398,
        "grad_norm": 1.558163046836853,
        "learning_rate": 0.00016810551605840336,
        "epoch": 0.0934656741108354,
        "step": 339
    },
    {
        "loss": 2.221,
        "grad_norm": 1.2266744375228882,
        "learning_rate": 0.00016791369166297147,
        "epoch": 0.0937413840639647,
        "step": 340
    },
    {
        "loss": 2.2582,
        "grad_norm": 1.2954820394515991,
        "learning_rate": 0.0001677214023114623,
        "epoch": 0.09401709401709402,
        "step": 341
    },
    {
        "loss": 2.422,
        "grad_norm": 1.125685453414917,
        "learning_rate": 0.00016752864932034233,
        "epoch": 0.09429280397022333,
        "step": 342
    },
    {
        "loss": 2.4816,
        "grad_norm": 1.1546542644500732,
        "learning_rate": 0.0001673354340092523,
        "epoch": 0.09456851392335264,
        "step": 343
    },
    {
        "loss": 2.2139,
        "grad_norm": 0.8370608687400818,
        "learning_rate": 0.00016714175770099813,
        "epoch": 0.09484422387648195,
        "step": 344
    },
    {
        "loss": 2.1882,
        "grad_norm": 1.8404667377471924,
        "learning_rate": 0.00016694762172154187,
        "epoch": 0.09511993382961124,
        "step": 345
    },
    {
        "loss": 2.3487,
        "grad_norm": 1.555289626121521,
        "learning_rate": 0.0001667530273999926,
        "epoch": 0.09539564378274056,
        "step": 346
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.283319115638733,
        "learning_rate": 0.00016655797606859732,
        "epoch": 0.09567135373586987,
        "step": 347
    },
    {
        "loss": 2.1517,
        "grad_norm": 1.8396908044815063,
        "learning_rate": 0.00016636246906273186,
        "epoch": 0.09594706368899918,
        "step": 348
    },
    {
        "loss": 2.2384,
        "grad_norm": 1.777928352355957,
        "learning_rate": 0.00016616650772089165,
        "epoch": 0.09622277364212849,
        "step": 349
    },
    {
        "loss": 1.7827,
        "grad_norm": 1.9879367351531982,
        "learning_rate": 0.00016597009338468276,
        "epoch": 0.09649848359525778,
        "step": 350
    },
    {
        "loss": 2.5578,
        "grad_norm": 1.0118193626403809,
        "learning_rate": 0.00016577322739881254,
        "epoch": 0.0967741935483871,
        "step": 351
    },
    {
        "loss": 2.1636,
        "grad_norm": 1.5735969543457031,
        "learning_rate": 0.0001655759111110804,
        "epoch": 0.0970499035015164,
        "step": 352
    },
    {
        "loss": 2.4367,
        "grad_norm": 0.8227862119674683,
        "learning_rate": 0.00016537814587236872,
        "epoch": 0.09732561345464572,
        "step": 353
    },
    {
        "loss": 2.4276,
        "grad_norm": 1.5152044296264648,
        "learning_rate": 0.00016517993303663347,
        "epoch": 0.09760132340777503,
        "step": 354
    },
    {
        "loss": 2.4686,
        "grad_norm": 0.9479897618293762,
        "learning_rate": 0.000164981273960895,
        "epoch": 0.09787703336090432,
        "step": 355
    },
    {
        "loss": 2.4706,
        "grad_norm": 1.7978143692016602,
        "learning_rate": 0.00016478217000522882,
        "epoch": 0.09815274331403363,
        "step": 356
    },
    {
        "loss": 2.1235,
        "grad_norm": 1.85631263256073,
        "learning_rate": 0.00016458262253275606,
        "epoch": 0.09842845326716294,
        "step": 357
    },
    {
        "loss": 2.4805,
        "grad_norm": 0.9251836538314819,
        "learning_rate": 0.00016438263290963448,
        "epoch": 0.09870416322029225,
        "step": 358
    },
    {
        "loss": 2.1681,
        "grad_norm": 1.630360722541809,
        "learning_rate": 0.0001641822025050487,
        "epoch": 0.09897987317342156,
        "step": 359
    },
    {
        "loss": 2.0114,
        "grad_norm": 1.3308073282241821,
        "learning_rate": 0.0001639813326912013,
        "epoch": 0.09925558312655088,
        "step": 360
    },
    {
        "loss": 2.2829,
        "grad_norm": 1.9967916011810303,
        "learning_rate": 0.000163780024843303,
        "epoch": 0.09953129307968017,
        "step": 361
    },
    {
        "loss": 2.462,
        "grad_norm": 1.1905124187469482,
        "learning_rate": 0.0001635782803395635,
        "epoch": 0.09980700303280948,
        "step": 362
    },
    {
        "loss": 2.379,
        "grad_norm": 1.5679857730865479,
        "learning_rate": 0.0001633761005611819,
        "epoch": 0.1000827129859388,
        "step": 363
    },
    {
        "loss": 2.3159,
        "grad_norm": 1.1461234092712402,
        "learning_rate": 0.00016317348689233742,
        "epoch": 0.1003584229390681,
        "step": 364
    },
    {
        "loss": 2.6016,
        "grad_norm": 1.1139557361602783,
        "learning_rate": 0.00016297044072017972,
        "epoch": 0.10063413289219741,
        "step": 365
    },
    {
        "loss": 2.6237,
        "grad_norm": 1.162712812423706,
        "learning_rate": 0.00016276696343481956,
        "epoch": 0.10090984284532671,
        "step": 366
    },
    {
        "loss": 2.6112,
        "grad_norm": 0.9585290551185608,
        "learning_rate": 0.0001625630564293192,
        "epoch": 0.10118555279845602,
        "step": 367
    },
    {
        "loss": 2.0041,
        "grad_norm": 1.247841477394104,
        "learning_rate": 0.00016235872109968282,
        "epoch": 0.10146126275158533,
        "step": 368
    },
    {
        "loss": 2.3428,
        "grad_norm": 1.2615183591842651,
        "learning_rate": 0.0001621539588448471,
        "epoch": 0.10173697270471464,
        "step": 369
    },
    {
        "loss": 2.3038,
        "grad_norm": 1.365747332572937,
        "learning_rate": 0.00016194877106667164,
        "epoch": 0.10201268265784395,
        "step": 370
    },
    {
        "loss": 1.6222,
        "grad_norm": 1.6903187036514282,
        "learning_rate": 0.00016174315916992914,
        "epoch": 0.10228839261097325,
        "step": 371
    },
    {
        "loss": 2.3322,
        "grad_norm": 1.6368571519851685,
        "learning_rate": 0.00016153712456229595,
        "epoch": 0.10256410256410256,
        "step": 372
    },
    {
        "loss": 2.0705,
        "grad_norm": 1.2481244802474976,
        "learning_rate": 0.00016133066865434254,
        "epoch": 0.10283981251723187,
        "step": 373
    },
    {
        "loss": 2.0779,
        "grad_norm": 1.7404509782791138,
        "learning_rate": 0.00016112379285952364,
        "epoch": 0.10311552247036118,
        "step": 374
    },
    {
        "loss": 2.595,
        "grad_norm": 1.4635963439941406,
        "learning_rate": 0.00016091649859416863,
        "epoch": 0.10339123242349049,
        "step": 375
    },
    {
        "loss": 2.5575,
        "grad_norm": 1.3442001342773438,
        "learning_rate": 0.00016070878727747189,
        "epoch": 0.1036669423766198,
        "step": 376
    },
    {
        "loss": 2.2734,
        "grad_norm": 1.7135885953903198,
        "learning_rate": 0.00016050066033148296,
        "epoch": 0.1039426523297491,
        "step": 377
    },
    {
        "loss": 2.6626,
        "grad_norm": 1.7438236474990845,
        "learning_rate": 0.0001602921191810971,
        "epoch": 0.10421836228287841,
        "step": 378
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.1326690912246704,
        "learning_rate": 0.00016008316525404509,
        "epoch": 0.10449407223600772,
        "step": 379
    },
    {
        "loss": 2.5794,
        "grad_norm": 1.3759995698928833,
        "learning_rate": 0.00015987379998088383,
        "epoch": 0.10476978218913703,
        "step": 380
    },
    {
        "loss": 2.0098,
        "grad_norm": 2.321446180343628,
        "learning_rate": 0.0001596640247949864,
        "epoch": 0.10504549214226634,
        "step": 381
    },
    {
        "loss": 2.2971,
        "grad_norm": 1.127435326576233,
        "learning_rate": 0.00015945384113253224,
        "epoch": 0.10532120209539564,
        "step": 382
    },
    {
        "loss": 2.3978,
        "grad_norm": 1.2368663549423218,
        "learning_rate": 0.0001592432504324973,
        "epoch": 0.10559691204852495,
        "step": 383
    },
    {
        "loss": 2.3482,
        "grad_norm": 1.0211026668548584,
        "learning_rate": 0.0001590322541366443,
        "epoch": 0.10587262200165426,
        "step": 384
    },
    {
        "loss": 2.1192,
        "grad_norm": 1.8274621963500977,
        "learning_rate": 0.0001588208536895127,
        "epoch": 0.10614833195478357,
        "step": 385
    },
    {
        "loss": 1.9679,
        "grad_norm": 1.520654559135437,
        "learning_rate": 0.00015860905053840895,
        "epoch": 0.10642404190791288,
        "step": 386
    },
    {
        "loss": 2.0476,
        "grad_norm": 1.3981512784957886,
        "learning_rate": 0.0001583968461333965,
        "epoch": 0.10669975186104218,
        "step": 387
    },
    {
        "loss": 1.9871,
        "grad_norm": 1.164318561553955,
        "learning_rate": 0.0001581842419272859,
        "epoch": 0.10697546181417149,
        "step": 388
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.198137879371643,
        "learning_rate": 0.00015797123937562484,
        "epoch": 0.1072511717673008,
        "step": 389
    },
    {
        "loss": 2.2264,
        "grad_norm": 1.7367740869522095,
        "learning_rate": 0.00015775783993668822,
        "epoch": 0.10752688172043011,
        "step": 390
    },
    {
        "loss": 1.8505,
        "grad_norm": 1.3298618793487549,
        "learning_rate": 0.0001575440450714681,
        "epoch": 0.10780259167355942,
        "step": 391
    },
    {
        "loss": 2.3722,
        "grad_norm": 0.9825763702392578,
        "learning_rate": 0.0001573298562436638,
        "epoch": 0.10807830162668873,
        "step": 392
    },
    {
        "loss": 2.4669,
        "grad_norm": 1.334357500076294,
        "learning_rate": 0.00015711527491967176,
        "epoch": 0.10835401157981803,
        "step": 393
    },
    {
        "loss": 2.2847,
        "grad_norm": 1.3306471109390259,
        "learning_rate": 0.00015690030256857557,
        "epoch": 0.10862972153294734,
        "step": 394
    },
    {
        "loss": 2.1975,
        "grad_norm": 1.2533901929855347,
        "learning_rate": 0.00015668494066213597,
        "epoch": 0.10890543148607665,
        "step": 395
    },
    {
        "loss": 1.7262,
        "grad_norm": 1.9452894926071167,
        "learning_rate": 0.00015646919067478055,
        "epoch": 0.10918114143920596,
        "step": 396
    },
    {
        "loss": 2.53,
        "grad_norm": 0.8934651613235474,
        "learning_rate": 0.00015625305408359402,
        "epoch": 0.10945685139233527,
        "step": 397
    },
    {
        "loss": 2.0778,
        "grad_norm": 1.6716794967651367,
        "learning_rate": 0.00015603653236830766,
        "epoch": 0.10973256134546457,
        "step": 398
    },
    {
        "loss": 2.3738,
        "grad_norm": 1.0507675409317017,
        "learning_rate": 0.00015581962701128962,
        "epoch": 0.11000827129859388,
        "step": 399
    },
    {
        "loss": 2.6558,
        "grad_norm": 1.2459735870361328,
        "learning_rate": 0.00015560233949753443,
        "epoch": 0.11028398125172319,
        "step": 400
    },
    {
        "loss": 2.2139,
        "grad_norm": 1.3133790493011475,
        "learning_rate": 0.00015538467131465298,
        "epoch": 0.1105596912048525,
        "step": 401
    },
    {
        "loss": 2.1289,
        "grad_norm": 1.5541741847991943,
        "learning_rate": 0.0001551666239528625,
        "epoch": 0.11083540115798181,
        "step": 402
    },
    {
        "loss": 1.9559,
        "grad_norm": 1.847042441368103,
        "learning_rate": 0.00015494819890497593,
        "epoch": 0.1111111111111111,
        "step": 403
    },
    {
        "loss": 2.1838,
        "grad_norm": 1.961074948310852,
        "learning_rate": 0.00015472939766639213,
        "epoch": 0.11138682106424042,
        "step": 404
    },
    {
        "loss": 2.0304,
        "grad_norm": 2.039401054382324,
        "learning_rate": 0.00015451022173508538,
        "epoch": 0.11166253101736973,
        "step": 405
    },
    {
        "loss": 2.074,
        "grad_norm": 1.4388313293457031,
        "learning_rate": 0.00015429067261159531,
        "epoch": 0.11193824097049904,
        "step": 406
    },
    {
        "loss": 2.4805,
        "grad_norm": 1.431280493736267,
        "learning_rate": 0.00015407075179901637,
        "epoch": 0.11221395092362835,
        "step": 407
    },
    {
        "loss": 2.446,
        "grad_norm": 1.7574623823165894,
        "learning_rate": 0.00015385046080298786,
        "epoch": 0.11248966087675766,
        "step": 408
    },
    {
        "loss": 2.4155,
        "grad_norm": 1.6261725425720215,
        "learning_rate": 0.00015362980113168336,
        "epoch": 0.11276537082988695,
        "step": 409
    },
    {
        "loss": 1.6704,
        "grad_norm": 1.6396002769470215,
        "learning_rate": 0.00015340877429580052,
        "epoch": 0.11304108078301627,
        "step": 410
    },
    {
        "loss": 2.4433,
        "grad_norm": 1.390641450881958,
        "learning_rate": 0.00015318738180855073,
        "epoch": 0.11331679073614558,
        "step": 411
    },
    {
        "loss": 2.2543,
        "grad_norm": 1.8140870332717896,
        "learning_rate": 0.00015296562518564874,
        "epoch": 0.11359250068927489,
        "step": 412
    },
    {
        "loss": 2.4783,
        "grad_norm": 1.441713571548462,
        "learning_rate": 0.00015274350594530217,
        "epoch": 0.1138682106424042,
        "step": 413
    },
    {
        "loss": 1.8236,
        "grad_norm": 1.5193418264389038,
        "learning_rate": 0.00015252102560820138,
        "epoch": 0.1141439205955335,
        "step": 414
    },
    {
        "loss": 2.5412,
        "grad_norm": 1.0098646879196167,
        "learning_rate": 0.00015229818569750879,
        "epoch": 0.1144196305486628,
        "step": 415
    },
    {
        "loss": 2.3771,
        "grad_norm": 1.046494722366333,
        "learning_rate": 0.0001520749877388486,
        "epoch": 0.11469534050179211,
        "step": 416
    },
    {
        "loss": 2.181,
        "grad_norm": 1.8487213850021362,
        "learning_rate": 0.00015185143326029623,
        "epoch": 0.11497105045492143,
        "step": 417
    },
    {
        "loss": 2.2072,
        "grad_norm": 1.0108013153076172,
        "learning_rate": 0.00015162752379236808,
        "epoch": 0.11524676040805074,
        "step": 418
    },
    {
        "loss": 2.1642,
        "grad_norm": 1.166123628616333,
        "learning_rate": 0.0001514032608680108,
        "epoch": 0.11552247036118003,
        "step": 419
    },
    {
        "loss": 1.9293,
        "grad_norm": 1.7515805959701538,
        "learning_rate": 0.00015117864602259094,
        "epoch": 0.11579818031430934,
        "step": 420
    },
    {
        "loss": 2.2422,
        "grad_norm": 1.1194077730178833,
        "learning_rate": 0.00015095368079388432,
        "epoch": 0.11607389026743865,
        "step": 421
    },
    {
        "loss": 2.6965,
        "grad_norm": 1.0270894765853882,
        "learning_rate": 0.00015072836672206573,
        "epoch": 0.11634960022056796,
        "step": 422
    },
    {
        "loss": 1.8853,
        "grad_norm": 2.180419683456421,
        "learning_rate": 0.0001505027053496981,
        "epoch": 0.11662531017369727,
        "step": 423
    },
    {
        "loss": 2.293,
        "grad_norm": 1.8398408889770508,
        "learning_rate": 0.00015027669822172222,
        "epoch": 0.11690102012682659,
        "step": 424
    },
    {
        "loss": 2.3421,
        "grad_norm": 1.0052410364151,
        "learning_rate": 0.00015005034688544578,
        "epoch": 0.11717673007995588,
        "step": 425
    },
    {
        "loss": 2.2035,
        "grad_norm": 1.4356709718704224,
        "learning_rate": 0.00014982365289053332,
        "epoch": 0.11745244003308519,
        "step": 426
    },
    {
        "loss": 2.5068,
        "grad_norm": 0.9055518507957458,
        "learning_rate": 0.0001495966177889951,
        "epoch": 0.1177281499862145,
        "step": 427
    },
    {
        "loss": 1.905,
        "grad_norm": 2.096553087234497,
        "learning_rate": 0.0001493692431351767,
        "epoch": 0.11800385993934381,
        "step": 428
    },
    {
        "loss": 2.6628,
        "grad_norm": 1.132924199104309,
        "learning_rate": 0.00014914153048574852,
        "epoch": 0.11827956989247312,
        "step": 429
    },
    {
        "loss": 2.4672,
        "grad_norm": 1.3897837400436401,
        "learning_rate": 0.00014891348139969485,
        "epoch": 0.11855527984560242,
        "step": 430
    },
    {
        "loss": 2.31,
        "grad_norm": 1.3101991415023804,
        "learning_rate": 0.00014868509743830332,
        "epoch": 0.11883098979873173,
        "step": 431
    },
    {
        "loss": 2.2287,
        "grad_norm": 1.1429980993270874,
        "learning_rate": 0.00014845638016515423,
        "epoch": 0.11910669975186104,
        "step": 432
    },
    {
        "loss": 2.2901,
        "grad_norm": 1.4399045705795288,
        "learning_rate": 0.0001482273311461099,
        "epoch": 0.11938240970499035,
        "step": 433
    },
    {
        "loss": 2.4484,
        "grad_norm": 0.8591238260269165,
        "learning_rate": 0.00014799795194930373,
        "epoch": 0.11965811965811966,
        "step": 434
    },
    {
        "loss": 2.4911,
        "grad_norm": 1.148876667022705,
        "learning_rate": 0.0001477682441451297,
        "epoch": 0.11993382961124896,
        "step": 435
    },
    {
        "loss": 2.5053,
        "grad_norm": 1.374822974205017,
        "learning_rate": 0.00014753820930623156,
        "epoch": 0.12020953956437827,
        "step": 436
    },
    {
        "loss": 1.8602,
        "grad_norm": 1.8238717317581177,
        "learning_rate": 0.00014730784900749195,
        "epoch": 0.12048524951750758,
        "step": 437
    },
    {
        "loss": 2.2079,
        "grad_norm": 1.7037640810012817,
        "learning_rate": 0.00014707716482602172,
        "epoch": 0.12076095947063689,
        "step": 438
    },
    {
        "loss": 2.2091,
        "grad_norm": 1.0726059675216675,
        "learning_rate": 0.00014684615834114915,
        "epoch": 0.1210366694237662,
        "step": 439
    },
    {
        "loss": 2.3492,
        "grad_norm": 1.2839266061782837,
        "learning_rate": 0.00014661483113440906,
        "epoch": 0.12131237937689551,
        "step": 440
    },
    {
        "loss": 2.3962,
        "grad_norm": 0.9141573905944824,
        "learning_rate": 0.00014638318478953202,
        "epoch": 0.12158808933002481,
        "step": 441
    },
    {
        "loss": 2.2681,
        "grad_norm": 1.0929394960403442,
        "learning_rate": 0.00014615122089243357,
        "epoch": 0.12186379928315412,
        "step": 442
    },
    {
        "loss": 2.0809,
        "grad_norm": 1.0695180892944336,
        "learning_rate": 0.0001459189410312032,
        "epoch": 0.12213950923628343,
        "step": 443
    },
    {
        "loss": 2.4293,
        "grad_norm": 0.9223105311393738,
        "learning_rate": 0.0001456863467960937,
        "epoch": 0.12241521918941274,
        "step": 444
    },
    {
        "loss": 2.3147,
        "grad_norm": 0.9444586634635925,
        "learning_rate": 0.00014545343977951001,
        "epoch": 0.12269092914254205,
        "step": 445
    },
    {
        "loss": 2.1721,
        "grad_norm": 1.6262435913085938,
        "learning_rate": 0.0001452202215759986,
        "epoch": 0.12296663909567135,
        "step": 446
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.7730190753936768,
        "learning_rate": 0.00014498669378223627,
        "epoch": 0.12324234904880066,
        "step": 447
    },
    {
        "loss": 2.3854,
        "grad_norm": 1.8464651107788086,
        "learning_rate": 0.00014475285799701951,
        "epoch": 0.12351805900192997,
        "step": 448
    },
    {
        "loss": 1.3613,
        "grad_norm": 2.2969326972961426,
        "learning_rate": 0.0001445187158212533,
        "epoch": 0.12379376895505928,
        "step": 449
    },
    {
        "loss": 1.9348,
        "grad_norm": 1.7654637098312378,
        "learning_rate": 0.0001442842688579403,
        "epoch": 0.12406947890818859,
        "step": 450
    },
    {
        "loss": 2.3577,
        "grad_norm": 1.9821192026138306,
        "learning_rate": 0.0001440495187121698,
        "epoch": 0.12434518886131789,
        "step": 451
    },
    {
        "loss": 1.5448,
        "grad_norm": 1.4210937023162842,
        "learning_rate": 0.00014381446699110678,
        "epoch": 0.1246208988144472,
        "step": 452
    },
    {
        "loss": 1.6248,
        "grad_norm": 2.0986764430999756,
        "learning_rate": 0.00014357911530398095,
        "epoch": 0.12489660876757651,
        "step": 453
    },
    {
        "loss": 2.2671,
        "grad_norm": 1.4326900243759155,
        "learning_rate": 0.00014334346526207554,
        "epoch": 0.1251723187207058,
        "step": 454
    },
    {
        "loss": 2.3739,
        "grad_norm": 1.3218562602996826,
        "learning_rate": 0.00014310751847871651,
        "epoch": 0.12544802867383512,
        "step": 455
    },
    {
        "loss": 1.6718,
        "grad_norm": 2.052459716796875,
        "learning_rate": 0.0001428712765692613,
        "epoch": 0.12572373862696443,
        "step": 456
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.8528202772140503,
        "learning_rate": 0.00014263474115108797,
        "epoch": 0.12599944858009374,
        "step": 457
    },
    {
        "loss": 2.0739,
        "grad_norm": 1.8152825832366943,
        "learning_rate": 0.00014239791384358397,
        "epoch": 0.12627515853322305,
        "step": 458
    },
    {
        "loss": 2.3275,
        "grad_norm": 1.363817811012268,
        "learning_rate": 0.0001421607962681351,
        "epoch": 0.12655086848635236,
        "step": 459
    },
    {
        "loss": 2.4126,
        "grad_norm": 1.309911847114563,
        "learning_rate": 0.00014192339004811443,
        "epoch": 0.12682657843948167,
        "step": 460
    },
    {
        "loss": 2.5832,
        "grad_norm": 1.1728277206420898,
        "learning_rate": 0.00014168569680887114,
        "epoch": 0.12710228839261098,
        "step": 461
    },
    {
        "loss": 2.006,
        "grad_norm": 1.72767174243927,
        "learning_rate": 0.0001414477181777195,
        "epoch": 0.1273779983457403,
        "step": 462
    },
    {
        "loss": 2.5217,
        "grad_norm": 1.2708150148391724,
        "learning_rate": 0.00014120945578392755,
        "epoch": 0.1276537082988696,
        "step": 463
    },
    {
        "loss": 1.9549,
        "grad_norm": 2.0861546993255615,
        "learning_rate": 0.0001409709112587061,
        "epoch": 0.1279294182519989,
        "step": 464
    },
    {
        "loss": 2.6948,
        "grad_norm": 1.6486461162567139,
        "learning_rate": 0.00014073208623519758,
        "epoch": 0.1282051282051282,
        "step": 465
    },
    {
        "loss": 1.9356,
        "grad_norm": 1.7624568939208984,
        "learning_rate": 0.00014049298234846465,
        "epoch": 0.1284808381582575,
        "step": 466
    },
    {
        "loss": 2.0732,
        "grad_norm": 1.5899897813796997,
        "learning_rate": 0.00014025360123547925,
        "epoch": 0.12875654811138681,
        "step": 467
    },
    {
        "loss": 2.201,
        "grad_norm": 1.3659794330596924,
        "learning_rate": 0.00014001394453511123,
        "epoch": 0.12903225806451613,
        "step": 468
    },
    {
        "loss": 1.7916,
        "grad_norm": 1.575219988822937,
        "learning_rate": 0.00013977401388811723,
        "epoch": 0.12930796801764544,
        "step": 469
    },
    {
        "loss": 2.549,
        "grad_norm": 0.871155858039856,
        "learning_rate": 0.00013953381093712936,
        "epoch": 0.12958367797077475,
        "step": 470
    },
    {
        "loss": 2.6998,
        "grad_norm": 1.2074308395385742,
        "learning_rate": 0.00013929333732664404,
        "epoch": 0.12985938792390406,
        "step": 471
    },
    {
        "loss": 2.631,
        "grad_norm": 1.2282299995422363,
        "learning_rate": 0.0001390525947030107,
        "epoch": 0.13013509787703337,
        "step": 472
    },
    {
        "loss": 2.5106,
        "grad_norm": 1.1104397773742676,
        "learning_rate": 0.00013881158471442054,
        "epoch": 0.13041080783016268,
        "step": 473
    },
    {
        "loss": 1.8601,
        "grad_norm": 1.1229842901229858,
        "learning_rate": 0.00013857030901089505,
        "epoch": 0.130686517783292,
        "step": 474
    },
    {
        "loss": 2.2362,
        "grad_norm": 1.145316243171692,
        "learning_rate": 0.0001383287692442751,
        "epoch": 0.13096222773642127,
        "step": 475
    },
    {
        "loss": 2.2339,
        "grad_norm": 1.4310853481292725,
        "learning_rate": 0.00013808696706820928,
        "epoch": 0.13123793768955058,
        "step": 476
    },
    {
        "loss": 2.2233,
        "grad_norm": 1.4941015243530273,
        "learning_rate": 0.0001378449041381427,
        "epoch": 0.1315136476426799,
        "step": 477
    },
    {
        "loss": 2.2145,
        "grad_norm": 1.7756383419036865,
        "learning_rate": 0.0001376025821113058,
        "epoch": 0.1317893575958092,
        "step": 478
    },
    {
        "loss": 2.2901,
        "grad_norm": 1.2375530004501343,
        "learning_rate": 0.00013736000264670263,
        "epoch": 0.13206506754893851,
        "step": 479
    },
    {
        "loss": 2.3377,
        "grad_norm": 1.0295052528381348,
        "learning_rate": 0.00013711716740509998,
        "epoch": 0.13234077750206782,
        "step": 480
    },
    {
        "loss": 2.1102,
        "grad_norm": 1.438259243965149,
        "learning_rate": 0.00013687407804901562,
        "epoch": 0.13261648745519714,
        "step": 481
    },
    {
        "loss": 1.7641,
        "grad_norm": 1.6003072261810303,
        "learning_rate": 0.00013663073624270707,
        "epoch": 0.13289219740832645,
        "step": 482
    },
    {
        "loss": 2.3323,
        "grad_norm": 0.809545636177063,
        "learning_rate": 0.00013638714365216028,
        "epoch": 0.13316790736145576,
        "step": 483
    },
    {
        "loss": 1.9548,
        "grad_norm": 1.7554633617401123,
        "learning_rate": 0.000136143301945078,
        "epoch": 0.13344361731458507,
        "step": 484
    },
    {
        "loss": 2.5348,
        "grad_norm": 1.79541015625,
        "learning_rate": 0.0001358992127908686,
        "epoch": 0.13371932726771438,
        "step": 485
    },
    {
        "loss": 1.8896,
        "grad_norm": 2.0704526901245117,
        "learning_rate": 0.0001356548778606345,
        "epoch": 0.13399503722084366,
        "step": 486
    },
    {
        "loss": 1.2774,
        "grad_norm": 1.744102954864502,
        "learning_rate": 0.00013541029882716083,
        "epoch": 0.13427074717397297,
        "step": 487
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.563033938407898,
        "learning_rate": 0.00013516547736490384,
        "epoch": 0.13454645712710228,
        "step": 488
    },
    {
        "loss": 1.4511,
        "grad_norm": 1.9009149074554443,
        "learning_rate": 0.00013492041514997957,
        "epoch": 0.1348221670802316,
        "step": 489
    },
    {
        "loss": 2.541,
        "grad_norm": 1.2990902662277222,
        "learning_rate": 0.0001346751138601523,
        "epoch": 0.1350978770333609,
        "step": 490
    },
    {
        "loss": 2.0651,
        "grad_norm": 1.5097545385360718,
        "learning_rate": 0.0001344295751748231,
        "epoch": 0.1353735869864902,
        "step": 491
    },
    {
        "loss": 2.3251,
        "grad_norm": 2.187072992324829,
        "learning_rate": 0.00013418380077501827,
        "epoch": 0.13564929693961952,
        "step": 492
    },
    {
        "loss": 2.0701,
        "grad_norm": 1.6609642505645752,
        "learning_rate": 0.00013393779234337794,
        "epoch": 0.13592500689274883,
        "step": 493
    },
    {
        "loss": 2.2751,
        "grad_norm": 1.7167829275131226,
        "learning_rate": 0.00013369155156414442,
        "epoch": 0.13620071684587814,
        "step": 494
    },
    {
        "loss": 2.4442,
        "grad_norm": 1.4059691429138184,
        "learning_rate": 0.0001334450801231508,
        "epoch": 0.13647642679900746,
        "step": 495
    },
    {
        "loss": 2.2138,
        "grad_norm": 2.735422372817993,
        "learning_rate": 0.00013319837970780934,
        "epoch": 0.13675213675213677,
        "step": 496
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.4968448877334595,
        "learning_rate": 0.00013295145200709988,
        "epoch": 0.13702784670526605,
        "step": 497
    },
    {
        "loss": 1.9478,
        "grad_norm": 1.8160899877548218,
        "learning_rate": 0.00013270429871155826,
        "epoch": 0.13730355665839536,
        "step": 498
    },
    {
        "loss": 2.2476,
        "grad_norm": 0.8164904713630676,
        "learning_rate": 0.00013245692151326498,
        "epoch": 0.13757926661152467,
        "step": 499
    },
    {
        "loss": 2.2926,
        "grad_norm": 1.4977532625198364,
        "learning_rate": 0.00013220932210583335,
        "epoch": 0.13785497656465398,
        "step": 500
    },
    {
        "loss": 2.3611,
        "grad_norm": 1.5344080924987793,
        "learning_rate": 0.0001319615021843979,
        "epoch": 0.1381306865177833,
        "step": 501
    },
    {
        "loss": 2.139,
        "grad_norm": 1.2709156274795532,
        "learning_rate": 0.00013171346344560304,
        "epoch": 0.1384063964709126,
        "step": 502
    },
    {
        "loss": 2.5685,
        "grad_norm": 0.912238359451294,
        "learning_rate": 0.00013146520758759107,
        "epoch": 0.1386821064240419,
        "step": 503
    },
    {
        "loss": 2.3949,
        "grad_norm": 1.0555065870285034,
        "learning_rate": 0.0001312167363099909,
        "epoch": 0.13895781637717122,
        "step": 504
    },
    {
        "loss": 1.7344,
        "grad_norm": 1.8448126316070557,
        "learning_rate": 0.0001309680513139062,
        "epoch": 0.13923352633030053,
        "step": 505
    },
    {
        "loss": 1.9255,
        "grad_norm": 1.5998220443725586,
        "learning_rate": 0.00013071915430190378,
        "epoch": 0.13950923628342984,
        "step": 506
    },
    {
        "loss": 2.1819,
        "grad_norm": 1.3000123500823975,
        "learning_rate": 0.0001304700469780021,
        "epoch": 0.13978494623655913,
        "step": 507
    },
    {
        "loss": 2.0703,
        "grad_norm": 1.6809420585632324,
        "learning_rate": 0.0001302207310476593,
        "epoch": 0.14006065618968844,
        "step": 508
    },
    {
        "loss": 2.469,
        "grad_norm": 0.914182186126709,
        "learning_rate": 0.00012997120821776182,
        "epoch": 0.14033636614281775,
        "step": 509
    },
    {
        "loss": 2.5104,
        "grad_norm": 1.4862171411514282,
        "learning_rate": 0.00012972148019661255,
        "epoch": 0.14061207609594706,
        "step": 510
    },
    {
        "loss": 2.5076,
        "grad_norm": 1.0997505187988281,
        "learning_rate": 0.0001294715486939192,
        "epoch": 0.14088778604907637,
        "step": 511
    },
    {
        "loss": 2.2418,
        "grad_norm": 1.5590476989746094,
        "learning_rate": 0.00012922141542078255,
        "epoch": 0.14116349600220568,
        "step": 512
    },
    {
        "loss": 2.4376,
        "grad_norm": 1.3261420726776123,
        "learning_rate": 0.00012897108208968474,
        "epoch": 0.141439205955335,
        "step": 513
    },
    {
        "loss": 2.2786,
        "grad_norm": 2.082146167755127,
        "learning_rate": 0.0001287205504144776,
        "epoch": 0.1417149159084643,
        "step": 514
    },
    {
        "loss": 1.7304,
        "grad_norm": 2.010931968688965,
        "learning_rate": 0.00012846982211037086,
        "epoch": 0.1419906258615936,
        "step": 515
    },
    {
        "loss": 1.4257,
        "grad_norm": 1.7785388231277466,
        "learning_rate": 0.00012821889889392044,
        "epoch": 0.14226633581472292,
        "step": 516
    },
    {
        "loss": 2.4597,
        "grad_norm": 2.0848281383514404,
        "learning_rate": 0.00012796778248301666,
        "epoch": 0.14254204576785223,
        "step": 517
    },
    {
        "loss": 2.3256,
        "grad_norm": 1.1641209125518799,
        "learning_rate": 0.00012771647459687254,
        "epoch": 0.14281775572098152,
        "step": 518
    },
    {
        "loss": 2.3272,
        "grad_norm": 1.4754188060760498,
        "learning_rate": 0.0001274649769560119,
        "epoch": 0.14309346567411083,
        "step": 519
    },
    {
        "loss": 2.1893,
        "grad_norm": 1.9841257333755493,
        "learning_rate": 0.00012721329128225785,
        "epoch": 0.14336917562724014,
        "step": 520
    },
    {
        "loss": 1.7194,
        "grad_norm": 1.8926239013671875,
        "learning_rate": 0.00012696141929872064,
        "epoch": 0.14364488558036945,
        "step": 521
    },
    {
        "loss": 1.8935,
        "grad_norm": 1.838057279586792,
        "learning_rate": 0.00012670936272978614,
        "epoch": 0.14392059553349876,
        "step": 522
    },
    {
        "loss": 1.7945,
        "grad_norm": 1.8369617462158203,
        "learning_rate": 0.00012645712330110396,
        "epoch": 0.14419630548662807,
        "step": 523
    },
    {
        "loss": 2.4655,
        "grad_norm": 1.0938555002212524,
        "learning_rate": 0.00012620470273957558,
        "epoch": 0.14447201543975738,
        "step": 524
    },
    {
        "loss": 2.4572,
        "grad_norm": 1.12178373336792,
        "learning_rate": 0.00012595210277334255,
        "epoch": 0.1447477253928867,
        "step": 525
    },
    {
        "loss": 2.0965,
        "grad_norm": 1.6620533466339111,
        "learning_rate": 0.00012569932513177472,
        "epoch": 0.145023435346016,
        "step": 526
    },
    {
        "loss": 2.6075,
        "grad_norm": 1.392582893371582,
        "learning_rate": 0.00012544637154545836,
        "epoch": 0.1452991452991453,
        "step": 527
    },
    {
        "loss": 2.2655,
        "grad_norm": 1.604414463043213,
        "learning_rate": 0.00012519324374618419,
        "epoch": 0.14557485525227462,
        "step": 528
    },
    {
        "loss": 2.1238,
        "grad_norm": 1.6393455266952515,
        "learning_rate": 0.00012493994346693586,
        "epoch": 0.1458505652054039,
        "step": 529
    },
    {
        "loss": 1.7466,
        "grad_norm": 1.3979097604751587,
        "learning_rate": 0.00012468647244187766,
        "epoch": 0.14612627515853321,
        "step": 530
    },
    {
        "loss": 2.2388,
        "grad_norm": 1.0499632358551025,
        "learning_rate": 0.00012443283240634294,
        "epoch": 0.14640198511166252,
        "step": 531
    },
    {
        "loss": 2.4825,
        "grad_norm": 1.0418612957000732,
        "learning_rate": 0.00012417902509682214,
        "epoch": 0.14667769506479184,
        "step": 532
    },
    {
        "loss": 2.1554,
        "grad_norm": 1.6522077322006226,
        "learning_rate": 0.0001239250522509509,
        "epoch": 0.14695340501792115,
        "step": 533
    },
    {
        "loss": 2.3412,
        "grad_norm": 1.428579568862915,
        "learning_rate": 0.00012367091560749817,
        "epoch": 0.14722911497105046,
        "step": 534
    },
    {
        "loss": 2.0354,
        "grad_norm": 2.1652252674102783,
        "learning_rate": 0.00012341661690635434,
        "epoch": 0.14750482492417977,
        "step": 535
    },
    {
        "loss": 2.1092,
        "grad_norm": 1.1505850553512573,
        "learning_rate": 0.0001231621578885192,
        "epoch": 0.14778053487730908,
        "step": 536
    },
    {
        "loss": 2.305,
        "grad_norm": 1.1503959894180298,
        "learning_rate": 0.00012290754029609016,
        "epoch": 0.1480562448304384,
        "step": 537
    },
    {
        "loss": 2.3327,
        "grad_norm": 1.9999750852584839,
        "learning_rate": 0.0001226527658722503,
        "epoch": 0.1483319547835677,
        "step": 538
    },
    {
        "loss": 1.6159,
        "grad_norm": 1.5467250347137451,
        "learning_rate": 0.00012239783636125643,
        "epoch": 0.14860766473669698,
        "step": 539
    },
    {
        "loss": 2.2811,
        "grad_norm": 1.9540462493896484,
        "learning_rate": 0.00012214275350842704,
        "epoch": 0.1488833746898263,
        "step": 540
    },
    {
        "loss": 2.3566,
        "grad_norm": 1.257933259010315,
        "learning_rate": 0.00012188751906013052,
        "epoch": 0.1491590846429556,
        "step": 541
    },
    {
        "loss": 2.1888,
        "grad_norm": 1.9851654767990112,
        "learning_rate": 0.00012163213476377309,
        "epoch": 0.1494347945960849,
        "step": 542
    },
    {
        "loss": 1.987,
        "grad_norm": 1.7496176958084106,
        "learning_rate": 0.0001213766023677869,
        "epoch": 0.14971050454921422,
        "step": 543
    },
    {
        "loss": 2.4318,
        "grad_norm": 1.0680408477783203,
        "learning_rate": 0.00012112092362161798,
        "epoch": 0.14998621450234353,
        "step": 544
    },
    {
        "loss": 2.6221,
        "grad_norm": 1.8321565389633179,
        "learning_rate": 0.00012086510027571438,
        "epoch": 0.15026192445547285,
        "step": 545
    },
    {
        "loss": 2.4171,
        "grad_norm": 1.4457372426986694,
        "learning_rate": 0.00012060913408151409,
        "epoch": 0.15053763440860216,
        "step": 546
    },
    {
        "loss": 2.4088,
        "grad_norm": 1.103020429611206,
        "learning_rate": 0.00012035302679143301,
        "epoch": 0.15081334436173147,
        "step": 547
    },
    {
        "loss": 2.3576,
        "grad_norm": 1.6625603437423706,
        "learning_rate": 0.00012009678015885318,
        "epoch": 0.15108905431486078,
        "step": 548
    },
    {
        "loss": 2.46,
        "grad_norm": 1.3555552959442139,
        "learning_rate": 0.00011984039593811045,
        "epoch": 0.1513647642679901,
        "step": 549
    },
    {
        "loss": 2.263,
        "grad_norm": 1.4954338073730469,
        "learning_rate": 0.00011958387588448274,
        "epoch": 0.15164047422111937,
        "step": 550
    },
    {
        "loss": 1.8817,
        "grad_norm": 1.9115722179412842,
        "learning_rate": 0.00011932722175417795,
        "epoch": 0.15191618417424868,
        "step": 551
    },
    {
        "loss": 2.2034,
        "grad_norm": 1.5729767084121704,
        "learning_rate": 0.00011907043530432173,
        "epoch": 0.152191894127378,
        "step": 552
    },
    {
        "loss": 2.4237,
        "grad_norm": 1.1620537042617798,
        "learning_rate": 0.00011881351829294585,
        "epoch": 0.1524676040805073,
        "step": 553
    },
    {
        "loss": 2.3556,
        "grad_norm": 1.5637025833129883,
        "learning_rate": 0.00011855647247897577,
        "epoch": 0.1527433140336366,
        "step": 554
    },
    {
        "loss": 1.5889,
        "grad_norm": 1.9161350727081299,
        "learning_rate": 0.00011829929962221887,
        "epoch": 0.15301902398676592,
        "step": 555
    },
    {
        "loss": 1.8816,
        "grad_norm": 1.6604663133621216,
        "learning_rate": 0.00011804200148335227,
        "epoch": 0.15329473393989523,
        "step": 556
    },
    {
        "loss": 2.177,
        "grad_norm": 1.557050108909607,
        "learning_rate": 0.00011778457982391075,
        "epoch": 0.15357044389302454,
        "step": 557
    },
    {
        "loss": 2.2774,
        "grad_norm": 1.3112132549285889,
        "learning_rate": 0.00011752703640627487,
        "epoch": 0.15384615384615385,
        "step": 558
    },
    {
        "loss": 2.2748,
        "grad_norm": 1.7868787050247192,
        "learning_rate": 0.00011726937299365863,
        "epoch": 0.15412186379928317,
        "step": 559
    },
    {
        "loss": 1.7365,
        "grad_norm": 1.707067608833313,
        "learning_rate": 0.00011701159135009771,
        "epoch": 0.15439757375241248,
        "step": 560
    },
    {
        "loss": 2.2474,
        "grad_norm": 1.0207672119140625,
        "learning_rate": 0.00011675369324043707,
        "epoch": 0.15467328370554176,
        "step": 561
    },
    {
        "loss": 2.3131,
        "grad_norm": 1.212999701499939,
        "learning_rate": 0.00011649568043031916,
        "epoch": 0.15494899365867107,
        "step": 562
    },
    {
        "loss": 1.2341,
        "grad_norm": 1.7922687530517578,
        "learning_rate": 0.00011623755468617163,
        "epoch": 0.15522470361180038,
        "step": 563
    },
    {
        "loss": 2.4362,
        "grad_norm": 1.2466425895690918,
        "learning_rate": 0.00011597931777519531,
        "epoch": 0.1555004135649297,
        "step": 564
    },
    {
        "loss": 2.6167,
        "grad_norm": 1.0085865259170532,
        "learning_rate": 0.00011572097146535217,
        "epoch": 0.155776123518059,
        "step": 565
    },
    {
        "loss": 2.271,
        "grad_norm": 1.4663091897964478,
        "learning_rate": 0.00011546251752535308,
        "epoch": 0.1560518334711883,
        "step": 566
    },
    {
        "loss": 2.266,
        "grad_norm": 0.8403858542442322,
        "learning_rate": 0.0001152039577246458,
        "epoch": 0.15632754342431762,
        "step": 567
    },
    {
        "loss": 2.07,
        "grad_norm": 1.1609697341918945,
        "learning_rate": 0.00011494529383340291,
        "epoch": 0.15660325337744693,
        "step": 568
    },
    {
        "loss": 1.8344,
        "grad_norm": 1.9158722162246704,
        "learning_rate": 0.00011468652762250946,
        "epoch": 0.15687896333057624,
        "step": 569
    },
    {
        "loss": 1.7735,
        "grad_norm": 2.3585081100463867,
        "learning_rate": 0.00011442766086355123,
        "epoch": 0.15715467328370555,
        "step": 570
    },
    {
        "loss": 2.4198,
        "grad_norm": 1.7285655736923218,
        "learning_rate": 0.00011416869532880217,
        "epoch": 0.15743038323683484,
        "step": 571
    },
    {
        "loss": 2.5088,
        "grad_norm": 1.2696444988250732,
        "learning_rate": 0.00011390963279121259,
        "epoch": 0.15770609318996415,
        "step": 572
    },
    {
        "loss": 2.4,
        "grad_norm": 1.4021165370941162,
        "learning_rate": 0.00011365047502439689,
        "epoch": 0.15798180314309346,
        "step": 573
    },
    {
        "loss": 2.2098,
        "grad_norm": 1.8172837495803833,
        "learning_rate": 0.00011339122380262145,
        "epoch": 0.15825751309622277,
        "step": 574
    },
    {
        "loss": 2.3074,
        "grad_norm": 2.0760984420776367,
        "learning_rate": 0.00011313188090079245,
        "epoch": 0.15853322304935208,
        "step": 575
    },
    {
        "loss": 2.0611,
        "grad_norm": 1.6823607683181763,
        "learning_rate": 0.00011287244809444372,
        "epoch": 0.1588089330024814,
        "step": 576
    },
    {
        "loss": 1.8059,
        "grad_norm": 2.1693365573883057,
        "learning_rate": 0.00011261292715972462,
        "epoch": 0.1590846429556107,
        "step": 577
    },
    {
        "loss": 2.0252,
        "grad_norm": 2.2806050777435303,
        "learning_rate": 0.00011235331987338788,
        "epoch": 0.15936035290874,
        "step": 578
    },
    {
        "loss": 2.1362,
        "grad_norm": 1.6522425413131714,
        "learning_rate": 0.00011209362801277739,
        "epoch": 0.15963606286186932,
        "step": 579
    },
    {
        "loss": 2.3982,
        "grad_norm": 1.7708613872528076,
        "learning_rate": 0.00011183385335581606,
        "epoch": 0.15991177281499863,
        "step": 580
    },
    {
        "loss": 1.8858,
        "grad_norm": 1.9987411499023438,
        "learning_rate": 0.00011157399768099366,
        "epoch": 0.16018748276812794,
        "step": 581
    },
    {
        "loss": 2.3246,
        "grad_norm": 1.4391870498657227,
        "learning_rate": 0.00011131406276735461,
        "epoch": 0.16046319272125723,
        "step": 582
    },
    {
        "loss": 2.4915,
        "grad_norm": 1.7181329727172852,
        "learning_rate": 0.00011105405039448586,
        "epoch": 0.16073890267438654,
        "step": 583
    },
    {
        "loss": 2.2789,
        "grad_norm": 1.914568305015564,
        "learning_rate": 0.00011079396234250463,
        "epoch": 0.16101461262751585,
        "step": 584
    },
    {
        "loss": 2.029,
        "grad_norm": 1.9525882005691528,
        "learning_rate": 0.00011053380039204631,
        "epoch": 0.16129032258064516,
        "step": 585
    },
    {
        "loss": 2.2009,
        "grad_norm": 1.188158631324768,
        "learning_rate": 0.0001102735663242521,
        "epoch": 0.16156603253377447,
        "step": 586
    },
    {
        "loss": 1.604,
        "grad_norm": 1.752935767173767,
        "learning_rate": 0.00011001326192075709,
        "epoch": 0.16184174248690378,
        "step": 587
    },
    {
        "loss": 2.3179,
        "grad_norm": 1.1414343118667603,
        "learning_rate": 0.00010975288896367783,
        "epoch": 0.1621174524400331,
        "step": 588
    },
    {
        "loss": 1.8834,
        "grad_norm": 1.569287896156311,
        "learning_rate": 0.0001094924492356002,
        "epoch": 0.1623931623931624,
        "step": 589
    },
    {
        "loss": 2.0808,
        "grad_norm": 1.6355905532836914,
        "learning_rate": 0.00010923194451956724,
        "epoch": 0.1626688723462917,
        "step": 590
    },
    {
        "loss": 2.1212,
        "grad_norm": 1.8115699291229248,
        "learning_rate": 0.00010897137659906688,
        "epoch": 0.16294458229942102,
        "step": 591
    },
    {
        "loss": 2.3579,
        "grad_norm": 1.2247856855392456,
        "learning_rate": 0.00010871074725801979,
        "epoch": 0.16322029225255033,
        "step": 592
    },
    {
        "loss": 2.3348,
        "grad_norm": 1.183202862739563,
        "learning_rate": 0.00010845005828076717,
        "epoch": 0.1634960022056796,
        "step": 593
    },
    {
        "loss": 2.2844,
        "grad_norm": 1.0980439186096191,
        "learning_rate": 0.00010818931145205844,
        "epoch": 0.16377171215880892,
        "step": 594
    },
    {
        "loss": 1.8875,
        "grad_norm": 1.9036409854888916,
        "learning_rate": 0.00010792850855703916,
        "epoch": 0.16404742211193823,
        "step": 595
    },
    {
        "loss": 2.4125,
        "grad_norm": 0.9734413027763367,
        "learning_rate": 0.00010766765138123866,
        "epoch": 0.16432313206506755,
        "step": 596
    },
    {
        "loss": 1.4373,
        "grad_norm": 1.9929215908050537,
        "learning_rate": 0.00010740674171055798,
        "epoch": 0.16459884201819686,
        "step": 597
    },
    {
        "loss": 2.0601,
        "grad_norm": 1.8235437870025635,
        "learning_rate": 0.00010714578133125743,
        "epoch": 0.16487455197132617,
        "step": 598
    },
    {
        "loss": 2.047,
        "grad_norm": 1.6052697896957397,
        "learning_rate": 0.00010688477202994463,
        "epoch": 0.16515026192445548,
        "step": 599
    },
    {
        "loss": 1.5583,
        "grad_norm": 1.90186607837677,
        "learning_rate": 0.00010662371559356204,
        "epoch": 0.1654259718775848,
        "step": 600
    },
    {
        "loss": 2.525,
        "grad_norm": 1.1475419998168945,
        "learning_rate": 0.00010636261380937482,
        "epoch": 0.1657016818307141,
        "step": 601
    },
    {
        "loss": 2.133,
        "grad_norm": 1.2269564867019653,
        "learning_rate": 0.00010610146846495867,
        "epoch": 0.1659773917838434,
        "step": 602
    },
    {
        "loss": 2.0774,
        "grad_norm": 1.310183048248291,
        "learning_rate": 0.0001058402813481874,
        "epoch": 0.1662531017369727,
        "step": 603
    },
    {
        "loss": 2.1557,
        "grad_norm": 0.8690149784088135,
        "learning_rate": 0.00010557905424722092,
        "epoch": 0.166528811690102,
        "step": 604
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.9848923683166504,
        "learning_rate": 0.00010531778895049282,
        "epoch": 0.1668045216432313,
        "step": 605
    },
    {
        "loss": 1.9771,
        "grad_norm": 1.2579712867736816,
        "learning_rate": 0.0001050564872466982,
        "epoch": 0.16708023159636062,
        "step": 606
    },
    {
        "loss": 2.2003,
        "grad_norm": 1.4869352579116821,
        "learning_rate": 0.00010479515092478138,
        "epoch": 0.16735594154948993,
        "step": 607
    },
    {
        "loss": 2.1072,
        "grad_norm": 1.5335071086883545,
        "learning_rate": 0.00010453378177392375,
        "epoch": 0.16763165150261924,
        "step": 608
    },
    {
        "loss": 2.4588,
        "grad_norm": 1.6162986755371094,
        "learning_rate": 0.00010427238158353144,
        "epoch": 0.16790736145574855,
        "step": 609
    },
    {
        "loss": 2.8493,
        "grad_norm": 1.2647178173065186,
        "learning_rate": 0.00010401095214322303,
        "epoch": 0.16818307140887787,
        "step": 610
    },
    {
        "loss": 2.3825,
        "grad_norm": 1.1658194065093994,
        "learning_rate": 0.0001037494952428174,
        "epoch": 0.16845878136200718,
        "step": 611
    },
    {
        "loss": 2.6445,
        "grad_norm": 1.4027702808380127,
        "learning_rate": 0.00010348801267232147,
        "epoch": 0.1687344913151365,
        "step": 612
    },
    {
        "loss": 2.2458,
        "grad_norm": 1.3777401447296143,
        "learning_rate": 0.00010322650622191778,
        "epoch": 0.1690102012682658,
        "step": 613
    },
    {
        "loss": 2.3514,
        "grad_norm": 1.2618565559387207,
        "learning_rate": 0.0001029649776819525,
        "epoch": 0.16928591122139508,
        "step": 614
    },
    {
        "loss": 2.1594,
        "grad_norm": 1.6570873260498047,
        "learning_rate": 0.00010270342884292295,
        "epoch": 0.1695616211745244,
        "step": 615
    },
    {
        "loss": 2.5273,
        "grad_norm": 0.8869355916976929,
        "learning_rate": 0.00010244186149546542,
        "epoch": 0.1698373311276537,
        "step": 616
    },
    {
        "loss": 1.8177,
        "grad_norm": 1.7899938821792603,
        "learning_rate": 0.00010218027743034295,
        "epoch": 0.170113041080783,
        "step": 617
    },
    {
        "loss": 2.1854,
        "grad_norm": 1.5737167596817017,
        "learning_rate": 0.00010191867843843302,
        "epoch": 0.17038875103391232,
        "step": 618
    },
    {
        "loss": 2.5056,
        "grad_norm": 0.9428221583366394,
        "learning_rate": 0.00010165706631071528,
        "epoch": 0.17066446098704163,
        "step": 619
    },
    {
        "loss": 2.2218,
        "grad_norm": 1.336944818496704,
        "learning_rate": 0.00010139544283825935,
        "epoch": 0.17094017094017094,
        "step": 620
    },
    {
        "loss": 2.2436,
        "grad_norm": 1.2684314250946045,
        "learning_rate": 0.0001011338098122125,
        "epoch": 0.17121588089330025,
        "step": 621
    },
    {
        "loss": 2.4186,
        "grad_norm": 1.3058239221572876,
        "learning_rate": 0.00010087216902378742,
        "epoch": 0.17149159084642956,
        "step": 622
    },
    {
        "loss": 2.2491,
        "grad_norm": 0.8240327835083008,
        "learning_rate": 0.00010061052226424987,
        "epoch": 0.17176730079955888,
        "step": 623
    },
    {
        "loss": 1.9997,
        "grad_norm": 1.5627154111862183,
        "learning_rate": 0.00010034887132490662,
        "epoch": 0.17204301075268819,
        "step": 624
    },
    {
        "loss": 2.4492,
        "grad_norm": 0.9992144107818604,
        "learning_rate": 0.000100087217997093,
        "epoch": 0.17231872070581747,
        "step": 625
    },
    {
        "loss": 2.197,
        "grad_norm": 1.3674603700637817,
        "learning_rate": 9.98255640721606e-05,
        "epoch": 0.17259443065894678,
        "step": 626
    },
    {
        "loss": 2.2229,
        "grad_norm": 1.3455605506896973,
        "learning_rate": 9.956391134146525e-05,
        "epoch": 0.1728701406120761,
        "step": 627
    },
    {
        "loss": 2.6652,
        "grad_norm": 0.9979743361473083,
        "learning_rate": 9.930226159635447e-05,
        "epoch": 0.1731458505652054,
        "step": 628
    },
    {
        "loss": 1.9595,
        "grad_norm": 1.7584420442581177,
        "learning_rate": 9.904061662815545e-05,
        "epoch": 0.1734215605183347,
        "step": 629
    },
    {
        "loss": 2.6134,
        "grad_norm": 1.1413136720657349,
        "learning_rate": 9.877897822816261e-05,
        "epoch": 0.17369727047146402,
        "step": 630
    },
    {
        "loss": 2.5363,
        "grad_norm": 1.1523958444595337,
        "learning_rate": 9.85173481876254e-05,
        "epoch": 0.17397298042459333,
        "step": 631
    },
    {
        "loss": 1.6594,
        "grad_norm": 2.1071574687957764,
        "learning_rate": 9.825572829773611e-05,
        "epoch": 0.17424869037772264,
        "step": 632
    },
    {
        "loss": 2.4495,
        "grad_norm": 1.68262779712677,
        "learning_rate": 9.799412034961742e-05,
        "epoch": 0.17452440033085195,
        "step": 633
    },
    {
        "loss": 2.3736,
        "grad_norm": 1.0021276473999023,
        "learning_rate": 9.773252613431037e-05,
        "epoch": 0.17480011028398126,
        "step": 634
    },
    {
        "loss": 1.7973,
        "grad_norm": 1.5688552856445312,
        "learning_rate": 9.747094744276189e-05,
        "epoch": 0.17507582023711055,
        "step": 635
    },
    {
        "loss": 2.0603,
        "grad_norm": 1.5935029983520508,
        "learning_rate": 9.720938606581268e-05,
        "epoch": 0.17535153019023986,
        "step": 636
    },
    {
        "loss": 2.3747,
        "grad_norm": 1.1448960304260254,
        "learning_rate": 9.69478437941849e-05,
        "epoch": 0.17562724014336917,
        "step": 637
    },
    {
        "loss": 2.0318,
        "grad_norm": 1.3003621101379395,
        "learning_rate": 9.668632241846988e-05,
        "epoch": 0.17590295009649848,
        "step": 638
    },
    {
        "loss": 2.5515,
        "grad_norm": 0.9181602001190186,
        "learning_rate": 9.642482372911595e-05,
        "epoch": 0.1761786600496278,
        "step": 639
    },
    {
        "loss": 2.3788,
        "grad_norm": 1.2347915172576904,
        "learning_rate": 9.616334951641601e-05,
        "epoch": 0.1764543700027571,
        "step": 640
    },
    {
        "loss": 2.5798,
        "grad_norm": 0.9815462231636047,
        "learning_rate": 9.59019015704955e-05,
        "epoch": 0.1767300799558864,
        "step": 641
    },
    {
        "loss": 1.9314,
        "grad_norm": 1.95697820186615,
        "learning_rate": 9.564048168130002e-05,
        "epoch": 0.17700578990901572,
        "step": 642
    },
    {
        "loss": 2.2987,
        "grad_norm": 1.5505753755569458,
        "learning_rate": 9.537909163858298e-05,
        "epoch": 0.17728149986214503,
        "step": 643
    },
    {
        "loss": 2.1688,
        "grad_norm": 1.7688194513320923,
        "learning_rate": 9.511773323189358e-05,
        "epoch": 0.17755720981527434,
        "step": 644
    },
    {
        "loss": 1.5265,
        "grad_norm": 2.312260627746582,
        "learning_rate": 9.485640825056434e-05,
        "epoch": 0.17783291976840365,
        "step": 645
    },
    {
        "loss": 2.2299,
        "grad_norm": 1.688890814781189,
        "learning_rate": 9.4595118483699e-05,
        "epoch": 0.17810862972153294,
        "step": 646
    },
    {
        "loss": 1.3432,
        "grad_norm": 1.9165186882019043,
        "learning_rate": 9.433386572016022e-05,
        "epoch": 0.17838433967466225,
        "step": 647
    },
    {
        "loss": 1.7038,
        "grad_norm": 1.8930896520614624,
        "learning_rate": 9.407265174855723e-05,
        "epoch": 0.17866004962779156,
        "step": 648
    },
    {
        "loss": 2.0543,
        "grad_norm": 1.6033040285110474,
        "learning_rate": 9.381147835723379e-05,
        "epoch": 0.17893575958092087,
        "step": 649
    },
    {
        "loss": 1.9915,
        "grad_norm": 1.9194060564041138,
        "learning_rate": 9.355034733425576e-05,
        "epoch": 0.17921146953405018,
        "step": 650
    },
    {
        "loss": 1.6117,
        "grad_norm": 1.7935281991958618,
        "learning_rate": 9.328926046739899e-05,
        "epoch": 0.1794871794871795,
        "step": 651
    },
    {
        "loss": 2.5869,
        "grad_norm": 1.5846956968307495,
        "learning_rate": 9.3028219544137e-05,
        "epoch": 0.1797628894403088,
        "step": 652
    },
    {
        "loss": 2.2743,
        "grad_norm": 1.7212544679641724,
        "learning_rate": 9.276722635162874e-05,
        "epoch": 0.1800385993934381,
        "step": 653
    },
    {
        "loss": 2.3342,
        "grad_norm": 1.5406081676483154,
        "learning_rate": 9.250628267670642e-05,
        "epoch": 0.18031430934656742,
        "step": 654
    },
    {
        "loss": 2.5443,
        "grad_norm": 1.1840665340423584,
        "learning_rate": 9.224539030586323e-05,
        "epoch": 0.18059001929969673,
        "step": 655
    },
    {
        "loss": 2.2519,
        "grad_norm": 1.313391923904419,
        "learning_rate": 9.198455102524113e-05,
        "epoch": 0.18086572925282604,
        "step": 656
    },
    {
        "loss": 2.2098,
        "grad_norm": 0.8062867522239685,
        "learning_rate": 9.172376662061859e-05,
        "epoch": 0.18114143920595532,
        "step": 657
    },
    {
        "loss": 1.6011,
        "grad_norm": 1.6126675605773926,
        "learning_rate": 9.146303887739836e-05,
        "epoch": 0.18141714915908463,
        "step": 658
    },
    {
        "loss": 2.2953,
        "grad_norm": 1.363948106765747,
        "learning_rate": 9.120236958059535e-05,
        "epoch": 0.18169285911221394,
        "step": 659
    },
    {
        "loss": 2.1885,
        "grad_norm": 1.3115794658660889,
        "learning_rate": 9.094176051482422e-05,
        "epoch": 0.18196856906534326,
        "step": 660
    },
    {
        "loss": 2.2703,
        "grad_norm": 0.8501406908035278,
        "learning_rate": 9.068121346428735e-05,
        "epoch": 0.18224427901847257,
        "step": 661
    },
    {
        "loss": 2.2582,
        "grad_norm": 1.1108478307724,
        "learning_rate": 9.042073021276256e-05,
        "epoch": 0.18251998897160188,
        "step": 662
    },
    {
        "loss": 2.4631,
        "grad_norm": 1.7802672386169434,
        "learning_rate": 9.016031254359081e-05,
        "epoch": 0.1827956989247312,
        "step": 663
    },
    {
        "loss": 1.919,
        "grad_norm": 1.4297810792922974,
        "learning_rate": 8.989996223966414e-05,
        "epoch": 0.1830714088778605,
        "step": 664
    },
    {
        "loss": 2.2199,
        "grad_norm": 1.5840880870819092,
        "learning_rate": 8.963968108341331e-05,
        "epoch": 0.1833471188309898,
        "step": 665
    },
    {
        "loss": 2.4004,
        "grad_norm": 1.7631158828735352,
        "learning_rate": 8.937947085679576e-05,
        "epoch": 0.18362282878411912,
        "step": 666
    },
    {
        "loss": 2.2736,
        "grad_norm": 1.3642264604568481,
        "learning_rate": 8.911933334128327e-05,
        "epoch": 0.1838985387372484,
        "step": 667
    },
    {
        "loss": 2.7516,
        "grad_norm": 0.9045529365539551,
        "learning_rate": 8.885927031784983e-05,
        "epoch": 0.1841742486903777,
        "step": 668
    },
    {
        "loss": 2.8158,
        "grad_norm": 1.0737495422363281,
        "learning_rate": 8.859928356695947e-05,
        "epoch": 0.18444995864350702,
        "step": 669
    },
    {
        "loss": 2.2149,
        "grad_norm": 1.5553516149520874,
        "learning_rate": 8.833937486855395e-05,
        "epoch": 0.18472566859663633,
        "step": 670
    },
    {
        "loss": 2.5095,
        "grad_norm": 1.4897509813308716,
        "learning_rate": 8.807954600204079e-05,
        "epoch": 0.18500137854976564,
        "step": 671
    },
    {
        "loss": 2.6144,
        "grad_norm": 1.3914241790771484,
        "learning_rate": 8.781979874628082e-05,
        "epoch": 0.18527708850289495,
        "step": 672
    },
    {
        "loss": 1.3313,
        "grad_norm": 2.389177083969116,
        "learning_rate": 8.756013487957625e-05,
        "epoch": 0.18555279845602426,
        "step": 673
    },
    {
        "loss": 2.2411,
        "grad_norm": 1.5359468460083008,
        "learning_rate": 8.730055617965835e-05,
        "epoch": 0.18582850840915358,
        "step": 674
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.0673682689666748,
        "learning_rate": 8.704106442367529e-05,
        "epoch": 0.18610421836228289,
        "step": 675
    },
    {
        "loss": 2.1527,
        "grad_norm": 1.5579383373260498,
        "learning_rate": 8.678166138818003e-05,
        "epoch": 0.1863799283154122,
        "step": 676
    },
    {
        "loss": 2.3379,
        "grad_norm": 1.5594735145568848,
        "learning_rate": 8.652234884911807e-05,
        "epoch": 0.1866556382685415,
        "step": 677
    },
    {
        "loss": 2.2672,
        "grad_norm": 1.7349282503128052,
        "learning_rate": 8.626312858181543e-05,
        "epoch": 0.1869313482216708,
        "step": 678
    },
    {
        "loss": 2.3598,
        "grad_norm": 1.1872210502624512,
        "learning_rate": 8.600400236096636e-05,
        "epoch": 0.1872070581748001,
        "step": 679
    },
    {
        "loss": 2.5661,
        "grad_norm": 1.390242576599121,
        "learning_rate": 8.574497196062124e-05,
        "epoch": 0.1874827681279294,
        "step": 680
    },
    {
        "loss": 2.3602,
        "grad_norm": 1.2256619930267334,
        "learning_rate": 8.548603915417445e-05,
        "epoch": 0.18775847808105872,
        "step": 681
    },
    {
        "loss": 1.5594,
        "grad_norm": 1.740086317062378,
        "learning_rate": 8.522720571435221e-05,
        "epoch": 0.18803418803418803,
        "step": 682
    },
    {
        "loss": 2.3181,
        "grad_norm": 1.4814618825912476,
        "learning_rate": 8.496847341320045e-05,
        "epoch": 0.18830989798731734,
        "step": 683
    },
    {
        "loss": 2.0742,
        "grad_norm": 1.3861455917358398,
        "learning_rate": 8.470984402207267e-05,
        "epoch": 0.18858560794044665,
        "step": 684
    },
    {
        "loss": 2.6501,
        "grad_norm": 1.0957392454147339,
        "learning_rate": 8.445131931161783e-05,
        "epoch": 0.18886131789357596,
        "step": 685
    },
    {
        "loss": 2.5048,
        "grad_norm": 1.1369149684906006,
        "learning_rate": 8.419290105176824e-05,
        "epoch": 0.18913702784670527,
        "step": 686
    },
    {
        "loss": 1.7029,
        "grad_norm": 2.016226053237915,
        "learning_rate": 8.393459101172734e-05,
        "epoch": 0.18941273779983459,
        "step": 687
    },
    {
        "loss": 1.7723,
        "grad_norm": 1.854324221611023,
        "learning_rate": 8.367639095995773e-05,
        "epoch": 0.1896884477529639,
        "step": 688
    },
    {
        "loss": 2.4494,
        "grad_norm": 1.793616771697998,
        "learning_rate": 8.341830266416903e-05,
        "epoch": 0.18996415770609318,
        "step": 689
    },
    {
        "loss": 2.6609,
        "grad_norm": 1.0397899150848389,
        "learning_rate": 8.316032789130566e-05,
        "epoch": 0.1902398676592225,
        "step": 690
    },
    {
        "loss": 2.2029,
        "grad_norm": 0.8200177550315857,
        "learning_rate": 8.29024684075349e-05,
        "epoch": 0.1905155776123518,
        "step": 691
    },
    {
        "loss": 2.3743,
        "grad_norm": 1.4153705835342407,
        "learning_rate": 8.26447259782347e-05,
        "epoch": 0.1907912875654811,
        "step": 692
    },
    {
        "loss": 1.9189,
        "grad_norm": 2.200406551361084,
        "learning_rate": 8.23871023679816e-05,
        "epoch": 0.19106699751861042,
        "step": 693
    },
    {
        "loss": 2.1548,
        "grad_norm": 1.6783251762390137,
        "learning_rate": 8.212959934053876e-05,
        "epoch": 0.19134270747173973,
        "step": 694
    },
    {
        "loss": 1.7389,
        "grad_norm": 1.9068979024887085,
        "learning_rate": 8.187221865884367e-05,
        "epoch": 0.19161841742486904,
        "step": 695
    },
    {
        "loss": 2.2744,
        "grad_norm": 1.5444374084472656,
        "learning_rate": 8.161496208499633e-05,
        "epoch": 0.19189412737799835,
        "step": 696
    },
    {
        "loss": 2.2497,
        "grad_norm": 1.5081080198287964,
        "learning_rate": 8.135783138024694e-05,
        "epoch": 0.19216983733112766,
        "step": 697
    },
    {
        "loss": 2.4165,
        "grad_norm": 0.9875911474227905,
        "learning_rate": 8.110082830498407e-05,
        "epoch": 0.19244554728425697,
        "step": 698
    },
    {
        "loss": 2.6761,
        "grad_norm": 1.0850963592529297,
        "learning_rate": 8.084395461872249e-05,
        "epoch": 0.19272125723738626,
        "step": 699
    },
    {
        "loss": 2.5211,
        "grad_norm": 1.3026902675628662,
        "learning_rate": 8.058721208009103e-05,
        "epoch": 0.19299696719051557,
        "step": 700
    },
    {
        "loss": 2.3043,
        "grad_norm": 1.1755880117416382,
        "learning_rate": 8.033060244682082e-05,
        "epoch": 0.19327267714364488,
        "step": 701
    },
    {
        "loss": 1.9651,
        "grad_norm": 1.6958097219467163,
        "learning_rate": 8.007412747573291e-05,
        "epoch": 0.1935483870967742,
        "step": 702
    },
    {
        "loss": 2.2756,
        "grad_norm": 1.8014590740203857,
        "learning_rate": 7.981778892272653e-05,
        "epoch": 0.1938240970499035,
        "step": 703
    },
    {
        "loss": 2.1926,
        "grad_norm": 1.06857430934906,
        "learning_rate": 7.956158854276695e-05,
        "epoch": 0.1940998070030328,
        "step": 704
    },
    {
        "loss": 2.1984,
        "grad_norm": 1.208579421043396,
        "learning_rate": 7.930552808987337e-05,
        "epoch": 0.19437551695616212,
        "step": 705
    },
    {
        "loss": 2.1097,
        "grad_norm": 0.8373614549636841,
        "learning_rate": 7.904960931710714e-05,
        "epoch": 0.19465122690929143,
        "step": 706
    },
    {
        "loss": 2.38,
        "grad_norm": 1.5505106449127197,
        "learning_rate": 7.879383397655951e-05,
        "epoch": 0.19492693686242074,
        "step": 707
    },
    {
        "loss": 2.3643,
        "grad_norm": 1.2902182340621948,
        "learning_rate": 7.853820381933986e-05,
        "epoch": 0.19520264681555005,
        "step": 708
    },
    {
        "loss": 1.9407,
        "grad_norm": 1.7431834936141968,
        "learning_rate": 7.82827205955635e-05,
        "epoch": 0.19547835676867936,
        "step": 709
    },
    {
        "loss": 1.8163,
        "grad_norm": 1.8712831735610962,
        "learning_rate": 7.80273860543399e-05,
        "epoch": 0.19575406672180864,
        "step": 710
    },
    {
        "loss": 2.4314,
        "grad_norm": 1.3107917308807373,
        "learning_rate": 7.777220194376051e-05,
        "epoch": 0.19602977667493796,
        "step": 711
    },
    {
        "loss": 2.2359,
        "grad_norm": 1.452005386352539,
        "learning_rate": 7.751717001088696e-05,
        "epoch": 0.19630548662806727,
        "step": 712
    },
    {
        "loss": 2.6845,
        "grad_norm": 0.9879441261291504,
        "learning_rate": 7.7262292001739e-05,
        "epoch": 0.19658119658119658,
        "step": 713
    },
    {
        "loss": 2.4227,
        "grad_norm": 1.452553391456604,
        "learning_rate": 7.700756966128256e-05,
        "epoch": 0.1968569065343259,
        "step": 714
    },
    {
        "loss": 2.3481,
        "grad_norm": 0.9677034020423889,
        "learning_rate": 7.675300473341784e-05,
        "epoch": 0.1971326164874552,
        "step": 715
    },
    {
        "loss": 2.3538,
        "grad_norm": 1.4155762195587158,
        "learning_rate": 7.649859896096738e-05,
        "epoch": 0.1974083264405845,
        "step": 716
    },
    {
        "loss": 2.2827,
        "grad_norm": 1.8930243253707886,
        "learning_rate": 7.624435408566401e-05,
        "epoch": 0.19768403639371382,
        "step": 717
    },
    {
        "loss": 1.7465,
        "grad_norm": 1.2984915971755981,
        "learning_rate": 7.599027184813913e-05,
        "epoch": 0.19795974634684313,
        "step": 718
    },
    {
        "loss": 2.1985,
        "grad_norm": 0.9752087593078613,
        "learning_rate": 7.573635398791055e-05,
        "epoch": 0.19823545629997244,
        "step": 719
    },
    {
        "loss": 2.3673,
        "grad_norm": 1.4603217840194702,
        "learning_rate": 7.548260224337078e-05,
        "epoch": 0.19851116625310175,
        "step": 720
    },
    {
        "loss": 2.727,
        "grad_norm": 0.9201823472976685,
        "learning_rate": 7.52290183517751e-05,
        "epoch": 0.19878687620623103,
        "step": 721
    },
    {
        "loss": 1.9264,
        "grad_norm": 1.5500856637954712,
        "learning_rate": 7.49756040492295e-05,
        "epoch": 0.19906258615936034,
        "step": 722
    },
    {
        "loss": 1.7103,
        "grad_norm": 2.0038044452667236,
        "learning_rate": 7.472236107067905e-05,
        "epoch": 0.19933829611248965,
        "step": 723
    },
    {
        "loss": 2.1666,
        "grad_norm": 1.8884618282318115,
        "learning_rate": 7.446929114989576e-05,
        "epoch": 0.19961400606561897,
        "step": 724
    },
    {
        "loss": 1.855,
        "grad_norm": 1.7827022075653076,
        "learning_rate": 7.421639601946693e-05,
        "epoch": 0.19988971601874828,
        "step": 725
    },
    {
        "loss": 2.1889,
        "grad_norm": 1.0969133377075195,
        "learning_rate": 7.396367741078318e-05,
        "epoch": 0.2001654259718776,
        "step": 726
    },
    {
        "loss": 1.3716,
        "grad_norm": 1.9571059942245483,
        "learning_rate": 7.371113705402657e-05,
        "epoch": 0.2004411359250069,
        "step": 727
    },
    {
        "loss": 1.9453,
        "grad_norm": 1.0392612218856812,
        "learning_rate": 7.345877667815885e-05,
        "epoch": 0.2007168458781362,
        "step": 728
    },
    {
        "loss": 2.3878,
        "grad_norm": 1.5272554159164429,
        "learning_rate": 7.320659801090953e-05,
        "epoch": 0.20099255583126552,
        "step": 729
    },
    {
        "loss": 1.3638,
        "grad_norm": 2.505957841873169,
        "learning_rate": 7.295460277876411e-05,
        "epoch": 0.20126826578439483,
        "step": 730
    },
    {
        "loss": 1.6985,
        "grad_norm": 2.0118722915649414,
        "learning_rate": 7.270279270695228e-05,
        "epoch": 0.2015439757375241,
        "step": 731
    },
    {
        "loss": 2.3699,
        "grad_norm": 1.8750834465026855,
        "learning_rate": 7.245116951943598e-05,
        "epoch": 0.20181968569065342,
        "step": 732
    },
    {
        "loss": 2.2513,
        "grad_norm": 2.1029365062713623,
        "learning_rate": 7.219973493889779e-05,
        "epoch": 0.20209539564378273,
        "step": 733
    },
    {
        "loss": 2.5919,
        "grad_norm": 1.3706471920013428,
        "learning_rate": 7.194849068672893e-05,
        "epoch": 0.20237110559691204,
        "step": 734
    },
    {
        "loss": 1.8824,
        "grad_norm": 2.5358388423919678,
        "learning_rate": 7.169743848301768e-05,
        "epoch": 0.20264681555004135,
        "step": 735
    },
    {
        "loss": 2.4334,
        "grad_norm": 0.8419846892356873,
        "learning_rate": 7.144658004653745e-05,
        "epoch": 0.20292252550317066,
        "step": 736
    },
    {
        "loss": 2.0509,
        "grad_norm": 1.4322131872177124,
        "learning_rate": 7.119591709473503e-05,
        "epoch": 0.20319823545629997,
        "step": 737
    },
    {
        "loss": 2.162,
        "grad_norm": 1.7253332138061523,
        "learning_rate": 7.094545134371898e-05,
        "epoch": 0.20347394540942929,
        "step": 738
    },
    {
        "loss": 2.2004,
        "grad_norm": 1.1063860654830933,
        "learning_rate": 7.069518450824764e-05,
        "epoch": 0.2037496553625586,
        "step": 739
    },
    {
        "loss": 2.3608,
        "grad_norm": 1.3962513208389282,
        "learning_rate": 7.044511830171758e-05,
        "epoch": 0.2040253653156879,
        "step": 740
    },
    {
        "loss": 2.074,
        "grad_norm": 1.6998831033706665,
        "learning_rate": 7.019525443615185e-05,
        "epoch": 0.20430107526881722,
        "step": 741
    },
    {
        "loss": 1.9075,
        "grad_norm": 1.733311653137207,
        "learning_rate": 6.994559462218811e-05,
        "epoch": 0.2045767852219465,
        "step": 742
    },
    {
        "loss": 2.4343,
        "grad_norm": 1.3517673015594482,
        "learning_rate": 6.969614056906716e-05,
        "epoch": 0.2048524951750758,
        "step": 743
    },
    {
        "loss": 2.5014,
        "grad_norm": 1.676842212677002,
        "learning_rate": 6.944689398462096e-05,
        "epoch": 0.20512820512820512,
        "step": 744
    },
    {
        "loss": 2.6378,
        "grad_norm": 0.9511610865592957,
        "learning_rate": 6.919785657526118e-05,
        "epoch": 0.20540391508133443,
        "step": 745
    },
    {
        "loss": 2.0129,
        "grad_norm": 1.496307611465454,
        "learning_rate": 6.894903004596742e-05,
        "epoch": 0.20567962503446374,
        "step": 746
    },
    {
        "loss": 1.7226,
        "grad_norm": 1.7197765111923218,
        "learning_rate": 6.870041610027547e-05,
        "epoch": 0.20595533498759305,
        "step": 747
    },
    {
        "loss": 2.1827,
        "grad_norm": 0.8937548398971558,
        "learning_rate": 6.84520164402658e-05,
        "epoch": 0.20623104494072236,
        "step": 748
    },
    {
        "loss": 1.9339,
        "grad_norm": 1.417070984840393,
        "learning_rate": 6.820383276655169e-05,
        "epoch": 0.20650675489385167,
        "step": 749
    },
    {
        "loss": 2.3354,
        "grad_norm": 1.7721893787384033,
        "learning_rate": 6.795586677826789e-05,
        "epoch": 0.20678246484698098,
        "step": 750
    },
    {
        "loss": 2.5586,
        "grad_norm": 1.081018090248108,
        "learning_rate": 6.770812017305865e-05,
        "epoch": 0.2070581748001103,
        "step": 751
    },
    {
        "loss": 2.3639,
        "grad_norm": 1.5727088451385498,
        "learning_rate": 6.74605946470664e-05,
        "epoch": 0.2073338847532396,
        "step": 752
    },
    {
        "loss": 2.4339,
        "grad_norm": 1.0875444412231445,
        "learning_rate": 6.721329189491989e-05,
        "epoch": 0.2076095947063689,
        "step": 753
    },
    {
        "loss": 1.5833,
        "grad_norm": 2.151742696762085,
        "learning_rate": 6.696621360972277e-05,
        "epoch": 0.2078853046594982,
        "step": 754
    },
    {
        "loss": 2.1917,
        "grad_norm": 1.456330418586731,
        "learning_rate": 6.671936148304191e-05,
        "epoch": 0.2081610146126275,
        "step": 755
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.4837279319763184,
        "learning_rate": 6.64727372048958e-05,
        "epoch": 0.20843672456575682,
        "step": 756
    },
    {
        "loss": 1.982,
        "grad_norm": 2.071782112121582,
        "learning_rate": 6.622634246374302e-05,
        "epoch": 0.20871243451888613,
        "step": 757
    },
    {
        "loss": 2.2533,
        "grad_norm": 1.5495084524154663,
        "learning_rate": 6.598017894647076e-05,
        "epoch": 0.20898814447201544,
        "step": 758
    },
    {
        "loss": 2.2686,
        "grad_norm": 1.1996619701385498,
        "learning_rate": 6.573424833838312e-05,
        "epoch": 0.20926385442514475,
        "step": 759
    },
    {
        "loss": 2.5249,
        "grad_norm": 1.0126302242279053,
        "learning_rate": 6.54885523231896e-05,
        "epoch": 0.20953956437827406,
        "step": 760
    },
    {
        "loss": 2.5131,
        "grad_norm": 1.12920081615448,
        "learning_rate": 6.524309258299368e-05,
        "epoch": 0.20981527433140337,
        "step": 761
    },
    {
        "loss": 2.3394,
        "grad_norm": 1.1924256086349487,
        "learning_rate": 6.499787079828125e-05,
        "epoch": 0.21009098428453268,
        "step": 762
    },
    {
        "loss": 1.9119,
        "grad_norm": 2.0610454082489014,
        "learning_rate": 6.475288864790897e-05,
        "epoch": 0.21036669423766197,
        "step": 763
    },
    {
        "loss": 2.2427,
        "grad_norm": 1.3558355569839478,
        "learning_rate": 6.450814780909305e-05,
        "epoch": 0.21064240419079128,
        "step": 764
    },
    {
        "loss": 2.2998,
        "grad_norm": 1.6580917835235596,
        "learning_rate": 6.426364995739749e-05,
        "epoch": 0.2109181141439206,
        "step": 765
    },
    {
        "loss": 1.8779,
        "grad_norm": 1.7183811664581299,
        "learning_rate": 6.40193967667228e-05,
        "epoch": 0.2111938240970499,
        "step": 766
    },
    {
        "loss": 1.957,
        "grad_norm": 2.0435562133789062,
        "learning_rate": 6.377538990929449e-05,
        "epoch": 0.2114695340501792,
        "step": 767
    },
    {
        "loss": 2.4845,
        "grad_norm": 1.3088723421096802,
        "learning_rate": 6.353163105565151e-05,
        "epoch": 0.21174524400330852,
        "step": 768
    },
    {
        "loss": 2.541,
        "grad_norm": 1.425838589668274,
        "learning_rate": 6.328812187463504e-05,
        "epoch": 0.21202095395643783,
        "step": 769
    },
    {
        "loss": 2.2449,
        "grad_norm": 1.5499953031539917,
        "learning_rate": 6.304486403337679e-05,
        "epoch": 0.21229666390956714,
        "step": 770
    },
    {
        "loss": 2.4435,
        "grad_norm": 1.3501096963882446,
        "learning_rate": 6.280185919728783e-05,
        "epoch": 0.21257237386269645,
        "step": 771
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.7824169397354126,
        "learning_rate": 6.255910903004708e-05,
        "epoch": 0.21284808381582576,
        "step": 772
    },
    {
        "loss": 2.2699,
        "grad_norm": 1.5815125703811646,
        "learning_rate": 6.231661519358984e-05,
        "epoch": 0.21312379376895507,
        "step": 773
    },
    {
        "loss": 1.9683,
        "grad_norm": 1.2809852361679077,
        "learning_rate": 6.20743793480966e-05,
        "epoch": 0.21339950372208435,
        "step": 774
    },
    {
        "loss": 2.5231,
        "grad_norm": 1.5801302194595337,
        "learning_rate": 6.18324031519815e-05,
        "epoch": 0.21367521367521367,
        "step": 775
    },
    {
        "loss": 2.5402,
        "grad_norm": 0.9902428984642029,
        "learning_rate": 6.159068826188109e-05,
        "epoch": 0.21395092362834298,
        "step": 776
    },
    {
        "loss": 1.9182,
        "grad_norm": 1.6151682138442993,
        "learning_rate": 6.134923633264296e-05,
        "epoch": 0.2142266335814723,
        "step": 777
    },
    {
        "loss": 2.4902,
        "grad_norm": 1.2408074140548706,
        "learning_rate": 6.110804901731431e-05,
        "epoch": 0.2145023435346016,
        "step": 778
    },
    {
        "loss": 2.2412,
        "grad_norm": 1.7082560062408447,
        "learning_rate": 6.0867127967130834e-05,
        "epoch": 0.2147780534877309,
        "step": 779
    },
    {
        "loss": 2.0129,
        "grad_norm": 1.4249327182769775,
        "learning_rate": 6.062647483150521e-05,
        "epoch": 0.21505376344086022,
        "step": 780
    },
    {
        "loss": 2.3321,
        "grad_norm": 1.0756617784500122,
        "learning_rate": 6.038609125801596e-05,
        "epoch": 0.21532947339398953,
        "step": 781
    },
    {
        "loss": 2.3983,
        "grad_norm": 0.8775233030319214,
        "learning_rate": 6.014597889239606e-05,
        "epoch": 0.21560518334711884,
        "step": 782
    },
    {
        "loss": 2.4706,
        "grad_norm": 0.9199336171150208,
        "learning_rate": 5.9906139378521744e-05,
        "epoch": 0.21588089330024815,
        "step": 783
    },
    {
        "loss": 2.07,
        "grad_norm": 1.7954778671264648,
        "learning_rate": 5.966657435840126e-05,
        "epoch": 0.21615660325337746,
        "step": 784
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.6721216440200806,
        "learning_rate": 5.9427285472163505e-05,
        "epoch": 0.21643231320650674,
        "step": 785
    },
    {
        "loss": 2.2296,
        "grad_norm": 1.5129473209381104,
        "learning_rate": 5.9188274358047e-05,
        "epoch": 0.21670802315963605,
        "step": 786
    },
    {
        "loss": 1.7611,
        "grad_norm": 1.9023189544677734,
        "learning_rate": 5.894954265238846e-05,
        "epoch": 0.21698373311276536,
        "step": 787
    },
    {
        "loss": 2.5231,
        "grad_norm": 1.481829285621643,
        "learning_rate": 5.871109198961174e-05,
        "epoch": 0.21725944306589468,
        "step": 788
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.7425774335861206,
        "learning_rate": 5.847292400221663e-05,
        "epoch": 0.21753515301902399,
        "step": 789
    },
    {
        "loss": 2.1261,
        "grad_norm": 1.5994335412979126,
        "learning_rate": 5.8235040320767586e-05,
        "epoch": 0.2178108629721533,
        "step": 790
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.71048903465271,
        "learning_rate": 5.79974425738826e-05,
        "epoch": 0.2180865729252826,
        "step": 791
    },
    {
        "loss": 2.4629,
        "grad_norm": 1.17703115940094,
        "learning_rate": 5.77601323882222e-05,
        "epoch": 0.21836228287841192,
        "step": 792
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.4992003440856934,
        "learning_rate": 5.752311138847809e-05,
        "epoch": 0.21863799283154123,
        "step": 793
    },
    {
        "loss": 1.8405,
        "grad_norm": 1.5998191833496094,
        "learning_rate": 5.7286381197362094e-05,
        "epoch": 0.21891370278467054,
        "step": 794
    },
    {
        "loss": 1.8084,
        "grad_norm": 1.6312968730926514,
        "learning_rate": 5.704994343559522e-05,
        "epoch": 0.21918941273779982,
        "step": 795
    },
    {
        "loss": 2.3255,
        "grad_norm": 1.7392677068710327,
        "learning_rate": 5.681379972189631e-05,
        "epoch": 0.21946512269092913,
        "step": 796
    },
    {
        "loss": 2.2461,
        "grad_norm": 1.3877575397491455,
        "learning_rate": 5.657795167297104e-05,
        "epoch": 0.21974083264405844,
        "step": 797
    },
    {
        "loss": 1.8971,
        "grad_norm": 2.136362314224243,
        "learning_rate": 5.6342400903501033e-05,
        "epoch": 0.22001654259718775,
        "step": 798
    },
    {
        "loss": 2.1226,
        "grad_norm": 1.460667371749878,
        "learning_rate": 5.610714902613252e-05,
        "epoch": 0.22029225255031706,
        "step": 799
    },
    {
        "loss": 1.5252,
        "grad_norm": 2.1774258613586426,
        "learning_rate": 5.5872197651465476e-05,
        "epoch": 0.22056796250344637,
        "step": 800
    },
    {
        "loss": 2.4063,
        "grad_norm": 1.0996867418289185,
        "learning_rate": 5.563754838804251e-05,
        "epoch": 0.22084367245657568,
        "step": 801
    },
    {
        "loss": 2.3216,
        "grad_norm": 1.3637751340866089,
        "learning_rate": 5.540320284233802e-05,
        "epoch": 0.221119382409705,
        "step": 802
    },
    {
        "loss": 1.2446,
        "grad_norm": 2.214850425720215,
        "learning_rate": 5.516916261874692e-05,
        "epoch": 0.2213950923628343,
        "step": 803
    },
    {
        "loss": 1.6722,
        "grad_norm": 1.9585189819335938,
        "learning_rate": 5.493542931957385e-05,
        "epoch": 0.22167080231596362,
        "step": 804
    },
    {
        "loss": 1.4291,
        "grad_norm": 1.8465955257415771,
        "learning_rate": 5.4702004545022256e-05,
        "epoch": 0.22194651226909293,
        "step": 805
    },
    {
        "loss": 2.1432,
        "grad_norm": 1.1075563430786133,
        "learning_rate": 5.44688898931832e-05,
        "epoch": 0.2222222222222222,
        "step": 806
    },
    {
        "loss": 1.7798,
        "grad_norm": 1.4161531925201416,
        "learning_rate": 5.423608696002461e-05,
        "epoch": 0.22249793217535152,
        "step": 807
    },
    {
        "loss": 2.2288,
        "grad_norm": 1.5431405305862427,
        "learning_rate": 5.400359733938028e-05,
        "epoch": 0.22277364212848083,
        "step": 808
    },
    {
        "loss": 2.1632,
        "grad_norm": 1.19547438621521,
        "learning_rate": 5.37714226229391e-05,
        "epoch": 0.22304935208161014,
        "step": 809
    },
    {
        "loss": 2.1023,
        "grad_norm": 1.7675732374191284,
        "learning_rate": 5.353956440023387e-05,
        "epoch": 0.22332506203473945,
        "step": 810
    },
    {
        "loss": 2.3805,
        "grad_norm": 1.4930742979049683,
        "learning_rate": 5.330802425863064e-05,
        "epoch": 0.22360077198786876,
        "step": 811
    },
    {
        "loss": 2.0469,
        "grad_norm": 1.356141448020935,
        "learning_rate": 5.307680378331788e-05,
        "epoch": 0.22387648194099807,
        "step": 812
    },
    {
        "loss": 2.0114,
        "grad_norm": 1.6907055377960205,
        "learning_rate": 5.284590455729541e-05,
        "epoch": 0.22415219189412738,
        "step": 813
    },
    {
        "loss": 1.9283,
        "grad_norm": 2.061115026473999,
        "learning_rate": 5.261532816136372e-05,
        "epoch": 0.2244279018472567,
        "step": 814
    },
    {
        "loss": 2.3086,
        "grad_norm": 1.2760931253433228,
        "learning_rate": 5.2385076174113215e-05,
        "epoch": 0.224703611800386,
        "step": 815
    },
    {
        "loss": 2.4434,
        "grad_norm": 1.626940131187439,
        "learning_rate": 5.215515017191316e-05,
        "epoch": 0.22497932175351532,
        "step": 816
    },
    {
        "loss": 1.952,
        "grad_norm": 1.5039759874343872,
        "learning_rate": 5.192555172890112e-05,
        "epoch": 0.2252550317066446,
        "step": 817
    },
    {
        "loss": 2.1089,
        "grad_norm": 2.0513839721679688,
        "learning_rate": 5.1696282416972066e-05,
        "epoch": 0.2255307416597739,
        "step": 818
    },
    {
        "loss": 2.5735,
        "grad_norm": 1.1183022260665894,
        "learning_rate": 5.1467343805767696e-05,
        "epoch": 0.22580645161290322,
        "step": 819
    },
    {
        "loss": 1.9448,
        "grad_norm": 1.9294404983520508,
        "learning_rate": 5.12387374626656e-05,
        "epoch": 0.22608216156603253,
        "step": 820
    },
    {
        "loss": 1.7687,
        "grad_norm": 1.6706180572509766,
        "learning_rate": 5.101046495276851e-05,
        "epoch": 0.22635787151916184,
        "step": 821
    },
    {
        "loss": 1.9701,
        "grad_norm": 1.6273040771484375,
        "learning_rate": 5.078252783889379e-05,
        "epoch": 0.22663358147229115,
        "step": 822
    },
    {
        "loss": 2.142,
        "grad_norm": 1.4212037324905396,
        "learning_rate": 5.055492768156247e-05,
        "epoch": 0.22690929142542046,
        "step": 823
    },
    {
        "loss": 1.4579,
        "grad_norm": 3.1514692306518555,
        "learning_rate": 5.0327666038988665e-05,
        "epoch": 0.22718500137854977,
        "step": 824
    },
    {
        "loss": 2.3136,
        "grad_norm": 1.2505779266357422,
        "learning_rate": 5.010074446706905e-05,
        "epoch": 0.22746071133167908,
        "step": 825
    },
    {
        "loss": 2.2708,
        "grad_norm": 1.3923816680908203,
        "learning_rate": 4.987416451937198e-05,
        "epoch": 0.2277364212848084,
        "step": 826
    },
    {
        "loss": 2.0116,
        "grad_norm": 1.5646591186523438,
        "learning_rate": 4.964792774712698e-05,
        "epoch": 0.22801213123793768,
        "step": 827
    },
    {
        "loss": 2.3692,
        "grad_norm": 0.9326833486557007,
        "learning_rate": 4.9422035699214054e-05,
        "epoch": 0.228287841191067,
        "step": 828
    },
    {
        "loss": 2.0502,
        "grad_norm": 1.8979613780975342,
        "learning_rate": 4.919648992215324e-05,
        "epoch": 0.2285635511441963,
        "step": 829
    },
    {
        "loss": 1.9441,
        "grad_norm": 1.3574813604354858,
        "learning_rate": 4.8971291960093826e-05,
        "epoch": 0.2288392610973256,
        "step": 830
    },
    {
        "loss": 1.7885,
        "grad_norm": 1.9731522798538208,
        "learning_rate": 4.874644335480383e-05,
        "epoch": 0.22911497105045492,
        "step": 831
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.2710152864456177,
        "learning_rate": 4.8521945645659595e-05,
        "epoch": 0.22939068100358423,
        "step": 832
    },
    {
        "loss": 2.2807,
        "grad_norm": 1.4145681858062744,
        "learning_rate": 4.829780036963504e-05,
        "epoch": 0.22966639095671354,
        "step": 833
    },
    {
        "loss": 1.6173,
        "grad_norm": 1.907967209815979,
        "learning_rate": 4.8074009061291194e-05,
        "epoch": 0.22994210090984285,
        "step": 834
    },
    {
        "loss": 2.356,
        "grad_norm": 1.683335781097412,
        "learning_rate": 4.785057325276585e-05,
        "epoch": 0.23021781086297216,
        "step": 835
    },
    {
        "loss": 2.3059,
        "grad_norm": 0.8241989612579346,
        "learning_rate": 4.7627494473762856e-05,
        "epoch": 0.23049352081610147,
        "step": 836
    },
    {
        "loss": 2.1135,
        "grad_norm": 1.0591068267822266,
        "learning_rate": 4.740477425154176e-05,
        "epoch": 0.23076923076923078,
        "step": 837
    },
    {
        "loss": 2.3094,
        "grad_norm": 1.531230092048645,
        "learning_rate": 4.718241411090729e-05,
        "epoch": 0.23104494072236006,
        "step": 838
    },
    {
        "loss": 1.6328,
        "grad_norm": 1.5979403257369995,
        "learning_rate": 4.696041557419907e-05,
        "epoch": 0.23132065067548938,
        "step": 839
    },
    {
        "loss": 2.1649,
        "grad_norm": 0.9715415835380554,
        "learning_rate": 4.673878016128096e-05,
        "epoch": 0.23159636062861869,
        "step": 840
    },
    {
        "loss": 2.2578,
        "grad_norm": 1.3860880136489868,
        "learning_rate": 4.65175093895308e-05,
        "epoch": 0.231872070581748,
        "step": 841
    },
    {
        "loss": 1.3968,
        "grad_norm": 1.50968599319458,
        "learning_rate": 4.6296604773830074e-05,
        "epoch": 0.2321477805348773,
        "step": 842
    },
    {
        "loss": 2.1876,
        "grad_norm": 1.909271240234375,
        "learning_rate": 4.607606782655338e-05,
        "epoch": 0.23242349048800662,
        "step": 843
    },
    {
        "loss": 2.4081,
        "grad_norm": 0.8994197249412537,
        "learning_rate": 4.5855900057558175e-05,
        "epoch": 0.23269920044113593,
        "step": 844
    },
    {
        "loss": 1.5284,
        "grad_norm": 2.167555809020996,
        "learning_rate": 4.563610297417437e-05,
        "epoch": 0.23297491039426524,
        "step": 845
    },
    {
        "loss": 2.2027,
        "grad_norm": 1.6608349084854126,
        "learning_rate": 4.541667808119419e-05,
        "epoch": 0.23325062034739455,
        "step": 846
    },
    {
        "loss": 2.0228,
        "grad_norm": 2.316188335418701,
        "learning_rate": 4.519762688086162e-05,
        "epoch": 0.23352633030052386,
        "step": 847
    },
    {
        "loss": 1.9237,
        "grad_norm": 1.3535308837890625,
        "learning_rate": 4.4978950872862255e-05,
        "epoch": 0.23380204025365317,
        "step": 848
    },
    {
        "loss": 2.2689,
        "grad_norm": 2.004051923751831,
        "learning_rate": 4.476065155431312e-05,
        "epoch": 0.23407775020678245,
        "step": 849
    },
    {
        "loss": 1.5358,
        "grad_norm": 2.8219878673553467,
        "learning_rate": 4.454273041975219e-05,
        "epoch": 0.23435346015991176,
        "step": 850
    },
    {
        "loss": 2.5055,
        "grad_norm": 1.0238006114959717,
        "learning_rate": 4.4325188961128307e-05,
        "epoch": 0.23462917011304107,
        "step": 851
    },
    {
        "loss": 2.0839,
        "grad_norm": 1.3822308778762817,
        "learning_rate": 4.410802866779105e-05,
        "epoch": 0.23490488006617039,
        "step": 852
    },
    {
        "loss": 1.9689,
        "grad_norm": 1.1407567262649536,
        "learning_rate": 4.389125102648034e-05,
        "epoch": 0.2351805900192997,
        "step": 853
    },
    {
        "loss": 2.0408,
        "grad_norm": 1.5366414785385132,
        "learning_rate": 4.367485752131637e-05,
        "epoch": 0.235456299972429,
        "step": 854
    },
    {
        "loss": 2.4485,
        "grad_norm": 0.8986397385597229,
        "learning_rate": 4.345884963378941e-05,
        "epoch": 0.23573200992555832,
        "step": 855
    },
    {
        "loss": 2.4353,
        "grad_norm": 1.2613577842712402,
        "learning_rate": 4.3243228842749776e-05,
        "epoch": 0.23600771987868763,
        "step": 856
    },
    {
        "loss": 2.2782,
        "grad_norm": 1.3033872842788696,
        "learning_rate": 4.3027996624397504e-05,
        "epoch": 0.23628342983181694,
        "step": 857
    },
    {
        "loss": 2.172,
        "grad_norm": 1.062787413597107,
        "learning_rate": 4.281315445227239e-05,
        "epoch": 0.23655913978494625,
        "step": 858
    },
    {
        "loss": 2.045,
        "grad_norm": 1.477637767791748,
        "learning_rate": 4.259870379724392e-05,
        "epoch": 0.23683484973807553,
        "step": 859
    },
    {
        "loss": 1.852,
        "grad_norm": 1.4508652687072754,
        "learning_rate": 4.238464612750107e-05,
        "epoch": 0.23711055969120484,
        "step": 860
    },
    {
        "loss": 2.3256,
        "grad_norm": 1.6022727489471436,
        "learning_rate": 4.2170982908542336e-05,
        "epoch": 0.23738626964433415,
        "step": 861
    },
    {
        "loss": 2.6562,
        "grad_norm": 1.5178020000457764,
        "learning_rate": 4.195771560316577e-05,
        "epoch": 0.23766197959746346,
        "step": 862
    },
    {
        "loss": 2.3547,
        "grad_norm": 1.4642434120178223,
        "learning_rate": 4.1744845671458796e-05,
        "epoch": 0.23793768955059277,
        "step": 863
    },
    {
        "loss": 2.5211,
        "grad_norm": 0.8381893038749695,
        "learning_rate": 4.1532374570788355e-05,
        "epoch": 0.23821339950372208,
        "step": 864
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.9220592975616455,
        "learning_rate": 4.132030375579084e-05,
        "epoch": 0.2384891094568514,
        "step": 865
    },
    {
        "loss": 2.0532,
        "grad_norm": 2.0315449237823486,
        "learning_rate": 4.110863467836227e-05,
        "epoch": 0.2387648194099807,
        "step": 866
    },
    {
        "loss": 2.4309,
        "grad_norm": 0.9963082075119019,
        "learning_rate": 4.089736878764815e-05,
        "epoch": 0.23904052936311002,
        "step": 867
    },
    {
        "loss": 2.2785,
        "grad_norm": 1.7890557050704956,
        "learning_rate": 4.068650753003368e-05,
        "epoch": 0.23931623931623933,
        "step": 868
    },
    {
        "loss": 2.2228,
        "grad_norm": 1.0723373889923096,
        "learning_rate": 4.047605234913391e-05,
        "epoch": 0.23959194926936864,
        "step": 869
    },
    {
        "loss": 1.943,
        "grad_norm": 1.6741149425506592,
        "learning_rate": 4.0266004685783646e-05,
        "epoch": 0.23986765922249792,
        "step": 870
    },
    {
        "loss": 2.1857,
        "grad_norm": 1.5296802520751953,
        "learning_rate": 4.005636597802784e-05,
        "epoch": 0.24014336917562723,
        "step": 871
    },
    {
        "loss": 2.2357,
        "grad_norm": 1.4164608716964722,
        "learning_rate": 3.984713766111152e-05,
        "epoch": 0.24041907912875654,
        "step": 872
    },
    {
        "loss": 2.4931,
        "grad_norm": 1.169486165046692,
        "learning_rate": 3.963832116747015e-05,
        "epoch": 0.24069478908188585,
        "step": 873
    },
    {
        "loss": 1.6762,
        "grad_norm": 1.811956763267517,
        "learning_rate": 3.9429917926719686e-05,
        "epoch": 0.24097049903501516,
        "step": 874
    },
    {
        "loss": 2.379,
        "grad_norm": 1.0870972871780396,
        "learning_rate": 3.9221929365646814e-05,
        "epoch": 0.24124620898814447,
        "step": 875
    },
    {
        "loss": 2.0662,
        "grad_norm": 1.8600504398345947,
        "learning_rate": 3.901435690819923e-05,
        "epoch": 0.24152191894127378,
        "step": 876
    },
    {
        "loss": 2.1504,
        "grad_norm": 1.0475435256958008,
        "learning_rate": 3.8807201975475945e-05,
        "epoch": 0.2417976288944031,
        "step": 877
    },
    {
        "loss": 2.4187,
        "grad_norm": 1.387861967086792,
        "learning_rate": 3.860046598571737e-05,
        "epoch": 0.2420733388475324,
        "step": 878
    },
    {
        "loss": 1.8054,
        "grad_norm": 0.8058510422706604,
        "learning_rate": 3.839415035429573e-05,
        "epoch": 0.24234904880066171,
        "step": 879
    },
    {
        "loss": 2.5207,
        "grad_norm": 1.3665715456008911,
        "learning_rate": 3.818825649370547e-05,
        "epoch": 0.24262475875379103,
        "step": 880
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.9118359088897705,
        "learning_rate": 3.798278581355334e-05,
        "epoch": 0.2429004687069203,
        "step": 881
    },
    {
        "loss": 2.4384,
        "grad_norm": 1.5780590772628784,
        "learning_rate": 3.777773972054889e-05,
        "epoch": 0.24317617866004962,
        "step": 882
    },
    {
        "loss": 1.5601,
        "grad_norm": 2.6823155879974365,
        "learning_rate": 3.757311961849497e-05,
        "epoch": 0.24345188861317893,
        "step": 883
    },
    {
        "loss": 2.6665,
        "grad_norm": 1.3275632858276367,
        "learning_rate": 3.73689269082778e-05,
        "epoch": 0.24372759856630824,
        "step": 884
    },
    {
        "loss": 1.8552,
        "grad_norm": 1.9942070245742798,
        "learning_rate": 3.7165162987857674e-05,
        "epoch": 0.24400330851943755,
        "step": 885
    },
    {
        "loss": 2.6021,
        "grad_norm": 1.1017253398895264,
        "learning_rate": 3.696182925225917e-05,
        "epoch": 0.24427901847256686,
        "step": 886
    },
    {
        "loss": 1.5217,
        "grad_norm": 1.8123680353164673,
        "learning_rate": 3.675892709356185e-05,
        "epoch": 0.24455472842569617,
        "step": 887
    },
    {
        "loss": 2.3068,
        "grad_norm": 1.1077529191970825,
        "learning_rate": 3.655645790089043e-05,
        "epoch": 0.24483043837882548,
        "step": 888
    },
    {
        "loss": 2.5376,
        "grad_norm": 1.0115731954574585,
        "learning_rate": 3.6354423060405465e-05,
        "epoch": 0.2451061483319548,
        "step": 889
    },
    {
        "loss": 1.3716,
        "grad_norm": 2.041815996170044,
        "learning_rate": 3.615282395529389e-05,
        "epoch": 0.2453818582850841,
        "step": 890
    },
    {
        "loss": 2.4158,
        "grad_norm": 1.3566004037857056,
        "learning_rate": 3.595166196575938e-05,
        "epoch": 0.2456575682382134,
        "step": 891
    },
    {
        "loss": 1.9223,
        "grad_norm": 1.1288654804229736,
        "learning_rate": 3.5750938469013006e-05,
        "epoch": 0.2459332781913427,
        "step": 892
    },
    {
        "loss": 2.1442,
        "grad_norm": 1.476241111755371,
        "learning_rate": 3.555065483926387e-05,
        "epoch": 0.246208988144472,
        "step": 893
    },
    {
        "loss": 2.3195,
        "grad_norm": 1.105933427810669,
        "learning_rate": 3.5350812447709544e-05,
        "epoch": 0.24648469809760132,
        "step": 894
    },
    {
        "loss": 2.1802,
        "grad_norm": 2.420610189437866,
        "learning_rate": 3.5151412662526786e-05,
        "epoch": 0.24676040805073063,
        "step": 895
    },
    {
        "loss": 1.3891,
        "grad_norm": 2.5624523162841797,
        "learning_rate": 3.495245684886211e-05,
        "epoch": 0.24703611800385994,
        "step": 896
    },
    {
        "loss": 2.3953,
        "grad_norm": 1.1597434282302856,
        "learning_rate": 3.4753946368822586e-05,
        "epoch": 0.24731182795698925,
        "step": 897
    },
    {
        "loss": 2.0393,
        "grad_norm": 1.2780135869979858,
        "learning_rate": 3.4555882581466295e-05,
        "epoch": 0.24758753791011856,
        "step": 898
    },
    {
        "loss": 1.5465,
        "grad_norm": 2.2610182762145996,
        "learning_rate": 3.435826684279314e-05,
        "epoch": 0.24786324786324787,
        "step": 899
    },
    {
        "loss": 2.4651,
        "grad_norm": 1.4590165615081787,
        "learning_rate": 3.4161100505735665e-05,
        "epoch": 0.24813895781637718,
        "step": 900
    },
    {
        "loss": 2.0352,
        "grad_norm": 1.3325896263122559,
        "learning_rate": 3.396438492014957e-05,
        "epoch": 0.2484146677695065,
        "step": 901
    },
    {
        "loss": 2.4457,
        "grad_norm": 1.2132586240768433,
        "learning_rate": 3.3768121432804614e-05,
        "epoch": 0.24869037772263577,
        "step": 902
    },
    {
        "loss": 2.4821,
        "grad_norm": 1.0631755590438843,
        "learning_rate": 3.357231138737542e-05,
        "epoch": 0.24896608767576509,
        "step": 903
    },
    {
        "loss": 1.7427,
        "grad_norm": 1.528784155845642,
        "learning_rate": 3.337695612443216e-05,
        "epoch": 0.2492417976288944,
        "step": 904
    },
    {
        "loss": 2.5398,
        "grad_norm": 1.0361703634262085,
        "learning_rate": 3.318205698143144e-05,
        "epoch": 0.2495175075820237,
        "step": 905
    },
    {
        "loss": 1.5506,
        "grad_norm": 1.707614541053772,
        "learning_rate": 3.298761529270714e-05,
        "epoch": 0.24979321753515302,
        "step": 906
    },
    {
        "loss": 2.1678,
        "grad_norm": 1.8327044248580933,
        "learning_rate": 3.279363238946138e-05,
        "epoch": 0.2500689274882823,
        "step": 907
    },
    {
        "loss": 1.8811,
        "grad_norm": 1.4423969984054565,
        "learning_rate": 3.260010959975517e-05,
        "epoch": 0.2503446374414116,
        "step": 908
    },
    {
        "loss": 2.2947,
        "grad_norm": 1.4915562868118286,
        "learning_rate": 3.2407048248499485e-05,
        "epoch": 0.2506203473945409,
        "step": 909
    },
    {
        "loss": 2.0617,
        "grad_norm": 2.0012009143829346,
        "learning_rate": 3.2214449657446254e-05,
        "epoch": 0.25089605734767023,
        "step": 910
    },
    {
        "loss": 1.8662,
        "grad_norm": 1.5428410768508911,
        "learning_rate": 3.2022315145179124e-05,
        "epoch": 0.25117176730079954,
        "step": 911
    },
    {
        "loss": 1.9331,
        "grad_norm": 1.9675010442733765,
        "learning_rate": 3.183064602710457e-05,
        "epoch": 0.25144747725392885,
        "step": 912
    },
    {
        "loss": 1.9957,
        "grad_norm": 1.6295355558395386,
        "learning_rate": 3.16394436154428e-05,
        "epoch": 0.25172318720705816,
        "step": 913
    },
    {
        "loss": 2.2933,
        "grad_norm": 1.3560746908187866,
        "learning_rate": 3.144870921921891e-05,
        "epoch": 0.2519988971601875,
        "step": 914
    },
    {
        "loss": 1.555,
        "grad_norm": 2.3046882152557373,
        "learning_rate": 3.1258444144253776e-05,
        "epoch": 0.2522746071133168,
        "step": 915
    },
    {
        "loss": 2.4082,
        "grad_norm": 1.0059269666671753,
        "learning_rate": 3.106864969315512e-05,
        "epoch": 0.2525503170664461,
        "step": 916
    },
    {
        "loss": 1.93,
        "grad_norm": 1.062855839729309,
        "learning_rate": 3.087932716530876e-05,
        "epoch": 0.2528260270195754,
        "step": 917
    },
    {
        "loss": 2.4548,
        "grad_norm": 0.842657208442688,
        "learning_rate": 3.0690477856869525e-05,
        "epoch": 0.2531017369727047,
        "step": 918
    },
    {
        "loss": 2.5964,
        "grad_norm": 1.3413136005401611,
        "learning_rate": 3.0502103060752397e-05,
        "epoch": 0.253377446925834,
        "step": 919
    },
    {
        "loss": 1.8295,
        "grad_norm": 2.2521743774414062,
        "learning_rate": 3.0314204066623864e-05,
        "epoch": 0.25365315687896334,
        "step": 920
    },
    {
        "loss": 1.7001,
        "grad_norm": 2.473510265350342,
        "learning_rate": 3.012678216089281e-05,
        "epoch": 0.25392886683209265,
        "step": 921
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.080356478691101,
        "learning_rate": 2.9939838626701888e-05,
        "epoch": 0.25420457678522196,
        "step": 922
    },
    {
        "loss": 2.115,
        "grad_norm": 1.715017557144165,
        "learning_rate": 2.9753374743918637e-05,
        "epoch": 0.25448028673835127,
        "step": 923
    },
    {
        "loss": 2.3168,
        "grad_norm": 1.2388099431991577,
        "learning_rate": 2.9567391789126884e-05,
        "epoch": 0.2547559966914806,
        "step": 924
    },
    {
        "loss": 2.6314,
        "grad_norm": 1.3229612112045288,
        "learning_rate": 2.9381891035617792e-05,
        "epoch": 0.2550317066446099,
        "step": 925
    },
    {
        "loss": 2.5374,
        "grad_norm": 1.2885477542877197,
        "learning_rate": 2.9196873753381215e-05,
        "epoch": 0.2553074165977392,
        "step": 926
    },
    {
        "loss": 2.1399,
        "grad_norm": 1.557673692703247,
        "learning_rate": 2.901234120909715e-05,
        "epoch": 0.2555831265508685,
        "step": 927
    },
    {
        "loss": 2.4658,
        "grad_norm": 2.024247646331787,
        "learning_rate": 2.8828294666126832e-05,
        "epoch": 0.2558588365039978,
        "step": 928
    },
    {
        "loss": 2.3666,
        "grad_norm": 1.2476047277450562,
        "learning_rate": 2.864473538450422e-05,
        "epoch": 0.2561345464571271,
        "step": 929
    },
    {
        "loss": 1.6824,
        "grad_norm": 1.5638885498046875,
        "learning_rate": 2.8461664620927396e-05,
        "epoch": 0.2564102564102564,
        "step": 930
    },
    {
        "loss": 1.4534,
        "grad_norm": 1.5668916702270508,
        "learning_rate": 2.8279083628749858e-05,
        "epoch": 0.2566859663633857,
        "step": 931
    },
    {
        "loss": 2.2514,
        "grad_norm": 1.1952333450317383,
        "learning_rate": 2.8096993657972015e-05,
        "epoch": 0.256961676316515,
        "step": 932
    },
    {
        "loss": 2.4229,
        "grad_norm": 2.5380210876464844,
        "learning_rate": 2.791539595523255e-05,
        "epoch": 0.2572373862696443,
        "step": 933
    },
    {
        "loss": 2.118,
        "grad_norm": 1.7802455425262451,
        "learning_rate": 2.773429176380007e-05,
        "epoch": 0.25751309622277363,
        "step": 934
    },
    {
        "loss": 2.3338,
        "grad_norm": 1.3103764057159424,
        "learning_rate": 2.7553682323564344e-05,
        "epoch": 0.25778880617590294,
        "step": 935
    },
    {
        "loss": 2.5452,
        "grad_norm": 1.2519476413726807,
        "learning_rate": 2.7373568871027955e-05,
        "epoch": 0.25806451612903225,
        "step": 936
    },
    {
        "loss": 2.1719,
        "grad_norm": 1.5121861696243286,
        "learning_rate": 2.7193952639297917e-05,
        "epoch": 0.25834022608216156,
        "step": 937
    },
    {
        "loss": 1.7352,
        "grad_norm": 1.7515164613723755,
        "learning_rate": 2.7014834858076997e-05,
        "epoch": 0.25861593603529087,
        "step": 938
    },
    {
        "loss": 1.9945,
        "grad_norm": 1.1989705562591553,
        "learning_rate": 2.6836216753655476e-05,
        "epoch": 0.2588916459884202,
        "step": 939
    },
    {
        "loss": 2.4599,
        "grad_norm": 1.3236156702041626,
        "learning_rate": 2.6658099548902783e-05,
        "epoch": 0.2591673559415495,
        "step": 940
    },
    {
        "loss": 1.8298,
        "grad_norm": 1.9899781942367554,
        "learning_rate": 2.6480484463258935e-05,
        "epoch": 0.2594430658946788,
        "step": 941
    },
    {
        "loss": 1.9482,
        "grad_norm": 2.464407444000244,
        "learning_rate": 2.6303372712726393e-05,
        "epoch": 0.2597187758478081,
        "step": 942
    },
    {
        "loss": 2.6449,
        "grad_norm": 1.669865369796753,
        "learning_rate": 2.6126765509861538e-05,
        "epoch": 0.2599944858009374,
        "step": 943
    },
    {
        "loss": 1.7808,
        "grad_norm": 1.8048722743988037,
        "learning_rate": 2.5950664063766626e-05,
        "epoch": 0.26027019575406674,
        "step": 944
    },
    {
        "loss": 2.3246,
        "grad_norm": 1.7505916357040405,
        "learning_rate": 2.5775069580081246e-05,
        "epoch": 0.26054590570719605,
        "step": 945
    },
    {
        "loss": 2.0442,
        "grad_norm": 1.3170396089553833,
        "learning_rate": 2.559998326097418e-05,
        "epoch": 0.26082161566032536,
        "step": 946
    },
    {
        "loss": 1.6414,
        "grad_norm": 1.848419189453125,
        "learning_rate": 2.542540630513527e-05,
        "epoch": 0.26109732561345467,
        "step": 947
    },
    {
        "loss": 2.211,
        "grad_norm": 0.841842532157898,
        "learning_rate": 2.5251339907767012e-05,
        "epoch": 0.261373035566584,
        "step": 948
    },
    {
        "loss": 2.521,
        "grad_norm": 1.2708810567855835,
        "learning_rate": 2.5077785260576535e-05,
        "epoch": 0.2616487455197133,
        "step": 949
    },
    {
        "loss": 2.0365,
        "grad_norm": 1.234590768814087,
        "learning_rate": 2.490474355176734e-05,
        "epoch": 0.26192445547284254,
        "step": 950
    },
    {
        "loss": 1.9571,
        "grad_norm": 1.7593556642532349,
        "learning_rate": 2.473221596603127e-05,
        "epoch": 0.26220016542597185,
        "step": 951
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.845773458480835,
        "learning_rate": 2.4560203684540295e-05,
        "epoch": 0.26247587537910116,
        "step": 952
    },
    {
        "loss": 1.6745,
        "grad_norm": 1.8332901000976562,
        "learning_rate": 2.4388707884938432e-05,
        "epoch": 0.2627515853322305,
        "step": 953
    },
    {
        "loss": 2.5447,
        "grad_norm": 0.9898735880851746,
        "learning_rate": 2.421772974133384e-05,
        "epoch": 0.2630272952853598,
        "step": 954
    },
    {
        "loss": 1.8302,
        "grad_norm": 1.9540271759033203,
        "learning_rate": 2.4047270424290523e-05,
        "epoch": 0.2633030052384891,
        "step": 955
    },
    {
        "loss": 2.2122,
        "grad_norm": 1.54735267162323,
        "learning_rate": 2.387733110082052e-05,
        "epoch": 0.2635787151916184,
        "step": 956
    },
    {
        "loss": 1.4289,
        "grad_norm": 2.0261800289154053,
        "learning_rate": 2.3707912934375888e-05,
        "epoch": 0.2638544251447477,
        "step": 957
    },
    {
        "loss": 2.5534,
        "grad_norm": 1.354560136795044,
        "learning_rate": 2.3539017084840608e-05,
        "epoch": 0.26413013509787703,
        "step": 958
    },
    {
        "loss": 1.4081,
        "grad_norm": 1.6980799436569214,
        "learning_rate": 2.3370644708522803e-05,
        "epoch": 0.26440584505100634,
        "step": 959
    },
    {
        "loss": 2.2399,
        "grad_norm": 1.3791922330856323,
        "learning_rate": 2.320279695814669e-05,
        "epoch": 0.26468155500413565,
        "step": 960
    },
    {
        "loss": 2.2302,
        "grad_norm": 1.3644570112228394,
        "learning_rate": 2.303547498284483e-05,
        "epoch": 0.26495726495726496,
        "step": 961
    },
    {
        "loss": 2.4948,
        "grad_norm": 1.4159530401229858,
        "learning_rate": 2.2868679928150138e-05,
        "epoch": 0.26523297491039427,
        "step": 962
    },
    {
        "loss": 1.5036,
        "grad_norm": 1.8540312051773071,
        "learning_rate": 2.2702412935988037e-05,
        "epoch": 0.2655086848635236,
        "step": 963
    },
    {
        "loss": 2.3555,
        "grad_norm": 1.0909699201583862,
        "learning_rate": 2.2536675144668805e-05,
        "epoch": 0.2657843948166529,
        "step": 964
    },
    {
        "loss": 2.6067,
        "grad_norm": 1.3872779607772827,
        "learning_rate": 2.2371467688879554e-05,
        "epoch": 0.2660601047697822,
        "step": 965
    },
    {
        "loss": 2.2929,
        "grad_norm": 1.2504135370254517,
        "learning_rate": 2.2206791699676588e-05,
        "epoch": 0.2663358147229115,
        "step": 966
    },
    {
        "loss": 2.5343,
        "grad_norm": 1.4018795490264893,
        "learning_rate": 2.204264830447772e-05,
        "epoch": 0.2666115246760408,
        "step": 967
    },
    {
        "loss": 2.4669,
        "grad_norm": 1.1404105424880981,
        "learning_rate": 2.1879038627054394e-05,
        "epoch": 0.26688723462917013,
        "step": 968
    },
    {
        "loss": 2.2528,
        "grad_norm": 1.3656591176986694,
        "learning_rate": 2.1715963787524098e-05,
        "epoch": 0.26716294458229944,
        "step": 969
    },
    {
        "loss": 1.9078,
        "grad_norm": 1.4330493211746216,
        "learning_rate": 2.1553424902342635e-05,
        "epoch": 0.26743865453542875,
        "step": 970
    },
    {
        "loss": 2.3012,
        "grad_norm": 1.397774338722229,
        "learning_rate": 2.1391423084296623e-05,
        "epoch": 0.267714364488558,
        "step": 971
    },
    {
        "loss": 1.6877,
        "grad_norm": 2.0350515842437744,
        "learning_rate": 2.1229959442495663e-05,
        "epoch": 0.2679900744416873,
        "step": 972
    },
    {
        "loss": 2.0066,
        "grad_norm": 2.062812328338623,
        "learning_rate": 2.1069035082364864e-05,
        "epoch": 0.26826578439481663,
        "step": 973
    },
    {
        "loss": 2.115,
        "grad_norm": 1.82650625705719,
        "learning_rate": 2.0908651105637332e-05,
        "epoch": 0.26854149434794594,
        "step": 974
    },
    {
        "loss": 1.6575,
        "grad_norm": 1.6623728275299072,
        "learning_rate": 2.0748808610346492e-05,
        "epoch": 0.26881720430107525,
        "step": 975
    },
    {
        "loss": 2.1448,
        "grad_norm": 1.5041730403900146,
        "learning_rate": 2.0589508690818617e-05,
        "epoch": 0.26909291425420456,
        "step": 976
    },
    {
        "loss": 2.2495,
        "grad_norm": 1.4667102098464966,
        "learning_rate": 2.0430752437665447e-05,
        "epoch": 0.2693686242073339,
        "step": 977
    },
    {
        "loss": 2.07,
        "grad_norm": 1.8228360414505005,
        "learning_rate": 2.027254093777655e-05,
        "epoch": 0.2696443341604632,
        "step": 978
    },
    {
        "loss": 2.7689,
        "grad_norm": 1.5961260795593262,
        "learning_rate": 2.0114875274311994e-05,
        "epoch": 0.2699200441135925,
        "step": 979
    },
    {
        "loss": 2.2827,
        "grad_norm": 1.5821114778518677,
        "learning_rate": 1.9957756526694848e-05,
        "epoch": 0.2701957540667218,
        "step": 980
    },
    {
        "loss": 2.0264,
        "grad_norm": 1.2514187097549438,
        "learning_rate": 1.980118577060397e-05,
        "epoch": 0.2704714640198511,
        "step": 981
    },
    {
        "loss": 2.1063,
        "grad_norm": 1.5407387018203735,
        "learning_rate": 1.9645164077966372e-05,
        "epoch": 0.2707471739729804,
        "step": 982
    },
    {
        "loss": 2.2637,
        "grad_norm": 1.6432826519012451,
        "learning_rate": 1.9489692516950065e-05,
        "epoch": 0.27102288392610974,
        "step": 983
    },
    {
        "loss": 2.5006,
        "grad_norm": 0.998524010181427,
        "learning_rate": 1.9334772151956782e-05,
        "epoch": 0.27129859387923905,
        "step": 984
    },
    {
        "loss": 2.2844,
        "grad_norm": 1.2615207433700562,
        "learning_rate": 1.918040404361453e-05,
        "epoch": 0.27157430383236836,
        "step": 985
    },
    {
        "loss": 1.8731,
        "grad_norm": 2.0292418003082275,
        "learning_rate": 1.902658924877043e-05,
        "epoch": 0.27185001378549767,
        "step": 986
    },
    {
        "loss": 2.1224,
        "grad_norm": 1.892976999282837,
        "learning_rate": 1.887332882048347e-05,
        "epoch": 0.272125723738627,
        "step": 987
    },
    {
        "loss": 2.0812,
        "grad_norm": 1.7951589822769165,
        "learning_rate": 1.8720623808017323e-05,
        "epoch": 0.2724014336917563,
        "step": 988
    },
    {
        "loss": 2.1943,
        "grad_norm": 1.0014605522155762,
        "learning_rate": 1.8568475256833073e-05,
        "epoch": 0.2726771436448856,
        "step": 989
    },
    {
        "loss": 1.9716,
        "grad_norm": 1.7017379999160767,
        "learning_rate": 1.841688420858213e-05,
        "epoch": 0.2729528535980149,
        "step": 990
    },
    {
        "loss": 2.161,
        "grad_norm": 1.9400029182434082,
        "learning_rate": 1.8265851701099145e-05,
        "epoch": 0.2732285635511442,
        "step": 991
    },
    {
        "loss": 1.8268,
        "grad_norm": 1.8009483814239502,
        "learning_rate": 1.8115378768394774e-05,
        "epoch": 0.27350427350427353,
        "step": 992
    },
    {
        "loss": 2.3715,
        "grad_norm": 1.116438865661621,
        "learning_rate": 1.7965466440648636e-05,
        "epoch": 0.2737799834574028,
        "step": 993
    },
    {
        "loss": 2.2918,
        "grad_norm": 0.9171891212463379,
        "learning_rate": 1.7816115744202434e-05,
        "epoch": 0.2740556934105321,
        "step": 994
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.6654400825500488,
        "learning_rate": 1.766732770155264e-05,
        "epoch": 0.2743314033636614,
        "step": 995
    },
    {
        "loss": 1.7253,
        "grad_norm": 1.548516035079956,
        "learning_rate": 1.75191033313437e-05,
        "epoch": 0.2746071133167907,
        "step": 996
    },
    {
        "loss": 2.1397,
        "grad_norm": 1.5313756465911865,
        "learning_rate": 1.7371443648360995e-05,
        "epoch": 0.27488282326992003,
        "step": 997
    },
    {
        "loss": 2.4324,
        "grad_norm": 1.205892562866211,
        "learning_rate": 1.722434966352393e-05,
        "epoch": 0.27515853322304934,
        "step": 998
    },
    {
        "loss": 2.084,
        "grad_norm": 2.1819002628326416,
        "learning_rate": 1.7077822383878948e-05,
        "epoch": 0.27543424317617865,
        "step": 999
    },
    {
        "loss": 2.256,
        "grad_norm": 1.3060282468795776,
        "learning_rate": 1.6931862812592648e-05,
        "epoch": 0.27570995312930796,
        "step": 1000
    },
    {
        "loss": 2.356,
        "grad_norm": 1.522999882698059,
        "learning_rate": 1.6786471948945027e-05,
        "epoch": 0.27598566308243727,
        "step": 1001
    },
    {
        "loss": 2.3981,
        "grad_norm": 1.7106564044952393,
        "learning_rate": 1.6641650788322448e-05,
        "epoch": 0.2762613730355666,
        "step": 1002
    },
    {
        "loss": 2.3448,
        "grad_norm": 1.1795547008514404,
        "learning_rate": 1.6497400322210966e-05,
        "epoch": 0.2765370829886959,
        "step": 1003
    },
    {
        "loss": 2.2621,
        "grad_norm": 1.4431687593460083,
        "learning_rate": 1.635372153818956e-05,
        "epoch": 0.2768127929418252,
        "step": 1004
    },
    {
        "loss": 2.3324,
        "grad_norm": 1.0848020315170288,
        "learning_rate": 1.6210615419923226e-05,
        "epoch": 0.2770885028949545,
        "step": 1005
    },
    {
        "loss": 2.6226,
        "grad_norm": 0.9824405312538147,
        "learning_rate": 1.6068082947156382e-05,
        "epoch": 0.2773642128480838,
        "step": 1006
    },
    {
        "loss": 2.3842,
        "grad_norm": 1.4214177131652832,
        "learning_rate": 1.5926125095706068e-05,
        "epoch": 0.27763992280121313,
        "step": 1007
    },
    {
        "loss": 2.0151,
        "grad_norm": 1.6843743324279785,
        "learning_rate": 1.578474283745538e-05,
        "epoch": 0.27791563275434245,
        "step": 1008
    },
    {
        "loss": 1.7992,
        "grad_norm": 1.9422365427017212,
        "learning_rate": 1.5643937140346677e-05,
        "epoch": 0.27819134270747176,
        "step": 1009
    },
    {
        "loss": 1.5709,
        "grad_norm": 2.178387403488159,
        "learning_rate": 1.5503708968374998e-05,
        "epoch": 0.27846705266060107,
        "step": 1010
    },
    {
        "loss": 2.3591,
        "grad_norm": 1.0026050806045532,
        "learning_rate": 1.5364059281581587e-05,
        "epoch": 0.2787427626137304,
        "step": 1011
    },
    {
        "loss": 2.1984,
        "grad_norm": 1.0280208587646484,
        "learning_rate": 1.5224989036047133e-05,
        "epoch": 0.2790184725668597,
        "step": 1012
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.5385758876800537,
        "learning_rate": 1.5086499183885294e-05,
        "epoch": 0.279294182519989,
        "step": 1013
    },
    {
        "loss": 2.0722,
        "grad_norm": 1.75330650806427,
        "learning_rate": 1.4948590673236285e-05,
        "epoch": 0.27956989247311825,
        "step": 1014
    },
    {
        "loss": 2.0662,
        "grad_norm": 1.5594933032989502,
        "learning_rate": 1.481126444826021e-05,
        "epoch": 0.27984560242624756,
        "step": 1015
    },
    {
        "loss": 2.1998,
        "grad_norm": 1.0978535413742065,
        "learning_rate": 1.4674521449130718e-05,
        "epoch": 0.2801213123793769,
        "step": 1016
    },
    {
        "loss": 1.7725,
        "grad_norm": 1.4431326389312744,
        "learning_rate": 1.45383626120285e-05,
        "epoch": 0.2803970223325062,
        "step": 1017
    },
    {
        "loss": 1.7338,
        "grad_norm": 2.486032724380493,
        "learning_rate": 1.4402788869134975e-05,
        "epoch": 0.2806727322856355,
        "step": 1018
    },
    {
        "loss": 2.2724,
        "grad_norm": 1.1111807823181152,
        "learning_rate": 1.4267801148625792e-05,
        "epoch": 0.2809484422387648,
        "step": 1019
    },
    {
        "loss": 1.9533,
        "grad_norm": 1.5226908922195435,
        "learning_rate": 1.4133400374664507e-05,
        "epoch": 0.2812241521918941,
        "step": 1020
    },
    {
        "loss": 2.186,
        "grad_norm": 1.8640022277832031,
        "learning_rate": 1.3999587467396358e-05,
        "epoch": 0.2814998621450234,
        "step": 1021
    },
    {
        "loss": 1.7425,
        "grad_norm": 1.9520320892333984,
        "learning_rate": 1.3866363342941813e-05,
        "epoch": 0.28177557209815274,
        "step": 1022
    },
    {
        "loss": 2.3189,
        "grad_norm": 1.3612059354782104,
        "learning_rate": 1.3733728913390398e-05,
        "epoch": 0.28205128205128205,
        "step": 1023
    },
    {
        "loss": 2.6879,
        "grad_norm": 1.723048448562622,
        "learning_rate": 1.360168508679438e-05,
        "epoch": 0.28232699200441136,
        "step": 1024
    },
    {
        "loss": 2.35,
        "grad_norm": 0.9326004981994629,
        "learning_rate": 1.3470232767162671e-05,
        "epoch": 0.28260270195754067,
        "step": 1025
    },
    {
        "loss": 2.2047,
        "grad_norm": 1.369196891784668,
        "learning_rate": 1.3339372854454513e-05,
        "epoch": 0.28287841191067,
        "step": 1026
    },
    {
        "loss": 1.8036,
        "grad_norm": 2.1887567043304443,
        "learning_rate": 1.3209106244573344e-05,
        "epoch": 0.2831541218637993,
        "step": 1027
    },
    {
        "loss": 1.4145,
        "grad_norm": 2.0709266662597656,
        "learning_rate": 1.3079433829360754e-05,
        "epoch": 0.2834298318169286,
        "step": 1028
    },
    {
        "loss": 2.1774,
        "grad_norm": 0.9396743774414062,
        "learning_rate": 1.2950356496590233e-05,
        "epoch": 0.2837055417700579,
        "step": 1029
    },
    {
        "loss": 1.9847,
        "grad_norm": 1.8233497142791748,
        "learning_rate": 1.2821875129961214e-05,
        "epoch": 0.2839812517231872,
        "step": 1030
    },
    {
        "loss": 1.3123,
        "grad_norm": 2.6158461570739746,
        "learning_rate": 1.2693990609092986e-05,
        "epoch": 0.28425696167631653,
        "step": 1031
    },
    {
        "loss": 1.9992,
        "grad_norm": 0.8861050009727478,
        "learning_rate": 1.2566703809518654e-05,
        "epoch": 0.28453267162944584,
        "step": 1032
    },
    {
        "loss": 2.4118,
        "grad_norm": 1.1189985275268555,
        "learning_rate": 1.2440015602679145e-05,
        "epoch": 0.28480838158257515,
        "step": 1033
    },
    {
        "loss": 2.1778,
        "grad_norm": 1.5226728916168213,
        "learning_rate": 1.2313926855917236e-05,
        "epoch": 0.28508409153570446,
        "step": 1034
    },
    {
        "loss": 2.1759,
        "grad_norm": 1.0324320793151855,
        "learning_rate": 1.2188438432471705e-05,
        "epoch": 0.2853598014888337,
        "step": 1035
    },
    {
        "loss": 2.1002,
        "grad_norm": 1.375654697418213,
        "learning_rate": 1.2063551191471278e-05,
        "epoch": 0.28563551144196303,
        "step": 1036
    },
    {
        "loss": 2.4068,
        "grad_norm": 1.022477626800537,
        "learning_rate": 1.1939265987928826e-05,
        "epoch": 0.28591122139509234,
        "step": 1037
    },
    {
        "loss": 2.4741,
        "grad_norm": 1.3013828992843628,
        "learning_rate": 1.1815583672735575e-05,
        "epoch": 0.28618693134822165,
        "step": 1038
    },
    {
        "loss": 2.3612,
        "grad_norm": 1.6696346998214722,
        "learning_rate": 1.1692505092655071e-05,
        "epoch": 0.28646264130135096,
        "step": 1039
    },
    {
        "loss": 1.832,
        "grad_norm": 2.0733208656311035,
        "learning_rate": 1.1570031090317713e-05,
        "epoch": 0.2867383512544803,
        "step": 1040
    },
    {
        "loss": 2.2688,
        "grad_norm": 1.5301629304885864,
        "learning_rate": 1.1448162504214621e-05,
        "epoch": 0.2870140612076096,
        "step": 1041
    },
    {
        "loss": 2.4147,
        "grad_norm": 1.085423231124878,
        "learning_rate": 1.1326900168692112e-05,
        "epoch": 0.2872897711607389,
        "step": 1042
    },
    {
        "loss": 1.7048,
        "grad_norm": 2.2752788066864014,
        "learning_rate": 1.1206244913946017e-05,
        "epoch": 0.2875654811138682,
        "step": 1043
    },
    {
        "loss": 2.1132,
        "grad_norm": 1.7077665328979492,
        "learning_rate": 1.1086197566015832e-05,
        "epoch": 0.2878411910669975,
        "step": 1044
    },
    {
        "loss": 2.1529,
        "grad_norm": 1.5524710416793823,
        "learning_rate": 1.0966758946779176e-05,
        "epoch": 0.2881169010201268,
        "step": 1045
    },
    {
        "loss": 2.4174,
        "grad_norm": 0.7511825561523438,
        "learning_rate": 1.0847929873946183e-05,
        "epoch": 0.28839261097325614,
        "step": 1046
    },
    {
        "loss": 1.8002,
        "grad_norm": 1.5492833852767944,
        "learning_rate": 1.0729711161053823e-05,
        "epoch": 0.28866832092638545,
        "step": 1047
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.233806848526001,
        "learning_rate": 1.0612103617460379e-05,
        "epoch": 0.28894403087951476,
        "step": 1048
    },
    {
        "loss": 2.1198,
        "grad_norm": 1.8394038677215576,
        "learning_rate": 1.0495108048339875e-05,
        "epoch": 0.28921974083264407,
        "step": 1049
    },
    {
        "loss": 2.4518,
        "grad_norm": 0.9880228638648987,
        "learning_rate": 1.0378725254676657e-05,
        "epoch": 0.2894954507857734,
        "step": 1050
    },
    {
        "loss": 2.3436,
        "grad_norm": 1.0702073574066162,
        "learning_rate": 1.0262956033259775e-05,
        "epoch": 0.2897711607389027,
        "step": 1051
    },
    {
        "loss": 2.4024,
        "grad_norm": 1.2762458324432373,
        "learning_rate": 1.0147801176677585e-05,
        "epoch": 0.290046870692032,
        "step": 1052
    },
    {
        "loss": 2.3549,
        "grad_norm": 1.4478957653045654,
        "learning_rate": 1.0033261473312405e-05,
        "epoch": 0.2903225806451613,
        "step": 1053
    },
    {
        "loss": 1.9371,
        "grad_norm": 1.577974557876587,
        "learning_rate": 9.919337707334974e-06,
        "epoch": 0.2905982905982906,
        "step": 1054
    },
    {
        "loss": 2.5353,
        "grad_norm": 1.2758152484893799,
        "learning_rate": 9.80603065869915e-06,
        "epoch": 0.29087400055141993,
        "step": 1055
    },
    {
        "loss": 1.9061,
        "grad_norm": 1.7738369703292847,
        "learning_rate": 9.693341103136632e-06,
        "epoch": 0.29114971050454924,
        "step": 1056
    },
    {
        "loss": 1.4962,
        "grad_norm": 2.116142988204956,
        "learning_rate": 9.581269812151539e-06,
        "epoch": 0.2914254204576785,
        "step": 1057
    },
    {
        "loss": 2.1442,
        "grad_norm": 0.9757530689239502,
        "learning_rate": 9.46981755301518e-06,
        "epoch": 0.2917011304108078,
        "step": 1058
    },
    {
        "loss": 2.4776,
        "grad_norm": 1.5750528573989868,
        "learning_rate": 9.358985088760796e-06,
        "epoch": 0.2919768403639371,
        "step": 1059
    },
    {
        "loss": 1.7405,
        "grad_norm": 2.350599765777588,
        "learning_rate": 9.248773178178405e-06,
        "epoch": 0.29225255031706643,
        "step": 1060
    },
    {
        "loss": 2.3918,
        "grad_norm": 1.7894972562789917,
        "learning_rate": 9.139182575809446e-06,
        "epoch": 0.29252826027019574,
        "step": 1061
    },
    {
        "loss": 2.5793,
        "grad_norm": 0.9880396723747253,
        "learning_rate": 9.030214031941753e-06,
        "epoch": 0.29280397022332505,
        "step": 1062
    },
    {
        "loss": 1.7996,
        "grad_norm": 1.8779932260513306,
        "learning_rate": 8.921868292604407e-06,
        "epoch": 0.29307968017645436,
        "step": 1063
    },
    {
        "loss": 2.3034,
        "grad_norm": 1.1873643398284912,
        "learning_rate": 8.814146099562536e-06,
        "epoch": 0.29335539012958367,
        "step": 1064
    },
    {
        "loss": 1.762,
        "grad_norm": 1.6000937223434448,
        "learning_rate": 8.707048190312317e-06,
        "epoch": 0.293631100082713,
        "step": 1065
    },
    {
        "loss": 2.2008,
        "grad_norm": 1.8282437324523926,
        "learning_rate": 8.60057529807593e-06,
        "epoch": 0.2939068100358423,
        "step": 1066
    },
    {
        "loss": 2.3704,
        "grad_norm": 1.0534794330596924,
        "learning_rate": 8.494728151796493e-06,
        "epoch": 0.2941825199889716,
        "step": 1067
    },
    {
        "loss": 2.5764,
        "grad_norm": 1.3741109371185303,
        "learning_rate": 8.389507476133085e-06,
        "epoch": 0.2944582299421009,
        "step": 1068
    },
    {
        "loss": 2.3391,
        "grad_norm": 1.2142471075057983,
        "learning_rate": 8.28491399145579e-06,
        "epoch": 0.2947339398952302,
        "step": 1069
    },
    {
        "loss": 2.5667,
        "grad_norm": 1.225544810295105,
        "learning_rate": 8.180948413840816e-06,
        "epoch": 0.29500964984835953,
        "step": 1070
    },
    {
        "loss": 1.9266,
        "grad_norm": 1.6868922710418701,
        "learning_rate": 8.077611455065492e-06,
        "epoch": 0.29528535980148884,
        "step": 1071
    },
    {
        "loss": 2.4908,
        "grad_norm": 0.9507197141647339,
        "learning_rate": 7.97490382260343e-06,
        "epoch": 0.29556106975461816,
        "step": 1072
    },
    {
        "loss": 2.1233,
        "grad_norm": 1.9409738779067993,
        "learning_rate": 7.872826219619788e-06,
        "epoch": 0.29583677970774747,
        "step": 1073
    },
    {
        "loss": 2.5768,
        "grad_norm": 1.1766743659973145,
        "learning_rate": 7.77137934496628e-06,
        "epoch": 0.2961124896608768,
        "step": 1074
    },
    {
        "loss": 2.0332,
        "grad_norm": 0.8602304458618164,
        "learning_rate": 7.670563893176496e-06,
        "epoch": 0.2963881996140061,
        "step": 1075
    },
    {
        "loss": 2.3879,
        "grad_norm": 1.009119987487793,
        "learning_rate": 7.570380554461165e-06,
        "epoch": 0.2966639095671354,
        "step": 1076
    },
    {
        "loss": 2.4231,
        "grad_norm": 1.4143537282943726,
        "learning_rate": 7.470830014703367e-06,
        "epoch": 0.2969396195202647,
        "step": 1077
    },
    {
        "loss": 2.4007,
        "grad_norm": 1.3095980882644653,
        "learning_rate": 7.371912955453863e-06,
        "epoch": 0.29721532947339396,
        "step": 1078
    },
    {
        "loss": 1.7433,
        "grad_norm": 1.7694917917251587,
        "learning_rate": 7.273630053926406e-06,
        "epoch": 0.2974910394265233,
        "step": 1079
    },
    {
        "loss": 2.5011,
        "grad_norm": 1.0718952417373657,
        "learning_rate": 7.175981982993208e-06,
        "epoch": 0.2977667493796526,
        "step": 1080
    },
    {
        "loss": 2.271,
        "grad_norm": 1.4291961193084717,
        "learning_rate": 7.078969411180158e-06,
        "epoch": 0.2980424593327819,
        "step": 1081
    },
    {
        "loss": 2.6348,
        "grad_norm": 0.9259580969810486,
        "learning_rate": 6.982593002662385e-06,
        "epoch": 0.2983181692859112,
        "step": 1082
    },
    {
        "loss": 2.2576,
        "grad_norm": 1.013625144958496,
        "learning_rate": 6.886853417259698e-06,
        "epoch": 0.2985938792390405,
        "step": 1083
    },
    {
        "loss": 1.9161,
        "grad_norm": 1.8730870485305786,
        "learning_rate": 6.791751310431993e-06,
        "epoch": 0.2988695891921698,
        "step": 1084
    },
    {
        "loss": 2.2732,
        "grad_norm": 1.8018091917037964,
        "learning_rate": 6.6972873332747825e-06,
        "epoch": 0.29914529914529914,
        "step": 1085
    },
    {
        "loss": 1.6466,
        "grad_norm": 1.7611196041107178,
        "learning_rate": 6.6034621325148595e-06,
        "epoch": 0.29942100909842845,
        "step": 1086
    },
    {
        "loss": 1.917,
        "grad_norm": 1.871091365814209,
        "learning_rate": 6.510276350505706e-06,
        "epoch": 0.29969671905155776,
        "step": 1087
    },
    {
        "loss": 2.3609,
        "grad_norm": 1.4542995691299438,
        "learning_rate": 6.417730625223162e-06,
        "epoch": 0.29997242900468707,
        "step": 1088
    },
    {
        "loss": 2.1509,
        "grad_norm": 1.5500221252441406,
        "learning_rate": 6.3258255902610695e-06,
        "epoch": 0.3002481389578164,
        "step": 1089
    },
    {
        "loss": 2.0432,
        "grad_norm": 1.4611719846725464,
        "learning_rate": 6.234561874826961e-06,
        "epoch": 0.3005238489109457,
        "step": 1090
    },
    {
        "loss": 2.229,
        "grad_norm": 1.6367019414901733,
        "learning_rate": 6.143940103737689e-06,
        "epoch": 0.300799558864075,
        "step": 1091
    },
    {
        "loss": 1.3415,
        "grad_norm": 1.9447886943817139,
        "learning_rate": 6.053960897415156e-06,
        "epoch": 0.3010752688172043,
        "step": 1092
    },
    {
        "loss": 2.5557,
        "grad_norm": 0.7772877812385559,
        "learning_rate": 5.9646248718821185e-06,
        "epoch": 0.3013509787703336,
        "step": 1093
    },
    {
        "loss": 2.086,
        "grad_norm": 0.9300052523612976,
        "learning_rate": 5.875932638757942e-06,
        "epoch": 0.30162668872346293,
        "step": 1094
    },
    {
        "loss": 1.5818,
        "grad_norm": 2.1355040073394775,
        "learning_rate": 5.787884805254384e-06,
        "epoch": 0.30190239867659224,
        "step": 1095
    },
    {
        "loss": 1.6137,
        "grad_norm": 2.169065237045288,
        "learning_rate": 5.700481974171445e-06,
        "epoch": 0.30217810862972155,
        "step": 1096
    },
    {
        "loss": 2.3286,
        "grad_norm": 0.9849557280540466,
        "learning_rate": 5.613724743893334e-06,
        "epoch": 0.30245381858285086,
        "step": 1097
    },
    {
        "loss": 2.7111,
        "grad_norm": 1.1645839214324951,
        "learning_rate": 5.527613708384227e-06,
        "epoch": 0.3027295285359802,
        "step": 1098
    },
    {
        "loss": 2.4938,
        "grad_norm": 1.6710271835327148,
        "learning_rate": 5.4421494571842755e-06,
        "epoch": 0.30300523848910943,
        "step": 1099
    },
    {
        "loss": 2.128,
        "grad_norm": 1.528954267501831,
        "learning_rate": 5.357332575405627e-06,
        "epoch": 0.30328094844223874,
        "step": 1100
    },
    {
        "loss": 1.8963,
        "grad_norm": 1.2160028219223022,
        "learning_rate": 5.273163643728274e-06,
        "epoch": 0.30355665839536805,
        "step": 1101
    },
    {
        "loss": 2.3087,
        "grad_norm": 1.467017650604248,
        "learning_rate": 5.189643238396214e-06,
        "epoch": 0.30383236834849736,
        "step": 1102
    },
    {
        "loss": 2.0266,
        "grad_norm": 1.7953473329544067,
        "learning_rate": 5.10677193121345e-06,
        "epoch": 0.30410807830162667,
        "step": 1103
    },
    {
        "loss": 1.8333,
        "grad_norm": 1.7738736867904663,
        "learning_rate": 5.0245502895400645e-06,
        "epoch": 0.304383788254756,
        "step": 1104
    },
    {
        "loss": 2.4128,
        "grad_norm": 1.1129592657089233,
        "learning_rate": 4.94297887628834e-06,
        "epoch": 0.3046594982078853,
        "step": 1105
    },
    {
        "loss": 1.9357,
        "grad_norm": 1.7144964933395386,
        "learning_rate": 4.8620582499189326e-06,
        "epoch": 0.3049352081610146,
        "step": 1106
    },
    {
        "loss": 1.8807,
        "grad_norm": 2.1706254482269287,
        "learning_rate": 4.781788964437017e-06,
        "epoch": 0.3052109181141439,
        "step": 1107
    },
    {
        "loss": 1.8482,
        "grad_norm": 1.393964171409607,
        "learning_rate": 4.702171569388525e-06,
        "epoch": 0.3054866280672732,
        "step": 1108
    },
    {
        "loss": 2.2599,
        "grad_norm": 1.4400445222854614,
        "learning_rate": 4.623206609856334e-06,
        "epoch": 0.30576233802040254,
        "step": 1109
    },
    {
        "loss": 2.1451,
        "grad_norm": 1.236505150794983,
        "learning_rate": 4.5448946264566084e-06,
        "epoch": 0.30603804797353185,
        "step": 1110
    },
    {
        "loss": 1.4253,
        "grad_norm": 1.8402458429336548,
        "learning_rate": 4.4672361553350085e-06,
        "epoch": 0.30631375792666116,
        "step": 1111
    },
    {
        "loss": 2.4961,
        "grad_norm": 1.5846995115280151,
        "learning_rate": 4.390231728163097e-06,
        "epoch": 0.30658946787979047,
        "step": 1112
    },
    {
        "loss": 1.8524,
        "grad_norm": 2.1121127605438232,
        "learning_rate": 4.313881872134673e-06,
        "epoch": 0.3068651778329198,
        "step": 1113
    },
    {
        "loss": 2.1197,
        "grad_norm": 1.539021611213684,
        "learning_rate": 4.238187109962144e-06,
        "epoch": 0.3071408877860491,
        "step": 1114
    },
    {
        "loss": 1.544,
        "grad_norm": 2.332249164581299,
        "learning_rate": 4.163147959872949e-06,
        "epoch": 0.3074165977391784,
        "step": 1115
    },
    {
        "loss": 1.8596,
        "grad_norm": 2.16060471534729,
        "learning_rate": 4.088764935606049e-06,
        "epoch": 0.3076923076923077,
        "step": 1116
    },
    {
        "loss": 2.444,
        "grad_norm": 1.1537748575210571,
        "learning_rate": 4.015038546408401e-06,
        "epoch": 0.307968017645437,
        "step": 1117
    },
    {
        "loss": 2.2397,
        "grad_norm": 1.305729866027832,
        "learning_rate": 3.941969297031401e-06,
        "epoch": 0.30824372759856633,
        "step": 1118
    },
    {
        "loss": 2.367,
        "grad_norm": 1.205955982208252,
        "learning_rate": 3.8695576877275205e-06,
        "epoch": 0.30851943755169564,
        "step": 1119
    },
    {
        "loss": 2.6159,
        "grad_norm": 1.1727968454360962,
        "learning_rate": 3.797804214246847e-06,
        "epoch": 0.30879514750482495,
        "step": 1120
    },
    {
        "loss": 2.1087,
        "grad_norm": 1.4835776090621948,
        "learning_rate": 3.7267093678336693e-06,
        "epoch": 0.3090708574579542,
        "step": 1121
    },
    {
        "loss": 2.4854,
        "grad_norm": 1.712390661239624,
        "learning_rate": 3.6562736352231286e-06,
        "epoch": 0.3093465674110835,
        "step": 1122
    },
    {
        "loss": 1.8104,
        "grad_norm": 1.5817373991012573,
        "learning_rate": 3.5864974986379085e-06,
        "epoch": 0.30962227736421283,
        "step": 1123
    },
    {
        "loss": 2.3165,
        "grad_norm": 1.7228307723999023,
        "learning_rate": 3.5173814357849057e-06,
        "epoch": 0.30989798731734214,
        "step": 1124
    },
    {
        "loss": 1.3115,
        "grad_norm": 1.8096717596054077,
        "learning_rate": 3.448925919851953e-06,
        "epoch": 0.31017369727047145,
        "step": 1125
    },
    {
        "loss": 2.5241,
        "grad_norm": 1.007224202156067,
        "learning_rate": 3.381131419504613e-06,
        "epoch": 0.31044940722360076,
        "step": 1126
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.118333101272583,
        "learning_rate": 3.313998398882956e-06,
        "epoch": 0.31072511717673007,
        "step": 1127
    },
    {
        "loss": 1.9601,
        "grad_norm": 1.6370004415512085,
        "learning_rate": 3.2475273175983535e-06,
        "epoch": 0.3110008271298594,
        "step": 1128
    },
    {
        "loss": 2.165,
        "grad_norm": 1.6543246507644653,
        "learning_rate": 3.181718630730379e-06,
        "epoch": 0.3112765370829887,
        "step": 1129
    },
    {
        "loss": 1.8123,
        "grad_norm": 1.8275436162948608,
        "learning_rate": 3.1165727888236573e-06,
        "epoch": 0.311552247036118,
        "step": 1130
    },
    {
        "loss": 2.0783,
        "grad_norm": 1.4392744302749634,
        "learning_rate": 3.052090237884797e-06,
        "epoch": 0.3118279569892473,
        "step": 1131
    },
    {
        "loss": 2.0261,
        "grad_norm": 1.8618394136428833,
        "learning_rate": 2.9882714193793293e-06,
        "epoch": 0.3121036669423766,
        "step": 1132
    },
    {
        "loss": 2.2219,
        "grad_norm": 1.6327487230300903,
        "learning_rate": 2.9251167702286754e-06,
        "epoch": 0.31237937689550593,
        "step": 1133
    },
    {
        "loss": 2.1674,
        "grad_norm": 1.3581053018569946,
        "learning_rate": 2.8626267228071933e-06,
        "epoch": 0.31265508684863524,
        "step": 1134
    },
    {
        "loss": 1.9878,
        "grad_norm": 1.9190667867660522,
        "learning_rate": 2.8008017049391823e-06,
        "epoch": 0.31293079680176455,
        "step": 1135
    },
    {
        "loss": 1.5992,
        "grad_norm": 1.7156054973602295,
        "learning_rate": 2.739642139895937e-06,
        "epoch": 0.31320650675489387,
        "step": 1136
    },
    {
        "loss": 2.4484,
        "grad_norm": 1.1933174133300781,
        "learning_rate": 2.6791484463929205e-06,
        "epoch": 0.3134822167080232,
        "step": 1137
    },
    {
        "loss": 2.0264,
        "grad_norm": 2.359799861907959,
        "learning_rate": 2.6193210385868417e-06,
        "epoch": 0.3137579266611525,
        "step": 1138
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.24221670627594,
        "learning_rate": 2.5601603260727914e-06,
        "epoch": 0.3140336366142818,
        "step": 1139
    },
    {
        "loss": 1.9342,
        "grad_norm": 1.397050142288208,
        "learning_rate": 2.5016667138815343e-06,
        "epoch": 0.3143093465674111,
        "step": 1140
    },
    {
        "loss": 2.0704,
        "grad_norm": 1.1923664808273315,
        "learning_rate": 2.4438406024766546e-06,
        "epoch": 0.3145850565205404,
        "step": 1141
    },
    {
        "loss": 1.9261,
        "grad_norm": 1.2919411659240723,
        "learning_rate": 2.3866823877518376e-06,
        "epoch": 0.3148607664736697,
        "step": 1142
    },
    {
        "loss": 2.0246,
        "grad_norm": 1.6593741178512573,
        "learning_rate": 2.3301924610281577e-06,
        "epoch": 0.315136476426799,
        "step": 1143
    },
    {
        "loss": 2.1771,
        "grad_norm": 1.7478262186050415,
        "learning_rate": 2.274371209051429e-06,
        "epoch": 0.3154121863799283,
        "step": 1144
    },
    {
        "loss": 1.8542,
        "grad_norm": 1.6951695680618286,
        "learning_rate": 2.219219013989493e-06,
        "epoch": 0.3156878963330576,
        "step": 1145
    },
    {
        "loss": 2.317,
        "grad_norm": 1.3211290836334229,
        "learning_rate": 2.1647362534296667e-06,
        "epoch": 0.3159636062861869,
        "step": 1146
    },
    {
        "loss": 2.0375,
        "grad_norm": 1.1721978187561035,
        "learning_rate": 2.1109233003761333e-06,
        "epoch": 0.3162393162393162,
        "step": 1147
    },
    {
        "loss": 2.4515,
        "grad_norm": 1.094929575920105,
        "learning_rate": 2.057780523247388e-06,
        "epoch": 0.31651502619244554,
        "step": 1148
    },
    {
        "loss": 2.1502,
        "grad_norm": 1.3901288509368896,
        "learning_rate": 2.005308285873675e-06,
        "epoch": 0.31679073614557485,
        "step": 1149
    },
    {
        "loss": 1.8687,
        "grad_norm": 2.228212833404541,
        "learning_rate": 1.95350694749461e-06,
        "epoch": 0.31706644609870416,
        "step": 1150
    },
    {
        "loss": 2.0643,
        "grad_norm": 1.531156063079834,
        "learning_rate": 1.902376862756583e-06,
        "epoch": 0.31734215605183347,
        "step": 1151
    },
    {
        "loss": 2.1543,
        "grad_norm": 1.2427953481674194,
        "learning_rate": 1.8519183817104157e-06,
        "epoch": 0.3176178660049628,
        "step": 1152
    },
    {
        "loss": 1.8574,
        "grad_norm": 1.6850062608718872,
        "learning_rate": 1.8021318498089301e-06,
        "epoch": 0.3178935759580921,
        "step": 1153
    },
    {
        "loss": 2.1821,
        "grad_norm": 1.7839139699935913,
        "learning_rate": 1.75301760790465e-06,
        "epoch": 0.3181692859112214,
        "step": 1154
    },
    {
        "loss": 1.6552,
        "grad_norm": 1.8451718091964722,
        "learning_rate": 1.7045759922473481e-06,
        "epoch": 0.3184449958643507,
        "step": 1155
    },
    {
        "loss": 1.5846,
        "grad_norm": 2.084749698638916,
        "learning_rate": 1.6568073344818469e-06,
        "epoch": 0.31872070581748,
        "step": 1156
    },
    {
        "loss": 2.4455,
        "grad_norm": 0.9632251262664795,
        "learning_rate": 1.609711961645699e-06,
        "epoch": 0.31899641577060933,
        "step": 1157
    },
    {
        "loss": 2.6188,
        "grad_norm": 1.050728440284729,
        "learning_rate": 1.5632901961669554e-06,
        "epoch": 0.31927212572373864,
        "step": 1158
    },
    {
        "loss": 1.9666,
        "grad_norm": 1.0524760484695435,
        "learning_rate": 1.5175423558619895e-06,
        "epoch": 0.31954783567686795,
        "step": 1159
    },
    {
        "loss": 2.3957,
        "grad_norm": 1.373590350151062,
        "learning_rate": 1.4724687539332649e-06,
        "epoch": 0.31982354562999726,
        "step": 1160
    },
    {
        "loss": 1.864,
        "grad_norm": 2.248852014541626,
        "learning_rate": 1.4280696989672383e-06,
        "epoch": 0.3200992555831266,
        "step": 1161
    },
    {
        "loss": 2.1846,
        "grad_norm": 1.3027944564819336,
        "learning_rate": 1.3843454949322266e-06,
        "epoch": 0.3203749655362559,
        "step": 1162
    },
    {
        "loss": 2.3187,
        "grad_norm": 1.0357168912887573,
        "learning_rate": 1.3412964411763096e-06,
        "epoch": 0.32065067548938514,
        "step": 1163
    },
    {
        "loss": 1.6647,
        "grad_norm": 2.259169340133667,
        "learning_rate": 1.2989228324253422e-06,
        "epoch": 0.32092638544251445,
        "step": 1164
    },
    {
        "loss": 2.3021,
        "grad_norm": 1.028282642364502,
        "learning_rate": 1.257224958780867e-06,
        "epoch": 0.32120209539564376,
        "step": 1165
    },
    {
        "loss": 2.0057,
        "grad_norm": 1.117230772972107,
        "learning_rate": 1.216203105718139e-06,
        "epoch": 0.32147780534877307,
        "step": 1166
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.6742690801620483,
        "learning_rate": 1.1758575540842254e-06,
        "epoch": 0.3217535153019024,
        "step": 1167
    },
    {
        "loss": 1.6392,
        "grad_norm": 1.866215467453003,
        "learning_rate": 1.1361885800960204e-06,
        "epoch": 0.3220292252550317,
        "step": 1168
    },
    {
        "loss": 2.3157,
        "grad_norm": 1.4976075887680054,
        "learning_rate": 1.0971964553383784e-06,
        "epoch": 0.322304935208161,
        "step": 1169
    },
    {
        "loss": 2.1692,
        "grad_norm": 1.728286623954773,
        "learning_rate": 1.058881446762272e-06,
        "epoch": 0.3225806451612903,
        "step": 1170
    },
    {
        "loss": 2.274,
        "grad_norm": 1.3746209144592285,
        "learning_rate": 1.0212438166829374e-06,
        "epoch": 0.3228563551144196,
        "step": 1171
    },
    {
        "loss": 2.5662,
        "grad_norm": 1.6715370416641235,
        "learning_rate": 9.842838227780982e-07,
        "epoch": 0.32313206506754893,
        "step": 1172
    },
    {
        "loss": 1.8506,
        "grad_norm": 2.6853890419006348,
        "learning_rate": 9.480017180861556e-07,
        "epoch": 0.32340777502067825,
        "step": 1173
    },
    {
        "loss": 1.4198,
        "grad_norm": 2.917166233062744,
        "learning_rate": 9.123977510045345e-07,
        "epoch": 0.32368348497380756,
        "step": 1174
    },
    {
        "loss": 2.2067,
        "grad_norm": 2.132960796356201,
        "learning_rate": 8.774721652879403e-07,
        "epoch": 0.32395919492693687,
        "step": 1175
    },
    {
        "loss": 1.9438,
        "grad_norm": 1.5609644651412964,
        "learning_rate": 8.432252000466378e-07,
        "epoch": 0.3242349048800662,
        "step": 1176
    },
    {
        "loss": 2.0132,
        "grad_norm": 1.917656660079956,
        "learning_rate": 8.096570897449418e-07,
        "epoch": 0.3245106148331955,
        "step": 1177
    },
    {
        "loss": 2.3105,
        "grad_norm": 1.1034716367721558,
        "learning_rate": 7.767680641994845e-07,
        "epoch": 0.3247863247863248,
        "step": 1178
    },
    {
        "loss": 2.0633,
        "grad_norm": 1.300727128982544,
        "learning_rate": 7.445583485776952e-07,
        "epoch": 0.3250620347394541,
        "step": 1179
    },
    {
        "loss": 2.0691,
        "grad_norm": 1.212308645248413,
        "learning_rate": 7.130281633962898e-07,
        "epoch": 0.3253377446925834,
        "step": 1180
    },
    {
        "loss": 2.1241,
        "grad_norm": 1.2892273664474487,
        "learning_rate": 6.82177724519717e-07,
        "epoch": 0.32561345464571273,
        "step": 1181
    },
    {
        "loss": 2.48,
        "grad_norm": 1.5039596557617188,
        "learning_rate": 6.520072431586699e-07,
        "epoch": 0.32588916459884204,
        "step": 1182
    },
    {
        "loss": 1.5975,
        "grad_norm": 1.3473507165908813,
        "learning_rate": 6.225169258686769e-07,
        "epoch": 0.32616487455197135,
        "step": 1183
    },
    {
        "loss": 2.1363,
        "grad_norm": 1.3551548719406128,
        "learning_rate": 5.937069745486911e-07,
        "epoch": 0.32644058450510066,
        "step": 1184
    },
    {
        "loss": 2.4866,
        "grad_norm": 1.452858567237854,
        "learning_rate": 5.655775864396695e-07,
        "epoch": 0.3267162944582299,
        "step": 1185
    },
    {
        "loss": 2.3786,
        "grad_norm": 1.2002592086791992,
        "learning_rate": 5.381289541232515e-07,
        "epoch": 0.3269920044113592,
        "step": 1186
    },
    {
        "loss": 1.9776,
        "grad_norm": 1.4477157592773438,
        "learning_rate": 5.11361265520427e-07,
        "epoch": 0.32726771436448854,
        "step": 1187
    },
    {
        "loss": 2.4402,
        "grad_norm": 1.4283347129821777,
        "learning_rate": 4.852747038902594e-07,
        "epoch": 0.32754342431761785,
        "step": 1188
    },
    {
        "loss": 1.4004,
        "grad_norm": 1.010422706604004,
        "learning_rate": 4.598694478286314e-07,
        "epoch": 0.32781913427074716,
        "step": 1189
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.2722219228744507,
        "learning_rate": 4.351456712670121e-07,
        "epoch": 0.32809484422387647,
        "step": 1190
    },
    {
        "loss": 2.5444,
        "grad_norm": 1.1868053674697876,
        "learning_rate": 4.1110354347125844e-07,
        "epoch": 0.3283705541770058,
        "step": 1191
    },
    {
        "loss": 1.9084,
        "grad_norm": 1.4160667657852173,
        "learning_rate": 3.8774322904050476e-07,
        "epoch": 0.3286462641301351,
        "step": 1192
    },
    {
        "loss": 2.1418,
        "grad_norm": 1.531873106956482,
        "learning_rate": 3.6506488790596374e-07,
        "epoch": 0.3289219740832644,
        "step": 1193
    },
    {
        "loss": 2.1888,
        "grad_norm": 1.0202761888504028,
        "learning_rate": 3.4306867532990506e-07,
        "epoch": 0.3291976840363937,
        "step": 1194
    },
    {
        "loss": 1.6558,
        "grad_norm": 1.8061823844909668,
        "learning_rate": 3.2175474190451194e-07,
        "epoch": 0.329473393989523,
        "step": 1195
    },
    {
        "loss": 2.0657,
        "grad_norm": 1.7394726276397705,
        "learning_rate": 3.011232335509151e-07,
        "epoch": 0.32974910394265233,
        "step": 1196
    },
    {
        "loss": 2.3518,
        "grad_norm": 1.5178515911102295,
        "learning_rate": 2.811742915182047e-07,
        "epoch": 0.33002481389578164,
        "step": 1197
    },
    {
        "loss": 2.3025,
        "grad_norm": 1.2139806747436523,
        "learning_rate": 2.619080523823869e-07,
        "epoch": 0.33030052384891095,
        "step": 1198
    },
    {
        "loss": 2.3463,
        "grad_norm": 1.468017339706421,
        "learning_rate": 2.433246480455065e-07,
        "epoch": 0.33057623380204026,
        "step": 1199
    },
    {
        "loss": 1.814,
        "grad_norm": 1.6745518445968628,
        "learning_rate": 2.2542420573472556e-07,
        "epoch": 0.3308519437551696,
        "step": 1200
    },
    {
        "loss": 2.282,
        "grad_norm": 1.5313071012496948,
        "learning_rate": 2.0820684800147983e-07,
        "epoch": 0.3311276537082989,
        "step": 1201
    },
    {
        "loss": 2.0214,
        "grad_norm": 1.5031604766845703,
        "learning_rate": 1.916726927206014e-07,
        "epoch": 0.3314033636614282,
        "step": 1202
    },
    {
        "loss": 1.1056,
        "grad_norm": 1.9750906229019165,
        "learning_rate": 1.7582185308953058e-07,
        "epoch": 0.3316790736145575,
        "step": 1203
    },
    {
        "loss": 1.8223,
        "grad_norm": 1.8996400833129883,
        "learning_rate": 1.6065443762752764e-07,
        "epoch": 0.3319547835676868,
        "step": 1204
    },
    {
        "loss": 2.1658,
        "grad_norm": 0.9966720938682556,
        "learning_rate": 1.4617055017495108e-07,
        "epoch": 0.33223049352081613,
        "step": 1205
    },
    {
        "loss": 2.6317,
        "grad_norm": 2.6269733905792236,
        "learning_rate": 1.3237028989254718e-07,
        "epoch": 0.3325062034739454,
        "step": 1206
    },
    {
        "loss": 1.8506,
        "grad_norm": 1.882423758506775,
        "learning_rate": 1.1925375126072835e-07,
        "epoch": 0.3327819134270747,
        "step": 1207
    },
    {
        "loss": 2.4991,
        "grad_norm": 0.8942424654960632,
        "learning_rate": 1.0682102407899574e-07,
        "epoch": 0.333057623380204,
        "step": 1208
    },
    {
        "loss": 2.1192,
        "grad_norm": 1.1702040433883667,
        "learning_rate": 9.507219346526209e-08,
        "epoch": 0.3333333333333333,
        "step": 1209
    },
    {
        "loss": 2.1534,
        "grad_norm": 1.2457247972488403,
        "learning_rate": 8.400733985528542e-08,
        "epoch": 0.3336090432864626,
        "step": 1210
    },
    {
        "loss": 2.137,
        "grad_norm": 1.3809679746627808,
        "learning_rate": 7.362653900215844e-08,
        "epoch": 0.33388475323959194,
        "step": 1211
    },
    {
        "loss": 2.2518,
        "grad_norm": 1.0447925329208374,
        "learning_rate": 6.392986197572005e-08,
        "epoch": 0.33416046319272125,
        "step": 1212
    },
    {
        "loss": 2.2085,
        "grad_norm": 2.083483934402466,
        "learning_rate": 5.491737516214457e-08,
        "epoch": 0.33443617314585056,
        "step": 1213
    },
    {
        "loss": 2.0494,
        "grad_norm": 1.2808916568756104,
        "learning_rate": 4.6589140263431084e-08,
        "epoch": 0.33471188309897987,
        "step": 1214
    },
    {
        "loss": 2.2517,
        "grad_norm": 1.2366414070129395,
        "learning_rate": 3.89452142969815e-08,
        "epoch": 0.3349875930521092,
        "step": 1215
    },
    {
        "loss": 2.3505,
        "grad_norm": 1.855046272277832,
        "learning_rate": 3.198564959526751e-08,
        "epoch": 0.3352633030052385,
        "step": 1216
    },
    {
        "loss": 2.1789,
        "grad_norm": 1.6626434326171875,
        "learning_rate": 2.5710493805386483e-08,
        "epoch": 0.3355390129583678,
        "step": 1217
    },
    {
        "loss": 2.7658,
        "grad_norm": 1.0313276052474976,
        "learning_rate": 2.011978988881724e-08,
        "epoch": 0.3358147229114971,
        "step": 1218
    },
    {
        "loss": 2.2466,
        "grad_norm": 1.1920582056045532,
        "learning_rate": 1.521357612108698e-08,
        "epoch": 0.3360904328646264,
        "step": 1219
    },
    {
        "loss": 2.2962,
        "grad_norm": 1.3656067848205566,
        "learning_rate": 1.0991886091482606e-08,
        "epoch": 0.33636614281775573,
        "step": 1220
    },
    {
        "loss": 1.7891,
        "grad_norm": 0.8600540161132812,
        "learning_rate": 7.45474870289531e-09,
        "epoch": 0.33664185277088504,
        "step": 1221
    },
    {
        "loss": 2.4044,
        "grad_norm": 1.2073432207107544,
        "learning_rate": 4.602188171543009e-09,
        "epoch": 0.33691756272401435,
        "step": 1222
    },
    {
        "loss": 2.5453,
        "grad_norm": 1.11306631565094,
        "learning_rate": 2.4342240268371288e-09,
        "epoch": 0.33719327267714366,
        "step": 1223
    },
    {
        "loss": 1.8114,
        "grad_norm": 1.8904032707214355,
        "learning_rate": 9.50871111293772e-10,
        "epoch": 0.337468982630273,
        "step": 1224
    },
    {
        "loss": 2.265,
        "grad_norm": 0.8899682760238647,
        "learning_rate": 1.5213958033388765e-10,
        "epoch": 0.3377446925834023,
        "step": 1225
    },
    {
        "loss": 1.7223,
        "grad_norm": 2.185336112976074,
        "learning_rate": 0.0001999999619650977,
        "epoch": 0.3380204025365316,
        "step": 1226
    },
    {
        "loss": 2.2073,
        "grad_norm": 1.4077413082122803,
        "learning_rate": 0.00019999939144214158,
        "epoch": 0.33829611248966085,
        "step": 1227
    },
    {
        "loss": 2.2266,
        "grad_norm": 1.3168731927871704,
        "learning_rate": 0.00019999813629545732,
        "epoch": 0.33857182244279016,
        "step": 1228
    },
    {
        "loss": 2.3806,
        "grad_norm": 1.6995797157287598,
        "learning_rate": 0.00019999619653363797,
        "epoch": 0.33884753239591947,
        "step": 1229
    },
    {
        "loss": 1.9909,
        "grad_norm": 1.349929690361023,
        "learning_rate": 0.00019999357216996368,
        "epoch": 0.3391232423490488,
        "step": 1230
    },
    {
        "loss": 2.3701,
        "grad_norm": 1.6480461359024048,
        "learning_rate": 0.00019999026322240157,
        "epoch": 0.3393989523021781,
        "step": 1231
    },
    {
        "loss": 2.1502,
        "grad_norm": 1.2133842706680298,
        "learning_rate": 0.00019998626971360562,
        "epoch": 0.3396746622553074,
        "step": 1232
    },
    {
        "loss": 2.2605,
        "grad_norm": 1.890199065208435,
        "learning_rate": 0.00019998159167091656,
        "epoch": 0.3399503722084367,
        "step": 1233
    },
    {
        "loss": 1.741,
        "grad_norm": 1.3826884031295776,
        "learning_rate": 0.0001999762291263615,
        "epoch": 0.340226082161566,
        "step": 1234
    },
    {
        "loss": 2.3487,
        "grad_norm": 1.4927767515182495,
        "learning_rate": 0.000199970182116654,
        "epoch": 0.34050179211469533,
        "step": 1235
    },
    {
        "loss": 1.965,
        "grad_norm": 1.1439439058303833,
        "learning_rate": 0.00019996345068319345,
        "epoch": 0.34077750206782464,
        "step": 1236
    },
    {
        "loss": 2.2369,
        "grad_norm": 1.4062457084655762,
        "learning_rate": 0.00019995603487206527,
        "epoch": 0.34105321202095396,
        "step": 1237
    },
    {
        "loss": 1.5602,
        "grad_norm": 2.1958537101745605,
        "learning_rate": 0.00019994793473404005,
        "epoch": 0.34132892197408327,
        "step": 1238
    },
    {
        "loss": 2.4021,
        "grad_norm": 1.0761997699737549,
        "learning_rate": 0.00019993915032457364,
        "epoch": 0.3416046319272126,
        "step": 1239
    },
    {
        "loss": 2.5058,
        "grad_norm": 1.2933332920074463,
        "learning_rate": 0.00019992968170380655,
        "epoch": 0.3418803418803419,
        "step": 1240
    },
    {
        "loss": 2.7732,
        "grad_norm": 1.292117953300476,
        "learning_rate": 0.00019991952893656364,
        "epoch": 0.3421560518334712,
        "step": 1241
    },
    {
        "loss": 2.1074,
        "grad_norm": 2.073254108428955,
        "learning_rate": 0.00019990869209235353,
        "epoch": 0.3424317617866005,
        "step": 1242
    },
    {
        "loss": 1.0622,
        "grad_norm": 1.448161244392395,
        "learning_rate": 0.0001998971712453683,
        "epoch": 0.3427074717397298,
        "step": 1243
    },
    {
        "loss": 2.4273,
        "grad_norm": 0.9615991115570068,
        "learning_rate": 0.00019988496647448288,
        "epoch": 0.34298318169285913,
        "step": 1244
    },
    {
        "loss": 2.2574,
        "grad_norm": 1.1146737337112427,
        "learning_rate": 0.00019987207786325458,
        "epoch": 0.34325889164598844,
        "step": 1245
    },
    {
        "loss": 2.2446,
        "grad_norm": 1.6223081350326538,
        "learning_rate": 0.00019985850549992233,
        "epoch": 0.34353460159911775,
        "step": 1246
    },
    {
        "loss": 2.1329,
        "grad_norm": 1.22881281375885,
        "learning_rate": 0.0001998442494774064,
        "epoch": 0.34381031155224706,
        "step": 1247
    },
    {
        "loss": 1.9143,
        "grad_norm": 1.1939817667007446,
        "learning_rate": 0.00019982930989330747,
        "epoch": 0.34408602150537637,
        "step": 1248
    },
    {
        "loss": 2.5021,
        "grad_norm": 1.3119181394577026,
        "learning_rate": 0.0001998136868499061,
        "epoch": 0.3443617314585056,
        "step": 1249
    },
    {
        "loss": 2.1988,
        "grad_norm": 1.301460862159729,
        "learning_rate": 0.000199797380454162,
        "epoch": 0.34463744141163494,
        "step": 1250
    },
    {
        "loss": 2.103,
        "grad_norm": 1.0247081518173218,
        "learning_rate": 0.0001997803908177133,
        "epoch": 0.34491315136476425,
        "step": 1251
    },
    {
        "loss": 1.6085,
        "grad_norm": 2.677307605743408,
        "learning_rate": 0.00019976271805687582,
        "epoch": 0.34518886131789356,
        "step": 1252
    },
    {
        "loss": 2.2594,
        "grad_norm": 1.7429665327072144,
        "learning_rate": 0.00019974436229264217,
        "epoch": 0.34546457127102287,
        "step": 1253
    },
    {
        "loss": 1.5264,
        "grad_norm": 2.114853620529175,
        "learning_rate": 0.00019972532365068103,
        "epoch": 0.3457402812241522,
        "step": 1254
    },
    {
        "loss": 1.7744,
        "grad_norm": 2.898803949356079,
        "learning_rate": 0.0001997056022613363,
        "epoch": 0.3460159911772815,
        "step": 1255
    },
    {
        "loss": 2.406,
        "grad_norm": 1.124950647354126,
        "learning_rate": 0.0001996851982596261,
        "epoch": 0.3462917011304108,
        "step": 1256
    },
    {
        "loss": 2.4847,
        "grad_norm": 1.257351279258728,
        "learning_rate": 0.00019966411178524188,
        "epoch": 0.3465674110835401,
        "step": 1257
    },
    {
        "loss": 2.1051,
        "grad_norm": 1.4422881603240967,
        "learning_rate": 0.00019964234298254754,
        "epoch": 0.3468431210366694,
        "step": 1258
    },
    {
        "loss": 2.4227,
        "grad_norm": 1.3884432315826416,
        "learning_rate": 0.00019961989200057838,
        "epoch": 0.34711883098979873,
        "step": 1259
    },
    {
        "loss": 2.3207,
        "grad_norm": 1.3211535215377808,
        "learning_rate": 0.00019959675899304012,
        "epoch": 0.34739454094292804,
        "step": 1260
    },
    {
        "loss": 2.1389,
        "grad_norm": 1.1481716632843018,
        "learning_rate": 0.00019957294411830775,
        "epoch": 0.34767025089605735,
        "step": 1261
    },
    {
        "loss": 2.12,
        "grad_norm": 1.9851700067520142,
        "learning_rate": 0.0001995484475394245,
        "epoch": 0.34794596084918666,
        "step": 1262
    },
    {
        "loss": 1.3597,
        "grad_norm": 2.7731127738952637,
        "learning_rate": 0.0001995232694241009,
        "epoch": 0.348221670802316,
        "step": 1263
    },
    {
        "loss": 1.9353,
        "grad_norm": 1.322757601737976,
        "learning_rate": 0.00019949740994471322,
        "epoch": 0.3484973807554453,
        "step": 1264
    },
    {
        "loss": 2.1609,
        "grad_norm": 1.6884255409240723,
        "learning_rate": 0.00019947086927830275,
        "epoch": 0.3487730907085746,
        "step": 1265
    },
    {
        "loss": 2.296,
        "grad_norm": 1.0352154970169067,
        "learning_rate": 0.0001994436476065743,
        "epoch": 0.3490488006617039,
        "step": 1266
    },
    {
        "loss": 1.5437,
        "grad_norm": 1.8938822746276855,
        "learning_rate": 0.00019941574511589497,
        "epoch": 0.3493245106148332,
        "step": 1267
    },
    {
        "loss": 2.3313,
        "grad_norm": 1.2476274967193604,
        "learning_rate": 0.00019938716199729303,
        "epoch": 0.3496002205679625,
        "step": 1268
    },
    {
        "loss": 1.5758,
        "grad_norm": 2.2659847736358643,
        "learning_rate": 0.0001993578984464565,
        "epoch": 0.34987593052109184,
        "step": 1269
    },
    {
        "loss": 2.2206,
        "grad_norm": 1.879544973373413,
        "learning_rate": 0.00019932795466373179,
        "epoch": 0.3501516404742211,
        "step": 1270
    },
    {
        "loss": 2.315,
        "grad_norm": 1.6150319576263428,
        "learning_rate": 0.0001992973308541224,
        "epoch": 0.3504273504273504,
        "step": 1271
    },
    {
        "loss": 2.3174,
        "grad_norm": 1.2762507200241089,
        "learning_rate": 0.00019926602722728743,
        "epoch": 0.3507030603804797,
        "step": 1272
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.4654228687286377,
        "learning_rate": 0.0001992340439975403,
        "epoch": 0.350978770333609,
        "step": 1273
    },
    {
        "loss": 1.5009,
        "grad_norm": 1.6412144899368286,
        "learning_rate": 0.0001992013813838471,
        "epoch": 0.35125448028673834,
        "step": 1274
    },
    {
        "loss": 2.0694,
        "grad_norm": 1.2065560817718506,
        "learning_rate": 0.0001991680396098252,
        "epoch": 0.35153019023986765,
        "step": 1275
    },
    {
        "loss": 1.9867,
        "grad_norm": 1.8285526037216187,
        "learning_rate": 0.00019913401890374164,
        "epoch": 0.35180590019299696,
        "step": 1276
    },
    {
        "loss": 2.2607,
        "grad_norm": 1.6338287591934204,
        "learning_rate": 0.00019909931949851174,
        "epoch": 0.35208161014612627,
        "step": 1277
    },
    {
        "loss": 2.1928,
        "grad_norm": 1.467543601989746,
        "learning_rate": 0.00019906394163169723,
        "epoch": 0.3523573200992556,
        "step": 1278
    },
    {
        "loss": 2.306,
        "grad_norm": 1.3873554468154907,
        "learning_rate": 0.00019902788554550493,
        "epoch": 0.3526330300523849,
        "step": 1279
    },
    {
        "loss": 1.3425,
        "grad_norm": 2.1549038887023926,
        "learning_rate": 0.00019899115148678482,
        "epoch": 0.3529087400055142,
        "step": 1280
    },
    {
        "loss": 2.5351,
        "grad_norm": 1.1851426362991333,
        "learning_rate": 0.00019895373970702852,
        "epoch": 0.3531844499586435,
        "step": 1281
    },
    {
        "loss": 1.4278,
        "grad_norm": 1.9510701894760132,
        "learning_rate": 0.00019891565046236754,
        "epoch": 0.3534601599117728,
        "step": 1282
    },
    {
        "loss": 1.9102,
        "grad_norm": 1.9477064609527588,
        "learning_rate": 0.0001988768840135714,
        "epoch": 0.35373586986490213,
        "step": 1283
    },
    {
        "loss": 2.0904,
        "grad_norm": 1.1157939434051514,
        "learning_rate": 0.0001988374406260461,
        "epoch": 0.35401157981803144,
        "step": 1284
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.6920409202575684,
        "learning_rate": 0.000198797320569832,
        "epoch": 0.35428728977116075,
        "step": 1285
    },
    {
        "loss": 2.0122,
        "grad_norm": 1.2776011228561401,
        "learning_rate": 0.00019875652411960228,
        "epoch": 0.35456299972429006,
        "step": 1286
    },
    {
        "loss": 2.1078,
        "grad_norm": 2.091742753982544,
        "learning_rate": 0.00019871505155466074,
        "epoch": 0.3548387096774194,
        "step": 1287
    },
    {
        "loss": 2.5367,
        "grad_norm": 1.0046306848526,
        "learning_rate": 0.00019867290315894018,
        "epoch": 0.3551144196305487,
        "step": 1288
    },
    {
        "loss": 2.7418,
        "grad_norm": 0.9798656105995178,
        "learning_rate": 0.00019863007922100024,
        "epoch": 0.355390129583678,
        "step": 1289
    },
    {
        "loss": 1.8891,
        "grad_norm": 2.046926259994507,
        "learning_rate": 0.00019858658003402558,
        "epoch": 0.3556658395368073,
        "step": 1290
    },
    {
        "loss": 2.5664,
        "grad_norm": 1.4116469621658325,
        "learning_rate": 0.00019854240589582374,
        "epoch": 0.35594154948993656,
        "step": 1291
    },
    {
        "loss": 1.6555,
        "grad_norm": 1.6577452421188354,
        "learning_rate": 0.00019849755710882322,
        "epoch": 0.35621725944306587,
        "step": 1292
    },
    {
        "loss": 1.9262,
        "grad_norm": 1.443973183631897,
        "learning_rate": 0.00019845203398007134,
        "epoch": 0.3564929693961952,
        "step": 1293
    },
    {
        "loss": 2.4447,
        "grad_norm": 1.6147509813308716,
        "learning_rate": 0.00019840583682123204,
        "epoch": 0.3567686793493245,
        "step": 1294
    },
    {
        "loss": 1.9317,
        "grad_norm": 1.8904095888137817,
        "learning_rate": 0.00019835896594858405,
        "epoch": 0.3570443893024538,
        "step": 1295
    },
    {
        "loss": 2.1801,
        "grad_norm": 1.2293179035186768,
        "learning_rate": 0.00019831142168301842,
        "epoch": 0.3573200992555831,
        "step": 1296
    },
    {
        "loss": 1.9589,
        "grad_norm": 1.682590365409851,
        "learning_rate": 0.0001982632043500364,
        "epoch": 0.3575958092087124,
        "step": 1297
    },
    {
        "loss": 2.0981,
        "grad_norm": 1.1042263507843018,
        "learning_rate": 0.0001982143142797474,
        "epoch": 0.35787151916184173,
        "step": 1298
    },
    {
        "loss": 2.7134,
        "grad_norm": 1.0282560586929321,
        "learning_rate": 0.00019816475180686643,
        "epoch": 0.35814722911497104,
        "step": 1299
    },
    {
        "loss": 1.6152,
        "grad_norm": 2.2669904232025146,
        "learning_rate": 0.00019811451727071196,
        "epoch": 0.35842293906810035,
        "step": 1300
    },
    {
        "loss": 2.0742,
        "grad_norm": 1.9906119108200073,
        "learning_rate": 0.0001980636110152037,
        "epoch": 0.35869864902122967,
        "step": 1301
    },
    {
        "loss": 2.6121,
        "grad_norm": 1.3263390064239502,
        "learning_rate": 0.00019801203338886002,
        "epoch": 0.358974358974359,
        "step": 1302
    },
    {
        "loss": 2.0627,
        "grad_norm": 1.5568021535873413,
        "learning_rate": 0.0001979597847447958,
        "epoch": 0.3592500689274883,
        "step": 1303
    },
    {
        "loss": 2.4385,
        "grad_norm": 0.9876254200935364,
        "learning_rate": 0.0001979068654407198,
        "epoch": 0.3595257788806176,
        "step": 1304
    },
    {
        "loss": 1.9331,
        "grad_norm": 2.0708167552948,
        "learning_rate": 0.00019785327583893235,
        "epoch": 0.3598014888337469,
        "step": 1305
    },
    {
        "loss": 1.4525,
        "grad_norm": 2.0038394927978516,
        "learning_rate": 0.00019779901630632277,
        "epoch": 0.3600771987868762,
        "step": 1306
    },
    {
        "loss": 1.9945,
        "grad_norm": 1.4145630598068237,
        "learning_rate": 0.000197744087214367,
        "epoch": 0.36035290874000553,
        "step": 1307
    },
    {
        "loss": 2.3116,
        "grad_norm": 1.50235116481781,
        "learning_rate": 0.0001976884889391249,
        "epoch": 0.36062861869313484,
        "step": 1308
    },
    {
        "loss": 2.4632,
        "grad_norm": 1.489594578742981,
        "learning_rate": 0.00019763222186123782,
        "epoch": 0.36090432864626415,
        "step": 1309
    },
    {
        "loss": 1.5125,
        "grad_norm": 2.355026960372925,
        "learning_rate": 0.00019757528636592578,
        "epoch": 0.36118003859939346,
        "step": 1310
    },
    {
        "loss": 2.0879,
        "grad_norm": 1.2989389896392822,
        "learning_rate": 0.00019751768284298517,
        "epoch": 0.36145574855252277,
        "step": 1311
    },
    {
        "loss": 2.4103,
        "grad_norm": 1.3313987255096436,
        "learning_rate": 0.0001974594116867857,
        "epoch": 0.3617314585056521,
        "step": 1312
    },
    {
        "loss": 2.284,
        "grad_norm": 1.4016399383544922,
        "learning_rate": 0.00019740047329626798,
        "epoch": 0.36200716845878134,
        "step": 1313
    },
    {
        "loss": 2.2707,
        "grad_norm": 0.9408451318740845,
        "learning_rate": 0.00019734086807494068,
        "epoch": 0.36228287841191065,
        "step": 1314
    },
    {
        "loss": 1.6725,
        "grad_norm": 1.934630274772644,
        "learning_rate": 0.00019728059643087778,
        "epoch": 0.36255858836503996,
        "step": 1315
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.1428954601287842,
        "learning_rate": 0.00019721965877671577,
        "epoch": 0.36283429831816927,
        "step": 1316
    },
    {
        "loss": 2.3541,
        "grad_norm": 1.4498934745788574,
        "learning_rate": 0.00019715805552965077,
        "epoch": 0.3631100082712986,
        "step": 1317
    },
    {
        "loss": 2.3284,
        "grad_norm": 1.196873426437378,
        "learning_rate": 0.00019709578711143584,
        "epoch": 0.3633857182244279,
        "step": 1318
    },
    {
        "loss": 2.1614,
        "grad_norm": 1.7339799404144287,
        "learning_rate": 0.00019703285394837797,
        "epoch": 0.3636614281775572,
        "step": 1319
    },
    {
        "loss": 2.399,
        "grad_norm": 1.1051734685897827,
        "learning_rate": 0.00019696925647133507,
        "epoch": 0.3639371381306865,
        "step": 1320
    },
    {
        "loss": 2.3378,
        "grad_norm": 1.2979940176010132,
        "learning_rate": 0.0001969049951157133,
        "epoch": 0.3642128480838158,
        "step": 1321
    },
    {
        "loss": 2.163,
        "grad_norm": 1.5325013399124146,
        "learning_rate": 0.00019684007032146376,
        "epoch": 0.36448855803694513,
        "step": 1322
    },
    {
        "loss": 2.0845,
        "grad_norm": 1.904056191444397,
        "learning_rate": 0.00019677448253307972,
        "epoch": 0.36476426799007444,
        "step": 1323
    },
    {
        "loss": 1.84,
        "grad_norm": 2.276716947555542,
        "learning_rate": 0.00019670823219959348,
        "epoch": 0.36503997794320375,
        "step": 1324
    },
    {
        "loss": 2.1797,
        "grad_norm": 1.5949615240097046,
        "learning_rate": 0.0001966413197745733,
        "epoch": 0.36531568789633306,
        "step": 1325
    },
    {
        "loss": 1.8864,
        "grad_norm": 2.090254783630371,
        "learning_rate": 0.00019657374571612033,
        "epoch": 0.3655913978494624,
        "step": 1326
    },
    {
        "loss": 2.504,
        "grad_norm": 2.033329963684082,
        "learning_rate": 0.00019650551048686537,
        "epoch": 0.3658671078025917,
        "step": 1327
    },
    {
        "loss": 1.8673,
        "grad_norm": 2.0675227642059326,
        "learning_rate": 0.0001964366145539659,
        "epoch": 0.366142817755721,
        "step": 1328
    },
    {
        "loss": 2.2906,
        "grad_norm": 1.8509440422058105,
        "learning_rate": 0.00019636705838910263,
        "epoch": 0.3664185277088503,
        "step": 1329
    },
    {
        "loss": 2.488,
        "grad_norm": 1.7717100381851196,
        "learning_rate": 0.00019629684246847648,
        "epoch": 0.3666942376619796,
        "step": 1330
    },
    {
        "loss": 1.7922,
        "grad_norm": 2.2180747985839844,
        "learning_rate": 0.0001962259672728053,
        "epoch": 0.3669699476151089,
        "step": 1331
    },
    {
        "loss": 1.6701,
        "grad_norm": 1.8832826614379883,
        "learning_rate": 0.00019615443328732036,
        "epoch": 0.36724565756823824,
        "step": 1332
    },
    {
        "loss": 2.4527,
        "grad_norm": 1.509048342704773,
        "learning_rate": 0.00019608224100176332,
        "epoch": 0.36752136752136755,
        "step": 1333
    },
    {
        "loss": 1.9536,
        "grad_norm": 1.7860606908798218,
        "learning_rate": 0.00019600939091038265,
        "epoch": 0.3677970774744968,
        "step": 1334
    },
    {
        "loss": 2.1574,
        "grad_norm": 1.2769107818603516,
        "learning_rate": 0.0001959358835119305,
        "epoch": 0.3680727874276261,
        "step": 1335
    },
    {
        "loss": 2.2619,
        "grad_norm": 1.1967796087265015,
        "learning_rate": 0.00019586171930965898,
        "epoch": 0.3683484973807554,
        "step": 1336
    },
    {
        "loss": 1.7151,
        "grad_norm": 1.8826438188552856,
        "learning_rate": 0.00019578689881131686,
        "epoch": 0.36862420733388473,
        "step": 1337
    },
    {
        "loss": 2.1184,
        "grad_norm": 1.0587996244430542,
        "learning_rate": 0.0001957114225291462,
        "epoch": 0.36889991728701405,
        "step": 1338
    },
    {
        "loss": 2.3863,
        "grad_norm": 1.2015823125839233,
        "learning_rate": 0.00019563529097987864,
        "epoch": 0.36917562724014336,
        "step": 1339
    },
    {
        "loss": 1.9047,
        "grad_norm": 1.9190897941589355,
        "learning_rate": 0.00019555850468473205,
        "epoch": 0.36945133719327267,
        "step": 1340
    },
    {
        "loss": 2.644,
        "grad_norm": 1.358314871788025,
        "learning_rate": 0.00019548106416940678,
        "epoch": 0.369727047146402,
        "step": 1341
    },
    {
        "loss": 2.3566,
        "grad_norm": 1.7169415950775146,
        "learning_rate": 0.0001954029699640822,
        "epoch": 0.3700027570995313,
        "step": 1342
    },
    {
        "loss": 2.0052,
        "grad_norm": 1.8301228284835815,
        "learning_rate": 0.00019532422260341305,
        "epoch": 0.3702784670526606,
        "step": 1343
    },
    {
        "loss": 2.2391,
        "grad_norm": 1.208432674407959,
        "learning_rate": 0.00019524482262652575,
        "epoch": 0.3705541770057899,
        "step": 1344
    },
    {
        "loss": 2.5561,
        "grad_norm": 1.1185325384140015,
        "learning_rate": 0.00019516477057701464,
        "epoch": 0.3708298869589192,
        "step": 1345
    },
    {
        "loss": 1.9534,
        "grad_norm": 1.2461421489715576,
        "learning_rate": 0.0001950840670029384,
        "epoch": 0.37110559691204853,
        "step": 1346
    },
    {
        "loss": 2.488,
        "grad_norm": 1.5579354763031006,
        "learning_rate": 0.0001950027124568162,
        "epoch": 0.37138130686517784,
        "step": 1347
    },
    {
        "loss": 2.3983,
        "grad_norm": 1.3663749694824219,
        "learning_rate": 0.000194920707495624,
        "epoch": 0.37165701681830715,
        "step": 1348
    },
    {
        "loss": 2.0423,
        "grad_norm": 1.5233078002929688,
        "learning_rate": 0.00019483805268079055,
        "epoch": 0.37193272677143646,
        "step": 1349
    },
    {
        "loss": 2.1789,
        "grad_norm": 1.5172481536865234,
        "learning_rate": 0.00019475474857819387,
        "epoch": 0.37220843672456577,
        "step": 1350
    },
    {
        "loss": 2.3051,
        "grad_norm": 1.0819087028503418,
        "learning_rate": 0.00019467079575815702,
        "epoch": 0.3724841466776951,
        "step": 1351
    },
    {
        "loss": 2.4919,
        "grad_norm": 1.310520052909851,
        "learning_rate": 0.00019458619479544443,
        "epoch": 0.3727598566308244,
        "step": 1352
    },
    {
        "loss": 1.5289,
        "grad_norm": 2.302269697189331,
        "learning_rate": 0.00019450094626925792,
        "epoch": 0.3730355665839537,
        "step": 1353
    },
    {
        "loss": 2.3527,
        "grad_norm": 1.6399240493774414,
        "learning_rate": 0.00019441505076323265,
        "epoch": 0.373311276537083,
        "step": 1354
    },
    {
        "loss": 2.2984,
        "grad_norm": 1.2616556882858276,
        "learning_rate": 0.00019432850886543323,
        "epoch": 0.37358698649021227,
        "step": 1355
    },
    {
        "loss": 2.1454,
        "grad_norm": 1.4282352924346924,
        "learning_rate": 0.00019424132116834965,
        "epoch": 0.3738626964433416,
        "step": 1356
    },
    {
        "loss": 2.3954,
        "grad_norm": 1.6443812847137451,
        "learning_rate": 0.00019415348826889317,
        "epoch": 0.3741384063964709,
        "step": 1357
    },
    {
        "loss": 1.8566,
        "grad_norm": 1.4753212928771973,
        "learning_rate": 0.00019406501076839233,
        "epoch": 0.3744141163496002,
        "step": 1358
    },
    {
        "loss": 2.3572,
        "grad_norm": 1.6669672727584839,
        "learning_rate": 0.0001939758892725888,
        "epoch": 0.3746898263027295,
        "step": 1359
    },
    {
        "loss": 1.979,
        "grad_norm": 1.4695093631744385,
        "learning_rate": 0.00019388612439163314,
        "epoch": 0.3749655362558588,
        "step": 1360
    },
    {
        "loss": 1.9145,
        "grad_norm": 1.886117935180664,
        "learning_rate": 0.00019379571674008083,
        "epoch": 0.37524124620898813,
        "step": 1361
    },
    {
        "loss": 2.149,
        "grad_norm": 1.2692856788635254,
        "learning_rate": 0.00019370466693688785,
        "epoch": 0.37551695616211744,
        "step": 1362
    },
    {
        "loss": 2.4227,
        "grad_norm": 1.153438687324524,
        "learning_rate": 0.00019361297560540654,
        "epoch": 0.37579266611524675,
        "step": 1363
    },
    {
        "loss": 2.1008,
        "grad_norm": 2.194141149520874,
        "learning_rate": 0.00019352064337338135,
        "epoch": 0.37606837606837606,
        "step": 1364
    },
    {
        "loss": 2.2233,
        "grad_norm": 2.159224033355713,
        "learning_rate": 0.00019342767087294452,
        "epoch": 0.3763440860215054,
        "step": 1365
    },
    {
        "loss": 2.4941,
        "grad_norm": 1.5569744110107422,
        "learning_rate": 0.00019333405874061163,
        "epoch": 0.3766197959746347,
        "step": 1366
    },
    {
        "loss": 2.5875,
        "grad_norm": 0.9397294521331787,
        "learning_rate": 0.00019323980761727752,
        "epoch": 0.376895505927764,
        "step": 1367
    },
    {
        "loss": 2.2635,
        "grad_norm": 1.2682878971099854,
        "learning_rate": 0.00019314491814821172,
        "epoch": 0.3771712158808933,
        "step": 1368
    },
    {
        "loss": 1.4265,
        "grad_norm": 1.2943263053894043,
        "learning_rate": 0.0001930493909830539,
        "epoch": 0.3774469258340226,
        "step": 1369
    },
    {
        "loss": 2.5014,
        "grad_norm": 1.2402663230895996,
        "learning_rate": 0.0001929532267758097,
        "epoch": 0.37772263578715193,
        "step": 1370
    },
    {
        "loss": 2.3903,
        "grad_norm": 1.5367889404296875,
        "learning_rate": 0.0001928564261848462,
        "epoch": 0.37799834574028124,
        "step": 1371
    },
    {
        "loss": 2.3162,
        "grad_norm": 1.3971974849700928,
        "learning_rate": 0.00019275898987288712,
        "epoch": 0.37827405569341055,
        "step": 1372
    },
    {
        "loss": 2.2705,
        "grad_norm": 1.582932472229004,
        "learning_rate": 0.00019266091850700875,
        "epoch": 0.37854976564653986,
        "step": 1373
    },
    {
        "loss": 2.5389,
        "grad_norm": 1.0897045135498047,
        "learning_rate": 0.0001925622127586349,
        "epoch": 0.37882547559966917,
        "step": 1374
    },
    {
        "loss": 2.0287,
        "grad_norm": 1.9664300680160522,
        "learning_rate": 0.00019246287330353275,
        "epoch": 0.3791011855527985,
        "step": 1375
    },
    {
        "loss": 2.0977,
        "grad_norm": 1.4933046102523804,
        "learning_rate": 0.00019236290082180787,
        "epoch": 0.3793768955059278,
        "step": 1376
    },
    {
        "loss": 1.938,
        "grad_norm": 1.5134085416793823,
        "learning_rate": 0.00019226229599789978,
        "epoch": 0.37965260545905705,
        "step": 1377
    },
    {
        "loss": 2.0469,
        "grad_norm": 1.2033888101577759,
        "learning_rate": 0.00019216105952057715,
        "epoch": 0.37992831541218636,
        "step": 1378
    },
    {
        "loss": 2.3735,
        "grad_norm": 1.0298395156860352,
        "learning_rate": 0.00019205919208293315,
        "epoch": 0.38020402536531567,
        "step": 1379
    },
    {
        "loss": 2.2215,
        "grad_norm": 2.0296144485473633,
        "learning_rate": 0.00019195669438238069,
        "epoch": 0.380479735318445,
        "step": 1380
    },
    {
        "loss": 2.1419,
        "grad_norm": 1.534394383430481,
        "learning_rate": 0.0001918535671206476,
        "epoch": 0.3807554452715743,
        "step": 1381
    },
    {
        "loss": 1.4808,
        "grad_norm": 2.0314769744873047,
        "learning_rate": 0.0001917498110037719,
        "epoch": 0.3810311552247036,
        "step": 1382
    },
    {
        "loss": 1.9731,
        "grad_norm": 2.1229748725891113,
        "learning_rate": 0.0001916454267420969,
        "epoch": 0.3813068651778329,
        "step": 1383
    },
    {
        "loss": 2.079,
        "grad_norm": 1.779715657234192,
        "learning_rate": 0.00019154041505026634,
        "epoch": 0.3815825751309622,
        "step": 1384
    },
    {
        "loss": 1.7577,
        "grad_norm": 1.934985876083374,
        "learning_rate": 0.0001914347766472196,
        "epoch": 0.38185828508409153,
        "step": 1385
    },
    {
        "loss": 2.641,
        "grad_norm": 0.9645094275474548,
        "learning_rate": 0.00019132851225618666,
        "epoch": 0.38213399503722084,
        "step": 1386
    },
    {
        "loss": 2.3399,
        "grad_norm": 1.7916741371154785,
        "learning_rate": 0.0001912216226046831,
        "epoch": 0.38240970499035015,
        "step": 1387
    },
    {
        "loss": 2.537,
        "grad_norm": 1.6906548738479614,
        "learning_rate": 0.0001911141084245054,
        "epoch": 0.38268541494347946,
        "step": 1388
    },
    {
        "loss": 2.5557,
        "grad_norm": 1.0614547729492188,
        "learning_rate": 0.00019100597045172558,
        "epoch": 0.3829611248966088,
        "step": 1389
    },
    {
        "loss": 2.3898,
        "grad_norm": 1.121311068534851,
        "learning_rate": 0.00019089720942668635,
        "epoch": 0.3832368348497381,
        "step": 1390
    },
    {
        "loss": 2.3705,
        "grad_norm": 1.1282330751419067,
        "learning_rate": 0.000190787826093996,
        "epoch": 0.3835125448028674,
        "step": 1391
    },
    {
        "loss": 2.5324,
        "grad_norm": 1.281639575958252,
        "learning_rate": 0.00019067782120252345,
        "epoch": 0.3837882547559967,
        "step": 1392
    },
    {
        "loss": 2.4787,
        "grad_norm": 1.3080203533172607,
        "learning_rate": 0.00019056719550539277,
        "epoch": 0.384063964709126,
        "step": 1393
    },
    {
        "loss": 2.0801,
        "grad_norm": 1.2578190565109253,
        "learning_rate": 0.00019045594975997836,
        "epoch": 0.3843396746622553,
        "step": 1394
    },
    {
        "loss": 2.0725,
        "grad_norm": 1.3268874883651733,
        "learning_rate": 0.00019034408472789967,
        "epoch": 0.38461538461538464,
        "step": 1395
    },
    {
        "loss": 2.1558,
        "grad_norm": 1.3325828313827515,
        "learning_rate": 0.00019023160117501587,
        "epoch": 0.38489109456851395,
        "step": 1396
    },
    {
        "loss": 2.1785,
        "grad_norm": 2.2931339740753174,
        "learning_rate": 0.00019011849987142073,
        "epoch": 0.38516680452164326,
        "step": 1397
    },
    {
        "loss": 1.8403,
        "grad_norm": 3.184171199798584,
        "learning_rate": 0.00019000478159143734,
        "epoch": 0.3854425144747725,
        "step": 1398
    },
    {
        "loss": 2.2441,
        "grad_norm": 1.2438610792160034,
        "learning_rate": 0.0001898904471136128,
        "epoch": 0.3857182244279018,
        "step": 1399
    },
    {
        "loss": 1.6912,
        "grad_norm": 2.1398003101348877,
        "learning_rate": 0.00018977549722071278,
        "epoch": 0.38599393438103113,
        "step": 1400
    },
    {
        "loss": 2.3139,
        "grad_norm": 1.0878092050552368,
        "learning_rate": 0.00018965993269971632,
        "epoch": 0.38626964433416044,
        "step": 1401
    },
    {
        "loss": 2.0807,
        "grad_norm": 1.2608789205551147,
        "learning_rate": 0.00018954375434181045,
        "epoch": 0.38654535428728976,
        "step": 1402
    },
    {
        "loss": 2.2545,
        "grad_norm": 1.469421625137329,
        "learning_rate": 0.00018942696294238452,
        "epoch": 0.38682106424041907,
        "step": 1403
    },
    {
        "loss": 1.9831,
        "grad_norm": 1.5711381435394287,
        "learning_rate": 0.00018930955930102506,
        "epoch": 0.3870967741935484,
        "step": 1404
    },
    {
        "loss": 2.5523,
        "grad_norm": 1.1949489116668701,
        "learning_rate": 0.00018919154422151015,
        "epoch": 0.3873724841466777,
        "step": 1405
    },
    {
        "loss": 2.0681,
        "grad_norm": 1.4689981937408447,
        "learning_rate": 0.00018907291851180392,
        "epoch": 0.387648194099807,
        "step": 1406
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.017677664756775,
        "learning_rate": 0.00018895368298405113,
        "epoch": 0.3879239040529363,
        "step": 1407
    },
    {
        "loss": 1.8973,
        "grad_norm": 1.4059128761291504,
        "learning_rate": 0.0001888338384545714,
        "epoch": 0.3881996140060656,
        "step": 1408
    },
    {
        "loss": 2.5135,
        "grad_norm": 1.2904311418533325,
        "learning_rate": 0.00018871338574385385,
        "epoch": 0.38847532395919493,
        "step": 1409
    },
    {
        "loss": 2.3933,
        "grad_norm": 1.1436296701431274,
        "learning_rate": 0.0001885923256765513,
        "epoch": 0.38875103391232424,
        "step": 1410
    },
    {
        "loss": 2.2257,
        "grad_norm": 1.7740600109100342,
        "learning_rate": 0.0001884706590814748,
        "epoch": 0.38902674386545355,
        "step": 1411
    },
    {
        "loss": 1.9287,
        "grad_norm": 1.8003277778625488,
        "learning_rate": 0.0001883483867915877,
        "epoch": 0.38930245381858286,
        "step": 1412
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.5335197448730469,
        "learning_rate": 0.0001882255096440003,
        "epoch": 0.38957816377171217,
        "step": 1413
    },
    {
        "loss": 1.9847,
        "grad_norm": 1.0535396337509155,
        "learning_rate": 0.0001881020284799638,
        "epoch": 0.3898538737248415,
        "step": 1414
    },
    {
        "loss": 2.7902,
        "grad_norm": 1.3111070394515991,
        "learning_rate": 0.00018797794414486467,
        "epoch": 0.3901295836779708,
        "step": 1415
    },
    {
        "loss": 2.389,
        "grad_norm": 1.4213991165161133,
        "learning_rate": 0.0001878532574882189,
        "epoch": 0.3904052936311001,
        "step": 1416
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.4503288269042969,
        "learning_rate": 0.0001877279693636661,
        "epoch": 0.3906810035842294,
        "step": 1417
    },
    {
        "loss": 2.5624,
        "grad_norm": 1.1064308881759644,
        "learning_rate": 0.00018760208062896375,
        "epoch": 0.3909567135373587,
        "step": 1418
    },
    {
        "loss": 2.5928,
        "grad_norm": 1.0884805917739868,
        "learning_rate": 0.00018747559214598122,
        "epoch": 0.391232423490488,
        "step": 1419
    },
    {
        "loss": 2.4397,
        "grad_norm": 1.2497602701187134,
        "learning_rate": 0.000187348504780694,
        "epoch": 0.3915081334436173,
        "step": 1420
    },
    {
        "loss": 2.2991,
        "grad_norm": 1.1326255798339844,
        "learning_rate": 0.00018722081940317754,
        "epoch": 0.3917838433967466,
        "step": 1421
    },
    {
        "loss": 1.9075,
        "grad_norm": 0.8433361649513245,
        "learning_rate": 0.00018709253688760165,
        "epoch": 0.3920595533498759,
        "step": 1422
    },
    {
        "loss": 1.3019,
        "grad_norm": 2.039034843444824,
        "learning_rate": 0.00018696365811222417,
        "epoch": 0.3923352633030052,
        "step": 1423
    },
    {
        "loss": 2.6303,
        "grad_norm": 1.0771275758743286,
        "learning_rate": 0.00018683418395938517,
        "epoch": 0.39261097325613453,
        "step": 1424
    },
    {
        "loss": 2.1336,
        "grad_norm": 1.4744306802749634,
        "learning_rate": 0.0001867041153155008,
        "epoch": 0.39288668320926384,
        "step": 1425
    },
    {
        "loss": 2.2709,
        "grad_norm": 1.6044960021972656,
        "learning_rate": 0.00018657345307105728,
        "epoch": 0.39316239316239315,
        "step": 1426
    },
    {
        "loss": 2.2018,
        "grad_norm": 1.7960457801818848,
        "learning_rate": 0.00018644219812060476,
        "epoch": 0.39343810311552246,
        "step": 1427
    },
    {
        "loss": 2.0244,
        "grad_norm": 2.192720890045166,
        "learning_rate": 0.00018631035136275128,
        "epoch": 0.3937138130686518,
        "step": 1428
    },
    {
        "loss": 2.127,
        "grad_norm": 1.2468626499176025,
        "learning_rate": 0.0001861779137001565,
        "epoch": 0.3939895230217811,
        "step": 1429
    },
    {
        "loss": 2.1247,
        "grad_norm": 1.6059216260910034,
        "learning_rate": 0.00018604488603952564,
        "epoch": 0.3942652329749104,
        "step": 1430
    },
    {
        "loss": 2.4633,
        "grad_norm": 1.2274236679077148,
        "learning_rate": 0.0001859112692916031,
        "epoch": 0.3945409429280397,
        "step": 1431
    },
    {
        "loss": 2.0178,
        "grad_norm": 1.478891134262085,
        "learning_rate": 0.0001857770643711665,
        "epoch": 0.394816652881169,
        "step": 1432
    },
    {
        "loss": 1.5231,
        "grad_norm": 1.956719160079956,
        "learning_rate": 0.00018564227219702005,
        "epoch": 0.3950923628342983,
        "step": 1433
    },
    {
        "loss": 2.2249,
        "grad_norm": 1.627255916595459,
        "learning_rate": 0.00018550689369198868,
        "epoch": 0.39536807278742764,
        "step": 1434
    },
    {
        "loss": 2.4152,
        "grad_norm": 1.731845736503601,
        "learning_rate": 0.00018537092978291133,
        "epoch": 0.39564378274055695,
        "step": 1435
    },
    {
        "loss": 2.3539,
        "grad_norm": 2.1481380462646484,
        "learning_rate": 0.0001852343814006349,
        "epoch": 0.39591949269368626,
        "step": 1436
    },
    {
        "loss": 2.2178,
        "grad_norm": 1.575545072555542,
        "learning_rate": 0.00018509724948000765,
        "epoch": 0.39619520264681557,
        "step": 1437
    },
    {
        "loss": 2.5679,
        "grad_norm": 1.4799379110336304,
        "learning_rate": 0.000184959534959873,
        "epoch": 0.3964709125999449,
        "step": 1438
    },
    {
        "loss": 1.6864,
        "grad_norm": 2.203280210494995,
        "learning_rate": 0.00018482123878306295,
        "epoch": 0.3967466225530742,
        "step": 1439
    },
    {
        "loss": 2.099,
        "grad_norm": 1.8432307243347168,
        "learning_rate": 0.0001846823618963917,
        "epoch": 0.3970223325062035,
        "step": 1440
    },
    {
        "loss": 2.3283,
        "grad_norm": 0.8994532823562622,
        "learning_rate": 0.00018454290525064918,
        "epoch": 0.39729804245933276,
        "step": 1441
    },
    {
        "loss": 2.5114,
        "grad_norm": 1.1329796314239502,
        "learning_rate": 0.00018440286980059447,
        "epoch": 0.39757375241246207,
        "step": 1442
    },
    {
        "loss": 1.5726,
        "grad_norm": 1.6057829856872559,
        "learning_rate": 0.00018426225650494934,
        "epoch": 0.3978494623655914,
        "step": 1443
    },
    {
        "loss": 1.9943,
        "grad_norm": 1.937785029411316,
        "learning_rate": 0.0001841210663263916,
        "epoch": 0.3981251723187207,
        "step": 1444
    },
    {
        "loss": 2.0138,
        "grad_norm": 1.4057210683822632,
        "learning_rate": 0.00018397930023154863,
        "epoch": 0.39840088227185,
        "step": 1445
    },
    {
        "loss": 2.0017,
        "grad_norm": 1.6966906785964966,
        "learning_rate": 0.00018383695919099065,
        "epoch": 0.3986765922249793,
        "step": 1446
    },
    {
        "loss": 2.0115,
        "grad_norm": 1.7118133306503296,
        "learning_rate": 0.00018369404417922414,
        "epoch": 0.3989523021781086,
        "step": 1447
    },
    {
        "loss": 2.469,
        "grad_norm": 1.2592318058013916,
        "learning_rate": 0.00018355055617468507,
        "epoch": 0.39922801213123793,
        "step": 1448
    },
    {
        "loss": 1.4899,
        "grad_norm": 2.018605947494507,
        "learning_rate": 0.00018340649615973247,
        "epoch": 0.39950372208436724,
        "step": 1449
    },
    {
        "loss": 1.8174,
        "grad_norm": 1.4350917339324951,
        "learning_rate": 0.00018326186512064127,
        "epoch": 0.39977943203749655,
        "step": 1450
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.197942852973938,
        "learning_rate": 0.00018311666404759595,
        "epoch": 0.40005514199062586,
        "step": 1451
    },
    {
        "loss": 2.0899,
        "grad_norm": 1.5730501413345337,
        "learning_rate": 0.00018297089393468362,
        "epoch": 0.4003308519437552,
        "step": 1452
    },
    {
        "loss": 2.4305,
        "grad_norm": 1.5927118062973022,
        "learning_rate": 0.000182824555779887,
        "epoch": 0.4006065618968845,
        "step": 1453
    },
    {
        "loss": 2.3118,
        "grad_norm": 0.8998891711235046,
        "learning_rate": 0.00018267765058507804,
        "epoch": 0.4008822718500138,
        "step": 1454
    },
    {
        "loss": 2.4028,
        "grad_norm": 1.0082247257232666,
        "learning_rate": 0.00018253017935601062,
        "epoch": 0.4011579818031431,
        "step": 1455
    },
    {
        "loss": 2.0692,
        "grad_norm": 1.6583516597747803,
        "learning_rate": 0.00018238214310231394,
        "epoch": 0.4014336917562724,
        "step": 1456
    },
    {
        "loss": 2.5651,
        "grad_norm": 1.5108826160430908,
        "learning_rate": 0.00018223354283748545,
        "epoch": 0.4017094017094017,
        "step": 1457
    },
    {
        "loss": 1.2287,
        "grad_norm": 2.261760711669922,
        "learning_rate": 0.00018208437957888406,
        "epoch": 0.40198511166253104,
        "step": 1458
    },
    {
        "loss": 2.0491,
        "grad_norm": 1.7802832126617432,
        "learning_rate": 0.00018193465434772303,
        "epoch": 0.40226082161566035,
        "step": 1459
    },
    {
        "loss": 2.3872,
        "grad_norm": 2.0717711448669434,
        "learning_rate": 0.00018178436816906305,
        "epoch": 0.40253653156878966,
        "step": 1460
    },
    {
        "loss": 2.5184,
        "grad_norm": 1.1049704551696777,
        "learning_rate": 0.00018163352207180525,
        "epoch": 0.40281224152191897,
        "step": 1461
    },
    {
        "loss": 2.5428,
        "grad_norm": 1.8804256916046143,
        "learning_rate": 0.00018148211708868403,
        "epoch": 0.4030879514750482,
        "step": 1462
    },
    {
        "loss": 2.3697,
        "grad_norm": 1.5174530744552612,
        "learning_rate": 0.00018133015425626016,
        "epoch": 0.40336366142817753,
        "step": 1463
    },
    {
        "loss": 2.3934,
        "grad_norm": 1.0851466655731201,
        "learning_rate": 0.00018117763461491363,
        "epoch": 0.40363937138130684,
        "step": 1464
    },
    {
        "loss": 2.2796,
        "grad_norm": 1.0518449544906616,
        "learning_rate": 0.0001810245592088364,
        "epoch": 0.40391508133443615,
        "step": 1465
    },
    {
        "loss": 1.9661,
        "grad_norm": 1.1824527978897095,
        "learning_rate": 0.00018087092908602542,
        "epoch": 0.40419079128756547,
        "step": 1466
    },
    {
        "loss": 2.4026,
        "grad_norm": 1.1530604362487793,
        "learning_rate": 0.00018071674529827533,
        "epoch": 0.4044665012406948,
        "step": 1467
    },
    {
        "loss": 1.7174,
        "grad_norm": 1.6548501253128052,
        "learning_rate": 0.00018056200890117138,
        "epoch": 0.4047422111938241,
        "step": 1468
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.9351414442062378,
        "learning_rate": 0.0001804067209540821,
        "epoch": 0.4050179211469534,
        "step": 1469
    },
    {
        "loss": 2.1387,
        "grad_norm": 1.573903203010559,
        "learning_rate": 0.0001802508825201521,
        "epoch": 0.4052936311000827,
        "step": 1470
    },
    {
        "loss": 2.2509,
        "grad_norm": 1.6998422145843506,
        "learning_rate": 0.0001800944946662948,
        "epoch": 0.405569341053212,
        "step": 1471
    },
    {
        "loss": 2.3057,
        "grad_norm": 1.5021321773529053,
        "learning_rate": 0.000179937558463185,
        "epoch": 0.40584505100634133,
        "step": 1472
    },
    {
        "loss": 2.5431,
        "grad_norm": 1.0811764001846313,
        "learning_rate": 0.00017978007498525175,
        "epoch": 0.40612076095947064,
        "step": 1473
    },
    {
        "loss": 2.2068,
        "grad_norm": 1.3028026819229126,
        "learning_rate": 0.0001796220453106709,
        "epoch": 0.40639647091259995,
        "step": 1474
    },
    {
        "loss": 2.5478,
        "grad_norm": 1.5284603834152222,
        "learning_rate": 0.00017946347052135767,
        "epoch": 0.40667218086572926,
        "step": 1475
    },
    {
        "loss": 2.1944,
        "grad_norm": 1.127044677734375,
        "learning_rate": 0.0001793043517029593,
        "epoch": 0.40694789081885857,
        "step": 1476
    },
    {
        "loss": 2.1232,
        "grad_norm": 1.53788423538208,
        "learning_rate": 0.0001791446899448476,
        "epoch": 0.4072236007719879,
        "step": 1477
    },
    {
        "loss": 2.3769,
        "grad_norm": 1.6154946088790894,
        "learning_rate": 0.00017898448634011147,
        "epoch": 0.4074993107251172,
        "step": 1478
    },
    {
        "loss": 1.5374,
        "grad_norm": 2.015620470046997,
        "learning_rate": 0.00017882374198554952,
        "epoch": 0.4077750206782465,
        "step": 1479
    },
    {
        "loss": 1.7378,
        "grad_norm": 1.5224170684814453,
        "learning_rate": 0.00017866245798166245,
        "epoch": 0.4080507306313758,
        "step": 1480
    },
    {
        "loss": 2.2642,
        "grad_norm": 1.3636291027069092,
        "learning_rate": 0.0001785006354326455,
        "epoch": 0.4083264405845051,
        "step": 1481
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.5216232538223267,
        "learning_rate": 0.000178338275446381,
        "epoch": 0.40860215053763443,
        "step": 1482
    },
    {
        "loss": 2.0388,
        "grad_norm": 1.8568509817123413,
        "learning_rate": 0.00017817537913443073,
        "epoch": 0.4088778604907637,
        "step": 1483
    },
    {
        "loss": 2.2598,
        "grad_norm": 1.5358729362487793,
        "learning_rate": 0.0001780119476120282,
        "epoch": 0.409153570443893,
        "step": 1484
    },
    {
        "loss": 2.0365,
        "grad_norm": 2.009145736694336,
        "learning_rate": 0.0001778479819980713,
        "epoch": 0.4094292803970223,
        "step": 1485
    },
    {
        "loss": 2.1552,
        "grad_norm": 1.7722018957138062,
        "learning_rate": 0.0001776834834151143,
        "epoch": 0.4097049903501516,
        "step": 1486
    },
    {
        "loss": 2.4732,
        "grad_norm": 1.2719074487686157,
        "learning_rate": 0.0001775184529893604,
        "epoch": 0.40998070030328093,
        "step": 1487
    },
    {
        "loss": 2.3161,
        "grad_norm": 1.0403790473937988,
        "learning_rate": 0.00017735289185065396,
        "epoch": 0.41025641025641024,
        "step": 1488
    },
    {
        "loss": 2.1486,
        "grad_norm": 1.6958420276641846,
        "learning_rate": 0.00017718680113247264,
        "epoch": 0.41053212020953955,
        "step": 1489
    },
    {
        "loss": 1.8672,
        "grad_norm": 1.7852256298065186,
        "learning_rate": 0.0001770201819719199,
        "epoch": 0.41080783016266886,
        "step": 1490
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.665194034576416,
        "learning_rate": 0.00017685303550971698,
        "epoch": 0.4110835401157982,
        "step": 1491
    },
    {
        "loss": 2.3616,
        "grad_norm": 1.4001842737197876,
        "learning_rate": 0.00017668536289019518,
        "epoch": 0.4113592500689275,
        "step": 1492
    },
    {
        "loss": 2.0378,
        "grad_norm": 1.0425612926483154,
        "learning_rate": 0.00017651716526128798,
        "epoch": 0.4116349600220568,
        "step": 1493
    },
    {
        "loss": 2.1876,
        "grad_norm": 1.50534188747406,
        "learning_rate": 0.00017634844377452334,
        "epoch": 0.4119106699751861,
        "step": 1494
    },
    {
        "loss": 2.4447,
        "grad_norm": 1.345501184463501,
        "learning_rate": 0.00017617919958501556,
        "epoch": 0.4121863799283154,
        "step": 1495
    },
    {
        "loss": 2.0012,
        "grad_norm": 1.83766770362854,
        "learning_rate": 0.0001760094338514576,
        "epoch": 0.4124620898814447,
        "step": 1496
    },
    {
        "loss": 1.5631,
        "grad_norm": 1.547347903251648,
        "learning_rate": 0.00017583914773611304,
        "epoch": 0.41273779983457404,
        "step": 1497
    },
    {
        "loss": 2.6592,
        "grad_norm": 1.6100478172302246,
        "learning_rate": 0.0001756683424048081,
        "epoch": 0.41301350978770335,
        "step": 1498
    },
    {
        "loss": 2.2353,
        "grad_norm": 0.9313831925392151,
        "learning_rate": 0.00017549701902692372,
        "epoch": 0.41328921974083266,
        "step": 1499
    },
    {
        "loss": 2.424,
        "grad_norm": 1.0071804523468018,
        "learning_rate": 0.00017532517877538758,
        "epoch": 0.41356492969396197,
        "step": 1500
    },
    {
        "loss": 2.3087,
        "grad_norm": 1.348495364189148,
        "learning_rate": 0.0001751528228266659,
        "epoch": 0.4138406396470913,
        "step": 1501
    },
    {
        "loss": 2.261,
        "grad_norm": 1.4709184169769287,
        "learning_rate": 0.00017497995236075568,
        "epoch": 0.4141163496002206,
        "step": 1502
    },
    {
        "loss": 1.7604,
        "grad_norm": 1.8347729444503784,
        "learning_rate": 0.00017480656856117625,
        "epoch": 0.4143920595533499,
        "step": 1503
    },
    {
        "loss": 2.4959,
        "grad_norm": 1.3949979543685913,
        "learning_rate": 0.00017463267261496153,
        "epoch": 0.4146677695064792,
        "step": 1504
    },
    {
        "loss": 2.1926,
        "grad_norm": 1.2113310098648071,
        "learning_rate": 0.00017445826571265167,
        "epoch": 0.41494347945960847,
        "step": 1505
    },
    {
        "loss": 2.4503,
        "grad_norm": 1.2043735980987549,
        "learning_rate": 0.00017428334904828497,
        "epoch": 0.4152191894127378,
        "step": 1506
    },
    {
        "loss": 2.5874,
        "grad_norm": 1.2863322496414185,
        "learning_rate": 0.00017410792381938967,
        "epoch": 0.4154948993658671,
        "step": 1507
    },
    {
        "loss": 2.0751,
        "grad_norm": 1.9548009634017944,
        "learning_rate": 0.00017393199122697585,
        "epoch": 0.4157706093189964,
        "step": 1508
    },
    {
        "loss": 2.3475,
        "grad_norm": 1.2002599239349365,
        "learning_rate": 0.00017375555247552712,
        "epoch": 0.4160463192721257,
        "step": 1509
    },
    {
        "loss": 2.5715,
        "grad_norm": 1.0575823783874512,
        "learning_rate": 0.00017357860877299238,
        "epoch": 0.416322029225255,
        "step": 1510
    },
    {
        "loss": 2.0057,
        "grad_norm": 1.346957802772522,
        "learning_rate": 0.0001734011613307776,
        "epoch": 0.41659773917838433,
        "step": 1511
    },
    {
        "loss": 2.3186,
        "grad_norm": 1.2715191841125488,
        "learning_rate": 0.00017322321136373743,
        "epoch": 0.41687344913151364,
        "step": 1512
    },
    {
        "loss": 2.1931,
        "grad_norm": 1.5083470344543457,
        "learning_rate": 0.000173044760090167,
        "epoch": 0.41714915908464295,
        "step": 1513
    },
    {
        "loss": 2.3603,
        "grad_norm": 1.2006200551986694,
        "learning_rate": 0.0001728658087317935,
        "epoch": 0.41742486903777226,
        "step": 1514
    },
    {
        "loss": 2.294,
        "grad_norm": 1.6900197267532349,
        "learning_rate": 0.00017268635851376785,
        "epoch": 0.41770057899090157,
        "step": 1515
    },
    {
        "loss": 1.9585,
        "grad_norm": 0.9192642569541931,
        "learning_rate": 0.00017250641066465628,
        "epoch": 0.4179762889440309,
        "step": 1516
    },
    {
        "loss": 2.3856,
        "grad_norm": 1.6785756349563599,
        "learning_rate": 0.00017232596641643202,
        "epoch": 0.4182519988971602,
        "step": 1517
    },
    {
        "loss": 2.1523,
        "grad_norm": 2.334286689758301,
        "learning_rate": 0.00017214502700446668,
        "epoch": 0.4185277088502895,
        "step": 1518
    },
    {
        "loss": 2.3382,
        "grad_norm": 1.5048068761825562,
        "learning_rate": 0.00017196359366752195,
        "epoch": 0.4188034188034188,
        "step": 1519
    },
    {
        "loss": 2.069,
        "grad_norm": 1.5330289602279663,
        "learning_rate": 0.00017178166764774108,
        "epoch": 0.4190791287565481,
        "step": 1520
    },
    {
        "loss": 2.1974,
        "grad_norm": 1.3238940238952637,
        "learning_rate": 0.00017159925019064032,
        "epoch": 0.41935483870967744,
        "step": 1521
    },
    {
        "loss": 2.0588,
        "grad_norm": 1.387010931968689,
        "learning_rate": 0.0001714163425451005,
        "epoch": 0.41963054866280675,
        "step": 1522
    },
    {
        "loss": 2.3317,
        "grad_norm": 1.5410979986190796,
        "learning_rate": 0.00017123294596335838,
        "epoch": 0.41990625861593606,
        "step": 1523
    },
    {
        "loss": 2.19,
        "grad_norm": 1.6439534425735474,
        "learning_rate": 0.00017104906170099807,
        "epoch": 0.42018196856906537,
        "step": 1524
    },
    {
        "loss": 2.2882,
        "grad_norm": 1.7224432229995728,
        "learning_rate": 0.00017086469101694257,
        "epoch": 0.4204576785221947,
        "step": 1525
    },
    {
        "loss": 1.9605,
        "grad_norm": 1.793520450592041,
        "learning_rate": 0.000170679835173445,
        "epoch": 0.42073338847532393,
        "step": 1526
    },
    {
        "loss": 2.1855,
        "grad_norm": 1.373926043510437,
        "learning_rate": 0.00017049449543608003,
        "epoch": 0.42100909842845324,
        "step": 1527
    },
    {
        "loss": 2.3247,
        "grad_norm": 1.6432477235794067,
        "learning_rate": 0.00017030867307373518,
        "epoch": 0.42128480838158255,
        "step": 1528
    },
    {
        "loss": 2.085,
        "grad_norm": 1.39853036403656,
        "learning_rate": 0.00017012236935860225,
        "epoch": 0.42156051833471186,
        "step": 1529
    },
    {
        "loss": 2.2683,
        "grad_norm": 1.9163564443588257,
        "learning_rate": 0.00016993558556616835,
        "epoch": 0.4218362282878412,
        "step": 1530
    },
    {
        "loss": 1.7365,
        "grad_norm": 1.811278223991394,
        "learning_rate": 0.00016974832297520748,
        "epoch": 0.4221119382409705,
        "step": 1531
    },
    {
        "loss": 2.3861,
        "grad_norm": 1.5947757959365845,
        "learning_rate": 0.00016956058286777154,
        "epoch": 0.4223876481940998,
        "step": 1532
    },
    {
        "loss": 2.1541,
        "grad_norm": 1.9669748544692993,
        "learning_rate": 0.00016937236652918172,
        "epoch": 0.4226633581472291,
        "step": 1533
    },
    {
        "loss": 2.3577,
        "grad_norm": 1.320772647857666,
        "learning_rate": 0.00016918367524801955,
        "epoch": 0.4229390681003584,
        "step": 1534
    },
    {
        "loss": 1.5989,
        "grad_norm": 1.8686869144439697,
        "learning_rate": 0.00016899451031611811,
        "epoch": 0.42321477805348773,
        "step": 1535
    },
    {
        "loss": 2.4068,
        "grad_norm": 1.8209686279296875,
        "learning_rate": 0.00016880487302855347,
        "epoch": 0.42349048800661704,
        "step": 1536
    },
    {
        "loss": 2.0357,
        "grad_norm": 1.5511795282363892,
        "learning_rate": 0.0001686147646836352,
        "epoch": 0.42376619795974635,
        "step": 1537
    },
    {
        "loss": 2.2259,
        "grad_norm": 1.3939266204833984,
        "learning_rate": 0.00016842418658289816,
        "epoch": 0.42404190791287566,
        "step": 1538
    },
    {
        "loss": 2.6934,
        "grad_norm": 1.2874032258987427,
        "learning_rate": 0.0001682331400310932,
        "epoch": 0.42431761786600497,
        "step": 1539
    },
    {
        "loss": 2.4911,
        "grad_norm": 1.525564432144165,
        "learning_rate": 0.00016804162633617825,
        "epoch": 0.4245933278191343,
        "step": 1540
    },
    {
        "loss": 2.2477,
        "grad_norm": 1.2524858713150024,
        "learning_rate": 0.0001678496468093095,
        "epoch": 0.4248690377722636,
        "step": 1541
    },
    {
        "loss": 2.3288,
        "grad_norm": 1.2567781209945679,
        "learning_rate": 0.0001676572027648324,
        "epoch": 0.4251447477253929,
        "step": 1542
    },
    {
        "loss": 1.7152,
        "grad_norm": 2.313746452331543,
        "learning_rate": 0.00016746429552027249,
        "epoch": 0.4254204576785222,
        "step": 1543
    },
    {
        "loss": 1.9054,
        "grad_norm": 1.5240070819854736,
        "learning_rate": 0.0001672709263963266,
        "epoch": 0.4256961676316515,
        "step": 1544
    },
    {
        "loss": 1.4245,
        "grad_norm": 1.8785980939865112,
        "learning_rate": 0.0001670770967168537,
        "epoch": 0.42597187758478083,
        "step": 1545
    },
    {
        "loss": 1.9458,
        "grad_norm": 1.6243945360183716,
        "learning_rate": 0.00016688280780886584,
        "epoch": 0.42624758753791014,
        "step": 1546
    },
    {
        "loss": 2.126,
        "grad_norm": 1.0722332000732422,
        "learning_rate": 0.0001666880610025191,
        "epoch": 0.4265232974910394,
        "step": 1547
    },
    {
        "loss": 2.2877,
        "grad_norm": 1.2461540699005127,
        "learning_rate": 0.00016649285763110442,
        "epoch": 0.4267990074441687,
        "step": 1548
    },
    {
        "loss": 2.2267,
        "grad_norm": 2.17858624458313,
        "learning_rate": 0.00016629719903103851,
        "epoch": 0.427074717397298,
        "step": 1549
    },
    {
        "loss": 1.9649,
        "grad_norm": 1.7039071321487427,
        "learning_rate": 0.00016610108654185475,
        "epoch": 0.42735042735042733,
        "step": 1550
    },
    {
        "loss": 2.2201,
        "grad_norm": 1.3345578908920288,
        "learning_rate": 0.00016590452150619394,
        "epoch": 0.42762613730355664,
        "step": 1551
    },
    {
        "loss": 1.6644,
        "grad_norm": 1.5596153736114502,
        "learning_rate": 0.00016570750526979515,
        "epoch": 0.42790184725668595,
        "step": 1552
    },
    {
        "loss": 2.1559,
        "grad_norm": 1.5034345388412476,
        "learning_rate": 0.00016551003918148648,
        "epoch": 0.42817755720981526,
        "step": 1553
    },
    {
        "loss": 2.0012,
        "grad_norm": 1.8080370426177979,
        "learning_rate": 0.00016531212459317587,
        "epoch": 0.4284532671629446,
        "step": 1554
    },
    {
        "loss": 2.1863,
        "grad_norm": 1.602611780166626,
        "learning_rate": 0.00016511376285984177,
        "epoch": 0.4287289771160739,
        "step": 1555
    },
    {
        "loss": 1.9191,
        "grad_norm": 1.4404298067092896,
        "learning_rate": 0.000164914955339524,
        "epoch": 0.4290046870692032,
        "step": 1556
    },
    {
        "loss": 2.0554,
        "grad_norm": 1.0122169256210327,
        "learning_rate": 0.0001647157033933142,
        "epoch": 0.4292803970223325,
        "step": 1557
    },
    {
        "loss": 2.553,
        "grad_norm": 1.1997040510177612,
        "learning_rate": 0.00016451600838534688,
        "epoch": 0.4295561069754618,
        "step": 1558
    },
    {
        "loss": 2.3329,
        "grad_norm": 1.6425753831863403,
        "learning_rate": 0.0001643158716827897,
        "epoch": 0.4298318169285911,
        "step": 1559
    },
    {
        "loss": 2.1712,
        "grad_norm": 1.142952799797058,
        "learning_rate": 0.00016411529465583437,
        "epoch": 0.43010752688172044,
        "step": 1560
    },
    {
        "loss": 2.1866,
        "grad_norm": 1.197511911392212,
        "learning_rate": 0.00016391427867768719,
        "epoch": 0.43038323683484975,
        "step": 1561
    },
    {
        "loss": 2.3092,
        "grad_norm": 0.9565675854682922,
        "learning_rate": 0.00016371282512455957,
        "epoch": 0.43065894678797906,
        "step": 1562
    },
    {
        "loss": 2.1198,
        "grad_norm": 1.4358775615692139,
        "learning_rate": 0.00016351093537565875,
        "epoch": 0.43093465674110837,
        "step": 1563
    },
    {
        "loss": 1.8335,
        "grad_norm": 1.4258471727371216,
        "learning_rate": 0.00016330861081317833,
        "epoch": 0.4312103666942377,
        "step": 1564
    },
    {
        "loss": 2.2182,
        "grad_norm": 1.5461722612380981,
        "learning_rate": 0.0001631058528222886,
        "epoch": 0.431486076647367,
        "step": 1565
    },
    {
        "loss": 2.3535,
        "grad_norm": 1.3491661548614502,
        "learning_rate": 0.0001629026627911274,
        "epoch": 0.4317617866004963,
        "step": 1566
    },
    {
        "loss": 1.9366,
        "grad_norm": 1.4037567377090454,
        "learning_rate": 0.00016269904211079031,
        "epoch": 0.4320374965536256,
        "step": 1567
    },
    {
        "loss": 2.2785,
        "grad_norm": 1.1561353206634521,
        "learning_rate": 0.00016249499217532135,
        "epoch": 0.4323132065067549,
        "step": 1568
    },
    {
        "loss": 1.2733,
        "grad_norm": 1.6264920234680176,
        "learning_rate": 0.00016229051438170327,
        "epoch": 0.4325889164598842,
        "step": 1569
    },
    {
        "loss": 1.4959,
        "grad_norm": 2.145965814590454,
        "learning_rate": 0.00016208561012984806,
        "epoch": 0.4328646264130135,
        "step": 1570
    },
    {
        "loss": 2.2789,
        "grad_norm": 1.9316776990890503,
        "learning_rate": 0.00016188028082258745,
        "epoch": 0.4331403363661428,
        "step": 1571
    },
    {
        "loss": 2.3693,
        "grad_norm": 1.8024663925170898,
        "learning_rate": 0.0001616745278656631,
        "epoch": 0.4334160463192721,
        "step": 1572
    },
    {
        "loss": 2.439,
        "grad_norm": 1.0391035079956055,
        "learning_rate": 0.0001614683526677172,
        "epoch": 0.4336917562724014,
        "step": 1573
    },
    {
        "loss": 1.8633,
        "grad_norm": 1.5993750095367432,
        "learning_rate": 0.00016126175664028263,
        "epoch": 0.43396746622553073,
        "step": 1574
    },
    {
        "loss": 2.3505,
        "grad_norm": 1.1654138565063477,
        "learning_rate": 0.00016105474119777337,
        "epoch": 0.43424317617866004,
        "step": 1575
    },
    {
        "loss": 2.2063,
        "grad_norm": 1.3026927709579468,
        "learning_rate": 0.00016084730775747498,
        "epoch": 0.43451888613178935,
        "step": 1576
    },
    {
        "loss": 2.0498,
        "grad_norm": 1.2261778116226196,
        "learning_rate": 0.0001606394577395346,
        "epoch": 0.43479459608491866,
        "step": 1577
    },
    {
        "loss": 1.8414,
        "grad_norm": 2.576941967010498,
        "learning_rate": 0.00016043119256695148,
        "epoch": 0.43507030603804797,
        "step": 1578
    },
    {
        "loss": 2.2798,
        "grad_norm": 1.303424596786499,
        "learning_rate": 0.00016022251366556702,
        "epoch": 0.4353460159911773,
        "step": 1579
    },
    {
        "loss": 2.0307,
        "grad_norm": 1.6706486940383911,
        "learning_rate": 0.00016001342246405525,
        "epoch": 0.4356217259443066,
        "step": 1580
    },
    {
        "loss": 2.4072,
        "grad_norm": 0.9005613923072815,
        "learning_rate": 0.0001598039203939128,
        "epoch": 0.4358974358974359,
        "step": 1581
    },
    {
        "loss": 2.1619,
        "grad_norm": 0.9253801107406616,
        "learning_rate": 0.00015959400888944933,
        "epoch": 0.4361731458505652,
        "step": 1582
    },
    {
        "loss": 1.9732,
        "grad_norm": 2.2881593704223633,
        "learning_rate": 0.00015938368938777755,
        "epoch": 0.4364488558036945,
        "step": 1583
    },
    {
        "loss": 2.2315,
        "grad_norm": 1.6588579416275024,
        "learning_rate": 0.0001591729633288034,
        "epoch": 0.43672456575682383,
        "step": 1584
    },
    {
        "loss": 1.7801,
        "grad_norm": 2.0236294269561768,
        "learning_rate": 0.0001589618321552163,
        "epoch": 0.43700027570995315,
        "step": 1585
    },
    {
        "loss": 2.3807,
        "grad_norm": 1.2982710599899292,
        "learning_rate": 0.00015875029731247914,
        "epoch": 0.43727598566308246,
        "step": 1586
    },
    {
        "loss": 1.8572,
        "grad_norm": 2.1180977821350098,
        "learning_rate": 0.00015853836024881847,
        "epoch": 0.43755169561621177,
        "step": 1587
    },
    {
        "loss": 2.2413,
        "grad_norm": 1.3113934993743896,
        "learning_rate": 0.00015832602241521452,
        "epoch": 0.4378274055693411,
        "step": 1588
    },
    {
        "loss": 2.2208,
        "grad_norm": 1.3153032064437866,
        "learning_rate": 0.00015811328526539134,
        "epoch": 0.4381031155224704,
        "step": 1589
    },
    {
        "loss": 2.2128,
        "grad_norm": 1.6205463409423828,
        "learning_rate": 0.00015790015025580684,
        "epoch": 0.43837882547559964,
        "step": 1590
    },
    {
        "loss": 2.1664,
        "grad_norm": 1.7018409967422485,
        "learning_rate": 0.00015768661884564273,
        "epoch": 0.43865453542872895,
        "step": 1591
    },
    {
        "loss": 2.3299,
        "grad_norm": 1.5557031631469727,
        "learning_rate": 0.00015747269249679458,
        "epoch": 0.43893024538185826,
        "step": 1592
    },
    {
        "loss": 2.3983,
        "grad_norm": 1.511317491531372,
        "learning_rate": 0.00015725837267386184,
        "epoch": 0.4392059553349876,
        "step": 1593
    },
    {
        "loss": 2.2518,
        "grad_norm": 1.1340250968933105,
        "learning_rate": 0.00015704366084413784,
        "epoch": 0.4394816652881169,
        "step": 1594
    },
    {
        "loss": 1.9947,
        "grad_norm": 1.821001648902893,
        "learning_rate": 0.00015682855847759964,
        "epoch": 0.4397573752412462,
        "step": 1595
    },
    {
        "loss": 1.6738,
        "grad_norm": 1.6044408082962036,
        "learning_rate": 0.00015661306704689806,
        "epoch": 0.4400330851943755,
        "step": 1596
    },
    {
        "loss": 2.334,
        "grad_norm": 0.9590471982955933,
        "learning_rate": 0.0001563971880273475,
        "epoch": 0.4403087951475048,
        "step": 1597
    },
    {
        "loss": 2.5616,
        "grad_norm": 1.0785707235336304,
        "learning_rate": 0.000156180922896916,
        "epoch": 0.4405845051006341,
        "step": 1598
    },
    {
        "loss": 2.2976,
        "grad_norm": 1.2255219221115112,
        "learning_rate": 0.00015596427313621495,
        "epoch": 0.44086021505376344,
        "step": 1599
    },
    {
        "loss": 2.4077,
        "grad_norm": 1.2608559131622314,
        "learning_rate": 0.00015574724022848906,
        "epoch": 0.44113592500689275,
        "step": 1600
    },
    {
        "loss": 2.3861,
        "grad_norm": 1.3120696544647217,
        "learning_rate": 0.0001555298256596061,
        "epoch": 0.44141163496002206,
        "step": 1601
    },
    {
        "loss": 1.9211,
        "grad_norm": 1.5817667245864868,
        "learning_rate": 0.0001553120309180469,
        "epoch": 0.44168734491315137,
        "step": 1602
    },
    {
        "loss": 2.3403,
        "grad_norm": 1.4952890872955322,
        "learning_rate": 0.00015509385749489502,
        "epoch": 0.4419630548662807,
        "step": 1603
    },
    {
        "loss": 1.4433,
        "grad_norm": 2.5388975143432617,
        "learning_rate": 0.00015487530688382655,
        "epoch": 0.44223876481941,
        "step": 1604
    },
    {
        "loss": 1.8339,
        "grad_norm": 2.056171417236328,
        "learning_rate": 0.00015465638058109998,
        "epoch": 0.4425144747725393,
        "step": 1605
    },
    {
        "loss": 2.3563,
        "grad_norm": 1.360848307609558,
        "learning_rate": 0.00015443708008554578,
        "epoch": 0.4427901847256686,
        "step": 1606
    },
    {
        "loss": 2.4509,
        "grad_norm": 1.2633886337280273,
        "learning_rate": 0.00015421740689855638,
        "epoch": 0.4430658946787979,
        "step": 1607
    },
    {
        "loss": 2.2717,
        "grad_norm": 1.3315649032592773,
        "learning_rate": 0.00015399736252407563,
        "epoch": 0.44334160463192723,
        "step": 1608
    },
    {
        "loss": 2.244,
        "grad_norm": 1.0682576894760132,
        "learning_rate": 0.00015377694846858874,
        "epoch": 0.44361731458505654,
        "step": 1609
    },
    {
        "loss": 2.5893,
        "grad_norm": 1.4008442163467407,
        "learning_rate": 0.0001535561662411118,
        "epoch": 0.44389302453818585,
        "step": 1610
    },
    {
        "loss": 2.3141,
        "grad_norm": 1.7143430709838867,
        "learning_rate": 0.00015333501735318158,
        "epoch": 0.4441687344913151,
        "step": 1611
    },
    {
        "loss": 2.2918,
        "grad_norm": 1.2968823909759521,
        "learning_rate": 0.00015311350331884499,
        "epoch": 0.4444444444444444,
        "step": 1612
    },
    {
        "loss": 2.1082,
        "grad_norm": 1.0908840894699097,
        "learning_rate": 0.0001528916256546489,
        "epoch": 0.44472015439757373,
        "step": 1613
    },
    {
        "loss": 1.9818,
        "grad_norm": 1.5428403615951538,
        "learning_rate": 0.0001526693858796297,
        "epoch": 0.44499586435070304,
        "step": 1614
    },
    {
        "loss": 2.6639,
        "grad_norm": 1.052176594734192,
        "learning_rate": 0.00015244678551530293,
        "epoch": 0.44527157430383235,
        "step": 1615
    },
    {
        "loss": 2.0847,
        "grad_norm": 1.2757716178894043,
        "learning_rate": 0.00015222382608565273,
        "epoch": 0.44554728425696166,
        "step": 1616
    },
    {
        "loss": 2.0611,
        "grad_norm": 1.3713157176971436,
        "learning_rate": 0.00015200050911712157,
        "epoch": 0.445822994210091,
        "step": 1617
    },
    {
        "loss": 2.3652,
        "grad_norm": 1.8696458339691162,
        "learning_rate": 0.0001517768361385997,
        "epoch": 0.4460987041632203,
        "step": 1618
    },
    {
        "loss": 2.4674,
        "grad_norm": 0.8935949802398682,
        "learning_rate": 0.0001515528086814147,
        "epoch": 0.4463744141163496,
        "step": 1619
    },
    {
        "loss": 2.5313,
        "grad_norm": 1.3385225534439087,
        "learning_rate": 0.00015132842827932108,
        "epoch": 0.4466501240694789,
        "step": 1620
    },
    {
        "loss": 1.456,
        "grad_norm": 1.4567385911941528,
        "learning_rate": 0.00015110369646848963,
        "epoch": 0.4469258340226082,
        "step": 1621
    },
    {
        "loss": 2.0886,
        "grad_norm": 1.3539878129959106,
        "learning_rate": 0.000150878614787497,
        "epoch": 0.4472015439757375,
        "step": 1622
    },
    {
        "loss": 1.7582,
        "grad_norm": 1.7387800216674805,
        "learning_rate": 0.00015065318477731522,
        "epoch": 0.44747725392886684,
        "step": 1623
    },
    {
        "loss": 2.1804,
        "grad_norm": 1.0198724269866943,
        "learning_rate": 0.00015042740798130097,
        "epoch": 0.44775296388199615,
        "step": 1624
    },
    {
        "loss": 1.7458,
        "grad_norm": 1.9861915111541748,
        "learning_rate": 0.0001502012859451852,
        "epoch": 0.44802867383512546,
        "step": 1625
    },
    {
        "loss": 1.2013,
        "grad_norm": 1.5638484954833984,
        "learning_rate": 0.00014997482021706247,
        "epoch": 0.44830438378825477,
        "step": 1626
    },
    {
        "loss": 2.4293,
        "grad_norm": 1.2109370231628418,
        "learning_rate": 0.00014974801234738026,
        "epoch": 0.4485800937413841,
        "step": 1627
    },
    {
        "loss": 2.3978,
        "grad_norm": 1.3699736595153809,
        "learning_rate": 0.00014952086388892854,
        "epoch": 0.4488558036945134,
        "step": 1628
    },
    {
        "loss": 1.7859,
        "grad_norm": 1.8834060430526733,
        "learning_rate": 0.00014929337639682906,
        "epoch": 0.4491315136476427,
        "step": 1629
    },
    {
        "loss": 2.0633,
        "grad_norm": 1.7218295335769653,
        "learning_rate": 0.0001490655514285246,
        "epoch": 0.449407223600772,
        "step": 1630
    },
    {
        "loss": 1.6856,
        "grad_norm": 1.7068156003952026,
        "learning_rate": 0.00014883739054376847,
        "epoch": 0.4496829335539013,
        "step": 1631
    },
    {
        "loss": 1.7787,
        "grad_norm": 1.578079104423523,
        "learning_rate": 0.0001486088953046137,
        "epoch": 0.44995864350703063,
        "step": 1632
    },
    {
        "loss": 2.1221,
        "grad_norm": 1.6238341331481934,
        "learning_rate": 0.00014838006727540247,
        "epoch": 0.4502343534601599,
        "step": 1633
    },
    {
        "loss": 2.5777,
        "grad_norm": 1.6577565670013428,
        "learning_rate": 0.00014815090802275524,
        "epoch": 0.4505100634132892,
        "step": 1634
    },
    {
        "loss": 2.1012,
        "grad_norm": 1.5675948858261108,
        "learning_rate": 0.0001479214191155603,
        "epoch": 0.4507857733664185,
        "step": 1635
    },
    {
        "loss": 2.2053,
        "grad_norm": 1.5074915885925293,
        "learning_rate": 0.00014769160212496255,
        "epoch": 0.4510614833195478,
        "step": 1636
    },
    {
        "loss": 2.3532,
        "grad_norm": 1.4894465208053589,
        "learning_rate": 0.00014746145862435338,
        "epoch": 0.45133719327267713,
        "step": 1637
    },
    {
        "loss": 2.2104,
        "grad_norm": 1.5511442422866821,
        "learning_rate": 0.00014723099018935928,
        "epoch": 0.45161290322580644,
        "step": 1638
    },
    {
        "loss": 2.0306,
        "grad_norm": 1.1402215957641602,
        "learning_rate": 0.0001470001983978315,
        "epoch": 0.45188861317893575,
        "step": 1639
    },
    {
        "loss": 1.6647,
        "grad_norm": 1.319387435913086,
        "learning_rate": 0.00014676908482983501,
        "epoch": 0.45216432313206506,
        "step": 1640
    },
    {
        "loss": 1.7091,
        "grad_norm": 1.531435489654541,
        "learning_rate": 0.00014653765106763776,
        "epoch": 0.45244003308519437,
        "step": 1641
    },
    {
        "loss": 2.1216,
        "grad_norm": 2.2159128189086914,
        "learning_rate": 0.00014630589869569984,
        "epoch": 0.4527157430383237,
        "step": 1642
    },
    {
        "loss": 1.6787,
        "grad_norm": 1.584214448928833,
        "learning_rate": 0.00014607382930066266,
        "epoch": 0.452991452991453,
        "step": 1643
    },
    {
        "loss": 2.0812,
        "grad_norm": 1.6305006742477417,
        "learning_rate": 0.00014584144447133795,
        "epoch": 0.4532671629445823,
        "step": 1644
    },
    {
        "loss": 2.6692,
        "grad_norm": 1.7245962619781494,
        "learning_rate": 0.00014560874579869722,
        "epoch": 0.4535428728977116,
        "step": 1645
    },
    {
        "loss": 2.468,
        "grad_norm": 1.21096670627594,
        "learning_rate": 0.00014537573487586033,
        "epoch": 0.4538185828508409,
        "step": 1646
    },
    {
        "loss": 2.2209,
        "grad_norm": 1.5296903848648071,
        "learning_rate": 0.0001451424132980852,
        "epoch": 0.45409429280397023,
        "step": 1647
    },
    {
        "loss": 1.5517,
        "grad_norm": 2.0868334770202637,
        "learning_rate": 0.00014490878266275634,
        "epoch": 0.45437000275709954,
        "step": 1648
    },
    {
        "loss": 1.6579,
        "grad_norm": 1.4246091842651367,
        "learning_rate": 0.00014467484456937428,
        "epoch": 0.45464571271022886,
        "step": 1649
    },
    {
        "loss": 2.2808,
        "grad_norm": 1.4835580587387085,
        "learning_rate": 0.00014444060061954446,
        "epoch": 0.45492142266335817,
        "step": 1650
    },
    {
        "loss": 2.4182,
        "grad_norm": 1.6701040267944336,
        "learning_rate": 0.0001442060524169663,
        "epoch": 0.4551971326164875,
        "step": 1651
    },
    {
        "loss": 2.3386,
        "grad_norm": 1.5481410026550293,
        "learning_rate": 0.00014397120156742227,
        "epoch": 0.4554728425696168,
        "step": 1652
    },
    {
        "loss": 2.1651,
        "grad_norm": 1.4891079664230347,
        "learning_rate": 0.0001437360496787667,
        "epoch": 0.4557485525227461,
        "step": 1653
    },
    {
        "loss": 2.0097,
        "grad_norm": 1.8355125188827515,
        "learning_rate": 0.00014350059836091506,
        "epoch": 0.45602426247587535,
        "step": 1654
    },
    {
        "loss": 2.3071,
        "grad_norm": 1.3944275379180908,
        "learning_rate": 0.00014326484922583282,
        "epoch": 0.45629997242900466,
        "step": 1655
    },
    {
        "loss": 1.5476,
        "grad_norm": 2.2283830642700195,
        "learning_rate": 0.0001430288038875242,
        "epoch": 0.456575682382134,
        "step": 1656
    },
    {
        "loss": 1.9275,
        "grad_norm": 1.7802057266235352,
        "learning_rate": 0.0001427924639620215,
        "epoch": 0.4568513923352633,
        "step": 1657
    },
    {
        "loss": 2.4242,
        "grad_norm": 1.2369308471679688,
        "learning_rate": 0.00014255583106737377,
        "epoch": 0.4571271022883926,
        "step": 1658
    },
    {
        "loss": 1.9103,
        "grad_norm": 1.131266713142395,
        "learning_rate": 0.00014231890682363577,
        "epoch": 0.4574028122415219,
        "step": 1659
    },
    {
        "loss": 1.6352,
        "grad_norm": 1.7644742727279663,
        "learning_rate": 0.00014208169285285707,
        "epoch": 0.4576785221946512,
        "step": 1660
    },
    {
        "loss": 2.255,
        "grad_norm": 1.7970942258834839,
        "learning_rate": 0.00014184419077907054,
        "epoch": 0.4579542321477805,
        "step": 1661
    },
    {
        "loss": 2.214,
        "grad_norm": 1.527597188949585,
        "learning_rate": 0.00014160640222828175,
        "epoch": 0.45822994210090984,
        "step": 1662
    },
    {
        "loss": 2.4573,
        "grad_norm": 1.7660609483718872,
        "learning_rate": 0.00014136832882845736,
        "epoch": 0.45850565205403915,
        "step": 1663
    },
    {
        "loss": 2.5148,
        "grad_norm": 1.206503987312317,
        "learning_rate": 0.00014112997220951435,
        "epoch": 0.45878136200716846,
        "step": 1664
    },
    {
        "loss": 1.3998,
        "grad_norm": 2.3746113777160645,
        "learning_rate": 0.00014089133400330855,
        "epoch": 0.45905707196029777,
        "step": 1665
    },
    {
        "loss": 1.9809,
        "grad_norm": 1.596276879310608,
        "learning_rate": 0.00014065241584362373,
        "epoch": 0.4593327819134271,
        "step": 1666
    },
    {
        "loss": 2.5607,
        "grad_norm": 1.2555510997772217,
        "learning_rate": 0.00014041321936616026,
        "epoch": 0.4596084918665564,
        "step": 1667
    },
    {
        "loss": 1.8049,
        "grad_norm": 1.9762015342712402,
        "learning_rate": 0.0001401737462085239,
        "epoch": 0.4598842018196857,
        "step": 1668
    },
    {
        "loss": 1.8916,
        "grad_norm": 2.212815523147583,
        "learning_rate": 0.00013993399801021477,
        "epoch": 0.460159911772815,
        "step": 1669
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.5148228406906128,
        "learning_rate": 0.0001396939764126158,
        "epoch": 0.4604356217259443,
        "step": 1670
    },
    {
        "loss": 1.2322,
        "grad_norm": 3.0732598304748535,
        "learning_rate": 0.00013945368305898193,
        "epoch": 0.46071133167907363,
        "step": 1671
    },
    {
        "loss": 1.5435,
        "grad_norm": 2.4497220516204834,
        "learning_rate": 0.0001392131195944284,
        "epoch": 0.46098704163220294,
        "step": 1672
    },
    {
        "loss": 2.002,
        "grad_norm": 1.028225302696228,
        "learning_rate": 0.0001389722876659198,
        "epoch": 0.46126275158533225,
        "step": 1673
    },
    {
        "loss": 2.231,
        "grad_norm": 1.4691612720489502,
        "learning_rate": 0.0001387311889222587,
        "epoch": 0.46153846153846156,
        "step": 1674
    },
    {
        "loss": 2.3702,
        "grad_norm": 1.4030394554138184,
        "learning_rate": 0.00013848982501407436,
        "epoch": 0.4618141714915908,
        "step": 1675
    },
    {
        "loss": 2.0156,
        "grad_norm": 1.683464765548706,
        "learning_rate": 0.00013824819759381143,
        "epoch": 0.46208988144472013,
        "step": 1676
    },
    {
        "loss": 2.3221,
        "grad_norm": 1.5578656196594238,
        "learning_rate": 0.00013800630831571864,
        "epoch": 0.46236559139784944,
        "step": 1677
    },
    {
        "loss": 2.0196,
        "grad_norm": 1.1428229808807373,
        "learning_rate": 0.0001377641588358374,
        "epoch": 0.46264130135097875,
        "step": 1678
    },
    {
        "loss": 1.7359,
        "grad_norm": 1.9303184747695923,
        "learning_rate": 0.0001375217508119907,
        "epoch": 0.46291701130410806,
        "step": 1679
    },
    {
        "loss": 2.1657,
        "grad_norm": 1.3853542804718018,
        "learning_rate": 0.00013727908590377134,
        "epoch": 0.46319272125723737,
        "step": 1680
    },
    {
        "loss": 1.9564,
        "grad_norm": 1.1058082580566406,
        "learning_rate": 0.0001370361657725311,
        "epoch": 0.4634684312103667,
        "step": 1681
    },
    {
        "loss": 1.5652,
        "grad_norm": 1.9432283639907837,
        "learning_rate": 0.00013679299208136893,
        "epoch": 0.463744141163496,
        "step": 1682
    },
    {
        "loss": 2.0175,
        "grad_norm": 1.7017154693603516,
        "learning_rate": 0.00013654956649511974,
        "epoch": 0.4640198511166253,
        "step": 1683
    },
    {
        "loss": 2.9167,
        "grad_norm": 1.1166876554489136,
        "learning_rate": 0.000136305890680343,
        "epoch": 0.4642955610697546,
        "step": 1684
    },
    {
        "loss": 2.4054,
        "grad_norm": 1.1698839664459229,
        "learning_rate": 0.0001360619663053113,
        "epoch": 0.4645712710228839,
        "step": 1685
    },
    {
        "loss": 1.5313,
        "grad_norm": 1.77582585811615,
        "learning_rate": 0.00013581779503999894,
        "epoch": 0.46484698097601324,
        "step": 1686
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.697800874710083,
        "learning_rate": 0.00013557337855607055,
        "epoch": 0.46512269092914255,
        "step": 1687
    },
    {
        "loss": 2.6197,
        "grad_norm": 1.8881542682647705,
        "learning_rate": 0.00013532871852686956,
        "epoch": 0.46539840088227186,
        "step": 1688
    },
    {
        "loss": 2.5863,
        "grad_norm": 1.0851056575775146,
        "learning_rate": 0.00013508381662740674,
        "epoch": 0.46567411083540117,
        "step": 1689
    },
    {
        "loss": 2.3839,
        "grad_norm": 1.0782390832901,
        "learning_rate": 0.00013483867453434879,
        "epoch": 0.4659498207885305,
        "step": 1690
    },
    {
        "loss": 2.2956,
        "grad_norm": 1.1634790897369385,
        "learning_rate": 0.00013459329392600692,
        "epoch": 0.4662255307416598,
        "step": 1691
    },
    {
        "loss": 1.8763,
        "grad_norm": 1.1747897863388062,
        "learning_rate": 0.00013434767648232523,
        "epoch": 0.4665012406947891,
        "step": 1692
    },
    {
        "loss": 2.0251,
        "grad_norm": 1.922082543373108,
        "learning_rate": 0.0001341018238848692,
        "epoch": 0.4667769506479184,
        "step": 1693
    },
    {
        "loss": 1.0751,
        "grad_norm": 2.0511677265167236,
        "learning_rate": 0.00013385573781681434,
        "epoch": 0.4670526606010477,
        "step": 1694
    },
    {
        "loss": 1.9924,
        "grad_norm": 1.6893725395202637,
        "learning_rate": 0.00013360941996293444,
        "epoch": 0.46732837055417703,
        "step": 1695
    },
    {
        "loss": 1.7038,
        "grad_norm": 2.1020348072052,
        "learning_rate": 0.00013336287200959028,
        "epoch": 0.46760408050730634,
        "step": 1696
    },
    {
        "loss": 2.1629,
        "grad_norm": 2.3377766609191895,
        "learning_rate": 0.000133116095644718,
        "epoch": 0.4678797904604356,
        "step": 1697
    },
    {
        "loss": 2.5342,
        "grad_norm": 1.8099007606506348,
        "learning_rate": 0.00013286909255781726,
        "epoch": 0.4681555004135649,
        "step": 1698
    },
    {
        "loss": 2.2071,
        "grad_norm": 1.2071571350097656,
        "learning_rate": 0.00013262186443994026,
        "epoch": 0.4684312103666942,
        "step": 1699
    },
    {
        "loss": 1.7441,
        "grad_norm": 1.7657904624938965,
        "learning_rate": 0.00013237441298367958,
        "epoch": 0.46870692031982353,
        "step": 1700
    },
    {
        "loss": 2.2232,
        "grad_norm": 1.5101655721664429,
        "learning_rate": 0.00013212673988315693,
        "epoch": 0.46898263027295284,
        "step": 1701
    },
    {
        "loss": 1.9936,
        "grad_norm": 1.5321038961410522,
        "learning_rate": 0.00013187884683401143,
        "epoch": 0.46925834022608215,
        "step": 1702
    },
    {
        "loss": 2.2878,
        "grad_norm": 1.2759922742843628,
        "learning_rate": 0.00013163073553338803,
        "epoch": 0.46953405017921146,
        "step": 1703
    },
    {
        "loss": 1.9307,
        "grad_norm": 2.01900315284729,
        "learning_rate": 0.000131382407679926,
        "epoch": 0.46980976013234077,
        "step": 1704
    },
    {
        "loss": 2.3624,
        "grad_norm": 1.4961600303649902,
        "learning_rate": 0.00013113386497374694,
        "epoch": 0.4700854700854701,
        "step": 1705
    },
    {
        "loss": 2.5228,
        "grad_norm": 1.1065101623535156,
        "learning_rate": 0.00013088510911644365,
        "epoch": 0.4703611800385994,
        "step": 1706
    },
    {
        "loss": 1.3694,
        "grad_norm": 1.8477154970169067,
        "learning_rate": 0.0001306361418110681,
        "epoch": 0.4706368899917287,
        "step": 1707
    },
    {
        "loss": 1.8827,
        "grad_norm": 1.373287320137024,
        "learning_rate": 0.00013038696476211988,
        "epoch": 0.470912599944858,
        "step": 1708
    },
    {
        "loss": 2.6015,
        "grad_norm": 1.5119825601577759,
        "learning_rate": 0.00013013757967553463,
        "epoch": 0.4711883098979873,
        "step": 1709
    },
    {
        "loss": 1.8508,
        "grad_norm": 1.104620337486267,
        "learning_rate": 0.0001298879882586722,
        "epoch": 0.47146401985111663,
        "step": 1710
    },
    {
        "loss": 2.3233,
        "grad_norm": 1.0381094217300415,
        "learning_rate": 0.00012963819222030503,
        "epoch": 0.47173972980424594,
        "step": 1711
    },
    {
        "loss": 2.3062,
        "grad_norm": 2.2634217739105225,
        "learning_rate": 0.00012938819327060644,
        "epoch": 0.47201543975737525,
        "step": 1712
    },
    {
        "loss": 2.1809,
        "grad_norm": 1.026519775390625,
        "learning_rate": 0.00012913799312113904,
        "epoch": 0.47229114971050457,
        "step": 1713
    },
    {
        "loss": 2.078,
        "grad_norm": 1.8434417247772217,
        "learning_rate": 0.00012888759348484281,
        "epoch": 0.4725668596636339,
        "step": 1714
    },
    {
        "loss": 2.2368,
        "grad_norm": 1.1776494979858398,
        "learning_rate": 0.00012863699607602356,
        "epoch": 0.4728425696167632,
        "step": 1715
    },
    {
        "loss": 2.2699,
        "grad_norm": 1.2855032682418823,
        "learning_rate": 0.000128386202610341,
        "epoch": 0.4731182795698925,
        "step": 1716
    },
    {
        "loss": 2.3898,
        "grad_norm": 1.5354942083358765,
        "learning_rate": 0.00012813521480479722,
        "epoch": 0.4733939895230218,
        "step": 1717
    },
    {
        "loss": 2.0721,
        "grad_norm": 1.416467308998108,
        "learning_rate": 0.00012788403437772466,
        "epoch": 0.47366969947615106,
        "step": 1718
    },
    {
        "loss": 1.8874,
        "grad_norm": 1.8874672651290894,
        "learning_rate": 0.00012763266304877466,
        "epoch": 0.4739454094292804,
        "step": 1719
    },
    {
        "loss": 2.2898,
        "grad_norm": 1.3207048177719116,
        "learning_rate": 0.00012738110253890543,
        "epoch": 0.4742211193824097,
        "step": 1720
    },
    {
        "loss": 1.7543,
        "grad_norm": 1.6880567073822021,
        "learning_rate": 0.00012712935457037037,
        "epoch": 0.474496829335539,
        "step": 1721
    },
    {
        "loss": 2.0834,
        "grad_norm": 1.8529930114746094,
        "learning_rate": 0.00012687742086670632,
        "epoch": 0.4747725392886683,
        "step": 1722
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.6707018613815308,
        "learning_rate": 0.00012662530315272168,
        "epoch": 0.4750482492417976,
        "step": 1723
    },
    {
        "loss": 2.3676,
        "grad_norm": 1.451688289642334,
        "learning_rate": 0.0001263730031544847,
        "epoch": 0.4753239591949269,
        "step": 1724
    },
    {
        "loss": 1.9856,
        "grad_norm": 2.291928291320801,
        "learning_rate": 0.00012612052259931145,
        "epoch": 0.47559966914805624,
        "step": 1725
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.6866849660873413,
        "learning_rate": 0.0001258678632157543,
        "epoch": 0.47587537910118555,
        "step": 1726
    },
    {
        "loss": 2.7527,
        "grad_norm": 1.2894948720932007,
        "learning_rate": 0.00012561502673358988,
        "epoch": 0.47615108905431486,
        "step": 1727
    },
    {
        "loss": 2.5937,
        "grad_norm": 1.3430840969085693,
        "learning_rate": 0.00012536201488380722,
        "epoch": 0.47642679900744417,
        "step": 1728
    },
    {
        "loss": 2.473,
        "grad_norm": 1.2355313301086426,
        "learning_rate": 0.00012510882939859608,
        "epoch": 0.4767025089605735,
        "step": 1729
    },
    {
        "loss": 2.0306,
        "grad_norm": 1.6469181776046753,
        "learning_rate": 0.00012485547201133488,
        "epoch": 0.4769782189137028,
        "step": 1730
    },
    {
        "loss": 2.3754,
        "grad_norm": 1.7496349811553955,
        "learning_rate": 0.00012460194445657898,
        "epoch": 0.4772539288668321,
        "step": 1731
    },
    {
        "loss": 2.1458,
        "grad_norm": 1.813454270362854,
        "learning_rate": 0.00012434824847004872,
        "epoch": 0.4775296388199614,
        "step": 1732
    },
    {
        "loss": 2.6267,
        "grad_norm": 1.295497179031372,
        "learning_rate": 0.0001240943857886176,
        "epoch": 0.4778053487730907,
        "step": 1733
    },
    {
        "loss": 2.8498,
        "grad_norm": 1.5756880044937134,
        "learning_rate": 0.00012384035815030039,
        "epoch": 0.47808105872622003,
        "step": 1734
    },
    {
        "loss": 2.1332,
        "grad_norm": 1.406662940979004,
        "learning_rate": 0.00012358616729424112,
        "epoch": 0.47835676867934934,
        "step": 1735
    },
    {
        "loss": 2.0369,
        "grad_norm": 1.204537034034729,
        "learning_rate": 0.0001233318149607013,
        "epoch": 0.47863247863247865,
        "step": 1736
    },
    {
        "loss": 1.9356,
        "grad_norm": 1.6030292510986328,
        "learning_rate": 0.00012307730289104794,
        "epoch": 0.47890818858560796,
        "step": 1737
    },
    {
        "loss": 2.4176,
        "grad_norm": 1.3414682149887085,
        "learning_rate": 0.0001228226328277417,
        "epoch": 0.4791838985387373,
        "step": 1738
    },
    {
        "loss": 1.9048,
        "grad_norm": 1.5020157098770142,
        "learning_rate": 0.00012256780651432486,
        "epoch": 0.47945960849186653,
        "step": 1739
    },
    {
        "loss": 1.2048,
        "grad_norm": 1.4375371932983398,
        "learning_rate": 0.00012231282569540942,
        "epoch": 0.47973531844499584,
        "step": 1740
    },
    {
        "loss": 2.1014,
        "grad_norm": 1.6356494426727295,
        "learning_rate": 0.00012205769211666525,
        "epoch": 0.48001102839812515,
        "step": 1741
    },
    {
        "loss": 1.8177,
        "grad_norm": 2.0622332096099854,
        "learning_rate": 0.00012180240752480791,
        "epoch": 0.48028673835125446,
        "step": 1742
    },
    {
        "loss": 1.8865,
        "grad_norm": 1.9602956771850586,
        "learning_rate": 0.000121546973667587,
        "epoch": 0.48056244830438377,
        "step": 1743
    },
    {
        "loss": 2.3465,
        "grad_norm": 1.1288299560546875,
        "learning_rate": 0.00012129139229377398,
        "epoch": 0.4808381582575131,
        "step": 1744
    },
    {
        "loss": 2.1854,
        "grad_norm": 1.5475881099700928,
        "learning_rate": 0.00012103566515315012,
        "epoch": 0.4811138682106424,
        "step": 1745
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.0982125997543335,
        "learning_rate": 0.00012077979399649486,
        "epoch": 0.4813895781637717,
        "step": 1746
    },
    {
        "loss": 2.2549,
        "grad_norm": 1.0582637786865234,
        "learning_rate": 0.00012052378057557347,
        "epoch": 0.481665288116901,
        "step": 1747
    },
    {
        "loss": 2.0257,
        "grad_norm": 0.814954936504364,
        "learning_rate": 0.00012026762664312523,
        "epoch": 0.4819409980700303,
        "step": 1748
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.7814151048660278,
        "learning_rate": 0.00012001133395285147,
        "epoch": 0.48221670802315963,
        "step": 1749
    },
    {
        "loss": 2.3778,
        "grad_norm": 1.887947916984558,
        "learning_rate": 0.00011975490425940336,
        "epoch": 0.48249241797628895,
        "step": 1750
    },
    {
        "loss": 2.0366,
        "grad_norm": 1.207564115524292,
        "learning_rate": 0.0001194983393183702,
        "epoch": 0.48276812792941826,
        "step": 1751
    },
    {
        "loss": 1.5617,
        "grad_norm": 1.9114494323730469,
        "learning_rate": 0.00011924164088626706,
        "epoch": 0.48304383788254757,
        "step": 1752
    },
    {
        "loss": 2.104,
        "grad_norm": 3.0962703227996826,
        "learning_rate": 0.00011898481072052306,
        "epoch": 0.4833195478356769,
        "step": 1753
    },
    {
        "loss": 2.3258,
        "grad_norm": 1.3912575244903564,
        "learning_rate": 0.00011872785057946914,
        "epoch": 0.4835952577888062,
        "step": 1754
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.660029411315918,
        "learning_rate": 0.00011847076222232609,
        "epoch": 0.4838709677419355,
        "step": 1755
    },
    {
        "loss": 2.0103,
        "grad_norm": 1.9138151407241821,
        "learning_rate": 0.00011821354740919259,
        "epoch": 0.4841466776950648,
        "step": 1756
    },
    {
        "loss": 2.0322,
        "grad_norm": 1.1443175077438354,
        "learning_rate": 0.0001179562079010329,
        "epoch": 0.4844223876481941,
        "step": 1757
    },
    {
        "loss": 2.2161,
        "grad_norm": 1.1107566356658936,
        "learning_rate": 0.00011769874545966513,
        "epoch": 0.48469809760132343,
        "step": 1758
    },
    {
        "loss": 2.5375,
        "grad_norm": 1.536111831665039,
        "learning_rate": 0.00011744116184774896,
        "epoch": 0.48497380755445274,
        "step": 1759
    },
    {
        "loss": 1.6925,
        "grad_norm": 1.7875713109970093,
        "learning_rate": 0.00011718345882877366,
        "epoch": 0.48524951750758205,
        "step": 1760
    },
    {
        "loss": 2.4542,
        "grad_norm": 0.8913880586624146,
        "learning_rate": 0.00011692563816704598,
        "epoch": 0.4855252274607113,
        "step": 1761
    },
    {
        "loss": 2.0948,
        "grad_norm": 1.730151891708374,
        "learning_rate": 0.00011666770162767806,
        "epoch": 0.4858009374138406,
        "step": 1762
    },
    {
        "loss": 2.1394,
        "grad_norm": 1.4342612028121948,
        "learning_rate": 0.00011640965097657544,
        "epoch": 0.4860766473669699,
        "step": 1763
    },
    {
        "loss": 2.0528,
        "grad_norm": 1.4623627662658691,
        "learning_rate": 0.00011615148798042483,
        "epoch": 0.48635235732009924,
        "step": 1764
    },
    {
        "loss": 2.155,
        "grad_norm": 1.32538902759552,
        "learning_rate": 0.00011589321440668213,
        "epoch": 0.48662806727322855,
        "step": 1765
    },
    {
        "loss": 2.2638,
        "grad_norm": 1.1431605815887451,
        "learning_rate": 0.00011563483202356026,
        "epoch": 0.48690377722635786,
        "step": 1766
    },
    {
        "loss": 1.4573,
        "grad_norm": 1.9295679330825806,
        "learning_rate": 0.00011537634260001711,
        "epoch": 0.48717948717948717,
        "step": 1767
    },
    {
        "loss": 2.2552,
        "grad_norm": 1.3558508157730103,
        "learning_rate": 0.00011511774790574335,
        "epoch": 0.4874551971326165,
        "step": 1768
    },
    {
        "loss": 1.8167,
        "grad_norm": 1.3866561651229858,
        "learning_rate": 0.0001148590497111504,
        "epoch": 0.4877309070857458,
        "step": 1769
    },
    {
        "loss": 2.1122,
        "grad_norm": 1.5821137428283691,
        "learning_rate": 0.00011460024978735826,
        "epoch": 0.4880066170388751,
        "step": 1770
    },
    {
        "loss": 2.2666,
        "grad_norm": 1.0915207862854004,
        "learning_rate": 0.00011434134990618344,
        "epoch": 0.4882823269920044,
        "step": 1771
    },
    {
        "loss": 2.3988,
        "grad_norm": 1.219952940940857,
        "learning_rate": 0.00011408235184012668,
        "epoch": 0.4885580369451337,
        "step": 1772
    },
    {
        "loss": 2.3343,
        "grad_norm": 1.5961039066314697,
        "learning_rate": 0.00011382325736236103,
        "epoch": 0.48883374689826303,
        "step": 1773
    },
    {
        "loss": 1.4357,
        "grad_norm": 1.5710440874099731,
        "learning_rate": 0.00011356406824671953,
        "epoch": 0.48910945685139234,
        "step": 1774
    },
    {
        "loss": 1.5984,
        "grad_norm": 1.7374409437179565,
        "learning_rate": 0.00011330478626768319,
        "epoch": 0.48938516680452165,
        "step": 1775
    },
    {
        "loss": 1.9687,
        "grad_norm": 1.453912615776062,
        "learning_rate": 0.00011304541320036876,
        "epoch": 0.48966087675765096,
        "step": 1776
    },
    {
        "loss": 2.0589,
        "grad_norm": 1.1574797630310059,
        "learning_rate": 0.0001127859508205166,
        "epoch": 0.4899365867107803,
        "step": 1777
    },
    {
        "loss": 2.4587,
        "grad_norm": 1.6672813892364502,
        "learning_rate": 0.00011252640090447855,
        "epoch": 0.4902122966639096,
        "step": 1778
    },
    {
        "loss": 1.9892,
        "grad_norm": 1.032194972038269,
        "learning_rate": 0.00011226676522920571,
        "epoch": 0.4904880066170389,
        "step": 1779
    },
    {
        "loss": 2.0751,
        "grad_norm": 1.3811811208724976,
        "learning_rate": 0.00011200704557223636,
        "epoch": 0.4907637165701682,
        "step": 1780
    },
    {
        "loss": 1.5183,
        "grad_norm": 1.4756723642349243,
        "learning_rate": 0.00011174724371168374,
        "epoch": 0.4910394265232975,
        "step": 1781
    },
    {
        "loss": 2.6655,
        "grad_norm": 1.2670568227767944,
        "learning_rate": 0.0001114873614262238,
        "epoch": 0.4913151364764268,
        "step": 1782
    },
    {
        "loss": 2.282,
        "grad_norm": 1.7924848794937134,
        "learning_rate": 0.00011122740049508322,
        "epoch": 0.4915908464295561,
        "step": 1783
    },
    {
        "loss": 2.0796,
        "grad_norm": 1.547169804573059,
        "learning_rate": 0.00011096736269802699,
        "epoch": 0.4918665563826854,
        "step": 1784
    },
    {
        "loss": 2.089,
        "grad_norm": 1.8139398097991943,
        "learning_rate": 0.00011070724981534644,
        "epoch": 0.4921422663358147,
        "step": 1785
    },
    {
        "loss": 2.3534,
        "grad_norm": 2.113522529602051,
        "learning_rate": 0.00011044706362784694,
        "epoch": 0.492417976288944,
        "step": 1786
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.0502214431762695,
        "learning_rate": 0.00011018680591683567,
        "epoch": 0.4926936862420733,
        "step": 1787
    },
    {
        "loss": 2.2229,
        "grad_norm": 1.0925594568252563,
        "learning_rate": 0.00010992647846410955,
        "epoch": 0.49296939619520264,
        "step": 1788
    },
    {
        "loss": 1.96,
        "grad_norm": 2.092897415161133,
        "learning_rate": 0.0001096660830519429,
        "epoch": 0.49324510614833195,
        "step": 1789
    },
    {
        "loss": 1.9917,
        "grad_norm": 1.4315993785858154,
        "learning_rate": 0.00010940562146307536,
        "epoch": 0.49352081610146126,
        "step": 1790
    },
    {
        "loss": 2.3186,
        "grad_norm": 1.5126124620437622,
        "learning_rate": 0.00010914509548069967,
        "epoch": 0.49379652605459057,
        "step": 1791
    },
    {
        "loss": 2.5967,
        "grad_norm": 1.277091145515442,
        "learning_rate": 0.00010888450688844929,
        "epoch": 0.4940722360077199,
        "step": 1792
    },
    {
        "loss": 2.0493,
        "grad_norm": 2.126375198364258,
        "learning_rate": 0.0001086238574703865,
        "epoch": 0.4943479459608492,
        "step": 1793
    },
    {
        "loss": 2.1398,
        "grad_norm": 1.0878586769104004,
        "learning_rate": 0.00010836314901098985,
        "epoch": 0.4946236559139785,
        "step": 1794
    },
    {
        "loss": 1.9197,
        "grad_norm": 2.067213535308838,
        "learning_rate": 0.00010810238329514224,
        "epoch": 0.4948993658671078,
        "step": 1795
    },
    {
        "loss": 2.4123,
        "grad_norm": 1.5953540802001953,
        "learning_rate": 0.00010784156210811846,
        "epoch": 0.4951750758202371,
        "step": 1796
    },
    {
        "loss": 1.7783,
        "grad_norm": 1.5644084215164185,
        "learning_rate": 0.0001075806872355731,
        "epoch": 0.49545078577336643,
        "step": 1797
    },
    {
        "loss": 2.3138,
        "grad_norm": 0.909298837184906,
        "learning_rate": 0.00010731976046352836,
        "epoch": 0.49572649572649574,
        "step": 1798
    },
    {
        "loss": 2.1569,
        "grad_norm": 1.577335238456726,
        "learning_rate": 0.00010705878357836168,
        "epoch": 0.49600220567962505,
        "step": 1799
    },
    {
        "loss": 2.2044,
        "grad_norm": 2.1806581020355225,
        "learning_rate": 0.00010679775836679357,
        "epoch": 0.49627791563275436,
        "step": 1800
    },
    {
        "loss": 1.2787,
        "grad_norm": 1.375175952911377,
        "learning_rate": 0.00010653668661587551,
        "epoch": 0.4965536255858837,
        "step": 1801
    },
    {
        "loss": 1.8317,
        "grad_norm": 1.7719968557357788,
        "learning_rate": 0.00010627557011297748,
        "epoch": 0.496829335539013,
        "step": 1802
    },
    {
        "loss": 1.4039,
        "grad_norm": 1.6181210279464722,
        "learning_rate": 0.00010601441064577595,
        "epoch": 0.49710504549214224,
        "step": 1803
    },
    {
        "loss": 2.4428,
        "grad_norm": 1.4343962669372559,
        "learning_rate": 0.00010575321000224142,
        "epoch": 0.49738075544527155,
        "step": 1804
    },
    {
        "loss": 2.5013,
        "grad_norm": 1.1788225173950195,
        "learning_rate": 0.00010549196997062636,
        "epoch": 0.49765646539840086,
        "step": 1805
    },
    {
        "loss": 1.3665,
        "grad_norm": 1.973164677619934,
        "learning_rate": 0.00010523069233945297,
        "epoch": 0.49793217535153017,
        "step": 1806
    },
    {
        "loss": 2.0573,
        "grad_norm": 1.3790509700775146,
        "learning_rate": 0.00010496937889750071,
        "epoch": 0.4982078853046595,
        "step": 1807
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.6379705667495728,
        "learning_rate": 0.00010470803143379437,
        "epoch": 0.4984835952577888,
        "step": 1808
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.2858903408050537,
        "learning_rate": 0.00010444665173759152,
        "epoch": 0.4987593052109181,
        "step": 1809
    },
    {
        "loss": 1.5014,
        "grad_norm": 2.122666120529175,
        "learning_rate": 0.00010418524159837053,
        "epoch": 0.4990350151640474,
        "step": 1810
    },
    {
        "loss": 2.1491,
        "grad_norm": 2.2890067100524902,
        "learning_rate": 0.00010392380280581808,
        "epoch": 0.4993107251171767,
        "step": 1811
    },
    {
        "loss": 1.7213,
        "grad_norm": 1.6084378957748413,
        "learning_rate": 0.0001036623371498171,
        "epoch": 0.49958643507030603,
        "step": 1812
    },
    {
        "loss": 2.4194,
        "grad_norm": 0.9934415817260742,
        "learning_rate": 0.00010340084642043441,
        "epoch": 0.49986214502343534,
        "step": 1813
    },
    {
        "loss": 2.392,
        "grad_norm": 1.3092637062072754,
        "learning_rate": 0.00010313933240790848,
        "epoch": 0.5001378549765646,
        "step": 1814
    },
    {
        "loss": 1.7091,
        "grad_norm": 2.3727993965148926,
        "learning_rate": 0.00010287779690263717,
        "epoch": 0.5004135649296939,
        "step": 1815
    },
    {
        "loss": 2.0659,
        "grad_norm": 1.6538670063018799,
        "learning_rate": 0.0001026162416951655,
        "epoch": 0.5006892748828232,
        "step": 1816
    },
    {
        "loss": 1.9995,
        "grad_norm": 1.5915929079055786,
        "learning_rate": 0.00010235466857617342,
        "epoch": 0.5009649848359525,
        "step": 1817
    },
    {
        "loss": 1.9553,
        "grad_norm": 2.2555103302001953,
        "learning_rate": 0.00010209307933646346,
        "epoch": 0.5012406947890818,
        "step": 1818
    },
    {
        "loss": 2.1687,
        "grad_norm": 1.579638957977295,
        "learning_rate": 0.00010183147576694847,
        "epoch": 0.5015164047422112,
        "step": 1819
    },
    {
        "loss": 1.8679,
        "grad_norm": 1.4587076902389526,
        "learning_rate": 0.00010156985965863954,
        "epoch": 0.5017921146953405,
        "step": 1820
    },
    {
        "loss": 1.8178,
        "grad_norm": 2.4217183589935303,
        "learning_rate": 0.00010130823280263344,
        "epoch": 0.5020678246484698,
        "step": 1821
    },
    {
        "loss": 2.2838,
        "grad_norm": 1.4712921380996704,
        "learning_rate": 0.00010104659699010066,
        "epoch": 0.5023435346015991,
        "step": 1822
    },
    {
        "loss": 2.4741,
        "grad_norm": 1.2255563735961914,
        "learning_rate": 0.00010078495401227296,
        "epoch": 0.5026192445547284,
        "step": 1823
    },
    {
        "loss": 2.0369,
        "grad_norm": 1.9014915227890015,
        "learning_rate": 0.00010052330566043113,
        "epoch": 0.5028949545078577,
        "step": 1824
    },
    {
        "loss": 2.3446,
        "grad_norm": 1.0346213579177856,
        "learning_rate": 0.00010026165372589278,
        "epoch": 0.503170664460987,
        "step": 1825
    },
    {
        "loss": 2.176,
        "grad_norm": 1.9761974811553955,
        "learning_rate": 0.0001,
        "epoch": 0.5034463744141163,
        "step": 1826
    },
    {
        "loss": 2.6165,
        "grad_norm": 1.5066275596618652,
        "learning_rate": 9.973834627410725e-05,
        "epoch": 0.5037220843672456,
        "step": 1827
    },
    {
        "loss": 2.4708,
        "grad_norm": 1.3369405269622803,
        "learning_rate": 9.94766943395689e-05,
        "epoch": 0.503997794320375,
        "step": 1828
    },
    {
        "loss": 1.7626,
        "grad_norm": 1.883522391319275,
        "learning_rate": 9.921504598772705e-05,
        "epoch": 0.5042735042735043,
        "step": 1829
    },
    {
        "loss": 1.9601,
        "grad_norm": 1.54108464717865,
        "learning_rate": 9.895340300989935e-05,
        "epoch": 0.5045492142266336,
        "step": 1830
    },
    {
        "loss": 1.1889,
        "grad_norm": 1.6906561851501465,
        "learning_rate": 9.869176719736657e-05,
        "epoch": 0.5048249241797629,
        "step": 1831
    },
    {
        "loss": 1.8719,
        "grad_norm": 2.103663682937622,
        "learning_rate": 9.843014034136047e-05,
        "epoch": 0.5051006341328922,
        "step": 1832
    },
    {
        "loss": 2.6008,
        "grad_norm": 1.5517048835754395,
        "learning_rate": 9.816852423305154e-05,
        "epoch": 0.5053763440860215,
        "step": 1833
    },
    {
        "loss": 2.4799,
        "grad_norm": 1.3163039684295654,
        "learning_rate": 9.790692066353656e-05,
        "epoch": 0.5056520540391508,
        "step": 1834
    },
    {
        "loss": 1.6646,
        "grad_norm": 1.6889679431915283,
        "learning_rate": 9.76453314238266e-05,
        "epoch": 0.5059277639922801,
        "step": 1835
    },
    {
        "loss": 2.0997,
        "grad_norm": 1.0983030796051025,
        "learning_rate": 9.73837583048345e-05,
        "epoch": 0.5062034739454094,
        "step": 1836
    },
    {
        "loss": 2.3389,
        "grad_norm": 0.8642099499702454,
        "learning_rate": 9.712220309736286e-05,
        "epoch": 0.5064791838985387,
        "step": 1837
    },
    {
        "loss": 2.1788,
        "grad_norm": 1.9224952459335327,
        "learning_rate": 9.686066759209156e-05,
        "epoch": 0.506754893851668,
        "step": 1838
    },
    {
        "loss": 2.1675,
        "grad_norm": 1.5696238279342651,
        "learning_rate": 9.659915357956561e-05,
        "epoch": 0.5070306038047974,
        "step": 1839
    },
    {
        "loss": 2.2057,
        "grad_norm": 1.5424258708953857,
        "learning_rate": 9.633766285018291e-05,
        "epoch": 0.5073063137579267,
        "step": 1840
    },
    {
        "loss": 1.8232,
        "grad_norm": 1.7516200542449951,
        "learning_rate": 9.607619719418194e-05,
        "epoch": 0.507582023711056,
        "step": 1841
    },
    {
        "loss": 1.5372,
        "grad_norm": 2.065319061279297,
        "learning_rate": 9.581475840162949e-05,
        "epoch": 0.5078577336641853,
        "step": 1842
    },
    {
        "loss": 2.0701,
        "grad_norm": 1.808050274848938,
        "learning_rate": 9.555334826240849e-05,
        "epoch": 0.5081334436173146,
        "step": 1843
    },
    {
        "loss": 1.9027,
        "grad_norm": 2.3046560287475586,
        "learning_rate": 9.529196856620565e-05,
        "epoch": 0.5084091535704439,
        "step": 1844
    },
    {
        "loss": 2.2238,
        "grad_norm": 1.473001480102539,
        "learning_rate": 9.503062110249931e-05,
        "epoch": 0.5086848635235732,
        "step": 1845
    },
    {
        "loss": 2.1993,
        "grad_norm": 1.2790145874023438,
        "learning_rate": 9.476930766054705e-05,
        "epoch": 0.5089605734767025,
        "step": 1846
    },
    {
        "loss": 2.0458,
        "grad_norm": 1.4822732210159302,
        "learning_rate": 9.450803002937365e-05,
        "epoch": 0.5092362834298318,
        "step": 1847
    },
    {
        "loss": 2.5302,
        "grad_norm": 1.9650912284851074,
        "learning_rate": 9.424678999775861e-05,
        "epoch": 0.5095119933829612,
        "step": 1848
    },
    {
        "loss": 1.8787,
        "grad_norm": 1.4791748523712158,
        "learning_rate": 9.398558935422408e-05,
        "epoch": 0.5097877033360905,
        "step": 1849
    },
    {
        "loss": 1.4688,
        "grad_norm": 1.1859333515167236,
        "learning_rate": 9.372442988702253e-05,
        "epoch": 0.5100634132892198,
        "step": 1850
    },
    {
        "loss": 1.9942,
        "grad_norm": 1.9701600074768066,
        "learning_rate": 9.34633133841245e-05,
        "epoch": 0.5103391232423491,
        "step": 1851
    },
    {
        "loss": 1.8677,
        "grad_norm": 1.2656328678131104,
        "learning_rate": 9.320224163320645e-05,
        "epoch": 0.5106148331954784,
        "step": 1852
    },
    {
        "loss": 1.8918,
        "grad_norm": 2.5160973072052,
        "learning_rate": 9.294121642163836e-05,
        "epoch": 0.5108905431486077,
        "step": 1853
    },
    {
        "loss": 2.2497,
        "grad_norm": 1.7263367176055908,
        "learning_rate": 9.268023953647165e-05,
        "epoch": 0.511166253101737,
        "step": 1854
    },
    {
        "loss": 2.2939,
        "grad_norm": 1.132758378982544,
        "learning_rate": 9.241931276442692e-05,
        "epoch": 0.5114419630548663,
        "step": 1855
    },
    {
        "loss": 1.7656,
        "grad_norm": 1.9330137968063354,
        "learning_rate": 9.215843789188156e-05,
        "epoch": 0.5117176730079956,
        "step": 1856
    },
    {
        "loss": 1.8315,
        "grad_norm": 1.404186487197876,
        "learning_rate": 9.189761670485779e-05,
        "epoch": 0.5119933829611248,
        "step": 1857
    },
    {
        "loss": 2.2111,
        "grad_norm": 1.091268539428711,
        "learning_rate": 9.163685098901016e-05,
        "epoch": 0.5122690929142542,
        "step": 1858
    },
    {
        "loss": 2.0074,
        "grad_norm": 1.6171754598617554,
        "learning_rate": 9.137614252961352e-05,
        "epoch": 0.5125448028673835,
        "step": 1859
    },
    {
        "loss": 1.9819,
        "grad_norm": 1.496878981590271,
        "learning_rate": 9.111549311155074e-05,
        "epoch": 0.5128205128205128,
        "step": 1860
    },
    {
        "loss": 2.3686,
        "grad_norm": 1.1734064817428589,
        "learning_rate": 9.085490451930037e-05,
        "epoch": 0.5130962227736421,
        "step": 1861
    },
    {
        "loss": 2.4053,
        "grad_norm": 1.3935539722442627,
        "learning_rate": 9.059437853692466e-05,
        "epoch": 0.5133719327267714,
        "step": 1862
    },
    {
        "loss": 2.0501,
        "grad_norm": 1.5810223817825317,
        "learning_rate": 9.033391694805713e-05,
        "epoch": 0.5136476426799007,
        "step": 1863
    },
    {
        "loss": 2.1436,
        "grad_norm": 1.0641376972198486,
        "learning_rate": 9.007352153589047e-05,
        "epoch": 0.51392335263303,
        "step": 1864
    },
    {
        "loss": 2.2186,
        "grad_norm": 1.5762122869491577,
        "learning_rate": 8.981319408316435e-05,
        "epoch": 0.5141990625861593,
        "step": 1865
    },
    {
        "loss": 2.246,
        "grad_norm": 1.1864715814590454,
        "learning_rate": 8.955293637215307e-05,
        "epoch": 0.5144747725392886,
        "step": 1866
    },
    {
        "loss": 2.2278,
        "grad_norm": 1.2686173915863037,
        "learning_rate": 8.929275018465357e-05,
        "epoch": 0.514750482492418,
        "step": 1867
    },
    {
        "loss": 2.0899,
        "grad_norm": 1.2045131921768188,
        "learning_rate": 8.903263730197303e-05,
        "epoch": 0.5150261924455473,
        "step": 1868
    },
    {
        "loss": 2.5869,
        "grad_norm": 1.3399813175201416,
        "learning_rate": 8.87725995049168e-05,
        "epoch": 0.5153019023986766,
        "step": 1869
    },
    {
        "loss": 2.2551,
        "grad_norm": 1.7040307521820068,
        "learning_rate": 8.851263857377622e-05,
        "epoch": 0.5155776123518059,
        "step": 1870
    },
    {
        "loss": 1.3993,
        "grad_norm": 2.793311834335327,
        "learning_rate": 8.825275628831628e-05,
        "epoch": 0.5158533223049352,
        "step": 1871
    },
    {
        "loss": 2.113,
        "grad_norm": 1.4112110137939453,
        "learning_rate": 8.799295442776366e-05,
        "epoch": 0.5161290322580645,
        "step": 1872
    },
    {
        "loss": 2.591,
        "grad_norm": 1.156529426574707,
        "learning_rate": 8.77332347707943e-05,
        "epoch": 0.5164047422111938,
        "step": 1873
    },
    {
        "loss": 1.7961,
        "grad_norm": 1.8457714319229126,
        "learning_rate": 8.747359909552146e-05,
        "epoch": 0.5166804521643231,
        "step": 1874
    },
    {
        "loss": 2.4328,
        "grad_norm": 1.2636499404907227,
        "learning_rate": 8.721404917948342e-05,
        "epoch": 0.5169561621174524,
        "step": 1875
    },
    {
        "loss": 1.7429,
        "grad_norm": 2.338353157043457,
        "learning_rate": 8.695458679963126e-05,
        "epoch": 0.5172318720705817,
        "step": 1876
    },
    {
        "loss": 2.1311,
        "grad_norm": 1.1857000589370728,
        "learning_rate": 8.669521373231683e-05,
        "epoch": 0.517507582023711,
        "step": 1877
    },
    {
        "loss": 2.1359,
        "grad_norm": 1.391160488128662,
        "learning_rate": 8.643593175328048e-05,
        "epoch": 0.5177832919768404,
        "step": 1878
    },
    {
        "loss": 2.0003,
        "grad_norm": 1.7218728065490723,
        "learning_rate": 8.6176742637639e-05,
        "epoch": 0.5180590019299697,
        "step": 1879
    },
    {
        "loss": 2.3692,
        "grad_norm": 1.530859112739563,
        "learning_rate": 8.591764815987336e-05,
        "epoch": 0.518334711883099,
        "step": 1880
    },
    {
        "loss": 2.5697,
        "grad_norm": 0.9803801774978638,
        "learning_rate": 8.565865009381659e-05,
        "epoch": 0.5186104218362283,
        "step": 1881
    },
    {
        "loss": 1.495,
        "grad_norm": 2.314776659011841,
        "learning_rate": 8.539975021264177e-05,
        "epoch": 0.5188861317893576,
        "step": 1882
    },
    {
        "loss": 2.4261,
        "grad_norm": 1.5558439493179321,
        "learning_rate": 8.514095028884962e-05,
        "epoch": 0.5191618417424869,
        "step": 1883
    },
    {
        "loss": 2.3956,
        "grad_norm": 1.1317155361175537,
        "learning_rate": 8.488225209425669e-05,
        "epoch": 0.5194375516956162,
        "step": 1884
    },
    {
        "loss": 2.389,
        "grad_norm": 1.5598883628845215,
        "learning_rate": 8.462365739998293e-05,
        "epoch": 0.5197132616487455,
        "step": 1885
    },
    {
        "loss": 2.1129,
        "grad_norm": 1.2065469026565552,
        "learning_rate": 8.436516797643975e-05,
        "epoch": 0.5199889716018748,
        "step": 1886
    },
    {
        "loss": 1.9892,
        "grad_norm": 1.9721200466156006,
        "learning_rate": 8.410678559331791e-05,
        "epoch": 0.5202646815550042,
        "step": 1887
    },
    {
        "loss": 1.8397,
        "grad_norm": 1.2368603944778442,
        "learning_rate": 8.384851201957519e-05,
        "epoch": 0.5205403915081335,
        "step": 1888
    },
    {
        "loss": 2.335,
        "grad_norm": 1.5952125787734985,
        "learning_rate": 8.35903490234246e-05,
        "epoch": 0.5208161014612628,
        "step": 1889
    },
    {
        "loss": 1.835,
        "grad_norm": 2.2934582233428955,
        "learning_rate": 8.333229837232196e-05,
        "epoch": 0.5210918114143921,
        "step": 1890
    },
    {
        "loss": 1.7811,
        "grad_norm": 1.8778963088989258,
        "learning_rate": 8.307436183295405e-05,
        "epoch": 0.5213675213675214,
        "step": 1891
    },
    {
        "loss": 1.9048,
        "grad_norm": 2.6848864555358887,
        "learning_rate": 8.281654117122636e-05,
        "epoch": 0.5216432313206507,
        "step": 1892
    },
    {
        "loss": 1.1884,
        "grad_norm": 2.001070261001587,
        "learning_rate": 8.255883815225105e-05,
        "epoch": 0.52191894127378,
        "step": 1893
    },
    {
        "loss": 2.3328,
        "grad_norm": 1.256546139717102,
        "learning_rate": 8.23012545403349e-05,
        "epoch": 0.5221946512269093,
        "step": 1894
    },
    {
        "loss": 1.8349,
        "grad_norm": 1.9446446895599365,
        "learning_rate": 8.204379209896712e-05,
        "epoch": 0.5224703611800386,
        "step": 1895
    },
    {
        "loss": 2.0236,
        "grad_norm": 1.6822130680084229,
        "learning_rate": 8.178645259080743e-05,
        "epoch": 0.522746071133168,
        "step": 1896
    },
    {
        "loss": 1.1849,
        "grad_norm": 1.7909270524978638,
        "learning_rate": 8.152923777767392e-05,
        "epoch": 0.5230217810862973,
        "step": 1897
    },
    {
        "loss": 1.7923,
        "grad_norm": 1.4307613372802734,
        "learning_rate": 8.127214942053087e-05,
        "epoch": 0.5232974910394266,
        "step": 1898
    },
    {
        "loss": 2.2443,
        "grad_norm": 1.3344480991363525,
        "learning_rate": 8.101518927947698e-05,
        "epoch": 0.5235732009925558,
        "step": 1899
    },
    {
        "loss": 1.898,
        "grad_norm": 1.5688385963439941,
        "learning_rate": 8.075835911373297e-05,
        "epoch": 0.5238489109456851,
        "step": 1900
    },
    {
        "loss": 1.9823,
        "grad_norm": 1.147009253501892,
        "learning_rate": 8.050166068162983e-05,
        "epoch": 0.5241246208988144,
        "step": 1901
    },
    {
        "loss": 2.4039,
        "grad_norm": 1.15895414352417,
        "learning_rate": 8.024509574059666e-05,
        "epoch": 0.5244003308519437,
        "step": 1902
    },
    {
        "loss": 1.9401,
        "grad_norm": 1.9365036487579346,
        "learning_rate": 7.998866604714857e-05,
        "epoch": 0.524676040805073,
        "step": 1903
    },
    {
        "loss": 2.1327,
        "grad_norm": 1.7485215663909912,
        "learning_rate": 7.97323733568748e-05,
        "epoch": 0.5249517507582023,
        "step": 1904
    },
    {
        "loss": 1.8751,
        "grad_norm": 2.2515013217926025,
        "learning_rate": 7.947621942442655e-05,
        "epoch": 0.5252274607113316,
        "step": 1905
    },
    {
        "loss": 1.8831,
        "grad_norm": 1.2679402828216553,
        "learning_rate": 7.922020600350515e-05,
        "epoch": 0.525503170664461,
        "step": 1906
    },
    {
        "loss": 1.4285,
        "grad_norm": 1.7322235107421875,
        "learning_rate": 7.89643348468499e-05,
        "epoch": 0.5257788806175903,
        "step": 1907
    },
    {
        "loss": 1.8852,
        "grad_norm": 2.3232321739196777,
        "learning_rate": 7.870860770622606e-05,
        "epoch": 0.5260545905707196,
        "step": 1908
    },
    {
        "loss": 1.9162,
        "grad_norm": 1.5822662115097046,
        "learning_rate": 7.8453026332413e-05,
        "epoch": 0.5263303005238489,
        "step": 1909
    },
    {
        "loss": 1.5057,
        "grad_norm": 1.8937963247299194,
        "learning_rate": 7.81975924751921e-05,
        "epoch": 0.5266060104769782,
        "step": 1910
    },
    {
        "loss": 1.6522,
        "grad_norm": 2.4174933433532715,
        "learning_rate": 7.794230788333477e-05,
        "epoch": 0.5268817204301075,
        "step": 1911
    },
    {
        "loss": 2.0464,
        "grad_norm": 1.9412506818771362,
        "learning_rate": 7.768717430459061e-05,
        "epoch": 0.5271574303832368,
        "step": 1912
    },
    {
        "loss": 2.2487,
        "grad_norm": 1.3085047006607056,
        "learning_rate": 7.743219348567517e-05,
        "epoch": 0.5274331403363661,
        "step": 1913
    },
    {
        "loss": 1.9064,
        "grad_norm": 1.7903858423233032,
        "learning_rate": 7.717736717225835e-05,
        "epoch": 0.5277088502894954,
        "step": 1914
    },
    {
        "loss": 1.6709,
        "grad_norm": 2.0648000240325928,
        "learning_rate": 7.692269710895209e-05,
        "epoch": 0.5279845602426247,
        "step": 1915
    },
    {
        "loss": 1.9132,
        "grad_norm": 1.4422039985656738,
        "learning_rate": 7.666818503929874e-05,
        "epoch": 0.5282602701957541,
        "step": 1916
    },
    {
        "loss": 2.3469,
        "grad_norm": 1.731333613395691,
        "learning_rate": 7.641383270575892e-05,
        "epoch": 0.5285359801488834,
        "step": 1917
    },
    {
        "loss": 2.2755,
        "grad_norm": 0.9905375242233276,
        "learning_rate": 7.615964184969964e-05,
        "epoch": 0.5288116901020127,
        "step": 1918
    },
    {
        "loss": 1.7201,
        "grad_norm": 1.7562037706375122,
        "learning_rate": 7.590561421138241e-05,
        "epoch": 0.529087400055142,
        "step": 1919
    },
    {
        "loss": 2.2911,
        "grad_norm": 1.6786879301071167,
        "learning_rate": 7.565175152995129e-05,
        "epoch": 0.5293631100082713,
        "step": 1920
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.1648638248443604,
        "learning_rate": 7.539805554342104e-05,
        "epoch": 0.5296388199614006,
        "step": 1921
    },
    {
        "loss": 2.5245,
        "grad_norm": 1.2240374088287354,
        "learning_rate": 7.514452798866514e-05,
        "epoch": 0.5299145299145299,
        "step": 1922
    },
    {
        "loss": 2.3522,
        "grad_norm": 1.2542794942855835,
        "learning_rate": 7.489117060140393e-05,
        "epoch": 0.5301902398676592,
        "step": 1923
    },
    {
        "loss": 1.9166,
        "grad_norm": 1.4479478597640991,
        "learning_rate": 7.46379851161928e-05,
        "epoch": 0.5304659498207885,
        "step": 1924
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.8899469375610352,
        "learning_rate": 7.438497326641014e-05,
        "epoch": 0.5307416597739179,
        "step": 1925
    },
    {
        "loss": 1.6659,
        "grad_norm": 2.1689484119415283,
        "learning_rate": 7.413213678424573e-05,
        "epoch": 0.5310173697270472,
        "step": 1926
    },
    {
        "loss": 2.1444,
        "grad_norm": 1.147747278213501,
        "learning_rate": 7.387947740068856e-05,
        "epoch": 0.5312930796801765,
        "step": 1927
    },
    {
        "loss": 1.402,
        "grad_norm": 1.6682344675064087,
        "learning_rate": 7.362699684551533e-05,
        "epoch": 0.5315687896333058,
        "step": 1928
    },
    {
        "loss": 2.2739,
        "grad_norm": 1.664568305015564,
        "learning_rate": 7.337469684727833e-05,
        "epoch": 0.5318444995864351,
        "step": 1929
    },
    {
        "loss": 2.0185,
        "grad_norm": 1.2610939741134644,
        "learning_rate": 7.31225791332937e-05,
        "epoch": 0.5321202095395644,
        "step": 1930
    },
    {
        "loss": 1.9384,
        "grad_norm": 1.165101408958435,
        "learning_rate": 7.287064542962966e-05,
        "epoch": 0.5323959194926937,
        "step": 1931
    },
    {
        "loss": 2.6886,
        "grad_norm": 1.316154956817627,
        "learning_rate": 7.26188974610946e-05,
        "epoch": 0.532671629445823,
        "step": 1932
    },
    {
        "loss": 2.0573,
        "grad_norm": 1.6688586473464966,
        "learning_rate": 7.236733695122535e-05,
        "epoch": 0.5329473393989523,
        "step": 1933
    },
    {
        "loss": 1.941,
        "grad_norm": 1.8228580951690674,
        "learning_rate": 7.211596562227536e-05,
        "epoch": 0.5332230493520816,
        "step": 1934
    },
    {
        "loss": 1.911,
        "grad_norm": 1.6664577722549438,
        "learning_rate": 7.18647851952028e-05,
        "epoch": 0.533498759305211,
        "step": 1935
    },
    {
        "loss": 1.7143,
        "grad_norm": 1.862816572189331,
        "learning_rate": 7.161379738965902e-05,
        "epoch": 0.5337744692583403,
        "step": 1936
    },
    {
        "loss": 2.3231,
        "grad_norm": 1.4218549728393555,
        "learning_rate": 7.136300392397647e-05,
        "epoch": 0.5340501792114696,
        "step": 1937
    },
    {
        "loss": 2.5162,
        "grad_norm": 0.9683691263198853,
        "learning_rate": 7.111240651515721e-05,
        "epoch": 0.5343258891645989,
        "step": 1938
    },
    {
        "loss": 2.1302,
        "grad_norm": 1.2254478931427002,
        "learning_rate": 7.086200687886103e-05,
        "epoch": 0.5346015991177282,
        "step": 1939
    },
    {
        "loss": 2.3733,
        "grad_norm": 1.5623990297317505,
        "learning_rate": 7.061180672939363e-05,
        "epoch": 0.5348773090708575,
        "step": 1940
    },
    {
        "loss": 2.5824,
        "grad_norm": 1.246011734008789,
        "learning_rate": 7.03618077796951e-05,
        "epoch": 0.5351530190239868,
        "step": 1941
    },
    {
        "loss": 1.8829,
        "grad_norm": 1.79948091506958,
        "learning_rate": 7.01120117413279e-05,
        "epoch": 0.535428728977116,
        "step": 1942
    },
    {
        "loss": 2.4606,
        "grad_norm": 1.1602325439453125,
        "learning_rate": 6.986242032446545e-05,
        "epoch": 0.5357044389302453,
        "step": 1943
    },
    {
        "loss": 1.8628,
        "grad_norm": 1.2404171228408813,
        "learning_rate": 6.961303523788021e-05,
        "epoch": 0.5359801488833746,
        "step": 1944
    },
    {
        "loss": 2.2624,
        "grad_norm": 1.181396722793579,
        "learning_rate": 6.9363858188932e-05,
        "epoch": 0.536255858836504,
        "step": 1945
    },
    {
        "loss": 2.4085,
        "grad_norm": 1.2909437417984009,
        "learning_rate": 6.911489088355645e-05,
        "epoch": 0.5365315687896333,
        "step": 1946
    },
    {
        "loss": 2.1684,
        "grad_norm": 2.195945978164673,
        "learning_rate": 6.886613502625316e-05,
        "epoch": 0.5368072787427626,
        "step": 1947
    },
    {
        "loss": 2.7248,
        "grad_norm": 0.8975013494491577,
        "learning_rate": 6.861759232007412e-05,
        "epoch": 0.5370829886958919,
        "step": 1948
    },
    {
        "loss": 2.2394,
        "grad_norm": 1.1181974411010742,
        "learning_rate": 6.836926446661204e-05,
        "epoch": 0.5373586986490212,
        "step": 1949
    },
    {
        "loss": 1.889,
        "grad_norm": 1.900170922279358,
        "learning_rate": 6.812115316598866e-05,
        "epoch": 0.5376344086021505,
        "step": 1950
    },
    {
        "loss": 2.4455,
        "grad_norm": 1.4085359573364258,
        "learning_rate": 6.787326011684318e-05,
        "epoch": 0.5379101185552798,
        "step": 1951
    },
    {
        "loss": 2.1883,
        "grad_norm": 1.352427363395691,
        "learning_rate": 6.76255870163205e-05,
        "epoch": 0.5381858285084091,
        "step": 1952
    },
    {
        "loss": 1.4595,
        "grad_norm": 1.1657706499099731,
        "learning_rate": 6.73781355600598e-05,
        "epoch": 0.5384615384615384,
        "step": 1953
    },
    {
        "loss": 2.0912,
        "grad_norm": 1.276833415031433,
        "learning_rate": 6.71309074421828e-05,
        "epoch": 0.5387372484146677,
        "step": 1954
    },
    {
        "loss": 1.1796,
        "grad_norm": 2.4334118366241455,
        "learning_rate": 6.688390435528211e-05,
        "epoch": 0.5390129583677971,
        "step": 1955
    },
    {
        "loss": 2.388,
        "grad_norm": 1.0007281303405762,
        "learning_rate": 6.66371279904098e-05,
        "epoch": 0.5392886683209264,
        "step": 1956
    },
    {
        "loss": 2.4943,
        "grad_norm": 1.2915565967559814,
        "learning_rate": 6.639058003706565e-05,
        "epoch": 0.5395643782740557,
        "step": 1957
    },
    {
        "loss": 1.8125,
        "grad_norm": 1.4931840896606445,
        "learning_rate": 6.614426218318576e-05,
        "epoch": 0.539840088227185,
        "step": 1958
    },
    {
        "loss": 2.6065,
        "grad_norm": 1.388563871383667,
        "learning_rate": 6.58981761151309e-05,
        "epoch": 0.5401157981803143,
        "step": 1959
    },
    {
        "loss": 2.6695,
        "grad_norm": 1.0786844491958618,
        "learning_rate": 6.565232351767485e-05,
        "epoch": 0.5403915081334436,
        "step": 1960
    },
    {
        "loss": 2.3563,
        "grad_norm": 1.4325424432754517,
        "learning_rate": 6.540670607399315e-05,
        "epoch": 0.5406672180865729,
        "step": 1961
    },
    {
        "loss": 2.3445,
        "grad_norm": 1.5674268007278442,
        "learning_rate": 6.516132546565128e-05,
        "epoch": 0.5409429280397022,
        "step": 1962
    },
    {
        "loss": 1.9228,
        "grad_norm": 2.171626329421997,
        "learning_rate": 6.491618337259338e-05,
        "epoch": 0.5412186379928315,
        "step": 1963
    },
    {
        "loss": 1.1481,
        "grad_norm": 2.052835464477539,
        "learning_rate": 6.467128147313053e-05,
        "epoch": 0.5414943479459609,
        "step": 1964
    },
    {
        "loss": 1.5041,
        "grad_norm": 1.8193672895431519,
        "learning_rate": 6.442662144392953e-05,
        "epoch": 0.5417700578990902,
        "step": 1965
    },
    {
        "loss": 2.5764,
        "grad_norm": 1.3173521757125854,
        "learning_rate": 6.418220496000115e-05,
        "epoch": 0.5420457678522195,
        "step": 1966
    },
    {
        "loss": 1.9765,
        "grad_norm": 1.6260720491409302,
        "learning_rate": 6.39380336946888e-05,
        "epoch": 0.5423214778053488,
        "step": 1967
    },
    {
        "loss": 1.8237,
        "grad_norm": 2.0132036209106445,
        "learning_rate": 6.369410931965712e-05,
        "epoch": 0.5425971877584781,
        "step": 1968
    },
    {
        "loss": 2.5947,
        "grad_norm": 1.0893549919128418,
        "learning_rate": 6.345043350488035e-05,
        "epoch": 0.5428728977116074,
        "step": 1969
    },
    {
        "loss": 2.5193,
        "grad_norm": 1.0758039951324463,
        "learning_rate": 6.320700791863114e-05,
        "epoch": 0.5431486076647367,
        "step": 1970
    },
    {
        "loss": 2.6152,
        "grad_norm": 1.480462670326233,
        "learning_rate": 6.296383422746899e-05,
        "epoch": 0.543424317617866,
        "step": 1971
    },
    {
        "loss": 2.2811,
        "grad_norm": 0.9045078754425049,
        "learning_rate": 6.272091409622873e-05,
        "epoch": 0.5437000275709953,
        "step": 1972
    },
    {
        "loss": 2.8232,
        "grad_norm": 1.0750459432601929,
        "learning_rate": 6.247824918800943e-05,
        "epoch": 0.5439757375241246,
        "step": 1973
    },
    {
        "loss": 1.832,
        "grad_norm": 1.9522013664245605,
        "learning_rate": 6.223584116416267e-05,
        "epoch": 0.544251447477254,
        "step": 1974
    },
    {
        "loss": 2.0216,
        "grad_norm": 1.996708869934082,
        "learning_rate": 6.199369168428145e-05,
        "epoch": 0.5445271574303833,
        "step": 1975
    },
    {
        "loss": 2.2826,
        "grad_norm": 1.4572110176086426,
        "learning_rate": 6.175180240618865e-05,
        "epoch": 0.5448028673835126,
        "step": 1976
    },
    {
        "loss": 2.3237,
        "grad_norm": 1.471522331237793,
        "learning_rate": 6.15101749859257e-05,
        "epoch": 0.5450785773366419,
        "step": 1977
    },
    {
        "loss": 2.4069,
        "grad_norm": 1.6020156145095825,
        "learning_rate": 6.126881107774138e-05,
        "epoch": 0.5453542872897712,
        "step": 1978
    },
    {
        "loss": 1.8637,
        "grad_norm": 2.0652096271514893,
        "learning_rate": 6.102771233408029e-05,
        "epoch": 0.5456299972429005,
        "step": 1979
    },
    {
        "loss": 1.9681,
        "grad_norm": 1.397598385810852,
        "learning_rate": 6.078688040557169e-05,
        "epoch": 0.5459057071960298,
        "step": 1980
    },
    {
        "loss": 2.0245,
        "grad_norm": 1.0838885307312012,
        "learning_rate": 6.054631694101817e-05,
        "epoch": 0.5461814171491591,
        "step": 1981
    },
    {
        "loss": 2.3844,
        "grad_norm": 1.0915336608886719,
        "learning_rate": 6.0306023587384265e-05,
        "epoch": 0.5464571271022884,
        "step": 1982
    },
    {
        "loss": 1.6259,
        "grad_norm": 1.5941849946975708,
        "learning_rate": 6.0066001989785334e-05,
        "epoch": 0.5467328370554178,
        "step": 1983
    },
    {
        "loss": 1.764,
        "grad_norm": 2.1647698879241943,
        "learning_rate": 5.982625379147617e-05,
        "epoch": 0.5470085470085471,
        "step": 1984
    },
    {
        "loss": 2.417,
        "grad_norm": 1.3443350791931152,
        "learning_rate": 5.9586780633839825e-05,
        "epoch": 0.5472842569616763,
        "step": 1985
    },
    {
        "loss": 2.4264,
        "grad_norm": 1.2734583616256714,
        "learning_rate": 5.934758415637636e-05,
        "epoch": 0.5475599669148056,
        "step": 1986
    },
    {
        "loss": 2.4613,
        "grad_norm": 1.1828680038452148,
        "learning_rate": 5.9108665996691536e-05,
        "epoch": 0.5478356768679349,
        "step": 1987
    },
    {
        "loss": 2.5601,
        "grad_norm": 1.0034757852554321,
        "learning_rate": 5.887002779048576e-05,
        "epoch": 0.5481113868210642,
        "step": 1988
    },
    {
        "loss": 1.6462,
        "grad_norm": 1.9117926359176636,
        "learning_rate": 5.8631671171542715e-05,
        "epoch": 0.5483870967741935,
        "step": 1989
    },
    {
        "loss": 1.2042,
        "grad_norm": 1.8017741441726685,
        "learning_rate": 5.8393597771718336e-05,
        "epoch": 0.5486628067273228,
        "step": 1990
    },
    {
        "loss": 1.9818,
        "grad_norm": 1.4434140920639038,
        "learning_rate": 5.815580922092954e-05,
        "epoch": 0.5489385166804521,
        "step": 1991
    },
    {
        "loss": 2.2011,
        "grad_norm": 1.7329466342926025,
        "learning_rate": 5.7918307147143036e-05,
        "epoch": 0.5492142266335814,
        "step": 1992
    },
    {
        "loss": 2.3223,
        "grad_norm": 1.1431964635849,
        "learning_rate": 5.7681093176364274e-05,
        "epoch": 0.5494899365867107,
        "step": 1993
    },
    {
        "loss": 2.2164,
        "grad_norm": 1.1100085973739624,
        "learning_rate": 5.744416893262632e-05,
        "epoch": 0.5497656465398401,
        "step": 1994
    },
    {
        "loss": 1.581,
        "grad_norm": 1.049778699874878,
        "learning_rate": 5.7207536037978595e-05,
        "epoch": 0.5500413564929694,
        "step": 1995
    },
    {
        "loss": 1.8203,
        "grad_norm": 1.8281219005584717,
        "learning_rate": 5.697119611247587e-05,
        "epoch": 0.5503170664460987,
        "step": 1996
    },
    {
        "loss": 1.8707,
        "grad_norm": 2.1526310443878174,
        "learning_rate": 5.673515077416728e-05,
        "epoch": 0.550592776399228,
        "step": 1997
    },
    {
        "loss": 1.9463,
        "grad_norm": 1.377521276473999,
        "learning_rate": 5.6499401639085e-05,
        "epoch": 0.5508684863523573,
        "step": 1998
    },
    {
        "loss": 1.6958,
        "grad_norm": 1.0556915998458862,
        "learning_rate": 5.626395032123341e-05,
        "epoch": 0.5511441963054866,
        "step": 1999
    },
    {
        "loss": 2.1711,
        "grad_norm": 1.0994747877120972,
        "learning_rate": 5.6028798432577825e-05,
        "epoch": 0.5514199062586159,
        "step": 2000
    },
    {
        "loss": 2.2823,
        "grad_norm": 1.954492449760437,
        "learning_rate": 5.579394758303378e-05,
        "epoch": 0.5516956162117452,
        "step": 2001
    },
    {
        "loss": 2.0685,
        "grad_norm": 0.9255638718605042,
        "learning_rate": 5.5559399380455635e-05,
        "epoch": 0.5519713261648745,
        "step": 2002
    },
    {
        "loss": 2.4321,
        "grad_norm": 1.0063451528549194,
        "learning_rate": 5.53251554306258e-05,
        "epoch": 0.5522470361180039,
        "step": 2003
    },
    {
        "loss": 2.2256,
        "grad_norm": 1.6616019010543823,
        "learning_rate": 5.509121733724374e-05,
        "epoch": 0.5525227460711332,
        "step": 2004
    },
    {
        "loss": 2.1877,
        "grad_norm": 1.4030342102050781,
        "learning_rate": 5.4857586701914895e-05,
        "epoch": 0.5527984560242625,
        "step": 2005
    },
    {
        "loss": 2.0032,
        "grad_norm": 1.3106223344802856,
        "learning_rate": 5.4624265124139715e-05,
        "epoch": 0.5530741659773918,
        "step": 2006
    },
    {
        "loss": 2.279,
        "grad_norm": 1.727461814880371,
        "learning_rate": 5.439125420130288e-05,
        "epoch": 0.5533498759305211,
        "step": 2007
    },
    {
        "loss": 2.1311,
        "grad_norm": 1.518577218055725,
        "learning_rate": 5.415855552866211e-05,
        "epoch": 0.5536255858836504,
        "step": 2008
    },
    {
        "loss": 2.6125,
        "grad_norm": 1.2937095165252686,
        "learning_rate": 5.392617069933746e-05,
        "epoch": 0.5539012958367797,
        "step": 2009
    },
    {
        "loss": 1.7378,
        "grad_norm": 1.1082035303115845,
        "learning_rate": 5.369410130430024e-05,
        "epoch": 0.554177005789909,
        "step": 2010
    },
    {
        "loss": 2.1168,
        "grad_norm": 1.6942945718765259,
        "learning_rate": 5.3462348932362324e-05,
        "epoch": 0.5544527157430383,
        "step": 2011
    },
    {
        "loss": 2.0598,
        "grad_norm": 1.4890787601470947,
        "learning_rate": 5.3230915170165084e-05,
        "epoch": 0.5547284256961676,
        "step": 2012
    },
    {
        "loss": 1.8739,
        "grad_norm": 2.187281847000122,
        "learning_rate": 5.2999801602168566e-05,
        "epoch": 0.555004135649297,
        "step": 2013
    },
    {
        "loss": 2.2091,
        "grad_norm": 1.3799067735671997,
        "learning_rate": 5.2769009810640794e-05,
        "epoch": 0.5552798456024263,
        "step": 2014
    },
    {
        "loss": 1.6476,
        "grad_norm": 1.4944642782211304,
        "learning_rate": 5.253854137564671e-05,
        "epoch": 0.5555555555555556,
        "step": 2015
    },
    {
        "loss": 1.3131,
        "grad_norm": 1.6383678913116455,
        "learning_rate": 5.23083978750375e-05,
        "epoch": 0.5558312655086849,
        "step": 2016
    },
    {
        "loss": 1.7762,
        "grad_norm": 1.345053791999817,
        "learning_rate": 5.20785808844398e-05,
        "epoch": 0.5561069754618142,
        "step": 2017
    },
    {
        "loss": 2.2795,
        "grad_norm": 1.365622878074646,
        "learning_rate": 5.1849091977244835e-05,
        "epoch": 0.5563826854149435,
        "step": 2018
    },
    {
        "loss": 2.4253,
        "grad_norm": 1.2609837055206299,
        "learning_rate": 5.161993272459764e-05,
        "epoch": 0.5566583953680728,
        "step": 2019
    },
    {
        "loss": 2.0371,
        "grad_norm": 1.2681089639663696,
        "learning_rate": 5.139110469538638e-05,
        "epoch": 0.5569341053212021,
        "step": 2020
    },
    {
        "loss": 2.5414,
        "grad_norm": 1.213287591934204,
        "learning_rate": 5.116260945623164e-05,
        "epoch": 0.5572098152743314,
        "step": 2021
    },
    {
        "loss": 1.9261,
        "grad_norm": 1.874851942062378,
        "learning_rate": 5.09344485714755e-05,
        "epoch": 0.5574855252274608,
        "step": 2022
    },
    {
        "loss": 2.353,
        "grad_norm": 1.3173383474349976,
        "learning_rate": 5.070662360317101e-05,
        "epoch": 0.5577612351805901,
        "step": 2023
    },
    {
        "loss": 1.8193,
        "grad_norm": 2.2017414569854736,
        "learning_rate": 5.047913611107153e-05,
        "epoch": 0.5580369451337194,
        "step": 2024
    },
    {
        "loss": 2.459,
        "grad_norm": 1.412046194076538,
        "learning_rate": 5.0251987652619835e-05,
        "epoch": 0.5583126550868487,
        "step": 2025
    },
    {
        "loss": 1.8852,
        "grad_norm": 1.8074641227722168,
        "learning_rate": 5.0025179782937645e-05,
        "epoch": 0.558588365039978,
        "step": 2026
    },
    {
        "loss": 2.0221,
        "grad_norm": 1.3145413398742676,
        "learning_rate": 4.979871405481486e-05,
        "epoch": 0.5588640749931072,
        "step": 2027
    },
    {
        "loss": 1.6131,
        "grad_norm": 2.747635841369629,
        "learning_rate": 4.9572592018699105e-05,
        "epoch": 0.5591397849462365,
        "step": 2028
    },
    {
        "loss": 1.9299,
        "grad_norm": 1.5622851848602295,
        "learning_rate": 4.9346815222684875e-05,
        "epoch": 0.5594154948993658,
        "step": 2029
    },
    {
        "loss": 2.282,
        "grad_norm": 1.8163541555404663,
        "learning_rate": 4.9121385212503056e-05,
        "epoch": 0.5596912048524951,
        "step": 2030
    },
    {
        "loss": 2.0553,
        "grad_norm": 1.5640499591827393,
        "learning_rate": 4.889630353151046e-05,
        "epoch": 0.5599669148056244,
        "step": 2031
    },
    {
        "loss": 2.616,
        "grad_norm": 1.7943286895751953,
        "learning_rate": 4.867157172067902e-05,
        "epoch": 0.5602426247587537,
        "step": 2032
    },
    {
        "loss": 2.0108,
        "grad_norm": 1.3018754720687866,
        "learning_rate": 4.844719131858536e-05,
        "epoch": 0.5605183347118831,
        "step": 2033
    },
    {
        "loss": 2.0838,
        "grad_norm": 1.5353113412857056,
        "learning_rate": 4.822316386140039e-05,
        "epoch": 0.5607940446650124,
        "step": 2034
    },
    {
        "loss": 1.4664,
        "grad_norm": 1.598878264427185,
        "learning_rate": 4.799949088287852e-05,
        "epoch": 0.5610697546181417,
        "step": 2035
    },
    {
        "loss": 2.1908,
        "grad_norm": 1.5723446607589722,
        "learning_rate": 4.777617391434736e-05,
        "epoch": 0.561345464571271,
        "step": 2036
    },
    {
        "loss": 2.1347,
        "grad_norm": 1.3920490741729736,
        "learning_rate": 4.755321448469713e-05,
        "epoch": 0.5616211745244003,
        "step": 2037
    },
    {
        "loss": 2.0406,
        "grad_norm": 1.5067745447158813,
        "learning_rate": 4.7330614120370363e-05,
        "epoch": 0.5618968844775296,
        "step": 2038
    },
    {
        "loss": 2.3962,
        "grad_norm": 1.540334939956665,
        "learning_rate": 4.71083743453512e-05,
        "epoch": 0.5621725944306589,
        "step": 2039
    },
    {
        "loss": 1.9657,
        "grad_norm": 1.6662942171096802,
        "learning_rate": 4.688649668115509e-05,
        "epoch": 0.5624483043837882,
        "step": 2040
    },
    {
        "loss": 2.33,
        "grad_norm": 1.7998539209365845,
        "learning_rate": 4.6664982646818515e-05,
        "epoch": 0.5627240143369175,
        "step": 2041
    },
    {
        "loss": 1.7311,
        "grad_norm": 1.5368056297302246,
        "learning_rate": 4.644383375888828e-05,
        "epoch": 0.5629997242900469,
        "step": 2042
    },
    {
        "loss": 1.6386,
        "grad_norm": 1.40068781375885,
        "learning_rate": 4.622305153141132e-05,
        "epoch": 0.5632754342431762,
        "step": 2043
    },
    {
        "loss": 2.3219,
        "grad_norm": 1.7244200706481934,
        "learning_rate": 4.600263747592445e-05,
        "epoch": 0.5635511441963055,
        "step": 2044
    },
    {
        "loss": 1.9965,
        "grad_norm": 0.9345765113830566,
        "learning_rate": 4.578259310144373e-05,
        "epoch": 0.5638268541494348,
        "step": 2045
    },
    {
        "loss": 2.4451,
        "grad_norm": 1.1184207201004028,
        "learning_rate": 4.556291991445432e-05,
        "epoch": 0.5641025641025641,
        "step": 2046
    },
    {
        "loss": 2.7018,
        "grad_norm": 0.8904258012771606,
        "learning_rate": 4.53436194189001e-05,
        "epoch": 0.5643782740556934,
        "step": 2047
    },
    {
        "loss": 1.7933,
        "grad_norm": 1.7170459032058716,
        "learning_rate": 4.512469311617351e-05,
        "epoch": 0.5646539840088227,
        "step": 2048
    },
    {
        "loss": 1.8457,
        "grad_norm": 1.798000454902649,
        "learning_rate": 4.490614250510507e-05,
        "epoch": 0.564929693961952,
        "step": 2049
    },
    {
        "loss": 2.2036,
        "grad_norm": 1.6498781442642212,
        "learning_rate": 4.468796908195315e-05,
        "epoch": 0.5652054039150813,
        "step": 2050
    },
    {
        "loss": 1.7677,
        "grad_norm": 2.2541732788085938,
        "learning_rate": 4.4470174340393975e-05,
        "epoch": 0.5654811138682106,
        "step": 2051
    },
    {
        "loss": 1.932,
        "grad_norm": 1.7081506252288818,
        "learning_rate": 4.425275977151093e-05,
        "epoch": 0.56575682382134,
        "step": 2052
    },
    {
        "loss": 1.6046,
        "grad_norm": 2.1564300060272217,
        "learning_rate": 4.403572686378499e-05,
        "epoch": 0.5660325337744693,
        "step": 2053
    },
    {
        "loss": 1.7156,
        "grad_norm": 2.071462631225586,
        "learning_rate": 4.381907710308395e-05,
        "epoch": 0.5663082437275986,
        "step": 2054
    },
    {
        "loss": 2.5171,
        "grad_norm": 1.4779386520385742,
        "learning_rate": 4.360281197265246e-05,
        "epoch": 0.5665839536807279,
        "step": 2055
    },
    {
        "loss": 1.624,
        "grad_norm": 1.532818078994751,
        "learning_rate": 4.3386932953101886e-05,
        "epoch": 0.5668596636338572,
        "step": 2056
    },
    {
        "loss": 2.3185,
        "grad_norm": 1.7407095432281494,
        "learning_rate": 4.3171441522400316e-05,
        "epoch": 0.5671353735869865,
        "step": 2057
    },
    {
        "loss": 2.3942,
        "grad_norm": 0.9866055250167847,
        "learning_rate": 4.295633915586213e-05,
        "epoch": 0.5674110835401158,
        "step": 2058
    },
    {
        "loss": 2.3699,
        "grad_norm": 1.3274348974227905,
        "learning_rate": 4.2741627326138135e-05,
        "epoch": 0.5676867934932451,
        "step": 2059
    },
    {
        "loss": 2.3068,
        "grad_norm": 1.8754303455352783,
        "learning_rate": 4.252730750320538e-05,
        "epoch": 0.5679625034463744,
        "step": 2060
    },
    {
        "loss": 2.2791,
        "grad_norm": 1.1312850713729858,
        "learning_rate": 4.2313381154357235e-05,
        "epoch": 0.5682382133995038,
        "step": 2061
    },
    {
        "loss": 2.1002,
        "grad_norm": 1.2174971103668213,
        "learning_rate": 4.209984974419312e-05,
        "epoch": 0.5685139233526331,
        "step": 2062
    },
    {
        "loss": 2.0509,
        "grad_norm": 2.2226061820983887,
        "learning_rate": 4.188671473460858e-05,
        "epoch": 0.5687896333057624,
        "step": 2063
    },
    {
        "loss": 1.2758,
        "grad_norm": 3.880786895751953,
        "learning_rate": 4.1673977584785426e-05,
        "epoch": 0.5690653432588917,
        "step": 2064
    },
    {
        "loss": 2.6108,
        "grad_norm": 1.0849204063415527,
        "learning_rate": 4.14616397511815e-05,
        "epoch": 0.569341053212021,
        "step": 2065
    },
    {
        "loss": 1.8418,
        "grad_norm": 1.3303296566009521,
        "learning_rate": 4.12497026875208e-05,
        "epoch": 0.5696167631651503,
        "step": 2066
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.706510305404663,
        "learning_rate": 4.103816784478365e-05,
        "epoch": 0.5698924731182796,
        "step": 2067
    },
    {
        "loss": 2.4462,
        "grad_norm": 1.3223967552185059,
        "learning_rate": 4.0827036671196564e-05,
        "epoch": 0.5701681830714089,
        "step": 2068
    },
    {
        "loss": 1.3647,
        "grad_norm": 2.5780367851257324,
        "learning_rate": 4.061631061222243e-05,
        "epoch": 0.5704438930245382,
        "step": 2069
    },
    {
        "loss": 2.2879,
        "grad_norm": 1.3138943910598755,
        "learning_rate": 4.0405991110550615e-05,
        "epoch": 0.5707196029776674,
        "step": 2070
    },
    {
        "loss": 1.8974,
        "grad_norm": 1.2511186599731445,
        "learning_rate": 4.019607960608716e-05,
        "epoch": 0.5709953129307968,
        "step": 2071
    },
    {
        "loss": 1.9739,
        "grad_norm": 1.2734850645065308,
        "learning_rate": 3.9986577535944734e-05,
        "epoch": 0.5712710228839261,
        "step": 2072
    },
    {
        "loss": 2.2513,
        "grad_norm": 1.5819257497787476,
        "learning_rate": 3.9777486334432926e-05,
        "epoch": 0.5715467328370554,
        "step": 2073
    },
    {
        "loss": 1.5506,
        "grad_norm": 1.9766958951950073,
        "learning_rate": 3.956880743304848e-05,
        "epoch": 0.5718224427901847,
        "step": 2074
    },
    {
        "loss": 1.9183,
        "grad_norm": 1.6432386636734009,
        "learning_rate": 3.936054226046535e-05,
        "epoch": 0.572098152743314,
        "step": 2075
    },
    {
        "loss": 2.3075,
        "grad_norm": 1.3404330015182495,
        "learning_rate": 3.915269224252499e-05,
        "epoch": 0.5723738626964433,
        "step": 2076
    },
    {
        "loss": 2.1534,
        "grad_norm": 1.97923743724823,
        "learning_rate": 3.894525880222657e-05,
        "epoch": 0.5726495726495726,
        "step": 2077
    },
    {
        "loss": 1.9735,
        "grad_norm": 1.4411842823028564,
        "learning_rate": 3.873824335971736e-05,
        "epoch": 0.5729252826027019,
        "step": 2078
    },
    {
        "loss": 2.2001,
        "grad_norm": 1.2224425077438354,
        "learning_rate": 3.853164733228278e-05,
        "epoch": 0.5732009925558312,
        "step": 2079
    },
    {
        "loss": 2.0748,
        "grad_norm": 1.8685165643692017,
        "learning_rate": 3.8325472134336824e-05,
        "epoch": 0.5734767025089605,
        "step": 2080
    },
    {
        "loss": 1.8662,
        "grad_norm": 1.730147123336792,
        "learning_rate": 3.8119719177412507e-05,
        "epoch": 0.5737524124620899,
        "step": 2081
    },
    {
        "loss": 2.1885,
        "grad_norm": 1.2789342403411865,
        "learning_rate": 3.79143898701519e-05,
        "epoch": 0.5740281224152192,
        "step": 2082
    },
    {
        "loss": 2.0096,
        "grad_norm": 1.5624316930770874,
        "learning_rate": 3.770948561829669e-05,
        "epoch": 0.5743038323683485,
        "step": 2083
    },
    {
        "loss": 2.2905,
        "grad_norm": 1.6281732320785522,
        "learning_rate": 3.7505007824678616e-05,
        "epoch": 0.5745795423214778,
        "step": 2084
    },
    {
        "loss": 1.4795,
        "grad_norm": 2.375073194503784,
        "learning_rate": 3.7300957889209655e-05,
        "epoch": 0.5748552522746071,
        "step": 2085
    },
    {
        "loss": 2.0139,
        "grad_norm": 1.4476593732833862,
        "learning_rate": 3.709733720887259e-05,
        "epoch": 0.5751309622277364,
        "step": 2086
    },
    {
        "loss": 2.1291,
        "grad_norm": 0.8622949719429016,
        "learning_rate": 3.6894147177711346e-05,
        "epoch": 0.5754066721808657,
        "step": 2087
    },
    {
        "loss": 2.3194,
        "grad_norm": 1.310330867767334,
        "learning_rate": 3.6691389186821643e-05,
        "epoch": 0.575682382133995,
        "step": 2088
    },
    {
        "loss": 2.2684,
        "grad_norm": 1.862877607345581,
        "learning_rate": 3.648906462434119e-05,
        "epoch": 0.5759580920871243,
        "step": 2089
    },
    {
        "loss": 1.6178,
        "grad_norm": 1.6944544315338135,
        "learning_rate": 3.6287174875440375e-05,
        "epoch": 0.5762338020402537,
        "step": 2090
    },
    {
        "loss": 2.5076,
        "grad_norm": 1.1756722927093506,
        "learning_rate": 3.6085721322312784e-05,
        "epoch": 0.576509511993383,
        "step": 2091
    },
    {
        "loss": 1.5719,
        "grad_norm": 1.476621389389038,
        "learning_rate": 3.5884705344165606e-05,
        "epoch": 0.5767852219465123,
        "step": 2092
    },
    {
        "loss": 2.1827,
        "grad_norm": 1.4327715635299683,
        "learning_rate": 3.568412831721025e-05,
        "epoch": 0.5770609318996416,
        "step": 2093
    },
    {
        "loss": 1.99,
        "grad_norm": 2.2848927974700928,
        "learning_rate": 3.548399161465309e-05,
        "epoch": 0.5773366418527709,
        "step": 2094
    },
    {
        "loss": 2.1509,
        "grad_norm": 1.8585731983184814,
        "learning_rate": 3.528429660668575e-05,
        "epoch": 0.5776123518059002,
        "step": 2095
    },
    {
        "loss": 1.9302,
        "grad_norm": 2.0073013305664062,
        "learning_rate": 3.5085044660475995e-05,
        "epoch": 0.5778880617590295,
        "step": 2096
    },
    {
        "loss": 1.8524,
        "grad_norm": 1.4058884382247925,
        "learning_rate": 3.4886237140158175e-05,
        "epoch": 0.5781637717121588,
        "step": 2097
    },
    {
        "loss": 2.4078,
        "grad_norm": 1.24027681350708,
        "learning_rate": 3.46878754068241e-05,
        "epoch": 0.5784394816652881,
        "step": 2098
    },
    {
        "loss": 2.5497,
        "grad_norm": 1.9681800603866577,
        "learning_rate": 3.44899608185135e-05,
        "epoch": 0.5787151916184174,
        "step": 2099
    },
    {
        "loss": 2.0589,
        "grad_norm": 1.1626214981079102,
        "learning_rate": 3.4292494730204804e-05,
        "epoch": 0.5789909015715468,
        "step": 2100
    },
    {
        "loss": 1.6721,
        "grad_norm": 1.8982399702072144,
        "learning_rate": 3.4095478493806024e-05,
        "epoch": 0.5792666115246761,
        "step": 2101
    },
    {
        "loss": 1.432,
        "grad_norm": 2.4785101413726807,
        "learning_rate": 3.389891345814523e-05,
        "epoch": 0.5795423214778054,
        "step": 2102
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.4289360046386719,
        "learning_rate": 3.370280096896145e-05,
        "epoch": 0.5798180314309347,
        "step": 2103
    },
    {
        "loss": 1.6306,
        "grad_norm": 1.638845682144165,
        "learning_rate": 3.350714236889555e-05,
        "epoch": 0.580093741384064,
        "step": 2104
    },
    {
        "loss": 2.2606,
        "grad_norm": 1.3577886819839478,
        "learning_rate": 3.331193899748087e-05,
        "epoch": 0.5803694513371933,
        "step": 2105
    },
    {
        "loss": 2.2392,
        "grad_norm": 1.889491319656372,
        "learning_rate": 3.311719219113413e-05,
        "epoch": 0.5806451612903226,
        "step": 2106
    },
    {
        "loss": 2.1892,
        "grad_norm": 1.4206947088241577,
        "learning_rate": 3.2922903283146246e-05,
        "epoch": 0.5809208712434519,
        "step": 2107
    },
    {
        "loss": 1.4213,
        "grad_norm": 1.8688613176345825,
        "learning_rate": 3.2729073603673366e-05,
        "epoch": 0.5811965811965812,
        "step": 2108
    },
    {
        "loss": 1.9779,
        "grad_norm": 1.5472652912139893,
        "learning_rate": 3.253570447972749e-05,
        "epoch": 0.5814722911497106,
        "step": 2109
    },
    {
        "loss": 2.619,
        "grad_norm": 1.5881683826446533,
        "learning_rate": 3.234279723516757e-05,
        "epoch": 0.5817480011028399,
        "step": 2110
    },
    {
        "loss": 2.2315,
        "grad_norm": 1.3483712673187256,
        "learning_rate": 3.2150353190690455e-05,
        "epoch": 0.5820237110559692,
        "step": 2111
    },
    {
        "loss": 2.4366,
        "grad_norm": 1.7047785520553589,
        "learning_rate": 3.195837366382174e-05,
        "epoch": 0.5822994210090985,
        "step": 2112
    },
    {
        "loss": 2.0894,
        "grad_norm": 3.103299856185913,
        "learning_rate": 3.17668599689068e-05,
        "epoch": 0.5825751309622277,
        "step": 2113
    },
    {
        "loss": 1.7829,
        "grad_norm": 1.7198774814605713,
        "learning_rate": 3.15758134171018e-05,
        "epoch": 0.582850840915357,
        "step": 2114
    },
    {
        "loss": 2.4612,
        "grad_norm": 1.2857332229614258,
        "learning_rate": 3.138523531636477e-05,
        "epoch": 0.5831265508684863,
        "step": 2115
    },
    {
        "loss": 0.7806,
        "grad_norm": 1.6941108703613281,
        "learning_rate": 3.119512697144654e-05,
        "epoch": 0.5834022608216156,
        "step": 2116
    },
    {
        "loss": 1.6742,
        "grad_norm": 1.2427897453308105,
        "learning_rate": 3.1005489683881814e-05,
        "epoch": 0.5836779707747449,
        "step": 2117
    },
    {
        "loss": 2.4264,
        "grad_norm": 1.3203881978988647,
        "learning_rate": 3.081632475198043e-05,
        "epoch": 0.5839536807278742,
        "step": 2118
    },
    {
        "loss": 2.0308,
        "grad_norm": 1.701786756515503,
        "learning_rate": 3.0627633470818264e-05,
        "epoch": 0.5842293906810035,
        "step": 2119
    },
    {
        "loss": 2.0197,
        "grad_norm": 1.24508535861969,
        "learning_rate": 3.0439417132228408e-05,
        "epoch": 0.5845051006341329,
        "step": 2120
    },
    {
        "loss": 2.0149,
        "grad_norm": 1.7458672523498535,
        "learning_rate": 3.025167702479249e-05,
        "epoch": 0.5847808105872622,
        "step": 2121
    },
    {
        "loss": 1.9788,
        "grad_norm": 1.6935545206069946,
        "learning_rate": 3.0064414433831622e-05,
        "epoch": 0.5850565205403915,
        "step": 2122
    },
    {
        "loss": 2.229,
        "grad_norm": 1.6375806331634521,
        "learning_rate": 2.9877630641397748e-05,
        "epoch": 0.5853322304935208,
        "step": 2123
    },
    {
        "loss": 1.9004,
        "grad_norm": 1.60674250125885,
        "learning_rate": 2.969132692626476e-05,
        "epoch": 0.5856079404466501,
        "step": 2124
    },
    {
        "loss": 2.3428,
        "grad_norm": 1.5120735168457031,
        "learning_rate": 2.9505504563919938e-05,
        "epoch": 0.5858836503997794,
        "step": 2125
    },
    {
        "loss": 2.0956,
        "grad_norm": 1.9929345846176147,
        "learning_rate": 2.9320164826554986e-05,
        "epoch": 0.5861593603529087,
        "step": 2126
    },
    {
        "loss": 1.042,
        "grad_norm": 2.711106777191162,
        "learning_rate": 2.9135308983057396e-05,
        "epoch": 0.586435070306038,
        "step": 2127
    },
    {
        "loss": 2.5234,
        "grad_norm": 1.1575371026992798,
        "learning_rate": 2.8950938299001907e-05,
        "epoch": 0.5867107802591673,
        "step": 2128
    },
    {
        "loss": 2.4421,
        "grad_norm": 0.831794023513794,
        "learning_rate": 2.8767054036641627e-05,
        "epoch": 0.5869864902122967,
        "step": 2129
    },
    {
        "loss": 2.1556,
        "grad_norm": 1.127009630203247,
        "learning_rate": 2.858365745489946e-05,
        "epoch": 0.587262200165426,
        "step": 2130
    },
    {
        "loss": 2.4352,
        "grad_norm": 1.7872463464736938,
        "learning_rate": 2.840074980935965e-05,
        "epoch": 0.5875379101185553,
        "step": 2131
    },
    {
        "loss": 1.8286,
        "grad_norm": 2.211669683456421,
        "learning_rate": 2.8218332352258903e-05,
        "epoch": 0.5878136200716846,
        "step": 2132
    },
    {
        "loss": 2.473,
        "grad_norm": 1.4646666049957275,
        "learning_rate": 2.8036406332478038e-05,
        "epoch": 0.5880893300248139,
        "step": 2133
    },
    {
        "loss": 1.9417,
        "grad_norm": 1.820399284362793,
        "learning_rate": 2.785497299553328e-05,
        "epoch": 0.5883650399779432,
        "step": 2134
    },
    {
        "loss": 2.1764,
        "grad_norm": 1.8509622812271118,
        "learning_rate": 2.767403358356795e-05,
        "epoch": 0.5886407499310725,
        "step": 2135
    },
    {
        "loss": 1.5483,
        "grad_norm": 2.153352975845337,
        "learning_rate": 2.7493589335343685e-05,
        "epoch": 0.5889164598842018,
        "step": 2136
    },
    {
        "loss": 2.3167,
        "grad_norm": 1.5767185688018799,
        "learning_rate": 2.7313641486232112e-05,
        "epoch": 0.5891921698373311,
        "step": 2137
    },
    {
        "loss": 2.066,
        "grad_norm": 1.906043529510498,
        "learning_rate": 2.7134191268206478e-05,
        "epoch": 0.5894678797904604,
        "step": 2138
    },
    {
        "loss": 2.132,
        "grad_norm": 1.289291501045227,
        "learning_rate": 2.6955239909832985e-05,
        "epoch": 0.5897435897435898,
        "step": 2139
    },
    {
        "loss": 2.1619,
        "grad_norm": 1.6184594631195068,
        "learning_rate": 2.677678863626253e-05,
        "epoch": 0.5900192996967191,
        "step": 2140
    },
    {
        "loss": 2.2123,
        "grad_norm": 1.153571605682373,
        "learning_rate": 2.659883866922237e-05,
        "epoch": 0.5902950096498484,
        "step": 2141
    },
    {
        "loss": 1.5104,
        "grad_norm": 2.229929208755493,
        "learning_rate": 2.6421391227007587e-05,
        "epoch": 0.5905707196029777,
        "step": 2142
    },
    {
        "loss": 1.8097,
        "grad_norm": 1.795030951499939,
        "learning_rate": 2.6244447524472858e-05,
        "epoch": 0.590846429556107,
        "step": 2143
    },
    {
        "loss": 2.0842,
        "grad_norm": 2.3456077575683594,
        "learning_rate": 2.606800877302411e-05,
        "epoch": 0.5911221395092363,
        "step": 2144
    },
    {
        "loss": 2.1414,
        "grad_norm": 1.6415036916732788,
        "learning_rate": 2.5892076180610315e-05,
        "epoch": 0.5913978494623656,
        "step": 2145
    },
    {
        "loss": 2.153,
        "grad_norm": 1.5033268928527832,
        "learning_rate": 2.571665095171504e-05,
        "epoch": 0.5916735594154949,
        "step": 2146
    },
    {
        "loss": 1.9134,
        "grad_norm": 1.5184005498886108,
        "learning_rate": 2.5541734287348297e-05,
        "epoch": 0.5919492693686242,
        "step": 2147
    },
    {
        "loss": 1.9502,
        "grad_norm": 1.618367314338684,
        "learning_rate": 2.536732738503843e-05,
        "epoch": 0.5922249793217536,
        "step": 2148
    },
    {
        "loss": 2.2973,
        "grad_norm": 1.0598299503326416,
        "learning_rate": 2.519343143882372e-05,
        "epoch": 0.5925006892748829,
        "step": 2149
    },
    {
        "loss": 2.5092,
        "grad_norm": 1.240177035331726,
        "learning_rate": 2.5020047639244327e-05,
        "epoch": 0.5927763992280122,
        "step": 2150
    },
    {
        "loss": 2.162,
        "grad_norm": 1.5443487167358398,
        "learning_rate": 2.4847177173334046e-05,
        "epoch": 0.5930521091811415,
        "step": 2151
    },
    {
        "loss": 2.6867,
        "grad_norm": 0.8285799622535706,
        "learning_rate": 2.46748212246124e-05,
        "epoch": 0.5933278191342708,
        "step": 2152
    },
    {
        "loss": 2.036,
        "grad_norm": 1.0577112436294556,
        "learning_rate": 2.4502980973076252e-05,
        "epoch": 0.5936035290874001,
        "step": 2153
    },
    {
        "loss": 2.2714,
        "grad_norm": 2.142214775085449,
        "learning_rate": 2.433165759519186e-05,
        "epoch": 0.5938792390405294,
        "step": 2154
    },
    {
        "loss": 2.3597,
        "grad_norm": 1.432275652885437,
        "learning_rate": 2.4160852263886934e-05,
        "epoch": 0.5941549489936586,
        "step": 2155
    },
    {
        "loss": 2.21,
        "grad_norm": 2.007951021194458,
        "learning_rate": 2.399056614854237e-05,
        "epoch": 0.5944306589467879,
        "step": 2156
    },
    {
        "loss": 2.3982,
        "grad_norm": 1.0855294466018677,
        "learning_rate": 2.38208004149844e-05,
        "epoch": 0.5947063688999172,
        "step": 2157
    },
    {
        "loss": 1.9965,
        "grad_norm": 1.602555751800537,
        "learning_rate": 2.3651556225476635e-05,
        "epoch": 0.5949820788530465,
        "step": 2158
    },
    {
        "loss": 1.9367,
        "grad_norm": 1.8617452383041382,
        "learning_rate": 2.348283473871199e-05,
        "epoch": 0.5952577888061759,
        "step": 2159
    },
    {
        "loss": 2.2659,
        "grad_norm": 1.030321717262268,
        "learning_rate": 2.3314637109804828e-05,
        "epoch": 0.5955334987593052,
        "step": 2160
    },
    {
        "loss": 2.4648,
        "grad_norm": 1.6549789905548096,
        "learning_rate": 2.3146964490282997e-05,
        "epoch": 0.5958092087124345,
        "step": 2161
    },
    {
        "loss": 1.9214,
        "grad_norm": 1.985824704170227,
        "learning_rate": 2.297981802808008e-05,
        "epoch": 0.5960849186655638,
        "step": 2162
    },
    {
        "loss": 2.488,
        "grad_norm": 1.529272437095642,
        "learning_rate": 2.281319886752735e-05,
        "epoch": 0.5963606286186931,
        "step": 2163
    },
    {
        "loss": 2.2968,
        "grad_norm": 1.4106553792953491,
        "learning_rate": 2.264710814934604e-05,
        "epoch": 0.5966363385718224,
        "step": 2164
    },
    {
        "loss": 1.9738,
        "grad_norm": 1.1871140003204346,
        "learning_rate": 2.248154701063959e-05,
        "epoch": 0.5969120485249517,
        "step": 2165
    },
    {
        "loss": 1.6386,
        "grad_norm": 1.5301940441131592,
        "learning_rate": 2.2316516584885716e-05,
        "epoch": 0.597187758478081,
        "step": 2166
    },
    {
        "loss": 1.9421,
        "grad_norm": 1.322974443435669,
        "learning_rate": 2.215201800192871e-05,
        "epoch": 0.5974634684312103,
        "step": 2167
    },
    {
        "loss": 1.826,
        "grad_norm": 1.8035204410552979,
        "learning_rate": 2.1988052387971802e-05,
        "epoch": 0.5977391783843397,
        "step": 2168
    },
    {
        "loss": 2.3856,
        "grad_norm": 1.1161531209945679,
        "learning_rate": 2.1824620865569324e-05,
        "epoch": 0.598014888337469,
        "step": 2169
    },
    {
        "loss": 2.4271,
        "grad_norm": 1.016120195388794,
        "learning_rate": 2.1661724553619035e-05,
        "epoch": 0.5982905982905983,
        "step": 2170
    },
    {
        "loss": 2.0212,
        "grad_norm": 1.7784405946731567,
        "learning_rate": 2.1499364567354517e-05,
        "epoch": 0.5985663082437276,
        "step": 2171
    },
    {
        "loss": 2.2628,
        "grad_norm": 1.166542649269104,
        "learning_rate": 2.1337542018337588e-05,
        "epoch": 0.5988420181968569,
        "step": 2172
    },
    {
        "loss": 2.0141,
        "grad_norm": 1.8985704183578491,
        "learning_rate": 2.1176258014450512e-05,
        "epoch": 0.5991177281499862,
        "step": 2173
    },
    {
        "loss": 2.0846,
        "grad_norm": 1.239439606666565,
        "learning_rate": 2.101551365988854e-05,
        "epoch": 0.5993934381031155,
        "step": 2174
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.6926662921905518,
        "learning_rate": 2.0855310055152444e-05,
        "epoch": 0.5996691480562448,
        "step": 2175
    },
    {
        "loss": 1.5822,
        "grad_norm": 2.0874223709106445,
        "learning_rate": 2.0695648297040736e-05,
        "epoch": 0.5999448580093741,
        "step": 2176
    },
    {
        "loss": 1.4324,
        "grad_norm": 1.1899151802062988,
        "learning_rate": 2.053652947864233e-05,
        "epoch": 0.6002205679625034,
        "step": 2177
    },
    {
        "loss": 2.2201,
        "grad_norm": 1.5425200462341309,
        "learning_rate": 2.037795468932909e-05,
        "epoch": 0.6004962779156328,
        "step": 2178
    },
    {
        "loss": 2.2127,
        "grad_norm": 1.2941182851791382,
        "learning_rate": 2.0219925014748254e-05,
        "epoch": 0.6007719878687621,
        "step": 2179
    },
    {
        "loss": 2.3066,
        "grad_norm": 1.9749987125396729,
        "learning_rate": 2.0062441536815023e-05,
        "epoch": 0.6010476978218914,
        "step": 2180
    },
    {
        "loss": 2.7011,
        "grad_norm": 1.6230666637420654,
        "learning_rate": 1.9905505333705223e-05,
        "epoch": 0.6013234077750207,
        "step": 2181
    },
    {
        "loss": 2.2408,
        "grad_norm": 1.261544942855835,
        "learning_rate": 1.9749117479847902e-05,
        "epoch": 0.60159911772815,
        "step": 2182
    },
    {
        "loss": 2.2611,
        "grad_norm": 1.3855781555175781,
        "learning_rate": 1.959327904591791e-05,
        "epoch": 0.6018748276812793,
        "step": 2183
    },
    {
        "loss": 2.3064,
        "grad_norm": 1.0328080654144287,
        "learning_rate": 1.9437991098828624e-05,
        "epoch": 0.6021505376344086,
        "step": 2184
    },
    {
        "loss": 2.3671,
        "grad_norm": 1.168103814125061,
        "learning_rate": 1.9283254701724696e-05,
        "epoch": 0.6024262475875379,
        "step": 2185
    },
    {
        "loss": 2.343,
        "grad_norm": 2.2014193534851074,
        "learning_rate": 1.9129070913974623e-05,
        "epoch": 0.6027019575406672,
        "step": 2186
    },
    {
        "loss": 2.2115,
        "grad_norm": 1.2711914777755737,
        "learning_rate": 1.8975440791163634e-05,
        "epoch": 0.6029776674937966,
        "step": 2187
    },
    {
        "loss": 2.4449,
        "grad_norm": 1.1889574527740479,
        "learning_rate": 1.882236538508638e-05,
        "epoch": 0.6032533774469259,
        "step": 2188
    },
    {
        "loss": 2.1572,
        "grad_norm": 1.2578765153884888,
        "learning_rate": 1.8669845743739846e-05,
        "epoch": 0.6035290874000552,
        "step": 2189
    },
    {
        "loss": 1.8885,
        "grad_norm": 1.5146483182907104,
        "learning_rate": 1.8517882911315997e-05,
        "epoch": 0.6038047973531845,
        "step": 2190
    },
    {
        "loss": 2.2229,
        "grad_norm": 1.277789831161499,
        "learning_rate": 1.8366477928194768e-05,
        "epoch": 0.6040805073063138,
        "step": 2191
    },
    {
        "loss": 1.8785,
        "grad_norm": 2.076502799987793,
        "learning_rate": 1.8215631830936953e-05,
        "epoch": 0.6043562172594431,
        "step": 2192
    },
    {
        "loss": 1.6783,
        "grad_norm": 1.606791377067566,
        "learning_rate": 1.8065345652276977e-05,
        "epoch": 0.6046319272125724,
        "step": 2193
    },
    {
        "loss": 2.0423,
        "grad_norm": 1.358349084854126,
        "learning_rate": 1.791562042111592e-05,
        "epoch": 0.6049076371657017,
        "step": 2194
    },
    {
        "loss": 1.6939,
        "grad_norm": 1.5552977323532104,
        "learning_rate": 1.776645716251454e-05,
        "epoch": 0.605183347118831,
        "step": 2195
    },
    {
        "loss": 1.355,
        "grad_norm": 1.2349929809570312,
        "learning_rate": 1.761785689768608e-05,
        "epoch": 0.6054590570719603,
        "step": 2196
    },
    {
        "loss": 2.0188,
        "grad_norm": 1.0754767656326294,
        "learning_rate": 1.74698206439894e-05,
        "epoch": 0.6057347670250897,
        "step": 2197
    },
    {
        "loss": 2.376,
        "grad_norm": 1.610971212387085,
        "learning_rate": 1.7322349414921968e-05,
        "epoch": 0.6060104769782189,
        "step": 2198
    },
    {
        "loss": 1.2457,
        "grad_norm": 2.0232231616973877,
        "learning_rate": 1.7175444220113013e-05,
        "epoch": 0.6062861869313482,
        "step": 2199
    },
    {
        "loss": 1.6977,
        "grad_norm": 1.85405433177948,
        "learning_rate": 1.7029106065316434e-05,
        "epoch": 0.6065618968844775,
        "step": 2200
    },
    {
        "loss": 2.2513,
        "grad_norm": 1.663637399673462,
        "learning_rate": 1.6883335952404046e-05,
        "epoch": 0.6068376068376068,
        "step": 2201
    },
    {
        "loss": 2.6527,
        "grad_norm": 1.1923025846481323,
        "learning_rate": 1.6738134879358748e-05,
        "epoch": 0.6071133167907361,
        "step": 2202
    },
    {
        "loss": 1.4355,
        "grad_norm": 2.0267250537872314,
        "learning_rate": 1.6593503840267564e-05,
        "epoch": 0.6073890267438654,
        "step": 2203
    },
    {
        "loss": 2.0603,
        "grad_norm": 1.5803102254867554,
        "learning_rate": 1.6449443825314914e-05,
        "epoch": 0.6076647366969947,
        "step": 2204
    },
    {
        "loss": 1.9975,
        "grad_norm": 1.2737538814544678,
        "learning_rate": 1.6305955820775888e-05,
        "epoch": 0.607940446650124,
        "step": 2205
    },
    {
        "loss": 2.1292,
        "grad_norm": 1.0296094417572021,
        "learning_rate": 1.6163040809009368e-05,
        "epoch": 0.6082161566032533,
        "step": 2206
    },
    {
        "loss": 2.1215,
        "grad_norm": 1.8090687990188599,
        "learning_rate": 1.6020699768451398e-05,
        "epoch": 0.6084918665563827,
        "step": 2207
    },
    {
        "loss": 2.1065,
        "grad_norm": 0.9588862061500549,
        "learning_rate": 1.5878933673608408e-05,
        "epoch": 0.608767576509512,
        "step": 2208
    },
    {
        "loss": 1.5072,
        "grad_norm": 2.023606538772583,
        "learning_rate": 1.5737743495050684e-05,
        "epoch": 0.6090432864626413,
        "step": 2209
    },
    {
        "loss": 1.7573,
        "grad_norm": 1.8356432914733887,
        "learning_rate": 1.559713019940555e-05,
        "epoch": 0.6093189964157706,
        "step": 2210
    },
    {
        "loss": 1.7734,
        "grad_norm": 2.411618709564209,
        "learning_rate": 1.545709474935082e-05,
        "epoch": 0.6095947063688999,
        "step": 2211
    },
    {
        "loss": 1.9684,
        "grad_norm": 1.594302773475647,
        "learning_rate": 1.5317638103608312e-05,
        "epoch": 0.6098704163220292,
        "step": 2212
    },
    {
        "loss": 1.7159,
        "grad_norm": 1.2476539611816406,
        "learning_rate": 1.5178761216937077e-05,
        "epoch": 0.6101461262751585,
        "step": 2213
    },
    {
        "loss": 2.1818,
        "grad_norm": 2.24623966217041,
        "learning_rate": 1.5040465040127016e-05,
        "epoch": 0.6104218362282878,
        "step": 2214
    },
    {
        "loss": 1.6841,
        "grad_norm": 2.438046455383301,
        "learning_rate": 1.490275051999237e-05,
        "epoch": 0.6106975461814171,
        "step": 2215
    },
    {
        "loss": 2.4495,
        "grad_norm": 1.2591136693954468,
        "learning_rate": 1.4765618599365139e-05,
        "epoch": 0.6109732561345464,
        "step": 2216
    },
    {
        "loss": 1.7125,
        "grad_norm": 1.4296728372573853,
        "learning_rate": 1.4629070217088691e-05,
        "epoch": 0.6112489660876758,
        "step": 2217
    },
    {
        "loss": 2.5445,
        "grad_norm": 1.5268187522888184,
        "learning_rate": 1.4493106308011339e-05,
        "epoch": 0.6115246760408051,
        "step": 2218
    },
    {
        "loss": 2.4679,
        "grad_norm": 1.2553216218948364,
        "learning_rate": 1.4357727802979958e-05,
        "epoch": 0.6118003859939344,
        "step": 2219
    },
    {
        "loss": 1.9652,
        "grad_norm": 1.5938442945480347,
        "learning_rate": 1.4222935628833545e-05,
        "epoch": 0.6120760959470637,
        "step": 2220
    },
    {
        "loss": 2.2838,
        "grad_norm": 1.689076542854309,
        "learning_rate": 1.4088730708396903e-05,
        "epoch": 0.612351805900193,
        "step": 2221
    },
    {
        "loss": 2.0557,
        "grad_norm": 1.7406991720199585,
        "learning_rate": 1.3955113960474398e-05,
        "epoch": 0.6126275158533223,
        "step": 2222
    },
    {
        "loss": 1.9802,
        "grad_norm": 1.786283254623413,
        "learning_rate": 1.3822086299843517e-05,
        "epoch": 0.6129032258064516,
        "step": 2223
    },
    {
        "loss": 1.6131,
        "grad_norm": 1.3144659996032715,
        "learning_rate": 1.3689648637248753e-05,
        "epoch": 0.6131789357595809,
        "step": 2224
    },
    {
        "loss": 2.0553,
        "grad_norm": 1.3512346744537354,
        "learning_rate": 1.355780187939526e-05,
        "epoch": 0.6134546457127102,
        "step": 2225
    },
    {
        "loss": 2.1735,
        "grad_norm": 2.1489639282226562,
        "learning_rate": 1.3426546928942763e-05,
        "epoch": 0.6137303556658396,
        "step": 2226
    },
    {
        "loss": 1.6508,
        "grad_norm": 2.0789873600006104,
        "learning_rate": 1.3295884684499238e-05,
        "epoch": 0.6140060656189689,
        "step": 2227
    },
    {
        "loss": 2.5546,
        "grad_norm": 1.0033599138259888,
        "learning_rate": 1.3165816040614843e-05,
        "epoch": 0.6142817755720982,
        "step": 2228
    },
    {
        "loss": 2.0304,
        "grad_norm": 1.277281641960144,
        "learning_rate": 1.3036341887775838e-05,
        "epoch": 0.6145574855252275,
        "step": 2229
    },
    {
        "loss": 1.9494,
        "grad_norm": 1.8969625234603882,
        "learning_rate": 1.2907463112398365e-05,
        "epoch": 0.6148331954783568,
        "step": 2230
    },
    {
        "loss": 1.9284,
        "grad_norm": 1.8733210563659668,
        "learning_rate": 1.277918059682246e-05,
        "epoch": 0.6151089054314861,
        "step": 2231
    },
    {
        "loss": 2.3941,
        "grad_norm": 1.6055561304092407,
        "learning_rate": 1.2651495219306031e-05,
        "epoch": 0.6153846153846154,
        "step": 2232
    },
    {
        "loss": 1.9748,
        "grad_norm": 1.3738375902175903,
        "learning_rate": 1.2524407854018793e-05,
        "epoch": 0.6156603253377447,
        "step": 2233
    },
    {
        "loss": 2.181,
        "grad_norm": 1.968943476676941,
        "learning_rate": 1.2397919371036271e-05,
        "epoch": 0.615936035290874,
        "step": 2234
    },
    {
        "loss": 1.8247,
        "grad_norm": 1.4623377323150635,
        "learning_rate": 1.2272030636333909e-05,
        "epoch": 0.6162117452440034,
        "step": 2235
    },
    {
        "loss": 2.5115,
        "grad_norm": 1.093714952468872,
        "learning_rate": 1.2146742511781129e-05,
        "epoch": 0.6164874551971327,
        "step": 2236
    },
    {
        "loss": 2.0766,
        "grad_norm": 1.7152609825134277,
        "learning_rate": 1.2022055855135361e-05,
        "epoch": 0.616763165150262,
        "step": 2237
    },
    {
        "loss": 2.1226,
        "grad_norm": 1.880224347114563,
        "learning_rate": 1.1897971520036221e-05,
        "epoch": 0.6170388751033913,
        "step": 2238
    },
    {
        "loss": 2.4119,
        "grad_norm": 1.4245961904525757,
        "learning_rate": 1.1774490355999713e-05,
        "epoch": 0.6173145850565206,
        "step": 2239
    },
    {
        "loss": 1.7699,
        "grad_norm": 2.192437171936035,
        "learning_rate": 1.1651613208412315e-05,
        "epoch": 0.6175902950096499,
        "step": 2240
    },
    {
        "loss": 2.245,
        "grad_norm": 1.5231980085372925,
        "learning_rate": 1.1529340918525223e-05,
        "epoch": 0.6178660049627791,
        "step": 2241
    },
    {
        "loss": 2.6096,
        "grad_norm": 1.243227243423462,
        "learning_rate": 1.1407674323448702e-05,
        "epoch": 0.6181417149159084,
        "step": 2242
    },
    {
        "loss": 2.1622,
        "grad_norm": 1.3999534845352173,
        "learning_rate": 1.1286614256146178e-05,
        "epoch": 0.6184174248690377,
        "step": 2243
    },
    {
        "loss": 2.0986,
        "grad_norm": 1.2026342153549194,
        "learning_rate": 1.1166161545428621e-05,
        "epoch": 0.618693134822167,
        "step": 2244
    },
    {
        "loss": 1.6207,
        "grad_norm": 1.8963606357574463,
        "learning_rate": 1.1046317015948881e-05,
        "epoch": 0.6189688447752963,
        "step": 2245
    },
    {
        "loss": 2.5499,
        "grad_norm": 1.3698278665542603,
        "learning_rate": 1.0927081488196078e-05,
        "epoch": 0.6192445547284257,
        "step": 2246
    },
    {
        "loss": 2.4758,
        "grad_norm": 1.2972939014434814,
        "learning_rate": 1.080845577848988e-05,
        "epoch": 0.619520264681555,
        "step": 2247
    },
    {
        "loss": 1.9105,
        "grad_norm": 1.7169365882873535,
        "learning_rate": 1.0690440698974957e-05,
        "epoch": 0.6197959746346843,
        "step": 2248
    },
    {
        "loss": 1.889,
        "grad_norm": 1.4218220710754395,
        "learning_rate": 1.0573037057615498e-05,
        "epoch": 0.6200716845878136,
        "step": 2249
    },
    {
        "loss": 1.9758,
        "grad_norm": 1.1945085525512695,
        "learning_rate": 1.0456245658189579e-05,
        "epoch": 0.6203473945409429,
        "step": 2250
    },
    {
        "loss": 1.4815,
        "grad_norm": 1.8900490999221802,
        "learning_rate": 1.0340067300283652e-05,
        "epoch": 0.6206231044940722,
        "step": 2251
    },
    {
        "loss": 2.0037,
        "grad_norm": 0.9878299832344055,
        "learning_rate": 1.0224502779287237e-05,
        "epoch": 0.6208988144472015,
        "step": 2252
    },
    {
        "loss": 1.5619,
        "grad_norm": 2.1823995113372803,
        "learning_rate": 1.010955288638723e-05,
        "epoch": 0.6211745244003308,
        "step": 2253
    },
    {
        "loss": 2.2109,
        "grad_norm": 1.374527096748352,
        "learning_rate": 9.99521840856268e-06,
        "epoch": 0.6214502343534601,
        "step": 2254
    },
    {
        "loss": 1.1872,
        "grad_norm": 1.3727951049804688,
        "learning_rate": 9.881500128579292e-06,
        "epoch": 0.6217259443065895,
        "step": 2255
    },
    {
        "loss": 1.6725,
        "grad_norm": 1.720494031906128,
        "learning_rate": 9.76839882498416e-06,
        "epoch": 0.6220016542597188,
        "step": 2256
    },
    {
        "loss": 2.5016,
        "grad_norm": 1.296792984008789,
        "learning_rate": 9.655915272100357e-06,
        "epoch": 0.6222773642128481,
        "step": 2257
    },
    {
        "loss": 1.9566,
        "grad_norm": 1.6840639114379883,
        "learning_rate": 9.54405024002164e-06,
        "epoch": 0.6225530741659774,
        "step": 2258
    },
    {
        "loss": 1.4964,
        "grad_norm": 1.637947678565979,
        "learning_rate": 9.432804494607239e-06,
        "epoch": 0.6228287841191067,
        "step": 2259
    },
    {
        "loss": 1.5779,
        "grad_norm": 1.7586265802383423,
        "learning_rate": 9.322178797476567e-06,
        "epoch": 0.623104494072236,
        "step": 2260
    },
    {
        "loss": 2.0827,
        "grad_norm": 1.2705705165863037,
        "learning_rate": 9.212173906003973e-06,
        "epoch": 0.6233802040253653,
        "step": 2261
    },
    {
        "loss": 2.0654,
        "grad_norm": 1.5532785654067993,
        "learning_rate": 9.102790573313669e-06,
        "epoch": 0.6236559139784946,
        "step": 2262
    },
    {
        "loss": 2.194,
        "grad_norm": 1.500044584274292,
        "learning_rate": 8.994029548274452e-06,
        "epoch": 0.6239316239316239,
        "step": 2263
    },
    {
        "loss": 2.2875,
        "grad_norm": 1.4026027917861938,
        "learning_rate": 8.885891575494621e-06,
        "epoch": 0.6242073338847532,
        "step": 2264
    },
    {
        "loss": 1.8395,
        "grad_norm": 2.172267436981201,
        "learning_rate": 8.778377395316894e-06,
        "epoch": 0.6244830438378826,
        "step": 2265
    },
    {
        "loss": 2.4529,
        "grad_norm": 1.0722273588180542,
        "learning_rate": 8.671487743813378e-06,
        "epoch": 0.6247587537910119,
        "step": 2266
    },
    {
        "loss": 2.2423,
        "grad_norm": 1.96015202999115,
        "learning_rate": 8.565223352780415e-06,
        "epoch": 0.6250344637441412,
        "step": 2267
    },
    {
        "loss": 2.2762,
        "grad_norm": 1.3514013290405273,
        "learning_rate": 8.459584949733657e-06,
        "epoch": 0.6253101736972705,
        "step": 2268
    },
    {
        "loss": 2.3194,
        "grad_norm": 1.7471239566802979,
        "learning_rate": 8.35457325790312e-06,
        "epoch": 0.6255858836503998,
        "step": 2269
    },
    {
        "loss": 1.9355,
        "grad_norm": 1.6230896711349487,
        "learning_rate": 8.250188996228114e-06,
        "epoch": 0.6258615936035291,
        "step": 2270
    },
    {
        "loss": 1.7029,
        "grad_norm": 2.13797664642334,
        "learning_rate": 8.146432879352406e-06,
        "epoch": 0.6261373035566584,
        "step": 2271
    },
    {
        "loss": 2.2084,
        "grad_norm": 1.745322346687317,
        "learning_rate": 8.043305617619312e-06,
        "epoch": 0.6264130135097877,
        "step": 2272
    },
    {
        "loss": 1.9488,
        "grad_norm": 1.5064064264297485,
        "learning_rate": 7.94080791706685e-06,
        "epoch": 0.626688723462917,
        "step": 2273
    },
    {
        "loss": 2.3343,
        "grad_norm": 1.4194321632385254,
        "learning_rate": 7.838940479422874e-06,
        "epoch": 0.6269644334160464,
        "step": 2274
    },
    {
        "loss": 2.2848,
        "grad_norm": 1.9886723756790161,
        "learning_rate": 7.737704002100231e-06,
        "epoch": 0.6272401433691757,
        "step": 2275
    },
    {
        "loss": 1.4297,
        "grad_norm": 1.850251317024231,
        "learning_rate": 7.637099178192153e-06,
        "epoch": 0.627515853322305,
        "step": 2276
    },
    {
        "loss": 1.9599,
        "grad_norm": 1.4731557369232178,
        "learning_rate": 7.537126696467278e-06,
        "epoch": 0.6277915632754343,
        "step": 2277
    },
    {
        "loss": 2.2862,
        "grad_norm": 1.4341281652450562,
        "learning_rate": 7.437787241365113e-06,
        "epoch": 0.6280672732285636,
        "step": 2278
    },
    {
        "loss": 1.808,
        "grad_norm": 2.154299259185791,
        "learning_rate": 7.339081492991284e-06,
        "epoch": 0.6283429831816929,
        "step": 2279
    },
    {
        "loss": 1.8842,
        "grad_norm": 0.9843443632125854,
        "learning_rate": 7.241010127112891e-06,
        "epoch": 0.6286186931348222,
        "step": 2280
    },
    {
        "loss": 1.7467,
        "grad_norm": 2.3494374752044678,
        "learning_rate": 7.1435738151538365e-06,
        "epoch": 0.6288944030879515,
        "step": 2281
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.3244884014129639,
        "learning_rate": 7.046773224190295e-06,
        "epoch": 0.6291701130410808,
        "step": 2282
    },
    {
        "loss": 2.4542,
        "grad_norm": 1.566529393196106,
        "learning_rate": 6.95060901694613e-06,
        "epoch": 0.62944582299421,
        "step": 2283
    },
    {
        "loss": 2.1837,
        "grad_norm": 1.5373327732086182,
        "learning_rate": 6.855081851788314e-06,
        "epoch": 0.6297215329473393,
        "step": 2284
    },
    {
        "loss": 2.5848,
        "grad_norm": 1.3990774154663086,
        "learning_rate": 6.760192382722463e-06,
        "epoch": 0.6299972429004687,
        "step": 2285
    },
    {
        "loss": 2.1401,
        "grad_norm": 1.5325181484222412,
        "learning_rate": 6.6659412593883755e-06,
        "epoch": 0.630272952853598,
        "step": 2286
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.3505088090896606,
        "learning_rate": 6.572329127055521e-06,
        "epoch": 0.6305486628067273,
        "step": 2287
    },
    {
        "loss": 2.5802,
        "grad_norm": 1.0161495208740234,
        "learning_rate": 6.479356626618649e-06,
        "epoch": 0.6308243727598566,
        "step": 2288
    },
    {
        "loss": 2.3277,
        "grad_norm": 1.5026953220367432,
        "learning_rate": 6.387024394593466e-06,
        "epoch": 0.6311000827129859,
        "step": 2289
    },
    {
        "loss": 2.4837,
        "grad_norm": 1.152944564819336,
        "learning_rate": 6.295333063112174e-06,
        "epoch": 0.6313757926661152,
        "step": 2290
    },
    {
        "loss": 2.0962,
        "grad_norm": 1.5730574131011963,
        "learning_rate": 6.204283259919197e-06,
        "epoch": 0.6316515026192445,
        "step": 2291
    },
    {
        "loss": 2.5284,
        "grad_norm": 1.603503704071045,
        "learning_rate": 6.11387560836687e-06,
        "epoch": 0.6319272125723738,
        "step": 2292
    },
    {
        "loss": 2.0864,
        "grad_norm": 1.7127876281738281,
        "learning_rate": 6.024110727411247e-06,
        "epoch": 0.6322029225255031,
        "step": 2293
    },
    {
        "loss": 2.3045,
        "grad_norm": 1.0516403913497925,
        "learning_rate": 5.934989231607702e-06,
        "epoch": 0.6324786324786325,
        "step": 2294
    },
    {
        "loss": 1.7527,
        "grad_norm": 1.6488215923309326,
        "learning_rate": 5.846511731106852e-06,
        "epoch": 0.6327543424317618,
        "step": 2295
    },
    {
        "loss": 2.1527,
        "grad_norm": 1.6476361751556396,
        "learning_rate": 5.758678831650366e-06,
        "epoch": 0.6330300523848911,
        "step": 2296
    },
    {
        "loss": 1.77,
        "grad_norm": 1.973096251487732,
        "learning_rate": 5.671491134566775e-06,
        "epoch": 0.6333057623380204,
        "step": 2297
    },
    {
        "loss": 2.2914,
        "grad_norm": 1.3433094024658203,
        "learning_rate": 5.584949236767345e-06,
        "epoch": 0.6335814722911497,
        "step": 2298
    },
    {
        "loss": 1.7809,
        "grad_norm": 1.36927330493927,
        "learning_rate": 5.499053730742087e-06,
        "epoch": 0.633857182244279,
        "step": 2299
    },
    {
        "loss": 2.0829,
        "grad_norm": 1.5889328718185425,
        "learning_rate": 5.413805204555578e-06,
        "epoch": 0.6341328921974083,
        "step": 2300
    },
    {
        "loss": 1.3196,
        "grad_norm": 1.4231183528900146,
        "learning_rate": 5.3292042418429996e-06,
        "epoch": 0.6344086021505376,
        "step": 2301
    },
    {
        "loss": 2.3399,
        "grad_norm": 1.1251850128173828,
        "learning_rate": 5.245251421806141e-06,
        "epoch": 0.6346843121036669,
        "step": 2302
    },
    {
        "loss": 2.4052,
        "grad_norm": 1.495924949645996,
        "learning_rate": 5.161947319209459e-06,
        "epoch": 0.6349600220567962,
        "step": 2303
    },
    {
        "loss": 1.405,
        "grad_norm": 1.4262094497680664,
        "learning_rate": 5.079292504376043e-06,
        "epoch": 0.6352357320099256,
        "step": 2304
    },
    {
        "loss": 1.7051,
        "grad_norm": 2.4993350505828857,
        "learning_rate": 4.997287543183804e-06,
        "epoch": 0.6355114419630549,
        "step": 2305
    },
    {
        "loss": 2.3503,
        "grad_norm": 1.053818941116333,
        "learning_rate": 4.915932997061612e-06,
        "epoch": 0.6357871519161842,
        "step": 2306
    },
    {
        "loss": 2.0397,
        "grad_norm": 1.1107860803604126,
        "learning_rate": 4.835229422985376e-06,
        "epoch": 0.6360628618693135,
        "step": 2307
    },
    {
        "loss": 2.2229,
        "grad_norm": 1.0582948923110962,
        "learning_rate": 4.755177373474273e-06,
        "epoch": 0.6363385718224428,
        "step": 2308
    },
    {
        "loss": 2.0835,
        "grad_norm": 0.8560842871665955,
        "learning_rate": 4.675777396586945e-06,
        "epoch": 0.6366142817755721,
        "step": 2309
    },
    {
        "loss": 2.284,
        "grad_norm": 2.416515588760376,
        "learning_rate": 4.597030035917804e-06,
        "epoch": 0.6368899917287014,
        "step": 2310
    },
    {
        "loss": 2.0781,
        "grad_norm": 1.622879981994629,
        "learning_rate": 4.5189358305932515e-06,
        "epoch": 0.6371657016818307,
        "step": 2311
    },
    {
        "loss": 1.1913,
        "grad_norm": 2.2451727390289307,
        "learning_rate": 4.441495315267974e-06,
        "epoch": 0.63744141163496,
        "step": 2312
    },
    {
        "loss": 2.094,
        "grad_norm": 1.5630508661270142,
        "learning_rate": 4.364709020121371e-06,
        "epoch": 0.6377171215880894,
        "step": 2313
    },
    {
        "loss": 1.9804,
        "grad_norm": 1.7167596817016602,
        "learning_rate": 4.2885774708538275e-06,
        "epoch": 0.6379928315412187,
        "step": 2314
    },
    {
        "loss": 2.0177,
        "grad_norm": 1.8241024017333984,
        "learning_rate": 4.213101188683144e-06,
        "epoch": 0.638268541494348,
        "step": 2315
    },
    {
        "loss": 2.4471,
        "grad_norm": 1.2417402267456055,
        "learning_rate": 4.138280690341045e-06,
        "epoch": 0.6385442514474773,
        "step": 2316
    },
    {
        "loss": 2.188,
        "grad_norm": 1.1549427509307861,
        "learning_rate": 4.064116488069503e-06,
        "epoch": 0.6388199614006066,
        "step": 2317
    },
    {
        "loss": 1.9611,
        "grad_norm": 1.3812028169631958,
        "learning_rate": 3.990609089617348e-06,
        "epoch": 0.6390956713537359,
        "step": 2318
    },
    {
        "loss": 2.1956,
        "grad_norm": 1.7041711807250977,
        "learning_rate": 3.917758998236709e-06,
        "epoch": 0.6393713813068652,
        "step": 2319
    },
    {
        "loss": 1.9391,
        "grad_norm": 1.6683121919631958,
        "learning_rate": 3.8455667126796715e-06,
        "epoch": 0.6396470912599945,
        "step": 2320
    },
    {
        "loss": 2.2048,
        "grad_norm": 1.7223669290542603,
        "learning_rate": 3.7740327271947316e-06,
        "epoch": 0.6399228012131238,
        "step": 2321
    },
    {
        "loss": 1.9828,
        "grad_norm": 1.5412622690200806,
        "learning_rate": 3.7031575315235157e-06,
        "epoch": 0.6401985111662531,
        "step": 2322
    },
    {
        "loss": 2.3857,
        "grad_norm": 0.9779989123344421,
        "learning_rate": 3.632941610897389e-06,
        "epoch": 0.6404742211193825,
        "step": 2323
    },
    {
        "loss": 2.2937,
        "grad_norm": 1.939793348312378,
        "learning_rate": 3.5633854460341286e-06,
        "epoch": 0.6407499310725118,
        "step": 2324
    },
    {
        "loss": 2.1934,
        "grad_norm": 1.7072672843933105,
        "learning_rate": 3.4944895131346246e-06,
        "epoch": 0.6410256410256411,
        "step": 2325
    },
    {
        "loss": 2.0891,
        "grad_norm": 1.6688015460968018,
        "learning_rate": 3.426254283879682e-06,
        "epoch": 0.6413013509787703,
        "step": 2326
    },
    {
        "loss": 2.2959,
        "grad_norm": 1.5547667741775513,
        "learning_rate": 3.3586802254266914e-06,
        "epoch": 0.6415770609318996,
        "step": 2327
    },
    {
        "loss": 2.149,
        "grad_norm": 1.725279450416565,
        "learning_rate": 3.29176780040652e-06,
        "epoch": 0.6418527708850289,
        "step": 2328
    },
    {
        "loss": 2.2546,
        "grad_norm": 1.2246670722961426,
        "learning_rate": 3.2255174669202805e-06,
        "epoch": 0.6421284808381582,
        "step": 2329
    },
    {
        "loss": 2.302,
        "grad_norm": 1.431734323501587,
        "learning_rate": 3.159929678536244e-06,
        "epoch": 0.6424041907912875,
        "step": 2330
    },
    {
        "loss": 2.0405,
        "grad_norm": 2.029045581817627,
        "learning_rate": 3.0950048842867117e-06,
        "epoch": 0.6426799007444168,
        "step": 2331
    },
    {
        "loss": 2.3818,
        "grad_norm": 1.3770278692245483,
        "learning_rate": 3.0307435286649145e-06,
        "epoch": 0.6429556106975461,
        "step": 2332
    },
    {
        "loss": 1.8045,
        "grad_norm": 1.6989703178405762,
        "learning_rate": 2.9671460516220494e-06,
        "epoch": 0.6432313206506755,
        "step": 2333
    },
    {
        "loss": 2.0492,
        "grad_norm": 1.6292942762374878,
        "learning_rate": 2.904212888564162e-06,
        "epoch": 0.6435070306038048,
        "step": 2334
    },
    {
        "loss": 2.0148,
        "grad_norm": 1.416975498199463,
        "learning_rate": 2.841944470349245e-06,
        "epoch": 0.6437827405569341,
        "step": 2335
    },
    {
        "loss": 1.9932,
        "grad_norm": 2.142894983291626,
        "learning_rate": 2.7803412232842664e-06,
        "epoch": 0.6440584505100634,
        "step": 2336
    },
    {
        "loss": 2.3107,
        "grad_norm": 1.3502424955368042,
        "learning_rate": 2.7194035691222365e-06,
        "epoch": 0.6443341604631927,
        "step": 2337
    },
    {
        "loss": 2.2889,
        "grad_norm": 2.2537453174591064,
        "learning_rate": 2.6591319250593217e-06,
        "epoch": 0.644609870416322,
        "step": 2338
    },
    {
        "loss": 1.9167,
        "grad_norm": 1.5682421922683716,
        "learning_rate": 2.5995267037320138e-06,
        "epoch": 0.6448855803694513,
        "step": 2339
    },
    {
        "loss": 2.2569,
        "grad_norm": 1.5458780527114868,
        "learning_rate": 2.5405883132143092e-06,
        "epoch": 0.6451612903225806,
        "step": 2340
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.6390812397003174,
        "learning_rate": 2.482317157014846e-06,
        "epoch": 0.6454370002757099,
        "step": 2341
    },
    {
        "loss": 2.13,
        "grad_norm": 1.6071475744247437,
        "learning_rate": 2.424713634074205e-06,
        "epoch": 0.6457127102288392,
        "step": 2342
    },
    {
        "loss": 2.4658,
        "grad_norm": 1.5266667604446411,
        "learning_rate": 2.3677781387622e-06,
        "epoch": 0.6459884201819686,
        "step": 2343
    },
    {
        "loss": 2.3745,
        "grad_norm": 1.816630482673645,
        "learning_rate": 2.3115110608751046e-06,
        "epoch": 0.6462641301350979,
        "step": 2344
    },
    {
        "loss": 1.9847,
        "grad_norm": 1.1249665021896362,
        "learning_rate": 2.2559127856330186e-06,
        "epoch": 0.6465398400882272,
        "step": 2345
    },
    {
        "loss": 1.4241,
        "grad_norm": 1.2893520593643188,
        "learning_rate": 2.20098369367725e-06,
        "epoch": 0.6468155500413565,
        "step": 2346
    },
    {
        "loss": 2.1334,
        "grad_norm": 1.7782697677612305,
        "learning_rate": 2.1467241610676813e-06,
        "epoch": 0.6470912599944858,
        "step": 2347
    },
    {
        "loss": 1.3054,
        "grad_norm": 1.7069776058197021,
        "learning_rate": 2.093134559280219e-06,
        "epoch": 0.6473669699476151,
        "step": 2348
    },
    {
        "loss": 2.336,
        "grad_norm": 1.0703290700912476,
        "learning_rate": 2.040215255204214e-06,
        "epoch": 0.6476426799007444,
        "step": 2349
    },
    {
        "loss": 2.3113,
        "grad_norm": 0.8265889883041382,
        "learning_rate": 1.987966611139991e-06,
        "epoch": 0.6479183898538737,
        "step": 2350
    },
    {
        "loss": 2.528,
        "grad_norm": 0.9377256035804749,
        "learning_rate": 1.9363889847963334e-06,
        "epoch": 0.648194099807003,
        "step": 2351
    },
    {
        "loss": 1.8342,
        "grad_norm": 1.9476335048675537,
        "learning_rate": 1.8854827292880572e-06,
        "epoch": 0.6484698097601324,
        "step": 2352
    },
    {
        "loss": 1.8979,
        "grad_norm": 1.2913117408752441,
        "learning_rate": 1.8352481931336095e-06,
        "epoch": 0.6487455197132617,
        "step": 2353
    },
    {
        "loss": 2.2479,
        "grad_norm": 1.5776586532592773,
        "learning_rate": 1.7856857202526168e-06,
        "epoch": 0.649021229666391,
        "step": 2354
    },
    {
        "loss": 1.9045,
        "grad_norm": 1.928131103515625,
        "learning_rate": 1.7367956499635961e-06,
        "epoch": 0.6492969396195203,
        "step": 2355
    },
    {
        "loss": 1.347,
        "grad_norm": 2.342416524887085,
        "learning_rate": 1.6885783169816037e-06,
        "epoch": 0.6495726495726496,
        "step": 2356
    },
    {
        "loss": 2.1862,
        "grad_norm": 1.6169519424438477,
        "learning_rate": 1.6410340514159571e-06,
        "epoch": 0.6498483595257789,
        "step": 2357
    },
    {
        "loss": 2.186,
        "grad_norm": 1.8619108200073242,
        "learning_rate": 1.594163178767971e-06,
        "epoch": 0.6501240694789082,
        "step": 2358
    },
    {
        "loss": 1.8867,
        "grad_norm": 1.759989619255066,
        "learning_rate": 1.5479660199286928e-06,
        "epoch": 0.6503997794320375,
        "step": 2359
    },
    {
        "loss": 2.3771,
        "grad_norm": 1.0679069757461548,
        "learning_rate": 1.5024428911767697e-06,
        "epoch": 0.6506754893851668,
        "step": 2360
    },
    {
        "loss": 1.8426,
        "grad_norm": 2.1171860694885254,
        "learning_rate": 1.457594104176241e-06,
        "epoch": 0.6509511993382961,
        "step": 2361
    },
    {
        "loss": 2.342,
        "grad_norm": 1.4746959209442139,
        "learning_rate": 1.413419965974405e-06,
        "epoch": 0.6512269092914255,
        "step": 2362
    },
    {
        "loss": 1.9894,
        "grad_norm": 1.7047151327133179,
        "learning_rate": 1.3699207789997447e-06,
        "epoch": 0.6515026192445548,
        "step": 2363
    },
    {
        "loss": 1.7528,
        "grad_norm": 0.9306032657623291,
        "learning_rate": 1.3270968410598273e-06,
        "epoch": 0.6517783291976841,
        "step": 2364
    },
    {
        "loss": 2.2303,
        "grad_norm": 1.4032213687896729,
        "learning_rate": 1.2849484453392624e-06,
        "epoch": 0.6520540391508134,
        "step": 2365
    },
    {
        "loss": 1.7925,
        "grad_norm": 1.6744436025619507,
        "learning_rate": 1.2434758803977374e-06,
        "epoch": 0.6523297491039427,
        "step": 2366
    },
    {
        "loss": 2.3319,
        "grad_norm": 1.4451123476028442,
        "learning_rate": 1.2026794301679968e-06,
        "epoch": 0.652605459057072,
        "step": 2367
    },
    {
        "loss": 2.4254,
        "grad_norm": 1.534559965133667,
        "learning_rate": 1.1625593739539308e-06,
        "epoch": 0.6528811690102013,
        "step": 2368
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.3019858598709106,
        "learning_rate": 1.1231159864286133e-06,
        "epoch": 0.6531568789633305,
        "step": 2369
    },
    {
        "loss": 1.9009,
        "grad_norm": 1.5584847927093506,
        "learning_rate": 1.0843495376324896e-06,
        "epoch": 0.6534325889164598,
        "step": 2370
    },
    {
        "loss": 2.3981,
        "grad_norm": 1.6575703620910645,
        "learning_rate": 1.0462602929714792e-06,
        "epoch": 0.6537082988695891,
        "step": 2371
    },
    {
        "loss": 2.3128,
        "grad_norm": 1.0973842144012451,
        "learning_rate": 1.008848513215177e-06,
        "epoch": 0.6539840088227185,
        "step": 2372
    },
    {
        "loss": 1.8932,
        "grad_norm": 1.8994696140289307,
        "learning_rate": 9.721144544950767e-07,
        "epoch": 0.6542597187758478,
        "step": 2373
    },
    {
        "loss": 2.3873,
        "grad_norm": 1.5850716829299927,
        "learning_rate": 9.360583683027613e-07,
        "epoch": 0.6545354287289771,
        "step": 2374
    },
    {
        "loss": 2.17,
        "grad_norm": 1.1291298866271973,
        "learning_rate": 9.006805014882824e-07,
        "epoch": 0.6548111386821064,
        "step": 2375
    },
    {
        "loss": 2.5359,
        "grad_norm": 1.1822792291641235,
        "learning_rate": 8.659810962583726e-07,
        "epoch": 0.6550868486352357,
        "step": 2376
    },
    {
        "loss": 2.0722,
        "grad_norm": 1.8210384845733643,
        "learning_rate": 8.319603901748351e-07,
        "epoch": 0.655362558588365,
        "step": 2377
    },
    {
        "loss": 2.1417,
        "grad_norm": 1.420261025428772,
        "learning_rate": 7.986186161529241e-07,
        "epoch": 0.6556382685414943,
        "step": 2378
    },
    {
        "loss": 2.768,
        "grad_norm": 1.4173029661178589,
        "learning_rate": 7.659560024597223e-07,
        "epoch": 0.6559139784946236,
        "step": 2379
    },
    {
        "loss": 2.0354,
        "grad_norm": 1.821165680885315,
        "learning_rate": 7.339727727125877e-07,
        "epoch": 0.6561896884477529,
        "step": 2380
    },
    {
        "loss": 1.7712,
        "grad_norm": 1.7854952812194824,
        "learning_rate": 7.026691458776325e-07,
        "epoch": 0.6564653984008822,
        "step": 2381
    },
    {
        "loss": 1.5453,
        "grad_norm": 2.22135853767395,
        "learning_rate": 6.720453362682344e-07,
        "epoch": 0.6567411083540116,
        "step": 2382
    },
    {
        "loss": 2.0816,
        "grad_norm": 1.2736989259719849,
        "learning_rate": 6.421015535435171e-07,
        "epoch": 0.6570168183071409,
        "step": 2383
    },
    {
        "loss": 2.3847,
        "grad_norm": 1.6304908990859985,
        "learning_rate": 6.128380027069724e-07,
        "epoch": 0.6572925282602702,
        "step": 2384
    },
    {
        "loss": 2.1301,
        "grad_norm": 1.668894648551941,
        "learning_rate": 5.84254884105051e-07,
        "epoch": 0.6575682382133995,
        "step": 2385
    },
    {
        "loss": 2.0541,
        "grad_norm": 1.6629263162612915,
        "learning_rate": 5.563523934257186e-07,
        "epoch": 0.6578439481665288,
        "step": 2386
    },
    {
        "loss": 1.966,
        "grad_norm": 1.4818766117095947,
        "learning_rate": 5.291307216972574e-07,
        "epoch": 0.6581196581196581,
        "step": 2387
    },
    {
        "loss": 2.4818,
        "grad_norm": 1.2882375717163086,
        "learning_rate": 5.02590055286789e-07,
        "epoch": 0.6583953680727874,
        "step": 2388
    },
    {
        "loss": 2.0239,
        "grad_norm": 1.793387770652771,
        "learning_rate": 4.767305758991314e-07,
        "epoch": 0.6586710780259167,
        "step": 2389
    },
    {
        "loss": 1.9872,
        "grad_norm": 1.7364994287490845,
        "learning_rate": 4.515524605754884e-07,
        "epoch": 0.658946787979046,
        "step": 2390
    },
    {
        "loss": 1.9921,
        "grad_norm": 1.8247243165969849,
        "learning_rate": 4.270558816922732e-07,
        "epoch": 0.6592224979321754,
        "step": 2391
    },
    {
        "loss": 1.6898,
        "grad_norm": 0.9436354637145996,
        "learning_rate": 4.0324100695989797e-07,
        "epoch": 0.6594982078853047,
        "step": 2392
    },
    {
        "loss": 2.5526,
        "grad_norm": 1.864197015762329,
        "learning_rate": 3.8010799942161946e-07,
        "epoch": 0.659773917838434,
        "step": 2393
    },
    {
        "loss": 2.2943,
        "grad_norm": 1.2416661977767944,
        "learning_rate": 3.5765701745247295e-07,
        "epoch": 0.6600496277915633,
        "step": 2394
    },
    {
        "loss": 2.1977,
        "grad_norm": 1.0942449569702148,
        "learning_rate": 3.3588821475815103e-07,
        "epoch": 0.6603253377446926,
        "step": 2395
    },
    {
        "loss": 1.8676,
        "grad_norm": 2.0264804363250732,
        "learning_rate": 3.148017403739267e-07,
        "epoch": 0.6606010476978219,
        "step": 2396
    },
    {
        "loss": 1.8178,
        "grad_norm": 1.6638089418411255,
        "learning_rate": 2.943977386637098e-07,
        "epoch": 0.6608767576509512,
        "step": 2397
    },
    {
        "loss": 2.3261,
        "grad_norm": 1.6927615404129028,
        "learning_rate": 2.746763493189697e-07,
        "epoch": 0.6611524676040805,
        "step": 2398
    },
    {
        "loss": 2.6405,
        "grad_norm": 1.351956844329834,
        "learning_rate": 2.556377073578475e-07,
        "epoch": 0.6614281775572098,
        "step": 2399
    },
    {
        "loss": 2.1662,
        "grad_norm": 1.5903668403625488,
        "learning_rate": 2.3728194312420126e-07,
        "epoch": 0.6617038875103392,
        "step": 2400
    },
    {
        "loss": 2.5501,
        "grad_norm": 1.0455924272537231,
        "learning_rate": 2.1960918228670635e-07,
        "epoch": 0.6619795974634685,
        "step": 2401
    },
    {
        "loss": 1.4261,
        "grad_norm": 2.4701244831085205,
        "learning_rate": 2.02619545838012e-07,
        "epoch": 0.6622553074165978,
        "step": 2402
    },
    {
        "loss": 2.3309,
        "grad_norm": 1.4434837102890015,
        "learning_rate": 1.863131500939086e-07,
        "epoch": 0.6625310173697271,
        "step": 2403
    },
    {
        "loss": 1.8302,
        "grad_norm": 1.4238197803497314,
        "learning_rate": 1.7069010669253926e-07,
        "epoch": 0.6628067273228564,
        "step": 2404
    },
    {
        "loss": 2.7368,
        "grad_norm": 1.392375111579895,
        "learning_rate": 1.5575052259361177e-07,
        "epoch": 0.6630824372759857,
        "step": 2405
    },
    {
        "loss": 2.1208,
        "grad_norm": 1.735327959060669,
        "learning_rate": 1.4149450007767684e-07,
        "epoch": 0.663358147229115,
        "step": 2406
    },
    {
        "loss": 2.103,
        "grad_norm": 1.413849949836731,
        "learning_rate": 1.2792213674545084e-07,
        "epoch": 0.6636338571822443,
        "step": 2407
    },
    {
        "loss": 1.946,
        "grad_norm": 1.5645945072174072,
        "learning_rate": 1.1503352551712754e-07,
        "epoch": 0.6639095671353736,
        "step": 2408
    },
    {
        "loss": 1.2804,
        "grad_norm": 1.9296544790267944,
        "learning_rate": 1.0282875463171193e-07,
        "epoch": 0.664185277088503,
        "step": 2409
    },
    {
        "loss": 2.0356,
        "grad_norm": 0.8574602007865906,
        "learning_rate": 9.130790764648733e-08,
        "epoch": 0.6644609870416323,
        "step": 2410
    },
    {
        "loss": 2.424,
        "grad_norm": 1.604719877243042,
        "learning_rate": 8.047106343638256e-08,
        "epoch": 0.6647366969947615,
        "step": 2411
    },
    {
        "loss": 2.4253,
        "grad_norm": 1.6835216283798218,
        "learning_rate": 7.031829619345009e-08,
        "epoch": 0.6650124069478908,
        "step": 2412
    },
    {
        "loss": 2.2115,
        "grad_norm": 1.594821810722351,
        "learning_rate": 6.084967542637765e-08,
        "epoch": 0.6652881169010201,
        "step": 2413
    },
    {
        "loss": 2.3494,
        "grad_norm": 1.4521219730377197,
        "learning_rate": 5.20652659599663e-08,
        "epoch": 0.6655638268541494,
        "step": 2414
    },
    {
        "loss": 2.6222,
        "grad_norm": 1.4699445962905884,
        "learning_rate": 4.396512793475305e-08,
        "epoch": 0.6658395368072787,
        "step": 2415
    },
    {
        "loss": 2.0024,
        "grad_norm": 1.2479221820831299,
        "learning_rate": 3.65493168065334e-08,
        "epoch": 0.666115246760408,
        "step": 2416
    },
    {
        "loss": 2.2247,
        "grad_norm": 2.282475709915161,
        "learning_rate": 2.98178833460172e-08,
        "epoch": 0.6663909567135373,
        "step": 2417
    },
    {
        "loss": 2.5409,
        "grad_norm": 1.0088982582092285,
        "learning_rate": 2.3770873638484513e-08,
        "epoch": 0.6666666666666666,
        "step": 2418
    },
    {
        "loss": 2.2716,
        "grad_norm": 1.9215337038040161,
        "learning_rate": 1.8408329083441365e-08,
        "epoch": 0.6669423766197959,
        "step": 2419
    },
    {
        "loss": 1.7328,
        "grad_norm": 2.134781837463379,
        "learning_rate": 1.3730286394364466e-08,
        "epoch": 0.6672180865729253,
        "step": 2420
    },
    {
        "loss": 2.1204,
        "grad_norm": 1.2722845077514648,
        "learning_rate": 9.736777598434721e-09,
        "epoch": 0.6674937965260546,
        "step": 2421
    },
    {
        "loss": 2.0888,
        "grad_norm": 1.8284839391708374,
        "learning_rate": 6.427830036337401e-09,
        "epoch": 0.6677695064791839,
        "step": 2422
    },
    {
        "loss": 2.3328,
        "grad_norm": 1.6284407377243042,
        "learning_rate": 3.8034663620512e-09,
        "epoch": 0.6680452164323132,
        "step": 2423
    },
    {
        "loss": 1.7432,
        "grad_norm": 1.3581479787826538,
        "learning_rate": 1.8637045426928013e-09,
        "epoch": 0.6683209263854425,
        "step": 2424
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.6287274360656738,
        "learning_rate": 6.085578584280605e-10,
        "epoch": 0.6685966363385718,
        "step": 2425
    },
    {
        "loss": 1.7397,
        "grad_norm": 2.054415702819824,
        "learning_rate": 3.8034902316574915e-11,
        "epoch": 0.6688723462917011,
        "step": 2426
    },
    {
        "loss": 1.8268,
        "grad_norm": 1.7640806436538696,
        "learning_rate": 0.00019999984786041967,
        "epoch": 0.6691480562448304,
        "step": 2427
    },
    {
        "loss": 2.2068,
        "grad_norm": 2.0635011196136475,
        "learning_rate": 0.00019999904912888872,
        "epoch": 0.6694237661979597,
        "step": 2428
    },
    {
        "loss": 2.2474,
        "grad_norm": 0.8219094276428223,
        "learning_rate": 0.00019999756577597318,
        "epoch": 0.669699476151089,
        "step": 2429
    },
    {
        "loss": 2.095,
        "grad_norm": 1.4138017892837524,
        "learning_rate": 0.00019999539781182847,
        "epoch": 0.6699751861042184,
        "step": 2430
    },
    {
        "loss": 2.1975,
        "grad_norm": 2.203683614730835,
        "learning_rate": 0.00019999254525129712,
        "epoch": 0.6702508960573477,
        "step": 2431
    },
    {
        "loss": 2.374,
        "grad_norm": 0.9662286639213562,
        "learning_rate": 0.00019998900811390852,
        "epoch": 0.670526606010477,
        "step": 2432
    },
    {
        "loss": 2.1081,
        "grad_norm": 1.3700790405273438,
        "learning_rate": 0.00019998478642387893,
        "epoch": 0.6708023159636063,
        "step": 2433
    },
    {
        "loss": 2.4763,
        "grad_norm": 1.2119159698486328,
        "learning_rate": 0.0001999798802101112,
        "epoch": 0.6710780259167356,
        "step": 2434
    },
    {
        "loss": 2.2796,
        "grad_norm": 1.7715219259262085,
        "learning_rate": 0.00019997428950619463,
        "epoch": 0.6713537358698649,
        "step": 2435
    },
    {
        "loss": 1.7559,
        "grad_norm": 1.5794930458068848,
        "learning_rate": 0.00019996801435040476,
        "epoch": 0.6716294458229942,
        "step": 2436
    },
    {
        "loss": 2.2891,
        "grad_norm": 2.1771702766418457,
        "learning_rate": 0.000199961054785703,
        "epoch": 0.6719051557761235,
        "step": 2437
    },
    {
        "loss": 1.8182,
        "grad_norm": 1.7393999099731445,
        "learning_rate": 0.0001999534108597366,
        "epoch": 0.6721808657292528,
        "step": 2438
    },
    {
        "loss": 1.9594,
        "grad_norm": 1.9317350387573242,
        "learning_rate": 0.00019994508262483786,
        "epoch": 0.6724565756823822,
        "step": 2439
    },
    {
        "loss": 2.0034,
        "grad_norm": 2.473564863204956,
        "learning_rate": 0.0001999360701380243,
        "epoch": 0.6727322856355115,
        "step": 2440
    },
    {
        "loss": 1.9425,
        "grad_norm": 1.0921597480773926,
        "learning_rate": 0.00019992637346099784,
        "epoch": 0.6730079955886408,
        "step": 2441
    },
    {
        "loss": 2.3141,
        "grad_norm": 1.8479397296905518,
        "learning_rate": 0.00019991599266014473,
        "epoch": 0.6732837055417701,
        "step": 2442
    },
    {
        "loss": 1.4883,
        "grad_norm": 2.2165966033935547,
        "learning_rate": 0.00019990492780653477,
        "epoch": 0.6735594154948994,
        "step": 2443
    },
    {
        "loss": 1.9823,
        "grad_norm": 1.8290754556655884,
        "learning_rate": 0.000199893178975921,
        "epoch": 0.6738351254480287,
        "step": 2444
    },
    {
        "loss": 2.0097,
        "grad_norm": 1.4534543752670288,
        "learning_rate": 0.00019988074624873928,
        "epoch": 0.674110835401158,
        "step": 2445
    },
    {
        "loss": 2.3192,
        "grad_norm": 1.139688491821289,
        "learning_rate": 0.00019986762971010747,
        "epoch": 0.6743865453542873,
        "step": 2446
    },
    {
        "loss": 1.5382,
        "grad_norm": 1.9420347213745117,
        "learning_rate": 0.00019985382944982507,
        "epoch": 0.6746622553074166,
        "step": 2447
    },
    {
        "loss": 1.9698,
        "grad_norm": 1.7574660778045654,
        "learning_rate": 0.00019983934556237247,
        "epoch": 0.674937965260546,
        "step": 2448
    },
    {
        "loss": 2.1246,
        "grad_norm": 1.172140121459961,
        "learning_rate": 0.00019982417814691048,
        "epoch": 0.6752136752136753,
        "step": 2449
    },
    {
        "loss": 2.3651,
        "grad_norm": 1.6436595916748047,
        "learning_rate": 0.00019980832730727942,
        "epoch": 0.6754893851668046,
        "step": 2450
    },
    {
        "loss": 2.226,
        "grad_norm": 1.3166992664337158,
        "learning_rate": 0.00019979179315199853,
        "epoch": 0.6757650951199339,
        "step": 2451
    },
    {
        "loss": 1.6026,
        "grad_norm": 1.6321053504943848,
        "learning_rate": 0.0001997745757942653,
        "epoch": 0.6760408050730632,
        "step": 2452
    },
    {
        "loss": 2.5345,
        "grad_norm": 1.5776607990264893,
        "learning_rate": 0.0001997566753519545,
        "epoch": 0.6763165150261925,
        "step": 2453
    },
    {
        "loss": 2.0065,
        "grad_norm": 1.9454656839370728,
        "learning_rate": 0.00019973809194761763,
        "epoch": 0.6765922249793217,
        "step": 2454
    },
    {
        "loss": 2.0075,
        "grad_norm": 1.464336633682251,
        "learning_rate": 0.0001997188257084818,
        "epoch": 0.676867934932451,
        "step": 2455
    },
    {
        "loss": 2.2984,
        "grad_norm": 1.472805380821228,
        "learning_rate": 0.00019969887676644907,
        "epoch": 0.6771436448855803,
        "step": 2456
    },
    {
        "loss": 1.6028,
        "grad_norm": 2.308303117752075,
        "learning_rate": 0.0001996782452580955,
        "epoch": 0.6774193548387096,
        "step": 2457
    },
    {
        "loss": 2.1087,
        "grad_norm": 1.32660710811615,
        "learning_rate": 0.00019965693132467012,
        "epoch": 0.6776950647918389,
        "step": 2458
    },
    {
        "loss": 1.4643,
        "grad_norm": 2.476091146469116,
        "learning_rate": 0.00019963493511209403,
        "epoch": 0.6779707747449683,
        "step": 2459
    },
    {
        "loss": 2.2671,
        "grad_norm": 1.6471649408340454,
        "learning_rate": 0.00019961225677095953,
        "epoch": 0.6782464846980976,
        "step": 2460
    },
    {
        "loss": 1.4204,
        "grad_norm": 2.4862287044525146,
        "learning_rate": 0.00019958889645652877,
        "epoch": 0.6785221946512269,
        "step": 2461
    },
    {
        "loss": 1.4734,
        "grad_norm": 1.69619619846344,
        "learning_rate": 0.000199564854328733,
        "epoch": 0.6787979046043562,
        "step": 2462
    },
    {
        "loss": 2.4377,
        "grad_norm": 1.2912856340408325,
        "learning_rate": 0.00019954013055217137,
        "epoch": 0.6790736145574855,
        "step": 2463
    },
    {
        "loss": 1.6596,
        "grad_norm": 2.042541980743408,
        "learning_rate": 0.00019951472529610974,
        "epoch": 0.6793493245106148,
        "step": 2464
    },
    {
        "loss": 2.4177,
        "grad_norm": 1.2602386474609375,
        "learning_rate": 0.00019948863873447959,
        "epoch": 0.6796250344637441,
        "step": 2465
    },
    {
        "loss": 2.0433,
        "grad_norm": 1.6169943809509277,
        "learning_rate": 0.00019946187104587674,
        "epoch": 0.6799007444168734,
        "step": 2466
    },
    {
        "loss": 2.6198,
        "grad_norm": 1.4574068784713745,
        "learning_rate": 0.00019943442241356034,
        "epoch": 0.6801764543700027,
        "step": 2467
    },
    {
        "loss": 1.9632,
        "grad_norm": 1.04890775680542,
        "learning_rate": 0.0001994062930254513,
        "epoch": 0.680452164323132,
        "step": 2468
    },
    {
        "loss": 1.6884,
        "grad_norm": 2.253068685531616,
        "learning_rate": 0.00019937748307413133,
        "epoch": 0.6807278742762614,
        "step": 2469
    },
    {
        "loss": 1.9665,
        "grad_norm": 1.7410500049591064,
        "learning_rate": 0.00019934799275684136,
        "epoch": 0.6810035842293907,
        "step": 2470
    },
    {
        "loss": 1.436,
        "grad_norm": 1.8476406335830688,
        "learning_rate": 0.0001993178222754803,
        "epoch": 0.68127929418252,
        "step": 2471
    },
    {
        "loss": 2.357,
        "grad_norm": 1.6950287818908691,
        "learning_rate": 0.00019928697183660372,
        "epoch": 0.6815550041356493,
        "step": 2472
    },
    {
        "loss": 1.5197,
        "grad_norm": 2.2239296436309814,
        "learning_rate": 0.00019925544165142232,
        "epoch": 0.6818307140887786,
        "step": 2473
    },
    {
        "loss": 2.2223,
        "grad_norm": 1.2589423656463623,
        "learning_rate": 0.00019922323193580053,
        "epoch": 0.6821064240419079,
        "step": 2474
    },
    {
        "loss": 2.187,
        "grad_norm": 1.6101773977279663,
        "learning_rate": 0.00019919034291025506,
        "epoch": 0.6823821339950372,
        "step": 2475
    },
    {
        "loss": 2.3674,
        "grad_norm": 1.7388981580734253,
        "learning_rate": 0.0001991567747999534,
        "epoch": 0.6826578439481665,
        "step": 2476
    },
    {
        "loss": 2.1761,
        "grad_norm": 1.6837934255599976,
        "learning_rate": 0.0001991225278347121,
        "epoch": 0.6829335539012958,
        "step": 2477
    },
    {
        "loss": 1.7283,
        "grad_norm": 2.278308629989624,
        "learning_rate": 0.00019908760224899547,
        "epoch": 0.6832092638544252,
        "step": 2478
    },
    {
        "loss": 2.3152,
        "grad_norm": 1.705098032951355,
        "learning_rate": 0.00019905199828191384,
        "epoch": 0.6834849738075545,
        "step": 2479
    },
    {
        "loss": 2.3228,
        "grad_norm": 1.533829927444458,
        "learning_rate": 0.00019901571617722191,
        "epoch": 0.6837606837606838,
        "step": 2480
    },
    {
        "loss": 1.9869,
        "grad_norm": 1.8824995756149292,
        "learning_rate": 0.00019897875618331707,
        "epoch": 0.6840363937138131,
        "step": 2481
    },
    {
        "loss": 1.9833,
        "grad_norm": 1.922349452972412,
        "learning_rate": 0.00019894111855323773,
        "epoch": 0.6843121036669424,
        "step": 2482
    },
    {
        "loss": 2.3261,
        "grad_norm": 1.5790736675262451,
        "learning_rate": 0.00019890280354466162,
        "epoch": 0.6845878136200717,
        "step": 2483
    },
    {
        "loss": 2.212,
        "grad_norm": 1.1260632276535034,
        "learning_rate": 0.000198863811419904,
        "epoch": 0.684863523573201,
        "step": 2484
    },
    {
        "loss": 2.2343,
        "grad_norm": 1.446520447731018,
        "learning_rate": 0.00019882414244591577,
        "epoch": 0.6851392335263303,
        "step": 2485
    },
    {
        "loss": 2.6623,
        "grad_norm": 1.6478602886199951,
        "learning_rate": 0.00019878379689428188,
        "epoch": 0.6854149434794596,
        "step": 2486
    },
    {
        "loss": 2.5366,
        "grad_norm": 1.2639573812484741,
        "learning_rate": 0.00019874277504121915,
        "epoch": 0.685690653432589,
        "step": 2487
    },
    {
        "loss": 1.8089,
        "grad_norm": 1.569399118423462,
        "learning_rate": 0.00019870107716757468,
        "epoch": 0.6859663633857183,
        "step": 2488
    },
    {
        "loss": 2.159,
        "grad_norm": 1.8849729299545288,
        "learning_rate": 0.0001986587035588237,
        "epoch": 0.6862420733388476,
        "step": 2489
    },
    {
        "loss": 1.8513,
        "grad_norm": 2.076636552810669,
        "learning_rate": 0.0001986156545050678,
        "epoch": 0.6865177832919769,
        "step": 2490
    },
    {
        "loss": 2.2329,
        "grad_norm": 1.3192964792251587,
        "learning_rate": 0.00019857193030103275,
        "epoch": 0.6867934932451062,
        "step": 2491
    },
    {
        "loss": 2.1292,
        "grad_norm": 1.1118156909942627,
        "learning_rate": 0.00019852753124606674,
        "epoch": 0.6870692031982355,
        "step": 2492
    },
    {
        "loss": 1.6797,
        "grad_norm": 1.1980584859848022,
        "learning_rate": 0.00019848245764413801,
        "epoch": 0.6873449131513648,
        "step": 2493
    },
    {
        "loss": 2.5261,
        "grad_norm": 1.2178131341934204,
        "learning_rate": 0.00019843670980383306,
        "epoch": 0.6876206231044941,
        "step": 2494
    },
    {
        "loss": 2.3995,
        "grad_norm": 1.533582091331482,
        "learning_rate": 0.00019839028803835432,
        "epoch": 0.6878963330576234,
        "step": 2495
    },
    {
        "loss": 1.8932,
        "grad_norm": 1.8098840713500977,
        "learning_rate": 0.00019834319266551816,
        "epoch": 0.6881720430107527,
        "step": 2496
    },
    {
        "loss": 1.6873,
        "grad_norm": 1.7878319025039673,
        "learning_rate": 0.00019829542400775268,
        "epoch": 0.6884477529638819,
        "step": 2497
    },
    {
        "loss": 1.6305,
        "grad_norm": 1.9004188776016235,
        "learning_rate": 0.00019824698239209536,
        "epoch": 0.6887234629170113,
        "step": 2498
    },
    {
        "loss": 1.9842,
        "grad_norm": 1.4560219049453735,
        "learning_rate": 0.00019819786815019109,
        "epoch": 0.6889991728701406,
        "step": 2499
    },
    {
        "loss": 2.0627,
        "grad_norm": 2.1625895500183105,
        "learning_rate": 0.0001981480816182896,
        "epoch": 0.6892748828232699,
        "step": 2500
    },
    {
        "loss": 2.0406,
        "grad_norm": 1.6770837306976318,
        "learning_rate": 0.00019809762313724344,
        "epoch": 0.6895505927763992,
        "step": 2501
    },
    {
        "loss": 1.6687,
        "grad_norm": 1.9476250410079956,
        "learning_rate": 0.00019804649305250538,
        "epoch": 0.6898263027295285,
        "step": 2502
    },
    {
        "loss": 2.3132,
        "grad_norm": 1.4204003810882568,
        "learning_rate": 0.00019799469171412635,
        "epoch": 0.6901020126826578,
        "step": 2503
    },
    {
        "loss": 1.9912,
        "grad_norm": 2.356964349746704,
        "learning_rate": 0.0001979422194767526,
        "epoch": 0.6903777226357871,
        "step": 2504
    },
    {
        "loss": 2.6053,
        "grad_norm": 1.451392412185669,
        "learning_rate": 0.0001978890766996239,
        "epoch": 0.6906534325889164,
        "step": 2505
    },
    {
        "loss": 2.3397,
        "grad_norm": 1.6560801267623901,
        "learning_rate": 0.0001978352637465703,
        "epoch": 0.6909291425420457,
        "step": 2506
    },
    {
        "loss": 2.5257,
        "grad_norm": 2.157122850418091,
        "learning_rate": 0.00019778078098601053,
        "epoch": 0.691204852495175,
        "step": 2507
    },
    {
        "loss": 2.4022,
        "grad_norm": 1.4592286348342896,
        "learning_rate": 0.00019772562879094856,
        "epoch": 0.6914805624483044,
        "step": 2508
    },
    {
        "loss": 2.1889,
        "grad_norm": 1.8559826612472534,
        "learning_rate": 0.00019766980753897184,
        "epoch": 0.6917562724014337,
        "step": 2509
    },
    {
        "loss": 2.4068,
        "grad_norm": 1.3568404912948608,
        "learning_rate": 0.00019761331761224815,
        "epoch": 0.692031982354563,
        "step": 2510
    },
    {
        "loss": 2.1136,
        "grad_norm": 1.7920963764190674,
        "learning_rate": 0.00019755615939752336,
        "epoch": 0.6923076923076923,
        "step": 2511
    },
    {
        "loss": 1.9142,
        "grad_norm": 1.2102181911468506,
        "learning_rate": 0.00019749833328611843,
        "epoch": 0.6925834022608216,
        "step": 2512
    },
    {
        "loss": 2.3573,
        "grad_norm": 1.4765723943710327,
        "learning_rate": 0.0001974398396739272,
        "epoch": 0.6928591122139509,
        "step": 2513
    },
    {
        "loss": 2.3276,
        "grad_norm": 1.7401273250579834,
        "learning_rate": 0.00019738067896141314,
        "epoch": 0.6931348221670802,
        "step": 2514
    },
    {
        "loss": 2.087,
        "grad_norm": 1.1815913915634155,
        "learning_rate": 0.00019732085155360708,
        "epoch": 0.6934105321202095,
        "step": 2515
    },
    {
        "loss": 1.2512,
        "grad_norm": 1.959148645401001,
        "learning_rate": 0.00019726035786010404,
        "epoch": 0.6936862420733388,
        "step": 2516
    },
    {
        "loss": 2.59,
        "grad_norm": 1.1850051879882812,
        "learning_rate": 0.00019719919829506083,
        "epoch": 0.6939619520264682,
        "step": 2517
    },
    {
        "loss": 2.2425,
        "grad_norm": 1.308598279953003,
        "learning_rate": 0.0001971373732771928,
        "epoch": 0.6942376619795975,
        "step": 2518
    },
    {
        "loss": 2.2669,
        "grad_norm": 1.5621161460876465,
        "learning_rate": 0.00019707488322977135,
        "epoch": 0.6945133719327268,
        "step": 2519
    },
    {
        "loss": 2.1656,
        "grad_norm": 2.349842071533203,
        "learning_rate": 0.00019701172858062064,
        "epoch": 0.6947890818858561,
        "step": 2520
    },
    {
        "loss": 1.8385,
        "grad_norm": 1.7984085083007812,
        "learning_rate": 0.00019694790976211522,
        "epoch": 0.6950647918389854,
        "step": 2521
    },
    {
        "loss": 1.6908,
        "grad_norm": 1.8914098739624023,
        "learning_rate": 0.0001968834272111763,
        "epoch": 0.6953405017921147,
        "step": 2522
    },
    {
        "loss": 2.1376,
        "grad_norm": 1.5911409854888916,
        "learning_rate": 0.00019681828136926962,
        "epoch": 0.695616211745244,
        "step": 2523
    },
    {
        "loss": 1.9867,
        "grad_norm": 1.6204222440719604,
        "learning_rate": 0.00019675247268240162,
        "epoch": 0.6958919216983733,
        "step": 2524
    },
    {
        "loss": 2.7542,
        "grad_norm": 1.1844996213912964,
        "learning_rate": 0.00019668600160111706,
        "epoch": 0.6961676316515026,
        "step": 2525
    },
    {
        "loss": 1.2381,
        "grad_norm": 2.151625633239746,
        "learning_rate": 0.00019661886858049536,
        "epoch": 0.696443341604632,
        "step": 2526
    },
    {
        "loss": 1.7396,
        "grad_norm": 1.7700846195220947,
        "learning_rate": 0.00019655107408014807,
        "epoch": 0.6967190515577613,
        "step": 2527
    },
    {
        "loss": 1.4035,
        "grad_norm": 2.348294973373413,
        "learning_rate": 0.00019648261856421506,
        "epoch": 0.6969947615108906,
        "step": 2528
    },
    {
        "loss": 1.8286,
        "grad_norm": 1.2046079635620117,
        "learning_rate": 0.0001964135025013621,
        "epoch": 0.6972704714640199,
        "step": 2529
    },
    {
        "loss": 2.1088,
        "grad_norm": 1.6783900260925293,
        "learning_rate": 0.00019634372636477684,
        "epoch": 0.6975461814171492,
        "step": 2530
    },
    {
        "loss": 2.3337,
        "grad_norm": 2.0023515224456787,
        "learning_rate": 0.00019627329063216635,
        "epoch": 0.6978218913702785,
        "step": 2531
    },
    {
        "loss": 2.092,
        "grad_norm": 1.9031816720962524,
        "learning_rate": 0.00019620219578575312,
        "epoch": 0.6980976013234078,
        "step": 2532
    },
    {
        "loss": 2.0052,
        "grad_norm": 2.8964531421661377,
        "learning_rate": 0.00019613044231227248,
        "epoch": 0.6983733112765371,
        "step": 2533
    },
    {
        "loss": 2.2306,
        "grad_norm": 2.2713797092437744,
        "learning_rate": 0.00019605803070296858,
        "epoch": 0.6986490212296664,
        "step": 2534
    },
    {
        "loss": 2.07,
        "grad_norm": 1.9544812440872192,
        "learning_rate": 0.0001959849614535916,
        "epoch": 0.6989247311827957,
        "step": 2535
    },
    {
        "loss": 2.5106,
        "grad_norm": 1.5703635215759277,
        "learning_rate": 0.0001959112350643939,
        "epoch": 0.699200441135925,
        "step": 2536
    },
    {
        "loss": 2.0209,
        "grad_norm": 1.5780694484710693,
        "learning_rate": 0.00019583685204012708,
        "epoch": 0.6994761510890544,
        "step": 2537
    },
    {
        "loss": 1.8289,
        "grad_norm": 2.6570494174957275,
        "learning_rate": 0.00019576181289003783,
        "epoch": 0.6997518610421837,
        "step": 2538
    },
    {
        "loss": 2.3562,
        "grad_norm": 1.0728906393051147,
        "learning_rate": 0.00019568611812786536,
        "epoch": 0.7000275709953129,
        "step": 2539
    },
    {
        "loss": 2.3771,
        "grad_norm": 1.593911051750183,
        "learning_rate": 0.00019560976827183687,
        "epoch": 0.7003032809484422,
        "step": 2540
    },
    {
        "loss": 1.7486,
        "grad_norm": 2.0511412620544434,
        "learning_rate": 0.00019553276384466498,
        "epoch": 0.7005789909015715,
        "step": 2541
    },
    {
        "loss": 2.0963,
        "grad_norm": 0.9767237305641174,
        "learning_rate": 0.00019545510537354336,
        "epoch": 0.7008547008547008,
        "step": 2542
    },
    {
        "loss": 2.2512,
        "grad_norm": 1.703906536102295,
        "learning_rate": 0.00019537679339014368,
        "epoch": 0.7011304108078301,
        "step": 2543
    },
    {
        "loss": 1.8253,
        "grad_norm": 1.750130295753479,
        "learning_rate": 0.00019529782843061147,
        "epoch": 0.7014061207609594,
        "step": 2544
    },
    {
        "loss": 2.0597,
        "grad_norm": 1.2785288095474243,
        "learning_rate": 0.00019521821103556298,
        "epoch": 0.7016818307140887,
        "step": 2545
    },
    {
        "loss": 2.3463,
        "grad_norm": 1.657149076461792,
        "learning_rate": 0.00019513794175008103,
        "epoch": 0.701957540667218,
        "step": 2546
    },
    {
        "loss": 1.2622,
        "grad_norm": 2.7045068740844727,
        "learning_rate": 0.00019505702112371167,
        "epoch": 0.7022332506203474,
        "step": 2547
    },
    {
        "loss": 2.0701,
        "grad_norm": 1.9342113733291626,
        "learning_rate": 0.00019497544971045988,
        "epoch": 0.7025089605734767,
        "step": 2548
    },
    {
        "loss": 1.5827,
        "grad_norm": 2.4947309494018555,
        "learning_rate": 0.00019489322806878656,
        "epoch": 0.702784670526606,
        "step": 2549
    },
    {
        "loss": 2.3276,
        "grad_norm": 1.0073611736297607,
        "learning_rate": 0.00019481035676160375,
        "epoch": 0.7030603804797353,
        "step": 2550
    },
    {
        "loss": 2.1901,
        "grad_norm": 0.9965458512306213,
        "learning_rate": 0.00019472683635627175,
        "epoch": 0.7033360904328646,
        "step": 2551
    },
    {
        "loss": 2.6865,
        "grad_norm": 1.1370360851287842,
        "learning_rate": 0.00019464266742459435,
        "epoch": 0.7036118003859939,
        "step": 2552
    },
    {
        "loss": 2.4867,
        "grad_norm": 1.6509515047073364,
        "learning_rate": 0.00019455785054281574,
        "epoch": 0.7038875103391232,
        "step": 2553
    },
    {
        "loss": 1.6626,
        "grad_norm": 2.2348833084106445,
        "learning_rate": 0.00019447238629161575,
        "epoch": 0.7041632202922525,
        "step": 2554
    },
    {
        "loss": 1.9404,
        "grad_norm": 1.8361281156539917,
        "learning_rate": 0.00019438627525610668,
        "epoch": 0.7044389302453818,
        "step": 2555
    },
    {
        "loss": 2.2207,
        "grad_norm": 1.2084730863571167,
        "learning_rate": 0.0001942995180258285,
        "epoch": 0.7047146401985112,
        "step": 2556
    },
    {
        "loss": 1.9128,
        "grad_norm": 1.3708736896514893,
        "learning_rate": 0.00019421211519474562,
        "epoch": 0.7049903501516405,
        "step": 2557
    },
    {
        "loss": 2.3352,
        "grad_norm": 1.7410178184509277,
        "learning_rate": 0.00019412406736124202,
        "epoch": 0.7052660601047698,
        "step": 2558
    },
    {
        "loss": 1.9425,
        "grad_norm": 1.4174870252609253,
        "learning_rate": 0.0001940353751281179,
        "epoch": 0.7055417700578991,
        "step": 2559
    },
    {
        "loss": 1.998,
        "grad_norm": 2.0883121490478516,
        "learning_rate": 0.00019394603910258482,
        "epoch": 0.7058174800110284,
        "step": 2560
    },
    {
        "loss": 2.6406,
        "grad_norm": 1.3753509521484375,
        "learning_rate": 0.00019385605989626236,
        "epoch": 0.7060931899641577,
        "step": 2561
    },
    {
        "loss": 1.2346,
        "grad_norm": 2.4366888999938965,
        "learning_rate": 0.000193765438125173,
        "epoch": 0.706368899917287,
        "step": 2562
    },
    {
        "loss": 2.0914,
        "grad_norm": 1.825322151184082,
        "learning_rate": 0.00019367417440973895,
        "epoch": 0.7066446098704163,
        "step": 2563
    },
    {
        "loss": 2.0722,
        "grad_norm": 1.9365079402923584,
        "learning_rate": 0.0001935822693747768,
        "epoch": 0.7069203198235456,
        "step": 2564
    },
    {
        "loss": 1.9395,
        "grad_norm": 1.1621770858764648,
        "learning_rate": 0.0001934897236494943,
        "epoch": 0.707196029776675,
        "step": 2565
    },
    {
        "loss": 1.9884,
        "grad_norm": 1.3871469497680664,
        "learning_rate": 0.0001933965378674851,
        "epoch": 0.7074717397298043,
        "step": 2566
    },
    {
        "loss": 2.422,
        "grad_norm": 1.5222569704055786,
        "learning_rate": 0.0001933027126667252,
        "epoch": 0.7077474496829336,
        "step": 2567
    },
    {
        "loss": 2.4305,
        "grad_norm": 1.5692954063415527,
        "learning_rate": 0.00019320824868956796,
        "epoch": 0.7080231596360629,
        "step": 2568
    },
    {
        "loss": 2.2549,
        "grad_norm": 1.928661823272705,
        "learning_rate": 0.00019311314658274032,
        "epoch": 0.7082988695891922,
        "step": 2569
    },
    {
        "loss": 2.3031,
        "grad_norm": 1.1343072652816772,
        "learning_rate": 0.00019301740699733758,
        "epoch": 0.7085745795423215,
        "step": 2570
    },
    {
        "loss": 1.9143,
        "grad_norm": 2.3719723224639893,
        "learning_rate": 0.00019292103058881987,
        "epoch": 0.7088502894954508,
        "step": 2571
    },
    {
        "loss": 2.1878,
        "grad_norm": 1.4379339218139648,
        "learning_rate": 0.00019282401801700675,
        "epoch": 0.7091259994485801,
        "step": 2572
    },
    {
        "loss": 2.2041,
        "grad_norm": 1.4031128883361816,
        "learning_rate": 0.0001927263699460736,
        "epoch": 0.7094017094017094,
        "step": 2573
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.7485897541046143,
        "learning_rate": 0.0001926280870445461,
        "epoch": 0.7096774193548387,
        "step": 2574
    },
    {
        "loss": 1.9209,
        "grad_norm": 1.6751152276992798,
        "learning_rate": 0.00019252916998529663,
        "epoch": 0.7099531293079681,
        "step": 2575
    },
    {
        "loss": 2.3424,
        "grad_norm": 1.1918119192123413,
        "learning_rate": 0.0001924296194455388,
        "epoch": 0.7102288392610974,
        "step": 2576
    },
    {
        "loss": 2.0226,
        "grad_norm": 1.6734015941619873,
        "learning_rate": 0.0001923294361068235,
        "epoch": 0.7105045492142267,
        "step": 2577
    },
    {
        "loss": 1.5522,
        "grad_norm": 2.225219964981079,
        "learning_rate": 0.00019222862065503368,
        "epoch": 0.710780259167356,
        "step": 2578
    },
    {
        "loss": 1.9509,
        "grad_norm": 1.5459336042404175,
        "learning_rate": 0.00019212717378038022,
        "epoch": 0.7110559691204853,
        "step": 2579
    },
    {
        "loss": 1.0191,
        "grad_norm": 0.9130048155784607,
        "learning_rate": 0.00019202509617739652,
        "epoch": 0.7113316790736146,
        "step": 2580
    },
    {
        "loss": 2.1052,
        "grad_norm": 1.4361714124679565,
        "learning_rate": 0.0001919223885449345,
        "epoch": 0.7116073890267439,
        "step": 2581
    },
    {
        "loss": 2.0867,
        "grad_norm": 1.341713786125183,
        "learning_rate": 0.00019181905158615915,
        "epoch": 0.7118830989798731,
        "step": 2582
    },
    {
        "loss": 1.6122,
        "grad_norm": 1.9526556730270386,
        "learning_rate": 0.00019171508600854422,
        "epoch": 0.7121588089330024,
        "step": 2583
    },
    {
        "loss": 2.0624,
        "grad_norm": 1.3091062307357788,
        "learning_rate": 0.00019161049252386687,
        "epoch": 0.7124345188861317,
        "step": 2584
    },
    {
        "loss": 2.2072,
        "grad_norm": 2.8487021923065186,
        "learning_rate": 0.00019150527184820352,
        "epoch": 0.712710228839261,
        "step": 2585
    },
    {
        "loss": 1.2044,
        "grad_norm": 1.1294562816619873,
        "learning_rate": 0.00019139942470192402,
        "epoch": 0.7129859387923904,
        "step": 2586
    },
    {
        "loss": 2.5082,
        "grad_norm": 1.227900743484497,
        "learning_rate": 0.0001912929518096877,
        "epoch": 0.7132616487455197,
        "step": 2587
    },
    {
        "loss": 2.2479,
        "grad_norm": 1.7858235836029053,
        "learning_rate": 0.0001911858539004374,
        "epoch": 0.713537358698649,
        "step": 2588
    },
    {
        "loss": 2.1573,
        "grad_norm": 1.903956413269043,
        "learning_rate": 0.0001910781317073956,
        "epoch": 0.7138130686517783,
        "step": 2589
    },
    {
        "loss": 2.3828,
        "grad_norm": 1.6959773302078247,
        "learning_rate": 0.0001909697859680582,
        "epoch": 0.7140887786049076,
        "step": 2590
    },
    {
        "loss": 2.5088,
        "grad_norm": 1.0514194965362549,
        "learning_rate": 0.00019086081742419056,
        "epoch": 0.7143644885580369,
        "step": 2591
    },
    {
        "loss": 1.9153,
        "grad_norm": 1.2262547016143799,
        "learning_rate": 0.00019075122682182154,
        "epoch": 0.7146401985111662,
        "step": 2592
    },
    {
        "loss": 2.3791,
        "grad_norm": 1.13846755027771,
        "learning_rate": 0.0001906410149112392,
        "epoch": 0.7149159084642955,
        "step": 2593
    },
    {
        "loss": 2.4402,
        "grad_norm": 1.6630502939224243,
        "learning_rate": 0.00019053018244698477,
        "epoch": 0.7151916184174248,
        "step": 2594
    },
    {
        "loss": 2.3845,
        "grad_norm": 1.3490662574768066,
        "learning_rate": 0.00019041873018784848,
        "epoch": 0.7154673283705542,
        "step": 2595
    },
    {
        "loss": 2.0637,
        "grad_norm": 2.0709939002990723,
        "learning_rate": 0.00019030665889686332,
        "epoch": 0.7157430383236835,
        "step": 2596
    },
    {
        "loss": 1.8818,
        "grad_norm": 1.2536503076553345,
        "learning_rate": 0.00019019396934130084,
        "epoch": 0.7160187482768128,
        "step": 2597
    },
    {
        "loss": 0.6817,
        "grad_norm": 4.850387096405029,
        "learning_rate": 0.000190080662292665,
        "epoch": 0.7162944582299421,
        "step": 2598
    },
    {
        "loss": 1.8011,
        "grad_norm": 2.2099268436431885,
        "learning_rate": 0.00018996673852668763,
        "epoch": 0.7165701681830714,
        "step": 2599
    },
    {
        "loss": 1.7906,
        "grad_norm": 1.5057827234268188,
        "learning_rate": 0.00018985219882332237,
        "epoch": 0.7168458781362007,
        "step": 2600
    },
    {
        "loss": 1.6735,
        "grad_norm": 1.86939537525177,
        "learning_rate": 0.00018973704396674026,
        "epoch": 0.71712158808933,
        "step": 2601
    },
    {
        "loss": 2.558,
        "grad_norm": 1.130256175994873,
        "learning_rate": 0.00018962127474532329,
        "epoch": 0.7173972980424593,
        "step": 2602
    },
    {
        "loss": 1.8216,
        "grad_norm": 1.820133924484253,
        "learning_rate": 0.00018950489195166013,
        "epoch": 0.7176730079955886,
        "step": 2603
    },
    {
        "loss": 2.093,
        "grad_norm": 1.8967137336730957,
        "learning_rate": 0.0001893878963825396,
        "epoch": 0.717948717948718,
        "step": 2604
    },
    {
        "loss": 2.2167,
        "grad_norm": 2.5662500858306885,
        "learning_rate": 0.0001892702888389462,
        "epoch": 0.7182244279018473,
        "step": 2605
    },
    {
        "loss": 1.6267,
        "grad_norm": 2.4360463619232178,
        "learning_rate": 0.00018915207012605375,
        "epoch": 0.7185001378549766,
        "step": 2606
    },
    {
        "loss": 1.8004,
        "grad_norm": 1.327064037322998,
        "learning_rate": 0.00018903324105322083,
        "epoch": 0.7187758478081059,
        "step": 2607
    },
    {
        "loss": 2.3899,
        "grad_norm": 1.1877894401550293,
        "learning_rate": 0.00018891380243398414,
        "epoch": 0.7190515577612352,
        "step": 2608
    },
    {
        "loss": 2.5782,
        "grad_norm": 1.8303683996200562,
        "learning_rate": 0.000188793755086054,
        "epoch": 0.7193272677143645,
        "step": 2609
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.8847756385803223,
        "learning_rate": 0.00018867309983130784,
        "epoch": 0.7196029776674938,
        "step": 2610
    },
    {
        "loss": 2.227,
        "grad_norm": 1.4886430501937866,
        "learning_rate": 0.00018855183749578539,
        "epoch": 0.7198786876206231,
        "step": 2611
    },
    {
        "loss": 2.0957,
        "grad_norm": 1.1433864831924438,
        "learning_rate": 0.00018842996890968224,
        "epoch": 0.7201543975737524,
        "step": 2612
    },
    {
        "loss": 2.1027,
        "grad_norm": 1.452540397644043,
        "learning_rate": 0.00018830749490734495,
        "epoch": 0.7204301075268817,
        "step": 2613
    },
    {
        "loss": 2.2871,
        "grad_norm": 0.9283888339996338,
        "learning_rate": 0.0001881844163272644,
        "epoch": 0.7207058174800111,
        "step": 2614
    },
    {
        "loss": 2.3907,
        "grad_norm": 1.6266144514083862,
        "learning_rate": 0.00018806073401207122,
        "epoch": 0.7209815274331404,
        "step": 2615
    },
    {
        "loss": 2.1074,
        "grad_norm": 1.067669153213501,
        "learning_rate": 0.0001879364488085287,
        "epoch": 0.7212572373862697,
        "step": 2616
    },
    {
        "loss": 2.5803,
        "grad_norm": 1.243517279624939,
        "learning_rate": 0.00018781156156752834,
        "epoch": 0.721532947339399,
        "step": 2617
    },
    {
        "loss": 2.6247,
        "grad_norm": 1.8600804805755615,
        "learning_rate": 0.00018768607314408273,
        "epoch": 0.7218086572925283,
        "step": 2618
    },
    {
        "loss": 1.3402,
        "grad_norm": 1.9425714015960693,
        "learning_rate": 0.0001875599843973209,
        "epoch": 0.7220843672456576,
        "step": 2619
    },
    {
        "loss": 1.7812,
        "grad_norm": 2.024888038635254,
        "learning_rate": 0.00018743329619048133,
        "epoch": 0.7223600771987869,
        "step": 2620
    },
    {
        "loss": 2.386,
        "grad_norm": 1.2684248685836792,
        "learning_rate": 0.00018730600939090706,
        "epoch": 0.7226357871519162,
        "step": 2621
    },
    {
        "loss": 1.8787,
        "grad_norm": 2.685455799102783,
        "learning_rate": 0.00018717812487003876,
        "epoch": 0.7229114971050455,
        "step": 2622
    },
    {
        "loss": 2.344,
        "grad_norm": 1.7924944162368774,
        "learning_rate": 0.00018704964350340982,
        "epoch": 0.7231872070581749,
        "step": 2623
    },
    {
        "loss": 2.4581,
        "grad_norm": 1.9046329259872437,
        "learning_rate": 0.00018692056617063924,
        "epoch": 0.7234629170113042,
        "step": 2624
    },
    {
        "loss": 2.4392,
        "grad_norm": 1.2315850257873535,
        "learning_rate": 0.0001867908937554267,
        "epoch": 0.7237386269644334,
        "step": 2625
    },
    {
        "loss": 2.2606,
        "grad_norm": 1.2568325996398926,
        "learning_rate": 0.00018666062714554546,
        "epoch": 0.7240143369175627,
        "step": 2626
    },
    {
        "loss": 2.3273,
        "grad_norm": 1.3910351991653442,
        "learning_rate": 0.0001865297672328374,
        "epoch": 0.724290046870692,
        "step": 2627
    },
    {
        "loss": 2.328,
        "grad_norm": 1.9040248394012451,
        "learning_rate": 0.0001863983149132056,
        "epoch": 0.7245657568238213,
        "step": 2628
    },
    {
        "loss": 2.5045,
        "grad_norm": 1.5796407461166382,
        "learning_rate": 0.00018626627108660966,
        "epoch": 0.7248414667769506,
        "step": 2629
    },
    {
        "loss": 1.9058,
        "grad_norm": 1.3823374509811401,
        "learning_rate": 0.00018613363665705816,
        "epoch": 0.7251171767300799,
        "step": 2630
    },
    {
        "loss": 2.2571,
        "grad_norm": 1.094316005706787,
        "learning_rate": 0.00018600041253260367,
        "epoch": 0.7253928866832092,
        "step": 2631
    },
    {
        "loss": 1.9365,
        "grad_norm": 1.1970294713974,
        "learning_rate": 0.00018586659962533547,
        "epoch": 0.7256685966363385,
        "step": 2632
    },
    {
        "loss": 1.6531,
        "grad_norm": 2.174299955368042,
        "learning_rate": 0.00018573219885137428,
        "epoch": 0.7259443065894678,
        "step": 2633
    },
    {
        "loss": 2.4097,
        "grad_norm": 1.1249840259552002,
        "learning_rate": 0.00018559721113086498,
        "epoch": 0.7262200165425972,
        "step": 2634
    },
    {
        "loss": 2.2738,
        "grad_norm": 0.9322595000267029,
        "learning_rate": 0.00018546163738797152,
        "epoch": 0.7264957264957265,
        "step": 2635
    },
    {
        "loss": 2.1227,
        "grad_norm": 1.8280460834503174,
        "learning_rate": 0.00018532547855086925,
        "epoch": 0.7267714364488558,
        "step": 2636
    },
    {
        "loss": 1.9845,
        "grad_norm": 1.7452998161315918,
        "learning_rate": 0.00018518873555173983,
        "epoch": 0.7270471464019851,
        "step": 2637
    },
    {
        "loss": 1.8239,
        "grad_norm": 1.803125262260437,
        "learning_rate": 0.0001850514093267637,
        "epoch": 0.7273228563551144,
        "step": 2638
    },
    {
        "loss": 2.3118,
        "grad_norm": 1.6275638341903687,
        "learning_rate": 0.00018491350081611474,
        "epoch": 0.7275985663082437,
        "step": 2639
    },
    {
        "loss": 2.5601,
        "grad_norm": 1.303330898284912,
        "learning_rate": 0.00018477501096395287,
        "epoch": 0.727874276261373,
        "step": 2640
    },
    {
        "loss": 2.2564,
        "grad_norm": 1.1482126712799072,
        "learning_rate": 0.00018463594071841847,
        "epoch": 0.7281499862145023,
        "step": 2641
    },
    {
        "loss": 1.9493,
        "grad_norm": 2.042407989501953,
        "learning_rate": 0.000184496291031625,
        "epoch": 0.7284256961676316,
        "step": 2642
    },
    {
        "loss": 2.4408,
        "grad_norm": 1.575332760810852,
        "learning_rate": 0.00018435606285965338,
        "epoch": 0.728701406120761,
        "step": 2643
    },
    {
        "loss": 2.0855,
        "grad_norm": 1.6431597471237183,
        "learning_rate": 0.0001842152571625446,
        "epoch": 0.7289771160738903,
        "step": 2644
    },
    {
        "loss": 2.5688,
        "grad_norm": 1.2338168621063232,
        "learning_rate": 0.00018407387490429396,
        "epoch": 0.7292528260270196,
        "step": 2645
    },
    {
        "loss": 1.7677,
        "grad_norm": 2.114781379699707,
        "learning_rate": 0.0001839319170528436,
        "epoch": 0.7295285359801489,
        "step": 2646
    },
    {
        "loss": 2.4922,
        "grad_norm": 1.4812240600585938,
        "learning_rate": 0.00018378938458007683,
        "epoch": 0.7298042459332782,
        "step": 2647
    },
    {
        "loss": 1.8773,
        "grad_norm": 2.0773680210113525,
        "learning_rate": 0.00018364627846181042,
        "epoch": 0.7300799558864075,
        "step": 2648
    },
    {
        "loss": 1.8768,
        "grad_norm": 1.5845376253128052,
        "learning_rate": 0.00018350259967778908,
        "epoch": 0.7303556658395368,
        "step": 2649
    },
    {
        "loss": 2.3917,
        "grad_norm": 1.030220627784729,
        "learning_rate": 0.00018335834921167753,
        "epoch": 0.7306313757926661,
        "step": 2650
    },
    {
        "loss": 2.076,
        "grad_norm": 1.0472708940505981,
        "learning_rate": 0.00018321352805105503,
        "epoch": 0.7309070857457954,
        "step": 2651
    },
    {
        "loss": 1.2992,
        "grad_norm": 2.485778331756592,
        "learning_rate": 0.0001830681371874073,
        "epoch": 0.7311827956989247,
        "step": 2652
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.6930646896362305,
        "learning_rate": 0.0001829221776161211,
        "epoch": 0.7314585056520541,
        "step": 2653
    },
    {
        "loss": 2.1501,
        "grad_norm": 1.6027824878692627,
        "learning_rate": 0.00018277565033647606,
        "epoch": 0.7317342156051834,
        "step": 2654
    },
    {
        "loss": 1.95,
        "grad_norm": 2.0150630474090576,
        "learning_rate": 0.00018262855635163905,
        "epoch": 0.7320099255583127,
        "step": 2655
    },
    {
        "loss": 1.4671,
        "grad_norm": 1.6587672233581543,
        "learning_rate": 0.0001824808966686563,
        "epoch": 0.732285635511442,
        "step": 2656
    },
    {
        "loss": 2.1267,
        "grad_norm": 1.592555284500122,
        "learning_rate": 0.00018233267229844744,
        "epoch": 0.7325613454645713,
        "step": 2657
    },
    {
        "loss": 2.1162,
        "grad_norm": 1.3859033584594727,
        "learning_rate": 0.00018218388425579756,
        "epoch": 0.7328370554177006,
        "step": 2658
    },
    {
        "loss": 2.0034,
        "grad_norm": 2.4367363452911377,
        "learning_rate": 0.0001820345335593514,
        "epoch": 0.7331127653708299,
        "step": 2659
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.5478320121765137,
        "learning_rate": 0.00018188462123160522,
        "epoch": 0.7333884753239592,
        "step": 2660
    },
    {
        "loss": 1.8823,
        "grad_norm": 1.5810476541519165,
        "learning_rate": 0.00018173414829890093,
        "epoch": 0.7336641852770885,
        "step": 2661
    },
    {
        "loss": 2.657,
        "grad_norm": 1.5476990938186646,
        "learning_rate": 0.00018158311579141785,
        "epoch": 0.7339398952302179,
        "step": 2662
    },
    {
        "loss": 1.9827,
        "grad_norm": 1.8664664030075073,
        "learning_rate": 0.00018143152474316701,
        "epoch": 0.7342156051833472,
        "step": 2663
    },
    {
        "loss": 2.146,
        "grad_norm": 2.351194143295288,
        "learning_rate": 0.00018127937619198266,
        "epoch": 0.7344913151364765,
        "step": 2664
    },
    {
        "loss": 2.003,
        "grad_norm": 1.5414471626281738,
        "learning_rate": 0.00018112667117951657,
        "epoch": 0.7347670250896058,
        "step": 2665
    },
    {
        "loss": 1.5751,
        "grad_norm": 1.9483836889266968,
        "learning_rate": 0.00018097341075122954,
        "epoch": 0.7350427350427351,
        "step": 2666
    },
    {
        "loss": 2.1983,
        "grad_norm": 1.6721808910369873,
        "learning_rate": 0.00018081959595638554,
        "epoch": 0.7353184449958643,
        "step": 2667
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.403275728225708,
        "learning_rate": 0.0001806652278480432,
        "epoch": 0.7355941549489936,
        "step": 2668
    },
    {
        "loss": 1.9839,
        "grad_norm": 1.4913033246994019,
        "learning_rate": 0.00018051030748305,
        "epoch": 0.7358698649021229,
        "step": 2669
    },
    {
        "loss": 2.2802,
        "grad_norm": 1.3067052364349365,
        "learning_rate": 0.0001803548359220336,
        "epoch": 0.7361455748552522,
        "step": 2670
    },
    {
        "loss": 1.7591,
        "grad_norm": 1.4215322732925415,
        "learning_rate": 0.00018019881422939608,
        "epoch": 0.7364212848083815,
        "step": 2671
    },
    {
        "loss": 2.3865,
        "grad_norm": 1.5476397275924683,
        "learning_rate": 0.00018004224347330512,
        "epoch": 0.7366969947615108,
        "step": 2672
    },
    {
        "loss": 1.8313,
        "grad_norm": 1.518896460533142,
        "learning_rate": 0.00017988512472568807,
        "epoch": 0.7369727047146402,
        "step": 2673
    },
    {
        "loss": 1.9836,
        "grad_norm": 1.502280354499817,
        "learning_rate": 0.00017972745906222342,
        "epoch": 0.7372484146677695,
        "step": 2674
    },
    {
        "loss": 1.7096,
        "grad_norm": 2.195765495300293,
        "learning_rate": 0.00017956924756233462,
        "epoch": 0.7375241246208988,
        "step": 2675
    },
    {
        "loss": 1.9238,
        "grad_norm": 1.8538577556610107,
        "learning_rate": 0.00017941049130918135,
        "epoch": 0.7377998345740281,
        "step": 2676
    },
    {
        "loss": 1.011,
        "grad_norm": 1.1962388753890991,
        "learning_rate": 0.00017925119138965356,
        "epoch": 0.7380755445271574,
        "step": 2677
    },
    {
        "loss": 2.1102,
        "grad_norm": 1.5507131814956665,
        "learning_rate": 0.00017909134889436263,
        "epoch": 0.7383512544802867,
        "step": 2678
    },
    {
        "loss": 2.2938,
        "grad_norm": 1.2390108108520508,
        "learning_rate": 0.00017893096491763518,
        "epoch": 0.738626964433416,
        "step": 2679
    },
    {
        "loss": 1.7663,
        "grad_norm": 1.4049967527389526,
        "learning_rate": 0.0001787700405575043,
        "epoch": 0.7389026743865453,
        "step": 2680
    },
    {
        "loss": 1.3865,
        "grad_norm": 2.41259765625,
        "learning_rate": 0.00017860857691570344,
        "epoch": 0.7391783843396746,
        "step": 2681
    },
    {
        "loss": 2.2148,
        "grad_norm": 1.6379200220108032,
        "learning_rate": 0.00017844657509765734,
        "epoch": 0.739454094292804,
        "step": 2682
    },
    {
        "loss": 1.9972,
        "grad_norm": 1.9553899765014648,
        "learning_rate": 0.000178284036212476,
        "epoch": 0.7397298042459333,
        "step": 2683
    },
    {
        "loss": 2.6917,
        "grad_norm": 1.4642952680587769,
        "learning_rate": 0.0001781209613729456,
        "epoch": 0.7400055141990626,
        "step": 2684
    },
    {
        "loss": 2.3898,
        "grad_norm": 1.0978500843048096,
        "learning_rate": 0.00017795735169552233,
        "epoch": 0.7402812241521919,
        "step": 2685
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.902461290359497,
        "learning_rate": 0.00017779320830032334,
        "epoch": 0.7405569341053212,
        "step": 2686
    },
    {
        "loss": 2.0007,
        "grad_norm": 2.3530776500701904,
        "learning_rate": 0.00017762853231112052,
        "epoch": 0.7408326440584505,
        "step": 2687
    },
    {
        "loss": 1.814,
        "grad_norm": 2.5359816551208496,
        "learning_rate": 0.00017746332485533116,
        "epoch": 0.7411083540115798,
        "step": 2688
    },
    {
        "loss": 1.4279,
        "grad_norm": 1.9719538688659668,
        "learning_rate": 0.00017729758706401202,
        "epoch": 0.7413840639647091,
        "step": 2689
    },
    {
        "loss": 2.004,
        "grad_norm": 1.7557168006896973,
        "learning_rate": 0.00017713132007184985,
        "epoch": 0.7416597739178384,
        "step": 2690
    },
    {
        "loss": 2.3541,
        "grad_norm": 1.5060631036758423,
        "learning_rate": 0.00017696452501715523,
        "epoch": 0.7419354838709677,
        "step": 2691
    },
    {
        "loss": 2.2996,
        "grad_norm": 1.651301383972168,
        "learning_rate": 0.0001767972030418533,
        "epoch": 0.7422111938240971,
        "step": 2692
    },
    {
        "loss": 1.3808,
        "grad_norm": 1.9608831405639648,
        "learning_rate": 0.0001766293552914773,
        "epoch": 0.7424869037772264,
        "step": 2693
    },
    {
        "loss": 1.939,
        "grad_norm": 1.9956375360488892,
        "learning_rate": 0.00017646098291515937,
        "epoch": 0.7427626137303557,
        "step": 2694
    },
    {
        "loss": 2.1243,
        "grad_norm": 1.3156923055648804,
        "learning_rate": 0.00017629208706562417,
        "epoch": 0.743038323683485,
        "step": 2695
    },
    {
        "loss": 2.7638,
        "grad_norm": 1.4723141193389893,
        "learning_rate": 0.00017612266889917945,
        "epoch": 0.7433140336366143,
        "step": 2696
    },
    {
        "loss": 1.7138,
        "grad_norm": 1.9660656452178955,
        "learning_rate": 0.00017595272957570955,
        "epoch": 0.7435897435897436,
        "step": 2697
    },
    {
        "loss": 1.7246,
        "grad_norm": 1.607312560081482,
        "learning_rate": 0.00017578227025866615,
        "epoch": 0.7438654535428729,
        "step": 2698
    },
    {
        "loss": 2.037,
        "grad_norm": 1.6454137563705444,
        "learning_rate": 0.00017561129211506163,
        "epoch": 0.7441411634960022,
        "step": 2699
    },
    {
        "loss": 1.6454,
        "grad_norm": 1.8721919059753418,
        "learning_rate": 0.00017543979631545967,
        "epoch": 0.7444168734491315,
        "step": 2700
    },
    {
        "loss": 1.5922,
        "grad_norm": 1.754266619682312,
        "learning_rate": 0.0001752677840339688,
        "epoch": 0.7446925834022609,
        "step": 2701
    },
    {
        "loss": 2.3859,
        "grad_norm": 1.8149445056915283,
        "learning_rate": 0.0001750952564482326,
        "epoch": 0.7449682933553902,
        "step": 2702
    },
    {
        "loss": 2.199,
        "grad_norm": 2.153804063796997,
        "learning_rate": 0.00017492221473942355,
        "epoch": 0.7452440033085195,
        "step": 2703
    },
    {
        "loss": 2.4479,
        "grad_norm": 1.435404658317566,
        "learning_rate": 0.00017474866009223295,
        "epoch": 0.7455197132616488,
        "step": 2704
    },
    {
        "loss": 1.5262,
        "grad_norm": 1.7582696676254272,
        "learning_rate": 0.0001745745936948648,
        "epoch": 0.7457954232147781,
        "step": 2705
    },
    {
        "loss": 2.3998,
        "grad_norm": 1.5661433935165405,
        "learning_rate": 0.00017440001673902578,
        "epoch": 0.7460711331679074,
        "step": 2706
    },
    {
        "loss": 2.0352,
        "grad_norm": 2.1166019439697266,
        "learning_rate": 0.00017422493041991882,
        "epoch": 0.7463468431210367,
        "step": 2707
    },
    {
        "loss": 1.6002,
        "grad_norm": 2.4130890369415283,
        "learning_rate": 0.00017404933593623335,
        "epoch": 0.746622553074166,
        "step": 2708
    },
    {
        "loss": 2.3264,
        "grad_norm": 1.4892786741256714,
        "learning_rate": 0.0001738732344901385,
        "epoch": 0.7468982630272953,
        "step": 2709
    },
    {
        "loss": 2.2478,
        "grad_norm": 1.4967232942581177,
        "learning_rate": 0.0001736966272872736,
        "epoch": 0.7471739729804245,
        "step": 2710
    },
    {
        "loss": 2.0532,
        "grad_norm": 1.8498506546020508,
        "learning_rate": 0.00017351951553674114,
        "epoch": 0.7474496829335538,
        "step": 2711
    },
    {
        "loss": 2.2753,
        "grad_norm": 2.374305486679077,
        "learning_rate": 0.00017334190045109718,
        "epoch": 0.7477253928866832,
        "step": 2712
    },
    {
        "loss": 2.4713,
        "grad_norm": 1.9127612113952637,
        "learning_rate": 0.0001731637832463446,
        "epoch": 0.7480011028398125,
        "step": 2713
    },
    {
        "loss": 1.1518,
        "grad_norm": 2.1162798404693604,
        "learning_rate": 0.000172985165141923,
        "epoch": 0.7482768127929418,
        "step": 2714
    },
    {
        "loss": 2.3488,
        "grad_norm": 1.8061429262161255,
        "learning_rate": 0.00017280604736070215,
        "epoch": 0.7485525227460711,
        "step": 2715
    },
    {
        "loss": 2.1314,
        "grad_norm": 2.1174638271331787,
        "learning_rate": 0.00017262643112897198,
        "epoch": 0.7488282326992004,
        "step": 2716
    },
    {
        "loss": 2.3144,
        "grad_norm": 1.5139790773391724,
        "learning_rate": 0.00017244631767643573,
        "epoch": 0.7491039426523297,
        "step": 2717
    },
    {
        "loss": 2.2296,
        "grad_norm": 1.0919305086135864,
        "learning_rate": 0.0001722657082361999,
        "epoch": 0.749379652605459,
        "step": 2718
    },
    {
        "loss": 2.6022,
        "grad_norm": 1.1774438619613647,
        "learning_rate": 0.0001720846040447675,
        "epoch": 0.7496553625585883,
        "step": 2719
    },
    {
        "loss": 2.1513,
        "grad_norm": 1.3731143474578857,
        "learning_rate": 0.00017190300634202797,
        "epoch": 0.7499310725117176,
        "step": 2720
    },
    {
        "loss": 1.9137,
        "grad_norm": 2.1759707927703857,
        "learning_rate": 0.00017172091637125022,
        "epoch": 0.750206782464847,
        "step": 2721
    },
    {
        "loss": 2.1837,
        "grad_norm": 1.0507384538650513,
        "learning_rate": 0.00017153833537907257,
        "epoch": 0.7504824924179763,
        "step": 2722
    },
    {
        "loss": 2.2806,
        "grad_norm": 1.2833133935928345,
        "learning_rate": 0.00017135526461549584,
        "epoch": 0.7507582023711056,
        "step": 2723
    },
    {
        "loss": 2.547,
        "grad_norm": 1.5938791036605835,
        "learning_rate": 0.00017117170533387314,
        "epoch": 0.7510339123242349,
        "step": 2724
    },
    {
        "loss": 2.0309,
        "grad_norm": 1.693514108657837,
        "learning_rate": 0.0001709876587909029,
        "epoch": 0.7513096222773642,
        "step": 2725
    },
    {
        "loss": 2.326,
        "grad_norm": 1.2080554962158203,
        "learning_rate": 0.00017080312624661873,
        "epoch": 0.7515853322304935,
        "step": 2726
    },
    {
        "loss": 2.0169,
        "grad_norm": 1.9412823915481567,
        "learning_rate": 0.00017061810896438232,
        "epoch": 0.7518610421836228,
        "step": 2727
    },
    {
        "loss": 2.42,
        "grad_norm": 1.385564923286438,
        "learning_rate": 0.00017043260821087311,
        "epoch": 0.7521367521367521,
        "step": 2728
    },
    {
        "loss": 1.6743,
        "grad_norm": 2.687653064727783,
        "learning_rate": 0.0001702466252560814,
        "epoch": 0.7524124620898814,
        "step": 2729
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.7744202613830566,
        "learning_rate": 0.0001700601613732981,
        "epoch": 0.7526881720430108,
        "step": 2730
    },
    {
        "loss": 2.0535,
        "grad_norm": 1.6544361114501953,
        "learning_rate": 0.00016987321783910729,
        "epoch": 0.7529638819961401,
        "step": 2731
    },
    {
        "loss": 1.3769,
        "grad_norm": 2.420827627182007,
        "learning_rate": 0.0001696857959333761,
        "epoch": 0.7532395919492694,
        "step": 2732
    },
    {
        "loss": 1.7162,
        "grad_norm": 2.095207929611206,
        "learning_rate": 0.00016949789693924765,
        "epoch": 0.7535153019023987,
        "step": 2733
    },
    {
        "loss": 1.7324,
        "grad_norm": 1.9797108173370361,
        "learning_rate": 0.00016930952214313044,
        "epoch": 0.753791011855528,
        "step": 2734
    },
    {
        "loss": 2.1956,
        "grad_norm": 1.4422616958618164,
        "learning_rate": 0.00016912067283469132,
        "epoch": 0.7540667218086573,
        "step": 2735
    },
    {
        "loss": 2.4363,
        "grad_norm": 1.563610315322876,
        "learning_rate": 0.00016893135030684485,
        "epoch": 0.7543424317617866,
        "step": 2736
    },
    {
        "loss": 2.2469,
        "grad_norm": 1.9275511503219604,
        "learning_rate": 0.0001687415558557463,
        "epoch": 0.7546181417149159,
        "step": 2737
    },
    {
        "loss": 2.32,
        "grad_norm": 1.5095196962356567,
        "learning_rate": 0.00016855129078078104,
        "epoch": 0.7548938516680452,
        "step": 2738
    },
    {
        "loss": 2.1813,
        "grad_norm": 1.5853677988052368,
        "learning_rate": 0.00016836055638455726,
        "epoch": 0.7551695616211745,
        "step": 2739
    },
    {
        "loss": 2.489,
        "grad_norm": 1.7678580284118652,
        "learning_rate": 0.00016816935397289542,
        "epoch": 0.7554452715743039,
        "step": 2740
    },
    {
        "loss": 2.4582,
        "grad_norm": 1.0593987703323364,
        "learning_rate": 0.00016797768485482095,
        "epoch": 0.7557209815274332,
        "step": 2741
    },
    {
        "loss": 2.181,
        "grad_norm": 1.0154551267623901,
        "learning_rate": 0.0001677855503425537,
        "epoch": 0.7559966914805625,
        "step": 2742
    },
    {
        "loss": 1.88,
        "grad_norm": 1.9595599174499512,
        "learning_rate": 0.00016759295175150057,
        "epoch": 0.7562724014336918,
        "step": 2743
    },
    {
        "loss": 2.2051,
        "grad_norm": 1.2698373794555664,
        "learning_rate": 0.00016739989040024481,
        "epoch": 0.7565481113868211,
        "step": 2744
    },
    {
        "loss": 1.7352,
        "grad_norm": 1.5410445928573608,
        "learning_rate": 0.0001672063676105387,
        "epoch": 0.7568238213399504,
        "step": 2745
    },
    {
        "loss": 2.134,
        "grad_norm": 2.5446770191192627,
        "learning_rate": 0.00016701238470729282,
        "epoch": 0.7570995312930797,
        "step": 2746
    },
    {
        "loss": 2.5853,
        "grad_norm": 1.502730131149292,
        "learning_rate": 0.00016681794301856865,
        "epoch": 0.757375241246209,
        "step": 2747
    },
    {
        "loss": 1.9177,
        "grad_norm": 1.5789190530776978,
        "learning_rate": 0.00016662304387556783,
        "epoch": 0.7576509511993383,
        "step": 2748
    },
    {
        "loss": 2.6226,
        "grad_norm": 1.0550895929336548,
        "learning_rate": 0.00016642768861262466,
        "epoch": 0.7579266611524677,
        "step": 2749
    },
    {
        "loss": 2.3495,
        "grad_norm": 1.1792471408843994,
        "learning_rate": 0.00016623187856719534,
        "epoch": 0.758202371105597,
        "step": 2750
    },
    {
        "loss": 1.8326,
        "grad_norm": 1.9737273454666138,
        "learning_rate": 0.0001660356150798505,
        "epoch": 0.7584780810587263,
        "step": 2751
    },
    {
        "loss": 2.6224,
        "grad_norm": 1.4019060134887695,
        "learning_rate": 0.0001658388994942643,
        "epoch": 0.7587537910118556,
        "step": 2752
    },
    {
        "loss": 1.9491,
        "grad_norm": 1.9180494546890259,
        "learning_rate": 0.00016564173315720694,
        "epoch": 0.7590295009649848,
        "step": 2753
    },
    {
        "loss": 2.1141,
        "grad_norm": 1.7448930740356445,
        "learning_rate": 0.00016544411741853366,
        "epoch": 0.7593052109181141,
        "step": 2754
    },
    {
        "loss": 1.7422,
        "grad_norm": 1.0994014739990234,
        "learning_rate": 0.00016524605363117746,
        "epoch": 0.7595809208712434,
        "step": 2755
    },
    {
        "loss": 2.2182,
        "grad_norm": 1.6104010343551636,
        "learning_rate": 0.00016504754315113784,
        "epoch": 0.7598566308243727,
        "step": 2756
    },
    {
        "loss": 2.5683,
        "grad_norm": 1.2098397016525269,
        "learning_rate": 0.0001648485873374733,
        "epoch": 0.760132340777502,
        "step": 2757
    },
    {
        "loss": 1.5576,
        "grad_norm": 1.7848803997039795,
        "learning_rate": 0.0001646491875522904,
        "epoch": 0.7604080507306313,
        "step": 2758
    },
    {
        "loss": 1.273,
        "grad_norm": 2.2062339782714844,
        "learning_rate": 0.0001644493451607362,
        "epoch": 0.7606837606837606,
        "step": 2759
    },
    {
        "loss": 2.2989,
        "grad_norm": 1.4915635585784912,
        "learning_rate": 0.00016424906153098695,
        "epoch": 0.76095947063689,
        "step": 2760
    },
    {
        "loss": 2.6374,
        "grad_norm": 1.3394629955291748,
        "learning_rate": 0.0001640483380342407,
        "epoch": 0.7612351805900193,
        "step": 2761
    },
    {
        "loss": 2.2474,
        "grad_norm": 1.4254204034805298,
        "learning_rate": 0.00016384717604470608,
        "epoch": 0.7615108905431486,
        "step": 2762
    },
    {
        "loss": 1.8166,
        "grad_norm": 1.4148085117340088,
        "learning_rate": 0.0001636455769395946,
        "epoch": 0.7617866004962779,
        "step": 2763
    },
    {
        "loss": 2.4052,
        "grad_norm": 1.2640793323516846,
        "learning_rate": 0.00016344354209910954,
        "epoch": 0.7620623104494072,
        "step": 2764
    },
    {
        "loss": 2.104,
        "grad_norm": 1.649990439414978,
        "learning_rate": 0.00016324107290643824,
        "epoch": 0.7623380204025365,
        "step": 2765
    },
    {
        "loss": 2.5299,
        "grad_norm": 1.0011398792266846,
        "learning_rate": 0.00016303817074774077,
        "epoch": 0.7626137303556658,
        "step": 2766
    },
    {
        "loss": 2.0132,
        "grad_norm": 1.9673364162445068,
        "learning_rate": 0.00016283483701214242,
        "epoch": 0.7628894403087951,
        "step": 2767
    },
    {
        "loss": 2.421,
        "grad_norm": 1.0727055072784424,
        "learning_rate": 0.00016263107309172215,
        "epoch": 0.7631651502619244,
        "step": 2768
    },
    {
        "loss": 2.1673,
        "grad_norm": 1.4327183961868286,
        "learning_rate": 0.0001624268803815051,
        "epoch": 0.7634408602150538,
        "step": 2769
    },
    {
        "loss": 2.4297,
        "grad_norm": 1.7337162494659424,
        "learning_rate": 0.00016222226027945107,
        "epoch": 0.7637165701681831,
        "step": 2770
    },
    {
        "loss": 2.2549,
        "grad_norm": 1.544066071510315,
        "learning_rate": 0.00016201721418644674,
        "epoch": 0.7639922801213124,
        "step": 2771
    },
    {
        "loss": 1.7088,
        "grad_norm": 2.148228645324707,
        "learning_rate": 0.00016181174350629452,
        "epoch": 0.7642679900744417,
        "step": 2772
    },
    {
        "loss": 1.9476,
        "grad_norm": 1.4635547399520874,
        "learning_rate": 0.00016160584964570432,
        "epoch": 0.764543700027571,
        "step": 2773
    },
    {
        "loss": 2.4206,
        "grad_norm": 1.7132011651992798,
        "learning_rate": 0.00016139953401428261,
        "epoch": 0.7648194099807003,
        "step": 2774
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.0907986164093018,
        "learning_rate": 0.00016119279802452415,
        "epoch": 0.7650951199338296,
        "step": 2775
    },
    {
        "loss": 1.8149,
        "grad_norm": 2.776819944381714,
        "learning_rate": 0.00016098564309180072,
        "epoch": 0.7653708298869589,
        "step": 2776
    },
    {
        "loss": 2.8232,
        "grad_norm": 1.9030053615570068,
        "learning_rate": 0.00016077807063435325,
        "epoch": 0.7656465398400882,
        "step": 2777
    },
    {
        "loss": 2.1565,
        "grad_norm": 0.9142280220985413,
        "learning_rate": 0.0001605700820732803,
        "epoch": 0.7659222497932175,
        "step": 2778
    },
    {
        "loss": 2.55,
        "grad_norm": 1.2670890092849731,
        "learning_rate": 0.0001603616788325299,
        "epoch": 0.7661979597463469,
        "step": 2779
    },
    {
        "loss": 2.2418,
        "grad_norm": 2.34309720993042,
        "learning_rate": 0.00016015286233888841,
        "epoch": 0.7664736696994762,
        "step": 2780
    },
    {
        "loss": 2.3534,
        "grad_norm": 1.8290585279464722,
        "learning_rate": 0.00015994363402197222,
        "epoch": 0.7667493796526055,
        "step": 2781
    },
    {
        "loss": 2.4051,
        "grad_norm": 1.2232452630996704,
        "learning_rate": 0.00015973399531421634,
        "epoch": 0.7670250896057348,
        "step": 2782
    },
    {
        "loss": 2.81,
        "grad_norm": 2.1289188861846924,
        "learning_rate": 0.00015952394765086618,
        "epoch": 0.7673007995588641,
        "step": 2783
    },
    {
        "loss": 2.0066,
        "grad_norm": 1.7639845609664917,
        "learning_rate": 0.0001593134924699663,
        "epoch": 0.7675765095119934,
        "step": 2784
    },
    {
        "loss": 1.4173,
        "grad_norm": 2.476487874984741,
        "learning_rate": 0.00015910263121235197,
        "epoch": 0.7678522194651227,
        "step": 2785
    },
    {
        "loss": 1.0121,
        "grad_norm": 2.3106555938720703,
        "learning_rate": 0.00015889136532163772,
        "epoch": 0.768127929418252,
        "step": 2786
    },
    {
        "loss": 2.0943,
        "grad_norm": 1.578140377998352,
        "learning_rate": 0.00015867969624420923,
        "epoch": 0.7684036393713813,
        "step": 2787
    },
    {
        "loss": 2.3,
        "grad_norm": 1.8676140308380127,
        "learning_rate": 0.00015846762542921165,
        "epoch": 0.7686793493245107,
        "step": 2788
    },
    {
        "loss": 2.2606,
        "grad_norm": 1.8445791006088257,
        "learning_rate": 0.00015825515432854132,
        "epoch": 0.76895505927764,
        "step": 2789
    },
    {
        "loss": 1.7548,
        "grad_norm": 2.610842704772949,
        "learning_rate": 0.0001580422843968342,
        "epoch": 0.7692307692307693,
        "step": 2790
    },
    {
        "loss": 1.4182,
        "grad_norm": 2.297109603881836,
        "learning_rate": 0.00015782901709145776,
        "epoch": 0.7695064791838986,
        "step": 2791
    },
    {
        "loss": 2.2357,
        "grad_norm": 1.2775866985321045,
        "learning_rate": 0.00015761535387249893,
        "epoch": 0.7697821891370279,
        "step": 2792
    },
    {
        "loss": 1.4739,
        "grad_norm": 1.7873679399490356,
        "learning_rate": 0.0001574012962027562,
        "epoch": 0.7700578990901572,
        "step": 2793
    },
    {
        "loss": 2.2698,
        "grad_norm": 1.0832769870758057,
        "learning_rate": 0.0001571868455477276,
        "epoch": 0.7703336090432865,
        "step": 2794
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.3077629804611206,
        "learning_rate": 0.00015697200337560263,
        "epoch": 0.7706093189964157,
        "step": 2795
    },
    {
        "loss": 2.2176,
        "grad_norm": 2.601210355758667,
        "learning_rate": 0.00015675677115725023,
        "epoch": 0.770885028949545,
        "step": 2796
    },
    {
        "loss": 2.1434,
        "grad_norm": 2.392488956451416,
        "learning_rate": 0.0001565411503662107,
        "epoch": 0.7711607389026743,
        "step": 2797
    },
    {
        "loss": 1.9045,
        "grad_norm": 1.226387858390808,
        "learning_rate": 0.00015632514247868363,
        "epoch": 0.7714364488558036,
        "step": 2798
    },
    {
        "loss": 1.5646,
        "grad_norm": 3.0056354999542236,
        "learning_rate": 0.00015610874897351978,
        "epoch": 0.771712158808933,
        "step": 2799
    },
    {
        "loss": 2.3963,
        "grad_norm": 1.686715841293335,
        "learning_rate": 0.00015589197133220894,
        "epoch": 0.7719878687620623,
        "step": 2800
    },
    {
        "loss": 2.1425,
        "grad_norm": 1.1655826568603516,
        "learning_rate": 0.0001556748110388718,
        "epoch": 0.7722635787151916,
        "step": 2801
    },
    {
        "loss": 2.077,
        "grad_norm": 1.3581823110580444,
        "learning_rate": 0.0001554572695802478,
        "epoch": 0.7725392886683209,
        "step": 2802
    },
    {
        "loss": 1.7282,
        "grad_norm": 1.958838939666748,
        "learning_rate": 0.00015523934844568696,
        "epoch": 0.7728149986214502,
        "step": 2803
    },
    {
        "loss": 2.2482,
        "grad_norm": 1.5612517595291138,
        "learning_rate": 0.00015502104912713772,
        "epoch": 0.7730907085745795,
        "step": 2804
    },
    {
        "loss": 2.275,
        "grad_norm": 1.7679845094680786,
        "learning_rate": 0.0001548023731191385,
        "epoch": 0.7733664185277088,
        "step": 2805
    },
    {
        "loss": 2.2101,
        "grad_norm": 1.5833038091659546,
        "learning_rate": 0.0001545833219188058,
        "epoch": 0.7736421284808381,
        "step": 2806
    },
    {
        "loss": 2.2678,
        "grad_norm": 1.2118620872497559,
        "learning_rate": 0.00015436389702582574,
        "epoch": 0.7739178384339674,
        "step": 2807
    },
    {
        "loss": 1.9504,
        "grad_norm": 1.316394567489624,
        "learning_rate": 0.00015414409994244182,
        "epoch": 0.7741935483870968,
        "step": 2808
    },
    {
        "loss": 2.2244,
        "grad_norm": 1.1628011465072632,
        "learning_rate": 0.00015392393217344673,
        "epoch": 0.7744692583402261,
        "step": 2809
    },
    {
        "loss": 1.2717,
        "grad_norm": 1.8013612031936646,
        "learning_rate": 0.0001537033952261699,
        "epoch": 0.7747449682933554,
        "step": 2810
    },
    {
        "loss": 2.062,
        "grad_norm": 1.655825138092041,
        "learning_rate": 0.0001534824906104693,
        "epoch": 0.7750206782464847,
        "step": 2811
    },
    {
        "loss": 1.7756,
        "grad_norm": 2.0125362873077393,
        "learning_rate": 0.00015326121983871903,
        "epoch": 0.775296388199614,
        "step": 2812
    },
    {
        "loss": 1.8196,
        "grad_norm": 2.6929731369018555,
        "learning_rate": 0.00015303958442580104,
        "epoch": 0.7755720981527433,
        "step": 2813
    },
    {
        "loss": 2.0885,
        "grad_norm": 2.1002345085144043,
        "learning_rate": 0.0001528175858890927,
        "epoch": 0.7758478081058726,
        "step": 2814
    },
    {
        "loss": 2.112,
        "grad_norm": 1.397598147392273,
        "learning_rate": 0.00015259522574845836,
        "epoch": 0.7761235180590019,
        "step": 2815
    },
    {
        "loss": 2.0426,
        "grad_norm": 1.7829642295837402,
        "learning_rate": 0.00015237250552623715,
        "epoch": 0.7763992280121312,
        "step": 2816
    },
    {
        "loss": 2.41,
        "grad_norm": 1.3830552101135254,
        "learning_rate": 0.00015214942674723425,
        "epoch": 0.7766749379652605,
        "step": 2817
    },
    {
        "loss": 1.787,
        "grad_norm": 1.886338472366333,
        "learning_rate": 0.0001519259909387088,
        "epoch": 0.7769506479183899,
        "step": 2818
    },
    {
        "loss": 2.0242,
        "grad_norm": 1.1270713806152344,
        "learning_rate": 0.0001517021996303651,
        "epoch": 0.7772263578715192,
        "step": 2819
    },
    {
        "loss": 2.4299,
        "grad_norm": 1.4757481813430786,
        "learning_rate": 0.00015147805435434037,
        "epoch": 0.7775020678246485,
        "step": 2820
    },
    {
        "loss": 2.2357,
        "grad_norm": 2.202582836151123,
        "learning_rate": 0.00015125355664519628,
        "epoch": 0.7777777777777778,
        "step": 2821
    },
    {
        "loss": 2.7374,
        "grad_norm": 1.6654998064041138,
        "learning_rate": 0.00015102870803990616,
        "epoch": 0.7780534877309071,
        "step": 2822
    },
    {
        "loss": 1.9597,
        "grad_norm": 1.3831477165222168,
        "learning_rate": 0.00015080351007784688,
        "epoch": 0.7783291976840364,
        "step": 2823
    },
    {
        "loss": 1.358,
        "grad_norm": 1.6676132678985596,
        "learning_rate": 0.00015057796430078594,
        "epoch": 0.7786049076371657,
        "step": 2824
    },
    {
        "loss": 1.5933,
        "grad_norm": 1.6074589490890503,
        "learning_rate": 0.00015035207225287314,
        "epoch": 0.778880617590295,
        "step": 2825
    },
    {
        "loss": 2.0613,
        "grad_norm": 1.8035868406295776,
        "learning_rate": 0.00015012583548062802,
        "epoch": 0.7791563275434243,
        "step": 2826
    },
    {
        "loss": 2.0685,
        "grad_norm": 1.2898765802383423,
        "learning_rate": 0.00014989925553293105,
        "epoch": 0.7794320374965537,
        "step": 2827
    },
    {
        "loss": 2.2345,
        "grad_norm": 1.1827861070632935,
        "learning_rate": 0.0001496723339610113,
        "epoch": 0.779707747449683,
        "step": 2828
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.4665125608444214,
        "learning_rate": 0.00014944507231843765,
        "epoch": 0.7799834574028123,
        "step": 2829
    },
    {
        "loss": 2.296,
        "grad_norm": 1.0726162195205688,
        "learning_rate": 0.0001492174721611062,
        "epoch": 0.7802591673559416,
        "step": 2830
    },
    {
        "loss": 2.1898,
        "grad_norm": 1.3359237909317017,
        "learning_rate": 0.0001489895350472316,
        "epoch": 0.7805348773090709,
        "step": 2831
    },
    {
        "loss": 2.2418,
        "grad_norm": 1.9054944515228271,
        "learning_rate": 0.0001487612625373344,
        "epoch": 0.7808105872622002,
        "step": 2832
    },
    {
        "loss": 2.0941,
        "grad_norm": 1.5304795503616333,
        "learning_rate": 0.00014853265619423244,
        "epoch": 0.7810862972153295,
        "step": 2833
    },
    {
        "loss": 2.52,
        "grad_norm": 2.0405826568603516,
        "learning_rate": 0.00014830371758302793,
        "epoch": 0.7813620071684588,
        "step": 2834
    },
    {
        "loss": 1.459,
        "grad_norm": 2.0825443267822266,
        "learning_rate": 0.000148074448271099,
        "epoch": 0.7816377171215881,
        "step": 2835
    },
    {
        "loss": 2.2097,
        "grad_norm": 1.198565125465393,
        "learning_rate": 0.00014784484982808684,
        "epoch": 0.7819134270747174,
        "step": 2836
    },
    {
        "loss": 2.2695,
        "grad_norm": 1.9453598260879517,
        "learning_rate": 0.0001476149238258869,
        "epoch": 0.7821891370278468,
        "step": 2837
    },
    {
        "loss": 1.4634,
        "grad_norm": 2.875936985015869,
        "learning_rate": 0.00014738467183863626,
        "epoch": 0.782464846980976,
        "step": 2838
    },
    {
        "loss": 2.2579,
        "grad_norm": 1.803162932395935,
        "learning_rate": 0.00014715409544270472,
        "epoch": 0.7827405569341053,
        "step": 2839
    },
    {
        "loss": 2.2777,
        "grad_norm": 1.3648607730865479,
        "learning_rate": 0.00014692319621668212,
        "epoch": 0.7830162668872346,
        "step": 2840
    },
    {
        "loss": 2.2033,
        "grad_norm": 2.072866201400757,
        "learning_rate": 0.00014669197574136947,
        "epoch": 0.7832919768403639,
        "step": 2841
    },
    {
        "loss": 2.0683,
        "grad_norm": 1.6186137199401855,
        "learning_rate": 0.00014646043559976611,
        "epoch": 0.7835676867934932,
        "step": 2842
    },
    {
        "loss": 1.9881,
        "grad_norm": 2.5770716667175293,
        "learning_rate": 0.00014622857737706102,
        "epoch": 0.7838433967466225,
        "step": 2843
    },
    {
        "loss": 2.2422,
        "grad_norm": 1.2058155536651611,
        "learning_rate": 0.00014599640266061968,
        "epoch": 0.7841191066997518,
        "step": 2844
    },
    {
        "loss": 1.7441,
        "grad_norm": 1.8839054107666016,
        "learning_rate": 0.00014576391303997555,
        "epoch": 0.7843948166528811,
        "step": 2845
    },
    {
        "loss": 1.8684,
        "grad_norm": 1.0795519351959229,
        "learning_rate": 0.00014553111010681683,
        "epoch": 0.7846705266060104,
        "step": 2846
    },
    {
        "loss": 2.1076,
        "grad_norm": 1.7331295013427734,
        "learning_rate": 0.00014529799545497785,
        "epoch": 0.7849462365591398,
        "step": 2847
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.115901231765747,
        "learning_rate": 0.00014506457068042613,
        "epoch": 0.7852219465122691,
        "step": 2848
    },
    {
        "loss": 1.4465,
        "grad_norm": 2.068755865097046,
        "learning_rate": 0.0001448308373812532,
        "epoch": 0.7854976564653984,
        "step": 2849
    },
    {
        "loss": 1.8571,
        "grad_norm": 1.7763715982437134,
        "learning_rate": 0.00014459679715766197,
        "epoch": 0.7857733664185277,
        "step": 2850
    },
    {
        "loss": 2.2432,
        "grad_norm": 1.852880835533142,
        "learning_rate": 0.00014436245161195759,
        "epoch": 0.786049076371657,
        "step": 2851
    },
    {
        "loss": 2.049,
        "grad_norm": 1.811931848526001,
        "learning_rate": 0.00014412780234853453,
        "epoch": 0.7863247863247863,
        "step": 2852
    },
    {
        "loss": 2.2667,
        "grad_norm": 2.6575233936309814,
        "learning_rate": 0.0001438928509738676,
        "epoch": 0.7866004962779156,
        "step": 2853
    },
    {
        "loss": 2.214,
        "grad_norm": 1.3782801628112793,
        "learning_rate": 0.00014365759909649894,
        "epoch": 0.7868762062310449,
        "step": 2854
    },
    {
        "loss": 2.0055,
        "grad_norm": 1.5731538534164429,
        "learning_rate": 0.00014342204832702905,
        "epoch": 0.7871519161841742,
        "step": 2855
    },
    {
        "loss": 1.6125,
        "grad_norm": 1.5438313484191895,
        "learning_rate": 0.0001431862002781037,
        "epoch": 0.7874276261373035,
        "step": 2856
    },
    {
        "loss": 2.1889,
        "grad_norm": 1.3722866773605347,
        "learning_rate": 0.0001429500565644049,
        "epoch": 0.7877033360904329,
        "step": 2857
    },
    {
        "loss": 1.8754,
        "grad_norm": 2.997135877609253,
        "learning_rate": 0.0001427136188026379,
        "epoch": 0.7879790460435622,
        "step": 2858
    },
    {
        "loss": 1.3002,
        "grad_norm": 1.625849962234497,
        "learning_rate": 0.000142476888611522,
        "epoch": 0.7882547559966915,
        "step": 2859
    },
    {
        "loss": 1.4221,
        "grad_norm": 3.371170997619629,
        "learning_rate": 0.00014223986761177776,
        "epoch": 0.7885304659498208,
        "step": 2860
    },
    {
        "loss": 1.9779,
        "grad_norm": 1.1921908855438232,
        "learning_rate": 0.0001420025574261175,
        "epoch": 0.7888061759029501,
        "step": 2861
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.7223923206329346,
        "learning_rate": 0.0001417649596792324,
        "epoch": 0.7890818858560794,
        "step": 2862
    },
    {
        "loss": 2.3327,
        "grad_norm": 1.1762754917144775,
        "learning_rate": 0.0001415270759977835,
        "epoch": 0.7893575958092087,
        "step": 2863
    },
    {
        "loss": 1.9211,
        "grad_norm": 1.5384262800216675,
        "learning_rate": 0.00014128890801038823,
        "epoch": 0.789633305762338,
        "step": 2864
    },
    {
        "loss": 2.008,
        "grad_norm": 1.260979413986206,
        "learning_rate": 0.00014105045734761167,
        "epoch": 0.7899090157154673,
        "step": 2865
    },
    {
        "loss": 2.1382,
        "grad_norm": 1.3158972263336182,
        "learning_rate": 0.00014081172564195298,
        "epoch": 0.7901847256685967,
        "step": 2866
    },
    {
        "loss": 2.2151,
        "grad_norm": 1.7089765071868896,
        "learning_rate": 0.0001405727145278366,
        "epoch": 0.790460435621726,
        "step": 2867
    },
    {
        "loss": 1.9751,
        "grad_norm": 1.804386019706726,
        "learning_rate": 0.00014033342564159874,
        "epoch": 0.7907361455748553,
        "step": 2868
    },
    {
        "loss": 2.2962,
        "grad_norm": 1.5017085075378418,
        "learning_rate": 0.00014009386062147837,
        "epoch": 0.7910118555279846,
        "step": 2869
    },
    {
        "loss": 2.0959,
        "grad_norm": 1.2104759216308594,
        "learning_rate": 0.00013985402110760394,
        "epoch": 0.7912875654811139,
        "step": 2870
    },
    {
        "loss": 2.0135,
        "grad_norm": 1.5100972652435303,
        "learning_rate": 0.00013961390874198413,
        "epoch": 0.7915632754342432,
        "step": 2871
    },
    {
        "loss": 2.2946,
        "grad_norm": 1.3994252681732178,
        "learning_rate": 0.00013937352516849475,
        "epoch": 0.7918389853873725,
        "step": 2872
    },
    {
        "loss": 2.4763,
        "grad_norm": 1.358593225479126,
        "learning_rate": 0.0001391328720328693,
        "epoch": 0.7921146953405018,
        "step": 2873
    },
    {
        "loss": 1.7567,
        "grad_norm": 1.6077349185943604,
        "learning_rate": 0.00013889195098268567,
        "epoch": 0.7923904052936311,
        "step": 2874
    },
    {
        "loss": 1.9289,
        "grad_norm": 1.7981494665145874,
        "learning_rate": 0.00013865076366735717,
        "epoch": 0.7926661152467604,
        "step": 2875
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.104158878326416,
        "learning_rate": 0.0001384093117381189,
        "epoch": 0.7929418251998898,
        "step": 2876
    },
    {
        "loss": 1.8174,
        "grad_norm": 1.158464789390564,
        "learning_rate": 0.00013816759684801862,
        "epoch": 0.7932175351530191,
        "step": 2877
    },
    {
        "loss": 1.88,
        "grad_norm": 2.1170644760131836,
        "learning_rate": 0.00013792562065190338,
        "epoch": 0.7934932451061484,
        "step": 2878
    },
    {
        "loss": 1.5463,
        "grad_norm": 1.869560956954956,
        "learning_rate": 0.00013768338480641025,
        "epoch": 0.7937689550592777,
        "step": 2879
    },
    {
        "loss": 2.1344,
        "grad_norm": 1.350701093673706,
        "learning_rate": 0.0001374408909699529,
        "epoch": 0.794044665012407,
        "step": 2880
    },
    {
        "loss": 2.2663,
        "grad_norm": 1.9040539264678955,
        "learning_rate": 0.00013719814080271226,
        "epoch": 0.7943203749655362,
        "step": 2881
    },
    {
        "loss": 2.3819,
        "grad_norm": 1.7063952684402466,
        "learning_rate": 0.00013695513596662318,
        "epoch": 0.7945960849186655,
        "step": 2882
    },
    {
        "loss": 2.2089,
        "grad_norm": 1.5013153553009033,
        "learning_rate": 0.00013671187812536507,
        "epoch": 0.7948717948717948,
        "step": 2883
    },
    {
        "loss": 1.7381,
        "grad_norm": 2.0550127029418945,
        "learning_rate": 0.00013646836894434846,
        "epoch": 0.7951475048249241,
        "step": 2884
    },
    {
        "loss": 1.8822,
        "grad_norm": 1.2891082763671875,
        "learning_rate": 0.00013622461009070563,
        "epoch": 0.7954232147780534,
        "step": 2885
    },
    {
        "loss": 2.6488,
        "grad_norm": 1.6865538358688354,
        "learning_rate": 0.00013598060323327717,
        "epoch": 0.7956989247311828,
        "step": 2886
    },
    {
        "loss": 2.451,
        "grad_norm": 1.1658574342727661,
        "learning_rate": 0.00013573635004260264,
        "epoch": 0.7959746346843121,
        "step": 2887
    },
    {
        "loss": 2.2998,
        "grad_norm": 1.247663140296936,
        "learning_rate": 0.00013549185219090693,
        "epoch": 0.7962503446374414,
        "step": 2888
    },
    {
        "loss": 2.46,
        "grad_norm": 1.6675089597702026,
        "learning_rate": 0.00013524711135209114,
        "epoch": 0.7965260545905707,
        "step": 2889
    },
    {
        "loss": 2.2007,
        "grad_norm": 1.6751676797866821,
        "learning_rate": 0.00013500212920171875,
        "epoch": 0.7968017645437,
        "step": 2890
    },
    {
        "loss": 2.3455,
        "grad_norm": 1.4265718460083008,
        "learning_rate": 0.00013475690741700643,
        "epoch": 0.7970774744968293,
        "step": 2891
    },
    {
        "loss": 2.215,
        "grad_norm": 1.1138505935668945,
        "learning_rate": 0.0001345114476768104,
        "epoch": 0.7973531844499586,
        "step": 2892
    },
    {
        "loss": 2.4404,
        "grad_norm": 1.727137565612793,
        "learning_rate": 0.000134265751661617,
        "epoch": 0.7976288944030879,
        "step": 2893
    },
    {
        "loss": 2.1445,
        "grad_norm": 1.4337483644485474,
        "learning_rate": 0.0001340198210535292,
        "epoch": 0.7979046043562172,
        "step": 2894
    },
    {
        "loss": 2.026,
        "grad_norm": 1.7099955081939697,
        "learning_rate": 0.0001337736575362571,
        "epoch": 0.7981803143093466,
        "step": 2895
    },
    {
        "loss": 2.0006,
        "grad_norm": 1.9329839944839478,
        "learning_rate": 0.00013352726279510424,
        "epoch": 0.7984560242624759,
        "step": 2896
    },
    {
        "loss": 2.2028,
        "grad_norm": 1.0281445980072021,
        "learning_rate": 0.00013328063851695827,
        "epoch": 0.7987317342156052,
        "step": 2897
    },
    {
        "loss": 2.3356,
        "grad_norm": 1.3948861360549927,
        "learning_rate": 0.00013303378639027726,
        "epoch": 0.7990074441687345,
        "step": 2898
    },
    {
        "loss": 1.9385,
        "grad_norm": 1.9009230136871338,
        "learning_rate": 0.00013278670810508027,
        "epoch": 0.7992831541218638,
        "step": 2899
    },
    {
        "loss": 1.3769,
        "grad_norm": 2.647400140762329,
        "learning_rate": 0.00013253940535293363,
        "epoch": 0.7995588640749931,
        "step": 2900
    },
    {
        "loss": 1.6157,
        "grad_norm": 2.3301260471343994,
        "learning_rate": 0.00013229187982694148,
        "epoch": 0.7998345740281224,
        "step": 2901
    },
    {
        "loss": 2.1516,
        "grad_norm": 1.2183740139007568,
        "learning_rate": 0.00013204413322173212,
        "epoch": 0.8001102839812517,
        "step": 2902
    },
    {
        "loss": 2.0356,
        "grad_norm": 1.8722283840179443,
        "learning_rate": 0.00013179616723344844,
        "epoch": 0.800385993934381,
        "step": 2903
    },
    {
        "loss": 2.2986,
        "grad_norm": 1.3216948509216309,
        "learning_rate": 0.00013154798355973423,
        "epoch": 0.8006617038875103,
        "step": 2904
    },
    {
        "loss": 2.0989,
        "grad_norm": 2.054352283477783,
        "learning_rate": 0.00013129958389972466,
        "epoch": 0.8009374138406397,
        "step": 2905
    },
    {
        "loss": 2.2762,
        "grad_norm": 1.4091445207595825,
        "learning_rate": 0.00013105096995403258,
        "epoch": 0.801213123793769,
        "step": 2906
    },
    {
        "loss": 1.7066,
        "grad_norm": 2.128129482269287,
        "learning_rate": 0.00013080214342473898,
        "epoch": 0.8014888337468983,
        "step": 2907
    },
    {
        "loss": 1.9597,
        "grad_norm": 1.7536815404891968,
        "learning_rate": 0.00013055310601537906,
        "epoch": 0.8017645437000276,
        "step": 2908
    },
    {
        "loss": 2.3426,
        "grad_norm": 1.7862924337387085,
        "learning_rate": 0.000130303859430933,
        "epoch": 0.8020402536531569,
        "step": 2909
    },
    {
        "loss": 1.881,
        "grad_norm": 1.535577416419983,
        "learning_rate": 0.0001300544053778119,
        "epoch": 0.8023159636062862,
        "step": 2910
    },
    {
        "loss": 2.2443,
        "grad_norm": 1.6866909265518188,
        "learning_rate": 0.0001298047455638483,
        "epoch": 0.8025916735594155,
        "step": 2911
    },
    {
        "loss": 2.0058,
        "grad_norm": 1.3772215843200684,
        "learning_rate": 0.00012955488169828246,
        "epoch": 0.8028673835125448,
        "step": 2912
    },
    {
        "loss": 2.1029,
        "grad_norm": 2.11917781829834,
        "learning_rate": 0.00012930481549175253,
        "epoch": 0.8031430934656741,
        "step": 2913
    },
    {
        "loss": 1.7638,
        "grad_norm": 2.103641986846924,
        "learning_rate": 0.00012905454865628103,
        "epoch": 0.8034188034188035,
        "step": 2914
    },
    {
        "loss": 2.3228,
        "grad_norm": 1.7544151544570923,
        "learning_rate": 0.00012880408290526511,
        "epoch": 0.8036945133719328,
        "step": 2915
    },
    {
        "loss": 2.3975,
        "grad_norm": 1.1553078889846802,
        "learning_rate": 0.00012855341995346258,
        "epoch": 0.8039702233250621,
        "step": 2916
    },
    {
        "loss": 2.305,
        "grad_norm": 1.483775019645691,
        "learning_rate": 0.00012830256151698247,
        "epoch": 0.8042459332781914,
        "step": 2917
    },
    {
        "loss": 1.6949,
        "grad_norm": 2.2311930656433105,
        "learning_rate": 0.00012805150931327107,
        "epoch": 0.8045216432313207,
        "step": 2918
    },
    {
        "loss": 2.4327,
        "grad_norm": 1.113768219947815,
        "learning_rate": 0.0001278002650611024,
        "epoch": 0.80479735318445,
        "step": 2919
    },
    {
        "loss": 2.5176,
        "grad_norm": 1.5594335794448853,
        "learning_rate": 0.00012754883048056404,
        "epoch": 0.8050730631375793,
        "step": 2920
    },
    {
        "loss": 2.3949,
        "grad_norm": 1.5037002563476562,
        "learning_rate": 0.00012729720729304786,
        "epoch": 0.8053487730907086,
        "step": 2921
    },
    {
        "loss": 1.6763,
        "grad_norm": 2.0556538105010986,
        "learning_rate": 0.00012704539722123589,
        "epoch": 0.8056244830438379,
        "step": 2922
    },
    {
        "loss": 1.6545,
        "grad_norm": 1.4756649732589722,
        "learning_rate": 0.00012679340198909063,
        "epoch": 0.8059001929969671,
        "step": 2923
    },
    {
        "loss": 1.834,
        "grad_norm": 1.1737645864486694,
        "learning_rate": 0.00012654122332184117,
        "epoch": 0.8061759029500964,
        "step": 2924
    },
    {
        "loss": 1.9314,
        "grad_norm": 1.7471050024032593,
        "learning_rate": 0.00012628886294597358,
        "epoch": 0.8064516129032258,
        "step": 2925
    },
    {
        "loss": 2.2591,
        "grad_norm": 1.573705792427063,
        "learning_rate": 0.00012603632258921684,
        "epoch": 0.8067273228563551,
        "step": 2926
    },
    {
        "loss": 2.1336,
        "grad_norm": 1.6366008520126343,
        "learning_rate": 0.00012578360398053323,
        "epoch": 0.8070030328094844,
        "step": 2927
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.356959581375122,
        "learning_rate": 0.00012553070885010426,
        "epoch": 0.8072787427626137,
        "step": 2928
    },
    {
        "loss": 2.2786,
        "grad_norm": 1.7136175632476807,
        "learning_rate": 0.00012527763892932113,
        "epoch": 0.807554452715743,
        "step": 2929
    },
    {
        "loss": 2.4283,
        "grad_norm": 1.3269258737564087,
        "learning_rate": 0.00012502439595077052,
        "epoch": 0.8078301626688723,
        "step": 2930
    },
    {
        "loss": 2.1596,
        "grad_norm": 1.0744801759719849,
        "learning_rate": 0.00012477098164822507,
        "epoch": 0.8081058726220016,
        "step": 2931
    },
    {
        "loss": 1.8247,
        "grad_norm": 1.677123785018921,
        "learning_rate": 0.00012451739775662922,
        "epoch": 0.8083815825751309,
        "step": 2932
    },
    {
        "loss": 2.426,
        "grad_norm": 1.3355964422225952,
        "learning_rate": 0.0001242636460120896,
        "epoch": 0.8086572925282602,
        "step": 2933
    },
    {
        "loss": 2.2917,
        "grad_norm": 2.2050211429595947,
        "learning_rate": 0.0001240097281518609,
        "epoch": 0.8089330024813896,
        "step": 2934
    },
    {
        "loss": 1.7527,
        "grad_norm": 2.1254031658172607,
        "learning_rate": 0.00012375564591433612,
        "epoch": 0.8092087124345189,
        "step": 2935
    },
    {
        "loss": 1.5351,
        "grad_norm": 1.918686866760254,
        "learning_rate": 0.00012350140103903264,
        "epoch": 0.8094844223876482,
        "step": 2936
    },
    {
        "loss": 2.3303,
        "grad_norm": 1.1190693378448486,
        "learning_rate": 0.0001232469952665823,
        "epoch": 0.8097601323407775,
        "step": 2937
    },
    {
        "loss": 2.1598,
        "grad_norm": 2.084923028945923,
        "learning_rate": 0.00012299243033871745,
        "epoch": 0.8100358422939068,
        "step": 2938
    },
    {
        "loss": 2.5022,
        "grad_norm": 1.4122729301452637,
        "learning_rate": 0.00012273770799826116,
        "epoch": 0.8103115522470361,
        "step": 2939
    },
    {
        "loss": 2.1365,
        "grad_norm": 2.1812665462493896,
        "learning_rate": 0.00012248282998911307,
        "epoch": 0.8105872622001654,
        "step": 2940
    },
    {
        "loss": 2.1818,
        "grad_norm": 1.200195550918579,
        "learning_rate": 0.00012222779805623964,
        "epoch": 0.8108629721532947,
        "step": 2941
    },
    {
        "loss": 2.5169,
        "grad_norm": 1.3995654582977295,
        "learning_rate": 0.00012197261394566013,
        "epoch": 0.811138682106424,
        "step": 2942
    },
    {
        "loss": 1.5237,
        "grad_norm": 2.2391209602355957,
        "learning_rate": 0.00012171727940443664,
        "epoch": 0.8114143920595533,
        "step": 2943
    },
    {
        "loss": 2.4064,
        "grad_norm": 1.4234968423843384,
        "learning_rate": 0.00012146179618066016,
        "epoch": 0.8116901020126827,
        "step": 2944
    },
    {
        "loss": 2.2229,
        "grad_norm": 1.457283854484558,
        "learning_rate": 0.00012120616602344063,
        "epoch": 0.811965811965812,
        "step": 2945
    },
    {
        "loss": 2.3899,
        "grad_norm": 1.6383157968521118,
        "learning_rate": 0.00012095039068289288,
        "epoch": 0.8122415219189413,
        "step": 2946
    },
    {
        "loss": 1.5934,
        "grad_norm": 2.1748087406158447,
        "learning_rate": 0.00012069447191012677,
        "epoch": 0.8125172318720706,
        "step": 2947
    },
    {
        "loss": 1.7549,
        "grad_norm": 1.536702036857605,
        "learning_rate": 0.00012043841145723307,
        "epoch": 0.8127929418251999,
        "step": 2948
    },
    {
        "loss": 2.4775,
        "grad_norm": 1.3857457637786865,
        "learning_rate": 0.00012018221107727362,
        "epoch": 0.8130686517783292,
        "step": 2949
    },
    {
        "loss": 1.7089,
        "grad_norm": 1.146782636642456,
        "learning_rate": 0.0001199258725242671,
        "epoch": 0.8133443617314585,
        "step": 2950
    },
    {
        "loss": 2.4255,
        "grad_norm": 0.9326398372650146,
        "learning_rate": 0.00011966939755317936,
        "epoch": 0.8136200716845878,
        "step": 2951
    },
    {
        "loss": 1.9581,
        "grad_norm": 1.7017189264297485,
        "learning_rate": 0.00011941278791990899,
        "epoch": 0.8138957816377171,
        "step": 2952
    },
    {
        "loss": 1.3902,
        "grad_norm": 1.823768973350525,
        "learning_rate": 0.00011915604538127753,
        "epoch": 0.8141714915908465,
        "step": 2953
    },
    {
        "loss": 2.0426,
        "grad_norm": 1.8158323764801025,
        "learning_rate": 0.00011889917169501594,
        "epoch": 0.8144472015439758,
        "step": 2954
    },
    {
        "loss": 2.3349,
        "grad_norm": 1.2174886465072632,
        "learning_rate": 0.00011864216861975308,
        "epoch": 0.8147229114971051,
        "step": 2955
    },
    {
        "loss": 2.2186,
        "grad_norm": 3.113417625427246,
        "learning_rate": 0.00011838503791500371,
        "epoch": 0.8149986214502344,
        "step": 2956
    },
    {
        "loss": 1.9233,
        "grad_norm": 1.3558193445205688,
        "learning_rate": 0.00011812778134115637,
        "epoch": 0.8152743314033637,
        "step": 2957
    },
    {
        "loss": 2.1132,
        "grad_norm": 1.4079318046569824,
        "learning_rate": 0.00011787040065946126,
        "epoch": 0.815550041356493,
        "step": 2958
    },
    {
        "loss": 2.706,
        "grad_norm": 1.5061932802200317,
        "learning_rate": 0.00011761289763201841,
        "epoch": 0.8158257513096223,
        "step": 2959
    },
    {
        "loss": 2.2644,
        "grad_norm": 1.3543814420700073,
        "learning_rate": 0.00011735527402176534,
        "epoch": 0.8161014612627516,
        "step": 2960
    },
    {
        "loss": 2.0884,
        "grad_norm": 1.4746326208114624,
        "learning_rate": 0.00011709753159246511,
        "epoch": 0.8163771712158809,
        "step": 2961
    },
    {
        "loss": 2.2732,
        "grad_norm": 1.7993758916854858,
        "learning_rate": 0.00011683967210869435,
        "epoch": 0.8166528811690102,
        "step": 2962
    },
    {
        "loss": 1.7238,
        "grad_norm": 1.546417236328125,
        "learning_rate": 0.000116581697335831,
        "epoch": 0.8169285911221396,
        "step": 2963
    },
    {
        "loss": 2.4118,
        "grad_norm": 1.6370753049850464,
        "learning_rate": 0.0001163236090400423,
        "epoch": 0.8172043010752689,
        "step": 2964
    },
    {
        "loss": 0.8275,
        "grad_norm": 2.5977237224578857,
        "learning_rate": 0.0001160654089882727,
        "epoch": 0.8174800110283982,
        "step": 2965
    },
    {
        "loss": 2.1411,
        "grad_norm": 1.4447592496871948,
        "learning_rate": 0.00011580709894823178,
        "epoch": 0.8177557209815274,
        "step": 2966
    },
    {
        "loss": 2.1036,
        "grad_norm": 1.8713531494140625,
        "learning_rate": 0.00011554868068838218,
        "epoch": 0.8180314309346567,
        "step": 2967
    },
    {
        "loss": 1.3307,
        "grad_norm": 2.0865840911865234,
        "learning_rate": 0.00011529015597792734,
        "epoch": 0.818307140887786,
        "step": 2968
    },
    {
        "loss": 2.1696,
        "grad_norm": 1.3121490478515625,
        "learning_rate": 0.00011503152658679959,
        "epoch": 0.8185828508409153,
        "step": 2969
    },
    {
        "loss": 1.519,
        "grad_norm": 1.7500673532485962,
        "learning_rate": 0.00011477279428564782,
        "epoch": 0.8188585607940446,
        "step": 2970
    },
    {
        "loss": 1.824,
        "grad_norm": 2.1282131671905518,
        "learning_rate": 0.00011451396084582557,
        "epoch": 0.8191342707471739,
        "step": 2971
    },
    {
        "loss": 1.8347,
        "grad_norm": 1.476880669593811,
        "learning_rate": 0.00011425502803937877,
        "epoch": 0.8194099807003032,
        "step": 2972
    },
    {
        "loss": 2.4816,
        "grad_norm": 1.1848989725112915,
        "learning_rate": 0.00011399599763903366,
        "epoch": 0.8196856906534326,
        "step": 2973
    },
    {
        "loss": 1.9843,
        "grad_norm": 1.4653362035751343,
        "learning_rate": 0.00011373687141818458,
        "epoch": 0.8199614006065619,
        "step": 2974
    },
    {
        "loss": 1.7068,
        "grad_norm": 1.9318941831588745,
        "learning_rate": 0.00011347765115088194,
        "epoch": 0.8202371105596912,
        "step": 2975
    },
    {
        "loss": 1.8766,
        "grad_norm": 1.6563774347305298,
        "learning_rate": 0.00011321833861181998,
        "epoch": 0.8205128205128205,
        "step": 2976
    },
    {
        "loss": 2.1824,
        "grad_norm": 1.2168893814086914,
        "learning_rate": 0.00011295893557632473,
        "epoch": 0.8207885304659498,
        "step": 2977
    },
    {
        "loss": 2.2983,
        "grad_norm": 1.8717880249023438,
        "learning_rate": 0.00011269944382034166,
        "epoch": 0.8210642404190791,
        "step": 2978
    },
    {
        "loss": 2.2422,
        "grad_norm": 1.3179787397384644,
        "learning_rate": 0.00011243986512042377,
        "epoch": 0.8213399503722084,
        "step": 2979
    },
    {
        "loss": 1.962,
        "grad_norm": 2.100334405899048,
        "learning_rate": 0.0001121802012537192,
        "epoch": 0.8216156603253377,
        "step": 2980
    },
    {
        "loss": 2.5346,
        "grad_norm": 1.9239755868911743,
        "learning_rate": 0.00011192045399795923,
        "epoch": 0.821891370278467,
        "step": 2981
    },
    {
        "loss": 2.6257,
        "grad_norm": 1.7926138639450073,
        "learning_rate": 0.00011166062513144606,
        "epoch": 0.8221670802315963,
        "step": 2982
    },
    {
        "loss": 1.6656,
        "grad_norm": 2.0728578567504883,
        "learning_rate": 0.00011140071643304055,
        "epoch": 0.8224427901847257,
        "step": 2983
    },
    {
        "loss": 2.1284,
        "grad_norm": 1.311842918395996,
        "learning_rate": 0.00011114072968215019,
        "epoch": 0.822718500137855,
        "step": 2984
    },
    {
        "loss": 2.0759,
        "grad_norm": 0.9664639234542847,
        "learning_rate": 0.00011088066665871675,
        "epoch": 0.8229942100909843,
        "step": 2985
    },
    {
        "loss": 1.3251,
        "grad_norm": 1.629217505455017,
        "learning_rate": 0.00011062052914320426,
        "epoch": 0.8232699200441136,
        "step": 2986
    },
    {
        "loss": 2.4597,
        "grad_norm": 1.690748691558838,
        "learning_rate": 0.00011036031891658671,
        "epoch": 0.8235456299972429,
        "step": 2987
    },
    {
        "loss": 2.2647,
        "grad_norm": 1.7396615743637085,
        "learning_rate": 0.00011010003776033588,
        "epoch": 0.8238213399503722,
        "step": 2988
    },
    {
        "loss": 2.3579,
        "grad_norm": 1.6121302843093872,
        "learning_rate": 0.0001098396874564092,
        "epoch": 0.8240970499035015,
        "step": 2989
    },
    {
        "loss": 1.9418,
        "grad_norm": 1.1811097860336304,
        "learning_rate": 0.00010957926978723745,
        "epoch": 0.8243727598566308,
        "step": 2990
    },
    {
        "loss": 1.8216,
        "grad_norm": 1.7359917163848877,
        "learning_rate": 0.00010931878653571269,
        "epoch": 0.8246484698097601,
        "step": 2991
    },
    {
        "loss": 2.2154,
        "grad_norm": 1.9114397764205933,
        "learning_rate": 0.00010905823948517582,
        "epoch": 0.8249241797628895,
        "step": 2992
    },
    {
        "loss": 2.5826,
        "grad_norm": 1.7463200092315674,
        "learning_rate": 0.00010879763041940468,
        "epoch": 0.8251998897160188,
        "step": 2993
    },
    {
        "loss": 1.7726,
        "grad_norm": 1.8375296592712402,
        "learning_rate": 0.00010853696112260165,
        "epoch": 0.8254755996691481,
        "step": 2994
    },
    {
        "loss": 1.9274,
        "grad_norm": 1.8093574047088623,
        "learning_rate": 0.00010827623337938144,
        "epoch": 0.8257513096222774,
        "step": 2995
    },
    {
        "loss": 2.0983,
        "grad_norm": 1.8379225730895996,
        "learning_rate": 0.0001080154489747589,
        "epoch": 0.8260270195754067,
        "step": 2996
    },
    {
        "loss": 2.3928,
        "grad_norm": 1.3973993062973022,
        "learning_rate": 0.00010775460969413678,
        "epoch": 0.826302729528536,
        "step": 2997
    },
    {
        "loss": 2.0739,
        "grad_norm": 1.0632236003875732,
        "learning_rate": 0.00010749371732329359,
        "epoch": 0.8265784394816653,
        "step": 2998
    },
    {
        "loss": 2.3782,
        "grad_norm": 1.699064016342163,
        "learning_rate": 0.0001072327736483713,
        "epoch": 0.8268541494347946,
        "step": 2999
    },
    {
        "loss": 2.0288,
        "grad_norm": 2.109907627105713,
        "learning_rate": 0.00010697178045586303,
        "epoch": 0.8271298593879239,
        "step": 3000
    },
    {
        "loss": 2.1695,
        "grad_norm": 1.745733380317688,
        "learning_rate": 0.00010671073953260105,
        "epoch": 0.8274055693410532,
        "step": 3001
    },
    {
        "loss": 2.1001,
        "grad_norm": 1.6490774154663086,
        "learning_rate": 0.00010644965266574425,
        "epoch": 0.8276812792941826,
        "step": 3002
    },
    {
        "loss": 1.7257,
        "grad_norm": 2.1342639923095703,
        "learning_rate": 0.00010618852164276622,
        "epoch": 0.8279569892473119,
        "step": 3003
    },
    {
        "loss": 1.9937,
        "grad_norm": 1.608843207359314,
        "learning_rate": 0.0001059273482514428,
        "epoch": 0.8282326992004412,
        "step": 3004
    },
    {
        "loss": 2.5598,
        "grad_norm": 1.0817948579788208,
        "learning_rate": 0.0001056661342798398,
        "epoch": 0.8285084091535705,
        "step": 3005
    },
    {
        "loss": 1.1578,
        "grad_norm": 2.4967093467712402,
        "learning_rate": 0.00010540488151630102,
        "epoch": 0.8287841191066998,
        "step": 3006
    },
    {
        "loss": 1.5499,
        "grad_norm": 1.8994882106781006,
        "learning_rate": 0.00010514359174943567,
        "epoch": 0.8290598290598291,
        "step": 3007
    },
    {
        "loss": 2.2821,
        "grad_norm": 2.0136773586273193,
        "learning_rate": 0.00010488226676810644,
        "epoch": 0.8293355390129584,
        "step": 3008
    },
    {
        "loss": 2.0489,
        "grad_norm": 1.6261851787567139,
        "learning_rate": 0.00010462090836141705,
        "epoch": 0.8296112489660876,
        "step": 3009
    },
    {
        "loss": 1.4122,
        "grad_norm": 2.5942580699920654,
        "learning_rate": 0.00010435951831870001,
        "epoch": 0.8298869589192169,
        "step": 3010
    },
    {
        "loss": 2.4085,
        "grad_norm": 1.8220869302749634,
        "learning_rate": 0.00010409809842950452,
        "epoch": 0.8301626688723462,
        "step": 3011
    },
    {
        "loss": 2.0725,
        "grad_norm": 2.4156434535980225,
        "learning_rate": 0.00010383665048358402,
        "epoch": 0.8304383788254756,
        "step": 3012
    },
    {
        "loss": 1.7959,
        "grad_norm": 1.7672580480575562,
        "learning_rate": 0.00010357517627088406,
        "epoch": 0.8307140887786049,
        "step": 3013
    },
    {
        "loss": 2.2105,
        "grad_norm": 1.1968327760696411,
        "learning_rate": 0.00010331367758153014,
        "epoch": 0.8309897987317342,
        "step": 3014
    },
    {
        "loss": 1.9439,
        "grad_norm": 2.0146102905273438,
        "learning_rate": 0.00010305215620581512,
        "epoch": 0.8312655086848635,
        "step": 3015
    },
    {
        "loss": 1.9001,
        "grad_norm": 1.8600389957427979,
        "learning_rate": 0.00010279061393418734,
        "epoch": 0.8315412186379928,
        "step": 3016
    },
    {
        "loss": 1.7967,
        "grad_norm": 1.4359792470932007,
        "learning_rate": 0.00010252905255723812,
        "epoch": 0.8318169285911221,
        "step": 3017
    },
    {
        "loss": 1.8497,
        "grad_norm": 1.285994052886963,
        "learning_rate": 0.00010226747386568965,
        "epoch": 0.8320926385442514,
        "step": 3018
    },
    {
        "loss": 2.8932,
        "grad_norm": 1.2832272052764893,
        "learning_rate": 0.0001020058796503826,
        "epoch": 0.8323683484973807,
        "step": 3019
    },
    {
        "loss": 1.6532,
        "grad_norm": 1.7719069719314575,
        "learning_rate": 0.0001017442717022639,
        "epoch": 0.83264405845051,
        "step": 3020
    },
    {
        "loss": 2.4598,
        "grad_norm": 1.4804927110671997,
        "learning_rate": 0.00010148265181237462,
        "epoch": 0.8329197684036393,
        "step": 3021
    },
    {
        "loss": 1.3584,
        "grad_norm": 2.1840274333953857,
        "learning_rate": 0.00010122102177183741,
        "epoch": 0.8331954783567687,
        "step": 3022
    },
    {
        "loss": 1.3802,
        "grad_norm": 2.3762946128845215,
        "learning_rate": 0.00010095938337184459,
        "epoch": 0.833471188309898,
        "step": 3023
    },
    {
        "loss": 2.0938,
        "grad_norm": 1.9680736064910889,
        "learning_rate": 0.00010069773840364556,
        "epoch": 0.8337468982630273,
        "step": 3024
    },
    {
        "loss": 2.1567,
        "grad_norm": 1.9172828197479248,
        "learning_rate": 0.00010043608865853478,
        "epoch": 0.8340226082161566,
        "step": 3025
    },
    {
        "loss": 2.4407,
        "grad_norm": 1.6038343906402588,
        "learning_rate": 0.00010017443592783942,
        "epoch": 0.8342983181692859,
        "step": 3026
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.6306400299072266,
        "learning_rate": 9.991278200290702e-05,
        "epoch": 0.8345740281224152,
        "step": 3027
    },
    {
        "loss": 1.9825,
        "grad_norm": 1.9281140565872192,
        "learning_rate": 9.965112867509339e-05,
        "epoch": 0.8348497380755445,
        "step": 3028
    },
    {
        "loss": 2.1847,
        "grad_norm": 1.4838266372680664,
        "learning_rate": 9.938947773575014e-05,
        "epoch": 0.8351254480286738,
        "step": 3029
    },
    {
        "loss": 2.2128,
        "grad_norm": 1.3453433513641357,
        "learning_rate": 9.91278309762126e-05,
        "epoch": 0.8354011579818031,
        "step": 3030
    },
    {
        "loss": 2.0806,
        "grad_norm": 1.2407546043395996,
        "learning_rate": 9.886619018778753e-05,
        "epoch": 0.8356768679349325,
        "step": 3031
    },
    {
        "loss": 1.8687,
        "grad_norm": 1.4791157245635986,
        "learning_rate": 9.860455716174067e-05,
        "epoch": 0.8359525778880618,
        "step": 3032
    },
    {
        "loss": 2.3015,
        "grad_norm": 1.0736020803451538,
        "learning_rate": 9.834293368928476e-05,
        "epoch": 0.8362282878411911,
        "step": 3033
    },
    {
        "loss": 2.109,
        "grad_norm": 1.5115694999694824,
        "learning_rate": 9.8081321561567e-05,
        "epoch": 0.8365039977943204,
        "step": 3034
    },
    {
        "loss": 1.8174,
        "grad_norm": 2.0505785942077637,
        "learning_rate": 9.781972256965707e-05,
        "epoch": 0.8367797077474497,
        "step": 3035
    },
    {
        "loss": 2.3988,
        "grad_norm": 1.809154748916626,
        "learning_rate": 9.755813850453461e-05,
        "epoch": 0.837055417700579,
        "step": 3036
    },
    {
        "loss": 2.2214,
        "grad_norm": 0.9442036747932434,
        "learning_rate": 9.729657115707708e-05,
        "epoch": 0.8373311276537083,
        "step": 3037
    },
    {
        "loss": 1.9085,
        "grad_norm": 2.0653138160705566,
        "learning_rate": 9.703502231804752e-05,
        "epoch": 0.8376068376068376,
        "step": 3038
    },
    {
        "loss": 2.476,
        "grad_norm": 1.189658284187317,
        "learning_rate": 9.677349377808224e-05,
        "epoch": 0.8378825475599669,
        "step": 3039
    },
    {
        "loss": 2.4272,
        "grad_norm": 1.1819435358047485,
        "learning_rate": 9.651198732767856e-05,
        "epoch": 0.8381582575130962,
        "step": 3040
    },
    {
        "loss": 2.1683,
        "grad_norm": 1.6383185386657715,
        "learning_rate": 9.625050475718262e-05,
        "epoch": 0.8384339674662256,
        "step": 3041
    },
    {
        "loss": 2.4274,
        "grad_norm": 1.5772026777267456,
        "learning_rate": 9.598904785677699e-05,
        "epoch": 0.8387096774193549,
        "step": 3042
    },
    {
        "loss": 2.1565,
        "grad_norm": 1.2085883617401123,
        "learning_rate": 9.57276184164686e-05,
        "epoch": 0.8389853873724842,
        "step": 3043
    },
    {
        "loss": 2.4881,
        "grad_norm": 1.3180742263793945,
        "learning_rate": 9.546621822607626e-05,
        "epoch": 0.8392610973256135,
        "step": 3044
    },
    {
        "loss": 1.5689,
        "grad_norm": 1.8600409030914307,
        "learning_rate": 9.520484907521864e-05,
        "epoch": 0.8395368072787428,
        "step": 3045
    },
    {
        "loss": 2.1395,
        "grad_norm": 1.7189407348632812,
        "learning_rate": 9.494351275330184e-05,
        "epoch": 0.8398125172318721,
        "step": 3046
    },
    {
        "loss": 2.216,
        "grad_norm": 1.5489252805709839,
        "learning_rate": 9.46822110495072e-05,
        "epoch": 0.8400882271850014,
        "step": 3047
    },
    {
        "loss": 2.2325,
        "grad_norm": 1.7861217260360718,
        "learning_rate": 9.44209457527791e-05,
        "epoch": 0.8403639371381307,
        "step": 3048
    },
    {
        "loss": 2.0777,
        "grad_norm": 2.177277088165283,
        "learning_rate": 9.415971865181261e-05,
        "epoch": 0.84063964709126,
        "step": 3049
    },
    {
        "loss": 2.1006,
        "grad_norm": 1.8228226900100708,
        "learning_rate": 9.389853153504135e-05,
        "epoch": 0.8409153570443894,
        "step": 3050
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.066443920135498,
        "learning_rate": 9.363738619062521e-05,
        "epoch": 0.8411910669975186,
        "step": 3051
    },
    {
        "loss": 2.2507,
        "grad_norm": 1.6183922290802002,
        "learning_rate": 9.337628440643798e-05,
        "epoch": 0.8414667769506479,
        "step": 3052
    },
    {
        "loss": 2.8018,
        "grad_norm": 1.532736897468567,
        "learning_rate": 9.311522797005541e-05,
        "epoch": 0.8417424869037772,
        "step": 3053
    },
    {
        "loss": 2.5157,
        "grad_norm": 1.3451439142227173,
        "learning_rate": 9.28542186687426e-05,
        "epoch": 0.8420181968569065,
        "step": 3054
    },
    {
        "loss": 1.8402,
        "grad_norm": 2.243152618408203,
        "learning_rate": 9.259325828944205e-05,
        "epoch": 0.8422939068100358,
        "step": 3055
    },
    {
        "loss": 1.7832,
        "grad_norm": 1.6852447986602783,
        "learning_rate": 9.233234861876136e-05,
        "epoch": 0.8425696167631651,
        "step": 3056
    },
    {
        "loss": 1.6447,
        "grad_norm": 1.816409945487976,
        "learning_rate": 9.207149144296085e-05,
        "epoch": 0.8428453267162944,
        "step": 3057
    },
    {
        "loss": 1.2445,
        "grad_norm": 2.5639379024505615,
        "learning_rate": 9.181068854794158e-05,
        "epoch": 0.8431210366694237,
        "step": 3058
    },
    {
        "loss": 2.3424,
        "grad_norm": 1.382344365119934,
        "learning_rate": 9.154994171923284e-05,
        "epoch": 0.843396746622553,
        "step": 3059
    },
    {
        "loss": 1.9863,
        "grad_norm": 1.3271241188049316,
        "learning_rate": 9.128925274198023e-05,
        "epoch": 0.8436724565756824,
        "step": 3060
    },
    {
        "loss": 2.1984,
        "grad_norm": 1.193091630935669,
        "learning_rate": 9.102862340093314e-05,
        "epoch": 0.8439481665288117,
        "step": 3061
    },
    {
        "loss": 2.1683,
        "grad_norm": 1.7015143632888794,
        "learning_rate": 9.076805548043278e-05,
        "epoch": 0.844223876481941,
        "step": 3062
    },
    {
        "loss": 2.2353,
        "grad_norm": 1.4422616958618164,
        "learning_rate": 9.050755076439982e-05,
        "epoch": 0.8444995864350703,
        "step": 3063
    },
    {
        "loss": 2.4371,
        "grad_norm": 1.3664391040802002,
        "learning_rate": 9.024711103632218e-05,
        "epoch": 0.8447752963881996,
        "step": 3064
    },
    {
        "loss": 2.0002,
        "grad_norm": 1.2921761274337769,
        "learning_rate": 8.998673807924292e-05,
        "epoch": 0.8450510063413289,
        "step": 3065
    },
    {
        "loss": 1.7909,
        "grad_norm": 2.0193779468536377,
        "learning_rate": 8.972643367574793e-05,
        "epoch": 0.8453267162944582,
        "step": 3066
    },
    {
        "loss": 2.3602,
        "grad_norm": 1.4630331993103027,
        "learning_rate": 8.946619960795373e-05,
        "epoch": 0.8456024262475875,
        "step": 3067
    },
    {
        "loss": 2.5962,
        "grad_norm": 1.1737295389175415,
        "learning_rate": 8.92060376574954e-05,
        "epoch": 0.8458781362007168,
        "step": 3068
    },
    {
        "loss": 1.9843,
        "grad_norm": 1.8504499197006226,
        "learning_rate": 8.894594960551416e-05,
        "epoch": 0.8461538461538461,
        "step": 3069
    },
    {
        "loss": 2.1522,
        "grad_norm": 1.3457844257354736,
        "learning_rate": 8.868593723264542e-05,
        "epoch": 0.8464295561069755,
        "step": 3070
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.5980006456375122,
        "learning_rate": 8.842600231900638e-05,
        "epoch": 0.8467052660601048,
        "step": 3071
    },
    {
        "loss": 2.0509,
        "grad_norm": 1.4518802165985107,
        "learning_rate": 8.816614664418396e-05,
        "epoch": 0.8469809760132341,
        "step": 3072
    },
    {
        "loss": 2.1782,
        "grad_norm": 1.4359885454177856,
        "learning_rate": 8.790637198722265e-05,
        "epoch": 0.8472566859663634,
        "step": 3073
    },
    {
        "loss": 2.0842,
        "grad_norm": 1.2433940172195435,
        "learning_rate": 8.764668012661214e-05,
        "epoch": 0.8475323959194927,
        "step": 3074
    },
    {
        "loss": 2.3058,
        "grad_norm": 1.582113265991211,
        "learning_rate": 8.73870728402754e-05,
        "epoch": 0.847808105872622,
        "step": 3075
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.5808099508285522,
        "learning_rate": 8.71275519055563e-05,
        "epoch": 0.8480838158257513,
        "step": 3076
    },
    {
        "loss": 1.698,
        "grad_norm": 2.003894329071045,
        "learning_rate": 8.686811909920756e-05,
        "epoch": 0.8483595257788806,
        "step": 3077
    },
    {
        "loss": 2.6459,
        "grad_norm": 1.7064223289489746,
        "learning_rate": 8.660877619737856e-05,
        "epoch": 0.8486352357320099,
        "step": 3078
    },
    {
        "loss": 1.8657,
        "grad_norm": 1.7584311962127686,
        "learning_rate": 8.634952497560312e-05,
        "epoch": 0.8489109456851393,
        "step": 3079
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.0145641565322876,
        "learning_rate": 8.609036720878745e-05,
        "epoch": 0.8491866556382686,
        "step": 3080
    },
    {
        "loss": 1.5575,
        "grad_norm": 1.5880513191223145,
        "learning_rate": 8.583130467119787e-05,
        "epoch": 0.8494623655913979,
        "step": 3081
    },
    {
        "loss": 2.6615,
        "grad_norm": 1.1117879152297974,
        "learning_rate": 8.55723391364488e-05,
        "epoch": 0.8497380755445272,
        "step": 3082
    },
    {
        "loss": 2.1065,
        "grad_norm": 1.2361104488372803,
        "learning_rate": 8.531347237749055e-05,
        "epoch": 0.8500137854976565,
        "step": 3083
    },
    {
        "loss": 1.8001,
        "grad_norm": 1.3604074716567993,
        "learning_rate": 8.505470616659712e-05,
        "epoch": 0.8502894954507858,
        "step": 3084
    },
    {
        "loss": 2.0841,
        "grad_norm": 1.3618656396865845,
        "learning_rate": 8.479604227535422e-05,
        "epoch": 0.8505652054039151,
        "step": 3085
    },
    {
        "loss": 1.8817,
        "grad_norm": 1.678143858909607,
        "learning_rate": 8.453748247464694e-05,
        "epoch": 0.8508409153570444,
        "step": 3086
    },
    {
        "loss": 2.3355,
        "grad_norm": 1.6324009895324707,
        "learning_rate": 8.427902853464784e-05,
        "epoch": 0.8511166253101737,
        "step": 3087
    },
    {
        "loss": 1.9511,
        "grad_norm": 2.1030852794647217,
        "learning_rate": 8.40206822248047e-05,
        "epoch": 0.851392335263303,
        "step": 3088
    },
    {
        "loss": 2.3304,
        "grad_norm": 2.038910388946533,
        "learning_rate": 8.37624453138284e-05,
        "epoch": 0.8516680452164324,
        "step": 3089
    },
    {
        "loss": 2.1926,
        "grad_norm": 1.1531795263290405,
        "learning_rate": 8.350431956968088e-05,
        "epoch": 0.8519437551695617,
        "step": 3090
    },
    {
        "loss": 1.241,
        "grad_norm": 2.692307233810425,
        "learning_rate": 8.324630675956295e-05,
        "epoch": 0.852219465122691,
        "step": 3091
    },
    {
        "loss": 1.5554,
        "grad_norm": 2.414001941680908,
        "learning_rate": 8.298840864990231e-05,
        "epoch": 0.8524951750758203,
        "step": 3092
    },
    {
        "loss": 2.138,
        "grad_norm": 2.055368185043335,
        "learning_rate": 8.273062700634138e-05,
        "epoch": 0.8527708850289496,
        "step": 3093
    },
    {
        "loss": 1.3558,
        "grad_norm": 2.4095895290374756,
        "learning_rate": 8.247296359372515e-05,
        "epoch": 0.8530465949820788,
        "step": 3094
    },
    {
        "loss": 2.2093,
        "grad_norm": 1.533478856086731,
        "learning_rate": 8.221542017608926e-05,
        "epoch": 0.8533223049352081,
        "step": 3095
    },
    {
        "loss": 1.7285,
        "grad_norm": 1.3392819166183472,
        "learning_rate": 8.195799851664776e-05,
        "epoch": 0.8535980148883374,
        "step": 3096
    },
    {
        "loss": 2.0334,
        "grad_norm": 1.30946946144104,
        "learning_rate": 8.170070037778116e-05,
        "epoch": 0.8538737248414667,
        "step": 3097
    },
    {
        "loss": 2.2336,
        "grad_norm": 0.9366241693496704,
        "learning_rate": 8.144352752102424e-05,
        "epoch": 0.854149434794596,
        "step": 3098
    },
    {
        "loss": 1.9977,
        "grad_norm": 2.1045989990234375,
        "learning_rate": 8.118648170705417e-05,
        "epoch": 0.8544251447477254,
        "step": 3099
    },
    {
        "loss": 1.7739,
        "grad_norm": 1.464600920677185,
        "learning_rate": 8.092956469567829e-05,
        "epoch": 0.8547008547008547,
        "step": 3100
    },
    {
        "loss": 2.0583,
        "grad_norm": 1.5001391172409058,
        "learning_rate": 8.067277824582209e-05,
        "epoch": 0.854976564653984,
        "step": 3101
    },
    {
        "loss": 2.2496,
        "grad_norm": 1.5018203258514404,
        "learning_rate": 8.041612411551727e-05,
        "epoch": 0.8552522746071133,
        "step": 3102
    },
    {
        "loss": 2.1433,
        "grad_norm": 1.0542210340499878,
        "learning_rate": 8.015960406188957e-05,
        "epoch": 0.8555279845602426,
        "step": 3103
    },
    {
        "loss": 2.0835,
        "grad_norm": 1.718593716621399,
        "learning_rate": 7.990321984114684e-05,
        "epoch": 0.8558036945133719,
        "step": 3104
    },
    {
        "loss": 2.264,
        "grad_norm": 1.358415126800537,
        "learning_rate": 7.964697320856701e-05,
        "epoch": 0.8560794044665012,
        "step": 3105
    },
    {
        "loss": 2.1936,
        "grad_norm": 1.1517049074172974,
        "learning_rate": 7.939086591848595e-05,
        "epoch": 0.8563551144196305,
        "step": 3106
    },
    {
        "loss": 2.18,
        "grad_norm": 2.0096588134765625,
        "learning_rate": 7.913489972428565e-05,
        "epoch": 0.8566308243727598,
        "step": 3107
    },
    {
        "loss": 2.2834,
        "grad_norm": 1.4701178073883057,
        "learning_rate": 7.887907637838203e-05,
        "epoch": 0.8569065343258891,
        "step": 3108
    },
    {
        "loss": 2.1085,
        "grad_norm": 1.447936773300171,
        "learning_rate": 7.862339763221311e-05,
        "epoch": 0.8571822442790185,
        "step": 3109
    },
    {
        "loss": 2.3871,
        "grad_norm": 1.7051491737365723,
        "learning_rate": 7.836786523622693e-05,
        "epoch": 0.8574579542321478,
        "step": 3110
    },
    {
        "loss": 2.5732,
        "grad_norm": 1.241816520690918,
        "learning_rate": 7.811248093986949e-05,
        "epoch": 0.8577336641852771,
        "step": 3111
    },
    {
        "loss": 2.2497,
        "grad_norm": 2.2096657752990723,
        "learning_rate": 7.785724649157299e-05,
        "epoch": 0.8580093741384064,
        "step": 3112
    },
    {
        "loss": 2.1815,
        "grad_norm": 1.093170404434204,
        "learning_rate": 7.76021636387436e-05,
        "epoch": 0.8582850840915357,
        "step": 3113
    },
    {
        "loss": 1.7875,
        "grad_norm": 1.9906829595565796,
        "learning_rate": 7.73472341277497e-05,
        "epoch": 0.858560794044665,
        "step": 3114
    },
    {
        "loss": 2.1731,
        "grad_norm": 1.709455966949463,
        "learning_rate": 7.709245970390987e-05,
        "epoch": 0.8588365039977943,
        "step": 3115
    },
    {
        "loss": 1.4268,
        "grad_norm": 1.7681523561477661,
        "learning_rate": 7.683784211148085e-05,
        "epoch": 0.8591122139509236,
        "step": 3116
    },
    {
        "loss": 2.5444,
        "grad_norm": 1.3539928197860718,
        "learning_rate": 7.65833830936457e-05,
        "epoch": 0.8593879239040529,
        "step": 3117
    },
    {
        "loss": 2.1674,
        "grad_norm": 1.2196317911148071,
        "learning_rate": 7.632908439250184e-05,
        "epoch": 0.8596636338571823,
        "step": 3118
    },
    {
        "loss": 1.8741,
        "grad_norm": 1.4069366455078125,
        "learning_rate": 7.607494774904911e-05,
        "epoch": 0.8599393438103116,
        "step": 3119
    },
    {
        "loss": 1.7401,
        "grad_norm": 1.8047598600387573,
        "learning_rate": 7.58209749031779e-05,
        "epoch": 0.8602150537634409,
        "step": 3120
    },
    {
        "loss": 1.9296,
        "grad_norm": 1.988231897354126,
        "learning_rate": 7.55671675936571e-05,
        "epoch": 0.8604907637165702,
        "step": 3121
    },
    {
        "loss": 2.2097,
        "grad_norm": 1.418829083442688,
        "learning_rate": 7.531352755812239e-05,
        "epoch": 0.8607664736696995,
        "step": 3122
    },
    {
        "loss": 1.6287,
        "grad_norm": 1.4436362981796265,
        "learning_rate": 7.506005653306419e-05,
        "epoch": 0.8610421836228288,
        "step": 3123
    },
    {
        "loss": 1.4937,
        "grad_norm": 2.1448073387145996,
        "learning_rate": 7.480675625381584e-05,
        "epoch": 0.8613178935759581,
        "step": 3124
    },
    {
        "loss": 2.0319,
        "grad_norm": 1.680899977684021,
        "learning_rate": 7.455362845454172e-05,
        "epoch": 0.8615936035290874,
        "step": 3125
    },
    {
        "loss": 2.5169,
        "grad_norm": 1.3760179281234741,
        "learning_rate": 7.430067486822533e-05,
        "epoch": 0.8618693134822167,
        "step": 3126
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.376321315765381,
        "learning_rate": 7.404789722665752e-05,
        "epoch": 0.862145023435346,
        "step": 3127
    },
    {
        "loss": 2.2157,
        "grad_norm": 1.763814091682434,
        "learning_rate": 7.379529726042448e-05,
        "epoch": 0.8624207333884754,
        "step": 3128
    },
    {
        "loss": 2.2566,
        "grad_norm": 1.4150981903076172,
        "learning_rate": 7.354287669889608e-05,
        "epoch": 0.8626964433416047,
        "step": 3129
    },
    {
        "loss": 2.2872,
        "grad_norm": 1.2542248964309692,
        "learning_rate": 7.32906372702139e-05,
        "epoch": 0.862972153294734,
        "step": 3130
    },
    {
        "loss": 2.525,
        "grad_norm": 1.412894606590271,
        "learning_rate": 7.303858070127941e-05,
        "epoch": 0.8632478632478633,
        "step": 3131
    },
    {
        "loss": 2.4384,
        "grad_norm": 1.2388153076171875,
        "learning_rate": 7.278670871774223e-05,
        "epoch": 0.8635235732009926,
        "step": 3132
    },
    {
        "loss": 1.8341,
        "grad_norm": 1.1445385217666626,
        "learning_rate": 7.253502304398814e-05,
        "epoch": 0.8637992831541219,
        "step": 3133
    },
    {
        "loss": 2.0699,
        "grad_norm": 1.3924553394317627,
        "learning_rate": 7.228352540312753e-05,
        "epoch": 0.8640749931072512,
        "step": 3134
    },
    {
        "loss": 2.3466,
        "grad_norm": 1.3201324939727783,
        "learning_rate": 7.203221751698339e-05,
        "epoch": 0.8643507030603805,
        "step": 3135
    },
    {
        "loss": 1.215,
        "grad_norm": 1.827773928642273,
        "learning_rate": 7.178110110607961e-05,
        "epoch": 0.8646264130135098,
        "step": 3136
    },
    {
        "loss": 1.9133,
        "grad_norm": 1.366489052772522,
        "learning_rate": 7.15301778896292e-05,
        "epoch": 0.864902122966639,
        "step": 3137
    },
    {
        "loss": 2.2436,
        "grad_norm": 1.4735745191574097,
        "learning_rate": 7.127944958552246e-05,
        "epoch": 0.8651778329197684,
        "step": 3138
    },
    {
        "loss": 2.3874,
        "grad_norm": 1.0562695264816284,
        "learning_rate": 7.102891791031534e-05,
        "epoch": 0.8654535428728977,
        "step": 3139
    },
    {
        "loss": 2.3985,
        "grad_norm": 1.3123540878295898,
        "learning_rate": 7.077858457921752e-05,
        "epoch": 0.865729252826027,
        "step": 3140
    },
    {
        "loss": 2.02,
        "grad_norm": 2.1402382850646973,
        "learning_rate": 7.052845130608086e-05,
        "epoch": 0.8660049627791563,
        "step": 3141
    },
    {
        "loss": 1.8153,
        "grad_norm": 1.6397992372512817,
        "learning_rate": 7.027851980338751e-05,
        "epoch": 0.8662806727322856,
        "step": 3142
    },
    {
        "loss": 2.5481,
        "grad_norm": 1.3407392501831055,
        "learning_rate": 7.002879178223825e-05,
        "epoch": 0.8665563826854149,
        "step": 3143
    },
    {
        "loss": 1.6806,
        "grad_norm": 1.3773263692855835,
        "learning_rate": 6.977926895234078e-05,
        "epoch": 0.8668320926385442,
        "step": 3144
    },
    {
        "loss": 2.404,
        "grad_norm": 1.5922526121139526,
        "learning_rate": 6.952995302199798e-05,
        "epoch": 0.8671078025916735,
        "step": 3145
    },
    {
        "loss": 2.2314,
        "grad_norm": 1.2845971584320068,
        "learning_rate": 6.928084569809626e-05,
        "epoch": 0.8673835125448028,
        "step": 3146
    },
    {
        "loss": 2.0679,
        "grad_norm": 1.7835273742675781,
        "learning_rate": 6.903194868609387e-05,
        "epoch": 0.8676592224979321,
        "step": 3147
    },
    {
        "loss": 1.5215,
        "grad_norm": 1.763952374458313,
        "learning_rate": 6.878326369000915e-05,
        "epoch": 0.8679349324510615,
        "step": 3148
    },
    {
        "loss": 2.0298,
        "grad_norm": 2.0900774002075195,
        "learning_rate": 6.853479241240898e-05,
        "epoch": 0.8682106424041908,
        "step": 3149
    },
    {
        "loss": 1.9748,
        "grad_norm": 1.3224937915802002,
        "learning_rate": 6.828653655439703e-05,
        "epoch": 0.8684863523573201,
        "step": 3150
    },
    {
        "loss": 2.1703,
        "grad_norm": 1.5542967319488525,
        "learning_rate": 6.803849781560214e-05,
        "epoch": 0.8687620623104494,
        "step": 3151
    },
    {
        "loss": 1.413,
        "grad_norm": 1.7889355421066284,
        "learning_rate": 6.77906778941667e-05,
        "epoch": 0.8690377722635787,
        "step": 3152
    },
    {
        "loss": 2.2245,
        "grad_norm": 1.642160177230835,
        "learning_rate": 6.754307848673506e-05,
        "epoch": 0.869313482216708,
        "step": 3153
    },
    {
        "loss": 1.8499,
        "grad_norm": 1.9761457443237305,
        "learning_rate": 6.72957012884418e-05,
        "epoch": 0.8695891921698373,
        "step": 3154
    },
    {
        "loss": 1.5877,
        "grad_norm": 1.606524109840393,
        "learning_rate": 6.704854799290022e-05,
        "epoch": 0.8698649021229666,
        "step": 3155
    },
    {
        "loss": 0.7752,
        "grad_norm": 2.1878559589385986,
        "learning_rate": 6.68016202921907e-05,
        "epoch": 0.8701406120760959,
        "step": 3156
    },
    {
        "loss": 2.4004,
        "grad_norm": 1.8735041618347168,
        "learning_rate": 6.655491987684924e-05,
        "epoch": 0.8704163220292253,
        "step": 3157
    },
    {
        "loss": 2.503,
        "grad_norm": 1.7940926551818848,
        "learning_rate": 6.630844843585562e-05,
        "epoch": 0.8706920319823546,
        "step": 3158
    },
    {
        "loss": 2.0136,
        "grad_norm": 3.0478062629699707,
        "learning_rate": 6.606220765662213e-05,
        "epoch": 0.8709677419354839,
        "step": 3159
    },
    {
        "loss": 1.9971,
        "grad_norm": 2.012179136276245,
        "learning_rate": 6.581619922498179e-05,
        "epoch": 0.8712434518886132,
        "step": 3160
    },
    {
        "loss": 1.9956,
        "grad_norm": 1.3368778228759766,
        "learning_rate": 6.557042482517695e-05,
        "epoch": 0.8715191618417425,
        "step": 3161
    },
    {
        "loss": 1.242,
        "grad_norm": 2.4402244091033936,
        "learning_rate": 6.532488613984775e-05,
        "epoch": 0.8717948717948718,
        "step": 3162
    },
    {
        "loss": 2.0738,
        "grad_norm": 1.688306212425232,
        "learning_rate": 6.507958485002048e-05,
        "epoch": 0.8720705817480011,
        "step": 3163
    },
    {
        "loss": 2.5012,
        "grad_norm": 1.7311147451400757,
        "learning_rate": 6.483452263509623e-05,
        "epoch": 0.8723462917011304,
        "step": 3164
    },
    {
        "loss": 2.0087,
        "grad_norm": 1.4198031425476074,
        "learning_rate": 6.458970117283923e-05,
        "epoch": 0.8726220016542597,
        "step": 3165
    },
    {
        "loss": 1.9377,
        "grad_norm": 1.6334731578826904,
        "learning_rate": 6.434512213936554e-05,
        "epoch": 0.872897711607389,
        "step": 3166
    },
    {
        "loss": 2.2561,
        "grad_norm": 1.7591612339019775,
        "learning_rate": 6.410078720913149e-05,
        "epoch": 0.8731734215605184,
        "step": 3167
    },
    {
        "loss": 2.2204,
        "grad_norm": 1.3849705457687378,
        "learning_rate": 6.385669805492207e-05,
        "epoch": 0.8734491315136477,
        "step": 3168
    },
    {
        "loss": 1.8738,
        "grad_norm": 2.1294825077056885,
        "learning_rate": 6.36128563478398e-05,
        "epoch": 0.873724841466777,
        "step": 3169
    },
    {
        "loss": 2.5176,
        "grad_norm": 0.8794296383857727,
        "learning_rate": 6.336926375729297e-05,
        "epoch": 0.8740005514199063,
        "step": 3170
    },
    {
        "loss": 2.2857,
        "grad_norm": 1.7685636281967163,
        "learning_rate": 6.312592195098442e-05,
        "epoch": 0.8742762613730356,
        "step": 3171
    },
    {
        "loss": 2.313,
        "grad_norm": 1.8284410238265991,
        "learning_rate": 6.288283259490006e-05,
        "epoch": 0.8745519713261649,
        "step": 3172
    },
    {
        "loss": 1.1454,
        "grad_norm": 3.1023948192596436,
        "learning_rate": 6.26399973532974e-05,
        "epoch": 0.8748276812792942,
        "step": 3173
    },
    {
        "loss": 1.7783,
        "grad_norm": 1.8157099485397339,
        "learning_rate": 6.239741788869429e-05,
        "epoch": 0.8751033912324235,
        "step": 3174
    },
    {
        "loss": 2.0959,
        "grad_norm": 1.364241600036621,
        "learning_rate": 6.215509586185733e-05,
        "epoch": 0.8753791011855528,
        "step": 3175
    },
    {
        "loss": 2.139,
        "grad_norm": 1.4619323015213013,
        "learning_rate": 6.191303293179079e-05,
        "epoch": 0.8756548111386822,
        "step": 3176
    },
    {
        "loss": 2.1476,
        "grad_norm": 2.0007734298706055,
        "learning_rate": 6.167123075572495e-05,
        "epoch": 0.8759305210918115,
        "step": 3177
    },
    {
        "loss": 2.0679,
        "grad_norm": 1.5643177032470703,
        "learning_rate": 6.142969098910498e-05,
        "epoch": 0.8762062310449408,
        "step": 3178
    },
    {
        "loss": 1.5288,
        "grad_norm": 1.8817226886749268,
        "learning_rate": 6.118841528557954e-05,
        "epoch": 0.87648194099807,
        "step": 3179
    },
    {
        "loss": 1.6835,
        "grad_norm": 2.1510748863220215,
        "learning_rate": 6.094740529698933e-05,
        "epoch": 0.8767576509511993,
        "step": 3180
    },
    {
        "loss": 2.4122,
        "grad_norm": 1.7522881031036377,
        "learning_rate": 6.0706662673356006e-05,
        "epoch": 0.8770333609043286,
        "step": 3181
    },
    {
        "loss": 2.3099,
        "grad_norm": 1.1834275722503662,
        "learning_rate": 6.0466189062870706e-05,
        "epoch": 0.8773090708574579,
        "step": 3182
    },
    {
        "loss": 2.3571,
        "grad_norm": 1.7306352853775024,
        "learning_rate": 6.022598611188284e-05,
        "epoch": 0.8775847808105872,
        "step": 3183
    },
    {
        "loss": 2.1518,
        "grad_norm": 1.832667589187622,
        "learning_rate": 5.9986055464888844e-05,
        "epoch": 0.8778604907637165,
        "step": 3184
    },
    {
        "loss": 2.2317,
        "grad_norm": 1.2138773202896118,
        "learning_rate": 5.9746398764520814e-05,
        "epoch": 0.8781362007168458,
        "step": 3185
    },
    {
        "loss": 2.0381,
        "grad_norm": 1.6377872228622437,
        "learning_rate": 5.950701765153541e-05,
        "epoch": 0.8784119106699751,
        "step": 3186
    },
    {
        "loss": 2.0989,
        "grad_norm": 1.2469779253005981,
        "learning_rate": 5.926791376480247e-05,
        "epoch": 0.8786876206231045,
        "step": 3187
    },
    {
        "loss": 2.3086,
        "grad_norm": 1.1251399517059326,
        "learning_rate": 5.9029088741293936e-05,
        "epoch": 0.8789633305762338,
        "step": 3188
    },
    {
        "loss": 1.7812,
        "grad_norm": 1.8483117818832397,
        "learning_rate": 5.879054421607252e-05,
        "epoch": 0.8792390405293631,
        "step": 3189
    },
    {
        "loss": 2.1771,
        "grad_norm": 2.0855939388275146,
        "learning_rate": 5.8552281822280564e-05,
        "epoch": 0.8795147504824924,
        "step": 3190
    },
    {
        "loss": 2.0308,
        "grad_norm": 1.2850885391235352,
        "learning_rate": 5.831430319112892e-05,
        "epoch": 0.8797904604356217,
        "step": 3191
    },
    {
        "loss": 1.5261,
        "grad_norm": 2.1667726039886475,
        "learning_rate": 5.8076609951885655e-05,
        "epoch": 0.880066170388751,
        "step": 3192
    },
    {
        "loss": 2.235,
        "grad_norm": 1.1679421663284302,
        "learning_rate": 5.7839203731864956e-05,
        "epoch": 0.8803418803418803,
        "step": 3193
    },
    {
        "loss": 2.039,
        "grad_norm": 1.3972227573394775,
        "learning_rate": 5.760208615641609e-05,
        "epoch": 0.8806175902950096,
        "step": 3194
    },
    {
        "loss": 1.3738,
        "grad_norm": 1.595295786857605,
        "learning_rate": 5.7365258848912096e-05,
        "epoch": 0.8808933002481389,
        "step": 3195
    },
    {
        "loss": 2.0274,
        "grad_norm": 1.652990460395813,
        "learning_rate": 5.712872343073874e-05,
        "epoch": 0.8811690102012683,
        "step": 3196
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.2692897319793701,
        "learning_rate": 5.689248152128357e-05,
        "epoch": 0.8814447201543976,
        "step": 3197
    },
    {
        "loss": 2.4423,
        "grad_norm": 1.4741616249084473,
        "learning_rate": 5.665653473792453e-05,
        "epoch": 0.8817204301075269,
        "step": 3198
    },
    {
        "loss": 1.75,
        "grad_norm": 1.971635341644287,
        "learning_rate": 5.6420884696019085e-05,
        "epoch": 0.8819961400606562,
        "step": 3199
    },
    {
        "loss": 2.123,
        "grad_norm": 2.322425603866577,
        "learning_rate": 5.6185533008893246e-05,
        "epoch": 0.8822718500137855,
        "step": 3200
    },
    {
        "loss": 1.9598,
        "grad_norm": 1.9508074522018433,
        "learning_rate": 5.595048128783026e-05,
        "epoch": 0.8825475599669148,
        "step": 3201
    },
    {
        "loss": 1.7406,
        "grad_norm": 2.3538401126861572,
        "learning_rate": 5.571573114205978e-05,
        "epoch": 0.8828232699200441,
        "step": 3202
    },
    {
        "loss": 2.2569,
        "grad_norm": 1.3051674365997314,
        "learning_rate": 5.548128417874675e-05,
        "epoch": 0.8830989798731734,
        "step": 3203
    },
    {
        "loss": 1.84,
        "grad_norm": 1.5744975805282593,
        "learning_rate": 5.5247142002980556e-05,
        "epoch": 0.8833746898263027,
        "step": 3204
    },
    {
        "loss": 2.3351,
        "grad_norm": 1.3865233659744263,
        "learning_rate": 5.50133062177638e-05,
        "epoch": 0.883650399779432,
        "step": 3205
    },
    {
        "loss": 1.8887,
        "grad_norm": 1.5391474962234497,
        "learning_rate": 5.477977842400147e-05,
        "epoch": 0.8839261097325614,
        "step": 3206
    },
    {
        "loss": 2.1881,
        "grad_norm": 1.6041549444198608,
        "learning_rate": 5.454656022049005e-05,
        "epoch": 0.8842018196856907,
        "step": 3207
    },
    {
        "loss": 2.0331,
        "grad_norm": 1.721622347831726,
        "learning_rate": 5.431365320390639e-05,
        "epoch": 0.88447752963882,
        "step": 3208
    },
    {
        "loss": 2.3505,
        "grad_norm": 1.285130262374878,
        "learning_rate": 5.408105896879684e-05,
        "epoch": 0.8847532395919493,
        "step": 3209
    },
    {
        "loss": 1.6849,
        "grad_norm": 2.1549088954925537,
        "learning_rate": 5.384877910756649e-05,
        "epoch": 0.8850289495450786,
        "step": 3210
    },
    {
        "loss": 2.0054,
        "grad_norm": 2.2020721435546875,
        "learning_rate": 5.361681521046804e-05,
        "epoch": 0.8853046594982079,
        "step": 3211
    },
    {
        "loss": 1.9945,
        "grad_norm": 1.6785428524017334,
        "learning_rate": 5.338516886559104e-05,
        "epoch": 0.8855803694513372,
        "step": 3212
    },
    {
        "loss": 1.7197,
        "grad_norm": 1.8139536380767822,
        "learning_rate": 5.315384165885091e-05,
        "epoch": 0.8858560794044665,
        "step": 3213
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.3591405153274536,
        "learning_rate": 5.2922835173978355e-05,
        "epoch": 0.8861317893575958,
        "step": 3214
    },
    {
        "loss": 2.001,
        "grad_norm": 1.691114068031311,
        "learning_rate": 5.269215099250814e-05,
        "epoch": 0.8864074993107252,
        "step": 3215
    },
    {
        "loss": 1.7815,
        "grad_norm": 1.1352401971817017,
        "learning_rate": 5.246179069376849e-05,
        "epoch": 0.8866832092638545,
        "step": 3216
    },
    {
        "loss": 2.1976,
        "grad_norm": 1.9184000492095947,
        "learning_rate": 5.223175585487035e-05,
        "epoch": 0.8869589192169838,
        "step": 3217
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.6965991258621216,
        "learning_rate": 5.200204805069635e-05,
        "epoch": 0.8872346291701131,
        "step": 3218
    },
    {
        "loss": 1.7463,
        "grad_norm": 1.6013742685317993,
        "learning_rate": 5.177266885389016e-05,
        "epoch": 0.8875103391232424,
        "step": 3219
    },
    {
        "loss": 2.0036,
        "grad_norm": 1.935341715812683,
        "learning_rate": 5.1543619834845814e-05,
        "epoch": 0.8877860490763717,
        "step": 3220
    },
    {
        "loss": 2.0888,
        "grad_norm": 1.676587462425232,
        "learning_rate": 5.131490256169675e-05,
        "epoch": 0.888061759029501,
        "step": 3221
    },
    {
        "loss": 2.5338,
        "grad_norm": 1.3588780164718628,
        "learning_rate": 5.108651860030524e-05,
        "epoch": 0.8883374689826302,
        "step": 3222
    },
    {
        "loss": 2.1608,
        "grad_norm": 1.7733229398727417,
        "learning_rate": 5.085846951425153e-05,
        "epoch": 0.8886131789357595,
        "step": 3223
    },
    {
        "loss": 2.0178,
        "grad_norm": 1.9949944019317627,
        "learning_rate": 5.0630756864823346e-05,
        "epoch": 0.8888888888888888,
        "step": 3224
    },
    {
        "loss": 2.071,
        "grad_norm": 1.6068609952926636,
        "learning_rate": 5.0403382211005004e-05,
        "epoch": 0.8891645988420182,
        "step": 3225
    },
    {
        "loss": 2.1512,
        "grad_norm": 1.804733157157898,
        "learning_rate": 5.0176347109466734e-05,
        "epoch": 0.8894403087951475,
        "step": 3226
    },
    {
        "loss": 2.0471,
        "grad_norm": 1.5459797382354736,
        "learning_rate": 4.994965311455426e-05,
        "epoch": 0.8897160187482768,
        "step": 3227
    },
    {
        "loss": 1.7675,
        "grad_norm": 2.1775286197662354,
        "learning_rate": 4.972330177827789e-05,
        "epoch": 0.8899917287014061,
        "step": 3228
    },
    {
        "loss": 2.1907,
        "grad_norm": 1.1739610433578491,
        "learning_rate": 4.949729465030193e-05,
        "epoch": 0.8902674386545354,
        "step": 3229
    },
    {
        "loss": 2.2714,
        "grad_norm": 1.2375199794769287,
        "learning_rate": 4.927163327793433e-05,
        "epoch": 0.8905431486076647,
        "step": 3230
    },
    {
        "loss": 2.2035,
        "grad_norm": 1.5022993087768555,
        "learning_rate": 4.904631920611574e-05,
        "epoch": 0.890818858560794,
        "step": 3231
    },
    {
        "loss": 2.45,
        "grad_norm": 1.3713005781173706,
        "learning_rate": 4.882135397740916e-05,
        "epoch": 0.8910945685139233,
        "step": 3232
    },
    {
        "loss": 1.1228,
        "grad_norm": 2.7714812755584717,
        "learning_rate": 4.8596739131989256e-05,
        "epoch": 0.8913702784670526,
        "step": 3233
    },
    {
        "loss": 2.0061,
        "grad_norm": 1.78300940990448,
        "learning_rate": 4.837247620763198e-05,
        "epoch": 0.891645988420182,
        "step": 3234
    },
    {
        "loss": 1.9115,
        "grad_norm": 2.0911149978637695,
        "learning_rate": 4.814856673970384e-05,
        "epoch": 0.8919216983733113,
        "step": 3235
    },
    {
        "loss": 2.4413,
        "grad_norm": 1.158045768737793,
        "learning_rate": 4.792501226115147e-05,
        "epoch": 0.8921974083264406,
        "step": 3236
    },
    {
        "loss": 1.801,
        "grad_norm": 1.9832227230072021,
        "learning_rate": 4.7701814302491264e-05,
        "epoch": 0.8924731182795699,
        "step": 3237
    },
    {
        "loss": 1.7194,
        "grad_norm": 1.675601840019226,
        "learning_rate": 4.747897439179868e-05,
        "epoch": 0.8927488282326992,
        "step": 3238
    },
    {
        "loss": 2.261,
        "grad_norm": 1.823276400566101,
        "learning_rate": 4.72564940546979e-05,
        "epoch": 0.8930245381858285,
        "step": 3239
    },
    {
        "loss": 1.6941,
        "grad_norm": 1.8645833730697632,
        "learning_rate": 4.703437481435133e-05,
        "epoch": 0.8933002481389578,
        "step": 3240
    },
    {
        "loss": 2.3485,
        "grad_norm": 1.406127691268921,
        "learning_rate": 4.681261819144933e-05,
        "epoch": 0.8935759580920871,
        "step": 3241
    },
    {
        "loss": 1.7842,
        "grad_norm": 1.4371590614318848,
        "learning_rate": 4.659122570419956e-05,
        "epoch": 0.8938516680452164,
        "step": 3242
    },
    {
        "loss": 2.3711,
        "grad_norm": 1.5773354768753052,
        "learning_rate": 4.63701988683167e-05,
        "epoch": 0.8941273779983457,
        "step": 3243
    },
    {
        "loss": 2.1266,
        "grad_norm": 1.9254472255706787,
        "learning_rate": 4.61495391970122e-05,
        "epoch": 0.894403087951475,
        "step": 3244
    },
    {
        "loss": 2.4162,
        "grad_norm": 1.228817105293274,
        "learning_rate": 4.59292482009837e-05,
        "epoch": 0.8946787979046044,
        "step": 3245
    },
    {
        "loss": 2.2574,
        "grad_norm": 1.660067081451416,
        "learning_rate": 4.5709327388404756e-05,
        "epoch": 0.8949545078577337,
        "step": 3246
    },
    {
        "loss": 1.3849,
        "grad_norm": 2.129497528076172,
        "learning_rate": 4.548977826491466e-05,
        "epoch": 0.895230217810863,
        "step": 3247
    },
    {
        "loss": 2.5169,
        "grad_norm": 2.077810049057007,
        "learning_rate": 4.5270602333607936e-05,
        "epoch": 0.8955059277639923,
        "step": 3248
    },
    {
        "loss": 2.0642,
        "grad_norm": 1.5093451738357544,
        "learning_rate": 4.505180109502415e-05,
        "epoch": 0.8957816377171216,
        "step": 3249
    },
    {
        "loss": 2.019,
        "grad_norm": 1.6642067432403564,
        "learning_rate": 4.483337604713756e-05,
        "epoch": 0.8960573476702509,
        "step": 3250
    },
    {
        "loss": 0.9052,
        "grad_norm": 2.917797803878784,
        "learning_rate": 4.461532868534706e-05,
        "epoch": 0.8963330576233802,
        "step": 3251
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.266760230064392,
        "learning_rate": 4.439766050246566e-05,
        "epoch": 0.8966087675765095,
        "step": 3252
    },
    {
        "loss": 1.7,
        "grad_norm": 1.9186630249023438,
        "learning_rate": 4.418037298871044e-05,
        "epoch": 0.8968844775296388,
        "step": 3253
    },
    {
        "loss": 2.0803,
        "grad_norm": 1.3751264810562134,
        "learning_rate": 4.396346763169239e-05,
        "epoch": 0.8971601874827682,
        "step": 3254
    },
    {
        "loss": 2.1695,
        "grad_norm": 1.1981275081634521,
        "learning_rate": 4.374694591640606e-05,
        "epoch": 0.8974358974358975,
        "step": 3255
    },
    {
        "loss": 1.6995,
        "grad_norm": 1.788588523864746,
        "learning_rate": 4.353080932521947e-05,
        "epoch": 0.8977116073890268,
        "step": 3256
    },
    {
        "loss": 2.1059,
        "grad_norm": 2.023660182952881,
        "learning_rate": 4.331505933786409e-05,
        "epoch": 0.8979873173421561,
        "step": 3257
    },
    {
        "loss": 1.3016,
        "grad_norm": 3.0239548683166504,
        "learning_rate": 4.309969743142448e-05,
        "epoch": 0.8982630272952854,
        "step": 3258
    },
    {
        "loss": 1.8935,
        "grad_norm": 1.5851033926010132,
        "learning_rate": 4.2884725080328323e-05,
        "epoch": 0.8985387372484147,
        "step": 3259
    },
    {
        "loss": 1.4141,
        "grad_norm": 1.8961397409439087,
        "learning_rate": 4.267014375633626e-05,
        "epoch": 0.898814447201544,
        "step": 3260
    },
    {
        "loss": 2.4533,
        "grad_norm": 1.7682795524597168,
        "learning_rate": 4.245595492853197e-05,
        "epoch": 0.8990901571546733,
        "step": 3261
    },
    {
        "loss": 2.343,
        "grad_norm": 1.468862533569336,
        "learning_rate": 4.2242160063311875e-05,
        "epoch": 0.8993658671078026,
        "step": 3262
    },
    {
        "loss": 1.7959,
        "grad_norm": 1.9532305002212524,
        "learning_rate": 4.202876062437523e-05,
        "epoch": 0.899641577060932,
        "step": 3263
    },
    {
        "loss": 2.1827,
        "grad_norm": 1.9529937505722046,
        "learning_rate": 4.1815758072714186e-05,
        "epoch": 0.8999172870140613,
        "step": 3264
    },
    {
        "loss": 2.4264,
        "grad_norm": 1.3176281452178955,
        "learning_rate": 4.160315386660358e-05,
        "epoch": 0.9001929969671905,
        "step": 3265
    },
    {
        "loss": 2.4207,
        "grad_norm": 1.1702736616134644,
        "learning_rate": 4.139094946159111e-05,
        "epoch": 0.9004687069203198,
        "step": 3266
    },
    {
        "loss": 2.4525,
        "grad_norm": 1.5972321033477783,
        "learning_rate": 4.117914631048736e-05,
        "epoch": 0.9007444168734491,
        "step": 3267
    },
    {
        "loss": 1.8313,
        "grad_norm": 1.7443381547927856,
        "learning_rate": 4.096774586335578e-05,
        "epoch": 0.9010201268265784,
        "step": 3268
    },
    {
        "loss": 1.9358,
        "grad_norm": 1.7548636198043823,
        "learning_rate": 4.0756749567502774e-05,
        "epoch": 0.9012958367797077,
        "step": 3269
    },
    {
        "loss": 2.3494,
        "grad_norm": 1.7981834411621094,
        "learning_rate": 4.054615886746782e-05,
        "epoch": 0.901571546732837,
        "step": 3270
    },
    {
        "loss": 1.5418,
        "grad_norm": 2.3054683208465576,
        "learning_rate": 4.0335975205013665e-05,
        "epoch": 0.9018472566859663,
        "step": 3271
    },
    {
        "loss": 2.0807,
        "grad_norm": 0.9349201917648315,
        "learning_rate": 4.0126200019116255e-05,
        "epoch": 0.9021229666390956,
        "step": 3272
    },
    {
        "loss": 1.917,
        "grad_norm": 1.129175066947937,
        "learning_rate": 3.991683474595498e-05,
        "epoch": 0.902398676592225,
        "step": 3273
    },
    {
        "loss": 0.9798,
        "grad_norm": 1.9105020761489868,
        "learning_rate": 3.970788081890299e-05,
        "epoch": 0.9026743865453543,
        "step": 3274
    },
    {
        "loss": 1.3129,
        "grad_norm": 2.3090767860412598,
        "learning_rate": 3.949933966851711e-05,
        "epoch": 0.9029500964984836,
        "step": 3275
    },
    {
        "loss": 2.2948,
        "grad_norm": 1.808563232421875,
        "learning_rate": 3.9291212722528225e-05,
        "epoch": 0.9032258064516129,
        "step": 3276
    },
    {
        "loss": 2.1473,
        "grad_norm": 1.254845142364502,
        "learning_rate": 3.9083501405831444e-05,
        "epoch": 0.9035015164047422,
        "step": 3277
    },
    {
        "loss": 1.3989,
        "grad_norm": 2.637946844100952,
        "learning_rate": 3.887620714047644e-05,
        "epoch": 0.9037772263578715,
        "step": 3278
    },
    {
        "loss": 1.6739,
        "grad_norm": 2.1535332202911377,
        "learning_rate": 3.866933134565754e-05,
        "epoch": 0.9040529363110008,
        "step": 3279
    },
    {
        "loss": 2.1472,
        "grad_norm": 1.5439472198486328,
        "learning_rate": 3.846287543770412e-05,
        "epoch": 0.9043286462641301,
        "step": 3280
    },
    {
        "loss": 2.3959,
        "grad_norm": 1.1051076650619507,
        "learning_rate": 3.825684083007096e-05,
        "epoch": 0.9046043562172594,
        "step": 3281
    },
    {
        "loss": 2.4053,
        "grad_norm": 1.0529606342315674,
        "learning_rate": 3.805122893332844e-05,
        "epoch": 0.9048800661703887,
        "step": 3282
    },
    {
        "loss": 1.8192,
        "grad_norm": 2.0795021057128906,
        "learning_rate": 3.784604115515293e-05,
        "epoch": 0.905155776123518,
        "step": 3283
    },
    {
        "loss": 1.7076,
        "grad_norm": 2.8827874660491943,
        "learning_rate": 3.764127890031726e-05,
        "epoch": 0.9054314860766474,
        "step": 3284
    },
    {
        "loss": 2.3156,
        "grad_norm": 1.5555063486099243,
        "learning_rate": 3.743694357068089e-05,
        "epoch": 0.9057071960297767,
        "step": 3285
    },
    {
        "loss": 1.7909,
        "grad_norm": 1.2645695209503174,
        "learning_rate": 3.7233036565180524e-05,
        "epoch": 0.905982905982906,
        "step": 3286
    },
    {
        "loss": 2.0942,
        "grad_norm": 1.9890724420547485,
        "learning_rate": 3.702955927982034e-05,
        "epoch": 0.9062586159360353,
        "step": 3287
    },
    {
        "loss": 1.4274,
        "grad_norm": 2.186568260192871,
        "learning_rate": 3.682651310766265e-05,
        "epoch": 0.9065343258891646,
        "step": 3288
    },
    {
        "loss": 2.1496,
        "grad_norm": 1.772706389427185,
        "learning_rate": 3.6623899438818175e-05,
        "epoch": 0.9068100358422939,
        "step": 3289
    },
    {
        "loss": 1.9141,
        "grad_norm": 2.2033839225769043,
        "learning_rate": 3.642171966043658e-05,
        "epoch": 0.9070857457954232,
        "step": 3290
    },
    {
        "loss": 2.4941,
        "grad_norm": 1.697153091430664,
        "learning_rate": 3.621997515669707e-05,
        "epoch": 0.9073614557485525,
        "step": 3291
    },
    {
        "loss": 2.7277,
        "grad_norm": 1.1748664379119873,
        "learning_rate": 3.601866730879878e-05,
        "epoch": 0.9076371657016818,
        "step": 3292
    },
    {
        "loss": 2.1814,
        "grad_norm": 1.4306578636169434,
        "learning_rate": 3.581779749495134e-05,
        "epoch": 0.9079128756548112,
        "step": 3293
    },
    {
        "loss": 2.0064,
        "grad_norm": 1.069154143333435,
        "learning_rate": 3.561736709036561e-05,
        "epoch": 0.9081885856079405,
        "step": 3294
    },
    {
        "loss": 2.1368,
        "grad_norm": 1.375891923904419,
        "learning_rate": 3.541737746724401e-05,
        "epoch": 0.9084642955610698,
        "step": 3295
    },
    {
        "loss": 1.8238,
        "grad_norm": 1.034615159034729,
        "learning_rate": 3.521782999477128e-05,
        "epoch": 0.9087400055141991,
        "step": 3296
    },
    {
        "loss": 1.8595,
        "grad_norm": 2.1123108863830566,
        "learning_rate": 3.5018726039105046e-05,
        "epoch": 0.9090157154673284,
        "step": 3297
    },
    {
        "loss": 2.124,
        "grad_norm": 1.8333922624588013,
        "learning_rate": 3.48200669633666e-05,
        "epoch": 0.9092914254204577,
        "step": 3298
    },
    {
        "loss": 2.6863,
        "grad_norm": 1.590179443359375,
        "learning_rate": 3.462185412763136e-05,
        "epoch": 0.909567135373587,
        "step": 3299
    },
    {
        "loss": 1.616,
        "grad_norm": 3.3931572437286377,
        "learning_rate": 3.442408888891965e-05,
        "epoch": 0.9098428453267163,
        "step": 3300
    },
    {
        "loss": 1.7393,
        "grad_norm": 2.752708911895752,
        "learning_rate": 3.422677260118753e-05,
        "epoch": 0.9101185552798456,
        "step": 3301
    },
    {
        "loss": 2.5174,
        "grad_norm": 1.1186000108718872,
        "learning_rate": 3.402990661531731e-05,
        "epoch": 0.910394265232975,
        "step": 3302
    },
    {
        "loss": 2.207,
        "grad_norm": 1.2029284238815308,
        "learning_rate": 3.38334922791084e-05,
        "epoch": 0.9106699751861043,
        "step": 3303
    },
    {
        "loss": 1.771,
        "grad_norm": 2.1092920303344727,
        "learning_rate": 3.3637530937268234e-05,
        "epoch": 0.9109456851392336,
        "step": 3304
    },
    {
        "loss": 1.924,
        "grad_norm": 1.7901266813278198,
        "learning_rate": 3.3442023931402754e-05,
        "epoch": 0.9112213950923629,
        "step": 3305
    },
    {
        "loss": 1.1898,
        "grad_norm": 2.2441961765289307,
        "learning_rate": 3.3246972600007476e-05,
        "epoch": 0.9114971050454922,
        "step": 3306
    },
    {
        "loss": 1.9117,
        "grad_norm": 2.2971487045288086,
        "learning_rate": 3.3052378278458174e-05,
        "epoch": 0.9117728149986214,
        "step": 3307
    },
    {
        "loss": 2.5497,
        "grad_norm": 1.081503987312317,
        "learning_rate": 3.285824229900194e-05,
        "epoch": 0.9120485249517507,
        "step": 3308
    },
    {
        "loss": 1.7183,
        "grad_norm": 2.129718780517578,
        "learning_rate": 3.26645659907478e-05,
        "epoch": 0.91232423490488,
        "step": 3309
    },
    {
        "loss": 2.1759,
        "grad_norm": 1.1692824363708496,
        "learning_rate": 3.247135067965775e-05,
        "epoch": 0.9125999448580093,
        "step": 3310
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.8726296424865723,
        "learning_rate": 3.227859768853777e-05,
        "epoch": 0.9128756548111386,
        "step": 3311
    },
    {
        "loss": 1.9511,
        "grad_norm": 2.2442164421081543,
        "learning_rate": 3.208630833702859e-05,
        "epoch": 0.913151364764268,
        "step": 3312
    },
    {
        "loss": 1.6897,
        "grad_norm": 1.202825665473938,
        "learning_rate": 3.189448394159673e-05,
        "epoch": 0.9134270747173973,
        "step": 3313
    },
    {
        "loss": 0.8016,
        "grad_norm": 2.3527698516845703,
        "learning_rate": 3.170312581552548e-05,
        "epoch": 0.9137027846705266,
        "step": 3314
    },
    {
        "loss": 1.8696,
        "grad_norm": 2.050204277038574,
        "learning_rate": 3.151223526890602e-05,
        "epoch": 0.9139784946236559,
        "step": 3315
    },
    {
        "loss": 1.9661,
        "grad_norm": 1.851082682609558,
        "learning_rate": 3.1321813608628244e-05,
        "epoch": 0.9142542045767852,
        "step": 3316
    },
    {
        "loss": 1.8352,
        "grad_norm": 1.6473383903503418,
        "learning_rate": 3.113186213837193e-05,
        "epoch": 0.9145299145299145,
        "step": 3317
    },
    {
        "loss": 2.1509,
        "grad_norm": 1.7848087549209595,
        "learning_rate": 3.094238215859788e-05,
        "epoch": 0.9148056244830438,
        "step": 3318
    },
    {
        "loss": 2.153,
        "grad_norm": 1.2075332403182983,
        "learning_rate": 3.075337496653887e-05,
        "epoch": 0.9150813344361731,
        "step": 3319
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.9308675527572632,
        "learning_rate": 3.056484185619082e-05,
        "epoch": 0.9153570443893024,
        "step": 3320
    },
    {
        "loss": 2.4955,
        "grad_norm": 1.5248498916625977,
        "learning_rate": 3.037678411830407e-05,
        "epoch": 0.9156327543424317,
        "step": 3321
    },
    {
        "loss": 1.7879,
        "grad_norm": 1.2921545505523682,
        "learning_rate": 3.0189203040374304e-05,
        "epoch": 0.915908464295561,
        "step": 3322
    },
    {
        "loss": 2.3356,
        "grad_norm": 1.5374109745025635,
        "learning_rate": 3.0002099906633908e-05,
        "epoch": 0.9161841742486904,
        "step": 3323
    },
    {
        "loss": 2.2369,
        "grad_norm": 1.5532325506210327,
        "learning_rate": 2.9815475998043107e-05,
        "epoch": 0.9164598842018197,
        "step": 3324
    },
    {
        "loss": 2.416,
        "grad_norm": 1.0763890743255615,
        "learning_rate": 2.962933259228129e-05,
        "epoch": 0.916735594154949,
        "step": 3325
    },
    {
        "loss": 2.3743,
        "grad_norm": 1.4454351663589478,
        "learning_rate": 2.944367096373811e-05,
        "epoch": 0.9170113041080783,
        "step": 3326
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.1205732822418213,
        "learning_rate": 2.9258492383504842e-05,
        "epoch": 0.9172870140612076,
        "step": 3327
    },
    {
        "loss": 1.7525,
        "grad_norm": 2.2331840991973877,
        "learning_rate": 2.9073798119365747e-05,
        "epoch": 0.9175627240143369,
        "step": 3328
    },
    {
        "loss": 2.2828,
        "grad_norm": 1.7102022171020508,
        "learning_rate": 2.8889589435789255e-05,
        "epoch": 0.9178384339674662,
        "step": 3329
    },
    {
        "loss": 1.5221,
        "grad_norm": 2.071610450744629,
        "learning_rate": 2.8705867593919366e-05,
        "epoch": 0.9181141439205955,
        "step": 3330
    },
    {
        "loss": 1.828,
        "grad_norm": 1.5091997385025024,
        "learning_rate": 2.8522633851567115e-05,
        "epoch": 0.9183898538737248,
        "step": 3331
    },
    {
        "loss": 1.6181,
        "grad_norm": 1.7442381381988525,
        "learning_rate": 2.8339889463201786e-05,
        "epoch": 0.9186655638268542,
        "step": 3332
    },
    {
        "loss": 1.112,
        "grad_norm": 2.2851192951202393,
        "learning_rate": 2.815763567994244e-05,
        "epoch": 0.9189412737799835,
        "step": 3333
    },
    {
        "loss": 2.5341,
        "grad_norm": 1.1763315200805664,
        "learning_rate": 2.7975873749549288e-05,
        "epoch": 0.9192169837331128,
        "step": 3334
    },
    {
        "loss": 2.3466,
        "grad_norm": 1.1004750728607178,
        "learning_rate": 2.7794604916415267e-05,
        "epoch": 0.9194926936862421,
        "step": 3335
    },
    {
        "loss": 2.0547,
        "grad_norm": 1.484853744506836,
        "learning_rate": 2.761383042155733e-05,
        "epoch": 0.9197684036393714,
        "step": 3336
    },
    {
        "loss": 2.0044,
        "grad_norm": 1.3182483911514282,
        "learning_rate": 2.743355150260808e-05,
        "epoch": 0.9200441135925007,
        "step": 3337
    },
    {
        "loss": 1.7981,
        "grad_norm": 2.141627073287964,
        "learning_rate": 2.7253769393807337e-05,
        "epoch": 0.92031982354563,
        "step": 3338
    },
    {
        "loss": 2.3276,
        "grad_norm": 0.9996241927146912,
        "learning_rate": 2.7074485325993537e-05,
        "epoch": 0.9205955334987593,
        "step": 3339
    },
    {
        "loss": 2.0164,
        "grad_norm": 1.614816427230835,
        "learning_rate": 2.689570052659539e-05,
        "epoch": 0.9208712434518886,
        "step": 3340
    },
    {
        "loss": 1.3778,
        "grad_norm": 1.4986051321029663,
        "learning_rate": 2.671741621962357e-05,
        "epoch": 0.921146953405018,
        "step": 3341
    },
    {
        "loss": 1.5234,
        "grad_norm": 1.2899258136749268,
        "learning_rate": 2.6539633625662143e-05,
        "epoch": 0.9214226633581473,
        "step": 3342
    },
    {
        "loss": 1.9066,
        "grad_norm": 1.3159081935882568,
        "learning_rate": 2.636235396186032e-05,
        "epoch": 0.9216983733112766,
        "step": 3343
    },
    {
        "loss": 2.6294,
        "grad_norm": 1.2090715169906616,
        "learning_rate": 2.6185578441924118e-05,
        "epoch": 0.9219740832644059,
        "step": 3344
    },
    {
        "loss": 1.8385,
        "grad_norm": 1.5738674402236938,
        "learning_rate": 2.6009308276108123e-05,
        "epoch": 0.9222497932175352,
        "step": 3345
    },
    {
        "loss": 1.5931,
        "grad_norm": 1.8747831583023071,
        "learning_rate": 2.583354467120701e-05,
        "epoch": 0.9225255031706645,
        "step": 3346
    },
    {
        "loss": 2.323,
        "grad_norm": 1.139096736907959,
        "learning_rate": 2.5658288830547438e-05,
        "epoch": 0.9228012131237938,
        "step": 3347
    },
    {
        "loss": 2.1442,
        "grad_norm": 1.1933649778366089,
        "learning_rate": 2.548354195397983e-05,
        "epoch": 0.9230769230769231,
        "step": 3348
    },
    {
        "loss": 2.2522,
        "grad_norm": 1.5984232425689697,
        "learning_rate": 2.5309305237870008e-05,
        "epoch": 0.9233526330300524,
        "step": 3349
    },
    {
        "loss": 1.9526,
        "grad_norm": 2.276442289352417,
        "learning_rate": 2.5135579875091144e-05,
        "epoch": 0.9236283429831816,
        "step": 3350
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.7269471883773804,
        "learning_rate": 2.49623670550155e-05,
        "epoch": 0.923904052936311,
        "step": 3351
    },
    {
        "loss": 2.2158,
        "grad_norm": 1.3144830465316772,
        "learning_rate": 2.4789667963506412e-05,
        "epoch": 0.9241797628894403,
        "step": 3352
    },
    {
        "loss": 1.8941,
        "grad_norm": 2.1879003047943115,
        "learning_rate": 2.461748378291001e-05,
        "epoch": 0.9244554728425696,
        "step": 3353
    },
    {
        "loss": 2.1279,
        "grad_norm": 1.6348021030426025,
        "learning_rate": 2.4445815692047202e-05,
        "epoch": 0.9247311827956989,
        "step": 3354
    },
    {
        "loss": 2.1846,
        "grad_norm": 1.2522765398025513,
        "learning_rate": 2.4274664866205675e-05,
        "epoch": 0.9250068927488282,
        "step": 3355
    },
    {
        "loss": 2.1957,
        "grad_norm": 0.9516206383705139,
        "learning_rate": 2.410403247713169e-05,
        "epoch": 0.9252826027019575,
        "step": 3356
    },
    {
        "loss": 2.4013,
        "grad_norm": 1.2928495407104492,
        "learning_rate": 2.3933919693022168e-05,
        "epoch": 0.9255583126550868,
        "step": 3357
    },
    {
        "loss": 1.8973,
        "grad_norm": 1.482809066772461,
        "learning_rate": 2.3764327678516707e-05,
        "epoch": 0.9258340226082161,
        "step": 3358
    },
    {
        "loss": 1.6487,
        "grad_norm": 2.0093393325805664,
        "learning_rate": 2.3595257594689557e-05,
        "epoch": 0.9261097325613454,
        "step": 3359
    },
    {
        "loss": 2.1659,
        "grad_norm": 1.2393405437469482,
        "learning_rate": 2.3426710599041647e-05,
        "epoch": 0.9263854425144747,
        "step": 3360
    },
    {
        "loss": 2.0849,
        "grad_norm": 1.3657848834991455,
        "learning_rate": 2.32586878454927e-05,
        "epoch": 0.926661152467604,
        "step": 3361
    },
    {
        "loss": 2.436,
        "grad_norm": 2.2394933700561523,
        "learning_rate": 2.3091190484373415e-05,
        "epoch": 0.9269368624207334,
        "step": 3362
    },
    {
        "loss": 2.0072,
        "grad_norm": 1.8260831832885742,
        "learning_rate": 2.2924219662417412e-05,
        "epoch": 0.9272125723738627,
        "step": 3363
    },
    {
        "loss": 2.1149,
        "grad_norm": 1.525170087814331,
        "learning_rate": 2.2757776522753504e-05,
        "epoch": 0.927488282326992,
        "step": 3364
    },
    {
        "loss": 2.3534,
        "grad_norm": 1.3538869619369507,
        "learning_rate": 2.2591862204897896e-05,
        "epoch": 0.9277639922801213,
        "step": 3365
    },
    {
        "loss": 2.5404,
        "grad_norm": 1.7239243984222412,
        "learning_rate": 2.2426477844746273e-05,
        "epoch": 0.9280397022332506,
        "step": 3366
    },
    {
        "loss": 2.4653,
        "grad_norm": 0.9296047687530518,
        "learning_rate": 2.2261624574566053e-05,
        "epoch": 0.9283154121863799,
        "step": 3367
    },
    {
        "loss": 1.5028,
        "grad_norm": 1.7254045009613037,
        "learning_rate": 2.2097303522988778e-05,
        "epoch": 0.9285911221395092,
        "step": 3368
    },
    {
        "loss": 2.3193,
        "grad_norm": 1.5214269161224365,
        "learning_rate": 2.193351581500217e-05,
        "epoch": 0.9288668320926385,
        "step": 3369
    },
    {
        "loss": 1.7812,
        "grad_norm": 1.4674359560012817,
        "learning_rate": 2.177026257194256e-05,
        "epoch": 0.9291425420457678,
        "step": 3370
    },
    {
        "loss": 2.2606,
        "grad_norm": 1.1940900087356567,
        "learning_rate": 2.1607544911487177e-05,
        "epoch": 0.9294182519988972,
        "step": 3371
    },
    {
        "loss": 1.1273,
        "grad_norm": 2.747933864593506,
        "learning_rate": 2.144536394764656e-05,
        "epoch": 0.9296939619520265,
        "step": 3372
    },
    {
        "loss": 2.2539,
        "grad_norm": 1.2486162185668945,
        "learning_rate": 2.128372079075679e-05,
        "epoch": 0.9299696719051558,
        "step": 3373
    },
    {
        "loss": 2.2095,
        "grad_norm": 1.639892578125,
        "learning_rate": 2.1122616547471997e-05,
        "epoch": 0.9302453818582851,
        "step": 3374
    },
    {
        "loss": 1.8977,
        "grad_norm": 1.4561171531677246,
        "learning_rate": 2.0962052320756807e-05,
        "epoch": 0.9305210918114144,
        "step": 3375
    },
    {
        "loss": 2.0586,
        "grad_norm": 1.9033167362213135,
        "learning_rate": 2.0802029209878706e-05,
        "epoch": 0.9307968017645437,
        "step": 3376
    },
    {
        "loss": 2.1634,
        "grad_norm": 1.6126528978347778,
        "learning_rate": 2.0642548310400488e-05,
        "epoch": 0.931072511717673,
        "step": 3377
    },
    {
        "loss": 2.3766,
        "grad_norm": 1.0869073867797852,
        "learning_rate": 2.0483610714172962e-05,
        "epoch": 0.9313482216708023,
        "step": 3378
    },
    {
        "loss": 2.2119,
        "grad_norm": 1.5787452459335327,
        "learning_rate": 2.03252175093272e-05,
        "epoch": 0.9316239316239316,
        "step": 3379
    },
    {
        "loss": 1.7288,
        "grad_norm": 2.487454891204834,
        "learning_rate": 2.0167369780267264e-05,
        "epoch": 0.931899641577061,
        "step": 3380
    },
    {
        "loss": 1.9889,
        "grad_norm": 1.325023889541626,
        "learning_rate": 2.0010068607662723e-05,
        "epoch": 0.9321753515301903,
        "step": 3381
    },
    {
        "loss": 2.1873,
        "grad_norm": 1.1975177526474,
        "learning_rate": 1.9853315068441336e-05,
        "epoch": 0.9324510614833196,
        "step": 3382
    },
    {
        "loss": 2.1345,
        "grad_norm": 1.6175726652145386,
        "learning_rate": 1.9697110235781523e-05,
        "epoch": 0.9327267714364489,
        "step": 3383
    },
    {
        "loss": 1.8527,
        "grad_norm": 2.0693607330322266,
        "learning_rate": 1.9541455179105116e-05,
        "epoch": 0.9330024813895782,
        "step": 3384
    },
    {
        "loss": 2.0189,
        "grad_norm": 1.5903170108795166,
        "learning_rate": 1.938635096407011e-05,
        "epoch": 0.9332781913427075,
        "step": 3385
    },
    {
        "loss": 2.1514,
        "grad_norm": 1.4371936321258545,
        "learning_rate": 1.9231798652563194e-05,
        "epoch": 0.9335539012958368,
        "step": 3386
    },
    {
        "loss": 1.8019,
        "grad_norm": 1.7058221101760864,
        "learning_rate": 1.90777993026926e-05,
        "epoch": 0.9338296112489661,
        "step": 3387
    },
    {
        "loss": 2.3204,
        "grad_norm": 1.7000588178634644,
        "learning_rate": 1.8924353968780827e-05,
        "epoch": 0.9341053212020954,
        "step": 3388
    },
    {
        "loss": 1.5389,
        "grad_norm": 2.2913055419921875,
        "learning_rate": 1.8771463701357484e-05,
        "epoch": 0.9343810311552248,
        "step": 3389
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.0628761053085327,
        "learning_rate": 1.8619129547151936e-05,
        "epoch": 0.9346567411083541,
        "step": 3390
    },
    {
        "loss": 1.4727,
        "grad_norm": 1.6533911228179932,
        "learning_rate": 1.8467352549086303e-05,
        "epoch": 0.9349324510614834,
        "step": 3391
    },
    {
        "loss": 1.3026,
        "grad_norm": 1.9430443048477173,
        "learning_rate": 1.831613374626828e-05,
        "epoch": 0.9352081610146127,
        "step": 3392
    },
    {
        "loss": 2.5278,
        "grad_norm": 1.299410343170166,
        "learning_rate": 1.8165474173983965e-05,
        "epoch": 0.9354838709677419,
        "step": 3393
    },
    {
        "loss": 1.2755,
        "grad_norm": 2.121920108795166,
        "learning_rate": 1.801537486369077e-05,
        "epoch": 0.9357595809208712,
        "step": 3394
    },
    {
        "loss": 2.3807,
        "grad_norm": 1.666514277458191,
        "learning_rate": 1.786583684301051e-05,
        "epoch": 0.9360352908740005,
        "step": 3395
    },
    {
        "loss": 2.2476,
        "grad_norm": 1.5394260883331299,
        "learning_rate": 1.7716861135722162e-05,
        "epoch": 0.9363110008271298,
        "step": 3396
    },
    {
        "loss": 1.6781,
        "grad_norm": 1.948298692703247,
        "learning_rate": 1.7568448761754962e-05,
        "epoch": 0.9365867107802591,
        "step": 3397
    },
    {
        "loss": 1.9733,
        "grad_norm": 1.868728518486023,
        "learning_rate": 1.7420600737181404e-05,
        "epoch": 0.9368624207333884,
        "step": 3398
    },
    {
        "loss": 1.8955,
        "grad_norm": 2.2178566455841064,
        "learning_rate": 1.7273318074210354e-05,
        "epoch": 0.9371381306865177,
        "step": 3399
    },
    {
        "loss": 2.2256,
        "grad_norm": 1.3796521425247192,
        "learning_rate": 1.7126601781179984e-05,
        "epoch": 0.9374138406396471,
        "step": 3400
    },
    {
        "loss": 2.2507,
        "grad_norm": 1.5006203651428223,
        "learning_rate": 1.6980452862550933e-05,
        "epoch": 0.9376895505927764,
        "step": 3401
    },
    {
        "loss": 1.3726,
        "grad_norm": 2.908644914627075,
        "learning_rate": 1.683487231889944e-05,
        "epoch": 0.9379652605459057,
        "step": 3402
    },
    {
        "loss": 1.7301,
        "grad_norm": 2.4423294067382812,
        "learning_rate": 1.6689861146910645e-05,
        "epoch": 0.938240970499035,
        "step": 3403
    },
    {
        "loss": 2.173,
        "grad_norm": 2.033705711364746,
        "learning_rate": 1.6545420339371388e-05,
        "epoch": 0.9385166804521643,
        "step": 3404
    },
    {
        "loss": 2.2141,
        "grad_norm": 1.9329502582550049,
        "learning_rate": 1.6401550885163764e-05,
        "epoch": 0.9387923904052936,
        "step": 3405
    },
    {
        "loss": 2.02,
        "grad_norm": 2.0495026111602783,
        "learning_rate": 1.625825376925817e-05,
        "epoch": 0.9390681003584229,
        "step": 3406
    },
    {
        "loss": 1.8223,
        "grad_norm": 2.052790641784668,
        "learning_rate": 1.611552997270661e-05,
        "epoch": 0.9393438103115522,
        "step": 3407
    },
    {
        "loss": 1.9343,
        "grad_norm": 2.0103487968444824,
        "learning_rate": 1.5973380472636047e-05,
        "epoch": 0.9396195202646815,
        "step": 3408
    },
    {
        "loss": 1.6427,
        "grad_norm": 2.1158454418182373,
        "learning_rate": 1.5831806242241597e-05,
        "epoch": 0.9398952302178109,
        "step": 3409
    },
    {
        "loss": 1.9997,
        "grad_norm": 2.042525291442871,
        "learning_rate": 1.569080825077994e-05,
        "epoch": 0.9401709401709402,
        "step": 3410
    },
    {
        "loss": 1.9881,
        "grad_norm": 1.6207761764526367,
        "learning_rate": 1.555038746356261e-05,
        "epoch": 0.9404466501240695,
        "step": 3411
    },
    {
        "loss": 2.4383,
        "grad_norm": 1.4220409393310547,
        "learning_rate": 1.5410544841949548e-05,
        "epoch": 0.9407223600771988,
        "step": 3412
    },
    {
        "loss": 2.1621,
        "grad_norm": 2.2360830307006836,
        "learning_rate": 1.5271281343342337e-05,
        "epoch": 0.9409980700303281,
        "step": 3413
    },
    {
        "loss": 2.0421,
        "grad_norm": 1.4431583881378174,
        "learning_rate": 1.5132597921177683e-05,
        "epoch": 0.9412737799834574,
        "step": 3414
    },
    {
        "loss": 2.013,
        "grad_norm": 1.4632482528686523,
        "learning_rate": 1.4994495524921049e-05,
        "epoch": 0.9415494899365867,
        "step": 3415
    },
    {
        "loss": 1.9741,
        "grad_norm": 2.1636250019073486,
        "learning_rate": 1.4856975100059923e-05,
        "epoch": 0.941825199889716,
        "step": 3416
    },
    {
        "loss": 2.1863,
        "grad_norm": 1.8514597415924072,
        "learning_rate": 1.472003758809749e-05,
        "epoch": 0.9421009098428453,
        "step": 3417
    },
    {
        "loss": 1.9686,
        "grad_norm": 2.0668551921844482,
        "learning_rate": 1.4583683926546187e-05,
        "epoch": 0.9423766197959746,
        "step": 3418
    },
    {
        "loss": 2.2684,
        "grad_norm": 1.5102866888046265,
        "learning_rate": 1.4447915048921201e-05,
        "epoch": 0.942652329749104,
        "step": 3419
    },
    {
        "loss": 2.207,
        "grad_norm": 1.7351629734039307,
        "learning_rate": 1.4312731884734142e-05,
        "epoch": 0.9429280397022333,
        "step": 3420
    },
    {
        "loss": 1.6964,
        "grad_norm": 2.593898057937622,
        "learning_rate": 1.4178135359486622e-05,
        "epoch": 0.9432037496553626,
        "step": 3421
    },
    {
        "loss": 1.394,
        "grad_norm": 2.0017292499542236,
        "learning_rate": 1.404412639466406e-05,
        "epoch": 0.9434794596084919,
        "step": 3422
    },
    {
        "loss": 1.9587,
        "grad_norm": 1.1067439317703247,
        "learning_rate": 1.3910705907729183e-05,
        "epoch": 0.9437551695616212,
        "step": 3423
    },
    {
        "loss": 1.8996,
        "grad_norm": 1.4381451606750488,
        "learning_rate": 1.3777874812115831e-05,
        "epoch": 0.9440308795147505,
        "step": 3424
    },
    {
        "loss": 1.6726,
        "grad_norm": 1.9856301546096802,
        "learning_rate": 1.3645634017222797e-05,
        "epoch": 0.9443065894678798,
        "step": 3425
    },
    {
        "loss": 1.8793,
        "grad_norm": 1.7523938417434692,
        "learning_rate": 1.351398442840741e-05,
        "epoch": 0.9445822994210091,
        "step": 3426
    },
    {
        "loss": 2.3105,
        "grad_norm": 1.463046908378601,
        "learning_rate": 1.3382926946979479e-05,
        "epoch": 0.9448580093741384,
        "step": 3427
    },
    {
        "loss": 2.3974,
        "grad_norm": 0.9931506514549255,
        "learning_rate": 1.3252462470195104e-05,
        "epoch": 0.9451337193272678,
        "step": 3428
    },
    {
        "loss": 1.9545,
        "grad_norm": 1.7303394079208374,
        "learning_rate": 1.3122591891250468e-05,
        "epoch": 0.9454094292803971,
        "step": 3429
    },
    {
        "loss": 2.3797,
        "grad_norm": 1.3862576484680176,
        "learning_rate": 1.2993316099275799e-05,
        "epoch": 0.9456851392335264,
        "step": 3430
    },
    {
        "loss": 1.4769,
        "grad_norm": 2.588710308074951,
        "learning_rate": 1.2864635979329176e-05,
        "epoch": 0.9459608491866557,
        "step": 3431
    },
    {
        "loss": 2.5271,
        "grad_norm": 1.2281056642532349,
        "learning_rate": 1.2736552412390668e-05,
        "epoch": 0.946236559139785,
        "step": 3432
    },
    {
        "loss": 1.4217,
        "grad_norm": 1.673012375831604,
        "learning_rate": 1.2609066275356052e-05,
        "epoch": 0.9465122690929143,
        "step": 3433
    },
    {
        "loss": 1.8736,
        "grad_norm": 1.175098180770874,
        "learning_rate": 1.248217844103099e-05,
        "epoch": 0.9467879790460436,
        "step": 3434
    },
    {
        "loss": 1.827,
        "grad_norm": 1.1484616994857788,
        "learning_rate": 1.2355889778125018e-05,
        "epoch": 0.9470636889991728,
        "step": 3435
    },
    {
        "loss": 1.6226,
        "grad_norm": 1.9198718070983887,
        "learning_rate": 1.2230201151245535e-05,
        "epoch": 0.9473393989523021,
        "step": 3436
    },
    {
        "loss": 2.381,
        "grad_norm": 1.9509245157241821,
        "learning_rate": 1.2105113420891945e-05,
        "epoch": 0.9476151089054314,
        "step": 3437
    },
    {
        "loss": 2.2302,
        "grad_norm": 1.6405127048492432,
        "learning_rate": 1.1980627443449699e-05,
        "epoch": 0.9478908188585607,
        "step": 3438
    },
    {
        "loss": 2.4075,
        "grad_norm": 1.270120620727539,
        "learning_rate": 1.185674407118459e-05,
        "epoch": 0.9481665288116901,
        "step": 3439
    },
    {
        "loss": 2.2784,
        "grad_norm": 1.2333877086639404,
        "learning_rate": 1.1733464152236717e-05,
        "epoch": 0.9484422387648194,
        "step": 3440
    },
    {
        "loss": 2.1625,
        "grad_norm": 1.723657250404358,
        "learning_rate": 1.1610788530614758e-05,
        "epoch": 0.9487179487179487,
        "step": 3441
    },
    {
        "loss": 2.0298,
        "grad_norm": 0.9590442180633545,
        "learning_rate": 1.1488718046190294e-05,
        "epoch": 0.948993658671078,
        "step": 3442
    },
    {
        "loss": 2.2576,
        "grad_norm": 1.5840699672698975,
        "learning_rate": 1.136725353469188e-05,
        "epoch": 0.9492693686242073,
        "step": 3443
    },
    {
        "loss": 2.4881,
        "grad_norm": 1.9782828092575073,
        "learning_rate": 1.124639582769943e-05,
        "epoch": 0.9495450785773366,
        "step": 3444
    },
    {
        "loss": 2.5089,
        "grad_norm": 1.5092207193374634,
        "learning_rate": 1.1126145752638572e-05,
        "epoch": 0.9498207885304659,
        "step": 3445
    },
    {
        "loss": 1.9244,
        "grad_norm": 1.9128202199935913,
        "learning_rate": 1.100650413277482e-05,
        "epoch": 0.9500964984835952,
        "step": 3446
    },
    {
        "loss": 1.4171,
        "grad_norm": 2.5812699794769287,
        "learning_rate": 1.0887471787208082e-05,
        "epoch": 0.9503722084367245,
        "step": 3447
    },
    {
        "loss": 1.6094,
        "grad_norm": 2.3049087524414062,
        "learning_rate": 1.0769049530866992e-05,
        "epoch": 0.9506479183898539,
        "step": 3448
    },
    {
        "loss": 2.4982,
        "grad_norm": 1.3322241306304932,
        "learning_rate": 1.0651238174503386e-05,
        "epoch": 0.9509236283429832,
        "step": 3449
    },
    {
        "loss": 1.7028,
        "grad_norm": 1.6463264226913452,
        "learning_rate": 1.053403852468664e-05,
        "epoch": 0.9511993382961125,
        "step": 3450
    },
    {
        "loss": 2.1,
        "grad_norm": 1.39410400390625,
        "learning_rate": 1.0417451383798249e-05,
        "epoch": 0.9514750482492418,
        "step": 3451
    },
    {
        "loss": 2.401,
        "grad_norm": 1.6627684831619263,
        "learning_rate": 1.0301477550026318e-05,
        "epoch": 0.9517507582023711,
        "step": 3452
    },
    {
        "loss": 1.8417,
        "grad_norm": 1.4389779567718506,
        "learning_rate": 1.0186117817360064e-05,
        "epoch": 0.9520264681555004,
        "step": 3453
    },
    {
        "loss": 1.8435,
        "grad_norm": 1.5124577283859253,
        "learning_rate": 1.007137297558436e-05,
        "epoch": 0.9523021781086297,
        "step": 3454
    },
    {
        "loss": 1.8483,
        "grad_norm": 1.4537482261657715,
        "learning_rate": 9.957243810274453e-06,
        "epoch": 0.952577888061759,
        "step": 3455
    },
    {
        "loss": 2.3309,
        "grad_norm": 1.1399344205856323,
        "learning_rate": 9.843731102790421e-06,
        "epoch": 0.9528535980148883,
        "step": 3456
    },
    {
        "loss": 2.073,
        "grad_norm": 1.8834103345870972,
        "learning_rate": 9.730835630271918e-06,
        "epoch": 0.9531293079680176,
        "step": 3457
    },
    {
        "loss": 1.8132,
        "grad_norm": 1.4313206672668457,
        "learning_rate": 9.618558165632819e-06,
        "epoch": 0.953405017921147,
        "step": 3458
    },
    {
        "loss": 2.389,
        "grad_norm": 1.6404914855957031,
        "learning_rate": 9.50689947755602e-06,
        "epoch": 0.9536807278742763,
        "step": 3459
    },
    {
        "loss": 1.6643,
        "grad_norm": 2.0655269622802734,
        "learning_rate": 9.395860330488015e-06,
        "epoch": 0.9539564378274056,
        "step": 3460
    },
    {
        "loss": 1.9231,
        "grad_norm": 1.7839528322219849,
        "learning_rate": 9.285441484633761e-06,
        "epoch": 0.9542321477805349,
        "step": 3461
    },
    {
        "loss": 2.0593,
        "grad_norm": 1.3880950212478638,
        "learning_rate": 9.175643695951507e-06,
        "epoch": 0.9545078577336642,
        "step": 3462
    },
    {
        "loss": 2.1927,
        "grad_norm": 1.5726680755615234,
        "learning_rate": 9.066467716147543e-06,
        "epoch": 0.9547835676867935,
        "step": 3463
    },
    {
        "loss": 1.7988,
        "grad_norm": 1.5759391784667969,
        "learning_rate": 8.957914292671055e-06,
        "epoch": 0.9550592776399228,
        "step": 3464
    },
    {
        "loss": 1.7369,
        "grad_norm": 1.7761189937591553,
        "learning_rate": 8.849984168709091e-06,
        "epoch": 0.9553349875930521,
        "step": 3465
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.439949631690979,
        "learning_rate": 8.742678083181366e-06,
        "epoch": 0.9556106975461814,
        "step": 3466
    },
    {
        "loss": 1.6972,
        "grad_norm": 1.639050006866455,
        "learning_rate": 8.635996770735278e-06,
        "epoch": 0.9558864074993108,
        "step": 3467
    },
    {
        "loss": 1.6698,
        "grad_norm": 1.7066960334777832,
        "learning_rate": 8.529940961740845e-06,
        "epoch": 0.9561621174524401,
        "step": 3468
    },
    {
        "loss": 1.8963,
        "grad_norm": 1.451532244682312,
        "learning_rate": 8.424511382285727e-06,
        "epoch": 0.9564378274055694,
        "step": 3469
    },
    {
        "loss": 1.8162,
        "grad_norm": 2.6472058296203613,
        "learning_rate": 8.319708754170241e-06,
        "epoch": 0.9567135373586987,
        "step": 3470
    },
    {
        "loss": 1.693,
        "grad_norm": 1.6381475925445557,
        "learning_rate": 8.215533794902397e-06,
        "epoch": 0.956989247311828,
        "step": 3471
    },
    {
        "loss": 2.1597,
        "grad_norm": 1.5950936079025269,
        "learning_rate": 8.11198721769304e-06,
        "epoch": 0.9572649572649573,
        "step": 3472
    },
    {
        "loss": 1.9809,
        "grad_norm": 2.2779183387756348,
        "learning_rate": 8.009069731450913e-06,
        "epoch": 0.9575406672180866,
        "step": 3473
    },
    {
        "loss": 2.1645,
        "grad_norm": 2.1798529624938965,
        "learning_rate": 7.906782040777815e-06,
        "epoch": 0.9578163771712159,
        "step": 3474
    },
    {
        "loss": 2.0342,
        "grad_norm": 1.681606650352478,
        "learning_rate": 7.805124845963863e-06,
        "epoch": 0.9580920871243452,
        "step": 3475
    },
    {
        "loss": 2.1616,
        "grad_norm": 1.840134859085083,
        "learning_rate": 7.704098842982521e-06,
        "epoch": 0.9583677970774745,
        "step": 3476
    },
    {
        "loss": 1.5116,
        "grad_norm": 1.7608840465545654,
        "learning_rate": 7.603704723486016e-06,
        "epoch": 0.9586435070306039,
        "step": 3477
    },
    {
        "loss": 2.5933,
        "grad_norm": 1.6731019020080566,
        "learning_rate": 7.503943174800454e-06,
        "epoch": 0.9589192169837331,
        "step": 3478
    },
    {
        "loss": 1.4442,
        "grad_norm": 2.344627857208252,
        "learning_rate": 7.404814879921274e-06,
        "epoch": 0.9591949269368624,
        "step": 3479
    },
    {
        "loss": 2.1218,
        "grad_norm": 1.288604497909546,
        "learning_rate": 7.306320517508413e-06,
        "epoch": 0.9594706368899917,
        "step": 3480
    },
    {
        "loss": 1.9808,
        "grad_norm": 1.7226121425628662,
        "learning_rate": 7.20846076188173e-06,
        "epoch": 0.959746346843121,
        "step": 3481
    },
    {
        "loss": 1.9927,
        "grad_norm": 1.107861876487732,
        "learning_rate": 7.111236283016443e-06,
        "epoch": 0.9600220567962503,
        "step": 3482
    },
    {
        "loss": 2.4888,
        "grad_norm": 1.1321794986724854,
        "learning_rate": 7.0146477465384494e-06,
        "epoch": 0.9602977667493796,
        "step": 3483
    },
    {
        "loss": 1.8455,
        "grad_norm": 1.9283722639083862,
        "learning_rate": 6.9186958137198e-06,
        "epoch": 0.9605734767025089,
        "step": 3484
    },
    {
        "loss": 1.784,
        "grad_norm": 1.7499048709869385,
        "learning_rate": 6.823381141474183e-06,
        "epoch": 0.9608491866556382,
        "step": 3485
    },
    {
        "loss": 2.1216,
        "grad_norm": 2.335020065307617,
        "learning_rate": 6.728704382352458e-06,
        "epoch": 0.9611248966087675,
        "step": 3486
    },
    {
        "loss": 1.8861,
        "grad_norm": 1.5669723749160767,
        "learning_rate": 6.634666184538119e-06,
        "epoch": 0.9614006065618969,
        "step": 3487
    },
    {
        "loss": 1.5287,
        "grad_norm": 2.0902926921844482,
        "learning_rate": 6.54126719184287e-06,
        "epoch": 0.9616763165150262,
        "step": 3488
    },
    {
        "loss": 0.6379,
        "grad_norm": 1.8442126512527466,
        "learning_rate": 6.448508043702317e-06,
        "epoch": 0.9619520264681555,
        "step": 3489
    },
    {
        "loss": 1.8316,
        "grad_norm": 3.3137004375457764,
        "learning_rate": 6.356389375171445e-06,
        "epoch": 0.9622277364212848,
        "step": 3490
    },
    {
        "loss": 1.4761,
        "grad_norm": 2.5454444885253906,
        "learning_rate": 6.264911816920327e-06,
        "epoch": 0.9625034463744141,
        "step": 3491
    },
    {
        "loss": 2.1517,
        "grad_norm": 1.6427042484283447,
        "learning_rate": 6.174075995229889e-06,
        "epoch": 0.9627791563275434,
        "step": 3492
    },
    {
        "loss": 2.0624,
        "grad_norm": 1.5019947290420532,
        "learning_rate": 6.0838825319874834e-06,
        "epoch": 0.9630548662806727,
        "step": 3493
    },
    {
        "loss": 2.0686,
        "grad_norm": 1.5310195684432983,
        "learning_rate": 5.9943320446827155e-06,
        "epoch": 0.963330576233802,
        "step": 3494
    },
    {
        "loss": 1.1706,
        "grad_norm": 2.305739641189575,
        "learning_rate": 5.9054251464031875e-06,
        "epoch": 0.9636062861869313,
        "step": 3495
    },
    {
        "loss": 2.2131,
        "grad_norm": 1.2559529542922974,
        "learning_rate": 5.817162445830349e-06,
        "epoch": 0.9638819961400606,
        "step": 3496
    },
    {
        "loss": 1.5194,
        "grad_norm": 1.554080843925476,
        "learning_rate": 5.729544547235266e-06,
        "epoch": 0.96415770609319,
        "step": 3497
    },
    {
        "loss": 1.4094,
        "grad_norm": 2.7226736545562744,
        "learning_rate": 5.642572050474493e-06,
        "epoch": 0.9644334160463193,
        "step": 3498
    },
    {
        "loss": 2.0409,
        "grad_norm": 1.4217262268066406,
        "learning_rate": 5.556245550986039e-06,
        "epoch": 0.9647091259994486,
        "step": 3499
    },
    {
        "loss": 2.1254,
        "grad_norm": 1.353749394416809,
        "learning_rate": 5.470565639785163e-06,
        "epoch": 0.9649848359525779,
        "step": 3500
    },
    {
        "loss": 2.2126,
        "grad_norm": 2.0902981758117676,
        "learning_rate": 5.3855329034604555e-06,
        "epoch": 0.9652605459057072,
        "step": 3501
    },
    {
        "loss": 2.1317,
        "grad_norm": 1.4546922445297241,
        "learning_rate": 5.301147924169747e-06,
        "epoch": 0.9655362558588365,
        "step": 3502
    },
    {
        "loss": 1.9146,
        "grad_norm": 2.144752025604248,
        "learning_rate": 5.217411279636153e-06,
        "epoch": 0.9658119658119658,
        "step": 3503
    },
    {
        "loss": 2.1754,
        "grad_norm": 1.7927942276000977,
        "learning_rate": 5.134323543144093e-06,
        "epoch": 0.9660876757650951,
        "step": 3504
    },
    {
        "loss": 1.2941,
        "grad_norm": 1.9469317197799683,
        "learning_rate": 5.051885283535363e-06,
        "epoch": 0.9663633857182244,
        "step": 3505
    },
    {
        "loss": 2.2255,
        "grad_norm": 2.1501498222351074,
        "learning_rate": 4.970097065205326e-06,
        "epoch": 0.9666390956713538,
        "step": 3506
    },
    {
        "loss": 1.7741,
        "grad_norm": 1.422593116760254,
        "learning_rate": 4.888959448098907e-06,
        "epoch": 0.9669148056244831,
        "step": 3507
    },
    {
        "loss": 1.762,
        "grad_norm": 2.073915481567383,
        "learning_rate": 4.8084729877068805e-06,
        "epoch": 0.9671905155776124,
        "step": 3508
    },
    {
        "loss": 2.4273,
        "grad_norm": 1.3568898439407349,
        "learning_rate": 4.72863823506201e-06,
        "epoch": 0.9674662255307417,
        "step": 3509
    },
    {
        "loss": 1.6924,
        "grad_norm": 1.5753318071365356,
        "learning_rate": 4.649455736735275e-06,
        "epoch": 0.967741935483871,
        "step": 3510
    },
    {
        "loss": 1.8745,
        "grad_norm": 1.8459874391555786,
        "learning_rate": 4.570926034832134e-06,
        "epoch": 0.9680176454370003,
        "step": 3511
    },
    {
        "loss": 2.078,
        "grad_norm": 1.327651023864746,
        "learning_rate": 4.493049666988858e-06,
        "epoch": 0.9682933553901296,
        "step": 3512
    },
    {
        "loss": 1.7209,
        "grad_norm": 2.088881015777588,
        "learning_rate": 4.415827166368769e-06,
        "epoch": 0.9685690653432589,
        "step": 3513
    },
    {
        "loss": 2.5269,
        "grad_norm": 1.4593957662582397,
        "learning_rate": 4.339259061658652e-06,
        "epoch": 0.9688447752963882,
        "step": 3514
    },
    {
        "loss": 1.8316,
        "grad_norm": 2.1720118522644043,
        "learning_rate": 4.263345877065095e-06,
        "epoch": 0.9691204852495175,
        "step": 3515
    },
    {
        "loss": 1.9305,
        "grad_norm": 1.3762985467910767,
        "learning_rate": 4.188088132310963e-06,
        "epoch": 0.9693961952026469,
        "step": 3516
    },
    {
        "loss": 2.6104,
        "grad_norm": 1.7946205139160156,
        "learning_rate": 4.113486342631745e-06,
        "epoch": 0.9696719051557762,
        "step": 3517
    },
    {
        "loss": 1.5888,
        "grad_norm": 2.187966823577881,
        "learning_rate": 4.039541018772108e-06,
        "epoch": 0.9699476151089055,
        "step": 3518
    },
    {
        "loss": 1.8318,
        "grad_norm": 2.6129343509674072,
        "learning_rate": 3.966252666982384e-06,
        "epoch": 0.9702233250620348,
        "step": 3519
    },
    {
        "loss": 2.4098,
        "grad_norm": 1.202805519104004,
        "learning_rate": 3.8936217890150455e-06,
        "epoch": 0.9704990350151641,
        "step": 3520
    },
    {
        "loss": 1.7852,
        "grad_norm": 1.4467493295669556,
        "learning_rate": 3.821648882121365e-06,
        "epoch": 0.9707747449682933,
        "step": 3521
    },
    {
        "loss": 1.3677,
        "grad_norm": 1.677187204360962,
        "learning_rate": 3.75033443904792e-06,
        "epoch": 0.9710504549214226,
        "step": 3522
    },
    {
        "loss": 2.2178,
        "grad_norm": 1.5189437866210938,
        "learning_rate": 3.679678948033305e-06,
        "epoch": 0.9713261648745519,
        "step": 3523
    },
    {
        "loss": 2.1209,
        "grad_norm": 1.4119014739990234,
        "learning_rate": 3.6096828928047e-06,
        "epoch": 0.9716018748276812,
        "step": 3524
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.1057162284851074,
        "learning_rate": 3.540346752574619e-06,
        "epoch": 0.9718775847808105,
        "step": 3525
    },
    {
        "loss": 2.3921,
        "grad_norm": 1.632013201713562,
        "learning_rate": 3.4716710020376466e-06,
        "epoch": 0.9721532947339399,
        "step": 3526
    },
    {
        "loss": 1.4627,
        "grad_norm": 2.338621139526367,
        "learning_rate": 3.403656111367115e-06,
        "epoch": 0.9724290046870692,
        "step": 3527
    },
    {
        "loss": 2.0206,
        "grad_norm": 1.4045125246047974,
        "learning_rate": 3.3363025462119114e-06,
        "epoch": 0.9727047146401985,
        "step": 3528
    },
    {
        "loss": 1.8522,
        "grad_norm": 1.5981168746948242,
        "learning_rate": 3.2696107676933763e-06,
        "epoch": 0.9729804245933278,
        "step": 3529
    },
    {
        "loss": 2.4387,
        "grad_norm": 1.9758448600769043,
        "learning_rate": 3.203581232402009e-06,
        "epoch": 0.9732561345464571,
        "step": 3530
    },
    {
        "loss": 1.765,
        "grad_norm": 1.8591194152832031,
        "learning_rate": 3.138214392394423e-06,
        "epoch": 0.9735318444995864,
        "step": 3531
    },
    {
        "loss": 2.3392,
        "grad_norm": 1.5033560991287231,
        "learning_rate": 3.0735106951902294e-06,
        "epoch": 0.9738075544527157,
        "step": 3532
    },
    {
        "loss": 2.4663,
        "grad_norm": 1.8500667810440063,
        "learning_rate": 3.009470583769003e-06,
        "epoch": 0.974083264405845,
        "step": 3533
    },
    {
        "loss": 1.3417,
        "grad_norm": 1.4620558023452759,
        "learning_rate": 2.946094496567209e-06,
        "epoch": 0.9743589743589743,
        "step": 3534
    },
    {
        "loss": 2.4255,
        "grad_norm": 1.08488130569458,
        "learning_rate": 2.883382867475215e-06,
        "epoch": 0.9746346843121036,
        "step": 3535
    },
    {
        "loss": 2.2768,
        "grad_norm": 1.8543198108673096,
        "learning_rate": 2.8213361258343284e-06,
        "epoch": 0.974910394265233,
        "step": 3536
    },
    {
        "loss": 2.2969,
        "grad_norm": 1.330491065979004,
        "learning_rate": 2.759954696433864e-06,
        "epoch": 0.9751861042183623,
        "step": 3537
    },
    {
        "loss": 2.4478,
        "grad_norm": 1.5396174192428589,
        "learning_rate": 2.699238999508169e-06,
        "epoch": 0.9754618141714916,
        "step": 3538
    },
    {
        "loss": 1.603,
        "grad_norm": 1.515512466430664,
        "learning_rate": 2.639189450733892e-06,
        "epoch": 0.9757375241246209,
        "step": 3539
    },
    {
        "loss": 2.1267,
        "grad_norm": 1.2663568258285522,
        "learning_rate": 2.5798064612269636e-06,
        "epoch": 0.9760132340777502,
        "step": 3540
    },
    {
        "loss": 1.4566,
        "grad_norm": 1.6510398387908936,
        "learning_rate": 2.521090437539897e-06,
        "epoch": 0.9762889440308795,
        "step": 3541
    },
    {
        "loss": 2.3892,
        "grad_norm": 1.451004981994629,
        "learning_rate": 2.4630417816589592e-06,
        "epoch": 0.9765646539840088,
        "step": 3542
    },
    {
        "loss": 1.8463,
        "grad_norm": 1.2598053216934204,
        "learning_rate": 2.405660891001471e-06,
        "epoch": 0.9768403639371381,
        "step": 3543
    },
    {
        "loss": 1.2689,
        "grad_norm": 1.9250861406326294,
        "learning_rate": 2.348948158413e-06,
        "epoch": 0.9771160738902674,
        "step": 3544
    },
    {
        "loss": 2.1368,
        "grad_norm": 1.185551404953003,
        "learning_rate": 2.292903972164728e-06,
        "epoch": 0.9773917838433968,
        "step": 3545
    },
    {
        "loss": 2.4823,
        "grad_norm": 1.6331892013549805,
        "learning_rate": 2.2375287159507986e-06,
        "epoch": 0.9776674937965261,
        "step": 3546
    },
    {
        "loss": 2.1149,
        "grad_norm": 1.9827420711517334,
        "learning_rate": 2.1828227688856505e-06,
        "epoch": 0.9779432037496554,
        "step": 3547
    },
    {
        "loss": 2.3701,
        "grad_norm": 1.5753802061080933,
        "learning_rate": 2.1287865055014565e-06,
        "epoch": 0.9782189137027847,
        "step": 3548
    },
    {
        "loss": 2.126,
        "grad_norm": 1.7915642261505127,
        "learning_rate": 2.075420295745567e-06,
        "epoch": 0.978494623655914,
        "step": 3549
    },
    {
        "loss": 2.0375,
        "grad_norm": 1.6069140434265137,
        "learning_rate": 2.022724504977924e-06,
        "epoch": 0.9787703336090433,
        "step": 3550
    },
    {
        "loss": 2.3254,
        "grad_norm": 1.2473927736282349,
        "learning_rate": 1.9706994939686086e-06,
        "epoch": 0.9790460435621726,
        "step": 3551
    },
    {
        "loss": 2.3347,
        "grad_norm": 1.296059250831604,
        "learning_rate": 1.919345618895363e-06,
        "epoch": 0.9793217535153019,
        "step": 3552
    },
    {
        "loss": 2.0267,
        "grad_norm": 1.6287192106246948,
        "learning_rate": 1.86866323134115e-06,
        "epoch": 0.9795974634684312,
        "step": 3553
    },
    {
        "loss": 2.3651,
        "grad_norm": 1.5229038000106812,
        "learning_rate": 1.8186526782917212e-06,
        "epoch": 0.9798731734215606,
        "step": 3554
    },
    {
        "loss": 1.4197,
        "grad_norm": 1.7774993181228638,
        "learning_rate": 1.7693143021332736e-06,
        "epoch": 0.9801488833746899,
        "step": 3555
    },
    {
        "loss": 1.7988,
        "grad_norm": 2.3198318481445312,
        "learning_rate": 1.7206484406501078e-06,
        "epoch": 0.9804245933278192,
        "step": 3556
    },
    {
        "loss": 2.0205,
        "grad_norm": 1.7663531303405762,
        "learning_rate": 1.6726554270222627e-06,
        "epoch": 0.9807003032809485,
        "step": 3557
    },
    {
        "loss": 1.6214,
        "grad_norm": 2.1934680938720703,
        "learning_rate": 1.625335589823318e-06,
        "epoch": 0.9809760132340778,
        "step": 3558
    },
    {
        "loss": 2.042,
        "grad_norm": 2.1816506385803223,
        "learning_rate": 1.578689253018062e-06,
        "epoch": 0.9812517231872071,
        "step": 3559
    },
    {
        "loss": 1.5549,
        "grad_norm": 1.7666820287704468,
        "learning_rate": 1.5327167359603378e-06,
        "epoch": 0.9815274331403364,
        "step": 3560
    },
    {
        "loss": 1.9319,
        "grad_norm": 1.8412537574768066,
        "learning_rate": 1.4874183533908237e-06,
        "epoch": 0.9818031430934657,
        "step": 3561
    },
    {
        "loss": 2.378,
        "grad_norm": 0.990292489528656,
        "learning_rate": 1.442794415434867e-06,
        "epoch": 0.982078853046595,
        "step": 3562
    },
    {
        "loss": 1.7894,
        "grad_norm": 2.3857877254486084,
        "learning_rate": 1.3988452276004095e-06,
        "epoch": 0.9823545629997242,
        "step": 3563
    },
    {
        "loss": 2.0105,
        "grad_norm": 1.5268537998199463,
        "learning_rate": 1.3555710907758535e-06,
        "epoch": 0.9826302729528535,
        "step": 3564
    },
    {
        "loss": 2.1621,
        "grad_norm": 1.455231785774231,
        "learning_rate": 1.3129723012279992e-06,
        "epoch": 0.9829059829059829,
        "step": 3565
    },
    {
        "loss": 1.9779,
        "grad_norm": 1.5683493614196777,
        "learning_rate": 1.2710491506000455e-06,
        "epoch": 0.9831816928591122,
        "step": 3566
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.1387109756469727,
        "learning_rate": 1.2298019259095795e-06,
        "epoch": 0.9834574028122415,
        "step": 3567
    },
    {
        "loss": 1.4197,
        "grad_norm": 2.253077268600464,
        "learning_rate": 1.1892309095466126e-06,
        "epoch": 0.9837331127653708,
        "step": 3568
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.3311495780944824,
        "learning_rate": 1.149336379271626e-06,
        "epoch": 0.9840088227185001,
        "step": 3569
    },
    {
        "loss": 1.2095,
        "grad_norm": 2.5407567024230957,
        "learning_rate": 1.1101186082137061e-06,
        "epoch": 0.9842845326716294,
        "step": 3570
    },
    {
        "loss": 1.8326,
        "grad_norm": 1.8865151405334473,
        "learning_rate": 1.0715778648686558e-06,
        "epoch": 0.9845602426247587,
        "step": 3571
    },
    {
        "loss": 2.1243,
        "grad_norm": 2.067138433456421,
        "learning_rate": 1.0337144130971642e-06,
        "epoch": 0.984835952577888,
        "step": 3572
    },
    {
        "loss": 2.1265,
        "grad_norm": 1.757034182548523,
        "learning_rate": 9.96528512122963e-07,
        "epoch": 0.9851116625310173,
        "step": 3573
    },
    {
        "loss": 2.5211,
        "grad_norm": 1.28307044506073,
        "learning_rate": 9.600204165311266e-07,
        "epoch": 0.9853873724841467,
        "step": 3574
    },
    {
        "loss": 1.9769,
        "grad_norm": 1.7307487726211548,
        "learning_rate": 9.241903762662429e-07,
        "epoch": 0.985663082437276,
        "step": 3575
    },
    {
        "loss": 1.181,
        "grad_norm": 1.921316385269165,
        "learning_rate": 8.890386366307679e-07,
        "epoch": 0.9859387923904053,
        "step": 3576
    },
    {
        "loss": 2.3721,
        "grad_norm": 1.4978598356246948,
        "learning_rate": 8.545654382833168e-07,
        "epoch": 0.9862145023435346,
        "step": 3577
    },
    {
        "loss": 1.9833,
        "grad_norm": 1.5788935422897339,
        "learning_rate": 8.207710172370319e-07,
        "epoch": 0.9864902122966639,
        "step": 3578
    },
    {
        "loss": 1.8469,
        "grad_norm": 1.1902368068695068,
        "learning_rate": 7.876556048579286e-07,
        "epoch": 0.9867659222497932,
        "step": 3579
    },
    {
        "loss": 1.6269,
        "grad_norm": 1.7706555128097534,
        "learning_rate": 7.552194278633628e-07,
        "epoch": 0.9870416322029225,
        "step": 3580
    },
    {
        "loss": 2.4301,
        "grad_norm": 1.737176537513733,
        "learning_rate": 7.234627083204548e-07,
        "epoch": 0.9873173421560518,
        "step": 3581
    },
    {
        "loss": 1.8336,
        "grad_norm": 1.2650116682052612,
        "learning_rate": 6.92385663644568e-07,
        "epoch": 0.9875930521091811,
        "step": 3582
    },
    {
        "loss": 2.1401,
        "grad_norm": 1.3650723695755005,
        "learning_rate": 6.619885065978104e-07,
        "epoch": 0.9878687620623104,
        "step": 3583
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.4581291675567627,
        "learning_rate": 6.322714452876021e-07,
        "epoch": 0.9881444720154398,
        "step": 3584
    },
    {
        "loss": 2.3481,
        "grad_norm": 1.159306526184082,
        "learning_rate": 6.032346831652324e-07,
        "epoch": 0.9884201819685691,
        "step": 3585
    },
    {
        "loss": 2.3844,
        "grad_norm": 1.1876178979873657,
        "learning_rate": 5.748784190244827e-07,
        "epoch": 0.9886958919216984,
        "step": 3586
    },
    {
        "loss": 2.0336,
        "grad_norm": 1.764266014099121,
        "learning_rate": 5.472028470002388e-07,
        "epoch": 0.9889716018748277,
        "step": 3587
    },
    {
        "loss": 2.0492,
        "grad_norm": 1.539631724357605,
        "learning_rate": 5.202081565671924e-07,
        "epoch": 0.989247311827957,
        "step": 3588
    },
    {
        "loss": 2.5042,
        "grad_norm": 1.0526527166366577,
        "learning_rate": 4.93894532538508e-07,
        "epoch": 0.9895230217810863,
        "step": 3589
    },
    {
        "loss": 2.3764,
        "grad_norm": 1.3637726306915283,
        "learning_rate": 4.682621550646138e-07,
        "epoch": 0.9897987317342156,
        "step": 3590
    },
    {
        "loss": 1.688,
        "grad_norm": 1.6208831071853638,
        "learning_rate": 4.433111996319128e-07,
        "epoch": 0.9900744416873449,
        "step": 3591
    },
    {
        "loss": 1.845,
        "grad_norm": 1.5981634855270386,
        "learning_rate": 4.1904183706160674e-07,
        "epoch": 0.9903501516404742,
        "step": 3592
    },
    {
        "loss": 1.9757,
        "grad_norm": 1.077449083328247,
        "learning_rate": 3.9545423350851873e-07,
        "epoch": 0.9906258615936036,
        "step": 3593
    },
    {
        "loss": 2.3479,
        "grad_norm": 1.4801048040390015,
        "learning_rate": 3.725485504599502e-07,
        "epoch": 0.9909015715467329,
        "step": 3594
    },
    {
        "loss": 2.1062,
        "grad_norm": 1.1062328815460205,
        "learning_rate": 3.5032494473462576e-07,
        "epoch": 0.9911772814998622,
        "step": 3595
    },
    {
        "loss": 1.5303,
        "grad_norm": 2.1556031703948975,
        "learning_rate": 3.287835684815166e-07,
        "epoch": 0.9914529914529915,
        "step": 3596
    },
    {
        "loss": 2.277,
        "grad_norm": 1.8900295495986938,
        "learning_rate": 3.07924569178919e-07,
        "epoch": 0.9917287014061208,
        "step": 3597
    },
    {
        "loss": 2.441,
        "grad_norm": 1.2652413845062256,
        "learning_rate": 2.87748089633344e-07,
        "epoch": 0.9920044113592501,
        "step": 3598
    },
    {
        "loss": 2.2489,
        "grad_norm": 1.4623316526412964,
        "learning_rate": 2.682542679786071e-07,
        "epoch": 0.9922801213123794,
        "step": 3599
    },
    {
        "loss": 1.7866,
        "grad_norm": 1.5932109355926514,
        "learning_rate": 2.4944323767485124e-07,
        "epoch": 0.9925558312655087,
        "step": 3600
    },
    {
        "loss": 2.4459,
        "grad_norm": 1.0762531757354736,
        "learning_rate": 2.313151275076364e-07,
        "epoch": 0.992831541218638,
        "step": 3601
    },
    {
        "loss": 1.7005,
        "grad_norm": 1.9871505498886108,
        "learning_rate": 2.1387006158704037e-07,
        "epoch": 0.9931072511717673,
        "step": 3602
    },
    {
        "loss": 2.2187,
        "grad_norm": 1.5638766288757324,
        "learning_rate": 1.9710815934688153e-07,
        "epoch": 0.9933829611248967,
        "step": 3603
    },
    {
        "loss": 2.1862,
        "grad_norm": 1.3240898847579956,
        "learning_rate": 1.8102953554380853e-07,
        "epoch": 0.993658671078026,
        "step": 3604
    },
    {
        "loss": 2.4107,
        "grad_norm": 1.4958699941635132,
        "learning_rate": 1.6563430025655634e-07,
        "epoch": 0.9939343810311553,
        "step": 3605
    },
    {
        "loss": 1.8872,
        "grad_norm": 1.2269772291183472,
        "learning_rate": 1.5092255888521366e-07,
        "epoch": 0.9942100909842845,
        "step": 3606
    },
    {
        "loss": 1.7441,
        "grad_norm": 1.8062586784362793,
        "learning_rate": 1.3689441215044562e-07,
        "epoch": 0.9944858009374138,
        "step": 3607
    },
    {
        "loss": 1.7623,
        "grad_norm": 1.4242156744003296,
        "learning_rate": 1.2354995609288321e-07,
        "epoch": 0.9947615108905431,
        "step": 3608
    },
    {
        "loss": 2.5422,
        "grad_norm": 1.4491674900054932,
        "learning_rate": 1.1088928207236838e-07,
        "epoch": 0.9950372208436724,
        "step": 3609
    },
    {
        "loss": 1.9806,
        "grad_norm": 2.41422438621521,
        "learning_rate": 9.89124767674321e-08,
        "epoch": 0.9953129307968017,
        "step": 3610
    },
    {
        "loss": 1.6823,
        "grad_norm": 2.995004415512085,
        "learning_rate": 8.761962217460617e-08,
        "epoch": 0.995588640749931,
        "step": 3611
    },
    {
        "loss": 1.5514,
        "grad_norm": 1.5031647682189941,
        "learning_rate": 7.701079560793467e-08,
        "epoch": 0.9958643507030603,
        "step": 3612
    },
    {
        "loss": 1.8942,
        "grad_norm": 1.5792908668518066,
        "learning_rate": 6.708606969838549e-08,
        "epoch": 0.9961400606561897,
        "step": 3613
    },
    {
        "loss": 2.238,
        "grad_norm": 1.126118540763855,
        "learning_rate": 5.7845512393428504e-08,
        "epoch": 0.996415770609319,
        "step": 3614
    },
    {
        "loss": 2.712,
        "grad_norm": 1.6725916862487793,
        "learning_rate": 4.9289186956480435e-08,
        "epoch": 0.9966914805624483,
        "step": 3615
    },
    {
        "loss": 1.9438,
        "grad_norm": 1.8526769876480103,
        "learning_rate": 4.1417151966527404e-08,
        "epoch": 0.9969671905155776,
        "step": 3616
    },
    {
        "loss": 1.8771,
        "grad_norm": 1.1726703643798828,
        "learning_rate": 3.4229461317725195e-08,
        "epoch": 0.9972429004687069,
        "step": 3617
    },
    {
        "loss": 1.6768,
        "grad_norm": 1.5155665874481201,
        "learning_rate": 2.772616421899965e-08,
        "epoch": 0.9975186104218362,
        "step": 3618
    },
    {
        "loss": 2.2233,
        "grad_norm": 1.5963225364685059,
        "learning_rate": 2.1907305193757943e-08,
        "epoch": 0.9977943203749655,
        "step": 3619
    },
    {
        "loss": 2.3437,
        "grad_norm": 1.5452806949615479,
        "learning_rate": 1.6772924079511145e-08,
        "epoch": 0.9980700303280948,
        "step": 3620
    },
    {
        "loss": 1.7343,
        "grad_norm": 1.5637027025222778,
        "learning_rate": 1.232305602766326e-08,
        "epoch": 0.9983457402812241,
        "step": 3621
    },
    {
        "loss": 2.2336,
        "grad_norm": 1.289853811264038,
        "learning_rate": 8.557731503278099e-09,
        "epoch": 0.9986214502343534,
        "step": 3622
    },
    {
        "loss": 1.5294,
        "grad_norm": 2.608555555343628,
        "learning_rate": 5.4769762847795e-09,
        "epoch": 0.9988971601874828,
        "step": 3623
    },
    {
        "loss": 2.0076,
        "grad_norm": 1.7506154775619507,
        "learning_rate": 3.0808114639180317e-09,
        "epoch": 0.9991728701406121,
        "step": 3624
    },
    {
        "loss": 1.5046,
        "grad_norm": 2.3420369625091553,
        "learning_rate": 1.3692534454712303e-09,
        "epoch": 0.9994485800937414,
        "step": 3625
    },
    {
        "loss": 2.2374,
        "grad_norm": 1.3273944854736328,
        "learning_rate": 3.4231394726580437e-10,
        "epoch": 0.9997242900468707,
        "step": 3626
    },
    {
        "loss": 2.0132,
        "grad_norm": 2.7599105834960938,
        "learning_rate": 0.0,
        "epoch": 1.0,
        "step": 3627
    },
    {
        "train_runtime": 11034.5487,
        "train_samples_per_second": 0.657,
        "train_steps_per_second": 0.329,
        "total_flos": 1.648432712761344e+17,
        "train_loss": 2.1277506549988763,
        "epoch": 1.0,
        "step": 3627
    }
]