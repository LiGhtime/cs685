{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct the token tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# asins_small.json to be used for meta data matching\n",
    "json_file_asins = '../data/asins_small.json'\n",
    "with open(json_file_asins, \"r\") as file:\n",
    "    asin_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = []\n",
    "for asin in asin_dict:\n",
    "    movie_title = asin_dict[asin][0]\n",
    "    inputs = tokenizer(movie_title, return_tensors=\"pt\")\n",
    "    movie_titles.append(len(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles[133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(movie_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "True\n",
      "0\n",
      "<torch.cuda.device object at 0x7f456c467c10>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check version\n",
    "print(torch.__version__)\n",
    "\n",
    "# check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50103537ade648a0983da966d29ed94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# model_id = \"google/gemma-2b\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# hf_token = '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' # Limexu account, read access\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 4096 # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../outputs/model_04242024_150533/\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evluation NOTE:\n",
    "\n",
    "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
    "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asin:  B005TZGJBA\n",
      "movie_title:  The Spitfire Grill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[651, 231733, 47843]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, value in asin_dict.items():\n",
    "    print(\"asin: \", key)\n",
    "    print(\"movie_title: \", asin_dict[key][0])\n",
    "    break\n",
    "tokenizer(asin_dict['B005TZGJBA'][0], return_tensors=\"pt\", add_special_tokens=False)['input_ids'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a tire. We have many movie titles. We will insert each movie title as a path in the trie.\n",
    "# where each tokenized subword would be a node in the trie.\n",
    "# trie def here\n",
    "class TrieNode:\n",
    "    def __init__(self, id):\n",
    "        self.children = {}\n",
    "        self.node_value = id\n",
    "    \n",
    "class Trie:\n",
    "    def __init__(self, bos_token_id):\n",
    "        self.root = TrieNode(bos_token_id) # set start of sentence id as root node\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.pad_node = TrieNode(self.pad_token_id)\n",
    "        self.pad_node.children[self.pad_token_id] = self.pad_node\n",
    "    \n",
    "    def insert(self, input_ids_list):\n",
    "        node = self.root\n",
    "        for id in input_ids_list[1:]:\n",
    "            if id not in node.children:\n",
    "                node.children[id] = TrieNode(id)\n",
    "            node = node.children[id]\n",
    "        \n",
    "    def search_children(self, input_ids_list):\n",
    "        # search for the input_ids_list in the trie\n",
    "        # the path is presented in the input_ids_list\n",
    "        # return the last node in the path                  \n",
    "        # input_ids_list should always start with 2 (start of sentence id)\n",
    "        \n",
    "        node = self.root\n",
    "        traverse_depth = 0\n",
    "        if len(input_ids_list) == 1:\n",
    "            return node\n",
    "        for id in input_ids_list[1:]:\n",
    "            if id in node.children:\n",
    "                node = node.children[id]\n",
    "                traverse_depth += 1\n",
    "            else:\n",
    "                if id == self.pad_token_id:\n",
    "                    return self.pad_node\n",
    "                \n",
    "                raise ValueError(\"input_ids_list not found in the trie. Traverse failed at node: \", node.node_value, \n",
    "                                \" at depth: \", traverse_depth)\n",
    "        return node\n",
    "\n",
    "bos_token_id = tokenizer(tokenizer.bos_token, return_tensors=\"pt\", add_special_tokens=False)['input_ids'].tolist()[0][0]\n",
    "eos_token_id = tokenizer(tokenizer.eos_token, return_tensors=\"pt\", add_special_tokens=False)['input_ids'].tolist()[0][0]\n",
    "trie = Trie(bos_token_id)\n",
    "for key in asin_dict.keys():\n",
    "    movie_title = asin_dict[key][0]\n",
    "    # tokenize movie title\n",
    "    inputs = tokenizer(movie_title, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].tolist() # get the input ids\n",
    "    # print(\"input_ids: \", input_ids)\n",
    "    # append end of sentence id at the end of the input_ids\n",
    "    input_ids[0].append(eos_token_id)\n",
    "    # insert this input_ids as a path in the trie\n",
    "    for id in input_ids:\n",
    "        trie.insert(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# trie.root.node_value\n",
    "# trie.root.children\n",
    "# trie.search_children([2]).children == trie.root.children\n",
    "# print(trie.search_children([2]).children)\n",
    "\n",
    "# test_int = \n",
    "# move test_int to tensor\n",
    "# test_int_tensor = torch.tensor(test_int)\n",
    "# test_int_tensor.item() in trie.search_children([2, 58423, 8920, 235292, 13316, 235248, 235310]).children\n",
    "test = trie.search_children([2, 58423, 8920, 235292, 13316, 235248, 235310]).children\n",
    "for x in test:\n",
    "    print(x)\n",
    "# trie.search_children([2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using customized constraint logits processor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trie...\n",
      "new_token_input_ids:  [2]\n",
      "new_token_input_ids:  [2, 235284]\n",
      "new_token_input_ids:  [2, 235284, 235304]\n",
      "new_token_input_ids:  [2, 235284, 235304, 68978]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Yellow Stone23 Blast']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    LogitsProcessorList,\n",
    "    ConstraintLogitsProcessor,\n",
    "    TopKLogitsWarper,\n",
    "    TemperatureLogitsWarper,\n",
    "    BeamSearchScorer,\n",
    "    GenerationConfig,\n",
    ")\n",
    "\n",
    "encoder_input_str = \"Yellow Stone\"\n",
    "# encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # lets run beam search using 3 beams\n",
    "# num_beams = 3\n",
    "# # define decoder start token ids\n",
    "# input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "# input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "# # add encoder_outputs to model keyword arguments\n",
    "# model_kwargs = {\n",
    "#     \"encoder_outputs\": model.get_encoder()(\n",
    "#         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # instantiate beam scorer\n",
    "# beam_scorer = BeamSearchScorer(\n",
    "#     batch_size=1,\n",
    "#     max_length=model.config.max_length,\n",
    "#     num_beams=num_beams,\n",
    "#     device=model.device,\n",
    "# )\n",
    "\n",
    "# # construct token tire\n",
    "\n",
    "\n",
    "# # instantiate logits processors\n",
    "# # MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)\n",
    "# logits_processor = LogitsProcessorList(\n",
    "#     [ConstraintLogitsProcessor(eos_token_id=tokenizer.eos, constraint_tries=trie)]\n",
    "# )\n",
    "# # instantiate logits processors\n",
    "# logits_warper = LogitsProcessorList(\n",
    "#     [\n",
    "#         TopKLogitsWarper(50),\n",
    "#         TemperatureLogitsWarper(0.7),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # outputs = model.generate(do_sample=True.....)\n",
    "# outputs = model._beam_sample(\n",
    "#     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n",
    "# )\n",
    "\n",
    "# tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "custom_generation_config = GenerationConfig(\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    asin_dict=asin_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")\n",
    "inputs = tokenizer(encoder_input_str, return_tensors=\"pt\").to(model.device)\n",
    "# TODO: beam_size > 1\n",
    "outputs = model.generate(**inputs, num_beams=1, do_sample=True, max_new_tokens=20, generation_config=custom_generation_config)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded eval dataset from ./data/gemma_chat_eval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"Below is the previous historical purchases and reviews of the user:\\n```\\nItem title: Justified: The Complete Series \\n Item description: For six electrifying seasons, no crime series proved more combustible than the Peabody Award-winning Justified. At the explosive center of the action, Western-style, gun-slinging U.S. Marshal Raylan Givens (Timothy Olyphant) confronts murder, drugs, bank heists, mobsters, crime families, corrupt politicians and even his own tumultuous past – and never backs down. His ultimate adversary is the cunning, complex outlaw Boyd Crowder (Walter Goggins), but the real wild card is Ava Crowder (Joelle Carter), the mysterious woman torn between the two men and both sides of the law. From creator Graham Yost and based on legendary author Elmore Leonard’s crime novella “Fire in the Hole,” it all leads to a perfectly unexpected final showdown. \\n rating: 5.0 \\n review: Love this series....bought it n for my husband for his birthday-------\\n```\\nAnd here is the user's intention: I really enjoy intense, drama-filled series with great character development, surprise twists, and a stellar cast, so I'm looking for something similar to what I've watched before.\\nPlease infer the user's preference based on historical purchases and reviews along with the user's intention, and then recommend an item for this user. Please just give the title of the recommended item.\",\n",
       " 'Yellowstone: Season One[Blu-ray]')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read eval dataset from local\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_eval = load_from_disk(\"./data/gemma_chat_eval\")\n",
    "print(\"Loaded eval dataset from {}\".format(\"./data/gemma_chat_eval\"))\n",
    "\n",
    "index_i = 16\n",
    "dataset_eval[index_i]['input'], \\\n",
    "dataset_eval[index_i]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "<bos><start_of_turn>user\n",
      "Below is the previous historical purchases and reviews of the user:\n",
      "```\n",
      "Item title: Justified: The Complete Series \n",
      " Item description: For six electrifying seasons, no crime series proved more combustible than the Peabody Award-winning Justified. At the explosive center of the action, Western-style, gun-slinging U.S. Marshal Raylan Givens (Timothy Olyphant) confronts murder, drugs, bank heists, mobsters, crime families, corrupt politicians and even his own tumultuous past – and never backs down. His ultimate adversary is the cunning, complex outlaw Boyd Crowder (Walter Goggins), but the real wild card is Ava Crowder (Joelle Carter), the mysterious woman torn between the two men and both sides of the law. From creator Graham Yost and based on legendary author Elmore Leonard’s crime novella “Fire in the Hole,” it all leads to a perfectly unexpected final showdown. \n",
      " rating: 5.0 \n",
      " review: Love this series....bought it n for my husband for his birthday-------\n",
      "```\n",
      "And here is the user's intention: I really enjoy intense, drama-filled series with great character development, surprise twists, and a stellar cast, so I'm looking for something similar to what I've watched before.\n",
      "Please infer the user's preference based on historical purchases and reviews along with the user's intention, and then recommend an item for this user. Please just give the title of the recommended item.<end_of_turn>\n",
      "<start_of_turn>model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using customized constraint logits processor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trie...\n",
      "new_token_input_ids:  [2]\n",
      "new_token_input_ids:  [2, 651]\n",
      "The new_token_input_ids:  [2, 651, 21982]\n",
      "new_token_input_ids:  [2, 651, 21982, 235292]\n",
      "Wire: new_token_input_ids:  [2, 651, 21982, 235292, 13316]\n",
      "Season new_token_input_ids:  [2, 651, 21982, 235292, 13316, 235248]\n",
      "new_token_input_ids:  [2, 651, 21982, 235292, 13316, 235248, 235274]\n",
      "1<eos>\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    max_seq_length = 4096 # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    from transformers import TextStreamer\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"outputs/model_04242024_150533/\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    \n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "input = dataset_eval[index_i]['input']\n",
    "# input = dataset_eval[index_i]['input'] + \"\\n Please also explain yourself in one sentence or two, why the user might be interested in your recommended item.\"\n",
    "# input = \"Hello, I am looking for a movie that will make me feel happy and excited. I love action movies with a lot of suspense and thrill. I also enjoy movies with a lot of drama and romance. Can you recommend a movie that will make me feel happy and excited? And please explain why you recommend this movie.\"\n",
    "# input= \"Below is the previous historical purchases and reviews of the user:\\n```\\nItem title: One Step Behind \\n Item description: A dangerous man awakes in the care of a mysterious woman. Once the pieces of his past fall into place, he's faced with the stark choice of accepting his new found love or becoming the man he once was. \\n rating: 4.0 \\n review: Nice Hard Boiled Indie! Plenty of Bad Guys, Femme Fatales and one Big Bad Ass Reluctant Hero! If you like old school Film Noir -  more specifically Hard Boiled - you'll enjoy this little indie film. It reminded me a lot of the Coen Brother's BLOOD SIMPLE and another desert noir from the 90s starring Nick Cage RED ROCK WEST. It has that same pace and flavor to it and it pays of at the end. Well done!-------\\nItem title: Untouched \\n Item description: This legal drama explores the life-changing effects a secret abortion has on Mitch Thomas. A town startled by an infant found murdered in a dumpster, stirs Mitch, the reluctant attorney, conflicted by the love he lost, to unveil his haunting secret. \\n rating: 4.0 \\n review: Nice courtroom drama with it's heart in the right place. Beautifully shot and written, this is an intriguing story with a bittersweet ending. Really enjoyed it!-------\\nItem title: The Man In The Silo \\n Item description: A successful business man wakes up to find himself in a dilapidated grain silo, he must reconnect the dots of his traumatized memory to discover the truth of how he ended up there. \\n rating: 5.0 \\n review: This is a beautiful film full of great cinematic storytelling plus a classic re-hash of the Bernard Herman's Vertigo Score. Being a Hitchcock fan, I can honestly say that this is a true HITCHCOCKIAN film - which we hear a lot but hardly ever see. You can tell the film was made with so much attention to detail and passion by the great cinematography, transitions, acting, pace, acting and editing - and everything works together to achieve a great visual art piece. But don't get me wrong, it is also super exciting and entertaining as the film keeps you guessing from beginning to end. It is full of intrigue and suspense - and it is haunting. I highly recommend THE MAN IN THE SILO and I can almost guarantee you haven't seen anything like it!-------\\n```\\nAnd here is the user's intention: I really enjoy indie films with strong storytelling and emotional depth, so I'm looking for a movie that will steal my heart and make me want to fall in love all over again.\\nPlease infer the user's preference based on historical purchases and reviews along with the user's intention, and then recommend an item for this user and provide its description.\"\n",
    "# input = \"hello, can you recommend me a random movie? Maybe a romantic one? And please describe the reason why you recommend this movie.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    # \"### Input:\\n{inputs}\\n\\n### Response:{outputs}\".format(inputs= input, outputs= \"\"),\n",
    "    # \"{inputs}\".format(inputs= input),\n",
    "    prompt_template.format(input, \"\"),    \n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "custom_generation_config = GenerationConfig(\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    asin_dict=asin_dict,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    # \"### Input:\\n{inputs}\\n\\n### Response:{outputs}\".format(inputs= input, outputs= \"\"),\n",
    "    # \"{inputs}\".format(inputs= input),\n",
    "    prompt_template.format(input, \"\"),    \n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**inputs, num_beams=1, do_sample=True, max_new_tokens=64, generation_config=custom_generation_config, use_cache=True)\n",
    "# tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2048, num_beams=1, do_sample=True, generation_config=custom_generation_config, use_cache=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
