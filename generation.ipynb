{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434236"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# asins_small.json to be used for meta data matching\n",
    "# json_file_asins = './data/asins_small.json'\n",
    "json_file_asins = './data/meta_asins.json'\n",
    "with open(json_file_asins, \"r\") as file:\n",
    "    asin_dict = json.load(file)\n",
    "    \n",
    "len(asin_dict) # 748224 for the full meta bag # 434236 after filtering out the ones with no title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Spitfire Grill'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asin_dict['B005TZGJBA'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded eval dataset from ./data/gemma_chat_eval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"Below is the previous historical purchases and reviews of the user:\\n```\\nItem title: Justified: The Complete Series \\n Item description: For six electrifying seasons, no crime series proved more combustible than the Peabody Award-winning Justified. At the explosive center of the action, Western-style, gun-slinging U.S. Marshal Raylan Givens (Timothy Olyphant) confronts murder, drugs, bank heists, mobsters, crime families, corrupt politicians and even his own tumultuous past – and never backs down. His ultimate adversary is the cunning, complex outlaw Boyd Crowder (Walter Goggins), but the real wild card is Ava Crowder (Joelle Carter), the mysterious woman torn between the two men and both sides of the law. From creator Graham Yost and based on legendary author Elmore Leonard’s crime novella “Fire in the Hole,” it all leads to a perfectly unexpected final showdown. \\n rating: 5.0 \\n review: Love this series....bought it n for my husband for his birthday-------\\n```\\nAnd here is the user's intention: I really enjoy intense, drama-filled series with great character development, surprise twists, and a stellar cast, so I'm looking for something similar to what I've watched before.\\nPlease infer the user's preference based on historical purchases and reviews along with the user's intention, and then recommend an item for this user. Please just give the title of the recommended item.\",\n",
       " 'Yellowstone: Season One[Blu-ray]')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read eval dataset from local\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_eval = load_from_disk(\"./data/gemma_chat_eval\")\n",
    "print(\"Loaded eval dataset from {}\".format(\"./data/gemma_chat_eval\"))\n",
    "\n",
    "index_i = 16\n",
    "dataset_eval[index_i]['input'], \\\n",
    "dataset_eval[index_i]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using customized constraint logits processor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trie...\n",
      "new_token_input_ids_list[0]: [2]\n",
      "new_token_input_ids_list[0] size: 1\n",
      "new_token_input_ids_list[1]: [2]\n",
      "new_token_input_ids_list[1] size: 1\n",
      "new_token_input_ids_list[2]: [2]\n",
      "new_token_input_ids_list[2] size: 1\n",
      "new_token_input_ids_list[0]: [2, 6750]\n",
      "new_token_input_ids_list[0] size: 2\n",
      "new_token_input_ids_list[1]: [2, 6750]\n",
      "new_token_input_ids_list[1] size: 2\n",
      "new_token_input_ids_list[2]: [2, 235285]\n",
      "new_token_input_ids_list[2] size: 2\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269]\n",
      "new_token_input_ids_list[0] size: 3\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269]\n",
      "new_token_input_ids_list[1] size: 3\n",
      "new_token_input_ids_list[2]: [2, 6750, 75093]\n",
      "new_token_input_ids_list[2] size: 3\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269, 2439]\n",
      "new_token_input_ids_list[0] size: 4\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269, 2439]\n",
      "new_token_input_ids_list[1] size: 4\n",
      "new_token_input_ids_list[2]: [2, 6750, 235269, 3046]\n",
      "new_token_input_ids_list[2] size: 4\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269, 3046, 235265]\n",
      "new_token_input_ids_list[0] size: 5\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269, 2439, 8045]\n",
      "new_token_input_ids_list[1] size: 5\n",
      "new_token_input_ids_list[2]: [2, 6750, 235269, 2439, 8045]\n",
      "new_token_input_ids_list[2] size: 5\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269, 2439, 8045, 1842]\n",
      "new_token_input_ids_list[0] size: 6\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269, 2439, 8045, 1842]\n",
      "new_token_input_ids_list[1] size: 6\n",
      "new_token_input_ids_list[2]: [2, 6750, 235269, 3046, 235265, 59926]\n",
      "new_token_input_ids_list[2] size: 6\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269, 2439, 8045, 1842, 235336]\n",
      "new_token_input_ids_list[0] size: 7\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269, 2439, 8045, 1842, 235336]\n",
      "new_token_input_ids_list[1] size: 7\n",
      "new_token_input_ids_list[2]: [2, 6750, 235269, 3046, 235265, 59926, 235341]\n",
      "new_token_input_ids_list[2] size: 7\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269, 2439, 8045, 1842, 235336, 892]\n",
      "new_token_input_ids_list[0] size: 8\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269, 2439, 8045, 1842, 235336, 892]\n",
      "new_token_input_ids_list[1] size: 8\n",
      "new_token_input_ids_list[2]: [2, 6750, 235269, 3046, 235265, 59926, 235341, 714]\n",
      "new_token_input_ids_list[2] size: 8\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269, 2439, 8045, 1842, 235336, 892, 183950]\n",
      "new_token_input_ids_list[0] size: 9\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269, 2439, 8045, 1842, 235336, 892, 183950]\n",
      "new_token_input_ids_list[1] size: 9\n",
      "new_token_input_ids_list[2]: [2, 6750, 235269, 3046, 235265, 59926, 235341, 714, 44000]\n",
      "new_token_input_ids_list[2] size: 9\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269, 2439, 8045, 1842, 235336, 892, 183950, 235307]\n",
      "new_token_input_ids_list[0] size: 10\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269, 2439, 8045, 1842, 235336, 892, 183950, 235307]\n",
      "new_token_input_ids_list[1] size: 10\n",
      "new_token_input_ids_list[2]: [2, 6750, 235269, 3046, 235265, 59926, 235341, 714, 44000, 3855]\n",
      "new_token_input_ids_list[2] size: 10\n",
      "new_token_input_ids_list[0]: [2, 6750, 235269, 3046, 235265, 59926, 235341, 714, 44000, 3855, 576]\n",
      "new_token_input_ids_list[0] size: 11\n",
      "new_token_input_ids_list[1]: [2, 6750, 235269, 2439, 8045, 1842, 235336, 892, 183950, 235307, 0]\n",
      "new_token_input_ids_list[1] size: 11\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('input_ids_list not found in the trie. Traverse failed at node: ', 235307, ' at depth: ', 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m\n\u001b[1;32m     33\u001b[0m custom_generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig(\n\u001b[1;32m     34\u001b[0m     bos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbos_token_id,\n\u001b[1;32m     35\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# diversity_penalty=0.4,\u001b[39;00m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     54\u001b[0m [\n\u001b[1;32m     55\u001b[0m     prompt_template\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),    \n\u001b[1;32m     56\u001b[0m ], return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_generation_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# print(tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True))\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# outputs['scores'] is the tuple of probabilty distributions at every time step.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# text_streamer = TextStreamer(tokenizer)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2048, num_beams=1, do_sample=True, generation_config=custom_generation_config, use_cache=True)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/llama.py:976\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fast_generate\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m--> 976\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/generation/utils.py:1708\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1700\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1701\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1702\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1703\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1704\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1705\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1708\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1725\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1726\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1732\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1733\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/generation/utils.py:3570\u001b[0m, in \u001b[0;36mGenerationMixin._beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3561\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m   3562\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3563\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m   3565\u001b[0m \u001b[38;5;66;03m# # if logits_processor is our customized ConstraintLogitsProcessor(LogitsProcessor):\u001b[39;00m\n\u001b[1;32m   3566\u001b[0m \u001b[38;5;66;03m# if isinstance(logits_processor, ConstraintLogitsProcessor):\u001b[39;00m\n\u001b[1;32m   3567\u001b[0m \u001b[38;5;66;03m#     # call constraint logits processor\u001b[39;00m\n\u001b[1;32m   3568\u001b[0m \u001b[38;5;66;03m#     next_token_scores = logits_processor(input_ids, next_token_scores)\u001b[39;00m\n\u001b[1;32m   3569\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m-> 3570\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO: maybe need to add trie here\u001b[39;00m\n\u001b[1;32m   3571\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m logits_warper(input_ids, next_token_scores_processed)\n\u001b[1;32m   3572\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores_processed \u001b[38;5;241m+\u001b[39m beam_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mexpand_as(\n\u001b[1;32m   3573\u001b[0m     next_token_scores_processed\n\u001b[1;32m   3574\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/generation/logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/generation/logits_process.py:355\u001b[0m, in \u001b[0;36mConstraintLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_token_input_ids_list[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_token_input_ids_list[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_token_input_ids_list[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_token_input_ids_list[i])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 355\u001b[0m     current_node_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_token_input_ids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# print(\"current_node: \", current_node.node_value)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"current_node.children: \", current_node.children)\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m#         not_in_children_token_mask = torch.isin(vocab_tensor, token)\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m#         scores_processed = torch.where(not_in_children_token_mask, -math.inf, scores_processed)\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, current_node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(current_node_list):\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/generation/logits_process.py:292\u001b[0m, in \u001b[0;36mConstraintLogitsProcessor.__init__.<locals>.Trie.search_children\u001b[0;34m(self, input_ids_list)\u001b[0m\n\u001b[1;32m    287\u001b[0m         traverse_depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;66;03m# if id == self.pad_token_id:\u001b[39;00m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;66;03m#     return self.pad_node\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids_list not found in the trie. Traverse failed at node: \u001b[39m\u001b[38;5;124m\"\u001b[39m, node\u001b[38;5;241m.\u001b[39mnode_value, \n\u001b[1;32m    293\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at depth: \u001b[39m\u001b[38;5;124m\"\u001b[39m, traverse_depth)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "\u001b[0;31mValueError\u001b[0m: ('input_ids_list not found in the trie. Traverse failed at node: ', 235307, ' at depth: ', 9)"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 4096 # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"outputs/model_04242024_090830/\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    # model_name = \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "        \n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "# input = dataset_eval[index_i]['input']\n",
    "# input = dataset_eval[index_i]['input'] + \"\\n Please also explain yourself in one sentence or two, why the user might be interested in your recommended item.\"\n",
    "# input = \"Hello, I am looking for a movie that will make me feel happy and excited. I love action movies with a lot of suspense and thrill. I also enjoy movies with a lot of drama and romance. Can you recommend a movie that will make me feel happy and excited? And please explain why you recommend this movie.\"\n",
    "# input= \"Below is the previous historical purchases and reviews of the user:\\n```\\nItem title: One Step Behind \\n Item description: A dangerous man awakes in the care of a mysterious woman. Once the pieces of his past fall into place, he's faced with the stark choice of accepting his new found love or becoming the man he once was. \\n rating: 4.0 \\n review: Nice Hard Boiled Indie! Plenty of Bad Guys, Femme Fatales and one Big Bad Ass Reluctant Hero! If you like old school Film Noir -  more specifically Hard Boiled - you'll enjoy this little indie film. It reminded me a lot of the Coen Brother's BLOOD SIMPLE and another desert noir from the 90s starring Nick Cage RED ROCK WEST. It has that same pace and flavor to it and it pays of at the end. Well done!-------\\nItem title: Untouched \\n Item description: This legal drama explores the life-changing effects a secret abortion has on Mitch Thomas. A town startled by an infant found murdered in a dumpster, stirs Mitch, the reluctant attorney, conflicted by the love he lost, to unveil his haunting secret. \\n rating: 4.0 \\n review: Nice courtroom drama with it's heart in the right place. Beautifully shot and written, this is an intriguing story with a bittersweet ending. Really enjoyed it!-------\\nItem title: The Man In The Silo \\n Item description: A successful business man wakes up to find himself in a dilapidated grain silo, he must reconnect the dots of his traumatized memory to discover the truth of how he ended up there. \\n rating: 5.0 \\n review: This is a beautiful film full of great cinematic storytelling plus a classic re-hash of the Bernard Herman's Vertigo Score. Being a Hitchcock fan, I can honestly say that this is a true HITCHCOCKIAN film - which we hear a lot but hardly ever see. You can tell the film was made with so much attention to detail and passion by the great cinematography, transitions, acting, pace, acting and editing - and everything works together to achieve a great visual art piece. But don't get me wrong, it is also super exciting and entertaining as the film keeps you guessing from beginning to end. It is full of intrigue and suspense - and it is haunting. I highly recommend THE MAN IN THE SILO and I can almost guarantee you haven't seen anything like it!-------\\n```\\nAnd here is the user's intention: I really enjoy indie films with strong storytelling and emotional depth, so I'm looking for a movie that will steal my heart and make me want to fall in love all over again.\\nPlease infer the user's preference based on historical purchases and reviews along with the user's intention, and then recommend an item for this user and provide its description.\"\n",
    "input = \"hello, can you recommend me a random movie? Maybe a romantic one? And please describe the reason why you recommend this movie.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    # \"### Input:\\n{inputs}\\n\\n### Response:{outputs}\".format(inputs= input, outputs= \"\"),\n",
    "    # \"{inputs}\".format(inputs= input),\n",
    "    prompt_template.format(input, \"\"),    \n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "num_beams_parameter = 3\n",
    "custom_generation_config = GenerationConfig(\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    asin_dict=asin_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    # output_logits=True,\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    num_beams=num_beams_parameter, \n",
    "    num_return_sequences=num_beams_parameter,\n",
    "    max_new_tokens=35,\n",
    "    use_cache=True,\n",
    "    temperature=1,\n",
    "    # num_beam_groups=num_beams_parameter, # In this generation mode, `num_beams` should be divisible by `num_beam_groups`. `diversity_penalty` is not 0.0 or `num_beam_groups` is not 1, triggering group beam search. \n",
    "    # diversity_penalty=0.4,\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt_template.format(input, \"\"),    \n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config=custom_generation_config)\n",
    "# print(outputs)\n",
    "# print(tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True))\n",
    "# outputs['scores'] is the tuple of probabilty distributions at every time step.\n",
    "\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2048, num_beams=1, do_sample=True, generation_config=custom_generation_config, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences\n",
      "sequences_scores\n",
      "scores\n",
      "beam_indices\n",
      "past_key_values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 45]),)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, items in outputs.items():\n",
    "    print(k)\n",
    "outputs['sequences'].size(), \\\n",
    "    # outputs['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, What About Me? [VHS]\n",
      "tensor(-3.9763, device='cuda:0')\n",
      "\n",
      "Hey, What About Me? [VHS]\n",
      "tensor(-3.9763, device='cuda:0')\n",
      "\n",
      "Hey, What About Me? [VHS]\n",
      "tensor(-3.9763, device='cuda:0')\n",
      "\n",
      "Hey, Mr About Me? [VHS]\n",
      "tensor(-5.8166, device='cuda:0')\n",
      "\n",
      "Hey, Mr About Me? [VHS]\n",
      "tensor(-5.8294, device='cuda:0')\n",
      "\n",
      "Hey, Mr About Me? [VHS]\n",
      "tensor(-6.5981, device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(outputs['sequences'])):\n",
    "    # print(tokenizer.batch_decode(outputs['sequences'][i], skip_special_tokens=True))\n",
    "    sequence = \"\".join(tokenizer.batch_decode(outputs['sequences'][i], skip_special_tokens=True))\n",
    "    sequence = sequence.split(\"model\\n\")[1]\n",
    "    print(sequence)\n",
    "    print(outputs['sequences_scores'][i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.eval.metrics import ReciprocalRank\n",
    "\n",
    "metric = ReciprocalRank(k=1)\n",
    "metric.update(torch.tensor([[0, 0.1, 0.6, 0.3], [0, 0, 1, 0]]), torch.tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# outputs['scores'][-1]\n",
    "# check every token's probability distribution, if there is a token with a positive probability\n",
    "for i, token in enumerate(outputs['scores'][-1][0]):\n",
    "    if token != -math.inf:\n",
    "        print(f\"index: {i}, Score: {token}\")\n",
    "        print(tokenizer.decode(i))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read eval dataset from local\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_eval = load_from_disk(\"./data/gemma_chat_eval\")\n",
    "print(\"Loaded eval dataset from {}\".format(\"./data/gemma_chat_eval\"))\n",
    "\n",
    "index_i = 16\n",
    "dataset_eval[index_i]['input'], \\\n",
    "dataset_eval[index_i]['output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
