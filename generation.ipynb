{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct the token tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# asins_small.json to be used for meta data matching\n",
    "json_file_asins = './data/asins_small.json'\n",
    "with open(json_file_asins, \"r\") as file:\n",
    "    asin_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "True\n",
      "0\n",
      "<torch.cuda.device object at 0x000001D4FB8F0DC0>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check version\n",
    "print(torch.__version__)\n",
    "\n",
    "# check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873f08623c1747b9975b422af9b9f31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "hf_token = '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' # Limexu account, read access\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asin:  B005TZGJBA\n",
      "movie_title:  The Spitfire Grill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[651, 231733, 47843]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, value in asin_dict.items():\n",
    "    print(\"asin: \", key)\n",
    "    print(\"movie_title: \", asin_dict[key][0])\n",
    "    break\n",
    "tokenizer(asin_dict['B005TZGJBA'][0], return_tensors=\"pt\", add_special_tokens=False)['input_ids'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init a tire. We have many movie titles. We will insert each movie title as a path in the trie.\n",
    "# # where each tokenized subword would be a node in the trie.\n",
    "# class TrieNode:\n",
    "#     def __init__(self, id):\n",
    "#         self.children = {}\n",
    "#         self.node_value = id\n",
    "#         self.is_end_of_word = False\n",
    "    \n",
    "# class Trie:\n",
    "#     def __init__(self, bos_token_id):\n",
    "#         self.root = TrieNode(bos_token_id) # set start of sentence id as root node\n",
    "    \n",
    "#     def insert(self, input_ids_list):\n",
    "#         node = self.root\n",
    "#         for id in input_ids_list[1:]:\n",
    "#             if id not in node.children:\n",
    "#                 node.children[id] = TrieNode(id)\n",
    "#             node = node.children[id]\n",
    "#         node.is_end_of_word = True  \n",
    "        \n",
    "#     def search_children(self, input_ids_list):\n",
    "#         # search for the input_ids_list in the trie\n",
    "#         # the path is presented in the input_ids_list\n",
    "#         # return the last node in the path                  \n",
    "#         # input_ids_list should always start with 2 (start of sentence id)\n",
    "#         node = self.root\n",
    "#         traverse_depth = 0\n",
    "#         if len(input_ids_list) == 1:\n",
    "#             return node\n",
    "#         for id in input_ids_list[1:]:\n",
    "#             if id in node.children:\n",
    "#                 node = node.children[id]\n",
    "#                 traverse_depth += 1\n",
    "#             else:\n",
    "#                 raise ValueError(\"input_ids_list not found in the trie. Traverse failed at node: \", node.node_value, \n",
    "#                                  \" at depth: \", traverse_depth)\n",
    "#         return node\n",
    "\n",
    "# bos_token_id = tokenizer(tokenizer.bos_token, return_tensors=\"pt\", add_special_tokens=False)['input_ids'].tolist()[0][0]\n",
    "# eos_token_id = tokenizer(tokenizer.eos_token, return_tensors=\"pt\", add_special_tokens=False)['input_ids'].tolist()[0][0]\n",
    "# trie = Trie(bos_token_id)\n",
    "# for key in asin_dict.keys():\n",
    "#     movie_title = asin_dict[key][0]\n",
    "#     # tokenize movie title\n",
    "#     inputs = tokenizer(movie_title, return_tensors=\"pt\")\n",
    "#     input_ids = inputs['input_ids'].tolist() # get the input ids\n",
    "#     # print(\"input_ids: \", input_ids)\n",
    "#     # append end of sentence id at the end of the input_ids\n",
    "#     input_ids[0].append(eos_token_id)\n",
    "#     # insert this input_ids as a path in the trie\n",
    "#     for id in input_ids:\n",
    "#         trie.insert(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trie.root.node_value\n",
    "# trie.root.children\n",
    "# trie.search_children([2]).children == trie.root.children\n",
    "# print(trie.search_children([2]).children)\n",
    "\n",
    "# test_int = 651\n",
    "# # move test_int to tensor\n",
    "# test_int_tensor = torch.tensor(test_int)\n",
    "# test_int_tensor.item() in trie.search_children([2]).children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_token_input_ids:  [2]\n",
      "current_node:  2\n",
      "new_token_input_ids:  [2, 235303]\n",
      "current_node:  235303\n",
      "new_token_input_ids:  [2, 235303, 44939]\n",
      "current_node:  44939\n",
      "new_token_input_ids:  [2, 235303, 44939, 573]\n",
      "current_node:  573\n",
      "new_token_input_ids:  [2, 235303, 44939, 573, 13316]\n",
      "current_node:  13316\n",
      "new_token_input_ids:  [2, 235303, 44939, 573, 13316, 604]\n",
      "current_node:  604\n",
      "new_token_input_ids:  [2, 235303, 44939, 573, 13316, 604, 7377]\n",
      "current_node:  7377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"The office'Tis the Season for Love\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    LogitsProcessorList,\n",
    "    ConstraintLogitsProcessor,\n",
    "    TopKLogitsWarper,\n",
    "    TemperatureLogitsWarper,\n",
    "    BeamSearchScorer,\n",
    "    GenerationConfig,\n",
    ")\n",
    "\n",
    "encoder_input_str = \"The office\"\n",
    "# encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # lets run beam search using 3 beams\n",
    "# num_beams = 3\n",
    "# # define decoder start token ids\n",
    "# input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "# input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "# # add encoder_outputs to model keyword arguments\n",
    "# model_kwargs = {\n",
    "#     \"encoder_outputs\": model.get_encoder()(\n",
    "#         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # instantiate beam scorer\n",
    "# beam_scorer = BeamSearchScorer(\n",
    "#     batch_size=1,\n",
    "#     max_length=model.config.max_length,\n",
    "#     num_beams=num_beams,\n",
    "#     device=model.device,\n",
    "# )\n",
    "\n",
    "# # construct token tire\n",
    "\n",
    "\n",
    "# # instantiate logits processors\n",
    "# # MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)\n",
    "# logits_processor = LogitsProcessorList(\n",
    "#     [ConstraintLogitsProcessor(eos_token_id=tokenizer.eos, constraint_tries=trie)]\n",
    "# )\n",
    "# # instantiate logits processors\n",
    "# logits_warper = LogitsProcessorList(\n",
    "#     [\n",
    "#         TopKLogitsWarper(50),\n",
    "#         TemperatureLogitsWarper(0.7),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # outputs = model.generate(do_sample=True.....)\n",
    "# outputs = model._beam_sample(\n",
    "#     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n",
    "# )\n",
    "\n",
    "# tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "custom_generation_config = GenerationConfig(\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    asin_dict=asin_dict,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "inputs = tokenizer(encoder_input_str, return_tensors=\"pt\").to(model.device)\n",
    "# TODO: beam_size > 1\n",
    "outputs = model.generate(**inputs, num_beams=1, max_new_tokens=20, generation_config=custom_generation_config)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
