{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training data for the model\n",
    "import json\n",
    "\n",
    "# Function to read a .jsonl file\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse each line as JSON and append to the data list\n",
    "            data.append(json.loads(line))\n",
    "            # break\n",
    "    return data\n",
    "\n",
    "movies_meta_data = read_jsonl('./data/meta_Movies_and_TV.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_category: Prime Video\n",
      "title: Glee\n",
      "subtitle: UnentitledUnentitled\n",
      "average_rating: 4.7\n",
      "rating_number: 2004\n",
      "features: ['IMDb 6.8', '2013', '22 episodes', 'X-Ray', 'TV-14']\n",
      "description: ['Entering its fourth season, this year the members of New Directions compete amongst themselves to be the \"new Rachel\" and hold auditions to find new students. Meanwhile, the graduating class leaves the comforts of McKinley where Rachel struggles to please her demanding NYADA teacher (Kate Hudson) and Kurt second-guesses his decision to stay in Lima. Four newcomers also join the musical comedy.']\n",
      "price: 22.39\n",
      "images: [{'360w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX360_FMwebp_.jpg', '480w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX480_FMwebp_.jpg', '720w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX720_FMwebp_.jpg', '1080w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX1080_FMwebp_.jpg', '1440w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX1440_FMwebp_.jpg', '1920w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX1920_FMwebp_.jpg', 'variant': 'MAIN'}]\n",
      "videos: []\n",
      "store: None\n",
      "categories: ['Comedy', 'Drama', 'Arts, Entertainment, and Culture', 'Music Videos and Concerts']\n",
      "details: {'Content advisory': ['Violence', 'substance use', 'alcohol use', 'smoking', 'foul language', 'sexual content'], 'Audio languages': ['English'], 'Subtitles': ['English [CC]'], 'Directors': ['Bradley Buecker', 'Brad Falchuk', 'Eric Stoltz', 'Paris Barclay', 'Ian Brennan', 'Ryan Murphy', 'Alfonso Gomez-Rejon', 'Elodie Keene', 'Adam Shankman', 'Paul McCrane']}\n",
      "parent_asin: B00ABWKL3I\n",
      "bought_together: None\n",
      "*************************************************\n",
      "title: Glee\n",
      "description: Entering its fourth season, this year the members of New Directions compete amongst themselves to be the \"new Rachel\" and hold auditions to find new students. Meanwhile, the graduating class leaves the comforts of McKinley where Rachel struggles to please her demanding NYADA teacher (Kate Hudson) and Kurt second-guesses his decision to stay in Lima. Four newcomers also join the musical comedy.\n"
     ]
    }
   ],
   "source": [
    "for movie in movies_meta_data:\n",
    "    for key in movie.keys():\n",
    "        print(\"{0}: {1}\".format(key, movie[key]))\n",
    "    print(\"*************************************************\")\n",
    "    print(\"title: {0}\".format(movie['title']))\n",
    "    print(\"description: {0}\".format(\"\".join(movie['description'])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# asins_small.json to be used for meta data matching\n",
    "json_file_asins = './data/asins_small.json'\n",
    "# json_file_asins = './data/meta_asins.json'\n",
    "with open(json_file_asins, \"r\") as file:\n",
    "    asin_dict = json.load(file)\n",
    "    \n",
    "asin_list = []\n",
    "for asin in asin_dict:\n",
    "    asin_list.append(asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_meta_data = []\n",
    "for asin in asin_list:\n",
    "    title = asin_dict[asin][0]\n",
    "    description = \"\".join(asin_dict[asin][1])\n",
    "    if title is None or title.strip() == \"\" or description is None or description.strip() == \"\":\n",
    "        continue\n",
    "    movies_meta_data.append({\"title\": title, \"description\": description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'title': 'The Spitfire Grill',\n",
       "  'description': 'Oscar-winner Ellen Burstyn (The Exorcist) headlines this critically praised drama about a young woman, just out of prison, who finds spiritual redemption working at a cafe in Maine.'},\n",
       " 26846)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_meta_data[0], len(movies_meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "prompt_template_token_length:  11\n"
     ]
    }
   ],
   "source": [
    "# load and setup the model and tokenizer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 4096\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "def token_length(text):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    return len(input_ids[0])\n",
    "\n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "prompt_template_token_length = token_length(prompt_template)\n",
    "print(\"prompt_template_token_length: \", prompt_template_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26846/26846 [00:44<00:00, 603.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# construct dataset\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "encoding_title_to_desc_task = {\"input\": [], \"output\": []}\n",
    "encoding_desc_to_title_task = {\"input\": [], \"output\": []}\n",
    "for movie in tqdm(movies_meta_data):\n",
    "    title_to_desc_task_prompt_template = \"Here is the title of a movie: ```{0}```\\n Please write a description of the movie.\"\n",
    "    desc_to_title_task_prompt_template = \"Here is a description of a movie: ```{0}```\\n Please write the title of the movie.\"\n",
    "    if token_length(title_to_desc_task_prompt_template.format(movie['title']) + \"\".join(movie['description'])) <= max_seq_length - prompt_template_token_length:\n",
    "        encoding_title_to_desc_task[\"input\"].append(title_to_desc_task_prompt_template.format(movie['title']))\n",
    "        encoding_title_to_desc_task[\"output\"].append(\"\".join(movie['description']))\n",
    "        encoding_desc_to_title_task[\"input\"].append(desc_to_title_task_prompt_template.format(\"\".join(movie['description'])))\n",
    "        encoding_desc_to_title_task[\"output\"].append(movie['title'])\n",
    "\n",
    "encoding_title_to_desc_task = Dataset.from_dict(encoding_title_to_desc_task)\n",
    "encoding_desc_to_title_task = Dataset.from_dict(encoding_desc_to_title_task)\n",
    "\n",
    "# encoding_title_to_desc_task = encoding_title_to_desc_task.train_test_split(test_size=0.007)\n",
    "# encoding_title_to_desc_task_dataset_train, encoding_title_to_desc_task_dataset_eval = encoding_title_to_desc_task[\"train\"], encoding_title_to_desc_task[\"test\"]\n",
    "# encoding_desc_to_title_task = encoding_desc_to_title_task.train_test_split(test_size=0.007)\n",
    "# encoding_desc_to_title_task_dataset_train, encoding_desc_to_title_task_dataset_eval = encoding_desc_to_title_task[\"train\"], encoding_desc_to_title_task[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'output'],\n",
       "     num_rows: 26837\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'output'],\n",
       "     num_rows: 26837\n",
       " }))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_title_to_desc_task, encoding_desc_to_title_task, # before length filtering, num_rows = 434236\n",
    "# encoding_title_to_desc_task_dataset_train, encoding_title_to_desc_task_dataset_eval, encoding_desc_to_title_task_dataset_train, encoding_desc_to_title_task_dataset_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0ec72cb572456c873d3134a3622f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26837 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81bfd0b71dc40b0901e23a10e920eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26837 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save huggingface dataset to local\n",
    "encoding_title_to_desc_task.save_to_disk('./data/encoding_title_to_desc_task_fixed_empty_string_filter')\n",
    "encoding_desc_to_title_task.save_to_disk('./data/encoding_desc_to_title_task_fixed_empty_string_filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccb174fa3b545b4a0a81dfcfdae5b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/434213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 434213/434213 [05:26<00:00, 1328.77it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the distribution of length of (input + output) text in training dict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "encoding_title_to_desc_task_combined = encoding_title_to_desc_task.map(lambda x: {\"combined_text\": x[\"input\"] + x[\"output\"]})\n",
    "\n",
    "length_of_input_plus_output = []\n",
    "for text in tqdm(encoding_title_to_desc_task_combined['combined_text']):\n",
    "    length_of_input_plus_output.append(token_length(text))\n",
    "\n",
    "# Create the histogram and get the number of occurrences and the bin edges\n",
    "counts, bins, patches = plt.hist(length_of_input_plus_output, bins=20)\n",
    "\n",
    "# Loop over the patches (bars) and add a text label above each bar\n",
    "for count, bin, patch in zip(counts, bins, patches):\n",
    "    height = patch.get_height()\n",
    "    plt.text(patch.get_x() + patch.get_width() / 2, height + 5, str(int(count)),\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Length of Prompts')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Distribution of Length of Prompts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training Encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "hyper_params = {\n",
    "    # Model hyperparameters\n",
    "    \"max_seq_length\": 4096, # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "    \"dtype\": None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    \"load_in_4bit\": True, # Use 4bit quantization to reduce memory usage. Can be False.,\n",
    "    \"model_name\": \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    \"r\": 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # Add more to target more modules\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0, # Supports any, but = 0 is optimized\n",
    "    \"lora_bias\": \"none\", # Supports any, but = \"none\" is optimized\n",
    "    \"lora_use_gradient_checkpointing\": \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    \"lora_random_state\": 3407,\n",
    "    \"lora_use_rslora\": False, # We support rank stabilized LoRA\n",
    "    \"lora_loftq_config\": None, # And LoftQ\n",
    "    # Training hyperparameters\n",
    "    \"encoding_title_to_desc_task_path\": \"./data/encoding_title_to_desc_task\",\n",
    "    \"encoding_desc_to_title_task_path\": \"./data/encoding_desc_to_title_task\",\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_steps\": 25, # will replace num_warmup_steps in lr_scheduler_kwargs\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\": torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\": 1,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"cosine_with_restarts\",\n",
    "    \"lr_scheduler_kwargs\": {\"num_cycles\": 3}, # \"num_warmup_steps\" and \"num_training_steps\" will be added automatically\n",
    "    \"seed\": 3407,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "model, tokenizer= FastLanguageModel.from_pretrained(\n",
    "    model_name = hyper_params[\"model_name\"], # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = hyper_params[\"max_seq_length\"],\n",
    "    dtype = hyper_params[\"dtype\"],\n",
    "    load_in_4bit = hyper_params[\"load_in_4bit\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.4 patched 18 layers with 18 QKV layers, 18 O layers and 18 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# add lora to model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = hyper_params['r'],\n",
    "    target_modules = hyper_params['target_modules'],\n",
    "    lora_alpha = hyper_params['lora_alpha'],\n",
    "    lora_dropout = hyper_params['lora_dropout'],\n",
    "    bias = hyper_params['lora_bias'],\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = hyper_params['lora_use_gradient_checkpointing'],\n",
    "    random_state = hyper_params['lora_random_state'],\n",
    "    use_rslora = hyper_params['lora_use_rslora'], # We support rank stabilized LoRA\n",
    "    loftq_config = hyper_params['lora_loftq_config'], # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train encoding_title_to_desc_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read huggingface dataset from local\n",
    "encoding_title_to_desc_task = load_from_disk(hyper_params[\"encoding_title_to_desc_task_path\"])\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "prompt_template = prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # text = \"### Input:\\n{inputs_holder}\\n\\n### Response:{outputs_holder}\".format(inputs_holder= input, outputs_holder= output) + EOS_TOKEN\n",
    "        # gemma chat template:\n",
    "        text = prompt_template.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "train_dataset = encoding_title_to_desc_task.map(formatting_prompts_func, batched = True,)\n",
    "# train_dataset = encoding_desc_to_title_task.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# take samples from the dataset\n",
    "train_dataset = train_dataset.shuffle(seed=hyper_params[\"seed\"])\n",
    "train_dataset = train_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac231d9a85e14e4c9bcaaa3b4a9b6b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    # eval_dataset = test_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = hyper_params['max_seq_length'],\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = hyper_params['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps = hyper_params['gradient_accumulation_steps'],\n",
    "        warmup_steps = hyper_params['warmup_steps'],\n",
    "        num_train_epochs = hyper_params['num_train_epochs'],\n",
    "        # max_steps = 100,\n",
    "        learning_rate = hyper_params['learning_rate'],\n",
    "        fp16 = hyper_params['fp16'],\n",
    "        bf16 = hyper_params['bf16'],\n",
    "        logging_steps = hyper_params['logging_steps'],\n",
    "        optim = hyper_params['optim'],\n",
    "        weight_decay = hyper_params['weight_decay'],\n",
    "        lr_scheduler_type = hyper_params['lr_scheduler_type'],\n",
    "        lr_scheduler_kwargs = hyper_params['lr_scheduler_kwargs'],\n",
    "        seed = hyper_params['seed'],\n",
    "        output_dir = \"outputs\",\n",
    "        # fp16_full_eval = True,\n",
    "        # per_device_eval_batch_size = 1,\n",
    "        # eval_accumulation_steps = 1,\n",
    "        # evaluation_strategy = \"steps\", # epoch\n",
    "        # eval_steps = 100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 2 | Total steps = 100\n",
      " \"-____-\"     Number of trainable parameters = 9,805,824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.764100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.095100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.815800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.909700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.572600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.764100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.937700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model and loss history\n",
    "# get current datetime\n",
    "now = datetime.now()\n",
    "\n",
    "# name the model\n",
    "model_name = \"e_t2d_model_\" + now.strftime(\"%m%d%Y_%H%M%S\")\n",
    "model_path = \"outputs/\" + model_name\n",
    "model.save_pretrained(model_path) # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "\n",
    "# save hyperparameters as a json dict to model_path\n",
    "with open(model_path + \"/hyperparameters.json\", \"w\") as file:\n",
    "    json.dump(hyper_params, file, indent=4)\n",
    "        \n",
    "# save trainer.state.log_history to model_path\n",
    "with open(model_path + \"/trainer_state_log_history.json\", \"w\") as file:\n",
    "    json.dump(trainer.state.log_history, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train encoding_desc_to_title_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the cache\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf479082fdf84fdfbcb6aa1a01e131bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/434213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# similarly, we can train the model for encoding_desc_to_title_task......\n",
    "# read huggingface dataset from local\n",
    "encoding_desc_to_title_task = load_from_disk(hyper_params[\"encoding_desc_to_title_task_path\"])\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "prompt_template = prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # text = \"### Input:\\n{inputs_holder}\\n\\n### Response:{outputs_holder}\".format(inputs_holder= input, outputs_holder= output) + EOS_TOKEN\n",
    "        # gemma chat template:\n",
    "        text = prompt_template.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "train_dataset = encoding_desc_to_title_task.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# take samples from the dataset\n",
    "train_dataset = train_dataset.shuffle(seed=hyper_params[\"seed\"])\n",
    "train_dataset = train_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9460bce8171416a81cb86c86dd69de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    # eval_dataset = test_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = hyper_params['max_seq_length'],\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = hyper_params['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps = hyper_params['gradient_accumulation_steps'],\n",
    "        warmup_steps = hyper_params['warmup_steps'],\n",
    "        num_train_epochs = hyper_params['num_train_epochs'],\n",
    "        # max_steps = 100,\n",
    "        learning_rate = hyper_params['learning_rate'],\n",
    "        fp16 = hyper_params['fp16'],\n",
    "        bf16 = hyper_params['bf16'],\n",
    "        logging_steps = hyper_params['logging_steps'],\n",
    "        optim = hyper_params['optim'],\n",
    "        weight_decay = hyper_params['weight_decay'],\n",
    "        lr_scheduler_type = hyper_params['lr_scheduler_type'],\n",
    "        lr_scheduler_kwargs = hyper_params['lr_scheduler_kwargs'],\n",
    "        seed = hyper_params['seed'],\n",
    "        output_dir = \"outputs\",\n",
    "        # fp16_full_eval = True,\n",
    "        # per_device_eval_batch_size = 1,\n",
    "        # eval_accumulation_steps = 1,\n",
    "        # evaluation_strategy = \"steps\", # epoch\n",
    "        # eval_steps = 100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.932600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.592600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.551500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.734800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.393400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.724900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.841700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.949300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.731900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.727100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model and loss history\n",
    "# get current datetime\n",
    "now = datetime.now()\n",
    "\n",
    "# name the model\n",
    "model_name = \"e_d2t_model_\" + now.strftime(\"%m%d%Y_%H%M%S\")\n",
    "model_path = \"outputs/\" + model_name\n",
    "model.save_pretrained(model_path) # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "\n",
    "# save hyperparameters as a json dict to model_path\n",
    "with open(model_path + \"/hyperparameters.json\", \"w\") as file:\n",
    "    json.dump(hyper_params, file, indent=4)\n",
    "        \n",
    "# save trainer.state.log_history to model_path\n",
    "with open(model_path + \"/trainer_state_log_history.json\", \"w\") as file:\n",
    "    json.dump(trainer.state.log_history, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Here is the title of a movie: ```Glee```\\n Please write a description of the movie.',\n",
       " 'Entering its fourth season, this year the members of New Directions compete amongst themselves to be the \"new Rachel\" and hold auditions to find new students. Meanwhile, the graduating class leaves the comforts of McKinley where Rachel struggles to please her demanding NYADA teacher (Kate Hudson) and Kurt second-guesses his decision to stay in Lima. Four newcomers also join the musical comedy.')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read eval dataset from local\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_eval = load_from_disk(\"./data/encoding_title_to_desc_task\")\n",
    "\n",
    "# dataset_eval.num_rows\n",
    "# index_i = 6\n",
    "index_i = 0\n",
    "dataset_eval[index_i]['input'], \\\n",
    "dataset_eval[index_i]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6588869bfab043799a982bfe408eaf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/434213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering its fourth season, this year the members of New Directions compete amongst themselves to be the \"new Rachel\" and hold auditions to find new students. Meanwhile, the graduating class leaves the comforts of McKinley where Rachel struggles to please her demanding NYADA teacher (Kate Hudson) and Kurt second-guesses his decision to stay in Lima. Four newcomers also join the musical comedy.\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named text expected length 1000 but got length 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m : texts, }\n\u001b[0;32m---> 19\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatting_prompts_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# take samples from the dataset\u001b[39;00m\n\u001b[1;32m     22\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3407\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3570\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3568\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3570\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3571\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[1;32m   3572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/datasets/arrow_writer.py:571\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    569\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[1;32m    570\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[0;32m--> 571\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/pyarrow/table.pxi:4642\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/pyarrow/table.pxi:3922\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 2 named text expected length 1000 but got length 1"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # text = \"### Input:\\n{inputs_holder}\\n\\n### Response:{outputs_holder}\".format(inputs_holder= input, outputs_holder= output) + EOS_TOKEN\n",
    "        # gemma chat template:\n",
    "        text = prompt_template.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "        print(output)\n",
    "        break\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "train_dataset = dataset_eval.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# take samples from the dataset\n",
    "train_dataset = train_dataset.shuffle(seed=3407)\n",
    "train_dataset = train_dataset.select(range(15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Here is the title of a movie: ```The Mule [Blu-ray] [2019]```\\n Please write a description of the movie.',\n",
       " 'output': \"Clint Eastwood stars as Earl Stone, a man in his 80s who is broke, alone, and facing foreclosure of his business when he is offered a job that simply requires him to drive. Easy enough, but, unbeknownst to Earl, he's just signed on as a drug courier for a Mexican cartel. He does well-so well, in fact, that his cargo increases exponentially, and Earl is assigned a handler. But he isn't the only one keeping tabs on Earl; the mysterious new drug mule has also hit the radar of hard-charging DEA agent Colin Bates. And even as his money problems become a thing of the past, Earl's past mistakes start to weigh heavily on him, and it's uncertain if he'll have time to right those wrongs before law enforcement, or the cartel's enforcers, catch up to him.\",\n",
       " 'text': \"<start_of_turn>user\\nHere is the title of a movie: ```The Mule [Blu-ray] [2019]```\\n Please write a description of the movie.<end_of_turn>\\n<start_of_turn>model\\nClint Eastwood stars as Earl Stone, a man in his 80s who is broke, alone, and facing foreclosure of his business when he is offered a job that simply requires him to drive. Easy enough, but, unbeknownst to Earl, he's just signed on as a drug courier for a Mexican cartel. He does well-so well, in fact, that his cargo increases exponentially, and Earl is assigned a handler. But he isn't the only one keeping tabs on Earl; the mysterious new drug mule has also hit the radar of hard-charging DEA agent Colin Bates. And even as his money problems become a thing of the past, Earl's past mistakes start to weigh heavily on him, and it's uncertain if he'll have time to right those wrongs before law enforcement, or the cartel's enforcers, catch up to him.<eos>\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training results\n",
    "title_to_desc_task_prompt_template = \"Here is the title of a movie: ```{0}```\\n Please write a description of the movie.\"\n",
    "desc_to_title_task_prompt_template = \"Here is a description of a movie: ```{0}```\\n Please write the title of the movie.\"\n",
    "    \n",
    "test_input = title_to_desc_task_prompt_template.format(\"Glee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "CUDA memory usage is high. Cleared the cache.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096 # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"outputs/model_04242024_090830/\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    # model_name = \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    # model_name = \"outputs/checkpoint-1000\",\n",
    "    model_name = \"outputs/e_t2d_model_05142024_084000\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "        \n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    # \"### Input:\\n{inputs}\\n\\n### Response:{outputs}\".format(inputs= input, outputs= \"\"),\n",
    "    # \"{inputs}\".format(inputs= input),\n",
    "    prompt_template.format(test_input, \"\"),    \n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "num_beams_parameter = 5\n",
    "custom_generation_config = GenerationConfig(\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    asin_dict=None,\n",
    "    tokenizer=tokenizer,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    # output_logits=True,\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    num_beams=num_beams_parameter, \n",
    "    num_return_sequences=num_beams_parameter,\n",
    "    max_new_tokens=35,\n",
    "    use_cache=True,\n",
    "    temperature=1,\n",
    "    # num_beam_groups=5, # In this generation mode, `num_beams` should be divisible by `num_beam_groups`. `diversity_penalty` is not 0.0 or `num_beam_groups` is not 1, triggering group beam search. \n",
    "    # diversity_penalty=0.9, # `diversity_penalty` should be greater than `0.0`, otherwise your groups will be identical.\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     prompt_template.format(input, \"\"),    \n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config=custom_generation_config)\n",
    "\n",
    "# check CUDA memory usage\n",
    "used_memory = round(torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024, 3)\n",
    "# if used_memory close to 14GB, empty the cache\n",
    "if used_memory > 1:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA memory usage is high. Cleared the cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  user\n",
      "Here is the title of a movie: ```Glee```\n",
      " Please write a description of the movie.\n",
      "model\n",
      "Glee\n",
      "- Season 1\n",
      "- 1-disc box set\n",
      "*************************************************\n",
      "Generated text:  user\n",
      "Here is the title of a movie: ```Glee```\n",
      " Please write a description of the movie.\n",
      "model\n",
      "Glee\n",
      "*************************************************\n",
      "Generated text:  user\n",
      "Here is the title of a movie: ```Glee```\n",
      " Please write a description of the movie.\n",
      "model\n",
      "Glee\n",
      "*************************************************\n",
      "Generated text:  user\n",
      "Here is the title of a movie: ```Glee```\n",
      " Please write a description of the movie.\n",
      "model\n",
      "Glee\n",
      "- Season 2\n",
      "*************************************************\n",
      "Generated text:  user\n",
      "Here is the title of a movie: ```Glee```\n",
      " Please write a description of the movie.\n",
      "model\n",
      "Glee\n",
      "- Season 5\n",
      "*************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_beams_parameter):\n",
    "    # print(outputs['sequences'][i])\n",
    "    print(\"Generated text: \", \"\".join(tokenizer.decode(outputs['sequences'][i], skip_special_tokens=True)))\n",
    "    print(\"*************************************************\")\n",
    "# print(\"Hidden states: \", outputs['hidden_states'])\n",
    "# print(\"*************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predicting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 construct training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Distance based loss function - training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
