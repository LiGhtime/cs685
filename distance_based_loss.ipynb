{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training data for the model\n",
    "import json\n",
    "\n",
    "# Function to read a .jsonl file\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse each line as JSON and append to the data list\n",
    "            data.append(json.loads(line))\n",
    "            # break\n",
    "    return data\n",
    "\n",
    "movies_meta_data = read_jsonl('./data/meta_Movies_and_TV.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_category: Prime Video\n",
      "title: Glee\n",
      "subtitle: UnentitledUnentitled\n",
      "average_rating: 4.7\n",
      "rating_number: 2004\n",
      "features: ['IMDb 6.8', '2013', '22 episodes', 'X-Ray', 'TV-14']\n",
      "description: ['Entering its fourth season, this year the members of New Directions compete amongst themselves to be the \"new Rachel\" and hold auditions to find new students. Meanwhile, the graduating class leaves the comforts of McKinley where Rachel struggles to please her demanding NYADA teacher (Kate Hudson) and Kurt second-guesses his decision to stay in Lima. Four newcomers also join the musical comedy.']\n",
      "price: 22.39\n",
      "images: [{'360w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX360_FMwebp_.jpg', '480w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX480_FMwebp_.jpg', '720w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX720_FMwebp_.jpg', '1080w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX1080_FMwebp_.jpg', '1440w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX1440_FMwebp_.jpg', '1920w': 'https://images-na.ssl-images-amazon.com/images/S/pv-target-images/8251ee0b9f888d262cd817a5f1aee0b29ffed56a4535af898b827292f881e169._RI_SX1920_FMwebp_.jpg', 'variant': 'MAIN'}]\n",
      "videos: []\n",
      "store: None\n",
      "categories: ['Comedy', 'Drama', 'Arts, Entertainment, and Culture', 'Music Videos and Concerts']\n",
      "details: {'Content advisory': ['Violence', 'substance use', 'alcohol use', 'smoking', 'foul language', 'sexual content'], 'Audio languages': ['English'], 'Subtitles': ['English [CC]'], 'Directors': ['Bradley Buecker', 'Brad Falchuk', 'Eric Stoltz', 'Paris Barclay', 'Ian Brennan', 'Ryan Murphy', 'Alfonso Gomez-Rejon', 'Elodie Keene', 'Adam Shankman', 'Paul McCrane']}\n",
      "parent_asin: B00ABWKL3I\n",
      "bought_together: None\n",
      "*************************************************\n",
      "title: Glee\n",
      "description: Entering its fourth season, this year the members of New Directions compete amongst themselves to be the \"new Rachel\" and hold auditions to find new students. Meanwhile, the graduating class leaves the comforts of McKinley where Rachel struggles to please her demanding NYADA teacher (Kate Hudson) and Kurt second-guesses his decision to stay in Lima. Four newcomers also join the musical comedy.\n"
     ]
    }
   ],
   "source": [
    "for movie in movies_meta_data:\n",
    "    for key in movie.keys():\n",
    "        print(\"{0}: {1}\".format(key, movie[key]))\n",
    "    print(\"*************************************************\")\n",
    "    print(\"title: {0}\".format(movie['title']))\n",
    "    print(\"description: {0}\".format(\"\".join(movie['description'])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# asins_small.json to be used for meta data matching\n",
    "json_file_asins = './data/asins_small.json'\n",
    "# json_file_asins = './data/meta_asins.json'\n",
    "with open(json_file_asins, \"r\") as file:\n",
    "    asin_dict = json.load(file)\n",
    "    \n",
    "asin_list = []\n",
    "for asin in asin_dict:\n",
    "    asin_list.append(asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_meta_data = []\n",
    "for asin in asin_list:\n",
    "    title = asin_dict[asin][0]\n",
    "    description = \"\".join(asin_dict[asin][1])\n",
    "    if title is None or title.strip() == \"\" or description is None or description.strip() == \"\":\n",
    "        continue\n",
    "    movies_meta_data.append({\"title\": title, \"description\": description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'title': 'The Spitfire Grill',\n",
       "  'description': 'Oscar-winner Ellen Burstyn (The Exorcist) headlines this critically praised drama about a young woman, just out of prison, who finds spiritual redemption working at a cafe in Maine.'},\n",
       " 26846)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_meta_data[0], len(movies_meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "prompt_template_token_length:  11\n"
     ]
    }
   ],
   "source": [
    "# load and setup the model and tokenizer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 4096\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "def token_length(text):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    return len(input_ids[0])\n",
    "\n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "prompt_template_token_length = token_length(prompt_template)\n",
    "print(\"prompt_template_token_length: \", prompt_template_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26846/26846 [00:44<00:00, 603.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# construct dataset\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "encoding_title_to_desc_task = {\"input\": [], \"output\": []}\n",
    "encoding_desc_to_title_task = {\"input\": [], \"output\": []}\n",
    "for movie in tqdm(movies_meta_data):\n",
    "    title_to_desc_task_prompt_template = \"Here is the title of a movie: ```{0}```\\n Please write a description of the movie.\"\n",
    "    desc_to_title_task_prompt_template = \"Here is a description of a movie: ```{0}```\\n Please write the title of the movie.\"\n",
    "    if token_length(title_to_desc_task_prompt_template.format(movie['title']) + \"\".join(movie['description'])) <= max_seq_length - prompt_template_token_length:\n",
    "        encoding_title_to_desc_task[\"input\"].append(title_to_desc_task_prompt_template.format(movie['title']))\n",
    "        encoding_title_to_desc_task[\"output\"].append(\"\".join(movie['description']))\n",
    "        encoding_desc_to_title_task[\"input\"].append(desc_to_title_task_prompt_template.format(\"\".join(movie['description'])))\n",
    "        encoding_desc_to_title_task[\"output\"].append(movie['title'])\n",
    "\n",
    "encoding_title_to_desc_task = Dataset.from_dict(encoding_title_to_desc_task)\n",
    "encoding_desc_to_title_task = Dataset.from_dict(encoding_desc_to_title_task)\n",
    "\n",
    "# encoding_title_to_desc_task = encoding_title_to_desc_task.train_test_split(test_size=0.007)\n",
    "# encoding_title_to_desc_task_dataset_train, encoding_title_to_desc_task_dataset_eval = encoding_title_to_desc_task[\"train\"], encoding_title_to_desc_task[\"test\"]\n",
    "# encoding_desc_to_title_task = encoding_desc_to_title_task.train_test_split(test_size=0.007)\n",
    "# encoding_desc_to_title_task_dataset_train, encoding_desc_to_title_task_dataset_eval = encoding_desc_to_title_task[\"train\"], encoding_desc_to_title_task[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'output'],\n",
       "     num_rows: 26837\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'output'],\n",
       "     num_rows: 26837\n",
       " }))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_title_to_desc_task, encoding_desc_to_title_task, # before length filtering, num_rows = 434236\n",
    "# encoding_title_to_desc_task_dataset_train, encoding_title_to_desc_task_dataset_eval, encoding_desc_to_title_task_dataset_train, encoding_desc_to_title_task_dataset_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0ec72cb572456c873d3134a3622f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26837 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81bfd0b71dc40b0901e23a10e920eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26837 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save huggingface dataset to local\n",
    "encoding_title_to_desc_task.save_to_disk('./data/encoding_title_to_desc_task_fixed_empty_string_filter')\n",
    "encoding_desc_to_title_task.save_to_disk('./data/encoding_desc_to_title_task_fixed_empty_string_filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccb174fa3b545b4a0a81dfcfdae5b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/434213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 434213/434213 [05:26<00:00, 1328.77it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the distribution of length of (input + output) text in training dict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "encoding_title_to_desc_task_combined = encoding_title_to_desc_task.map(lambda x: {\"combined_text\": x[\"input\"] + x[\"output\"]})\n",
    "\n",
    "length_of_input_plus_output = []\n",
    "for text in tqdm(encoding_title_to_desc_task_combined['combined_text']):\n",
    "    length_of_input_plus_output.append(token_length(text))\n",
    "\n",
    "# Create the histogram and get the number of occurrences and the bin edges\n",
    "counts, bins, patches = plt.hist(length_of_input_plus_output, bins=20)\n",
    "\n",
    "# Loop over the patches (bars) and add a text label above each bar\n",
    "for count, bin, patch in zip(counts, bins, patches):\n",
    "    height = patch.get_height()\n",
    "    plt.text(patch.get_x() + patch.get_width() / 2, height + 5, str(int(count)),\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Length of Prompts')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Distribution of Length of Prompts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training Encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "hyper_params = {\n",
    "    # Model hyperparameters\n",
    "    \"max_seq_length\": 4096, # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "    \"dtype\": None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    \"load_in_4bit\": True, # Use 4bit quantization to reduce memory usage. Can be False.,\n",
    "    \"model_name\": \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    \"r\": 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # Add more to target more modules\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0, # Supports any, but = 0 is optimized\n",
    "    \"lora_bias\": \"none\", # Supports any, but = \"none\" is optimized\n",
    "    \"lora_use_gradient_checkpointing\": \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    \"lora_random_state\": 3407,\n",
    "    \"lora_use_rslora\": False, # We support rank stabilized LoRA\n",
    "    \"lora_loftq_config\": None, # And LoftQ\n",
    "    # Training hyperparameters\n",
    "    \"encoding_title_to_desc_task_path\": \"./data/encoding_title_to_desc_task\",\n",
    "    \"encoding_desc_to_title_task_path\": \"./data/encoding_desc_to_title_task\",\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_steps\": 25, # will replace num_warmup_steps in lr_scheduler_kwargs\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\": torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\": 1,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"cosine_with_restarts\",\n",
    "    \"lr_scheduler_kwargs\": {\"num_cycles\": 3}, # \"num_warmup_steps\" and \"num_training_steps\" will be added automatically\n",
    "    \"seed\": 3407,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "model, tokenizer= FastLanguageModel.from_pretrained(\n",
    "    model_name = hyper_params[\"model_name\"], # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = hyper_params[\"max_seq_length\"],\n",
    "    dtype = hyper_params[\"dtype\"],\n",
    "    load_in_4bit = hyper_params[\"load_in_4bit\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.4 patched 18 layers with 18 QKV layers, 18 O layers and 18 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# add lora to model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = hyper_params['r'],\n",
    "    target_modules = hyper_params['target_modules'],\n",
    "    lora_alpha = hyper_params['lora_alpha'],\n",
    "    lora_dropout = hyper_params['lora_dropout'],\n",
    "    bias = hyper_params['lora_bias'],\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = hyper_params['lora_use_gradient_checkpointing'],\n",
    "    random_state = hyper_params['lora_random_state'],\n",
    "    use_rslora = hyper_params['lora_use_rslora'], # We support rank stabilized LoRA\n",
    "    loftq_config = hyper_params['lora_loftq_config'], # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train encoding_title_to_desc_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read huggingface dataset from local\n",
    "encoding_title_to_desc_task = load_from_disk(hyper_params[\"encoding_title_to_desc_task_path\"])\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "prompt_template = prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # text = \"### Input:\\n{inputs_holder}\\n\\n### Response:{outputs_holder}\".format(inputs_holder= input, outputs_holder= output) + EOS_TOKEN\n",
    "        # gemma chat template:\n",
    "        text = prompt_template.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "train_dataset = encoding_title_to_desc_task.map(formatting_prompts_func, batched = True,)\n",
    "# train_dataset = encoding_desc_to_title_task.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# take samples from the dataset\n",
    "train_dataset = train_dataset.shuffle(seed=hyper_params[\"seed\"])\n",
    "train_dataset = train_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac231d9a85e14e4c9bcaaa3b4a9b6b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    # eval_dataset = test_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = hyper_params['max_seq_length'],\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = hyper_params['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps = hyper_params['gradient_accumulation_steps'],\n",
    "        warmup_steps = hyper_params['warmup_steps'],\n",
    "        num_train_epochs = hyper_params['num_train_epochs'],\n",
    "        # max_steps = 100,\n",
    "        learning_rate = hyper_params['learning_rate'],\n",
    "        fp16 = hyper_params['fp16'],\n",
    "        bf16 = hyper_params['bf16'],\n",
    "        logging_steps = hyper_params['logging_steps'],\n",
    "        optim = hyper_params['optim'],\n",
    "        weight_decay = hyper_params['weight_decay'],\n",
    "        lr_scheduler_type = hyper_params['lr_scheduler_type'],\n",
    "        lr_scheduler_kwargs = hyper_params['lr_scheduler_kwargs'],\n",
    "        seed = hyper_params['seed'],\n",
    "        output_dir = \"outputs\",\n",
    "        # fp16_full_eval = True,\n",
    "        # per_device_eval_batch_size = 1,\n",
    "        # eval_accumulation_steps = 1,\n",
    "        # evaluation_strategy = \"steps\", # epoch\n",
    "        # eval_steps = 100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 2 | Total steps = 100\n",
      " \"-____-\"     Number of trainable parameters = 9,805,824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.764100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.095100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.815800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.909700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.899700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.572600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.764100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.937700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model and loss history\n",
    "# get current datetime\n",
    "now = datetime.now()\n",
    "\n",
    "# name the model\n",
    "model_name = \"e_t2d_model_\" + now.strftime(\"%m%d%Y_%H%M%S\")\n",
    "model_path = \"outputs/\" + model_name\n",
    "model.save_pretrained(model_path) # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "\n",
    "# save hyperparameters as a json dict to model_path\n",
    "with open(model_path + \"/hyperparameters.json\", \"w\") as file:\n",
    "    json.dump(hyper_params, file, indent=4)\n",
    "        \n",
    "# save trainer.state.log_history to model_path\n",
    "with open(model_path + \"/trainer_state_log_history.json\", \"w\") as file:\n",
    "    json.dump(trainer.state.log_history, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train encoding_desc_to_title_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the cache\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf479082fdf84fdfbcb6aa1a01e131bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/434213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# similarly, we can train the model for encoding_desc_to_title_task......\n",
    "# read huggingface dataset from local\n",
    "encoding_desc_to_title_task = load_from_disk(hyper_params[\"encoding_desc_to_title_task_path\"])\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "prompt_template = prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # text = \"### Input:\\n{inputs_holder}\\n\\n### Response:{outputs_holder}\".format(inputs_holder= input, outputs_holder= output) + EOS_TOKEN\n",
    "        # gemma chat template:\n",
    "        text = prompt_template.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "train_dataset = encoding_desc_to_title_task.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# take samples from the dataset\n",
    "train_dataset = train_dataset.shuffle(seed=hyper_params[\"seed\"])\n",
    "train_dataset = train_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9460bce8171416a81cb86c86dd69de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    # eval_dataset = test_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = hyper_params['max_seq_length'],\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = hyper_params['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps = hyper_params['gradient_accumulation_steps'],\n",
    "        warmup_steps = hyper_params['warmup_steps'],\n",
    "        num_train_epochs = hyper_params['num_train_epochs'],\n",
    "        # max_steps = 100,\n",
    "        learning_rate = hyper_params['learning_rate'],\n",
    "        fp16 = hyper_params['fp16'],\n",
    "        bf16 = hyper_params['bf16'],\n",
    "        logging_steps = hyper_params['logging_steps'],\n",
    "        optim = hyper_params['optim'],\n",
    "        weight_decay = hyper_params['weight_decay'],\n",
    "        lr_scheduler_type = hyper_params['lr_scheduler_type'],\n",
    "        lr_scheduler_kwargs = hyper_params['lr_scheduler_kwargs'],\n",
    "        seed = hyper_params['seed'],\n",
    "        output_dir = \"outputs\",\n",
    "        # fp16_full_eval = True,\n",
    "        # per_device_eval_batch_size = 1,\n",
    "        # eval_accumulation_steps = 1,\n",
    "        # evaluation_strategy = \"steps\", # epoch\n",
    "        # eval_steps = 100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.932600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.592600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.551500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.734800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.393400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.724900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.841700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.949300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.731900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.727100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model and loss history\n",
    "# get current datetime\n",
    "now = datetime.now()\n",
    "\n",
    "# name the model\n",
    "model_name = \"e_d2t_model_\" + now.strftime(\"%m%d%Y_%H%M%S\")\n",
    "model_path = \"outputs/\" + model_name\n",
    "model.save_pretrained(model_path) # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "\n",
    "# save hyperparameters as a json dict to model_path\n",
    "with open(model_path + \"/hyperparameters.json\", \"w\") as file:\n",
    "    json.dump(hyper_params, file, indent=4)\n",
    "        \n",
    "# save trainer.state.log_history to model_path\n",
    "with open(model_path + \"/trainer_state_log_history.json\", \"w\") as file:\n",
    "    json.dump(trainer.state.log_history, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training results\n",
    "title_to_desc_task_prompt_template = \"Here is the title of a movie: ```{0}```\\n Please write a description of the movie.\"\n",
    "desc_to_title_task_prompt_template = \"Here is a description of a movie: ```{0}```\\n Please write the title of the movie.\"\n",
    "    \n",
    "test_input = title_to_desc_task_prompt_template.format(\"Seven Samurai\")\n",
    "# test_input = desc_to_title_task_prompt_template.format(\"Neo is a engineer, but he is also a hacker. One day, he meets Trinity, a beautiful stranger who leads him into the real world.\")\n",
    "# test_input = desc_to_title_task_prompt_template.format(\"A samurai answers a village's request for protection after he falls on hard times. The town needs protection from bandits, so the samurai gathers six others to help him teach the people how to defend themselves, and the villagers provide the soldiers with food. A giant battle occurs when 40 bandits attack the village.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096 # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"outputs/model_04242024_090830/\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    # model_name = \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    # model_name = \"outputs/checkpoint-1000\",\n",
    "    model_name = \"outputs/e_t2d_model_05152024_002328\",\n",
    "    # model_name = \"outputs/e_d2t_model_05152024_032245\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "        \n",
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt_template.format(test_input, \"\"),    \n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# num_beams_parameter = 5\n",
    "# custom_generation_config = GenerationConfig(\n",
    "#     bos_token_id=tokenizer.bos_token_id,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     asin_dict=None,\n",
    "#     tokenizer=tokenizer,\n",
    "#     return_dict_in_generate=True,\n",
    "#     output_scores=True,\n",
    "#     # output_logits=True,\n",
    "#     do_sample=True,\n",
    "#     early_stopping=True,\n",
    "#     num_beams=num_beams_parameter, \n",
    "#     num_return_sequences=num_beams_parameter,\n",
    "#     max_new_tokens=1024,\n",
    "#     use_cache=True,\n",
    "#     temperature=1,\n",
    "#     # num_beam_groups=5, # In this generation mode, `num_beams` should be divisible by `num_beam_groups`. `diversity_penalty` is not 0.0 or `num_beam_groups` is not 1, triggering group beam search. \n",
    "#     # diversity_penalty=0.9, # `diversity_penalty` should be greater than `0.0`, otherwise your groups will be identical.\n",
    "#     output_hidden_states=True,\n",
    "# )\n",
    "\n",
    "# outputs = model.generate(**inputs, generation_config=custom_generation_config)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True, return_dict_in_generate=True, output_hidden_states=True,)\n",
    "# check CUDA memory usage\n",
    "used_memory = round(torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024, 3)\n",
    "# if used_memory close to 14GB, empty the cache\n",
    "if used_memory > 14.3:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA memory usage is high. Cleared the cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  user\n",
      "Here is the title of a movie: ```Seven Samurai```\n",
      " Please write a description of the movie.\n",
      "model\n",
      "Product DescriptionSeven Samurai (DVD) (WS)Academy Award-winning director Akira Kurosawa (Rashomon, The Hidden Fortress) brings to life the epic tale of a band of 100 samurai who, in the wake of the Civil War, are hired by a wealthy landowner to defend his land and his people. The film is a sweeping epic of honor, loyalty and love, and a powerful story of the human spirit in the face of great adversity. The film stars Toshiro Mifune as the samurai leader, and is a masterpiece of samurai cinema.]]>Amazon.comSeven Samuraiis a classic samurai epic, a sweeping, epic tale of honor, loyalty, and love, and a powerful story of the human spirit in the face of great adversity. The film stars Toshiro Mifune as the samurai leader, and is a masterpiece of samurai cinema. The film is set in the aftermath of the Civil War, and the samurai are hired by a wealthy landowner to defend his land and his people. The film is a sweeping epic of honor, loyalty, and love, and a powerful story of the human spirit in the face of great adversity. The film is also a masterpiece of samurai cinema, and Mifune's performance is one of the most powerful in film history. The film is a masterpiece of samurai cinema, and Mifune's performance is one of the most powerful in film history. The film is also a masterpiece of the samurai genre, and Kurosawa's direction is masterful. The film is a masterpiece of samurai cinema, and Mifune's performance is one of the most powerful in film history. The film is also a masterpiece of the samurai genre, and Kurosawa's direction is masterful. The film is a masterpiece of samurai cinema, and Mifune's performance is one of the most powerful in film history. The film is also a masterpiece of the samurai genre, and Kurosawa's direction is masterful. The film is a masterpiece of samurai cinema, and Mifune's performance is one of the most powerful in film history. The film is also a masterpiece of the samurai genre, and Kurosawa's direction is masterful. The film is a masterpiece of samurai cinema, and Mifune's performance is one of the most powerful in film history. The film is also a masterpiece of the samurai genre, and Kurosawa's direction is masterful. The film is a masterpiece of samurai cinema, and Mifune's performance is one of the most powerful in film history\n",
      "*************************************************\n"
     ]
    }
   ],
   "source": [
    "# for i in range(num_beams_parameter):\n",
    "#     # print(outputs['sequences'][i])\n",
    "#     print(\"Generated text: \", \"\".join(tokenizer.decode(outputs['sequences'][i], skip_special_tokens=True)))\n",
    "#     print(\"*************************************************\")\n",
    "\n",
    "print(\"Generated text: \", \"\".join(tokenizer.decode(outputs['sequences'][0], skip_special_tokens=True)))\n",
    "print(\"*************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length:  30\n",
      "full length:  542\n",
      "newly generated token sequence length:  512\n"
     ]
    }
   ],
   "source": [
    "print(\"input length: \", len(inputs[0]))\n",
    "print(\"full length: \", len(outputs['sequences'][0]))\n",
    "print(\"newly generated token sequence length: \", len(outputs['sequences'][0]) - len(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer embeddings' shape:  torch.Size([1, 30, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0771, -0.1932, -0.1176,  ..., -0.8994, -0.8442, -0.6426]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " torch.Size([1, 2048]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden states\n",
    "# first [0] means the prefix embeddings.\n",
    "# second [-1] means take the last layer embeddings.\n",
    "print(\"last layer embeddings' shape: \", outputs.hidden_states[0][-1].shape) # embeddings, with the length of prefix\n",
    "# take average of the embeddings\n",
    "averaged_emb = outputs.hidden_states[0][-1].mean(dim=1)\n",
    "averaged_emb, averaged_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generating ground-truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the groud-truth movies\n",
    "import json\n",
    "\n",
    "with open('./data/gemma_chat_training_wo_description_fixed_empty_string_filter.json', 'r') as file:\n",
    "    gemma_train_wo_description = json.load(file)\n",
    "len(gemma_train_wo_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predicting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 construct training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def asin_dict_to_t2d_dict(original_dictionary):\n",
    "#     new_dictionary = {}\n",
    "#     for key, movie in asin_dict.items():\n",
    "#         new_dictionary[movie[0]] = ' '.join(movie[1])\n",
    "#     return new_dictionary\n",
    "\n",
    "# title_to_description_dict = asin_dict_to_t2d_dict(asin_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate the embeddings of movies using the t2d model\n",
    "# def get_embedding_from_t2d_by_title(title):\n",
    "    \n",
    "#     title_to_desc_task_prompt_template = \"Here is the title of a movie: ```{0}```\\n Please write a description of the movie.\"\n",
    "#     # generate input\n",
    "#     inputs = tokenizer(\n",
    "#     [\n",
    "#         prompt_template.format(title_to_desc_task_prompt_template, title),    \n",
    "#     ], return_tensors = \"pt\").to(\"cuda\")\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True, return_dict_in_generate=True, output_hidden_states=True,)\n",
    "    \n",
    "#     # get the averaged output vector of the prefix(prompt)\n",
    "#     embedding = outputs.hidden_states[0][-1].mean(dim=1)\n",
    "#     return embedding\n",
    "\n",
    "\n",
    "# def get_embedding_from_d2t_by_title(title):\n",
    "#     description = title_to_description_dict[title]\n",
    "#     desc_to_title_task_prompt_template = \"Here is a description of a movie: ```{0}```\\n Please write the title of the movie.\"\n",
    "#     # generate input\n",
    "#     inputs = tokenizer(\n",
    "#     [\n",
    "#         desc_to_title_task_prompt_template.format(title_to_desc_task_prompt_template, description),    \n",
    "#     ], return_tensors = \"pt\").to(\"cuda\")\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True, return_dict_in_generate=True, output_hidden_states=True,)\n",
    "    \n",
    "#     # get the averaged output vector of the prefix(prompt)\n",
    "#     embedding = outputs.hidden_states[0][-1].mean(dim=1)\n",
    "#     return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a list of five titles to test\n",
    "# test_title_list = gemma_train_wo_description['output'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# for title in test_title_list:\n",
    "#     embedding_from_t2d = get_embedding_from_t2d_by_title(title)\n",
    "#     embedding_from_d2t = get_embedding_from_d2t_by_title(title)\n",
    "#     print(\"t2d embedding shape:\", embedding_from_t2d.shape)\n",
    "#     print(\"d2t embedding shape:\", embedding_from_d2t.shape)\n",
    "#     print(embedding_from_t2d)\n",
    "#     print(embedding_from_d2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store the movie titles and tensors as key-value pairs\n",
    "# import h5py\n",
    "\n",
    "# def save_tensors(tensors, file_name):\n",
    "#     with h5py.File(file_name, 'w') as h5f:\n",
    "#         for key, tensor in tensors.items():\n",
    "#             h5f.create_dataset(key, data=tensor.numpy())  # Convert to numpy before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# title_t2d_embedding_dict = {}\n",
    "# title_d2t_embedding_dict = {}\n",
    "# for title in tqdm(gemma_train_wo_description['output']):\n",
    "#     embedding_from_t2d = get_embedding_from_t2d_by_title(title)\n",
    "#     embedding_from_d2t = get_embedding_from_d2t_by_title(title)\n",
    "#     title_t2d_embedding_dict[title] = embedding_from_t2d\n",
    "#     title_d2t_embedding_dict[title] = embedding_from_d2t\n",
    "\n",
    "# title_t2d_embedding_file = \"data/title_t2d_hidden_state_output.hdf5\"\n",
    "# title_d2t_embedding_file = \"data/title_d2t_hidden_state_output.hdf5\"\n",
    "\n",
    "\n",
    "# save_tensors(title_t2d_embedding_dict, title_t2d_embedding_file)\n",
    "# save_tensors(title_d2t_embedding_dict, title_d2t_embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6046\n"
     ]
    }
   ],
   "source": [
    "# load the embeddings from title_sbert_encoding.json\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Load from JSON file\n",
    "with open('data/title_sbert_encoding_wo_category.json', 'r') as file:\n",
    "    loaded_data = json.load(file)\n",
    "# Convert lists back to tensors\n",
    "data = {key: torch.tensor(value) for key, value in loaded_data.items()}\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/gemma_chat_training_wo_description_fixed_empty_string_filter.json', 'r') as file:\n",
    "#     training_data = json.load(file)\n",
    "# with open('data/gemma_chat_training_w_description_fixed_empty_string_filter.json', 'r') as file:\n",
    "#     training_data = json.load(file)\n",
    "    \n",
    "# read huggingface dataset from local\n",
    "from datasets import load_from_disk\n",
    "\n",
    "training_data = load_from_disk(\"./data/gemma_chat_train_fixed_empty_string_filter\")\n",
    "eval_data = load_from_disk('./data/gemma_chat_eval_fixed_empty_string_filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Below is the previous historical purchases and reviews of the user:\\n```\\nItem title: Hotel Transylvania: 3-Movie DVD Collection \\n Item description: Disc 1 - Hotel Transylvania & Hotel Transylvania 2 Disc 2 - Hotel Transylvania 3 Hotel Transylvania Welcome to Hotel Transylvania, Dracula’s lavish five-stake resort, where monsters and their families can live it up and no humans are allowed. Hotel Transylvania 2 The Drac-Pack is back in this hilarious comedy starring Drac, Mavis, Johnny and your favourite monsters in an all-new adventure! Hotel Transylvania 3 Join Drac, Mavis and all of your favourite monsters for fun and adventure on the high seas as they embark on a monster cruise vacation! \\n rating: 5.0 \\n review: Movies 1 & 2 are on the same disc. You’ll get 2 disks for the 3 movies. Our digital code worked for movie 3. Bummed it wasn’t included for all 3, but I wasn’t expecting one for the 3rd movie anyway. So that was kind of a bonus.-------\\n```\\nAnd here is the user's intention: I love animated movies with fun adventures, so I'm looking for something similar to my previous picks.\\nPlease infer the user's preference based on historical purchases and reviews along with the user's intention, and then recommend an item for this user. Please just give the title of the recommended item.\",\n",
       " 'Kung Fu Panda: 3-Movie Collection [DVD]')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = 0\n",
    "training_data['input'][test_index], training_data['output'][test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14507 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14507/14507 [23:57<00:00, 10.09it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 1006.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# ABORT: not including categories in the training data\n",
    "# new_training_data = {\"input\": [], \"output\": []}\n",
    "# for i in range(len(training_data['input'])):\n",
    "#     new_input = training_data['input'][i] \n",
    "#     new_output = training_data['output'][i]\n",
    "    \n",
    "#     new_input = \".\".join(new_input.split(\".\")[:-2]) + \". Please give the title, description and categories of the recommended item.\"\n",
    "#     new_output = data[new_output]\n",
    "    \n",
    "#     new_training_data['input'].append(new_input)\n",
    "#     new_training_data['output'].append(new_output)\n",
    "\n",
    "new_training_data = {\"input\": [], \"output\": []}\n",
    "for i in tqdm(range(len(training_data['input']))):\n",
    "    new_input = training_data['input'][i] \n",
    "    if training_data['output'][i].startswith(\"Title: \"):\n",
    "        new_output = training_data['output'][i].split(\"\\n\")[0].split(\"Title: \")[1].strip()\n",
    "    else:\n",
    "        new_output = training_data['output'][i]\n",
    "    \n",
    "    new_output = data[new_output]\n",
    "    \n",
    "    new_training_data['input'].append(new_input)\n",
    "    new_training_data['output'].append(new_output)\n",
    "\n",
    "new_eval_data = {\"input\": [], \"output\": []}\n",
    "for i in tqdm(range(len(eval_data['input']))):\n",
    "    new_input = eval_data['input'][i] \n",
    "    if eval_data['output'][i].startswith(\"Title: \"):\n",
    "        new_output = eval_data['output'][i].split(\"\\n\")[0].split(\"Title: \")[1].strip()\n",
    "    else:\n",
    "        new_output = eval_data['output'][i]\n",
    "    \n",
    "    new_output = data[new_output]\n",
    "    \n",
    "    new_eval_data['input'].append(new_input)\n",
    "    new_eval_data['output'].append(new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14507, 14507, 103, 103)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_training_data[\"input\"][0], new_training_data[\"output\"][0]\n",
    "len(new_training_data[\"input\"]), len(new_training_data[\"output\"]), \\\n",
    "len(new_eval_data[\"input\"]), len(new_eval_data[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset_predict_emb_task = Dataset.from_dict(new_training_data)\n",
    "dataset_predict_emb_task_test = Dataset.from_dict(new_eval_data)\n",
    "\n",
    "dataset_predict_emb_task = dataset_predict_emb_task.train_test_split(test_size=0.025)\n",
    "dataset_predict_emb_task_train, dataset_predict_emb_task_eval = dataset_predict_emb_task[\"train\"], dataset_predict_emb_task[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'output'],\n",
       "     num_rows: 14144\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'output'],\n",
       "     num_rows: 363\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'output'],\n",
       "     num_rows: 103\n",
       " }))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_predict_emb_task_train, dataset_predict_emb_task_eval, dataset_predict_emb_task_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee28567cb314cfc940a0f15b8437586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5902d46ab3904343a43febbd41377723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/363 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad75bd38d5245ba93216da5ddd4964a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_predict_emb_task_train.save_to_disk('./data/gemma_chat_train_predict_emb_task_fixed_empty_string_filter')\n",
    "dataset_predict_emb_task_eval.save_to_disk('./data/gemma_chat_eval_predict_emb_task_fixed_empty_string_filter')\n",
    "dataset_predict_emb_task_test.save_to_disk('./data/gemma_chat_test_predict_emb_task_fixed_empty_string_filter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Distance based loss function - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read huggingface dataset from local\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_train = load_from_disk(\"./data/gemma_chat_train_predict_emb_task_fixed_empty_string_filter\")\n",
    "dataset_eval = load_from_disk(\"./data/gemma_chat_eval_predict_emb_task_fixed_empty_string_filter\")\n",
    "dataset_test = load_from_disk(\"./data/gemma_chat_test_predict_emb_task_fixed_empty_string_filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.select(range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is the previous historical purchases and reviews of the user:\\n```\\nItem title: The Big Bang Theory: Season 6 \\n Item description: Big Bang Theory, The: The Complete Sixth Season (DVD) The CalTech geniuses of The Big Bang Theory return for season six after being thrown for a bell curve in the phenomenally funny fifth season of the hit show. In “TV’s top-rated comedy” (New York Post), Leonard has to “debug” his relationship with Penny once more after accidentally proposing to her in the heat of the moment, while Sheldon, Howard, and Raj continue to discover the feminine mystique is something that cannot be easily graphed or calculated. With even greater personal contact with his not girlfriend but “friend-who-happens-to-be-a-girl” Amy Farrah Fowler, Sheldon might have to make even more begrudging amendments to his “Relationship Agreement” while Raj continues to look for a girl as nice as Siri, and Howard gets ready for splashdown in both his NASA space travels and wedding to the spirited Bernadette. ]]> \\n rating: 5.0 \\n review: love the nerds-------\\nItem title: Charlie Wilson\\'s War (Widescreen Edition) \\n Item description: Product Description Academy Award® winners Tom Hanks, Julia Roberts and Philip Seymour Hoffman star in this compelling and witty film from Oscar®-winning director Mike Nichols and Primetime Emmy®-winning writer Aaron Sorkin (The West Wing). Based on the outrageous true story, Charlie Wilson\\'s War shows how one congressman who loved a good time, one Houston socialite who loved a good cause and one renegade CIA agent who loved a good fight conspired to bring about the largest covert operation in history. Bonus Content: The Making of Charlie Wilson\\'s War The Making of Charlie Wilson\\'s War Who Is Charlie Wilson? Who Is Charlie Wilson? ]]> Amazon.com Political movies about backroom negotiations need not be dry or heavy-handed, as Charlie Wilson\\'s War delightfully proves. Based on the true story of playboy congressman Wilson\\'s efforts to fund Afghanistan\\'s defense against the Soviet invasion of the 1980s, the film is borne along on breezy attitude and a peppery script by West Wing scribe Aaron Sorkin. Wilson, played by Tom Hanks (who also produced), is the perfect hero for this kind of tale, because there\\'s nothing perfect or heroic about him: He\\'s a highball-swilling, fanny-pinching gadabout who becomes radicalized on the issue of helping the Afghans against their mighty aggressor. He has help in the form of a right-wing Texas anti-Communist (Julia Roberts) with a genius for raising money, and a sardonic CIA operative (Philip Seymour Hoffman, stealing the show) who lacks all the social skills Wilson has in abundance. Sorkin\\'s syncopated speech is just the ticket for director Mike Nichols, who understands exactly how to keep this kind of political comedy popping (the complicated story comes in at a hair over 90 minutes, amazingly). Some scoundrels are on the right side of the angels, and the movie\\'s Charlie Wilson is one of them. -- Robert Horton Beyond Charlie Wilson\\'s War on DVD More Tom Hanks Films by Mike Nichols More Julia Roberts Stills from Charlie Wilson\\'s War (Click for larger image) \\n rating: 5.0 \\n review: Well my title said most of it and how we turn on our allies when it is convenient for corporate America.<br />Almost the same in Viet Nam and the Viet Min and later called the Viet Cong we trained for warfare in the 1940\\'s and fought in the 1950\\'s to the 1970\\'s.-------\\nItem title: Tora! Tora! Tora! (Two-Disc Collector\\'s Edition) \\n Item description: Martin Balsam, Jason Robards, Jr.. As Japanese fighter pilots waited for Tora! Tora! Tora!\" from their commander, Pearl Harbor was seemingly peaceful and calm! But the truth behind the tragic day that forced America into WWII was anything but peaceful. 2 DVDs. 1970/color/145 min/G/widescreen. \\n rating: 5.0 \\n review: Saw one of the &#34;Japanese&#34; carriers in San Diego tied up when on the Kitty Hawk in 1968-1969.  Love the movie...........-------\\nItem title: TCM Greatest Classic Film Collection: War (Battle of the Bulge / The Dawn Patrol / Gunga Din / Operation Pacific) \\n Item description: BATTLE OF THE BULGE This epic recreation of one of World War II\\'s pivotal clashes captures the explosive action of massive forces squaring off as well as the brave ingenuity of weary GIs trying to survive a cruel European winter. Ken Annakin (The Longest Day) directs an all-star juggernaut: Henry Fonda, Robert Shaw, Robert Ryan and more. THE DAWN PATROL Errol Flynn and David Niven take to the skies in this thrilling aerial action yarn as World War I British flyboys who unite in devil-may-care gallantry and in disdain for their commander (Basil Rathbone). But war\\'s realities will soon tarnish their bonhomie and change their disdain to understanding. GUNGA DIN Cary Grant, Victor McLaglen and Douglas Fairbanks Jr. star as cheeky soldiers of Queen and Empire armed with battlefield gallantry and chin-up heroics as they combat a murderous sect in colonial India. Director George Stevens orchestrates teeming battles, boisterous humor – and an indelible title-role performance by Sam Jaffe. OPERATION PACIFIC Meet the dive-for-glory torpedo devils of the USS Thunderfish as John Wayne commands a desperate World War II submarine mission deep into enemy territory. Patricia Neal and Ward Bond co-star. \\n rating: 5.0 \\n review: Some of the best timeless movies of men in combat all masterpieces and came in great condition-------\\nItem title: We Were Soldiers \\n Item description: Mel Gibson and Randall Wallace, the star and writer of Braveheart, reunite for this action-packed war movie that features explosive battle sequences, thrilling aerial photography and unforgettable military heroes who fought for their country, their loved ones and their freedom. The year is 1965 and America is at War with North Vietnam. Commanding the air cavalry is Lt. Col. Hal Moore (Gibson), a born leader committed to his troops. His target: the La Drang Valley, called \"The Valley of Death.\" As Moore prepares for one of the most violent battles in U.S. history, he delivers a stirring promise to his soldiers and their families: \"I will leave no man behind... dead or alive. We will all come home together.\" This heroic true story of commitment, courage and sacrifice also stars Madeline Stowe, Greg Kinnear, Sam Elliott, Chris Klein, Keri Russell and Barry Pepper. (English), The year is 1965 and America is at War with North Vietnam. Commanding the air cavalry is Lt. Col. Hal Moore (Gibson), a born leader committed to his troops. His target: the La Drang Valley, called \"The Valley of Death.\" As Moore prepares for one of the most violent battles in U.S. history, he delivers a stirring promise to his soldiers and their families: \"I will leave no man behind... dead or alive. We will all come home together.\" (Spanish) \\n rating: 5.0 \\n review: Have watched several times and is still interesting as you observe different actors.-------\\nItem title: Leap of Faith by Paramount \\n Item description:  \\n rating: 5.0 \\n review: Movie came in like new condition and played well.  This has to be one of Steve Martins best roles and movies.-------\\nItem title: Patton (Cinema Classics Collection) \\n Item description: Product Description Flamboyant Gen. George S. Patton receives accolades and censure as he fights World War II. Oscars for best picture, director Franklin J. Schaffner, actor Scott. Set Contains: Released in 2006 as part of Fox\\'s Cinema Classics Collection, this deluxe two-disc set of Patton is a worthy replacement for all previous DVD releases of Franklin J. Schaffner\\'s Oscar®-winning film. All of the bonus features from Fox\\'s previous DVD release are included here: Patton is presented with superior image and sound quality (it was one of only two features shot in the \"Dimension 150\" 70-millimeter format; the anamorphic 2.35:1 aspect ratio of previous DVDs has now been corrected to 2.20:1), and the 50-minute documentary \"The Making of Patton : A Tribute to Franklin J. Schaffner\" remains a thorough examination of the film\\'s production, including abundant behind-the-scenes footage, camera tests, and 1997 interviews with producer Frank McCarthy, composer Jerry Goldsmith, cinematographer Fred Koenkamp, Fox executive Richard Zanuck, and others including Oliver Stone, who makes the controversial assertion that several viewings of Patton led President Richard Nixon\\'s decision to bomb Cambodia during the Vietnam War (in turn leading to the genocidal rise of the Khmer Rouge). The combination of archival footage and interviews results in a concise examination of Schaffner\\'s career as a much-admired \"gentleman\\'s gentleman,\" and the film (along with Planet of the Apes ) that he\\'ll best be remembered for. The new features are even better. On Disc 1, Patton cowriter Francis Ford Coppola provides an interesting introduction, explaining how (as a military school dropout in his mid-20s) he was assigned to write the film, feeling it necessary to satisfy audiences by addressing all aspects of Gen. George S. Patton\\'s volatile and contradictory nature. Coppola\\'s feature-length commentary goes further in explaining his approach to the screenplay, including the now-classic opening speech, which Fox executives originally disliked, leading to Coppola\\'s dismissal and the hiring of cowriter Edmund H. North. The commentary loses momentum in mid-film, but Coppola livens up during the film\\'s final hour and recalls plenty of relevant details about his original screenplay, along with anecdotes about the production and cordial acknowledgements of North\\'s contributions. Disc 2 opens with \"History Through the Lens: Patton--A Rebel Revisited,\" an excellent 90-minute documentary that thoroughly compares the film to the facts of Patton\\'s career, arriving at the conclusion (supported by Patton\\'s own descendents) that the film is a remarkably accurate depiction of Patton\\'s larger-than-life persona. The 46-minute documentary \"Patton\\'s Ghost Corps\" is a riveting, sometimes heartbreaking celebration of the 94th Infantry in Patton\\'s XX Corps, who were abandoned in Germany while Patton pursued glory on other fronts. Many were unnecessarily killed, and in interviews taped in 2005, surviving members of the 94th understandably hold Patton responsible while expressing complex feelings (praise, damnation, and everything in between) for Patton\\'s brand of leadership. It\\'s obvious that many of these brave men are still haunted by their battlefield memories. Disc 2 is rounded out by two photo galleries: The production gallery is accompanied by Jerry Goldsmith\\'s complete Oscar®-nominated score, and the behind-the-scenes gallery is accompanied by a 53-minute audio essay (carried over from the previous DVD release) in which Patton scholar Charles M. Province thoroughly explores the film\\'s historical accuracy, along with authoritative biographical details about Patton\\'s life and military career. Both educational and entertaining, all of these features make this Cinema Classics edition of Patton an essential edition to anyone\\'s war-movie collection. --Jeff Shannon \\n rating: 5.0 \\n review: Can remember first time on the big screen.  Still great timeless.............-------\\nItem title: The Big Bang Theory: Season 9 [DVD] \\n Item description: The Big Bang Theory: The Complete Ninth Season (DVD) Prepare to enter phase nine of television’s perpetual laughter experiment as THE BIG BANG THEORY successfully fuses the elements of science and sitcom. Our two genius roommates – Leonard and Sheldon ¬– and their friends are back once again (smarter, but no wiser). Last season, Sheldon went soul-searching (on a train, of course) and was prepared to make some substantial revisions on his Relationship Agreement with Amy, when everything changed. Leonard, meanwhile, was off to Vegas with Penny to finally tie the knot in the season finale. Howard finds himself alone with Bernadette after the sudden passing of his mother; while Raj is not only talking to women – he’s getting exclusive with Emily (Laura Spencer). Together, they will all learn that life is far more complicated outside of the lab as love and friendship never produce predictable results! ]]> \\n rating: 5.0 \\n review: Love the nerds........-------\\n```\\nAnd here is the user\\'s intention: Hey there, I love movies with action, adventure, and a bit of mystery, so I\\'m always on the lookout for something new in that genre.\\nPlease infer the user\\'s preference based on historical purchases and reviews along with the user\\'s intention, and then recommend an item for this user. Please provide the recommended item\\'s title and description.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][\"input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "# tokenize the input from string to tensor\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "_, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2b-it-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"input\", return_tensors=\"pt\", padding=\"max_length\")[\"input_ids\"][:, :4096].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tokenized dataset\n",
    "tokenized_train_dataset = {\"input\": [], \"output\": []}\n",
    "tokenized_eval_dataset = {\"input\": [], \"output\": []}\n",
    "tokenized_test_dataset = {\"input\": [], \"output\": []}\n",
    "\n",
    "# iterate through the dataset and tokenize the input\n",
    "for i in range(len(dataset_train)):\n",
    "    tokenized_train_dataset[\"input\"].append(tokenizer(dataset_train[i][\"input\"], return_tensors=\"pt\")['input_ids'][:, :4096])\n",
    "    tokenized_train_dataset[\"output\"].append(dataset_train[i][\"output\"])\n",
    "    \n",
    "for i in range(len(dataset_eval)):\n",
    "    tokenized_eval_dataset[\"input\"].append(tokenizer(dataset_eval[i][\"input\"], return_tensors=\"pt\")['input_ids'][:, :4096])\n",
    "    tokenized_eval_dataset[\"output\"].append(dataset_eval[i][\"output\"])\n",
    "\n",
    "for i in range(len(dataset_test)):\n",
    "    tokenized_test_dataset[\"input\"].append(tokenizer(dataset_test[i][\"input\"], return_tensors=\"pt\")['input_ids'][:, :4096])\n",
    "    tokenized_test_dataset[\"output\"].append(dataset_test[i][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenized_train_dataset = Dataset.from_dict(tokenized_train_dataset)\n",
    "tokenized_eval_dataset = Dataset.from_dict(tokenized_eval_dataset)\n",
    "tokenized_test_dataset = Dataset.from_dict(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97c4661f3ba4b6cbbf376312f0eaa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9c0b22815c4f3796241061f3b93e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/363 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb0238a07f24d66b3fd86a21d4e435d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the tokenized dataset\n",
    "tokenized_train_dataset.save_to_disk('./data/gemma_chat_train_predict_emb_task_fixed_empty_string_filter_tokenized')\n",
    "tokenized_eval_dataset.save_to_disk('./data/gemma_chat_eval_predict_emb_task_fixed_empty_string_filter_tokenized')\n",
    "tokenized_test_dataset.save_to_disk('./data/gemma_chat_test_predict_emb_task_fixed_empty_string_filter_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read huggingface dataset from local\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# read the tokenized data\n",
    "tokenized_train_dataset = load_from_disk('./data/gemma_chat_train_predict_emb_task_fixed_empty_string_filter_tokenized')\n",
    "tokenized_eval_dataset = load_from_disk('./data/gemma_chat_eval_predict_emb_task_fixed_empty_string_filter_tokenized')\n",
    "tokenized_test_dataset = load_from_disk('./data/gemma_chat_test_predict_emb_task_fixed_empty_string_filter_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_train_dataset[0][\"input\"]),\\\n",
    "type(tokenized_train_dataset[0][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4096]),\n",
       " tensor([[    2, 33501,   603,  ...,     0,     0,     0]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_to_be_padded = 4096 - len(tokenized_test_dataset[0][\"input\"][0])\n",
    "# len_to_be_padded\n",
    "# add len_to_be_padded 0 to the end of tokenized_test_dataset[0][\"input\"][0]\n",
    "import torch\n",
    "from torch import zeros\n",
    "\n",
    "tensor = torch.tensor(tokenized_test_dataset[0][\"input\"])\n",
    "# pad to the last dimension\n",
    "padded_tensor = torch.nn.functional.pad(tensor, (0, len_to_be_padded), \"constant\", 0)\n",
    "padded_tensor.shape, padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837f5e31392c4c5e8a9bf043b634a1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m      8\u001b[0m hyper_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Model hyperparameters\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_seq_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4096\u001b[39m, \u001b[38;5;66;03m# 8192 | Choose any! We auto support RoPE Scaling internally!\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3407\u001b[39m,\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# load model and tokenizer\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m model, tokenizer\u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_seq_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_in_4bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/loader.py:138\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# In case the model supports tagging, add the unsloth tag.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_model_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/llama.py:1087\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1085\u001b[0m max_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_seq_length, model_max_seq_length)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1087\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_scaling\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py:3677\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3669\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3670\u001b[0m     (\n\u001b[1;32m   3671\u001b[0m         model,\n\u001b[1;32m   3672\u001b[0m         missing_keys,\n\u001b[1;32m   3673\u001b[0m         unexpected_keys,\n\u001b[1;32m   3674\u001b[0m         mismatched_keys,\n\u001b[1;32m   3675\u001b[0m         offload_index,\n\u001b[1;32m   3676\u001b[0m         error_msgs,\n\u001b[0;32m-> 3677\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3696\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py:4104\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4100\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4101\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4102\u001b[0m                 )\n\u001b[1;32m   4103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4104\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4112\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4119\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4121\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py:888\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    886\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:219\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    216\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    218\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 219\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:324\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_quantized:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/unsloth_env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:288\u001b[0m, in \u001b[0;36mParams4bit._quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[0;32m--> 288\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     w_4bit, quant_state \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mquantize_4bit(\n\u001b[1;32m    290\u001b[0m         w,\n\u001b[1;32m    291\u001b[0m         blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m         quant_storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_storage,\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "hyper_params = {\n",
    "    # Model hyperparameters\n",
    "    \"max_seq_length\": 4096, # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "    \"dtype\": None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    \"load_in_4bit\": True, # Use 4bit quantization to reduce memory usage. Can be False.,\n",
    "    # \"model_name\": \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    \"model_name\": \"outputs/model_05152024_105402_merged_16bit\",\n",
    "    \"r\": 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # Add more to target more modules\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0, # Supports any, but = 0 is optimized\n",
    "    \"lora_bias\": \"none\", # Supports any, but = \"none\" is optimized\n",
    "    \"lora_use_gradient_checkpointing\": \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    \"lora_random_state\": 3407,\n",
    "    \"lora_use_rslora\": False, # We support rank stabilized LoRA\n",
    "    \"lora_loftq_config\": None, # And LoftQ\n",
    "    # Training hyperparameters\n",
    "    \"dataset_train_path\": \"./data/gemma_chat_train_no_user_intention_fixed_empty_string_filter\",\n",
    "    \"dataset_eval_path\": \"./data/gemma_chat_eval_no_user_intention_fixed_empty_string_filter\",\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_steps\": 25, # will replace num_warmup_steps in lr_scheduler_kwargs\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\": torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\": 1,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"cosine_with_restarts\",\n",
    "    \"lr_scheduler_kwargs\": {\"num_cycles\": 3}, # \"num_warmup_steps\" and \"num_training_steps\" will be added automatically\n",
    "    \"seed\": 3407,\n",
    "}\n",
    "\n",
    "# load model and tokenizer\n",
    "model, tokenizer= FastLanguageModel.from_pretrained(\n",
    "    model_name = hyper_params[\"model_name\"], # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = hyper_params[\"max_seq_length\"],\n",
    "    dtype = hyper_params[\"dtype\"],\n",
    "    load_in_4bit = hyper_params[\"load_in_4bit\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
      "We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n",
      "To force `safe_serialization`, set it to `None` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 5.31 out of 12.68 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Unsloth: Saving outputs/model_05152024_105402_merged_16bit/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving outputs/model_05152024_105402_merged_16bit/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained_merged(hyper_params[\"model_name\"] + \"_merged_16bit\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaFixedRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# # show the module architecture of the model\n",
    "# for name, module in model.named_children():\n",
    "#     print(f\"\\t{name}\")\n",
    "#     if isinstance(module, torch.nn.Module):\n",
    "#         print(f\"\\t\\t{module}\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=2048, out_features=256000, bias=False),\n",
       " torch.Size([256000, 2048]),\n",
       " Parameter containing:\n",
       " tensor([[ 5.2344e-01, -3.5889e-02,  5.9814e-02,  ...,  7.7637e-02,\n",
       "           2.3535e-01,  3.8330e-02],\n",
       "         [ 1.5137e-01, -1.4453e-01, -1.1719e-01,  ..., -1.9409e-02,\n",
       "           4.9133e-03, -2.0508e-02],\n",
       "         [ 1.0352e-01,  5.0354e-03, -3.2715e-02,  ..., -1.7334e-02,\n",
       "          -8.9111e-03, -1.0254e-02],\n",
       "         ...,\n",
       "         [ 2.7344e-01,  1.2390e-02,  4.2236e-02,  ..., -4.8584e-02,\n",
       "           1.9165e-02, -3.0151e-02],\n",
       "         [ 2.9102e-01, -6.4453e-02,  6.2012e-02,  ..., -1.9653e-02,\n",
       "           7.1289e-02, -1.6689e-04],\n",
       "         [ 5.2344e-01, -3.5156e-02,  6.0791e-02,  ...,  7.6172e-02,\n",
       "           2.3828e-01,  3.9795e-02]], device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head, model.lm_head.weight.shape, model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-19.2300,  30.1120])\n",
      "tensor([-0.0000, 30.1120])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "m = nn.GELU()\n",
    "input = torch.tensor([-19.23, 30.112]) # torch.randn(2)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume `model` is your pre-trained model\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define a custom module with multi-head attention\n",
    "class MultiheadAttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, internal_dim, output_dim, num_heads):\n",
    "        super(MultiheadAttentionLayer, self).__init__()\n",
    "        self.input_linear = nn.Linear(embed_dim, internal_dim)\n",
    "        self.gelu_1 = nn.GELU()\n",
    "        self.multihead_attn = nn.MultiheadAttention(internal_dim, num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(internal_dim)\n",
    "        # self.dropout = nn.Dropout(0.1)\n",
    "        self.output_linear = nn.Linear(internal_dim, output_dim)\n",
    "        # self.gelu = nn.GELU()\n",
    "        # self.out_proj = nn.Linear(internal_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming `x` shape is (seq_len, batch_size, embed_dim)\n",
    "        x = self.input_linear(x)\n",
    "        x = self.gelu_1(x)\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        attn_output = self.layer_norm(attn_output + x)\n",
    "        # attn_output = self.dropout(attn_output)\n",
    "\n",
    "        x = self.output_linear(attn_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Replace the last layer with the new multi-head attention layer\n",
    "embed_dim = 2048\n",
    "internal_dim = 1024\n",
    "output_dim = 768\n",
    "num_heads = 4  # Choose the number of attention heads\n",
    "model.lm_head = MultiheadAttentionLayer(embed_dim, internal_dim, output_dim, num_heads).to(model.device)\n",
    "\n",
    "# Ensure the new layer's parameters are trainable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaFixedRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): MultiheadAttentionLayer(\n",
      "    (input_linear): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (gelu_1): GELU(approximate='none')\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_linear): Linear(in_features=1024, out_features=768, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model.lm_head: 7085824\n"
     ]
    }
   ],
   "source": [
    "# print the number of parameters in model.lm_head\n",
    "num_params = sum(p.numel() for p in model.lm_head.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters in model.lm_head: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1057, -0.2571, -0.6835,  ..., -1.1805,  1.4068,  1.0781]])\n",
      "tensor([[-0.2257, -0.7159, -0.7171,  0.3380,  0.4444, -0.3840,  0.1416,  0.0544,\n",
      "          0.8069, -0.4938,  0.2883, -0.2839, -0.4109, -0.8511, -0.0978,  0.3157,\n",
      "          0.3382, -0.9609, -0.8699,  0.1617, -0.2181, -0.1606, -0.2288, -0.1658,\n",
      "          0.5440,  0.2459, -0.9145, -0.6430, -0.4613,  0.9462,  0.5039, -0.6728,\n",
      "         -0.6619, -0.7505, -0.8355,  0.3612,  2.1938,  0.0500,  0.5779, -0.0472,\n",
      "         -0.0569,  0.3049,  0.5880,  0.1264, -0.1595, -0.2607,  0.9232,  0.3850,\n",
      "         -0.8146, -0.5146, -0.3933,  0.1463, -0.8668,  0.3277, -0.2041, -0.3501,\n",
      "         -0.6403, -0.0488, -0.7264, -0.7082,  0.2314, -1.2016,  0.4688, -0.0143,\n",
      "         -0.5662,  0.7314, -0.1330, -0.4407, -0.1094,  0.7634, -0.7521,  0.3928,\n",
      "         -0.3549,  0.5357,  0.0950, -1.0400,  1.1954, -0.1535, -0.8553, -0.5710,\n",
      "          1.2467, -0.4057, -0.2622,  0.2144, -0.0414, -1.4617,  0.4950, -0.6434,\n",
      "         -0.5937,  1.6970, -0.1899, -0.5188,  0.0471, -0.3217, -0.2310, -0.2148,\n",
      "          0.3131,  0.4611,  0.7411,  0.4632, -0.2983,  0.6026, -1.5798, -0.4181,\n",
      "         -0.3035,  0.3694, -0.0941,  0.5314,  0.8223, -0.0265, -0.8167,  0.6785,\n",
      "         -0.3417,  0.0721,  0.4715, -0.0372,  0.1512,  0.7748,  0.0824, -0.3636,\n",
      "          0.4827, -0.5670,  0.1850,  0.7201, -0.0369, -1.3075,  0.2864,  0.8815,\n",
      "          1.0284,  0.6208,  0.6418,  0.4242, -0.9146,  0.2398, -0.2614,  0.3609,\n",
      "          0.0537,  0.2219, -0.6329,  0.5132, -0.1951, -0.7178,  0.5360,  1.3470,\n",
      "         -0.6456,  0.0914, -0.8594, -0.3677,  0.6496, -1.2202,  0.5546, -0.6888,\n",
      "          0.0315,  1.2129,  0.4072, -0.2290, -0.7490, -0.1304, -0.6187,  0.0441,\n",
      "          0.6402, -0.0188,  0.1024,  0.5245,  0.4798,  0.3971,  0.4757, -0.2869,\n",
      "         -0.4186, -0.4656, -1.0271,  1.1267,  0.0210,  0.0377,  0.1258,  0.2176,\n",
      "          0.0302, -0.3047, -0.2919, -0.5084,  0.1610, -0.2942,  0.4904, -0.6092,\n",
      "          0.3057,  0.2155,  0.4387, -0.7329,  0.0882,  0.7452,  0.4967, -0.5022,\n",
      "         -0.2649,  0.0703,  0.0632,  1.0223,  0.2910,  0.7051, -1.4088, -0.0230,\n",
      "          0.0219, -0.0497,  0.1858, -0.5295, -0.2750,  0.1897,  0.6746, -0.1307,\n",
      "          0.1966, -0.3863, -0.7354,  0.2620,  0.1862,  0.3178,  0.1476, -0.4720,\n",
      "         -1.4637,  0.3506, -0.1058, -0.2350, -0.0474, -0.0034,  0.0716, -0.7431,\n",
      "          0.4834, -0.0163,  0.3677, -2.1133, -0.0025,  0.5411,  0.2028,  1.1653,\n",
      "          0.3135, -0.5760,  0.2176,  0.9068, -0.6288, -0.4028,  0.1958,  0.1436,\n",
      "          0.2357,  0.0315, -0.0054, -1.3195, -0.5488, -0.7409, -0.1740, -0.7522,\n",
      "          0.4365,  1.0951, -0.3895, -0.5122, -0.7795,  0.7406, -0.0691,  0.0460,\n",
      "          1.2864,  0.5946,  0.0138, -0.0704,  0.3197, -0.1057,  0.1403,  0.6148,\n",
      "          0.3936,  0.2402,  1.2799, -0.9167,  0.6841,  0.0699,  0.8750,  0.4569,\n",
      "          1.0234, -0.1109,  0.0077, -0.0776, -0.1744, -0.6437, -0.6699,  0.2083,\n",
      "          0.5628,  0.0219, -0.5791,  0.5238, -0.5624, -0.3785,  1.0288,  0.3456,\n",
      "          0.1184, -0.4287, -0.4213,  0.5296, -0.2185, -0.3683,  0.3818, -0.2256,\n",
      "          0.4891,  0.0250,  0.0769,  0.1011, -0.2451,  0.1776,  0.3664, -0.7497,\n",
      "          0.5082,  1.1798, -0.1234,  0.6717,  0.1995,  0.0888, -0.5728,  0.5024,\n",
      "          0.4766, -0.7596, -0.2189, -0.8094,  0.9321,  0.7131,  0.1937,  0.5608,\n",
      "         -0.1492,  0.1610,  0.4025,  0.5777, -0.0881, -0.0208,  0.2220,  0.2025,\n",
      "          1.0144, -0.5999, -0.4072, -0.2220,  0.2965,  0.2124,  0.0451, -0.1629,\n",
      "         -0.8229, -0.5391, -0.1935, -0.0430,  0.1278, -0.4705, -0.1628,  0.4467,\n",
      "         -1.0056, -0.4873,  0.4498, -0.1140, -1.3351,  0.7148,  0.1225,  0.6981,\n",
      "          0.0326,  0.0672,  1.1398,  0.2448,  0.1907, -0.5026,  0.9525, -0.0944,\n",
      "         -0.1605,  0.2868, -0.5593, -0.2961, -0.1688,  0.1450, -0.3987,  0.1834,\n",
      "         -1.5228,  0.3421, -0.0787, -0.0676, -0.7348, -0.1444,  0.9665,  0.1035,\n",
      "         -0.5668, -0.2577,  0.0432,  1.0803, -0.5028, -0.1123,  0.0971, -0.4596,\n",
      "          0.9089, -0.7391,  0.0061,  0.0079,  0.6442,  0.0346,  0.0600, -0.9299,\n",
      "          0.0571, -0.1006, -1.3683,  0.3360,  0.5770,  1.1719,  0.1853, -0.6048,\n",
      "         -1.0283, -1.1294, -0.4927, -0.5543, -0.1175, -0.0975, -0.2986,  0.4468,\n",
      "          0.9120, -0.3179, -0.4958,  0.5643,  0.9505, -0.3813,  0.4316,  0.7278,\n",
      "          0.5536,  0.5167, -1.6155,  0.6728, -0.4461, -0.9522, -0.1925, -0.1013,\n",
      "         -0.0651,  0.3139,  1.0893, -0.3280,  0.6616, -0.3308,  0.2863,  0.7786,\n",
      "          0.4492,  1.3529,  0.1837,  0.2360,  0.2158,  1.1273,  0.2963,  0.3202,\n",
      "          0.3145,  0.3948,  1.0657,  0.3991, -0.1552, -0.1476, -0.0181,  0.4033,\n",
      "         -0.4629, -0.0664, -0.2505, -0.5211, -0.9511, -0.3494,  0.3997, -0.1454,\n",
      "         -0.3159, -0.5387,  0.3008, -0.3075,  0.5870,  0.7408, -0.6119,  0.3014,\n",
      "         -1.2517, -0.2072,  0.0192,  0.3209, -0.1180,  0.2176, -0.4295, -0.0347,\n",
      "         -0.3148, -0.2636, -0.4062, -0.8322, -0.0874,  1.1131, -0.1752,  0.2810,\n",
      "          0.4227, -0.0623,  0.3081, -0.2287, -0.0943,  1.0351, -0.0549,  0.4444,\n",
      "          0.1858, -0.5100,  0.4750, -0.0461,  0.8220, -0.2201,  0.9487,  0.6155,\n",
      "         -0.4703,  0.8285,  0.8167,  1.7653,  0.3791, -0.5017,  0.6474,  0.3877,\n",
      "         -0.2494, -0.1349, -0.7251,  1.1345, -0.6076, -0.3554,  0.7929, -0.9697,\n",
      "          0.0158, -0.8743, -0.4811,  0.1737, -0.0819,  0.0146,  0.3765,  0.0467,\n",
      "         -0.4312,  0.3372,  0.9657,  0.1978,  0.5319,  0.2775, -0.8100, -0.3388,\n",
      "         -0.7515, -0.3347, -0.7703, -0.2292, -0.3277, -0.3581,  0.5968,  0.1065,\n",
      "          0.4008,  0.7975, -0.5420, -0.7252,  0.7743,  0.5241, -1.0135,  1.1045,\n",
      "         -0.9202, -0.5903,  0.3161,  0.1186, -0.7008,  0.0684, -0.9047,  0.4646,\n",
      "          0.9820,  0.5076,  0.5421, -0.1286,  0.2927,  0.4102, -0.3204, -0.2568,\n",
      "         -1.0136,  0.5026,  0.3347,  0.1073,  0.3331, -0.3958,  0.5541, -0.7446,\n",
      "          0.7653, -0.5548, -0.4121, -0.5661, -0.1005, -0.4274,  1.5738, -0.0161,\n",
      "         -0.4870, -0.3474,  0.0301,  0.1000, -0.2672,  1.0769,  0.2298, -1.3681,\n",
      "          0.9118, -0.2162, -0.2045, -0.1228,  0.6591,  0.2663, -0.3929,  0.3022,\n",
      "          0.4462, -0.8000, -0.2286,  0.0954,  0.8200,  0.3994,  0.4426,  0.5865,\n",
      "         -0.5515,  0.2911,  0.9481, -0.9315,  0.8987, -0.7060,  0.3200, -0.0746,\n",
      "         -0.5113,  0.5292,  0.7781, -0.0754, -0.2274, -0.1190,  0.5055, -0.2200,\n",
      "          0.2127,  0.2989, -0.2898, -0.3203, -0.1393,  0.0229,  1.0932, -0.0060,\n",
      "          1.1902,  0.9079, -0.2163, -0.2635, -0.5229, -0.1986,  0.5354,  0.0902,\n",
      "          0.2822,  0.1657, -0.1331, -0.6243, -0.0049, -0.1202,  0.1877,  0.0299,\n",
      "         -0.6945, -0.4600,  0.0310, -1.3258,  1.3257,  0.5207, -0.1317,  0.6132,\n",
      "          0.3022, -0.8877, -0.2446, -0.5844, -0.1491,  0.4196, -0.6816, -0.4597,\n",
      "         -0.6502,  0.2576, -0.9086, -0.6689,  0.2079, -0.1279,  0.0402, -0.1550,\n",
      "         -0.1157,  0.2989, -0.3218, -0.0852, -0.7741, -0.6988, -0.0130,  0.0557,\n",
      "         -0.4864,  0.4067, -0.7675, -0.4103, -0.4424,  0.0131,  0.0687, -0.4770,\n",
      "         -0.2445, -0.3666,  0.3070, -0.2327,  0.8463,  0.6454, -0.2769, -0.8571,\n",
      "         -0.4176, -0.1025,  0.0840, -0.7087, -0.4640,  0.1725, -0.3735,  0.4310,\n",
      "         -0.0061,  0.2966, -0.9098, -1.2188, -0.0527,  0.2663,  0.5133,  0.2102,\n",
      "          0.0962,  0.8244, -0.2785, -0.6037, -0.8786, -0.2369, -0.2245,  0.6961,\n",
      "         -0.7898, -0.2309, -0.1648, -1.0660, -0.3010,  0.3121, -0.0441, -0.5681,\n",
      "         -0.8097, -1.0598,  0.0956,  0.5994,  1.4430,  0.4186, -0.5779, -2.4211,\n",
      "         -0.8602, -0.7368,  0.7118, -0.3513, -0.6845,  0.2346, -1.7452,  0.0356,\n",
      "         -0.6980, -0.4957, -0.5780, -0.1397,  0.0678,  0.5362,  1.1401, -1.0111,\n",
      "         -0.0887, -0.2666,  0.0484, -0.5300, -0.1520,  0.1459,  0.4667,  1.0140,\n",
      "         -0.7042,  0.1530,  0.6347, -0.8801, -0.3418,  0.9105, -0.4141,  0.4911,\n",
      "          0.4165, -0.9863, -0.0696,  0.5579, -1.1979, -0.2644,  0.2923,  0.1638]],\n",
      "       grad_fn=<AddmmBackward0>) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "m = model.lm_head\n",
    "input = torch.randn(2048).unsqueeze(0)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0214, -0.0098,  0.0198,  ...,  0.0061,  0.0228,  0.0089],\n",
       "        [-0.0070, -0.0275, -0.0021,  ...,  0.0172,  0.0036, -0.0223],\n",
       "        [ 0.0096, -0.0292, -0.0087,  ...,  0.0038, -0.0144,  0.0084],\n",
       "        ...,\n",
       "        [ 0.0282, -0.0086, -0.0042,  ...,  0.0147,  0.0013, -0.0266],\n",
       "        [ 0.0056, -0.0129,  0.0097,  ..., -0.0282, -0.0310,  0.0033],\n",
       "        [-0.0124, -0.0124,  0.0010,  ...,  0.0042, -0.0208,  0.0097]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.multihead_attn.out_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define customized loss function\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. embedding distance loss with L2 regularization\n",
    "def embedding_distance_loss_with_l2(predicted_emb, ground_truth_emb, model, l2_lambda=0.01):\n",
    "    # cosine similarity loss\n",
    "    cosine_sim = F.cosine_similarity(predicted_emb, ground_truth_emb, dim=-1)\n",
    "    # converting similarity to a loss (minimizing negative similarity)\n",
    "    cosine_sim_loss = 1 - cosine_sim.mean()\n",
    "    # L2 regularization\n",
    "    l2_reg = sum(param.pow(2.0).sum() for param in model.parameters())\n",
    "    # total loss\n",
    "    loss = cosine_sim_loss + l2_lambda * l2_reg\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 2. triplet loss\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    # compute distances\n",
    "    pos_dist = F.cosine_embedding_loss(anchor, positive, torch.tensor(1.0))\n",
    "    neg_dist = F.cosine_embedding_loss(anchor, negative, torch.tensor(-1.0))\n",
    "    # compute triplet loss\n",
    "    loss = F.relu(pos_dist - neg_dist + margin)\n",
    "    \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0.]]),\n",
       " torch.Size([3, 25]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones(25) # a = torch.ones(1, 25)\n",
    "b = torch.ones(22) # b = torch.ones(1, 22)\n",
    "c = torch.ones(15) # c = torch.ones(1, 15)\n",
    "batch = torch.nn.utils.rnn.pad_sequence(tuple([a, b, c]), batch_first=True, padding_value=0)\n",
    "batch, batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [item['input'][0] for item in batch]\n",
    "    ground_truth_embs = [item['output'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    padded_ground_truth_embs = pad_sequence(ground_truth_embs, batch_first=True, padding_value=0)\n",
    "    # len_to_be_padded = hyper_params[\"max_seq_length\"] - len(inputs[0])\n",
    "    # padded_inputs = torch.nn.functional.pad(tensor, (0, len_to_be_padded), \"constant\", 0)\n",
    "\n",
    "    return padded_inputs, padded_ground_truth_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, zeros\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "train_loader = DataLoader(tokenized_train_dataset.with_format(\"torch\"), batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(tokenized_eval_dataset.with_format(\"torch\"), batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Scheduler with warmup\n",
    "num_train_steps_per_epoch = len(train_loader)\n",
    "num_epochs = 1\n",
    "total_train_steps = num_train_steps_per_epoch * num_epochs\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=25, num_training_steps=total_train_steps, num_cycles=2)\n",
    "\n",
    "l2_lambda = 0.01  # L2 regularization weight\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, val_loader, device, l2_lambda):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, ground_truth_embs in val_loader:\n",
    "            input_tensors = inputs.to(device)\n",
    "            ground_truth_embs = ground_truth_embs.to(device)\n",
    "            \n",
    "            predicted_embs = model(input_tensors)\n",
    "            loss = embedding_distance_loss_with_l2(predicted_embs, ground_truth_embs, model, l2_lambda)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, ground_truth_embs in train_loader:  # each batch contains a tuple (inputs, ground_truth_emb)\n",
    "        input_tensors = inputs.to(device)\n",
    "        ground_truth_emb = ground_truth_embs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_emb = model(input_tensors)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = embedding_distance_loss_with_l2(predicted_emb.logits, ground_truth_emb, model, l2_lambda)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = evaluate(model, val_loader, device, l2_lambda)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and log history loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# load results\n",
    "# -------------------------------------------------\n",
    "# model loading example:\n",
    "import torch\n",
    "\n",
    "# Path to the saved checkpoint\n",
    "model_save_path = \"outputs/distance_based_loss_checkpoint_20240517_074628/final_model_checkpoint.pth\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(model_save_path)\n",
    "\n",
    "# Retrieve the saved log history\n",
    "log_history = checkpoint['log_history']\n",
    "\n",
    "# Restore the model state\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(\"Model and log history loaded successfully.\")\n",
    "\n",
    "# Now you can access the log history\n",
    "train_losses = log_history['train_loss']\n",
    "eval_losses = log_history['eval_loss']\n",
    "\n",
    "# print(\"Train loss history:\", train_loss_history)\n",
    "# print(\"Eval loss history:\", eval_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot log_history\n",
    "# Sorting values by steps to align the data points correctly\n",
    "train_losses.sort(key=lambda x: x[0])\n",
    "eval_losses.sort(key=lambda x: x[0])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Unzipping for plotting\n",
    "if eval_losses != []:\n",
    "    train_steps, train_loss_values = zip(*train_losses)\n",
    "    eval_steps, eval_loss_values = zip(*eval_losses)\n",
    "    plt.plot(train_steps, train_loss_values, label='Training Loss', marker='o')\n",
    "    plt.plot(eval_steps, eval_loss_values, label='Evaluation Loss', marker='x')\n",
    "else:\n",
    "    train_steps, train_loss_values = zip(*train_losses)\n",
    "    plt.plot(train_steps, train_loss_values, label='Training Loss', marker='o')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}\"\n",
    "\n",
    "# input = \"Hello, I am looking for a movie that will make me feel happy and excited. I love action movies with a lot of suspense and thrill. I also enjoy movies with a lot of drama and romance. Can you recommend a movie that will make me feel happy and excited? And please explain why you recommend this movie.\"\n",
    "input = \"Below is the previous historical purchases and reviews of the user:\\n```\\nItem title: The Gathering \\n Item description: Product Description Gathering, The (1976 TVM) (DVD) Edward Asner and Maureen Stapleton star in this poignant and heartwarming story of the reconciliation between a successful businessman and the family he long ago abandoned to pursue a career. When Adam Thornton (Asner) learns that he is terminally ill, his estranged wife (Stapleton) insists that he spend one last Christmas with his now-adult children. Adam agrees--but insists that they not know of his illness. Now, in an idyllic, snow-covered New England town, the Thornton family tries to find reunion before it is too late. ]]> Amazon.com A fondly remembered holiday item from the latter phase of a great age of TV movies, The Gathering has an irresistible idea and a splendid central performance from Edward Asner, the actor for whom the term gruff but lovable was surely coined. We learn in the opening seconds of the film that Asner's character, a well-to-do businessman named Adam Thornton, has been diagnosed with a terminal illness; he's got three months left, at best. Time to make amends, and Adam reaches out to his ex-wife (Maureen Stapleton) and his mostly estranged kids for a final Christmas gathering. You won't find many surprises along the way, but the script by veteran screenwriter James Poe takes time for the small things, and gives Asner a believably conflicted character to play. It's directed by Randal Kleiser, who was just a year away from jumping to a big-screen career and scoring a smash with Grease . There's also some interest in watching a batch of young actors in the early stages of their careers: Bruce Davison, Gregory Harrison, Veronica Hamel, and Stephanie Zimbalist are in the Thornton extended family. If The Gathering has the basic look and feel of a TV movie (with wintry location work in Ohio), it nevertheless creates an honest, earned glow as it sorts through one man's final accounting of what has mattered in his life. --Robert Horton \\n rating: 5.0 \\n review: This version is an OFFICIAL release by MGM studios with closed captioning.<br /><br />Yes the movie looks a little dated, the clothes, the cars, etc...but that's just the external stuff.<br /><br />The story itself is timeless.<br /><br />It's nice to finally have on dvd with captions.  Thanks MGM and Amazon.-------\\nItem title: 84 Charing Cross Road \\n Item description: An American writer forms an enduring relationship with a London bookseller which is carried on over 20 years and across two continents. Genre: Feature Film-Drama Rating: PG Release Date: 23-SEP-2003 Media Type: DVD \\n rating: 5.0 \\n review: I bought the book because of  this movie.  Even when I read it I hear the voices of the these two wonderful actors.-------\\nPlease infer the user's preference based on historical purchases and reviews, and then recommend an item for this user. Please just give the title of the recommended item.\"\n",
    "# input = \"hello, can you recommend me a random movie? Maybe a romantic one? And please describe the reason why you recommend this movie.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    # \"### Input:\\n{inputs}\\n\\n### Response:{outputs}\".format(inputs= input, outputs= \"\"),\n",
    "    # \"{inputs}\".format(inputs= input),\n",
    "    prompt_template.format(input, \"\"),    \n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "model(**inputs).loss['logits'][-1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
