{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "hyper_params = {\n",
    "    # Model hyperparameters\n",
    "    \"max_seq_length\": 4096, # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "    \"dtype\": None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    \"load_in_4bit\": True, # Use 4bit quantization to reduce memory usage. Can be False.,\n",
    "    \"model_name\": \"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    \"r\": 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # Add more to target more modules\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0, # Supports any, but = 0 is optimized\n",
    "    \"lora_bias\": \"none\", # Supports any, but = \"none\" is optimized\n",
    "    \"lora_use_gradient_checkpointing\": \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    \"lora_random_state\": 3407,\n",
    "    \"lora_use_rslora\": False, # We support rank stabilized LoRA\n",
    "    \"lora_loftq_config\": None, # And LoftQ\n",
    "    # Training hyperparameters\n",
    "    \"dataset_train_path\": \"./data/gemma_chat_train\",\n",
    "    \"dataset_eval_path\": \"./data/gemma_chat_eval\",\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_steps\": 15, # will replace num_warmup_steps in lr_scheduler_kwargs\n",
    "    # \"num_train_epochs\": 1, # choose between num_train_epochs and max_steps\n",
    "    \"max_steps\": 60, # choose between num_train_epochs and max_steps\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"fp16\": not torch.cuda.is_bf16_supported(),\n",
    "    \"bf16\": torch.cuda.is_bf16_supported(),\n",
    "    \"logging_steps\": 1,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"cosine_with_restarts\",\n",
    "    \"lr_scheduler_kwargs\": {\"num_cycles\": 2}, # \"num_warmup_steps\" and \"num_training_steps\" will be added automatically\n",
    "    \"seed\": 3407,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = hyper_params[\"max_seq_length\"] # 8192 | Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = hyper_params[\"dtype\"] # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = hyper_params[\"load_in_4bit\"] # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer= FastLanguageModel.from_pretrained(\n",
    "    model_name = hyper_params[\"model_name\"],\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load meta movies\n",
    "import os\n",
    "import json\n",
    "\n",
    "# read ./data/asins_small.json\n",
    "with open('./data/asins_small.json', 'r') as f:\n",
    "    asins_small = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = []\n",
    "for asin in asins_small:\n",
    "    ## movie asin\n",
    "    #print(asin)\n",
    "    ## movie title:\n",
    "    #print(asins_small[asin][0])\n",
    "    ## movie description:\n",
    "    #print(' '.join(asins_small[asin][1]))\n",
    "    tokens = tokenizer(asins_small[asin][0], return_tensors=\"pt\")\n",
    "    # movie_titles.append((tokens.input_ids[0][1:].tolist(), asins_small[asin][0]))\n",
    "    movie_titles.append(tokens.input_ids[0][1:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([62631, 576, 37295],\n",
       " [56170,\n",
       "  235303,\n",
       "  235256,\n",
       "  3855,\n",
       "  235248,\n",
       "  235284,\n",
       "  591,\n",
       "  235274,\n",
       "  235315,\n",
       "  235315,\n",
       "  235304,\n",
       "  235275,\n",
       "  591,\n",
       "  9739,\n",
       "  235275,\n",
       "  892,\n",
       "  41330,\n",
       "  235290,\n",
       "  1040,\n",
       "  235307])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles[2834], movie_titles[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the token tree (trie) for the movie descriptions\n",
    "# the root must be the start of sentence token\n",
    "# the child nodes must be the tokens that follow the parent node in the sentence\n",
    "# movie_titles is the list of lists of tokens\n",
    "class TrieNode:\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.children = {}\n",
    "\n",
    "def construct_token_tree(movie_titles, bos_token):\n",
    "    root = TrieNode(bos_token)  # Root node is the start of sentence token\n",
    "\n",
    "    for title in movie_titles:\n",
    "        current_node = root\n",
    "        for token in title:\n",
    "            if token not in current_node.children:\n",
    "                current_node.children[token] = TrieNode(token)\n",
    "            current_node = current_node.children[token]\n",
    "\n",
    "    return root\n",
    "\n",
    "token_tree = construct_token_tree(movie_titles, tokenizer.bos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36911"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_tree.children[36911].token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
